<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Comprehensive Guide to Datasets and Dataloaders in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Comprehensive Guide to Datasets and Dataloaders in PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comprehensive-guide-to-datasets-and-dataloaders-in-pytorch-4d20f973d5d5?source=collection_archive---------6-----------------------#2024-06-15">https://towardsdatascience.com/comprehensive-guide-to-datasets-and-dataloaders-in-pytorch-4d20f973d5d5?source=collection_archive---------6-----------------------#2024-06-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9465" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The full guide to creating custom datasets and dataloaders for different models in PyTorch</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@rtdcunha?source=post_page---byline--4d20f973d5d5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan D'Cunha" class="l ep by dd de cx" src="../Images/7a39859e2b5e5b09ef2c60aaf6bb75ac.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NhxQFjIuOQ0Xh_bfwEk4vQ@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4d20f973d5d5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@rtdcunha?source=post_page---byline--4d20f973d5d5--------------------------------" rel="noopener follow">Ryan D'Cunha</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4d20f973d5d5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6e4efac0de583f1e865e0158db53f194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*poblxhms3Asf992XLDnNfg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Source: GPT4o Generated</figcaption></figure><p id="bac9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before you can build a machine learning model, you need to load your data into a dataset. Luckily, PyTorch has many commands to help with this entire process (if you are not familiar with PyTorch I recommend refreshing on the basics <a class="af ny" href="https://medium.com/@rtdcunha/a-beginners-guide-to-pytorch-6bc600ca4b8d" rel="noopener">here</a>).</p><p id="b4a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">PyTorch has good documentation to help with this process, but I have not found any comprehensive documentation or tutorials towards custom datasets. I’m first going to start with creating basic premade datasets and then work my way up to creating datasets from scratch for different models!</p><h1 id="6758" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What is a Dataset and Dataloader?</h1><p id="2966" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Before we dive into code for different use cases, let’s understand the difference between the two terms. Generally, you first create your dataset and then create a dataloader. A <strong class="ne fr">dataset</strong> contains the features and labels from each data point that will be fed into the model. A <strong class="ne fr">dataloader</strong> is a custom PyTorch iterable that makes it easy to load data with added features.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="faf6" class="pe oa fq pb b bg pf pg l ph pi">DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,<br/>           batch_sampler=None, num_workers=0, collate_fn=None,<br/>           pin_memory=False, drop_last=False, timeout=0,<br/>           worker_init_fn=None, *, prefetch_factor=2,<br/>           persistent_workers=False)</span></pre><p id="8f1b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The most common arguments in the dataloader are <em class="pj">batch_size</em>, <em class="pj">shuffle</em> (usually only for the training data), <em class="pj">num_workers</em> (to multi-process loading the data), and <em class="pj">pin_memory</em> (to put the fetched data Tensors in pinned memory and enable faster data transfer to CUDA-enabled GPUs).</p><blockquote class="pk pl pm"><p id="b2ff" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is recommended to set pin_memory = True instead of specifying num_workers due to multiprocessing complications with CUDA.</p></blockquote><h1 id="fc88" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Loading a Premade Dataset</h1><p id="ce09" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">In the case that your dataset is downloaded from online or locally, it will be extremely simple to create the dataset. I think PyTorch has good <a class="af ny" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank">documentation</a> on this, so I will be brief.</p><p id="abbd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you know the dataset is either from PyTorch or PyTorch-compatible, simply call the necessary imports and the dataset of choice:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="c74d" class="pe oa fq pb b bg pf pg l ph pi">from torch.utils.data import Dataset<br/>from torchvision import datasets<br/>from torchvision.transforms imports ToTensor<br/><br/>data = torchvision.datasets.CIFAR10('path', train=True, transform=ToTensor())</span></pre><p id="d606" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each dataset will have unique arguments to pass into it (found <a class="af ny" href="https://pytorch.org/vision/main/datasets.html" rel="noopener ugc nofollow" target="_blank">here</a>). In general, it will be the path the dataset is stored at, a boolean indicating if it needs to be downloaded or not (conveniently called download), whether it is training or testing, and if transforms need to be applied.</p><h1 id="1d0f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Transforms</h1><p id="581c" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">I dropped in that transforms can be applied to a dataset at the end of the last section, but what actually is a transform?</p><p id="06aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A <strong class="ne fr">transform</strong> is a method of manipulating data for preprocessing an image. There are many different facets to transforms. The most common transform, <em class="pj">ToTensor()</em>, will convert the dataset to tensors (needed to input into any model). Other transforms built into PyTorch (<em class="pj">torchvision.transforms</em>) include flipping, rotating, cropping, normalizing, and shifting images. These are typically used so the model can generalize better and doesn’t overfit to the training data. Data augmentations can also be used to artificially increase the size of the dataset if needed.</p><blockquote class="pk pl pm"><p id="be1b" class="nc nd pj ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Beware most torchvision transforms only accept Pillow image or tensor formats (not numpy). To convert, simply use</p></blockquote><p id="a199" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To convert from numpy, either create a torch tensor or use the following:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="1cc0" class="pe oa fq pb b bg pf pg l ph pi">From PIL import Image<br/># assume arr is a numpy array<br/># you may need to normalize and cast arr to np.uint8 depending on format<br/>img = Image.fromarray(arr)</span></pre><p id="2332" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Transforms can be applied simultaneously using <em class="pj">torchvision.transforms.compose</em>. You can combine as many transforms as needed for the dataset. An example is shown below:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="d0eb" class="pe oa fq pb b bg pf pg l ph pi">import torchvision.transforms.Compose<br/><br/>dataset_transform = transforms.Compose([<br/>        transforms.RandomResizedCrop(256),<br/>        transforms.RandomHorizontalFlip(),<br/>        transforms.ToTensor(),<br/>        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])<br/>])</span></pre><p id="f76b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Be sure to pass the saved transform as an argument into the dataset for it to be applied in the dataloader.</p><h1 id="e5f9" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Creating a Custom Dataset</h1><p id="08f9" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">In most cases of developing your own model, you will need a custom dataset. A common use case would be transfer learning to apply your own dataset on a pretrained model.</p><p id="045c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are 3 required parts to a PyTorch dataset class: <strong class="ne fr">initialization</strong>, <strong class="ne fr">length</strong>, and <strong class="ne fr">retrieving an element</strong>.</p><p id="82f4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">__init__</strong>: To initialize the dataset, pass in the raw and labeled data. The best practice is to pass in the raw image data and labeled data separately.</p><p id="04f6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">__len__</strong>: Return the length of the dataset. Before creating the dataset, the raw and labeled data should be checked to be the same size.</p><p id="8036" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">__getitem__</strong>: This is where all the data handling occurs to return a given index (idx) of the raw and labeled data. If any transforms need to be applied, the data must be converted to a tensor and transformed. If the initialization contained a path to the dataset, the path must be opened and data accessed/preprocessed before it can be returned.</p><p id="b03a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Example dataset for a semantic segmentation model:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="45b7" class="pe oa fq pb b bg pf pg l ph pi">from torch.utils.data import Dataset<br/>from torchvision import transforms<br/><br/>class ExampleDataset(Dataset):<br/>    """Example dataset"""<br/><br/>    def __init__(self, raw_img, data_mask, transform=None):<br/>        self.raw_img = raw_img<br/>        self.data_mask = data_mask<br/>        self.transform = transform<br/><br/>    def __len__(self):<br/>        return len(self.raw_img)<br/><br/>    def __getitem__(self, idx):<br/>        if torch.is_tensor(idx):<br/>            idx = idx.tolist()<br/><br/>        image = self.raw_img[idx]<br/>        mask = self.data_mask[idx]<br/><br/>        sample = {'image': image, 'mask': mask}<br/><br/>        if self.transform:<br/>            sample = self.transform(sample)<br/><br/>        return sample</span></pre></div></div></div><div class="ab cb pn po pp pq" role="separator"><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dc4b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is important to look at the input of the first layer of the model (especially for a pretrained model), to make sure the shape of the data matches the input shape. If not, you may need to adjust the dimensions. This is common if the input image is a greyscale n x n array, but the model requires a channel dimension (1 x 256 x 256).</p><p id="5964" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After the dataset and dataloader are applied, the format of the data should be NCHW (batch size, channel size, height, width). Reformatting can be done in the __getitem__ method before outputting to the model.</p><h2 id="322f" class="pv oa fq bf ob pw px py oe pz qa qb oh nl qc qd qe np qf qg qh nt qi qj qk ql bk">Splitting the Dataset</h2><p id="f047" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">While creating the dataset, you may want to split into a training, testing, and validation dataset. This can be done using a built-in PyTorch function and specifying the sizes. Make sure the dataset splits add up to the total length of the dataset.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="aa13" class="pe oa fq pb b bg pf pg l ph pi">from torch.utils.data import random_split<br/><br/>train, val, test = random_split(dataset, [train_size, val_size, test_size])</span></pre><h2 id="83c4" class="pv oa fq bf ob pw px py oe pz qa qb oh nl qc qd qe np qf qg qh nt qi qj qk ql bk">Data Labels</h2><p id="c0f9" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">There can be different data labels depending on the model: classification, object detection, or segmentation. A model classification label will contain a class label if it is multiclass or a binary number if it is binary. An object detection model will contain a bounding box of coordinates as the label. A semantic segmentation model will contain a binary mask matching the size of the raw image data. An instance segmentation contains all mask data in the raw image data.</p></div></div></div><div class="ab cb pn po pp pq" role="separator"><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt pu"/><span class="pr by bm ps pt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="54fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Creating a dataset is a foundational aspect of model development. By having a faulty dataset, there will be many errors downstream in training or evaluating the model. The most common errors to watch out for are shape or type mismatches. By following this and referring to PyTorch docs, you should have a working dataset!</p><h1 id="02b6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><div class="qm qn qo qp qq qr"><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html?source=post_page-----4d20f973d5d5--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qs ab ig"><div class="qt ab co cb qu qv"><h2 class="bf fr hw z io qw iq ir qx it iv fp bk">Datasets &amp; DataLoaders - PyTorch Tutorials 2.3.0+cu121 documentation</h2><div class="qy l"><h3 class="bf b hw z io qw iq ir qx it iv dx">Learn the Basics || Quickstart || Tensors || Datasets &amp; DataLoaders || Transforms || Build Model || Autograd ||…</h3></div><div class="qz l"><p class="bf b dy z io qw iq ir qx it iv dx">pytorch.org</p></div></div></div></a></div><div class="qm qn qo qp qq qr"><a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html?source=post_page-----4d20f973d5d5--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qs ab ig"><div class="qt ab co cb qu qv"><h2 class="bf fr hw z io qw iq ir qx it iv fp bk">Writing Custom Datasets, DataLoaders and Transforms - PyTorch Tutorials 2.3.0+cu121 documentation</h2><div class="qy l"><h3 class="bf b hw z io qw iq ir qx it iv dx">Author: Sasank Chilamkurthy A lot of effort in solving any machine learning problem goes into preparing the data…</h3></div><div class="qz l"><p class="bf b dy z io qw iq ir qx it iv dx">pytorch.org</p></div></div><div class="ra l"><div class="rb l rc rd re ra rf lr qr"/></div></div></a></div><div class="qm qn qo qp qq qr"><a href="https://pytorch.org/vision/stable/transforms.html?source=post_page-----4d20f973d5d5--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qs ab ig"><div class="qt ab co cb qu qv"><h2 class="bf fr hw z io qw iq ir qx it iv fp bk">Transforming and augmenting images - Torchvision 0.18 documentation</h2><div class="qy l"><h3 class="bf b hw z io qw iq ir qx it iv dx">Torchvision supports common computer vision transformations in the torchvision.transforms and torchvision.transforms.v2…</h3></div><div class="qz l"><p class="bf b dy z io qw iq ir qx it iv dx">pytorch.org</p></div></div></div></a></div><div class="qm qn qo qp qq qr"><a href="https://pytorch.org/vision/main/generated/torchvision.transforms.Compose.html?source=post_page-----4d20f973d5d5--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qs ab ig"><div class="qt ab co cb qu qv"><h2 class="bf fr hw z io qw iq ir qx it iv fp bk">Compose - Torchvision main documentation</h2><div class="qy l"><h3 class="bf b hw z io qw iq ir qx it iv dx">Run PyTorch locally or get started quickly with one of the supported cloud platforms</h3></div><div class="qz l"><p class="bf b dy z io qw iq ir qx it iv dx">pytorch.org</p></div></div></div></a></div></div></div></div></div>    
</body>
</html>