<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering Sample Size Calculations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering Sample Size Calculations</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-sample-size-calculations-75afcddd2ff3?source=collection_archive---------6-----------------------#2024-10-09">https://towardsdatascience.com/mastering-sample-size-calculations-75afcddd2ff3?source=collection_archive---------6-----------------------#2024-10-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6dac" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A/B Testing, Reject Inference, and How to Get the Right Sample Size for Your Experiments</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lucasbraga461?source=post_page---byline--75afcddd2ff3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lucas Braga" class="l ep by dd de cx" src="../Images/a652476cfec8d4f129d2d47a64c3e8c3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*3H1gxVegE0mWYl3nw5xXAg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--75afcddd2ff3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lucasbraga461?source=post_page---byline--75afcddd2ff3--------------------------------" rel="noopener follow">Lucas Braga</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--75afcddd2ff3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/1a94f5016a3129b52d57233ef1ea9dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7DyKUwZ1bqGjBcpyviVUDw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image created by the author</figcaption></figure><p id="6290" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">There are different statistical formulas for different scenarios. The first question to ask is: are you <strong class="nd fr">comparing two groups</strong>, such as in an <strong class="nd fr">A/B test</strong>, or are you <strong class="nd fr">selecting a sample from a population</strong> that is large enough to represent it?</p><p id="40c5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The latter is typically used in cases like holdout groups in transactions. These holdout groups can be crucial for assessing the performance of fraud prevention rules or for reject inference, where machine learning models for fraud detection are retrained. The holdout group is beneficial because it contains transactions that weren’t blocked by any rules or models, providing an unbiased view of performance. However, to ensure the holdout group is representative, you need to select a sample size that accurately reflects the population, which, together with sample sizing for A/B testing, we’ll explore it in this article.</p><p id="d26f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">After determining whether you’re <strong class="nd fr">comparing two groups</strong> (like in A/B testing) or <strong class="nd fr">taking a representative sample</strong> (like for reject inference), the next step is to define your success metric. <strong class="nd fr">Is it a proportion or an absolute number?</strong> For example, <strong class="nd fr">comparing two proportions</strong> could involve conversion rates or default rates, where the number of default transactions is divided by the total number of transactions. On the other hand, <strong class="nd fr">comparing two means</strong> applies when dealing with absolute values, such as total revenue or GMV (Gross Merchandise Value). In this case, you would compare the average revenue per customer, assuming customer-level randomization in your experiment.</p><h1 id="7591" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">1. Comparing two groups (e.g. A/B testing) — Sample Size</h1><p id="f3d9" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">The section 1.1 is about comparing two means, but most of the principles presented there will be the same for section 1.2.</p><h1 id="d61b" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">1.1. Comparing two Means (metric the average of an absolute number)</h1><p id="56f0" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">In this scenario, we are comparing two groups: a control group and a treatment group. The control group consists of customers with access to €100 credit through a lending program, while the treatment group consists of customers with access to €200 credit under the same program.</p><p id="bbb6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The goal of the experiment is to determine whether increasing the credit limit leads to higher customer spending.</p><p id="dbdc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Our success metric is defined as the <strong class="nd fr">average amount spent per customer per week</strong>, measured in euros.</p><p id="f8a9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the goal and success metric established, in a typical A/B test, we would also define the hypothesis, the randomization unit (in this case, the customer), and the target population (new customers granted credit). However, since the focus of this document is on sample size, we will not go into those details here.</p><p id="20e3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We will compare the <strong class="nd fr">average weekly spending per customer</strong> between the control group and the treatment group. Let’s proceed with calculating this metric using the following script:</p><p id="4bde" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="oy">Script 1: Computing the success metric, branch: Germany, period: 2024–05–01 to 2024–07–31.</em></p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="6d8c" class="pd ny fq pa b bg pe pf l pg ph">WITH customer_spending AS (<br/>SELECT<br/> branch_id,<br/> FORMAT_DATE('%G-%V', DATE(transaction_timestamp)) AS week_of_year,<br/> customer_id,<br/> SUM(transaction_value) AS total_amount_spent_eur<br/>FROM `project.dataset.credit_transactions`<br/>WHERE 1=1<br/> AND transaction_date BETWEEN '2024-05-01' AND '2024-07-31'<br/> AND branch_id LIKE 'Germany'<br/>GROUP BY branch_id, week_of_year, customer_id<br/>)<br/>, agg_per_week AS (<br/>SELECT<br/> branch_id,<br/> week_of_year,<br/> ROUND(AVG(total_amount_spent_eur), 1) AS avg_amount_spent_eur_per_customer,<br/>FROM customer_spending<br/>GROUP BY branch_id, week_of_year<br/>)<br/>SELECT *<br/>FROM agg_per_week<br/>ORDER BY 1,2;</span></pre><p id="4197" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the results, we observe the metric <strong class="nd fr">avg_amount_spent_eur_per_customer</strong> on a weekly basis. Over the last four weeks, the values have remained relatively stable, ranging between 35 and 54 euros. However, when considering all weeks over the past two months, the variance is higher. (See Image 1 for reference.)</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pi"><img src="../Images/279deef453754e038e447a2186f57ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/0*Ei9YM0Oc_ApmwNk7"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 1: Results of the script 1.</em></figcaption></figure><p id="c60f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Next, we calculate the variance of the success metric. To do this, we will use <strong class="nd fr">Script 2</strong> to compute both the variance and the average of the weekly spending across all weeks.</p><p id="3e87" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="oy">Script 2: Query to compute the variance of the success metric and average over all weeks.</em></p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="e35c" class="pd ny fq pa b bg pe pf l pg ph">WITH customer_spending AS (<br/>SELECT<br/> branch_id,<br/> FORMAT_DATE('%G-%V', DATE(transaction_timestamp)) AS week_of_year,<br/> customer_id,<br/> SUM(transaction_value) AS total_amount_spent_eur<br/>FROM `project.dataset.credit_transactions`<br/>WHERE 1=1<br/> AND transaction_date BETWEEN '2024-05-01' AND '2024-07-31'<br/> AND branch_id LIKE 'Germany'<br/>GROUP BY branch_id, week_of_year, customer_id<br/>)<br/>, agg_per_week AS (<br/>SELECT<br/> branch_id,<br/> week_of_year,<br/> ROUND(AVG(total_amount_spent_eur), 1) AS avg_amount_spent_eur_per_customer,<br/>FROM customer_spending<br/>GROUP BY branch_id, week_of_year<br/>)<br/>SELECT<br/> ROUND(AVG(avg_amount_spent_eur_per_customer),1) AS avg_amount_spent_eur_per_customer_per_week,<br/> ROUND(VAR_POP(avg_amount_spent_eur_per_customer),1) AS variance_avg_amount_spent_eur_per_customer<br/>FROM agg_per_week<br/>ORDER BY 1,2;</span></pre><p id="4989" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The result from <strong class="nd fr">Script 2</strong> shows that the variance is approximately 145.8 (see Image 2). Additionally, the <strong class="nd fr">average amount spent per user</strong>, considering all weeks over the past two months, is <strong class="nd fr">49.5 euros</strong>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pk"><img src="../Images/64278faf0614a71b9041fb7f28ac45ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*ytQKuFadsZ4ZvBmK"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 2: Results of Script 2.</em></figcaption></figure><p id="81c3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now that we’ve calculated the metric and found the average weekly spending per customer to be approximately <strong class="nd fr">49.5 euros</strong>, we can define the <strong class="nd fr">Minimum Detectable Effect (MDE)</strong>. Given the increase in credit from €100 to €200, we aim to detect a <strong class="nd fr">10% increase</strong> in spending, which corresponds to a new average of <strong class="nd fr">54.5 euros</strong> per customer per week.</p><p id="b583" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the variance calculated (145.8) and the MDE established, we can now plug these values into the formula to calculate the <strong class="nd fr">sample size</strong> required. We’ll use default values for <strong class="nd fr">alpha (5%)</strong> and <strong class="nd fr">beta (20%)</strong>:</p><ul class=""><li id="4183" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Significance Level (Alpha’s default value is α = 5%)</strong>: The alpha is a predetermined threshold used as a criteria to reject the null hypothesis. Alpha is the type I error (false positive), and the p-value needs to be lower than the alpha, so that we can reject the null hypothesis.</li><li id="a889" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Statistical Power (Beta’s default value is β = 20%)</strong>: It’s the probability that a test correctly rejects the null hypothesis when the alternative hypothesis is true, i.e. detecting an effect when the effect is present. Statistical Power = 1 — β, and β is the type II error (false negative).</li></ul><p id="c7c2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is the formula to calculate the required sample size per group (control and treatment) for comparing two means in a typical A/B test scenario:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pt"><img src="../Images/3319033fd8fc46b6a2bfc7d696caa84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*zIavNFRmVgbsPivW"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 3: Formula to calculate sample size when comparing two means.</em></figcaption></figure><ul class=""><li id="7588" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">n</strong> is the sample size per group.</li><li id="d8ab" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">σ²</strong> is the variance of the metric being tested (in this case, <strong class="nd fr">145.8</strong>). The factor 2σ² is used because we calculate the <strong class="nd fr">pooled variance</strong>, making it unbiased when comparing two samples.</li><li id="7bdb" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">δ (Delta)</strong>, represents the <strong class="nd fr">minimum detectable difference in means </strong>(effect size), which is the change we want to detect. That is calculated as: δ² = (μ₁ — μ₂)² , where <strong class="nd fr">μ₁</strong> is the mean of the control group and <strong class="nd fr">μ₂</strong> is the mean of the treatment group.</li><li id="5d46" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Zα/2</strong>​ is the <strong class="nd fr">z-score</strong> for the corresponding confidence level (e.g., <strong class="nd fr">1.96</strong> for <strong class="nd fr">95% confidence level</strong>).</li><li id="e1ad" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Zβ</strong> is the <strong class="nd fr">z-score</strong> associated with the desired power of the test (e.g., <strong class="nd fr">0.84</strong> for <strong class="nd fr">80% power</strong>).</li></ul><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="419a" class="pd ny fq pa b bg pe pf l pg ph">n = (2 * 145.8 * (1.96+0.84)^2) / (54.5-49.5)^2<br/>-&gt; n = 291.6 * 7.84 / 25<br/>-&gt; n = 2286.1 / 25<br/>-&gt; n =~ 92</span></pre><p id="0424" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Try it on my web app calculator at <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">Sample Size Calculator</a>, as shown in <strong class="nd fr">App Screenshot 1</strong>:</p><ul class=""><li id="0290" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Confidence Level</strong>: 95%</li><li id="9e4a" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Statistical Power</strong>: 80%</li><li id="f5f0" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Variance</strong>: 145.8</li><li id="589a" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Difference to Detect (Delta)</strong>: 5 (because the expected change is from €49.50 to €54.50)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/713d7337eda7cae42862e030b17ae896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KZSmy84-yqyOqk4X"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">App screenshot 1: Calculating the sample for comparing two means.</em></figcaption></figure><p id="679e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Based on the previous calculation, we would need <strong class="nd fr">92 users</strong> in the control group and <strong class="nd fr">92 users</strong> in the treatment group, for a total of <strong class="nd fr">184 samples</strong>.</p><p id="c1cb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s explore how changing the <strong class="nd fr">Minimum Detectable Effect (MDE)</strong> impacts the sample size. Smaller MDEs require larger sample sizes. For example, if we were aiming to detect a change of only <strong class="nd fr">€1 increase</strong> on average per user, instead of the <strong class="nd fr">€5 increase (10%)</strong> we used previously, the required sample size would increase significantly.</p><p id="dbc1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The smaller the MDE, the more sensitive the test needs to be, which means we need a larger sample to reliably detect such a small effect.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="4362" class="pd ny fq pa b bg pe pf l pg ph">n = (2 * 145.8 * (1.96+0.84)^2) / (50.5-49.5)^2<br/>-&gt; n = 291.6 * 7.84 / 1<br/>-&gt; n = 2286.1 / 1<br/>-&gt; n =~ 2287</span></pre><p id="6388" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We enter the following parameters into the web app calculator at <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">Sample Size Calculator</a>, as shown in <strong class="nd fr">App Screenshot 2</strong>:</p><ul class=""><li id="3c69" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Confidence Level</strong>: 95%</li><li id="ec2f" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Statistical Power</strong>: 80%</li><li id="faa3" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Variance</strong>: 145.8</li><li id="4be4" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Difference to Detect (Delta)</strong>: 1 (because the expected change is from €49.50 to €50.50)</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pw"><img src="../Images/dadc8ff02400ac38d2c7af78ab2fb1f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*yaU3JF7B8iUxpY5y"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">App screenshot 2: Calculating the sample for comparing two means with Delta = 1.</em></figcaption></figure><p id="660c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To detect a smaller effect, such as a <strong class="nd fr">€1 increase</strong> per user, we would require <strong class="nd fr">2,287 users</strong> in the control group and <strong class="nd fr">2,287 users</strong> in the treatment group, resulting in a total of <strong class="nd fr">4,574 samples</strong>.</p><p id="1e38" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Next, we’ll adjust the <strong class="nd fr">statistical power</strong> and <strong class="nd fr">significance level</strong> to recompute the required sample size. But first, let’s take a look at the <strong class="nd fr">z-score table</strong> to understand how the <strong class="nd fr">Z-value</strong> is derived.</p><p id="3696" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ve set <strong class="nd fr">beta = 0.2</strong>, meaning the current statistical power is <strong class="nd fr">80%</strong>. Referring to the z-score table (see <strong class="nd fr">Image 4</strong>), this corresponds to a <strong class="nd fr">z-score of 0.84</strong>, which is the value used in our previous formula.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj px"><img src="../Images/0e6df69db82894b33b17d0e9ac808d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QLY7Q_yEafmVF-GO"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 4: Finding the z-score for a statistical power of 80% on z-score table.</em></figcaption></figure><p id="8d16" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If we now adjust <strong class="nd fr">beta to 10%</strong>, which corresponds to a <strong class="nd fr">statistical power of 90%</strong>, we will find a <strong class="nd fr">z-value of 1.28</strong>. This value can be found on the z-score table (see <strong class="nd fr">Image 5</strong>).</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="8e58" class="pd ny fq pa b bg pe pf l pg ph">n = (2 * 145.8 * (1.96+1.28)^2) / (50.5-49.5)^2<br/>-&gt; n = 291.6 * 10.49 / 1<br/>-&gt; n = 3061.1 / 1<br/>-&gt; n =~ 3062</span></pre><p id="1e16" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the adjustment to a <strong class="nd fr">beta of 10%</strong> (statistical power of 90%) and using the <strong class="nd fr">z-value of 1.28</strong>, we now require <strong class="nd fr">3,062 users</strong> in both the control and treatment groups, for a total of <strong class="nd fr">6,124 samples</strong>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj py"><img src="../Images/a6d036fdb714dd539fce84c0d01c07e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H3IHAgqyZHrdsNYd"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 5: Finding the z-score for a statistical power of 90% on the z-score table.</em></figcaption></figure><p id="0231" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s determine how much traffic the <strong class="nd fr">6,124 samples</strong> represent. We can calculate this by finding the average volume of distinct customers per week. <strong class="nd fr">Script 3</strong> will help us retrieve this information using the time period from <strong class="nd fr">2024–05–01 to 2024–07–31</strong>.</p><p id="1667" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="oy">Script 3: Query to calculate the average weekly volume of distinct customers.</em></p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="bbed" class="pd ny fq pa b bg pe pf l pg ph">WITH customer_volume AS (<br/>SELECT<br/> branch_id,<br/> FORMAT_DATE('%G-%V', DATE(transaction_timestamp)) AS week_of_year,<br/> COUNT(DISTINCT customer_id) AS cntd_customers<br/>FROM `project.dataset.credit_transactions`<br/>WHERE 1=1<br/> AND transaction_date BETWEEN '2024-05-01' AND '2024-07-31'<br/> AND branch_id LIKE 'Germany'<br/>GROUP BY branch_id, week_of_year<br/>)<br/>SELECT<br/> ROUND(AVG(cntd_customers),1) AS avg_cntd_customers<br/>FROM customer_volume;</span></pre><p id="cb2c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The result from <strong class="nd fr">Script 3</strong> shows that, on average, there are <strong class="nd fr">185,443 distinct customers</strong> every week (see <strong class="nd fr">Image 5</strong>). Therefore, the <strong class="nd fr">6,124 samples</strong> represent approximately <strong class="nd fr">3.35%</strong> of the total weekly customer base.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pz"><img src="../Images/c2bdc30698591acd5f31b814526e949e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/0*saJEYVD2jeFbLNCV"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 5: Results from Script 3.</em></figcaption></figure><h1 id="a2d9" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">1.2. Comparing two Proportions (e.g. conversion rate, default rate)</h1><p id="0340" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">While most of the principles discussed in the previous section remain the same, the formula for comparing <strong class="nd fr">two proportions</strong> differs. This is because, instead of pre-computing the variance of the metric, we will now focus on the <strong class="nd fr">expected proportions of success</strong> in each group (see <strong class="nd fr">Image 6</strong>).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qa"><img src="../Images/d8c43c36156bef1324aebd54ec2e8021.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/0*95cVqRn3YHIexupF"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 6: Formula to calculate sample size for comparing two proportions.</em></figcaption></figure><p id="48c5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s return to the same scenario: we are comparing two groups. The control group consists of customers who have access to <strong class="nd fr">€100 credit</strong> on the <strong class="nd fr">credit lending program</strong>, while the treatment group consists of customers who have access to <strong class="nd fr">€200 credit</strong> in the same program.</p><p id="ab47" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This time, the success metric we are focusing on is the <strong class="nd fr">default rate</strong>. This could be part of the same experiment discussed in <strong class="nd fr">Section 1.1</strong>, where the default rate acts as a <strong class="nd fr">guardrail metric</strong>, or it could be an entirely separate experiment. In either case, the hypothesis is that giving customers more credit could lead to a higher default rate.</p><p id="ef2e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The goal of this experiment is to determine whether an increase in credit limits results in a <strong class="nd fr">higher default rate</strong>.</p><p id="3a97" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We define the success metric as the <strong class="nd fr">average default rate</strong> for all customers during the experiment week. Ideally, the experiment would run over a longer period to capture more data, but if that’s not possible, it’s essential to choose a <strong class="nd fr">week that is unbiased</strong>. You can verify this by analyzing the default rate over the past <strong class="nd fr">12–16 weeks</strong> to identify any specific patterns related to certain weeks of the month.</p><p id="d888" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s examine the data. <strong class="nd fr">Script 4</strong> will display the <strong class="nd fr">default rate per week</strong>, and the results can be seen in <strong class="nd fr">Image 7</strong>.</p><p id="ec5d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="oy">Script 4: Query to retrieve default rate per week.</em></p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="d66e" class="pd ny fq pa b bg pe pf l pg ph">SELECT<br/> branch_id,<br/> date_trunc(transaction_date, week) AS week_of_order,<br/> SUM(transaction_value) AS sum_disbursed_gmv,<br/> SUM(CASE WHEN is_completed THEN transaction_value ELSE 0 END) AS sum_collected_gmv,<br/> 1-(SUM(CASE WHEN is_completed THEN transaction_value ELSE 0 END)/SUM(transaction_value)) AS default_rate,<br/>FROM `project.dataset.credit_transactions`<br/>WHERE transaction_date BETWEEN '2024-02-01' AND '2024-04-30'<br/>  AND branch_id = 'Germany'<br/>GROUP BY 1,2<br/>ORDER BY 1,2;</span></pre><p id="e721" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Looking at the default rate metric, we notice some variability, particularly in the older weeks, but it has remained relatively stable over the past 5 weeks. The average default rate for the last 5 weeks is <strong class="nd fr">0.070</strong>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qb"><img src="../Images/acfeee9cf1209936b699b73877b1e6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qIaVBSFtNB4hxZGE"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 7: Results of the default rate per week.</em></figcaption></figure><p id="8deb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s assume that this default rate will be representative of the control group. The next question is: what default rate in the treatment group would be considered unacceptable? We can set the threshold: if the default rate in the treatment group increases to <strong class="nd fr">0.075</strong>, it would be too high. However, anything up to <strong class="nd fr">0.0749</strong> would still be acceptable.</p><p id="6a85" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A default rate of <strong class="nd fr">0.075</strong> represents approximately <strong class="nd fr">7.2% increase</strong> from the control group rate of <strong class="nd fr">0.070</strong>. This difference — 7.2% — is our <strong class="nd fr">Minimum Detectable Effect (MDE)</strong>.</p><p id="7104" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With these data points, we are now ready to compute the required <strong class="nd fr">sample size</strong>.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="92c6" class="pd ny fq pa b bg pe pf l pg ph">n = ( ((1.96+0.84)^2) * ((0.070*(1-0.070) + 0.075*(1-0.075)) ) / ( (0.070-0.075)^2 )<br/>-&gt; n = 7.84 * 0.134475 / 0.000025<br/>-&gt; n = 1.054284 / 0.000025<br/>-&gt; n =~ 42,171</span></pre><p id="b88d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We enter the following parameters into the web app calculator at <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">Sample Size Calculator</a>, as shown in <strong class="nd fr">App Screenshot 3</strong>:</p><ul class=""><li id="618b" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Confidence Level</strong>: 95%</li><li id="7efd" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Statistical Power</strong>: 80%</li><li id="eac3" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">First Proportion (p1)</strong>: 0.070</li><li id="8733" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Second Proportion (p2)</strong>: 0.075</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qc"><img src="../Images/b4b40f485edaa98373743b979f70ec20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*TTjgrAdb-nWGHFmg"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">App screenshot 3: Calculating the sample size for comparing two proportions.</em></figcaption></figure><p id="e3c3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To detect a <strong class="nd fr">7.2% increase</strong> in the default rate (from <strong class="nd fr">0.070</strong> to <strong class="nd fr">0.075</strong>), we would need <strong class="nd fr">42,171 users</strong> in both the control group and the treatment group, resulting in a total of <strong class="nd fr">84,343 samples</strong>.</p><p id="d41a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A sample size of <strong class="nd fr">84,343</strong> is quite large! We may not even have enough customers to run this analysis. But let’s explore why this is the case. We haven’t changed the default parameters for <strong class="nd fr">alpha</strong> and <strong class="nd fr">beta</strong>, meaning we kept the <strong class="nd fr">significance level</strong> at the default <strong class="nd fr">5%</strong> and the <strong class="nd fr">statistical power</strong> at the default <strong class="nd fr">80%</strong>. As we’ve discussed earlier, we could have been more conservative by choosing a lower significance level to reduce the chance of false positives, or we could have increased the statistical power to minimize the risk of false negatives.</p><p id="34ca" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So, what contributed to the large sample size? Is it the <strong class="nd fr">MDE</strong> of <strong class="nd fr">7.2%</strong>? The short answer: <strong class="nd fr">not exactly</strong>.</p><p id="74ca" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Consider this alternative scenario: we maintain the same <strong class="nd fr">significance level (5%)</strong>, <strong class="nd fr">statistical power (80%)</strong>, and <strong class="nd fr">MDE (7.2%)</strong>, but imagine that the <strong class="nd fr">default rate (p₁)</strong> was <strong class="nd fr">0.23 (23%)</strong> instead of <strong class="nd fr">0.070 (7.0%)</strong>. With a <strong class="nd fr">7.2% MDE</strong>, the new default rate for the treatment group (<strong class="nd fr">p₂</strong>) would be <strong class="nd fr">0.2466 (24.66%)</strong>. Notice that this is still a <strong class="nd fr">7.2% MDE</strong>, but the proportions are significantly higher than <strong class="nd fr">0.070 (7.0%)</strong> and <strong class="nd fr">0.075 (7.5%)</strong>.</p><p id="c804" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, when we perform the sample size calculation using these new values of <strong class="nd fr">p₁ = 0.23</strong> and <strong class="nd fr">p₂ = 0.2466</strong>, the results will differ. Let’s compute that next.</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="b288" class="pd ny fq pa b bg pe pf l pg ph">n = ( ((1.96+0.84)^2) * ((0.23*(1-0.23) + 0.2466*(1-0.2466)) ) / ( (0.2466-0.23)^2 )<br/>-&gt; n = 7.84 * 0.3628 / 0.00027556<br/>-&gt; n = 2.8450 / 0.00027556<br/>-&gt; n =~ 10,325</span></pre><p id="a6aa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With the new default rates (<strong class="nd fr">p₁ = 0.23</strong> and <strong class="nd fr">p₂ = 0.2466</strong>), we would need <strong class="nd fr">10,325 users</strong> in both the control and treatment groups, resulting in a total of <strong class="nd fr">20,649 samples</strong>. This is much more manageable compared to the previous sample size of 84,343. However, it’s important to note that the default rates in this scenario are in a completely different range.</p><p id="b211" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The key takeaway is that <strong class="nd fr">lower success rates</strong> (like default rates around <strong class="nd fr">7%</strong>) require <strong class="nd fr">larger sample sizes</strong>. When the proportions are smaller, detecting even modest differences (like a 7.2% increase) becomes more challenging, thus requiring more data to achieve the same statistical power and significance level.</p><h1 id="4d22" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">2. Sampling a population</h1><p id="e23d" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">This case differs from the A/B testing scenario, as we are now focusing on determining a <strong class="nd fr">sample size from a single group</strong>. The goal is to take a sample that accurately represents the population, allowing us to run an analysis and then extrapolate the results to estimate what would happen across the entire population.</p><p id="3c98" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Even though we are not comparing two groups, <strong class="nd fr">sampling from a population</strong> (a single group) still requires deciding whether you are estimating a <strong class="nd fr">mean</strong> or a <strong class="nd fr">proportion</strong>. The formulas for these scenarios are quite similar to those used in A/B testing.</p><p id="39b3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Take a look at <strong class="nd fr">images 8</strong> and <strong class="nd fr">9</strong>. Did you notice the similarities when comparing <strong class="nd fr">image 8 </strong>with <strong class="nd fr">image 3</strong> (sample size formula for comparing two means) and when comparing <strong class="nd fr">image 9</strong> with <strong class="nd fr">image 6</strong> (sample size formula for comparing two proportions)? They are indeed quite similar.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qd"><img src="../Images/5eda2848fc5b628130f111cc12b69c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/0*pc_eCDEFoEepyhLv"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 8: Sample size formula to estimate the mean of a population.</em></figcaption></figure><p id="d4fa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the case of estimating the mean:</p><ul class=""><li id="9633" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk">From image 8, the formula for sampling from one group, however, uses <strong class="nd fr">E</strong>, which stands for the <strong class="nd fr">Error</strong>.</li><li id="1741" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk">From image 3, the formula for comparing two groups uses <strong class="nd fr">delta (δ)</strong> to compare the difference between the two means.</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qe"><img src="../Images/60be487bef3d722c56c371ad2c62b607.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/0*8XBTx6z9hDutzhDM"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">Image 9: Sample size formula to estimate the proportion of a population.</em></figcaption></figure><p id="1e6b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In the case of estimating proportions:</p><ul class=""><li id="dc4e" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk">From image 9, for sampling from a single group, the formula for proportions also uses <strong class="nd fr">E</strong> instead, representing the <strong class="nd fr">Error</strong>.</li><li id="8f18" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk">From image 6, the formula for comparing two groups uses the <strong class="nd fr">MDE (Minimum Detectable Effect)</strong>, similar to delta, to compare the difference between two proportions.</li></ul><p id="5184" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, when should we use each of these formulas? Let’s explore two practical examples — one for estimating a <strong class="nd fr">mean</strong> and another for estimating a <strong class="nd fr">proportion</strong>.</p><h1 id="dd5e" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">2.1. Sampling a population — Estimating the mean</h1><p id="aafb" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Let’s say you want to better assess the <strong class="nd fr">risk of fraud</strong>, and to do so, you aim to estimate the <strong class="nd fr">average order value of fraudulent transactions</strong> by country and per week. This can be quite challenging because, ideally, most fraudulent transactions are already being blocked. To get a clearer picture, you would take a <strong class="nd fr">holdout group</strong> that is free of rules and models, which would serve as a reference for calculating the true average order value of fraudulent transactions.</p><p id="1efa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Suppose you select a specific country, and after reviewing historical data, you find that:</p><ul class=""><li id="0cb0" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk">The variance of this metric is <strong class="nd fr">€905</strong>.</li><li id="558d" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk">The average order value of fraudulent transactions is <strong class="nd fr">€100</strong>.<br/>(You can refer to <strong class="nd fr">Scripts 1 and 2</strong> for calculating the success metric and variance.)</li></ul><p id="f58b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since the variance is <strong class="nd fr">€905</strong>, the <strong class="nd fr">standard deviation</strong> (square root of variance) is approximately <strong class="nd fr">€30</strong>. Now, using a <strong class="nd fr">significance level of 5%</strong>, which corresponds to a <strong class="nd fr">z-score of 1.96</strong>, and assuming you’re comfortable with a <strong class="nd fr">10% margin of error</strong> (representing an Error of <strong class="nd fr">€10</strong>, or 10% of €100), the <strong class="nd fr">confidence interval</strong> at 95% would mean that with the correct sample size, you can say with <strong class="nd fr">95% confidence</strong> that the average value falls between <strong class="nd fr">€90 and €110</strong>.</p><p id="718a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, plugging these inputs into the sample size formula:</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="3a9c" class="pd ny fq pa b bg pe pf l pg ph">n = ( (1.96 * 30) / 10 )^2<br/>-&gt; n = (58.8/10)^2<br/>-&gt; n = 35</span></pre><p id="650c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We enter the following parameters into the web app calculator at <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">Sample Size Calculator</a>, as shown in <strong class="nd fr">App Screenshot 4</strong>:</p><ul class=""><li id="f2a5" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Confidence Level</strong>: 95%</li><li id="9ba3" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Variance</strong>: 905</li><li id="f626" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Error</strong>: 10</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qf"><img src="../Images/372ea787d2fa0e5d445df9000f6a8d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/0*vr8XZDUZrpV2rmjL"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">App screenshot 4: Calculating the sample size for estimating the mean when sampling a population.</em></figcaption></figure><p id="c6ea" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The result is that you would need <strong class="nd fr">35 samples</strong> to estimate the <strong class="nd fr">average order value of fraudulent transactions</strong> per country per week. However, that’s not the final sample size.</p><p id="7337" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since fraudulent transactions are relatively rare, you need to adjust for the <strong class="nd fr">proportion of fraudulent transactions</strong>. If the proportion of fraudulent transactions is <strong class="nd fr">1%</strong>, the actual number of samples you need to collect is:</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="a326" class="pd ny fq pa b bg pe pf l pg ph">n = 35/0.01<br/>-&gt; n = 3500</span></pre><p id="2f59" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thus, you would need 3,500 samples to ensure that fraudulent transactions are properly represented.</p><h1 id="077b" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">2.2. Sampling a population — Estimating a proportion</h1><p id="2119" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">In this scenario, our fraud rules and models are blocking a significant number of transactions. To assess how well our rules and models perform, we need to let a portion of the traffic bypass the rules and models so that we can evaluate the <strong class="nd fr">actual false positive rate</strong>. This group of transactions that passes through without any filtering is known as a <strong class="nd fr">holdout group</strong>. This is a common practice in fraud data science teams because it allows for both evaluating rule and model performance and reusing the holdout group for <strong class="nd fr">reject inference</strong>.</p><p id="68da" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Although we won’t go into detail about reject inference here, it’s worth briefly summarizing. <strong class="nd fr">Reject inference</strong> involves using the holdout group of unblocked transactions to learn patterns that help improve transaction blocking decisions. Several methods exist for this, with <strong class="nd fr">fuzzy augmentation</strong> being a popular one. The idea is to relabel previously rejected transactions using the holdout group’s data to train new models. This is particularly important in fraud modeling, where fraud rates are typically low (often less than 1%, and sometimes as low as 0.1% or lower). Increasing labeled data can improve model performance significantly.</p><p id="93de" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now that we understand the need to estimate a <strong class="nd fr">proportion</strong>, let’s dive into a practical use case to find out how many samples are needed.</p><p id="f6b0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For a certain branch, you analyze historical data and find that it processes <strong class="nd fr">50,000,000 orders</strong> in a month, of which <strong class="nd fr">50,000 are fraudulent</strong>, resulting in a <strong class="nd fr">0.1% fraud rate</strong>. Using a <strong class="nd fr">significance level of 5% (alpha)</strong> and a <strong class="nd fr">margin of error of 25%</strong>, we aim to estimate the true fraud proportion within a <strong class="nd fr">confidence interval of 95%</strong>. This means if the true fraud rate is <strong class="nd fr">0.001 (0.1%)</strong>, we would be estimating a range between <strong class="nd fr">0.00075</strong> and <strong class="nd fr">0.00125</strong>, with an Error of <strong class="nd fr">0.00025</strong>.</p><p id="33a7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Please note that margin of error and Error are two different things, the margin of error is a percentage value, and the Error is an absolute value. In the case where the fraud rate is 0.1% if we have a margin of error of 25% that represents an Error of 0.00025.</p><p id="66b0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s apply the formula:</p><ul class=""><li id="493e" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Zα/2</strong> = 1.96 (z-score for 95% confidence level)</li><li id="23b5" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">E</strong> = 0.00025 (Error)</li><li id="8691" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">p</strong> = 0.001 (fraud rate)</li></ul><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="2e36" class="pd ny fq pa b bg pe pf l pg ph">Zalpha/2= 1.96 <br/>-&gt; (Zalpha/2)^2= 3.8416<br/>E = 0.00025<br/>-&gt; E^2 = 0.0000000625<br/>p = 0.001<br/><br/>n =( 3.8416 * 0.001 * (1 - 0.001) ) / 0.0000000625<br/>-&gt; n = 0.0038377584 / 0.0000000625<br/>-&gt; n = 61,404</span></pre><p id="7e38" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We enter the following parameters into the web app calculator at <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">Sample Size Calculator</a>, as shown in <strong class="nd fr">App Screenshot 5</strong>:</p><ul class=""><li id="8552" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk"><strong class="nd fr">Confidence Level</strong>: 95%</li><li id="8584" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Proportion</strong>: 0.001</li><li id="5fc5" class="nb nc fq nd b go po nf ng gr pp ni nj nk pq nm nn no pr nq nr ns ps nu nv nw pl pm pn bk"><strong class="nd fr">Error</strong>: 0.00025</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qg"><img src="../Images/1b45a66985a09866e1bc154afe9004be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*lqVgdsCeiiRT-Pu6"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx"><em class="pj">App screenshot 5: Calculating the sample size for estimating a proportion when sampling a population.</em></figcaption></figure><p id="da14" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thus, <strong class="nd fr">61,404 samples</strong> are required in total. Given that there are <strong class="nd fr">50,000,000 transactions</strong> in a month, it would take <strong class="nd fr">less than 1 hour</strong> to collect this many samples if the holdout group represented <strong class="nd fr">100% of the traffic</strong>. However, this isn’t practical for a reliable experiment.</p><p id="547f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Instead, you would want to <strong class="nd fr">distribute the traffic</strong> across several days to avoid <strong class="nd fr">seasonality issues</strong>. Ideally, you would collect data over at least a week, ensuring representation from all weekdays while avoiding holidays or peak seasons. If you need to gather <strong class="nd fr">61,404 samples</strong> in a week, you would aim for <strong class="nd fr">8,772 samples per day</strong>. Since the daily traffic is around <strong class="nd fr">1,666,666 orders</strong>, the holdout group would need to represent <strong class="nd fr">0.53% of the total transactions</strong> each day, running over the course of a week.</p><h1 id="31a5" class="nx ny fq bf nz oa ob gq oc od oe gt of og oh oi oj ok ol om on oo op oq or os bk">Final notes</h1><p id="7a76" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">If you’d like to perform these calculations in Python, here are the relevant functions:</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="9d7c" class="pd ny fq pa b bg pe pf l pg ph">import math<br/><br/>def sample_size_comparing_two_means(variance, z_alpha, z_beta, delta):<br/>   return math.ceil((2 * variance * (z_alpha + z_beta) ** 2) / (delta ** 2))<br/><br/>def sample_size_comparing_two_proportions(p1, p2, z_alpha, z_beta):<br/>   numerator = (z_alpha + z_beta) ** 2 * ((p1 * (1 - p1)) + (p2 * (1 - p2)))<br/>   denominator = (p1 - p2) ** 2<br/>   return math.ceil(numerator / denominator)<br/><br/>def sample_size_estimating_mean(variance, z_alpha, margin_of_error):<br/>   sigma = variance ** 0.5<br/>   return math.ceil((z_alpha * sigma / margin_of_error) ** 2)<br/><br/>def sample_size_estimating_proportion(p, z_alpha, margin_of_error):<br/>   return math.ceil((z_alpha ** 2 * p * (1 - p)) / (margin_of_error ** 2))</span></pre><p id="36ed" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here's how you could calculate the sample size for comparing two means as in App screenshot 1 in section 1.1:</p><pre class="ml mm mn mo mp oz pa pb bp pc bb bk"><span id="a68d" class="pd ny fq pa b bg pe pf l pg ph">variance = 145.8<br/>z_alpha = 1.96<br/>z_beta = 0.84<br/>delta = 5<br/><br/>sample_size_comparing_two_means(<br/>    variance=variance, <br/>    z_alpha=z_alpha, <br/>    z_beta=z_beta, <br/>    delta=delta<br/>)<br/># OUTPUT: 92</span></pre><p id="4223" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">These functions are also available in the GitHub repository: <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">GitHub Sample Size Calculator</a>, this is also where you can find the link to the Interactive Sample Size Calculator.</p><p id="3fb8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Disclaimer</strong>: The images that resemble the results of a Google BigQuery job have been created by the author. The numbers shown are not based on any business data but were manually generated for illustrative purposes. The same applies to the SQL scripts — they are not from any businesses and were also manually generated. However, they are designed to <strong class="nd fr">closely resemble</strong> what a company using <strong class="nd fr">Google BigQuery</strong> as a framework might encounter.</p><ul class=""><li id="7c0b" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pl pm pn bk">Calculator written in Python and deployed in Google Cloud Run (Serverless environment) using a Docker container and Streamlit, see <a class="af pu" href="https://github.com/lucasbraga461/sample-size-calculator" rel="noopener ugc nofollow" target="_blank">code in GitHub</a> for reference.</li></ul></div></div></div></div>    
</body>
</html>