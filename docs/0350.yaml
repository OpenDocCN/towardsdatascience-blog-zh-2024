- en: 'Courage to Learn ML: Tackling Vanishing and Exploding Gradients (Part 1)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05](https://towardsdatascience.com/courage-to-learn-ml-tackling-vanishing-and-exploding-gradients-part-1-799debbf60a0?source=collection_archive---------15-----------------------#2024-02-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Melting Away DNN’s Gradient Challenges: A Scoop of Solutions and Insights'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page---byline--799debbf60a0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--799debbf60a0--------------------------------)
    ·13 min read·Feb 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea2e2eaf6631a22a9257066dd20ae32f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by the author using ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: In the last installment of the ‘[Courage to Learn ML](https://towardsdatascience.com/tagged/courage-to-learn-ml)’
    series, our learner and mentor focus on learning two essential theories of DNN
    training, gradient descent and backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Their journey began with a look at [how gradient descent is pivotal in minimizing
    the loss function](/courage-to-learn-ml-a-detailed-exploration-of-gradient-descent-and-popular-optimizers-022ecf97be7d).
    Curious about the complexities of computing gradients in deep neural networks
    across multiple hidden layers, the learner then turned to [backpropagation](/courage-to-learn-ml-explain-backpropagation-from-mathematical-theory-to-coding-practice-21e670415378).
    By decompose the backpropagation into 3 components, the learner learned about
    backpropagation and its use of the chain rule to calculate gradients efficiently
    across these layers. During this Q&A session, the learner questioned the importance
    of understanding these complex processes in an era of automated advanced deep
    learning frameworks, such as PyTorch and Tensorflow.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first post of our deep dive into Deep Learning, guided by the interactions
    between a learner and a mentor. To keep things digestible, I’ve decided to break
    down my DNN series into more manageable pieces. This way, I can explore each concept
    thoroughly without overwhelming you.
  prefs: []
  type: TYPE_NORMAL
