["```py\nimport pandas as pd\nimport requests\n```", "```py\ndef import_data(pages, start_year, end_year, search_terms):\n\n    \"\"\"\n    This function is used to use the OpenAlex API, conduct a search on works, a return a dataframe with associated works.\n\n    Inputs: \n        - pages: int, number of pages to loop through\n        - search_terms: str, keywords to search for (must be formatted according to OpenAlex standards)\n        - start_year and end_year: int, years to set as a range for filtering works\n    \"\"\"\n\n    #create an empty dataframe\n    search_results = pd.DataFrame()\n\n    for page in range(1, pages):\n\n        #use paramters to conduct request and format to a dataframe\n        response = requests.get(f'https://api.openalex.org/works?page={page}&per-page=200&filter=publication_year:{start_year}-{end_year},type:article&search={search_terms}')\n        data = pd.DataFrame(response.json()['results'])\n\n        #append to empty dataframe\n        search_results = pd.concat([search_results, data])\n\n    #subset to relevant features\n    search_results = search_results[[\"id\", \"title\", \"display_name\", \"publication_year\", \"publication_date\",\n                                        \"type\", \"countries_distinct_count\",\"institutions_distinct_count\",\n                                        \"has_fulltext\", \"cited_by_count\", \"keywords\", \"referenced_works_count\", \"abstract_inverted_index\"]]\n\n    return(search_results)\n```", "```py\n#search for Trusted AI and Autonomy\nai_search = import_data(35, 2016, 2024, \"'artificial intelligence' OR 'deep learn' OR 'neural net' OR 'autonomous' OR drone\")\n```", "```py\ndef undo_inverted_index(inverted_index):\n\n    \"\"\"\n    The purpose of the function is to 'undo' and inverted index. It inputs an inverted index and\n    returns the original string.\n    \"\"\"\n\n    #create empty lists to store uninverted index\n    word_index = []\n    words_unindexed = []\n\n    #loop through index and return key-value pairs\n    for k,v in inverted_index.items(): \n        for index in v: word_index.append([k,index])\n\n    #sort by the index\n    word_index = sorted(word_index, key = lambda x : x[1])\n\n    #join only the values and flatten\n    for pair in word_index:\n        words_unindexed.append(pair[0])\n    words_unindexed = ' '.join(words_unindexed)\n\n    return(words_unindexed)\n```", "```py\ndef preprocess(text):\n\n    \"\"\"\n    This function takes in a string, coverts it to lowercase, cleans\n    it (remove special character and numbers), and tokenizes it.\n    \"\"\"\n\n    #convert to lowercase\n    text = text.lower()\n\n    #remove special character and digits\n    text = re.sub(r'\\d+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n\n    #tokenize\n    tokens = nltk.word_tokenize(text)\n\n    return(tokens)\n```", "```py\ndef remove_stopwords(tokens):\n\n    \"\"\"\n    This function takes in a list of tokens (from the 'preprocess' function) and \n    removes a list of stopwords. Custom stopwords can be added to the 'custom_stopwords' list.\n    \"\"\"\n\n    #set default and custom stopwords\n    stop_words = nltk.corpus.stopwords.words('english')\n    custom_stopwords = []\n    stop_words.extend(custom_stopwords)\n\n    #filter out stopwords\n    filtered_tokens = [word for word in tokens if word not in stop_words]\n\n    return(filtered_tokens)\n```", "```py\ndef lemmatize(tokens):\n\n    \"\"\"\n    This function conducts lemmatization on a list of tokens (from the 'remove_stopwords' function).\n    This shortens each word down to its root form to improve modeling results.\n    \"\"\"\n\n    #initalize lemmatizer and lemmatize\n    lemmatizer = nltk.WordNetLemmatizer()\n    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\n    return(lemmatized_tokens)\n```", "```py\ndef clean_text(text):\n\n    \"\"\"\n    This function uses the previously defined functions to take a string and\\\n    run it through the entire data preprocessing process.\n    \"\"\"\n\n    #clean, tokenize, and lemmatize a string\n    tokens = preprocess(text)\n    filtered_tokens = remove_stopwords(tokens)\n    lemmatized_tokens = lemmatize(filtered_tokens)\n    clean_text = ' '.join(lemmatized_tokens)\n\n    return(clean_text)\n```", "```py\nimport gensim.corpora as corpora\nfrom gensim.corpora import Dictionary\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.ldamodel import LdaModel\n```", "```py\n#convert the preprocessed text to a list\ndocuments = list(data[\"clean_text\"])\n\n#seperate by ' ' to tokenize each article\ntexts = [x.split(' ') for x in documents]\n\n#construct word ID mappings\nid2word = Dictionary(texts)\n\n#use word ID mappings to build corpus\ncorpus = [id2word.doc2bow(text) for text in texts]\n```", "```py\n#build LDA model\nlda_model = LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, decay = 0.5,\n                     random_state = 0, chunksize = 100, alpha = 'auto', per_word_topics = True)\n```", "```py\n#compute coherence score\ncoherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = id2word, coherence = 'c_v')\ncoherence_score = coherence_model_lda.get_coherence()\nprint(coherence_score)\n```", "```py\nimport pyLDAvis\n\n#create Topic Distance Visualization \npyLDAvis.enable_notebook()\nlda_viz = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nlda_viz\n```", "```py\ndef lda_model_evaluation():\n\n    \"\"\"\n    This function loops through a number of parameters for an LDA model, creates the model,\n    computes the coherenece score, and saves the results in a pandas dataframe. The outputed dataframe\n    contains the values of the parameters tested and the resulting coherence score.\n    \"\"\"\n\n    #define empty lists to save results\n    topic_number, decay_rate_list, score  = [], [], []\n\n    #loop through a number of parameters\n    for topics in range(5,12):\n        for decay_rate in [0.5, 0.6, 0.7]:\n\n                #build LDA model\n                lda_model = LdaModel(corpus = corpus, id2word = id2word, num_topics = topics, decay = decay_rate,\n                               random_state = 0, chunksize = 100, alpha = 'auto', per_word_topics = True)\n\n                #compute coherence score\n                coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = id2word, coherence = 'c_v')\n                coherence_score = coherence_model_lda.get_coherence()\n\n                #append parameters to lists\n                topic_number.append(topics)\n                decay_rate_list.append(decay_rate)\n                score.append(coherence_score)\n\n                print(\"Model Saved\")\n\n    #gather result into a dataframe\n    results = {\"Number of Topics\": topic_number,\n                \"Decay Rate\": decay_rate_list,\n                \"Score\": score}\n\n    results = pd.DataFrame(results)\n\n    return(results) \n```"]