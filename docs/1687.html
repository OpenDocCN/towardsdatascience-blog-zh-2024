<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Bird’s-Eye View on the Evolution of Language Models for Text Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Bird’s-Eye View on the Evolution of Language Models for Text Generation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10">https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Aleksandr Perevalov" class="l ep by dd de cx" src="../Images/3b2512ac381b1dc2e72ae79d7e14285a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*2F64cIbq2Xh0cwyFQuPT5w.png"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------" rel="noopener follow">Aleksandr Perevalov</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">1</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="c991" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In this article I would like to share my notes on how language models (LMs) have been developing during the last decades. This text may serve a a gentle introduction and help to understand the conceptual points of LMs throughout their history. It’s worth mentioning that I don’t dive very deep into the implementation details and math behind it, however, the level of description is enough to understand LMs’ evolution properly.</p><h2 id="a5cb" class="mv mw fq bf mx my mz na nb nc nd ne nf mi ng nh ni mm nj nk nl mq nm nn no np bk">What is Language Modeling?</h2><p id="b0b3" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">Generally speaking, Language Modeling is a process of formalizing a language, in particular — natural language, in order to make it machine-readable and process it in various ways. Hence, it is not only about generating language, but also about language representation.</p><p id="c68d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The most popular association with “language modeling”, thanks to GenAI, is tightly connected with the text generation process. This is why my article considers the evolution of the language models from the text generation point of view.</p><h1 id="08e9" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">N-gram Language Models</h1><p id="bd9d" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">Although the foundation of n-gram LMs was created in the middle of 20th century, the widespread of such models has started in 1980s and 1990s.</p><p id="97f2" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The n-gram LMs make use of the <em class="op">Markov assumption</em>, which claims, in the context of LMs, that in the probability of a next word in a sequence depends only on the previous word(s). Therefore, the probability approximation of a word given its context with an n-gram LM can be formalized as follows:</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or os"><img src="../Images/a8655e4c191b867b3b0ed8009aaadbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jRTpyQz7noaO9SRP"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Probability of a next word given a sequence of previous ones can be approximated by the probability of a next word given the N previous ones (e.g. N=2 — bi-gram LM) (image by author)</figcaption></figure><p id="5635" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">where <em class="op">t</em> is the number of words in the whole sequence and <em class="op">N</em> is the size of the context (uni-gram (1), bi-gram (2), etc.). Now, the question is how to estimate those n-gram probabilities? The simplest approach is to use n-gram counts (to be calculated on a large text corpora in an “unsupervised” way):</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or pj"><img src="../Images/903414b12eb3d1f80e92ca41646a6bda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8zDMPXjJTrWAWoLJ"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">The probability of a next word given the N previous ones: numerator — how many times the resulting sequence occurs in the data, denominator — how many times the sequence of previous words occurs in the data (image by author)</figcaption></figure><p id="5212" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Obviously, the probability estimation from the equation above may appear to be naive. What if the numerator or even denominator values will be zero? This is why more advanced probability estimations include smoothing or backoff (e.g., <a class="af pk" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank">add-k smoothing</a>, <a class="af pk" href="https://aclanthology.org/D07-1090.pdf" rel="noopener ugc nofollow" target="_blank">stupid backoff</a>, <a class="af pk" href="https://ieeexplore.ieee.org/document/479394" rel="noopener ugc nofollow" target="_blank">Kneser-Ney smoothing</a>). We won’t explore those methods here, however, conceptually the probability estimation approach doesn’t change with any smoothing or backoff method. The high-level representation of an n-gram LM is shown below:</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or pl"><img src="../Images/e2479016e74d7eb7fb2bdc037d78f29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FRNYQ2JzfAASBrFXEBkHQ.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">The high-level representation of an n-gram LM (image by author)</figcaption></figure><p id="2245" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Having the counts calculated, how do we generate text from such LM? Essentially, the answer to this question applies to all LMs to be considered below. The process of selecting the next word given the probability distribution fron an LM is called <em class="op">sampling</em>. Here are couple <em class="op">sampling strategies </em>applicable to the n-gram LMs:</p><ul class=""><li id="1e5d" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pm pn po bk"><em class="op">greedy sampling</em> — select the word with the highest probability;</li><li id="2331" class="lx ly fq lz b ma pp mc md me pq mg mh mi pr mk ml mm ps mo mp mq pt ms mt mu pm pn po bk"><em class="op">random sampling</em>— select the random word following the probability<br/>distribution.</li></ul><h1 id="990f" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">Feedforward Neural Network Language Models</h1><p id="55bf" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">Despite smoothing and backoff, the probability estimation of the n-gram LMs is still intuitively too simple to model natural language. A game-changing approach of <a class="af pk" href="https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Yoshua Bengio et al. (2000)</a> was very simple yet innovative: <em class="op">what if instead of n-gram counts we will use neural networks to estimate word probabilities</em>? Although the paper claims that recurrent neural networks (RNNs) can be also used for this task, main content focuses on a feedforward neural network (FFNN) architecture.</p><p id="3f12" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The FFNN architecture proposed by Bengio is a simple multi-class classifier (the number of classes is the size of vocabulary <em class="op">V</em>). The training process is based on the task of predicting a missing word <em class="op">w</em> in the sequence of the context words <em class="op">c</em>: P (<em class="op">w|c</em>), where |<em class="op">c</em>| is the <em class="op">context window size</em>. The FFNN architecture proposed by Bengio et al. is shown below:</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or pu"><img src="../Images/20d54637d01b39090efa0804621bab9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIKMd1fO9nzAnfrumc19eA.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">FFNN architecture for next word probability estimation (image by author)</figcaption></figure><p id="df9f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Such FFNN-based LMs can be trained on a large text corpora in an self-supervised manner (i.e., no explicitly labeled dataset is required).</p><p id="940c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">What about sampling? In addition to the greedy and random strategies, there are two more that can be applied to NN-based LMs:</p><ul class=""><li id="84ad" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pm pn po bk"><em class="op">top-k sampling</em> — the same as greedy, but made within a renormalized set of top-k<br/>words (softmax is recalculated on top-k words),</li><li id="e0c3" class="lx ly fq lz b ma pp mc md me pq mg mh mi pr mk ml mm ps mo mp mq pt ms mt mu pm pn po bk"><em class="op">nucleus</em> <em class="op">sampling</em>— the same as top-k, but instead of k as a number use percentage.</li></ul><h1 id="26ba" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">Recurrent Neural Network Language Models</h1><p id="cb0f" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">To this point we were working with the assumption that the probability of the next word depends only on the previous one(s). We also considered a fixed context or n-gram size to estimate the probability. <em class="op">What if the connections between words are also important to consider</em>? <em class="op">What if we want to consider the whole sequence of preceding words to predict the next one</em>? This can be perfectly modeled by RNNs!</p><p id="1734" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Naturally, RNNs’ advantage is that they are able to capture dependencies of the whole word sequence while adding the hidden layer output from the previous step (<em class="op">t-1</em>) to the input from the current step (<em class="op">t</em>):</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div class="oq or pv"><img src="../Images/53aa8cc7d715ab41c6979e641f14d80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*YXT1X1Ec11_kv3VjvT17Yg.png"/></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Hidden layer output computation at step t for a simple RNN (image by author)</figcaption></figure><p id="f735" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">where <em class="op">h</em> — hidden layer output, <em class="op">g(x)</em> — activation function, <em class="op">U</em> and <em class="op">W</em> — weight matrices.</p><p id="1a8e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The RNNs are also trained following the self-supervised setting on a large text corpora to predict the next word given a sequence. The text generation is then performed via so-called <em class="op">autoregressive generation</em> process, which is also called <em class="op">causal language modeling generation</em>. The autoregressive generation with an RNN is demonstrated below:</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or pw"><img src="../Images/05e5eb97d1523a8ebe931b96c16f7489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2RRFbAf5YeQVxhO9EK5qlw.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Autoregressive text generation with a RNN and other examples listed (image by author)</figcaption></figure><p id="897f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In practice, canonical RNNs are rarely used for the LM tasks. Instead, there are improved RNN architectures such as stacked and <a class="af pk" href="https://ieeexplore.ieee.org/document/650093" rel="noopener ugc nofollow" target="_blank">bidirectional</a>, <a class="af pk" href="https://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">long short-term memory (LSTM)</a> and its variations.</p><p id="a7ee" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">One of the most remarkable RNN architectures was proposed by <a class="af pk" href="https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Sutskever et al. (2014) </a>—<em class="op"> </em>the <em class="op">encoder-decoder</em> (or seq2seq) LSTM-based architecture. Instead of simple autoregressive generation, seq2seq model encodes an input sequence to an intermediate representation — context vector — and then uses autoregressive generation to decode it.</p><p id="5097" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">However, the initial seq2seq architecture had a major bottleneck — the <em class="op">encoder narrows down the whole input sequence to the only one representation — context vector. </em>To remove this bottleneck, Bahdanau et al. (2014) introduces the attention mechanism, that (1) produces an individual context vector for every decoder hidden state (2) based on weighted encoder hidden states. Hence, the <em class="op">intuition behind the attention mechanism is that every input word impacts every output word and the intensity of this impact varies</em>.</p><p id="d267" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It is worth mentioning that RNN-based models are used for learning language representations. In particular, the most well known models are <a class="af pk" href="https://aclanthology.org/N18-1202.pdf" rel="noopener ugc nofollow" target="_blank">ELMo</a> (2018) and <a class="af pk" href="https://aclanthology.org/P18-1031.pdf" rel="noopener ugc nofollow" target="_blank">ULMFiT</a> (2018).</p><h2 id="798b" class="mv mw fq bf mx my mz na nb nc nd ne nf mi ng nh ni mm nj nk nl mq nm nn no np bk">Evaluation: Perplexity</h2><p id="b9c1" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">While considering LMs without applying them to a particular task (e.g. machine translation) there is one universal measure that may give us insights on how good is our LM is. This measure is called <em class="op">Perplexity</em>.</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div class="oq or px"><img src="../Images/7df8bd08bbce35a6225e3e6a28ac3318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*rVCaVG7RDMSGnX8njEwDdA.png"/></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">Perplexity formula (image by author)</figcaption></figure><p id="cc87" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">where <em class="op">p</em> — probability distribution of the words, <em class="op">N</em> — is the total number of words in the sequence, <em class="op">wi</em> — represents the <em class="op">i</em>-th word. Since Perplexity uses the concept of entropy, the intuition behind it is <em class="op">how unsure a particular model about the predicted sequence</em>. The lower the perplexity, the less uncertain the model is, and thus, the better it is at predicting the sample.</p><h1 id="7abd" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">Transformer Language Models</h1><p id="7b2d" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">The modern state-of-the-art LMs make use of the attention mechanism, introduced in the previous paragraph, and, in particular, <a class="af pk" href="https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="op">self-attention</em></a>, which is an integral part of the<a class="af pk" href="https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="op">transformer</em> architecture</a>.</p><p id="a524" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The transformer LMs have a significant advantage over the RNN LMs in terms of computation efficiency due to their ability to parallelize computations. In RNNs, sequences are processed one step at a time, this makes RNNs slower, especially for long sequences. In contrast, transformer models use a self-attention mechanism that allows them to process all positions in the sequence simultaneously. Below is a high-level representation of a transformer model with an <em class="op">LM head</em>.</p><figure class="ot ou ov ow ox oy oq or paragraph-image"><div role="button" tabindex="0" class="oz pa ed pb bh pc"><div class="oq or py"><img src="../Images/515e67db399b59feb3e17df0a4456a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSEQwYSw3Uns_L6zTElKug.jpeg"/></div></div><figcaption class="pe pf pg oq or ph pi bf b bg z dx">A simplified architecture of a transformer LM (image by author)</figcaption></figure><p id="ddad" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">To represent the input token, transformers add token and position embeddings together. The last hidden state of the last transformer layer is typically used to produce the next word probabilities via the LM head. The transformer LMs are <em class="op">pre-trained</em> following the self-supervised paradigm. When considering the decoder or encoder-decoder models, the pre-training task is to predict the next word in a sequence, similarly to the previous LMs.</p><p id="1e25" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">It is worth mentioning that <em class="op">the most advances in the language modeling since the inception of transformers (2017) are lying in the two major directions</em>: (1) <a class="af pk" href="https://huggingface.co/blog/large-language-models" rel="noopener ugc nofollow" target="_blank">model size scaling</a> and (2) <a class="af pk" href="https://arxiv.org/abs/2308.10792" rel="noopener ugc nofollow" target="_blank">instruction fine-tuning</a> including <a class="af pk" href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback" rel="noopener ugc nofollow" target="_blank">reinforcement learning with human feedback</a>.</p><h2 id="f2e0" class="mv mw fq bf mx my mz na nb nc nd ne nf mi ng nh ni mm nj nk nl mq nm nn no np bk">Evaluation: Instruction Benchmarks</h2><p id="fd9f" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">The instruction-tuned LMs are considered as general problem-solvers. Therefore, Perplexity might not be the best quality measure since it calculates the quality of such models <em class="op">implicitly</em>. The <em class="op">explicit</em> way of evaluating intruction-tuned LMs is based on on instruction benchmarks,<br/>such as <a class="af pk" href="https://arxiv.org/abs/2009.03300" rel="noopener ugc nofollow" target="_blank">Massive Multitask Language Understanding (MMLU)</a>, <a class="af pk" href="https://arxiv.org/abs/2107.03374" rel="noopener ugc nofollow" target="_blank">HumanEval for code</a>, <a class="af pk" href="https://arxiv.org/abs/2103.03874" rel="noopener ugc nofollow" target="_blank">Mathematical Problem Solving (MATH)</a>, and others.</p><h1 id="6c68" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">Summary</h1><p id="01a4" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">We considered here the evolution of language models in the context of text generation that covers at least last three decades. Despite not diving deeply into the details, it is clear how language models have been developing since the 1990s.</p><p id="4a74" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The n-gram language models approximated the next word probability using the n-gram counts and smoothing methods applied to it. To improve this approach, feedforward neural network architectures were proposed to approximate the word probability. While both n-gram and FFNN models considered only a fixed number of context and ignored the connections between the words in an input sentence, RNN LMs filled this gap by naturally considering connections between the words and the whole sequence of input tokens. Finally, the transformer LMs demonstrated better computation efficiency over RNNs as well as utilized self-attention mechanism for producing more contextualized representations.</p><p id="ff48" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Since the invention of the transformer architecture in 2017, the biggest advances in language modeling are considered to be the model size scaling and instruction fine-tuning including RLHF.</p><h2 id="b55e" class="mv mw fq bf mx my mz na nb nc nd ne nf mi ng nh ni mm nj nk nl mq nm nn no np bk">References</h2><p id="7580" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">I would like to acknowledge Dan Jurafsky and James H. Martin for their <a class="af pk" href="https://web.stanford.edu/~jurafsky/slp3/" rel="noopener ugc nofollow" target="_blank">Speech and Language Processing</a> book that was the main source of inspiration for this article.</p><p id="7d2f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The other references are included as hyperlinks in the text.</p><h1 id="3049" class="nv mw fq bf mx nw nx ny nb nz oa ob nf oc od oe of og oh oi oj ok ol om on oo bk">Postscriptum</h1><p id="bc71" class="pw-post-body-paragraph lx ly fq lz b ma nq mc md me nr mg mh mi ns mk ml mm nt mo mp mq nu ms mt mu fj bk">Text me [contact (at) perevalov (dot) com] or visit my <a class="af pk" href="https://perevalov.com/" rel="noopener ugc nofollow" target="_blank">website</a> if you want to get more knowledge on applying LLMs in real-world industrial use cases (e.g. AI Assistants, agent-based systems and many more).</p></div></div></div></div>    
</body>
</html>