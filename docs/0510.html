<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Complete Guide to Write your own Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Complete Guide to Write your own Transformers</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24">https://towardsdatascience.com/a-complete-guide-to-write-your-own-transformers-29e23f371ddd?source=collection_archive---------1-----------------------#2024-02-24</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8af3" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An end-to-end implementation of a Pytorch Transformer, in which we will cover key concepts such as self-attention, encoders, decoders, and much more.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Benjamin Etienne" class="l ep by dd de cx" src="../Images/cad8bc2d4b900575e76b7cf9debc9eea.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zgmYQ0PJ0MHXIc5mEYVSMA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@benjamin_47408?source=post_page---byline--29e23f371ddd--------------------------------" rel="noopener follow">Benjamin Etienne</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--29e23f371ddd--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 24, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/fd0bb4ec869d59ea716cca7fc1928d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8KyI1VxZesIz4M-H"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@shs521?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Susan Holt Simpson</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="b4a5" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Writing our own</h1><p id="bee6" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">When I decided to dig deeper into Transformer architectures, I often felt frustrated when reading or watching tutorials online as I felt they always missed something :</p><ul class=""><li id="597d" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk">Official tutorials from Tensorflow or Pytorch used their own APIs, thus staying high-level and forcing me to have to go in their codebase to see what was under the hood. Very time-consuming and not always easy to read 1000s of lines of code.</li><li id="9b3c" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk">Other tutorials with custom code I found (links at the end of the article) often oversimplified use cases and didn’t tackle concepts such as masking of variable-length sequence batch handling.</li></ul><p id="f3b3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I therefore decided to write my own Transformer to make sure I understood the concepts and be able to use it with any dataset.</p><p id="634d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">During this article, we will therefore follow a methodical approach in which we will implement a transformer layer by layer and block by block.</p><p id="4427" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There are obviously a lot of different implementations as well as high-level APIs from Pytorch or Tensorflow already available off the shelf, with — I am sure — better performance than the model we will build.</p><p id="27ed" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pi">“Ok, but why not use the TF/Pytorch implementations then” ?</em></strong></p><p id="311c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The purpose of this article is educational, and I have no pretention in beating Pytorch or Tensorflow implementations. I do believe that the theory and the code behind transformers is not straightforward, that is why I hope that going through this step-by-step tutorial will allow you to have a better grasp over these concepts and feel more comfortable when building your own code later.</p><p id="e454" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Another reasons to build your own transformer from scratch is that it will allow you to fully understand how to use the above APIs. If we look at the Pytorch implementation of the <code class="cx pj pk pl pm b">forward()</code> method of the Transformer class, you will see a lot of obscure keywords like :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/00baf4c1e185c524d03fd1440baf4db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnPBQWTUmGmbpuMmW8nXOw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">source : <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html" rel="noopener ugc nofollow" target="_blank">Pytorch docs</a></figcaption></figure><p id="7b46" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you are already familiar with these keywords, then you can happily skip this article.</p><p id="31e7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Otherwise, this article will walk you through each of these keywords with the underlying concepts.</p><h1 id="7aeb" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">A very short introduction to Transformers</h1><p id="d633" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">If you already heard about ChatGPT or Gemini, then you already met a transformer before. Actually, the “T” of ChatGPT stands for Transformer.</p><p id="208e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The architecture was first coined in 2017 by Google researchers in the “Attention is All you need” paper. It is quite revolutionary as previous models used to do sequence-to-sequence learning (machine translation, speech-to-text, etc…) relied on RNNs which were computationnally expensive in the sense they had to process sequences step by step, whereas Transformers only need to look once at the whole sequence, moving the time complexity from O(n) to O(1).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/7a4b8ca14307d51a11f291b0766a0cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ml-AVbcrZoPJ0Ta5ARcAAw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">(Vaswani et al, 2017)</a></figcaption></figure><p id="f97c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Applications of transformers are quite large in the domain of NLP, and include language translation, question answering, document summarization, text generation, etc.</p><p id="8046" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The overall architecture of a transformer is as below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pp"><img src="../Images/481fea3d28cd50a0773fafd6a1ef1366.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*8yA78jYVHbsCREC9obYFVQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h1 id="75ad" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Multi-head attention</h1><p id="2ee7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The first block we will implement is actually the most important part of a Transformer, and is called the Multi-head Attention. Let’s see where it sits in the overall architecture</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/99b4662ef876d9ab107782fdfebad4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*aibMCFA6wQXegBExGocaGQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="cf8e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Attention is a mechanism which is actually not specific to transformers, and which was already used in RNN sequence-to-sequence models.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/c24820a0c6d6b508bdf72cfb69d2469f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*khArrNUzPx2Nqw2aSVmBWQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Attention in a transformer (source: Tensorflow <a class="af nc" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">documentation</a>)</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/6b88cb8f3f37ee9819f5aed4b9578e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UUJL7jyDJMHNF4bCXvGJ3A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Attention in a transformer (source: Tensorflow <a class="af nc" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">documentation</a>)</figcaption></figure><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="47cb" class="pv ne fq pm b bg pw px l py pz">import torch<br/>import torch.nn as nn<br/>import math<br/><br/><br/>class MultiHeadAttention(nn.Module):<br/>    def __init__(self, hidden_dim=256, num_heads=4):<br/>        """<br/>        input_dim: Dimensionality of the input.<br/>        num_heads: The number of attention heads to split the input into.<br/>        """<br/>        super(MultiHeadAttention, self).__init__()<br/>        self.hidden_dim = hidden_dim<br/>        self.num_heads = num_heads<br/>        assert hidden_dim % num_heads == 0, "Hidden dim must be divisible by num heads"<br/>        self.Wv = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Value part<br/>        self.Wk = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Key part<br/>        self.Wq = nn.Linear(hidden_dim, hidden_dim, bias=False) # the Query part<br/>        self.Wo = nn.Linear(hidden_dim, hidden_dim, bias=False) # the output layer<br/>        <br/>        <br/>    def check_sdpa_inputs(self, x):<br/>        assert x.size(1) == self.num_heads, f"Expected size of x to be ({-1, self.num_heads, -1, self.hidden_dim // self.num_heads}), got {x.size()}"<br/>        assert x.size(3) == self.hidden_dim // self.num_heads<br/>        <br/>        <br/>    def scaled_dot_product_attention(<br/>            self, <br/>            query, <br/>            key, <br/>            value, <br/>            attention_mask=None, <br/>            key_padding_mask=None):<br/>        """<br/>        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)<br/>        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)<br/>        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)<br/>        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)<br/>        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)<br/>        <br/>    <br/>        """<br/>        self.check_sdpa_inputs(query)<br/>        self.check_sdpa_inputs(key)<br/>        self.check_sdpa_inputs(value)<br/>        <br/>        <br/>        d_k = query.size(-1)<br/>        tgt_len, src_len = query.size(-2), key.size(-2)<br/><br/>        <br/>        # logits = (B, H, tgt_len, E) * (B, H, E, src_len) = (B, H, tgt_len, src_len)<br/>        logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) <br/>        <br/>        # Attention mask here<br/>        if attention_mask is not None:<br/>            if attention_mask.dim() == 2:<br/>                assert attention_mask.size() == (tgt_len, src_len)<br/>                attention_mask = attention_mask.unsqueeze(0)<br/>                logits = logits + attention_mask<br/>            else:<br/>                raise ValueError(f"Attention mask size {attention_mask.size()}")<br/>        <br/>                <br/>        # Key mask here<br/>        if key_padding_mask is not None:<br/>            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2) # Broadcast over batch size, num heads<br/>            logits = logits + key_padding_mask<br/>        <br/>        <br/>        attention = torch.softmax(logits, dim=-1)<br/>        output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)<br/>        <br/>        return output, attention<br/><br/>    <br/>    def split_into_heads(self, x, num_heads):<br/>        batch_size, seq_length, hidden_dim = x.size()<br/>        x = x.view(batch_size, seq_length, num_heads, hidden_dim // num_heads)<br/>        <br/>        return x.transpose(1, 2) # Final dim will be (batch_size, num_heads, seq_length, , hidden_dim // num_heads)<br/><br/>    def combine_heads(self, x):<br/>        batch_size, num_heads, seq_length, head_hidden_dim = x.size()<br/>        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, num_heads * head_hidden_dim)<br/>        <br/>    <br/>    def forward(<br/>            self, <br/>            q, <br/>            k, <br/>            v, <br/>            attention_mask=None, <br/>            key_padding_mask=None):<br/>        """<br/>        q : tensor of shape (batch_size, query_sequence_length, hidden_dim)<br/>        k : tensor of shape (batch_size, key_sequence_length, hidden_dim)<br/>        v : tensor of shape (batch_size, key_sequence_length, hidden_dim)<br/>        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)<br/>        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)<br/>       <br/>        """<br/>        q = self.Wq(q)<br/>        k = self.Wk(k)<br/>        v = self.Wv(v)<br/><br/>        q = self.split_into_heads(q, self.num_heads)<br/>        k = self.split_into_heads(k, self.num_heads)<br/>        v = self.split_into_heads(v, self.num_heads)<br/>        <br/>        # attn_values, attn_weights = self.multihead_attn(q, k, v, attn_mask=attention_mask)<br/>        attn_values, attn_weights  = self.scaled_dot_product_attention(<br/>            query=q, <br/>            key=k, <br/>            value=v, <br/>            attention_mask=attention_mask,<br/>            key_padding_mask=key_padding_mask,<br/>        )<br/>        grouped = self.combine_heads(attn_values)<br/>        output = self.Wo(grouped)<br/>        <br/>        self.attention_weigths = attn_weights<br/>        <br/>        return output</span></pre><p id="d79b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We need to explain a few concepts here.</p><h2 id="2ad9" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">1) Queries, Keys and Values.</h2><blockquote class="qr qs qt"><p id="67af" class="nz oa pi ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The <strong class="ob fr"><em class="fq">query</em></strong><em class="fq"> </em>is the information you are trying to match, <br/>The <strong class="ob fr"><em class="fq">key</em></strong><em class="fq"> </em>and <strong class="ob fr"><em class="fq">values</em></strong><em class="fq"> </em>are the stored information.</p></blockquote><p id="ff23" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Think of that as using a dictionary : whenever using a Python dictionary, if your query doesn’t match the dictionary keys, you won’t be returned anything. But what if we want our dictionary to return a blend of information which are quite close ? Like if we had :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="5600" class="pv ne fq pm b bg pw px l py pz">d = {"panther": 1, "bear": 10, "dog":3}<br/>d["wolf"] = 0.2*d["panther"] + 0.7*d["dog"] + 0.1*d["bear"]</span></pre><p id="bd12" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This is basically what attention is about : looking at different parts of your data, and blend them to obtain a synthesis as an answer to your query.</p><p id="2d7f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The relevant part of the code is this one, where we compute the attention weights between the query and the keys</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="c8f3" class="pv ne fq pm b bg pw px l py pz">logits = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) # we compute the weights of attention</span></pre><p id="3550" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And this one, where we apply the normalized weights to the values :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="60fa" class="pv ne fq pm b bg pw px l py pz">attention = torch.softmax(logits, dim=-1)<br/>output = torch.matmul(attention, value) # (batch_size, num_heads, sequence_length, hidden_dim)</span></pre><h2 id="8438" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">2) Attention masking and padding</h2><p id="80e0" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">When attending to parts of a sequential input, we do not want to include useless or forbidden information.</p><p id="b060" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Useless information is for example padding: padding symbols, used to align all sequences in a batch to the same sequence size, should be ignored by our model. We will come back to that in the last section</p><p id="95d5" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Forbidden information is a bit more complex. When being trained, a model learns to encode the input sequence, and align targets to the inputs. However, as the inference process involves looking at previously emitted tokens to predict the next one (think of text generation in ChatGPT), we need to apply the same rules during training.</p><p id="249f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This is why we apply a <em class="pi">causal mask </em>to ensure that the targets, at each time step, can only see information from the past. Here is the corresponding section where the mask is applied (computing the mask is covered at the end)</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="119d" class="pv ne fq pm b bg pw px l py pz">if attention_mask is not None:<br/>    if attention_mask.dim() == 2:<br/>        assert attention_mask.size() == (tgt_len, src_len)<br/>        attention_mask = attention_mask.unsqueeze(0)<br/>        logits = logits + attention_mask</span></pre><h1 id="90cc" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Positional Encoding</h1><p id="07b7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">It corresponds to the following part of the Transformer:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qu"><img src="../Images/4c54461de6e757740b7df62624b38aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*CtvBBaPEMKMRK6insxd1xQ.png"/></div></figure><p id="984c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">When receiving and treating an input, a transformer has no sense of order as it looks at the sequence as a whole, in opposition to what RNNs do. We therefore need to add a hint of temporal order so that the transformer can learn dependencies.</p><p id="bb44" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The specific details of how positional encoding works is out of scope for this article, but feel free to read the original paper to understand.</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="0575" class="pv ne fq pm b bg pw px l py pz"># Taken from https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model<br/>class PositionalEncoding(nn.Module):<br/><br/>    def __init__(self, d_model, dropout=0.1, max_len=5000):<br/>        super(PositionalEncoding, self).__init__()<br/>        self.dropout = nn.Dropout(p=dropout)<br/>        <br/>        pe = torch.zeros(max_len, d_model)<br/>        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)<br/>        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))<br/>        <br/>        pe[:, 0::2] = torch.sin(position * div_term)<br/>        pe[:, 1::2] = torch.cos(position * div_term)<br/>        pe = pe.unsqueeze(0)<br/>        <br/>        self.register_buffer('pe', pe)<br/><br/>    def forward(self, x):<br/>        """<br/>        Arguments:<br/>            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``<br/>        """<br/>        x = x + self.pe[:, :x.size(1), :]<br/>        return x</span></pre><h1 id="d34e" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Encoders</h1><p id="996f" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We are getting close to having a full encoder working ! The encoder is the left part of the Transformer</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/8dea9506d84ec2bd26bf1791cd078aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*bsLsL-HA2f8rcxbZK7zo-w.png"/></div></figure><p id="5898" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will add a small part to our code, which is the Feed Forward part :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="8df6" class="pv ne fq pm b bg pw px l py pz">class PositionWiseFeedForward(nn.Module):<br/>    def __init__(self, d_model: int, d_ff: int):<br/>        super(PositionWiseFeedForward, self).__init__()<br/>        self.fc1 = nn.Linear(d_model, d_ff)<br/>        self.fc2 = nn.Linear(d_ff, d_model)<br/>        self.relu = nn.ReLU()<br/><br/>    def forward(self, x):<br/>        return self.fc2(self.relu(self.fc1(x)))</span></pre><p id="9a6c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Putting the pieces together, we get an Encoder module !</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="bd17" class="pv ne fq pm b bg pw px l py pz">class EncoderBlock(nn.Module):<br/>    def __init__(self, n_dim: int, dropout: float, n_heads: int):<br/>        super(EncoderBlock, self).__init__()<br/>        self.mha = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)<br/>        self.norm1 = nn.LayerNorm(n_dim)<br/>        self.ff = PositionWiseFeedForward(n_dim, n_dim)<br/>        self.norm2 = nn.LayerNorm(n_dim)<br/>        self.dropout = nn.Dropout(dropout)<br/>        <br/>    def forward(self, x, src_padding_mask=None):<br/>        assert x.ndim==3, "Expected input to be 3-dim, got {}".format(x.ndim)<br/>        att_output = self.mha(x, x, x, key_padding_mask=src_padding_mask)<br/>        x = x + self.dropout(self.norm1(att_output))<br/>        <br/>        ff_output = self.ff(x)<br/>        output = x + self.norm2(ff_output)<br/>       <br/>        return output</span></pre><p id="172c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As shown in the diagram, the Encoder actually contains N Encoder blocks or layers, as well as an Embedding layer for our inputs. Let’s therefore create an Encoder by adding the Embedding, the Positional Encoding and the Encoder blocks:</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="266e" class="pv ne fq pm b bg pw px l py pz">class Encoder(nn.Module):<br/>    def __init__(<br/>            self, <br/>            vocab_size: int, <br/>            n_dim: int, <br/>            dropout: float, <br/>            n_encoder_blocks: int,<br/>            n_heads: int):<br/>        <br/>        super(Encoder, self).__init__()<br/>        self.n_dim = n_dim<br/><br/>        self.embedding = nn.Embedding(<br/>            num_embeddings=vocab_size, <br/>            embedding_dim=n_dim<br/>        )<br/>        self.positional_encoding = PositionalEncoding(<br/>            d_model=n_dim, <br/>            dropout=dropout<br/>        )    <br/>        self.encoder_blocks = nn.ModuleList([<br/>            EncoderBlock(n_dim, dropout, n_heads) for _ in range(n_encoder_blocks)<br/>        ])<br/>        <br/>        <br/>    def forward(self, x, padding_mask=None):<br/>        x = self.embedding(x) * math.sqrt(self.n_dim)<br/>        x = self.positional_encoding(x)<br/>        for block in self.encoder_blocks:<br/>            x = block(x=x, src_padding_mask=padding_mask)<br/>        return x</span></pre><h1 id="7818" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Decoders</h1><p id="ca36" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The decoder part is the part on the left and requires a bit more crafting.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/e186299bbdb37043d1147a94c8bdde42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*4LBcSxxvX5uSwslKzkZYXg.png"/></div></figure><p id="89c9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There is something called <em class="pi">Masked Multi-Head Attention. </em>Remember what we said before about <em class="pi">causal mask </em>? Well this happens here. We will use the attention_mask parameter of our Multi-head attention module to represent this (more details about how we compute the mask at the end) :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="aede" class="pv ne fq pm b bg pw px l py pz"><br/># Stuff before<br/><br/>self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)<br/>masked_att_output = self.self_attention(<br/>    q=tgt, <br/>    k=tgt, <br/>    v=tgt, <br/>    attention_mask=tgt_mask, &lt;-- HERE IS THE CAUSAL MASK<br/>    key_padding_mask=tgt_padding_mask)<br/><br/># Stuff after</span></pre><p id="c997" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The second attention is called <em class="pi">cross-attention</em>. It will uses the decoder’s query to match with the encoder’s key &amp; values ! Beware : they can have different lengths during training, so it is usually a good practice to define clearly the expected shapes of inputs as follows :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="2bfc" class="pv ne fq pm b bg pw px l py pz">def scaled_dot_product_attention(<br/>            self, <br/>            query, <br/>            key, <br/>            value, <br/>            attention_mask=None, <br/>            key_padding_mask=None):<br/>        """<br/>        query : tensor of shape (batch_size, num_heads, query_sequence_length, hidden_dim//num_heads)<br/>        key : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)<br/>        value : tensor of shape (batch_size, num_heads, key_sequence_length, hidden_dim//num_heads)<br/>        attention_mask : tensor of shape (query_sequence_length, key_sequence_length)<br/>        key_padding_mask : tensor of shape (sequence_length, key_sequence_length)<br/>    <br/>        """</span></pre><p id="a495" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And here is the part where we use the encoder’s output, called <em class="pi">memory</em>, with our decoder input :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="2d53" class="pv ne fq pm b bg pw px l py pz"># Stuff before<br/>self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)<br/>cross_att_output = self.cross_attention(<br/>        q=x1, <br/>        k=memory, <br/>        v=memory, <br/>        attention_mask=None,  &lt;-- NO CAUSAL MASK HERE<br/>        key_padding_mask=memory_padding_mask) &lt;-- WE NEED TO USE THE PADDING OF THE SOURCE<br/># Stuff after</span></pre><p id="a1ab" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Putting the pieces together, we end up with this for the Decoder :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="0d66" class="pv ne fq pm b bg pw px l py pz">class DecoderBlock(nn.Module):<br/>    def __init__(self, n_dim: int, dropout: float, n_heads: int):<br/>        super(DecoderBlock, self).__init__()<br/>        <br/>        # The first Multi-Head Attention has a mask to avoid looking at the future<br/>        self.self_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)<br/>        self.norm1 = nn.LayerNorm(n_dim)<br/>        <br/>        # The second Multi-Head Attention will take inputs from the encoder as key/value inputs<br/>        self.cross_attention = MultiHeadAttention(hidden_dim=n_dim, num_heads=n_heads)<br/>        self.norm2 = nn.LayerNorm(n_dim)<br/>        <br/>        self.ff = PositionWiseFeedForward(n_dim, n_dim)<br/>        self.norm3 = nn.LayerNorm(n_dim)<br/>        # self.dropout = nn.Dropout(dropout)<br/>        <br/>        <br/>    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):<br/>        <br/>        masked_att_output = self.self_attention(<br/>            q=tgt, k=tgt, v=tgt, attention_mask=tgt_mask, key_padding_mask=tgt_padding_mask)<br/>        x1 = tgt + self.norm1(masked_att_output)<br/>        <br/>        cross_att_output = self.cross_attention(<br/>            q=x1, k=memory, v=memory, attention_mask=None, key_padding_mask=memory_padding_mask)<br/>        x2 = x1 + self.norm2(cross_att_output)<br/>        <br/>        ff_output = self.ff(x2)<br/>        output = x2 + self.norm3(ff_output)<br/><br/>        <br/>        return output<br/><br/>class Decoder(nn.Module):<br/>    def __init__(<br/>        self, <br/>        vocab_size: int, <br/>        n_dim: int, <br/>        dropout: float, <br/>        n_decoder_blocks: int,<br/>        n_heads: int):<br/>        <br/>        super(Decoder, self).__init__()<br/>        <br/>        self.embedding = nn.Embedding(<br/>            num_embeddings=vocab_size, <br/>            embedding_dim=n_dim,<br/>            padding_idx=0<br/>        )<br/>        self.positional_encoding = PositionalEncoding(<br/>            d_model=n_dim, <br/>            dropout=dropout<br/>        )<br/>          <br/>        self.decoder_blocks = nn.ModuleList([<br/>            DecoderBlock(n_dim, dropout, n_heads) for _ in range(n_decoder_blocks)<br/>        ])<br/>        <br/>        <br/>    def forward(self, tgt, memory, tgt_mask=None, tgt_padding_mask=None, memory_padding_mask=None):<br/>        x = self.embedding(tgt)<br/>        x = self.positional_encoding(x)<br/><br/>        for block in self.decoder_blocks:<br/>            x = block(<br/>                x, <br/>                memory, <br/>                tgt_mask=tgt_mask, <br/>                tgt_padding_mask=tgt_padding_mask, <br/>                memory_padding_mask=memory_padding_mask)<br/>        return x</span></pre><h1 id="affd" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Padding &amp; Masking</h1><p id="52ab" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Remember the Multi-head attention section where we mentionned excluding certain parts of the inputs when doing attention.</p><p id="9f23" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">During training, we consider batches of inputs and targets, wherein each instance may have a variable length. Consider the following example where we batch 4 words : banana, watermelon, pear, blueberry. In order to process them as a single batch, we need to align all words to the length of the longest word (watermelon). We will therefore add an extra token, PAD, to each word so they all end up with the same length as watermelon.</p><p id="809d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the below picture, the upper table represents the raw data, the lower table the encoded version:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/13b635dfa9a7c67680987264e66197b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyCxYMom3TimQY2VGNFGPQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by author)</figcaption></figure><p id="367e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In our case, we want to exclude padding indices from the attention weights being calculated. We can therefore compute a mask as follows, both for source and target data :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="ea72" class="pv ne fq pm b bg pw px l py pz">padding_mask = (x == PAD_IDX)</span></pre><p id="27fb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">What about causal masks now ? Well if we want, at each time step, that the model can attend only steps in the past, this means that for each time step T, the model can only attend to each step t for t in 1…T. It is a double for loop, we can therefore use a matrix to compute that :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/887c940f4c6dafa57ca2f5a0d50ce1f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gIU1WTNJle6N0tw6P-C4OA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">(image by author)</figcaption></figure><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="367f" class="pv ne fq pm b bg pw px l py pz">def generate_square_subsequent_mask(size: int):<br/>      """Generate a triangular (size, size) mask. From PyTorch docs."""<br/>      mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()<br/>      mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))<br/>      return mask</span></pre><h1 id="4a6a" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Case study : a Word-Reverse Transformer</h1><p id="c0bd" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s now build our Transformer by bringing parts together !</p><p id="cd2b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In our use case, we will use a very simple dataset to showcase how Transformers actually learn.</p><p id="a268" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr"><em class="pi">“But why use a Transformer to reverse words ? I already know how to do that in Python with word[::-1] !”</em></strong></p><p id="d60a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The objective here is to see whether the Transformer attention mechanism works. What we expect is to see attention weights to move from right to left when given an input sequence. If so, this means our Transformer has learned a very simple grammar, which is just reading from right to left, and could generalize to more complex grammars when doing real-life language translation.</p><p id="65cf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s first begin with our custom Transformer class :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="ae8c" class="pv ne fq pm b bg pw px l py pz">import torch<br/>import torch.nn as nn<br/>import math<br/><br/>from .encoder import Encoder<br/>from .decoder import Decoder<br/><br/><br/>class Transformer(nn.Module):<br/>    def __init__(self, **kwargs):<br/>        super(Transformer, self).__init__()<br/>        <br/>        for k, v in kwargs.items():<br/>            print(f" * {k}={v}")<br/>        <br/>        self.vocab_size = kwargs.get('vocab_size')<br/>        self.model_dim = kwargs.get('model_dim')<br/>        self.dropout = kwargs.get('dropout')<br/>        self.n_encoder_layers = kwargs.get('n_encoder_layers')<br/>        self.n_decoder_layers = kwargs.get('n_decoder_layers')<br/>        self.n_heads = kwargs.get('n_heads')<br/>        self.batch_size = kwargs.get('batch_size')<br/>        self.PAD_IDX = kwargs.get('pad_idx', 0)<br/><br/>        self.encoder = Encoder(<br/>            self.vocab_size, self.model_dim, self.dropout, self.n_encoder_layers, self.n_heads)<br/>        self.decoder = Decoder(<br/>            self.vocab_size, self.model_dim, self.dropout, self.n_decoder_layers, self.n_heads)<br/>        self.fc = nn.Linear(self.model_dim, self.vocab_size)<br/>        <br/><br/>    @staticmethod    <br/>    def generate_square_subsequent_mask(size: int):<br/>            """Generate a triangular (size, size) mask. From PyTorch docs."""<br/>            mask = (1 - torch.triu(torch.ones(size, size), diagonal=1)).bool()<br/>            mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))<br/>            return mask<br/><br/><br/>    def encode(<br/>            self, <br/>            x: torch.Tensor, <br/>        ) -&gt; torch.Tensor:<br/>        """<br/>        Input<br/>            x: (B, S) with elements in (0, C) where C is num_classes<br/>        Output<br/>            (B, S, E) embedding<br/>        """<br/><br/>        mask = (x == self.PAD_IDX).float()<br/>        encoder_padding_mask = mask.masked_fill(mask == 1, float('-inf'))<br/>        <br/>        # (B, S, E)<br/>        encoder_output = self.encoder(<br/>            x, <br/>            padding_mask=encoder_padding_mask<br/>        )  <br/>        <br/>        return encoder_output, encoder_padding_mask<br/>    <br/>    <br/>    def decode(<br/>            self, <br/>            tgt: torch.Tensor, <br/>            memory: torch.Tensor, <br/>            memory_padding_mask=None<br/>        ) -&gt; torch.Tensor:<br/>        """<br/>        B = Batch size<br/>        S = Source sequence length<br/>        L = Target sequence length<br/>        E = Model dimension<br/>        <br/>        Input<br/>            encoded_x: (B, S, E)<br/>            y: (B, L) with elements in (0, C) where C is num_classes<br/>        Output<br/>            (B, L, C) logits<br/>        """<br/>        <br/>        mask = (tgt == self.PAD_IDX).float()<br/>        tgt_padding_mask = mask.masked_fill(mask == 1, float('-inf'))<br/><br/>        decoder_output = self.decoder(<br/>            tgt=tgt, <br/>            memory=memory, <br/>            tgt_mask=self.generate_square_subsequent_mask(tgt.size(1)), <br/>            tgt_padding_mask=tgt_padding_mask, <br/>            memory_padding_mask=memory_padding_mask,<br/>        )  <br/>        output = self.fc(decoder_output)  # shape (B, L, C)<br/>        return output<br/><br/>        <br/>        <br/>    def forward(<br/>            self, <br/>            x: torch.Tensor, <br/>            y: torch.Tensor, <br/>        ) -&gt; torch.Tensor:<br/>        """<br/>        Input<br/>            x: (B, Sx) with elements in (0, C) where C is num_classes<br/>            y: (B, Sy) with elements in (0, C) where C is num_classes<br/>        Output<br/>            (B, L, C) logits<br/>        """<br/>        <br/>        # Encoder output shape (B, S, E)<br/>        encoder_output, encoder_padding_mask = self.encode(x)  <br/><br/>        # Decoder output shape (B, L, C)<br/>        decoder_output = self.decode(<br/>            tgt=y, <br/>            memory=encoder_output, <br/>            memory_padding_mask=encoder_padding_mask<br/>        )  <br/>        <br/>        return decoder_output</span></pre><h2 id="a707" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">Performing Inference with Greedy Decoding</h2><p id="28af" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We need to add a method which will act as the famous <code class="cx pj pk pl pm b">model.predict</code> of scikit.learn. The objective is to ask the model to dynamically output predictions given an input. During inference, there is not target : the model starts by outputting a token by attending to the output, and uses its own prediction to continue emitting tokens. This is why those models are often called auto-regressive models, as they use past predictions to predict to next one.</p><p id="12f8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The problem with greedy decoding is that it considers the token with the highest probability at each step. This can lead to very bad predictions if the first tokens are completely wrong. There are other decoding methods, such as Beam search, which consider a shortlist of candidate sequences (think of keeping top-k tokens at each time step instead of the argmax) and return the sequence with the highest total probability.</p><p id="e8e4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For now, let’s implement greedy decoding and add it to our Transformer model:</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="23b2" class="pv ne fq pm b bg pw px l py pz">def predict(<br/>            self,<br/>            x: torch.Tensor,<br/>            sos_idx: int=1,<br/>            eos_idx: int=2,<br/>            max_length: int=None<br/>        ) -&gt; torch.Tensor:<br/>        """<br/>        Method to use at inference time. Predict y from x one token at a time. This method is greedy<br/>        decoding. Beam search can be used instead for a potential accuracy boost.<br/><br/>        Input<br/>            x: str<br/>        Output<br/>            (B, L, C) logits<br/>        """<br/><br/>        # Pad the tokens with beginning and end of sentence tokens<br/>        x = torch.cat([<br/>            torch.tensor([sos_idx]), <br/>            x, <br/>            torch.tensor([eos_idx])]<br/>        ).unsqueeze(0)<br/><br/>        encoder_output, mask = self.transformer.encode(x) # (B, S, E)<br/>        <br/>        if not max_length:<br/>            max_length = x.size(1)<br/><br/>        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * sos_idx<br/>        for step in range(1, max_length):<br/>            y = outputs[:, :step]<br/>            probs = self.transformer.decode(y, encoder_output)<br/>            output = torch.argmax(probs, dim=-1)<br/>            <br/>            # Uncomment if you want to see step by step predicitons<br/>            # print(f"Knowing {y} we output {output[:, -1]}")<br/><br/>            if output[:, -1].detach().numpy() in (eos_idx, sos_idx):<br/>                break<br/>            outputs[:, step] = output[:, -1]<br/>            <br/>        <br/>        return outputs</span></pre><h2 id="538c" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">Creating toy data</h2><p id="9f06" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We define a small dataset which inverts words, meaning that “helloworld” will return “dlrowolleh”:</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="c749" class="pv ne fq pm b bg pw px l py pz">import numpy as np<br/>import torch<br/>from torch.utils.data import Dataset<br/><br/><br/>np.random.seed(0)<br/><br/>def generate_random_string():<br/>    len = np.random.randint(10, 20)<br/>    return "".join([chr(x) for x in np.random.randint(97, 97+26, len)])<br/><br/>class ReverseDataset(Dataset):<br/>    def __init__(self, n_samples, pad_idx, sos_idx, eos_idx):<br/>        super(ReverseDataset, self).__init__()<br/>        self.pad_idx = pad_idx<br/>        self.sos_idx = sos_idx<br/>        self.eos_idx = eos_idx<br/>        self.values = [generate_random_string() for _ in range(n_samples)]<br/>        self.labels = [x[::-1] for x in self.values]<br/><br/>    def __len__(self):<br/>        return len(self.values)  # number of samples in the dataset<br/><br/>    def __getitem__(self, index):<br/>        return self.text_transform(self.values[index].rstrip("\n")), \<br/>            self.text_transform(self.labels[index].rstrip("\n"))<br/>        <br/>    def text_transform(self, x):<br/>        return torch.tensor([self.sos_idx] + [ord(z)-97+3 for z in x] + [self.eos_idx])</span></pre><p id="8b55" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will now define training and evaluation steps :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="e435" class="pv ne fq pm b bg pw px l py pz">PAD_IDX = 0<br/>SOS_IDX = 1<br/>EOS_IDX = 2<br/><br/>def train(model, optimizer, loader, loss_fn, epoch):<br/>    model.train()<br/>    losses = 0<br/>    acc = 0<br/>    history_loss = []<br/>    history_acc = [] <br/><br/>    with tqdm(loader, position=0, leave=True) as tepoch:<br/>        for x, y in tepoch:<br/>            tepoch.set_description(f"Epoch {epoch}")<br/><br/>            optimizer.zero_grad()<br/>            logits = model(x, y[:, :-1])<br/>            loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))<br/>            loss.backward()<br/>            optimizer.step()<br/>            losses += loss.item()<br/>            <br/>            preds = logits.argmax(dim=-1)<br/>            masked_pred = preds * (y[:, 1:]!=PAD_IDX)<br/>            accuracy = (masked_pred == y[:, 1:]).float().mean()<br/>            acc += accuracy.item()<br/>            <br/>            history_loss.append(loss.item())<br/>            history_acc.append(accuracy.item())<br/>            tepoch.set_postfix(loss=loss.item(), accuracy=100. * accuracy.item())<br/><br/>    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc<br/><br/><br/>def evaluate(model, loader, loss_fn):<br/>    model.eval()<br/>    losses = 0<br/>    acc = 0<br/>    history_loss = []<br/>    history_acc = [] <br/><br/>    for x, y in tqdm(loader, position=0, leave=True):<br/><br/>        logits = model(x, y[:, :-1])<br/>        loss = loss_fn(logits.contiguous().view(-1, model.vocab_size), y[:, 1:].contiguous().view(-1))<br/>        losses += loss.item()<br/>        <br/>        preds = logits.argmax(dim=-1)<br/>        masked_pred = preds * (y[:, 1:]!=PAD_IDX)<br/>        accuracy = (masked_pred == y[:, 1:]).float().mean()<br/>        acc += accuracy.item()<br/>        <br/>        history_loss.append(loss.item())<br/>        history_acc.append(accuracy.item())<br/><br/>    return losses / len(list(loader)), acc / len(list(loader)), history_loss, history_acc</span></pre><p id="b946" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And train the model for a couple of epochs:</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="c187" class="pv ne fq pm b bg pw px l py pz">import torch<br/>import time<br/>import torch.nn as nn<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>from tqdm import tqdm<br/>from torch.utils.data import DataLoader<br/>from torch.nn.utils.rnn import pad_sequence<br/>from mpl_toolkits.axes_grid1 import ImageGrid<br/><br/><br/>def collate_fn(batch):<br/>    """ <br/>    This function pads inputs with PAD_IDX to have batches of equal length<br/>    """<br/>    src_batch, tgt_batch = [], []<br/>    for src_sample, tgt_sample in batch:<br/>        src_batch.append(src_sample)<br/>        tgt_batch.append(tgt_sample)<br/><br/>    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)<br/>    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX, batch_first=True)<br/>    return src_batch, tgt_batch<br/><br/># Model hyperparameters<br/>args = {<br/>    'vocab_size': 128,<br/>    'model_dim': 128,<br/>    'dropout': 0.1,<br/>    'n_encoder_layers': 1,<br/>    'n_decoder_layers': 1,<br/>    'n_heads': 4<br/>}<br/><br/># Define model here<br/>model = Transformer(**args)<br/><br/># Instantiate datasets<br/>train_iter = ReverseDataset(50000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)<br/>eval_iter = ReverseDataset(10000, pad_idx=PAD_IDX, sos_idx=SOS_IDX, eos_idx=EOS_IDX)<br/>dataloader_train = DataLoader(train_iter, batch_size=256, collate_fn=collate_fn)<br/>dataloader_val = DataLoader(eval_iter, batch_size=256, collate_fn=collate_fn)<br/><br/># During debugging, we ensure sources and targets are indeed reversed<br/># s, t = next(iter(dataloader_train))<br/># print(s[:4, ...])<br/># print(t[:4, ...])<br/># print(s.size())<br/><br/># Initialize model parameters<br/>for p in model.parameters():<br/>    if p.dim() &gt; 1:<br/>        nn.init.xavier_uniform_(p)<br/><br/># Define loss function : we ignore logits which are padding tokens<br/>loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)<br/><br/># Save history to dictionnary<br/>history = {<br/>    'train_loss': [],<br/>    'eval_loss': [],<br/>    'train_acc': [],<br/>    'eval_acc': []<br/>}<br/><br/># Main loop<br/>for epoch in range(1, 4):<br/>    start_time = time.time()<br/>    train_loss, train_acc, hist_loss, hist_acc = train(model, optimizer, dataloader_train, loss_fn, epoch)<br/>    history['train_loss'] += hist_loss<br/>    history['train_acc'] += hist_acc<br/>    end_time = time.time()<br/>    val_loss, val_acc, hist_loss, hist_acc = evaluate(model, dataloader_val, loss_fn)<br/>    history['eval_loss'] += hist_loss<br/>    history['eval_acc'] += hist_acc<br/>    print((f"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, Val loss: {val_loss:.3f}, Val acc: {val_acc:.3f} "f"Epoch time = {(end_time - start_time):.3f}s"))</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/9f7afec6c69db6a3ed76a02e7ce64609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUS_EiqLd1MbKLMIIguvpw.png"/></div></div></figure><h2 id="47eb" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">Visualize attention</h2><p id="83d3" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We define a little function to access the weights of the attention heads :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="0429" class="pv ne fq pm b bg pw px l py pz">fig = plt.figure(figsize=(10., 10.))<br/>images = model.decoder.decoder_blocks[0].cross_attention.attention_weigths[0,...].detach().numpy()<br/>grid = ImageGrid(fig, 111,  # similar to subplot(111)<br/>                nrows_ncols=(2, 2),  # creates 2x2 grid of axes<br/>                axes_pad=0.1,  # pad between axes in inch.<br/>                )<br/><br/>for ax, im in zip(grid, images):<br/>    # Iterating over the grid returns the Axes.<br/>    ax.imshow(im)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/58dc28e4552c6a895877083d7b02629a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJVhnc7YQEexzFUW3tNcBA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image from author</figcaption></figure><p id="4e0d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can see a nice right-to-left pattern, when reading weights from the top. Vertical parts at the bottom of the y-axis may surely represent masked weights due to padding mask</p><h2 id="ea57" class="qa ne fq bf nf qb qc qd ni qe qf qg nl oi qh qi qj om qk ql qm oq qn qo qp qq bk">Testing our model !</h2><p id="ebba" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">To test our model with new data, we will define a little <code class="cx pj pk pl pm b">Translator</code> class to help us with the decoding :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="3ce9" class="pv ne fq pm b bg pw px l py pz">class Translator(nn.Module):<br/>    def __init__(self, transformer):<br/>        super(Translator, self).__init__()<br/>        self.transformer = transformer<br/>    <br/>    @staticmethod<br/>    def str_to_tokens(s):<br/>        return [ord(z)-97+3 for z in s]<br/>    <br/>    @staticmethod<br/>    def tokens_to_str(tokens):<br/>        return "".join([chr(x+94) for x in tokens])<br/>    <br/>    def __call__(self, sentence, max_length=None, pad=False):<br/>        <br/>        x = torch.tensor(self.str_to_tokens(sentence))<br/>        x = torch.cat([torch.tensor([SOS_IDX]), x, torch.tensor([EOS_IDX])]).unsqueeze(0)<br/>        <br/>        encoder_output, mask = self.transformer.encode(x) # (B, S, E)<br/>        <br/>        if not max_length:<br/>            max_length = x.size(1)<br/>            <br/>        outputs = torch.ones((x.size()[0], max_length)).type_as(x).long() * SOS_IDX<br/>        <br/>        for step in range(1, max_length):<br/>            y = outputs[:, :step]<br/>            probs = self.transformer.decode(y, encoder_output)<br/>            output = torch.argmax(probs, dim=-1)<br/>            print(f"Knowing {y} we output {output[:, -1]}")<br/>            if output[:, -1].detach().numpy() in (EOS_IDX, SOS_IDX):<br/>                break<br/>            outputs[:, step] = output[:, -1]<br/>            <br/>        <br/>        return self.tokens_to_str(outputs[0])<br/><br/>translator = Translator(model)</span></pre><p id="f9c5" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You should be able to see the following :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/0a1a3a1b926b437cfc76477fd3c99811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXdU0GB9aGag3FeDZjK4Ag.png"/></div></div></figure><p id="4ee0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And if we print the attention head we will observe the following :</p><pre class="mm mn mo mp mq ps pm pt bp pu bb bk"><span id="0ee3" class="pv ne fq pm b bg pw px l py pz">fig = plt.figure()<br/>images = model.decoder.decoder_blocks[0].cross_attention.attention_weigths[0,...].detach().numpy().mean(axis=0)<br/><br/>fig, ax = plt.subplots(1,1, figsize=(10., 10.))<br/># Iterating over the grid returs the Axes.<br/>ax.set_yticks(range(len(out)))<br/>ax.set_xticks(range(len(sentence)))<br/><br/>ax.xaxis.set_label_position('top') <br/><br/>ax.set_xticklabels(iter(sentence))<br/>ax.set_yticklabels([f"step {i}" for i in range(len(out))])<br/>ax.imshow(images)</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/8481c6a87c9210c958afdf396844f629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vKzI9nNSfQEV2xrMbHmRig.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">image from author</figcaption></figure><p id="28df" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can clearly see that the model attends from right to left when inverting our sentence “reversethis” ! (The step 0 actually receives the beginning of sentence token).</p><h1 id="0716" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusion</h1><p id="4b94" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">That’s it, you are now able to write Transformer and use it with larger datasets to perform machine translation of create you own BERT for example !</p><p id="9afe" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I wanted this tutorial to show you the caveats when writing a Transformer : padding and masking are maybe the parts requiring the most attention (pun unintended) as they will define the good performance of the model during inference.</p><p id="3f51" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the following articles, we will look at how to create your own BERT model and how to use Equinox, a highly performant library on top of JAX.</p><p id="0918" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Stay tuned !</p><h1 id="f339" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Useful links</h1><p id="15f1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">(+) <a class="af nc" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">“The Annotated Transformer”</a><br/>(+) “<a class="af nc" href="https://peterbloem.nl/blog/transformers" rel="noopener ugc nofollow" target="_blank">Transformers from scratch</a>”<br/>(+) <a class="af nc" href="https://www.tensorflow.org/text/tutorials/transformer" rel="noopener ugc nofollow" target="_blank">“Neural machine translation with a Transformer and Keras”</a><br/>(+) <a class="af nc" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">“The Illustrated Transformer”</a><br/>(+) <a class="af nc" href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html" rel="noopener ugc nofollow" target="_blank">University of Amsterdam Deep Learning Tutorial </a><br/>(+) <a class="af nc" href="https://pytorch.org/tutorials/beginner/translation_transformer.html" rel="noopener ugc nofollow" target="_blank">Pytorch tutorial on Transformers</a></p></div></div></div></div>    
</body>
</html>