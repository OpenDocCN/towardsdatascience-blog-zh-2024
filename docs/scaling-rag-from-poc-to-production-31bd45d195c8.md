# 将RAG从概念验证（POC）扩展到生产

> 原文：[https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07](https://towardsdatascience.com/scaling-rag-from-poc-to-production-31bd45d195c8?source=collection_archive---------0-----------------------#2024-10-07)

## 启动和扩展的常见挑战与架构组件

[](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[![Anurag Bhagat](../Images/3711a1fce6e2a45d649e534f08c3d0ca.png)](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------) [Anurag Bhagat](https://medium.com/@bhagatanurag03?source=post_page---byline--31bd45d195c8--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31bd45d195c8--------------------------------) ·阅读时长7分钟·2024年10月7日

--

![](../Images/ee14c78dc3f1d708f7e587a83fe8d0db.png)

*来源：在AI（OpenAI的Dall-E模型）的帮助下生成*

# 1\. 引言

## 1.1\. RAG概述

那些沉浸于生成性AI及其在个人生产力应用之外的大规模应用的朋友们，可能已经接触过检索增强生成（RAG）的概念。RAG架构由两个关键组件组成——检索组件，它使用向量数据库对大量文档进行基于索引的搜索，然后将其传送给大语言模型（LLM）以生成基于提示中更丰富上下文的有依据的响应。

无论你是在构建面向客户的聊天机器人以回答重复性问题并减少客户服务代表的工作负担，还是在为工程师构建一款协同助手，帮助他们一步步浏览复杂的用户手册，RAG已经成为大语言模型（LLMs）应用的关键原型。这使得LLM能够根据成千上万的文档的事实基础，提供上下文相关的响应，从而减少幻觉现象并提高基于LLM的应用的可靠性。

## 1.2\. 为什么要从概念验证（POC）扩展到生产

如果你在问这个问题，我可能会挑战你回答：如果没有打算将其投入生产，为什么要构建一个POC（概念验证）？“试点地狱”是组织在开始实验时常见的风险，之后却陷入了实验模式。请记住，POC是昂贵的，真正的价值实现只有在进入生产并按规模执行时才会发生——无论是释放资源、提高效率，还是创造额外的收入流。

# 2\. 扩展RAG的关键挑战

## 2.1. 性能

RAG中的性能挑战有很多种形式。检索速度通常不是主要挑战，除非你的知识库有数百万份文档，即便如此，这个问题也可以通过搭建合适的基础设施来解决——当然，我们也受到推理时间的限制。我们遇到的第二个性能问题是如何获取“正确”的文段，以便传递给大语言模型（LLM）进行生成，且要保证高精度和高召回率。检索过程越差，LLM的回应就越缺乏上下文相关性。

## 2.2. 数据管理

我们都听过那句古老的谚语“垃圾进，垃圾出（GIGO）”。RAG不过是我们手头的一组工具，真正的价值来自于实际数据。由于RAG系统处理的是非结构化数据，因此它有自己的一系列挑战，包括但不限于——文档版本控制和格式转换（例如从PDF到文本），等等。

## 2.3. 风险

企业从试水到全面投入的最大犹豫之一，是使用基于AI的系统可能带来的风险。尽管使用RAG可以显著减少幻觉现象，但它们仍然存在非零的可能性。还有其他相关的风险，包括偏见、毒性、合规性风险等，这些风险可能带来长期的影响。

## 2.4. 集成到现有工作流程中

构建离线解决方案比较容易，但引入最终用户的视角至关重要，以确保解决方案不会让用户觉得负担重。没有用户愿意为了使用“新AI功能”而切换到另一个屏幕——用户希望AI功能能够内嵌到现有工作流程中，这样技术就能成为一种辅助工具，而不是日常工作中的干扰。

## 2.5. 成本

嗯，这一点似乎很显而易见，对吧？组织正在实施生成式人工智能（GenAI）用例，以便创造业务影响。如果实际收益低于我们的预期，或者出现成本超支，影响将大大减少，甚至可能完全抵消。

# 3. 扩展所需的架构组件

如果我们只谈论挑战而不谈论“那么我们该怎么做”，那就不公平了。你可以在架构堆栈中加入一些必要的组件，以克服或减少我们上面提到的一些问题。

## 3.1. 可扩展的向量数据库

许多团队通常会首先使用开源向量数据库，如[ChromaDB](https://www.trychroma.com/)，因为它们易于使用和定制，非常适合概念验证（POC）。然而，它在大规模部署中可能会面临挑战。这时，可扩展的向量数据库就发挥了作用（例如，[Pinecone](https://www.pinecone.io/)，[Weaviate](https://weaviate.io/platform)，[Milvus](https://milvus.io/)等），这些数据库针对高维向量搜索进行了优化，能够实现快速（亚毫秒级）、准确的检索，即使数据集的大小增加到百万或十亿向量，它们仍然能够高效运作，因为它们使用[近似最近邻](https://www.mongodb.com/resources/basics/ann-search)搜索技术。这些向量数据库提供API、插件和SDK，方便与现有工作流集成，同时支持横向扩展。根据平台的不同，可能需要探索[Databricks](https://www.databricks.com/product/machine-learning/vector-search)或[AWS](https://aws.amazon.com/opensearch-service/)提供的向量数据库。

![](../Images/6f8562c4e112308a47a06f348db3715d.png)

*来源：借助AI（OpenAI的Dall-E模型）生成*

## 3.2\. 缓存机制

缓存的概念几乎与互联网的诞生一样久远，[可以追溯到](https://cacm.acm.org/opinion/ibms-single-processor-supercomputer-efforts/#:~:text=a.,computer%20system%20to%20use%20cache)20世纪60年代。这个概念同样适用于生成型AI——如果有大量查询，可能达到数百万（在客户服务功能中非常常见），很可能许多查询是相同的或极为相似的。缓存技术可以避免向LLM发送请求，如果可以从最近的缓存响应中返回结果，这样不仅能减少成本，还能提高常见查询的响应速度。

这可以通过内存缓存（如[Redis](https://redis.io/)或[Memcached](https://memcached.org/)）来实现，针对较少的查询使用磁盘缓存，或使用分布式缓存（如Redis集群）。一些模型提供商（如Anthropic）提供[提示缓存](https://www.anthropic.com/news/prompt-caching)作为其API的一部分。

![](../Images/eb99db871ae334a898f3493fc951fc01.png)

*来源：借助AI（OpenAI的Dall-E模型）生成*

# 3.3\. 高级搜索技术

虽然并非一个明确的架构组件，但多种技术可以帮助提升搜索效率和准确性。以下是其中一些方法：

+   **混合搜索**：与仅依赖语义搜索（使用向量数据库）或关键词搜索不同，采用两者结合的方法来提升搜索效果。

+   **重新排序：** 使用 LLM 或 SLM 计算查询与每个搜索结果的相关性得分，并重新排序，从而仅提取并分享最相关的结果。这对于复杂领域或有许多文档返回的领域尤其有用。一个例子是 [Cohere’s Rerank](https://aws.amazon.com/marketplace/pp/prodview-pf7d2umihcseq)。

![](../Images/10211b4b3b6985b8800c7a4427794dc8.png)

*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*

# 3.4\. 负责任的人工智能层

您的负责任人工智能模块必须设计用来减轻偏见，确保透明度，与您组织的伦理价值观对齐，持续监控用户反馈，并跟踪符合相关行业/功能的法规要求。方法有很多，但从根本上来说，必须通过编程启用，并由人工监督。可以采用几种方式来实现：

+   **预处理：** 在用户查询发送到基础模型之前进行过滤。这可能包括检查偏见、有害内容、非预期用途等。

+   **后处理：** 在结果从基础模型返回后，向最终用户展示之前应用另一组检查。

这些检查可以作为小型可重用模块启用，您可以从外部提供商购买，或者根据自己的需求进行构建/定制。组织通常采用的一种方式是使用精心设计的提示和基础模型来编排工作流程，确保结果在通过所有检查之前不会呈现给最终用户。

![](../Images/184aa917e7c7eebc06ae752d9ef78de8.png)

*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*

## 3.5\. API 网关

API 网关可以发挥多种作用，帮助管理成本以及负责任的人工智能的各个方面：

+   提供统一的接口与基础模型互动，进行实验。

+   帮助开发精细化的成本和使用情况视图，按团队/用例/成本中心进行划分 — 包括速率限制、速度控制、配额管理

+   作为负责任的人工智能层，在请求/数据达到模型之前，过滤掉不适当的请求/数据。

+   启用审计追踪和访问控制

![](../Images/cb937658eacc1f107b854540ff8edb4e.png)

*来源：通过 AI（OpenAI 的 Dall-E 模型）生成*

# 4\. 这样就够了吗，还是我们需要更多？

当然不是。还有一些其他因素也需要考虑，包括但不限于：

+   该用例是否在您的用例路线图中占据战略地位？这使您能够获得领导支持，并获得适当的投资以支持开发和维护。

+   一个明确的评估标准，用于衡量应用程序在准确性、成本、延迟和负责任的人工智能等维度上的表现。

+   改善业务流程，保持知识的最新性，维护版本控制等。

+   设计 RAG 系统时，确保它仅在最终用户权限级别的基础上访问文档，以防止未经授权的访问。

+   使用设计思维将应用程序集成到最终用户的工作流程中。例如，如果你正在构建一个通过 Confluence 作为知识库回答技术问题的机器人，你是否应该构建一个独立的用户界面，还是将其与 Teams/Slack/其他用户已经使用的应用程序集成？

# 5. 结论

RAG 是一个典型的使用案例原型，也是组织首次尝试实施的几个用例之一。从POC到生产环境的 RAG 扩展面临诸多挑战，但通过精心的规划和执行，许多挑战是可以克服的。其中一些可以通过在架构和技术上的战术性投资来解决，而另一些则需要更好的战略方向和巧妙的规划。随着大语言模型（LLM）推理成本的不断下降，无论是由于推理成本的降低，还是开源模型的广泛采用，许多新用例的成本壁垒可能不再是问题。

*本文中的所有观点仅代表作者个人意见，并不代表对任何产品或服务的认可。*
