- en: 'MOIRAI-MOE: Upgrading MOIRAI with Mixture-of-Experts for Enhanced Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/moirai-moe-upgrading-moirai-with-mixture-of-experts-for-enhanced-forecasting-26a38017734f?source=collection_archive---------3-----------------------#2024-11-02](https://towardsdatascience.com/moirai-moe-upgrading-moirai-with-mixture-of-experts-for-enhanced-forecasting-26a38017734f?source=collection_archive---------3-----------------------#2024-11-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The popular foundation time-series model just got an update
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikoskafritsas?source=post_page---byline--26a38017734f--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page---byline--26a38017734f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--26a38017734f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--26a38017734f--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page---byline--26a38017734f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--26a38017734f--------------------------------)
    ·9 min read·Nov 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9226dcc182fdd8bd83c8b18f4665da4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[*Image Source*](https://arxiv.org/pdf/2410.10469)'
  prefs: []
  type: TYPE_NORMAL
- en: '**The race to build the Top foundation forecasting model is on!**'
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce’s ***MOIRAI***, one of the early foundation models, achieved high
    benchmark results and was open-sourced along with its pretraining dataset, LOTSA.
  prefs: []
  type: TYPE_NORMAL
- en: We extensively analyzed how MOIRAI works [here](https://medium.com/towards-data-science/moirai-salesforces-foundation-model-for-time-series-forecasting-4eff6c34093d)
    — and built an end-to-end project comparing MOIRAI with popular statistical models.
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce has now released an upgraded version — ***MOIRAI-MOE*** — with significant
    improvements, particularly the addition of **Mixture-of-Experts (MOE)**. We briefly
    discussed MOE when another model, [Time-MOE](https://aihorizonforecast.substack.com/p/time-moe-billion-scale-time-series),
    also used multiple experts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: How MOIRAI-MOE works and why it’s a powerful model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key differences between MOIRAI and MOIRAI-MOE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How MOIRAI-MOE’s use of Mixture-of-Experts enhances accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How Mixture-of-Experts generally solves frequency variation issues in foundation
    time-series models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: ✅ I’ve launched [**AI Horizon Forecast**](https://aihorizonforecast.substack.com/)**,**
    a newsletter focusing on time-series and innovative…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
