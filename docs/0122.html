<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mini-Max Optimization Design of Generative Adversarial Networks (GAN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mini-Max Optimization Design of Generative Adversarial Networks (GAN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12">https://towardsdatascience.com/mini-max-optimization-design-of-generative-adversarial-networks-gan-dc1b9ea44a02?source=collection_archive---------8-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="929a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Nested bi-level optimization and equilibrium seeking objective</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Michio Suginoo" class="l ep by dd de cx" src="../Images/15e4a70d17d163889cc902bf4409931a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kx7NRQ9KN0OWP2BRK-obxA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://deeporigami.medium.com/?source=post_page---byline--dc1b9ea44a02--------------------------------" rel="noopener follow">Michio Suginoo</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--dc1b9ea44a02--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="7890" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk">Introduction</h2><p id="5075" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no mt np nq nr mx ns nt nu nb nv nw nx ny fj bk">Generative Adversarial Networks (GAN) demonstrated outstanding performance in generating realistic synthetic data which were indistinguishable from the real data. Unfortunately, GAN caught the public’s attention because of its illegit applications, <a class="af nz" href="https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/" rel="noopener ugc nofollow" target="_blank">Deep Fake</a>. (Knight, 2018)</p><p id="73d1" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">As its name suggests, Generative Adversarial Nets (GAN) is composed of two networks: the generative network (the generator) and the adversarial network (the discriminator). Incorporating an adversarial scheme into its architecture makes GAN a special type of generative network.</p><p id="74c6" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Importantly, GAN is non-parametric, therefore, would not impose much formal statistical requirements such as the Markov chain. Instead of imposing statistical assumptions, with the help of the adversary network, the generative network learns the probability distribution of the real data through back-propagation in deep neural networks.</p><p id="871c" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">In order to generate realistic synthetic data, GAN pits these two agents against each other within its architecture. In this game, while the generator attempts to simulate synthetic samples which imitate real samples, the discriminator attempts to distinguish between the real samples and the synthetic ones. In other words, while the generator, <strong class="ni fr"><em class="of">G</em></strong>, tries to deceive the discriminator by counterfeiting, the discriminator, <strong class="ni fr"><em class="of">D</em></strong>, plays a role of the police to detect the synthetic (fake) data. (Goodfellow, et al., 2014, p. 1) In a way, these two agents attempt to achieve diametrically opposite objectives.</p><p id="667c" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">As they improve their skills, the synthetic data becomes indistinguishable from the real data. Thanks to its adversary (the discriminator), the generator learns how to better imitate the probability distribution of the given real data.</p><p id="4f76" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Since within its architecture GAN has to train two learners that attempt to achieve contrasting objectives through interactions, it has a unique optimization design (bi-level training mechanism and equilibrium seeking objectives). In this context, it took me a while to digest the architectural design of GAN. In this context, I decided to write this article to share my journey with those who are new to GAN so that they can comprehend the architectural peculiarity of GAN more smoothly. I hope that the readers find this article useful as a supplementary note.</p><p id="d6bb" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">As a precaution, GAN is heuristic. And there are many variants of GAN applications today. This article discusses only the architectural design of the original GAN.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d0bf" class="mi mj fq bf mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf bk"><strong class="al"><em class="oo">The Original GAN Design</em></strong></h2><p id="0224" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no mt np nq nr mx ns nt nu nb nv nw nx ny fj bk">The base architecture of the original GAN was introduced in a seminal paper: “<a class="af nz" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">Generative Adversarial Nets</a>” (Goodfellow, et al., 2014). In this original GAN paper, in order to train these two agents which pursue the diametrically opposite objectives, the co-authors designed a “<a class="af nz" href="https://en.wikipedia.org/wiki/Bilevel_optimization" rel="noopener ugc nofollow" target="_blank">bi-level optimization (training)</a>” architecture, in which one internal training block (training of the discriminator) is nested within another high-level training block (training of the generator). And GAN trains these two agents alternately in this bi-level training framework.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq or"><img src="../Images/ee36b648bf82a7b9e95df5ddb35f87b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycLOYA4e0aGva5nwJtA-aQ.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="74c8" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk"><strong class="ni fr"><em class="of">Discriminator and Generator</em></strong></p><p id="836c" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Now, let’s see what these two agents do in their learning process.</p><p id="6930" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">It is straight-forward that the discriminator is a binary classifier by design. Given mixed samples from the real data and the synthetic data, it classifies whether each sample is real (label=1) or fake/synthetic (label=0).</p><p id="d991" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">On the other hand, the generator is a noise distribution by design. And it is trained to imitate the probability distribution of the real dataset through an iteration. At every step of the training iteration, the learned generative model (updated generator) is copied and used as the new noise distribution. Thereafter, the new noise distribution is used to train the discriminator. (Goodfellow I. , 2015, p. 2).</p><p id="1420" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Let’s set the following:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pi"><img src="../Images/745830f89d07f931ccad830e0db5ad4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxbofQOvbxirwkbC9BqHKg.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="6347" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">We take the input noise, z, and calculate its prior distribution, G(z), to define the generator.</p><p id="4983" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">In this setting, the ultimate goal of the generator is to deceive the discriminator by transforming its own distribution as close as the distribution of the real dataset.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div class="op oq pj"><img src="../Images/d3502d26f13d2481622262adce417521.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*MRBLKmv_Pa4hHM-JZp6nxQ.png"/></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="1831" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk"><strong class="ni fr"><em class="of">Two Objective Functions: Mini-Max Game</em></strong></p><p id="cf7f" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Repeatedly, GAN has two agents to train within its architecture. And these agents have confronting objectives. Therefore, GAN has two objective functions: one for the discriminator and the other for the generator.</p><p id="c61c" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">On one hand, the discriminator, <em class="of">D</em>, as a binary classifier, needs to maximize the probability of correctly assigning labels to both the real data (label=1) and the synthetic data (label=0).</p><p id="fd3c" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">On the other hand, the ultimate goal of the generator is to fool the classifier by creating synthetic data which is indistinguishable from the real data. Therefore, the generator attempts to deceive the classifier so that the discriminator would incorrectly classify the synthetic data with label 1. In other words, the objective of the generator is “<em class="of">to maximize the probability of D making a mistake.</em>” (Goodfellow, et al., 2014, p. 1)</p><p id="c194" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">At a conceptual level, in order to achieve their diametrically opposite objectives, these two agents can refer to the following general log-likelihood formula, <em class="of">V</em>, typically used for a binary classification problem.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq pk"><img src="../Images/8b48358640e02c8898307c84e4b64a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCfyAY0x1i7bT5tAV6imGA.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="9263" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">While GAN trains the discriminator to maximize the objective function, it trains the generator to minimize the second term of the objective function. In this sense, the co-authors called the overall objective “<strong class="ni fr"><em class="of">minimax game</em></strong>”. (Goodfellow, et al., 2014, p. 3)</p><p id="e26a" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk"><strong class="ni fr"><em class="of">Non-saturation Modification:</em></strong></p><p id="0e0b" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">In its implementation, the co-authors encountered a problem of saturation during the early stage of training the generator.</p><blockquote class="pl pm pn"><p id="3323" class="ng nh of ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">“Early in learning, when G is poor, D can reject samples with high confidence because they are clearly different from the training data. In this case, log(1 — D(G(z))) saturates.”</p></blockquote><p id="8e34" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">In order to resolve the saturation issue, they converted the second term of the original log-likelihood objective function as follows and recommended to maximize it for the generator:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq po"><img src="../Images/1047088989096cf0cee24f1e93409952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zatXfLgggP6-EzW3PEZw2w.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="30a6" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">This formula reflects the objective of the generator “<em class="of">to maximize the probability of D making a mistake</em>”. (Goodfellow, et al., 2014, p. 1)</p><p id="653b" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk"><strong class="ni fr"><em class="of">Evaluation</em></strong></p><p id="7a2a" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">During the training, the generator continues creating better synthetic data to deceive the discriminator, while the discriminator is improving its detection ability. In this sense, the ultimate objective of GAN’s overall optimization is not to search for the global maximum of either of these objective functions. It is rather to seek an equilibrium point, where neither of agents can improve the performance. In a sese, at the equilibrium point, the discriminator is unable to distinguish between the real data and the synthetic data because the generator is able to create synthetic data as real as possible.</p><p id="ade2" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">This goal setting of the objective function is very unique for GAN. And one of the co-authors, Ian Goodfellow, describes the equilibrium point as follows:</p><blockquote class="pl pm pn"><p id="850e" class="ng nh of ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">“it corresponds to a saddle point that is a local maximum for the classifier and a local minimum for the generator” (Goodfellow I. , 2015, p. 2).</p></blockquote><p id="60a0" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Moreover, the equilibrium point can be conceptually represented by the probability of random guessing, 0.5 (50%).</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div class="op oq pp"><img src="../Images/3e74e529cd1bd6b29a7b12545688eb3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*lZCwTi2k5YqCRFGTRnPcVw.png"/></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="618b" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk"><strong class="ni fr"><em class="of">Alternating Training Process: Nested Bi-level Optimization</em></strong></p><p id="7456" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">In order to achieve this ultimate goal, GAN designed an alternating learning process in a “bi-level optimization” framework, in which the training loop of the discriminator is nested within another higher training loop of the generator. This bi-level optimization framework enables GAN to alternate the training process between these two agents: <em class="of">k-steps</em> of training D and one step of training G (Goodfellow, et al., 2014, p. 3). During the alternation of the two models, it is important to freeze the learning process of the other while training one model; “<em class="of">the generator is updated while holding the discriminator fixed, and vice versa</em>” (Goodfellow I. , 2015, p. 3).</p><p id="9847" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">The following algorithm revised the original algorithm presented in the original GAN paper in order to fully reflect the recommended conversion of the log-likelihood for the generator.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div class="op oq pq"><img src="../Images/1c7d25fd63b2c1395701db01c688802a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*iXAVkRmooCTZEM5HI_petg.png"/></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">Image by Author</figcaption></figure><p id="7040" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">As you see in the algorithm, while GAN takes samples from both the generative model (generator) and the real data during the forward propagation, it trains the both agents during the backpropagation. (Goodfellow, et al., 2014, p. 2) It follows the convention of deep neural networks.</p><p id="83c9" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">And GAN trains the discriminator first in the nested block, then trains the generator to fool the trained discriminator at every iteration at the upper loop. Then, it continues to iterate this bi-level training until it arrives at an equilibrium point as discussed earlier.</p><p id="b025" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Overall, technically, GAN learns the probability distribution of the real data through the generator; the discriminator is just one inner component built in the nested block within the generator’s learning mechanism. The objective function of the generator reflects the trained discriminator model on the piecemeal basis in its formula at the upper level optimization process. In other words, for every iteration, the generator keeps updating its objective function once the discriminator gets trained within the nested optimization block.</p><p id="a358" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">That pretty much paints GAN’s algorithmic design of model optimization.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ada2" class="pr mj fq bf mk ps pt gq mo pu pv gt ms pw px py pz qa qb qc qd qe qf qg qh qi bk"><strong class="al"><em class="oo">Summary</em></strong></h1><p id="2af7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no mt np nq nr mx ns nt nu nb nv nw nx ny fj bk">In order to alternately train two agents — the discriminator and the generator — GAN adopted a bi-level optimization framework, where the discriminator is trained in the inner block nested within the training block of the generator.</p><p id="07e8" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Since these two agents have diametrically opposite objectives (in the sense that the discriminator aims at maximizing its binary classifier’s objective function and the generator at minimizing it), the co-authors called the overall objective “<strong class="ni fr"><em class="of">minimax game</em></strong>”. (Goodfellow, et al., 2014, p. 3) Overall, GAN aims at its mini-max optimization (training) objective by seeking an equilibrium point, where the discriminator is no longer able to distinguish between the real data and the synthetic data because now the synthetic data the generator creates is indistinguishable from the real data.</p><p id="b8e9" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Its nested bi-level training framework and its equilibrium seeking objective setting (in contrast to maximization objectives) characterize the mini-max optimization framework of GAN.</p><p id="628e" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">At last, it is important to note that the leading author, Ian Goodfellow, characterizes that the original GAN is heuristic and has theoretical limitations. For example, convergence is not guaranteed when the objective function is not convex. In this context, he articulated that GAN is open for further innovative improvements. As a matter of fact, a wide range of evaluation measures have been explored for variants of GAN applications (Borji, 2018). In this sense, I would like to emphasize that the architectural design outlined in this article describes only the prototype of GAN that was introduced in the original GAN paper. Therefore, the architectural design introduced in this article is neither comprehensive nor universal for other types of GAN applications.</p><p id="5dc4" class="pw-post-body-paragraph ng nh fq ni b go oa nk nl gr ob nn no mt oc nq nr mx od nt nu nb oe nw nx ny fj bk">Given that the precautionary note is well communicated with the readers, I hope that this article is useful for those who are new to GAN in starting their own journey with GAN.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bb4a" class="pr mj fq bf mk ps pt gq mo pu pv gt ms pw px py pz qa qb qc qd qe qf qg qh qi bk">References</h1><ul class=""><li id="3693" class="ng nh fq ni b go nj nk nl gr nm nn no mt np nq nr mx ns nt nu nb nv nw nx ny qj qk ql bk">Borji, A. (2018, 10 24). Pros and Cons of GAN Evaluation Measures. Retrieved from ArXvi: <a class="af nz" href="https://arxiv.org/abs/1802.03446" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1802.03446</a></li><li id="5090" class="ng nh fq ni b go qm nk nl gr qn nn no mt qo nq nr mx qp nt nu nb qq nw nx ny qj qk ql bk">Goodfellow, I. (2015, 5 21). On distinguishability criteria for estimating generative models. Retrieved from ArXiv: <a class="af nz" href="https://arxiv.org/abs/1412.6515" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1412.6515</a></li><li id="78bf" class="ng nh fq ni b go qm nk nl gr qn nn no mt qo nq nr mx qp nt nu nb qq nw nx ny qj qk ql bk">Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengioz, Y. (2014, 6 10). Generative Adversarial Nets. Retrieved from arXiv: <a class="af nz" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.2661</a></li><li id="2ea4" class="ng nh fq ni b go qm nk nl gr qn nn no mt qo nq nr mx qp nt nu nb qq nw nx ny qj qk ql bk">Knight, W. (2018, 8 17). Fake America great again. Retrieved from MIT Technology Review: <a class="af nz" href="https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/" rel="noopener ugc nofollow" target="_blank">https://www.technologyreview.com/2018/08/17/240305/fake-america-great-again/</a></li></ul></div></div></div></div>    
</body>
</html>