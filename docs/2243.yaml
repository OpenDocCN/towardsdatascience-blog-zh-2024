- en: GGUF Quantization with Imatrix and K-Quantization to Run LLMs on Your CPU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gguf-quantization-with-imatrix-and-k-quantization-to-run-llms-on-your-cpu-02356b531926?source=collection_archive---------2-----------------------#2024-09-13](https://towardsdatascience.com/gguf-quantization-with-imatrix-and-k-quantization-to-run-llms-on-your-cpu-02356b531926?source=collection_archive---------2-----------------------#2024-09-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fast and accurate GGUF models for your CPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--02356b531926--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--02356b531926--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--02356b531926--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--02356b531926--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--02356b531926--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--02356b531926--------------------------------)
    ·7 min read·Sep 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/453132ddbe32fc254b97a076581313bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: GGUF is a binary file format designed for efficient storage and fast large language
    model (LLM) loading with GGML, a C-based tensor library for machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: GGUF encapsulates all necessary components for inference, including the tokenizer
    and code, within a single file. It supports the conversion of various language
    models, such as Llama 3, Phi, and Qwen2\. Additionally, it facilitates model quantization
    to lower precisions to improve speed and memory efficiency on CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: We often write “GGUF quantization” but GGUF itself is only a file format, not
    a quantization method. There are several quantization algorithms implemented in
    llama.cpp to reduce the model size and serialize the resulting model in the GGUF
    format.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we will see how to accurately quantize an LLM and convert
    it to GGUF, using an importance matrix (imatrix) and the K-Quantization method.
    I provide the GGUF conversion code for Gemma 2 Instruct, using an imatrix. It
    works the same with other models supported by llama.cpp: Qwen2, Llama 3, Phi-3,
    etc. We will also see how to evaluate the accuracy of the quantization and inference
    throughput of the resulting models.'
  prefs: []
  type: TYPE_NORMAL
