- en: Running the STORM AI Research System with Your Local Documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AI assisted research using FEMA disaster response documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    ¬∑16 min read¬∑Oct 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)'
  prefs: []
  type: TYPE_IMG
- en: STORM researches the topic via perspective-guided question asking in simulated
    conversations. [Source](https://arxiv.org/abs/2402.14207)
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs: []
  type: TYPE_NORMAL
- en: '*The use of LLM agents is becoming more common for tackling multi-step long-context
    research tasks where traditional RAG direct prompting methods can sometimes struggle.
    In this article, we will explore a new and promising technique developed by Stanford
    called* ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval
    and* ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*),
    which uses LLM agents to simulate ‚ÄòPerspective-guided conversations‚Äô to reach
    complex research goals and generate rich research articles that can be used by
    humans in their pre-writing research. STORM was initially developed to gather
    information from web sources but also supports searching a local document vector
    store. In this article we will see how to implement STORM for AI-supported research
    on local PDFs, using US FEMA disaster preparedness and assistance documentation.*'
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs been amazing to watch how using LLMs for knowledge retrieval has progressed
    in a relatively short period of time. Since the [first paper on Retrieval Augmented
    Generation (RAG)](https://arxiv.org/abs/2005.11401) in 2020, we have seen the
    ecosystem grow to include a [cornucopia of available technique](https://arxiv.org/html/2312.10997v5#S2)s.
    One of the more advanced is agentic RAG where LLM agents iterate and refine document
    retrieval in order to solve more complex research tasks. It‚Äôs similar to how a
    human might carry out research, exploring a range of different search queries
    to build a better idea of the context, sometimes discussing the topic with other
    humans, and synthesizing everything into a final result. Single-turn RAG, even
    employing techniques such as query expansion and reranking, can struggle with
    more complex multi-hop research tasks like this.
  prefs: []
  type: TYPE_NORMAL
- en: There are quite a few patterns for knowledge retrieval using agent frameworks
    such as [Autogen](https://microsoft.github.io/autogen/0.2/), [CrewAI](https://www.crewai.com),
    and [LangGraph](https://www.langchain.com/langgraph) as well as specific AI research
    assistants such as [GPT Researcher](https://github.com/assafelovic/gpt-researcher).
    In this article, we will look at an LLM-powered research writing system from Stanford
    University, called **S**ynthesis of **T**opic **O**utlines through **R**etrieval
    and **M**ulti-perspective Question Asking ([STORM](https://arxiv.org/abs/2402.14207)).
  prefs: []
  type: TYPE_NORMAL
- en: STORM AI research writing system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: STORM applies a clever technique where LLM agents simulate ‚ÄòPerspective-guided
    conversations‚Äô to reach a research goal as well as extend ‚Äòoutline-driven RAG‚Äô
    for richer article generation.
  prefs: []
  type: TYPE_NORMAL
- en: Configured to generate Wikipedia-style articles, it was tested with a cohort
    of 10 experienced Wikipedia editors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e52360332e7847eb9aa2237a73652ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Survey results of 10 experienced Wikipedia Editors on the perceived usefulness
    of STORM. [Source](https://arxiv.org/abs/2402.14207).
  prefs: []
  type: TYPE_NORMAL
- en: Reception on the whole was positive, 70% of the editors felt that it would be
    a useful tool in their *pre-writing* stage when researching a topic. I hope in
    the future surveys could include more than 10 editors, but it should be noted
    that authors also benchmarked traditional article generation methods using FreshWiki,
    a dataset of recent high-quality Wikipedia articles, where STORM was found to
    outperform previous techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4076dab0bb937a746d51e0eb93b03985.png)'
  prefs: []
  type: TYPE_IMG
- en: Human evaluation by 10 experienced Wikipedia editors for on 20 pairs of articles
    generated by STORM and *oRAG*. Each pair of articles is evaluated by two Wikipedia
    editors. [Source](https://arxiv.org/abs/2402.14207).
  prefs: []
  type: TYPE_NORMAL
- en: STORM is [open source](https://github.com/stanford-oval/storm/tree/main) and
    available as a [Python package](https://pypi.org/project/knowledge-storm/) with
    additional implementations using frameworks such as [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/).
    More recently STORM has been enhanced to support human-AI collaborative knowledge
    curation called [Co-STORM](https://www.arxiv.org/abs/2408.15232), putting a human
    right in the center of the AI-assisted research loop.
  prefs: []
  type: TYPE_NORMAL
- en: Though it significantly outperforms baseline methods in both automatic and human
    evaluations, there are some caveats that the authors acknowledge. It isn‚Äôt yet
    multimodal, doesn‚Äôt produce experienced human-quality content ‚Äî it isn‚Äôt positioned
    yet for this I feel, being more targeted for *pre-writing* research than final
    articles ‚Äî and there are some nuances around references that require some future
    work. That said, if you have a deep research task, it‚Äôs worth checking out.
  prefs: []
  type: TYPE_NORMAL
- en: You can try out STORM [online](https://storm.genie.stanford.edu/) ‚Äî it‚Äôs fun!
    ‚Äî configured to perform research using information on the web.
  prefs: []
  type: TYPE_NORMAL
- en: '**But what about running STORM with your own data?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many organizations will want to use AI research tools with their own internal
    data. The STORM authors have done a nice job of documenting various approaches
    of using STORM with different LLM providers and a local vector database, which
    means it is possible to run STORM on your own documents.
  prefs: []
  type: TYPE_NORMAL
- en: So let‚Äôs try this out!
  prefs: []
  type: TYPE_NORMAL
- en: Setup and code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs),
    which includes environment setup instructions and how to collate some sample documents
    for this demo.
  prefs: []
  type: TYPE_NORMAL
- en: FEMA disaster preparedness and assistance documentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use 34 PDF documents to help people prepare for and respond to disasters,
    as created by the United States Federal Emergency Management Agency ([FEMA](https://www.fema.gov)).
    These documents perhaps aren‚Äôt typically what people may want to use for writing
    deep research articles, but I‚Äôm interested in seeing how AI can help people prepare
    for disasters.
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶. and I have the code already written for processing FEMA reports from some
    earlier blog posts, which I‚Äôve included in the linked repo above. üòä
  prefs: []
  type: TYPE_NORMAL
- en: Parsing and Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have our documents, we need to split them into smaller documents so
    that STORM can search for specific topics within the corpus. Given STORM is originally
    aimed at generating Wikipedia-style articles, I opted to try two approaches, (i)
    Simply splitting the documents into sub-documents by page using [LangChain‚Äôs PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/),
    to create a crude simulation of a Wikipedia page which includes several sub-topics.
    Many FEMA PDFs are single-page documents that don‚Äôt look too dissimilar to Wikipedia
    articles; (ii) Further chunking the documents into smaller sections, more likely
    to cover a discrete sub-topic.
  prefs: []
  type: TYPE_NORMAL
- en: These are of course *very* basic approaches to parsing, but I wanted to see
    how results varied depending on the two techniques. Any serious use of STORM on
    local documents should invest in all the usual fun around paring optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Metadata enrichment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[STORM‚Äôs example documentation](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    requires that documents have metadata fields ‚ÄòURL‚Äô, ‚Äòtitle‚Äô, and ‚Äòdescription‚Äô,
    where ‚ÄòURL‚Äô should be unique. Since we are splitting up PDF documents, we don‚Äôt
    have titles and descriptions of individual pages and chunks, so I opted to generate
    these with a simple LLM call.'
  prefs: []
  type: TYPE_NORMAL
- en: For URLs, we have them for individual PDF pages, but for chunks within a page.
    Sophisticated knowledge retrieval systems can have metadata generated by layout
    detection models so the text chunk area can be highlighted in the corresponding
    PDF, but for this demo, I simply added an ‚Äò_id‚Äô query parameter the URL which
    does nothing but ensure they are unique for chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Building vector databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: STORM already supports the [Qdrant vector store](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq).
    I like to use frameworks such as LangChain and Llama Index where possible to make
    it easier to change providers down the road, so I opted to use LangChain to build
    a [local Qdrant vector database persisted to the local file system](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)
    rather than STORM‚Äôs automatic vector database management. I felt this offers more
    control and is more recognizable to those who already have pipelines for populating
    document vector stores.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Running STORM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The STORM repo has [some great examples](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    of different search engines and LLMs, as well as using a Qdrant vector store.
    I decided to combine various features from these, plus some extra post-processing
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Added ability to run with OpenAI or Ollama
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added support for passing in the vector database directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Added a function to parse the references metadata file to add references to
    the generated polished article. STORM generated these references in a JSON file
    but didn‚Äôt add them to the output article automatically. I‚Äôm not sure if this
    was due to some setting I missed, but references are key to evaluating any AI
    research technique, so I added this custom post-processing step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, I noticed that open models have more guidance in templates and personas
    due to their following instructions less accurately than commercial models. I
    liked the transparency of these controls and left them in for OpenAI so that I
    could adjust in future work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is everything (see [repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)
    for full code) ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôre ready to run STORM!
  prefs: []
  type: TYPE_NORMAL
- en: For the research topic, I picked something that would be challenging to answer
    with a typical RAG system and which wasn‚Äôt well covered in the PDF data so we
    can see how well attribution works ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äú***Compare the financial impact of different types of disasters and how those
    impact communities***‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: Running this for both databases ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Using OpenAI, the process took about 6 minutes on my Macbook pro M2 (16GB memory).
    I would note that other simpler queries where we have more supporting content
    in the underlying documents were much faster (< 30 seconds in some cases).
  prefs: []
  type: TYPE_NORMAL
- en: STORM results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: STORM generates a set of output files ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9352963a2332997ba080d259b2f9151b.png)'
  prefs: []
  type: TYPE_IMG
- en: Files generated by STORM, from which one markdown file was created combining
    the polished article with reference footnotes.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs interesting to review the **conversation_log.json** and **llm_call_history.json**
    to see the perspective-guided conversations component.
  prefs: []
  type: TYPE_NORMAL
- en: For our research topic ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: ‚Äú***Compare the financial impact of different types of disasters and how those
    impact communities***‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: You can find the generated articles here ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '[STORM generated article ‚Äî using text split by page](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[STORM generated article ‚Äî using text further chunked using RecursiveTextSplitter](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Some quick observations**'
  prefs: []
  type: TYPE_NORMAL
- en: This demo doesn‚Äôt get into a formal evaluation ‚Äî which can be [more involved
    than single-hop RAG systems](https://arxiv.org/abs/2401.15391) ‚Äî but here are
    some subjective observations that may or may not be useful ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: Parsing by page or by smaller chunks produces reasonable pre-reading reports
    that a human could use for researching areas related to the financial impact of
    disasters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both paring approaches provided citations throughout, but using smaller chunks
    seemed to result in fewer. See for example the Summary sections in both of the
    above articles. The more references to ground the analysis, the better!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Parsing by smaller chunks seemed to sometimes create citations that were not
    relevant, one of the citation challenges mentioned in the STORM paper. See for
    example citation for source ‚Äò10‚Äô in the [summary section](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md),
    which doesn‚Äôt correspond with the reference sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overall, as expected for an algorithm developed on Wiki articles, splitting
    text by PDF seemed to produce a [more cohesive and grounded article](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)
    (to me!)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though the input research topic wasn‚Äôt covered in great depth in the underlying
    documents, the generated report was a great starting point for further human analysis
  prefs: []
  type: TYPE_NORMAL
- en: Future Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We didn‚Äôt get into [Co-Storm](https://www.arxiv.org/abs/2408.15232) in this
    article, which brings a human into the loop. This seems a great direction for
    AI-empowered research and something I am investigating.
  prefs: []
  type: TYPE_NORMAL
- en: Future work could also look at adjusting the system prompts and personas to
    the business case. Currently, those prompts are targeted for a Wikipedia-like
    process ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41fbcc46e187420f6725280d4e787d88.png)'
  prefs: []
  type: TYPE_IMG
- en: STORM system prompts, illustrating the emphasis on creating Wikipedia-style
    articles. [Source](https://arxiv.org/abs/2402.14207)
  prefs: []
  type: TYPE_NORMAL
- en: Another possible direction is to extend STORM‚Äôs connectors beyond Qdrant, for
    example, to include other vector stores, or better still, generic support for
    Langchain and llama index vector stores. The authors encourage this type of thing,
    a PR involving [this file](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)
    may be in my future.
  prefs: []
  type: TYPE_NORMAL
- en: Running STORM without an internet connection would be an amazing thing, as it
    opens up possibilities for AI assistance in the field. As you can see from the
    demo code, I added the ability to run STORM with Ollama locally hosted models,
    but the token throughput rate was too low for the LLM agent discussion phase,
    so the system didn‚Äôt complete on my laptop with small quantized models. A topic
    for a future blog post perhaps!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, though the [online User Interface is very nice](https://storm.genie.stanford.edu),
    the [demo UI that comes with the repo](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)
    is very basic and not something that could be used in production. Perhaps the
    Standford team might release the advanced interface ‚Äî maybe it is already somewhere?
    ‚Äî if not then work would be needed here.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a quick demo to hopefully help people get started with using STORM on
    their own documents. I haven‚Äôt gone into systematic evaluation, something that
    would obviously need to be done if using STORM in a live environment. That said,
    I was impressed at how it seems to be able to get a relatively nuanced research
    topic and generate well-cited pre-writing research content that would help me
    in my own research.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401),
    Lewis et al., 2020'
  prefs: []
  type: TYPE_NORMAL
- en: '[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/html/2312.10997v5#S2),
    Yunfan et al., 2024'
  prefs: []
  type: TYPE_NORMAL
- en: '[Assisting in Writing Wikipedia-like Articles From Scratch with Large Language
    Models](https://arxiv.org/abs/2402.14207), Shao et al., 2024'
  prefs: []
  type: TYPE_NORMAL
- en: '[Into the Unknown Unknowns: Engaged Human Learning through Participation in
    Language Model Agent Conversations](https://www.arxiv.org/abs/2408.15232), Jiang
    et al., 2024'
  prefs: []
  type: TYPE_NORMAL
- en: '[MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries](https://arxiv.org/abs/2401.15391),
    Tang et al., 2024'
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs)
  prefs: []
  type: TYPE_NORMAL
- en: '***Please like this article if inclined and I‚Äôd be super delighted if you followed
    me! You can find more articles*** [***here***](/@astrobagel) ***or connect on***
    [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/)***.***'
  prefs: []
  type: TYPE_NORMAL
