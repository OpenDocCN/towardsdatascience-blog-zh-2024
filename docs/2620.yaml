- en: Running the STORM AI Research System with Your Local Documents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用本地文档运行STORM AI研究系统
- en: 原文：[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)
- en: AI assisted research using FEMA disaster response documents
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AI辅助研究FEMA灾难响应文档
- en: '[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew
    Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    ·16 min read·Oct 28, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)
    ·16分钟阅读·2024年10月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)'
- en: STORM researches the topic via perspective-guided question asking in simulated
    conversations. [Source](https://arxiv.org/abs/2402.14207)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: STORM通过视角引导的问题提问在模拟对话中研究主题。[来源](https://arxiv.org/abs/2402.14207)
- en: TL;DR
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR
- en: '*The use of LLM agents is becoming more common for tackling multi-step long-context
    research tasks where traditional RAG direct prompting methods can sometimes struggle.
    In this article, we will explore a new and promising technique developed by Stanford
    called* ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval
    and* ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*),
    which uses LLM agents to simulate ‘Perspective-guided conversations’ to reach
    complex research goals and generate rich research articles that can be used by
    humans in their pre-writing research. STORM was initially developed to gather
    information from web sources but also supports searching a local document vector
    store. In this article we will see how to implement STORM for AI-supported research
    on local PDFs, using US FEMA disaster preparedness and assistance documentation.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*LLM代理的使用在处理多步骤、长上下文的研究任务时变得越来越普遍，因为传统的RAG直接提示方法有时会遇到困难。在本文中，我们将探讨由斯坦福大学开发的一种新型且有前景的技术*
    ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval and*
    ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*),
    它使用LLM代理模拟“视角引导的对话”以实现复杂的研究目标，并生成丰富的研究文章，供人们在写作前研究使用。STORM最初是为了从网络资源中收集信息而开发的，但也支持搜索本地文档向量存储。在本文中，我们将展示如何实现STORM以支持基于AI的本地PDF研究，使用美国FEMA灾难准备和援助文档。*'
- en: It’s been amazing to watch how using LLMs for knowledge retrieval has progressed
    in a relatively short period of time. Since the [first paper on Retrieval Augmented
    Generation (RAG)](https://arxiv.org/abs/2005.11401) in 2020, we have seen the
    ecosystem grow to include a [cornucopia of available technique](https://arxiv.org/html/2312.10997v5#S2)s.
    One of the more advanced is agentic RAG where LLM agents iterate and refine document
    retrieval in order to solve more complex research tasks. It’s similar to how a
    human might carry out research, exploring a range of different search queries
    to build a better idea of the context, sometimes discussing the topic with other
    humans, and synthesizing everything into a final result. Single-turn RAG, even
    employing techniques such as query expansion and reranking, can struggle with
    more complex multi-hop research tasks like this.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 看到利用大语言模型（LLM）进行知识检索在相对较短的时间内取得的进展，真是令人惊叹。自从2020年发布的[首次关于检索增强生成（RAG）论文](https://arxiv.org/abs/2005.11401)以来，我们见证了这个生态系统的发展，现已涵盖了[一系列可用的技术](https://arxiv.org/html/2312.10997v5#S2)。其中更先进的技术之一是代理RAG，LLM代理通过迭代和优化文档检索来解决更复杂的研究任务。这类似于人类进行研究的方式，通过探索多种不同的搜索查询来建立更清晰的背景，有时还会与其他人讨论这一主题，并将所有信息整合成最终的结果。而单回合RAG，即便采用了查询扩展和重新排序等技术，在应对像这样的复杂多跳研究任务时，仍然存在困难。
- en: There are quite a few patterns for knowledge retrieval using agent frameworks
    such as [Autogen](https://microsoft.github.io/autogen/0.2/), [CrewAI](https://www.crewai.com),
    and [LangGraph](https://www.langchain.com/langgraph) as well as specific AI research
    assistants such as [GPT Researcher](https://github.com/assafelovic/gpt-researcher).
    In this article, we will look at an LLM-powered research writing system from Stanford
    University, called **S**ynthesis of **T**opic **O**utlines through **R**etrieval
    and **M**ulti-perspective Question Asking ([STORM](https://arxiv.org/abs/2402.14207)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用代理框架（如[Autogen](https://microsoft.github.io/autogen/0.2/)、[CrewAI](https://www.crewai.com)和[LangGraph](https://www.langchain.com/langgraph)）以及特定的AI研究助手（如[GPT
    Researcher](https://github.com/assafelovic/gpt-researcher)）进行知识检索的模式有很多。在本文中，我们将探讨斯坦福大学开发的一个由LLM驱动的研究写作系统，名为**S**ynthesis
    of **T**opic **O**utlines through **R**etrieval and **M**ulti-perspective Question
    Asking（[STORM](https://arxiv.org/abs/2402.14207)）。
- en: STORM AI research writing system
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: STORM AI研究写作系统
- en: STORM applies a clever technique where LLM agents simulate ‘Perspective-guided
    conversations’ to reach a research goal as well as extend ‘outline-driven RAG’
    for richer article generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: STORM应用了一种巧妙的技术，其中LLM代理模拟“视角引导对话”以达成研究目标，并扩展了“基于大纲的RAG”，以生成更丰富的文章内容。
- en: Configured to generate Wikipedia-style articles, it was tested with a cohort
    of 10 experienced Wikipedia editors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统配置为生成类似维基百科风格的文章，并在一组10名经验丰富的维基百科编辑者中进行了测试。
- en: '![](../Images/0e52360332e7847eb9aa2237a73652ac.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e52360332e7847eb9aa2237a73652ac.png)'
- en: Survey results of 10 experienced Wikipedia Editors on the perceived usefulness
    of STORM. [Source](https://arxiv.org/abs/2402.14207).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 10名经验丰富的维基百科编辑者对STORM在实际使用中的感知有用性的调查结果。[来源](https://arxiv.org/abs/2402.14207)。
- en: Reception on the whole was positive, 70% of the editors felt that it would be
    a useful tool in their *pre-writing* stage when researching a topic. I hope in
    the future surveys could include more than 10 editors, but it should be noted
    that authors also benchmarked traditional article generation methods using FreshWiki,
    a dataset of recent high-quality Wikipedia articles, where STORM was found to
    outperform previous techniques.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总体反响积极，70%的编辑认为该工具在他们的*写作前*阶段研究一个主题时会非常有用。希望未来的调查能够包括超过10名编辑，但需要注意的是，作者也通过使用FreshWiki（一组近期高质量的维基百科文章数据集）对传统文章生成方法进行了基准测试，其中STORM被发现优于以往的方法。
- en: '![](../Images/4076dab0bb937a746d51e0eb93b03985.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4076dab0bb937a746d51e0eb93b03985.png)'
- en: Human evaluation by 10 experienced Wikipedia editors for on 20 pairs of articles
    generated by STORM and *oRAG*. Each pair of articles is evaluated by two Wikipedia
    editors. [Source](https://arxiv.org/abs/2402.14207).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由10名经验丰富的维基百科编辑者对STORM和*oRAG*生成的20对文章进行人工评估。每对文章由两名维基百科编辑者评估。[来源](https://arxiv.org/abs/2402.14207)。
- en: STORM is [open source](https://github.com/stanford-oval/storm/tree/main) and
    available as a [Python package](https://pypi.org/project/knowledge-storm/) with
    additional implementations using frameworks such as [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/).
    More recently STORM has been enhanced to support human-AI collaborative knowledge
    curation called [Co-STORM](https://www.arxiv.org/abs/2408.15232), putting a human
    right in the center of the AI-assisted research loop.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: STORM是[开源的](https://github.com/stanford-oval/storm/tree/main)，并作为一个[Python包](https://pypi.org/project/knowledge-storm/)提供，另外还支持使用如[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/)等框架的实现。最近，STORM已被增强以支持一种名为[Co-STORM](https://www.arxiv.org/abs/2408.15232)的人类-AI协作知识策划方法，将人类置于AI辅助研究环路的中心。
- en: Though it significantly outperforms baseline methods in both automatic and human
    evaluations, there are some caveats that the authors acknowledge. It isn’t yet
    multimodal, doesn’t produce experienced human-quality content — it isn’t positioned
    yet for this I feel, being more targeted for *pre-writing* research than final
    articles — and there are some nuances around references that require some future
    work. That said, if you have a deep research task, it’s worth checking out.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它在自动化和人工评估中都显著优于基准方法，但作者也承认存在一些局限性。目前它还不是多模态的，生成的内容也未达到经验丰富的人工质量——我觉得它目前还不适合这一点，更多的是针对*写作前*的研究，而非最终文章——另外，参考文献方面也存在一些需要进一步改进的细节。话虽如此，如果你有一个深入的研究任务，值得一试。
- en: You can try out STORM [online](https://storm.genie.stanford.edu/) — it’s fun!
    — configured to perform research using information on the web.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[网上](https://storm.genie.stanford.edu/)试用STORM——它非常有趣！——并配置为使用网络信息进行研究。
- en: '**But what about running STORM with your own data?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**但如果用自己的数据运行STORM呢？**'
- en: Many organizations will want to use AI research tools with their own internal
    data. The STORM authors have done a nice job of documenting various approaches
    of using STORM with different LLM providers and a local vector database, which
    means it is possible to run STORM on your own documents.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 许多组织希望将AI研究工具与他们自己的内部数据结合使用。STORM的作者们做得很好，记录了如何将STORM与不同的LLM提供者以及本地向量数据库结合使用，这意味着你可以在自己的文档上运行STORM。
- en: So let’s try this out!
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来试试吧！
- en: Setup and code
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置与代码
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs),
    which includes environment setup instructions and how to collate some sample documents
    for this demo.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/dividor/storm-with-local-docs)找到本文的代码，包含了环境设置说明以及如何收集一些示例文档来进行演示。
- en: FEMA disaster preparedness and assistance documentation
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FEMA灾难准备和援助文档
- en: We will use 34 PDF documents to help people prepare for and respond to disasters,
    as created by the United States Federal Emergency Management Agency ([FEMA](https://www.fema.gov)).
    These documents perhaps aren’t typically what people may want to use for writing
    deep research articles, but I’m interested in seeing how AI can help people prepare
    for disasters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用34份由美国联邦应急管理局（[FEMA](https://www.fema.gov)）创建的PDF文档，帮助人们为灾难做准备并进行响应。这些文档可能通常不是人们用来撰写深入研究文章的内容，但我很想看看AI如何帮助人们为灾难做好准备。
- en: …. and I have the code already written for processing FEMA reports from some
    earlier blog posts, which I’ve included in the linked repo above. 😊
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ……我已经编写了处理FEMA报告的代码，这些代码来自之前的一些博客文章，已经包含在上面链接的代码库中。😊
- en: Parsing and Chunking
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解析与切分
- en: Once we have our documents, we need to split them into smaller documents so
    that STORM can search for specific topics within the corpus. Given STORM is originally
    aimed at generating Wikipedia-style articles, I opted to try two approaches, (i)
    Simply splitting the documents into sub-documents by page using [LangChain’s PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/),
    to create a crude simulation of a Wikipedia page which includes several sub-topics.
    Many FEMA PDFs are single-page documents that don’t look too dissimilar to Wikipedia
    articles; (ii) Further chunking the documents into smaller sections, more likely
    to cover a discrete sub-topic.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了文档，就需要将其拆分成更小的文档，以便STORM能够在语料库中查找特定的主题。由于STORM最初旨在生成类似维基百科的文章，我选择尝试两种方法：（i）简单地通过[LangChain的PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)按页将文档拆分为子文档，从而粗略模拟一个包含多个子主题的维基百科页面。许多FEMA的PDF是单页文档，看起来与维基百科文章相差不大；（ii）进一步将文档切分为更小的部分，更可能涵盖一个离散的子主题。
- en: These are of course *very* basic approaches to parsing, but I wanted to see
    how results varied depending on the two techniques. Any serious use of STORM on
    local documents should invest in all the usual fun around paring optimization.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这些当然是*非常*基础的解析方法，但我想看看这两种技术在结果上的差异。任何对STORM在本地文档中的严肃使用都应该投入所有常见的配对优化工作。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Metadata enrichment
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元数据增强
- en: '[STORM’s example documentation](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    requires that documents have metadata fields ‘URL’, ‘title’, and ‘description’,
    where ‘URL’ should be unique. Since we are splitting up PDF documents, we don’t
    have titles and descriptions of individual pages and chunks, so I opted to generate
    these with a simple LLM call.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[STORM的示例文档](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)要求文档具有元数据字段‘URL’，‘title’，和‘description’，其中‘URL’应该是唯一的。由于我们正在拆分PDF文档，因此没有单独页面和块的标题和描述，因此我选择使用简单的LLM调用来生成这些内容。'
- en: For URLs, we have them for individual PDF pages, but for chunks within a page.
    Sophisticated knowledge retrieval systems can have metadata generated by layout
    detection models so the text chunk area can be highlighted in the corresponding
    PDF, but for this demo, I simply added an ‘_id’ query parameter the URL which
    does nothing but ensure they are unique for chunks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于URLs，我们有每个PDF页面的链接，但对于页面内的块，复杂的知识检索系统可以通过布局检测模型生成元数据，这样文本块区域就可以在相应的PDF中高亮显示，但对于这个演示，我只是简单地在URL中添加了一个‘_id’查询参数，这个参数不会做任何事情，只是确保它们对于不同的块是唯一的。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Building vector databases
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建向量数据库
- en: STORM already supports the [Qdrant vector store](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq).
    I like to use frameworks such as LangChain and Llama Index where possible to make
    it easier to change providers down the road, so I opted to use LangChain to build
    a [local Qdrant vector database persisted to the local file system](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)
    rather than STORM’s automatic vector database management. I felt this offers more
    control and is more recognizable to those who already have pipelines for populating
    document vector stores.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: STORM已经支持[Qdrant向量存储](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq)。我喜欢在可能的情况下使用像LangChain和Llama
    Index这样的框架，这样可以在将来更容易地更换提供商，因此我选择使用LangChain构建一个[持久化到本地文件系统的本地Qdrant向量数据库](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)，而不是STORM的自动向量数据库管理。我认为这提供了更多的控制，并且对于那些已经有填充文档向量存储的管道的人来说更具可识别性。
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Running STORM
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行STORM
- en: 'The STORM repo has [some great examples](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)
    of different search engines and LLMs, as well as using a Qdrant vector store.
    I decided to combine various features from these, plus some extra post-processing
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: STORM的代码库有[一些很棒的示例](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)，包括不同的搜索引擎和LLM的使用，以及如何使用Qdrant向量存储。我决定结合这些示例中的各种功能，并添加一些额外的后处理，如下所示：
- en: Added ability to run with OpenAI or Ollama
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加了与OpenAI或Ollama一起运行的功能
- en: Added support for passing in the vector database directory
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加了支持传入向量数据库目录的功能
- en: Added a function to parse the references metadata file to add references to
    the generated polished article. STORM generated these references in a JSON file
    but didn’t add them to the output article automatically. I’m not sure if this
    was due to some setting I missed, but references are key to evaluating any AI
    research technique, so I added this custom post-processing step.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加了一个功能来解析参考文献元数据文件，将参考文献添加到生成的精炼文章中。STORM将这些参考文献生成在一个JSON文件中，但没有自动将它们添加到输出文章中。我不确定这是不是由于我错过了某个设置，但参考文献对于评估任何AI研究技术都至关重要，因此我增加了这个自定义后处理步骤。
- en: Finally, I noticed that open models have more guidance in templates and personas
    due to their following instructions less accurately than commercial models. I
    liked the transparency of these controls and left them in for OpenAI so that I
    could adjust in future work.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我注意到开放模型在模板和角色设定方面提供了更多的指导，因为它们执行指令的准确度不如商业模型。我喜欢这些控制的透明性，并将其保留在OpenAI中，以便将来在工作中进行调整。
- en: Here is everything (see [repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)
    for full code) …
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是所有内容（查看[repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)以获取完整代码）…
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’re ready to run STORM!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好运行STORM了！
- en: For the research topic, I picked something that would be challenging to answer
    with a typical RAG system and which wasn’t well covered in the PDF data so we
    can see how well attribution works …
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于研究课题，我选择了一个典型的RAG系统难以回答的主题，且在PDF数据中没有很好覆盖的内容，这样我们可以看到归因效果如何……
- en: “***Compare the financial impact of different types of disasters and how those
    impact communities***”
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: “***比较不同类型灾害的财务影响及其对社区的影响***”
- en: Running this for both databases …
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个数据库上运行此操作……
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using OpenAI, the process took about 6 minutes on my Macbook pro M2 (16GB memory).
    I would note that other simpler queries where we have more supporting content
    in the underlying documents were much faster (< 30 seconds in some cases).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI时，这个过程在我的Macbook Pro M2（16GB内存）上大约花费了6分钟。我需要指出的是，其他一些简单查询，尤其是底层文档中有更多支持内容的查询，要快得多（有些情况下不到30秒）。
- en: STORM results
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: STORM结果
- en: STORM generates a set of output files …
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: STORM生成了一组输出文件……
- en: '![](../Images/9352963a2332997ba080d259b2f9151b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9352963a2332997ba080d259b2f9151b.png)'
- en: Files generated by STORM, from which one markdown file was created combining
    the polished article with reference footnotes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由STORM生成的文件，其中一个Markdown文件结合了润色后的文章和参考脚注。
- en: It’s interesting to review the **conversation_log.json** and **llm_call_history.json**
    to see the perspective-guided conversations component.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 审视**conversation_log.json**和**llm_call_history.json**文件，查看以视角引导的对话组件，十分有趣。
- en: For our research topic …
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的研究课题……
- en: “***Compare the financial impact of different types of disasters and how those
    impact communities***”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: “***比较不同类型灾害的财务影响及其对社区的影响***”
- en: You can find the generated articles here …
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到生成的文章……
- en: '[STORM generated article — using text split by page](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[STORM生成的文章 — 使用按页面拆分的文本](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
- en: '[STORM generated article — using text further chunked using RecursiveTextSplitter](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[STORM生成的文章 — 使用递归文本拆分器进一步拆分的文本](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)'
- en: '**Some quick observations**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些快速观察**'
- en: This demo doesn’t get into a formal evaluation — which can be [more involved
    than single-hop RAG systems](https://arxiv.org/abs/2401.15391) — but here are
    some subjective observations that may or may not be useful …
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示没有进行正式的评估 — 这可能比单跳RAG系统[更为复杂](https://arxiv.org/abs/2401.15391) — 但这里有一些可能有用的主观观察……
- en: Parsing by page or by smaller chunks produces reasonable pre-reading reports
    that a human could use for researching areas related to the financial impact of
    disasters
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按页面或较小块拆分的解析会生成合理的预读报告，人类可以用来研究与灾害财务影响相关的领域
- en: Both paring approaches provided citations throughout, but using smaller chunks
    seemed to result in fewer. See for example the Summary sections in both of the
    above articles. The more references to ground the analysis, the better!
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两种配对方法都在整个过程中提供了引用，但使用更小的文本块似乎产生了更少的引用。请参见上述两篇文章中的总结部分。更多的引用可以为分析提供更坚实的基础！
- en: Parsing by smaller chunks seemed to sometimes create citations that were not
    relevant, one of the citation challenges mentioned in the STORM paper. See for
    example citation for source ‘10’ in the [summary section](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md),
    which doesn’t correspond with the reference sentence.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按较小块拆分的解析有时会产生不相关的引用，这是STORM论文中提到的引用问题之一。请参见[总结部分](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)中源‘10’的引用，该引用与参考句子不符。
- en: Overall, as expected for an algorithm developed on Wiki articles, splitting
    text by PDF seemed to produce a [more cohesive and grounded article](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)
    (to me!)
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体来说，正如我预期的那样，基于 Wiki 文章开发的算法，按 PDF 分割文本似乎能生成一篇[更具凝聚力和基础性的文章](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)（对我来说！）
- en: Even though the input research topic wasn’t covered in great depth in the underlying
    documents, the generated report was a great starting point for further human analysis
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管输入的研究主题在基础文档中并没有被深入探讨，但生成的报告为进一步的人类分析提供了一个很好的起点。
- en: Future Work
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的工作
- en: We didn’t get into [Co-Storm](https://www.arxiv.org/abs/2408.15232) in this
    article, which brings a human into the loop. This seems a great direction for
    AI-empowered research and something I am investigating.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中没有讨论[Co-Storm](https://www.arxiv.org/abs/2408.15232)，它将人类引入过程。这似乎是一个非常适合
    AI 驱动研究的方向，我正在研究这个方向。
- en: Future work could also look at adjusting the system prompts and personas to
    the business case. Currently, those prompts are targeted for a Wikipedia-like
    process …
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的工作还可以考虑将系统提示和人物角色调整到具体的商业案例中。目前，这些提示是为类似维基百科的过程设计的……
- en: '![](../Images/41fbcc46e187420f6725280d4e787d88.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41fbcc46e187420f6725280d4e787d88.png)'
- en: STORM system prompts, illustrating the emphasis on creating Wikipedia-style
    articles. [Source](https://arxiv.org/abs/2402.14207)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: STORM 系统提示，展示了如何强调创建维基百科风格的文章。[来源](https://arxiv.org/abs/2402.14207)
- en: Another possible direction is to extend STORM’s connectors beyond Qdrant, for
    example, to include other vector stores, or better still, generic support for
    Langchain and llama index vector stores. The authors encourage this type of thing,
    a PR involving [this file](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)
    may be in my future.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的方向是将 STORM 的连接器扩展到 Qdrant 以外的系统，例如，支持其他向量存储，或者更好的是，支持 Langchain 和 Llama
    index 向量存储的通用支持。作者鼓励这一类的工作，涉及[这个文件](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)的
    PR 可能是我未来的工作方向。
- en: Running STORM without an internet connection would be an amazing thing, as it
    opens up possibilities for AI assistance in the field. As you can see from the
    demo code, I added the ability to run STORM with Ollama locally hosted models,
    but the token throughput rate was too low for the LLM agent discussion phase,
    so the system didn’t complete on my laptop with small quantized models. A topic
    for a future blog post perhaps!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有互联网连接的情况下运行 STORM 将是一件令人兴奋的事情，因为它为现场的 AI 助手开辟了新的可能性。正如从演示代码中可以看到的，我增加了运行
    STORM 的能力，使用 Ollama 本地托管的模型，但由于令牌吞吐率过低，LLM 代理讨论阶段没有完成，因此系统在我使用小型量化模型的笔记本电脑上没有完成任务。这或许是未来博客文章的一个话题！
- en: Finally, though the [online User Interface is very nice](https://storm.genie.stanford.edu),
    the [demo UI that comes with the repo](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)
    is very basic and not something that could be used in production. Perhaps the
    Standford team might release the advanced interface — maybe it is already somewhere?
    — if not then work would be needed here.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，尽管[在线用户界面非常友好](https://storm.genie.stanford.edu)，但[随代码仓库附带的演示界面](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)非常基础，无法用于生产环境。也许斯坦福团队将发布更先进的界面——可能已经有了？——如果没有的话，仍然需要在这方面进行改进。
- en: Conclusions
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This is a quick demo to hopefully help people get started with using STORM on
    their own documents. I haven’t gone into systematic evaluation, something that
    would obviously need to be done if using STORM in a live environment. That said,
    I was impressed at how it seems to be able to get a relatively nuanced research
    topic and generate well-cited pre-writing research content that would help me
    in my own research.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简短的演示，旨在帮助人们开始使用 STORM 处理自己的文档。我没有进行系统的评估，如果在实际环境中使用 STORM，显然需要进行这项工作。尽管如此，我还是对
    STORM 能够生成相对细致的研究主题和生成有良好引用的预写作研究内容感到印象深刻，这将帮助我进行自己的研究。
- en: References
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401),
    Lewis et al., 2020'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[知识密集型 NLP 任务的检索增强生成](https://arxiv.org/abs/2005.11401)，Lewis 等人，2020年'
- en: '[Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/html/2312.10997v5#S2),
    Yunfan et al., 2024'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[大规模语言模型的检索增强生成：综述](https://arxiv.org/html/2312.10997v5#S2)，Yunfan 等人，2024年'
- en: '[Assisting in Writing Wikipedia-like Articles From Scratch with Large Language
    Models](https://arxiv.org/abs/2402.14207), Shao et al., 2024'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[从零开始利用大型语言模型协助撰写类似 Wikipedia 的文章](https://arxiv.org/abs/2402.14207)，Shao 等，2024'
- en: '[Into the Unknown Unknowns: Engaged Human Learning through Participation in
    Language Model Agent Conversations](https://www.arxiv.org/abs/2408.15232), Jiang
    et al., 2024'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[进入未知的未知：通过参与语言模型代理对话进行积极的人类学习](https://www.arxiv.org/abs/2408.15232)，Jiang
    等，2024'
- en: '[MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries](https://arxiv.org/abs/2401.15391),
    Tang et al., 2024'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiHop-RAG：多跳查询的检索增强生成基准测试](https://arxiv.org/abs/2401.15391)，Tang 等，2024'
- en: You can find the code for this article [here](https://github.com/dividor/storm-with-local-docs)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/dividor/storm-with-local-docs)找到本文的代码
- en: '***Please like this article if inclined and I’d be super delighted if you followed
    me! You can find more articles*** [***here***](/@astrobagel) ***or connect on***
    [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/)***.***'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果喜欢这篇文章，请点赞，如果你能关注我，我将非常高兴！你可以在*** [***这里***](/@astrobagel) ***找到更多文章***，***或者在***
    [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/) ***上联系我。***'
