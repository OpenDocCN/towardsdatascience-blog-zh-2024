<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Build a Generative Search Engine for Your Local Files Using Llama 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Build a Generative Search Engine for Your Local Files Using Llama 3</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08">https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cf0d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Use Qdrant, NVidia NIM API, or Llama 3 8B locally for your local GenAI assistant</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nikola Milosevic (Data Warrior)" class="l ep by dd de cx" src="../Images/ebea6501c00030561a59a4a12ab7a79a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*XSmifXlC__XRUMiB.jpg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------" rel="noopener follow">Nikola Milosevic (Data Warrior)</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="a103" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">On the 23rd of May, I received an email from a person at Nvidia inviting me to the <a class="af nf" href="https://www.nvidia.com/en-us/ai-data-science/generative-ai/developer-contest-with-langchain/?ncid=em-anno-571922&amp;DkwibgoBGQo58Ndn2XA_QZk4Oek6oAyhMHrbuo7f2iF8fmTr0phnYJSKrENRDiGu3MOeEry08HydZtz_EiC0eg=&amp;mkt_tok=MTU2LU9GTi03NDIAAAGTQxOVSz583h1Gr6TvRfnNz4TJLyss1TypLIotdWccXzlkMpJ8mAtsKiyWooZ5pnhPM3ALyJdxJh6gpak9YASo8xEIOWv-5FZaaptj4FmiBLsaCVMdI5w" rel="noopener ugc nofollow" target="_blank">Generative AI Agents Developer Contest by NVIDIA and LangChain</a>. My first thought was that it is quite a little time, and given we had a baby recently and my parents were supposed to come, I would not have time to participate. But then second thoughts came, and I decided that I could code something and submit it. I thought about what I could make for a few days, and one idea stuck with me — an Open-Source Generative Search Engine that lets you interact with local files. Microsoft Copilot already provides something like this, but I thought I could make an open-source version, for fun, and share a bit of learnings that I gathered during the quick coding of the system.</p><h1 id="4ef2" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">System Design</h1><p id="6e06" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">In order to build a local generative search engine or assistant, we would need several components:</p><ul class=""><li id="1059" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oh oi oj bk">An index with the content of the local files, with an information retrieval engine to retrieve the most relevant documents for a given query/question.</li><li id="9cf4" class="mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne oh oi oj bk">A language model to use selected content from local documents and generate a summarized answer</li><li id="c6fb" class="mj mk fq ml b go ok mn mo gr ol mq mr ms om mu mv mw on my mz na oo nc nd ne oh oi oj bk">A user interface</li></ul><p id="270e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How the components interact is presented in a diagram below.</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq or"><img src="../Images/93ac4331f5c89da60084b967ac949115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxfHfm8T1uSNlpHD6KJoAA.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">System design and architecture. Qdrant is used for vector store, while Streamlit is for the user interface. Llama 3 is either used via Nvidia NIM API (70B version) or is downloaded via HuggingFace (8B version). Document chunking is done using Langchain. Image by author</figcaption></figure><p id="9027" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we need to index our local files into the index that can be queried for the content of the local files. Then, when the user asks a question, we would use the created index, with some of the asymmetric paragraph or document embeddings to retrieve the most relevant documents that may contain the answer. The content of these documents and the question are passed to the deployed large language model, which would use the content of given documents to generate answers. In the instruction prompt, we would ask a large language model to also return references to the used document. Ultimately, everything will be visualized to the user on the user interface.</p><p id="24c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, let’s have a look in more detail at each of the components.</p><h1 id="a16f" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Semantic Index</h1><p id="a032" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We are building a semantic index that will provide us with the most relevant documents based on the similarity of the file's content and a given query. To create such an index we will use Qdrant as a vector store. Interestingly, a <a class="af nf" href="https://github.com/qdrant/qdrant-client" rel="noopener ugc nofollow" target="_blank">Qdrant client library</a> does not require a full installation of <a class="af nf" href="https://qdrant.tech/" rel="noopener ugc nofollow" target="_blank">Qdrant server</a> and can do a similarity of documents that fit in working memory (RAM). Therefore, all we need to do is to pip install Qdrant client.</p><p id="4cf3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can initialize Qdrant in the following way (note that the hf parameter is later defined due to the story flow, but with Qdrant client you already need to define which vectorization method and metric is being used):</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="f74b" class="pm nh fq pj b bg pn po l pp pq">from qdrant_client import QdrantClient<br/>from qdrant_client.models import Distance, VectorParams<br/>client = QdrantClient(path="qdrant/")<br/>collection_name = "MyCollection"<br/>if client.collection_exists(collection_name):<br/>    client.delete_collection(collection_name)<br/><br/>client.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))<br/>qdrant = Qdrant(client, collection_name, hf)</span></pre><p id="46d7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In order to create a vector index, we will have to embed the documents on the hard drive. For embeddings, we will have to select the right embedding method and the right vector comparison metric. Several paragraph, sentence, or word embedding methods can be used, with varied results. The main issue with creating vector search, based on the documents, is the problem of asymmetric search. Asymmetric search problems are common to information retrieval and happen when one has short queries and long documents. Word or sentence embeddings are often fine-tuned to provide similarity scores based on documents of similar size (sentences, or paragraphs). Once that is not the case, the proper information retrieval may fail.</p><p id="5ddd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, we can find an embedding methodology that would work well on asymmetric search problems. For example, models fine-tuned on the MSMARCO dataset usually work well. MSMARCO dataset is based on Bing Search queries and documents and has been released by Microsoft. Therefore, it is ideal for the problem we are dealing with.</p><p id="5612" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For this particular implementation, I have selected an already fine-tuned model, called:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="f10a" class="pm nh fq pj b bg pn po l pp pq">sentence-transformers/msmarco-bert-base-dot-v5</span></pre><p id="9491" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This model is based on BERT and it was fine-tuned using dot product as a similarity metric. We have already initialized qdrant client to use dot product as a similarity metric in line (note this model has dimension of 768):</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="838e" class="pm nh fq pj b bg pn po l pp pq">client.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))</span></pre><p id="197f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We could use other metrics, such as cosine similarity, however, given this model is fine-tuned using dot product, we will get the best performance using this metric. On top of that, thinking geometrically: Cosine similarity focuses solely on the difference in angles, whereas the dot product takes into account both angle and magnitude. By normalizing data to have uniform magnitudes, the two measures become equivalent. In situations where ignoring magnitude is beneficial, cosine similarity is useful. However, the dot product is a more suitable similarity measure if the magnitude is significant.</p><p id="162f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The code for initializing the MSMarco model is (in case you have available GPU, use it. by all means):</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="c4ae" class="pm nh fq pj b bg pn po l pp pq">    model_name = "sentence-transformers/msmarco-bert-base-dot-v5"<br/>    model_kwargs = {'device': 'cpu'}<br/>    encode_kwargs = {'normalize_embeddings': True}<br/>    hf = HuggingFaceEmbeddings(<br/>        model_name=model_name,<br/>        model_kwargs=model_kwargs,<br/>        encode_kwargs=encode_kwargs<br/>    )</span></pre><p id="d7b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next problem: we need to deal with is that BERT-like models have limited context size, due to the quadratic memory requirements of transformer models. In the case of many BERT-like models, this context size is set to 512 tokens. There are two options: (1) we can base our answer only on the first 512 tokens and ignore the rest of the document, or (2) create an index, where one document will be split into multiple chunks and stored in the index as chunks. In the first case, we would lose a lot of important information, and therefore, we picked the second variant. To chunk documents, we can use a prebuilt chunker from LangChain:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="b5b9" class="pm nh fq pj b bg pn po l pp pq">from langchain_text_splitters import TokenTextSplitter<br/>text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)<br/>texts = text_splitter.split_text(file_content)<br/>metadata = []<br/>for i in range(0,len(texts)):<br/>    metadata.append({"path":file})<br/>qdrant.add_texts(texts,metadatas=metadata)</span></pre><p id="03af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the provided part of the code, we chunk text into the size of 500 tokens, with a window of 50 overlapping tokens. This way we keep a bit of context on the places where chunks end or begin. In the rest of the code, we create metadata with the document path on the user’s hard disk and add these chunks with metadata to the index.</p><p id="9a9e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, before we add the content of the files to the index, we need to read it. Even before we read files, we need to get all the files we need to index. For the sake of simplicity, in this project, the user can define a folder that he/she would like to index. The indexer retrieves all the files from that folder and its subfolder in a recursive manner and indexes files that are supported (we will look at how to support PDF, Word, PPT, and TXT).</p><p id="6a22" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can retrieve all the files in a given folder and its subfolder in a recursive way:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="ebed" class="pm nh fq pj b bg pn po l pp pq">def get_files(dir):<br/>    file_list = []<br/>    for f in listdir(dir):<br/>        if isfile(join(dir,f)):<br/>            file_list.append(join(dir,f))<br/>        elif isdir(join(dir,f)):<br/>            file_list= file_list + get_files(join(dir,f))<br/>    return file_list</span></pre><p id="4c4e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once all the files are retrieved in the list, we can read the content of files containing text. In this tool, for start, we will support MS Word documents (with extension “.docx”), PDF documents, MS PowerPoint presentations (with extension “.pptx”), and plain text files (with extension “.txt”).</p><p id="a142" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In order to read MS Word documents, we can use the docx-python library. The function reading documents into a string variable would look something like this:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="0d78" class="pm nh fq pj b bg pn po l pp pq">import docx<br/>def getTextFromWord(filename):<br/>    doc = docx.Document(filename)<br/>    fullText = []<br/>    for para in doc.paragraphs:<br/>        fullText.append(para.text)<br/>    return '\n'.join(fullText)</span></pre><p id="2a89" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A similar thing can be done with MS PowerPoint files. For this, we will need to download and install the pptx-python library and write a function like this:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="fcbc" class="pm nh fq pj b bg pn po l pp pq">from pptx import Presentation<br/>def getTextFromPPTX(filename):<br/>    prs = Presentation(filename)<br/>    fullText = []<br/>    for slide in prs.slides:<br/>        for shape in slide.shapes:<br/>            fullText.append(shape.text)<br/>    return '\n'.join(fullText)</span></pre><p id="a2a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Reading text files is pretty simple:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="d524" class="pm nh fq pj b bg pn po l pp pq">f = open(file,'r')<br/>file_content = f.read()<br/>f.close()</span></pre><p id="dede" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For PDF files we will in this case use the PyPDF2 library:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="6a3f" class="pm nh fq pj b bg pn po l pp pq">reader = PyPDF2.PdfReader(file)<br/>for i in range(0,len(reader.pages)):<br/>    file_content = file_content + " "+reader.pages[i].extract_text()</span></pre><p id="8755" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, the whole indexing function would look something like this:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="3e95" class="pm nh fq pj b bg pn po l pp pq">file_content = ""<br/>    for file in onlyfiles:<br/>        file_content = ""<br/>        if file.endswith(".pdf"):<br/>            print("indexing "+file)<br/>            reader = PyPDF2.PdfReader(file)<br/>            for i in range(0,len(reader.pages)):<br/>                file_content = file_content + " "+reader.pages[i].extract_text()<br/>        elif file.endswith(".txt"):<br/>            print("indexing " + file)<br/>            f = open(file,'r')<br/>            file_content = f.read()<br/>            f.close()<br/>        elif file.endswith(".docx"):<br/>            print("indexing " + file)<br/>            file_content = getTextFromWord(file)<br/>        elif file.endswith(".pptx"):<br/>            print("indexing " + file)<br/>            file_content = getTextFromPPTX(file)<br/>        else:<br/>            continue<br/>        text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)<br/>        texts = text_splitter.split_text(file_content)<br/>        metadata = []<br/>        for i in range(0,len(texts)):<br/>            metadata.append({"path":file})<br/>        qdrant.add_texts(texts,metadatas=metadata)<br/>    print(onlyfiles)<br/>    print("Finished indexing!")</span></pre><p id="490b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As we stated, we use TokenTextSplitter from LangChain to create chunks of 500 tokens with 50 token overlap. Now, when we have created an index, we can create a web service for querying it and generating answers.</p><h1 id="ead7" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Generative Search API</h1><p id="ffa9" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We will create a web service using FastAPI to host our generative search engine. The API will access the Qdrant client with the indexed data we created in the previous section, perform a search using a vector similarity metric, use the top chunks to generate an answer with the Llama 3 model, and finally provide the answer back to the user.</p><p id="d41b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In order to initialize and import libraries for the generative search component, we can use the following code:</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="60da" class="pm nh fq pj b bg pn po l pp pq">from fastapi import FastAPI<br/>from langchain_community.embeddings import HuggingFaceEmbeddings<br/>from langchain_qdrant import Qdrant<br/>from qdrant_client import QdrantClient<br/>from pydantic import BaseModel<br/>import torch<br/>from transformers import AutoTokenizer, AutoModelForCausalLM<br/>import environment_var<br/>import os<br/>from openai import OpenAI<br/><br/>class Item(BaseModel):<br/>    query: str<br/>    def __init__(self, query: str) -&gt; None:<br/>        super().__init__(query=query)</span></pre><p id="61fd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As previously mentioned, we are using FastAPI to create the API interface. We will utilize the qdrant_client library to access the indexed data we created and leverage the langchain_qdrant library for additional support. For embeddings and loading Llama 3 models locally, we will use the PyTorch and Transformers libraries. Additionally, we will make calls to the NVIDIA NIM API using the OpenAI library, with the API keys stored in the environment_var (for both Nvidia and HuggingFace) file we created.</p><p id="a61b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We create class Item, derived from BaseModel in Pydantic to pass as parameters to request functions. It will have one field, called query.</p><p id="0fee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, we can start initializing our machine-learning models</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="c696" class="pm nh fq pj b bg pn po l pp pq">model_name = "sentence-transformers/msmarco-bert-base-dot-v5"<br/>model_kwargs = {'device': 'cpu'}<br/>encode_kwargs = {'normalize_embeddings': True}<br/>hf = HuggingFaceEmbeddings(<br/>    model_name=model_name,<br/>    model_kwargs=model_kwargs,<br/>    encode_kwargs=encode_kwargs<br/>)<br/><br/>os.environ["HF_TOKEN"] = environment_var.hf_token<br/>use_nvidia_api = False<br/>use_quantized = True<br/>if environment_var.nvidia_key !="":<br/>    client_ai = OpenAI(<br/>        base_url="https://integrate.api.nvidia.com/v1",<br/>        api_key=environment_var.nvidia_key<br/>    )<br/>    use_nvidia_api = True<br/>elif use_quantized:<br/>    model_id = "Kameshr/LLAMA-3-Quantized"<br/>    tokenizer = AutoTokenizer.from_pretrained(model_id)<br/>    model = AutoModelForCausalLM.from_pretrained(<br/>        model_id,<br/>        torch_dtype=torch.float16,<br/>        device_map="auto",<br/>    )<br/>else:<br/>    model_id = "meta-llama/Meta-Llama-3-8B-Instruct"<br/>    tokenizer = AutoTokenizer.from_pretrained(model_id)<br/>    model = AutoModelForCausalLM.from_pretrained(<br/>        model_id,<br/>        torch_dtype=torch.float16,<br/>        device_map="auto",<br/>    )</span></pre><p id="ba61" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the first few lines, we load weights for the BERT-based model fine-tuned on MSMARCO data that we have also used to index our documents.</p><p id="a77a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then, we check whether nvidia_key is provided, and if it is, we use the OpenAI library to call NVIDIA NIM API. When we use NVIDIA NIM API, we can use a big version of the Llama 3 instruct model, with 70B parameters. In case nvidia_key is not provided, we will load Llama 3 locally. However, locally, at least for most consumer electronics, it would not be possible to load the 70B parameters model. Therefore, we will either load the Llama 3 8B parameter model or the Llama 3 8B parameters model that has been additionally quantized. With quantization, we save space and enable model execution on less RAM. For example, Llama 3 8B usually needs about 14GB of GPU RAM, while Llama 3 8B quantized would be able to run on 6GB of GPU RAM. Therefore, we load either a full or quantized model depending on a parameter.</p><p id="84bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can now initialize the Qdrant client</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="e720" class="pm nh fq pj b bg pn po l pp pq">client = QdrantClient(path="qdrant/")<br/>collection_name = "MyCollection"<br/>qdrant = Qdrant(client, collection_name, hf)</span></pre><p id="d828" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also, FastAPI and create a first mock GET function</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="5836" class="pm nh fq pj b bg pn po l pp pq">app = FastAPI()<br/><br/><br/>@app.get("/")<br/>async def root():<br/>    return {"message": "Hello World"}</span></pre><p id="a093" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This function would return JSON in format {“message”:”Hello World”}</p><p id="7242" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, for this API to be functional, we will create two functions, one that performs only semantic search, while the other would perform search and then put the top 10 chunks as a context and generate an answer, referencing documents it used.</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="bc97" class="pm nh fq pj b bg pn po l pp pq">@app.post("/search")<br/>def search(Item:Item):<br/>    query = Item.query<br/>    search_result = qdrant.similarity_search(<br/>        query=query, k=10<br/>    )<br/>    i = 0<br/>    list_res = []<br/>    for res in search_result:<br/>        list_res.append({"id":i,"path":res.metadata.get("path"),"content":res.page_content})<br/>    return list_res<br/><br/>@app.post("/ask_localai")<br/>async def ask_localai(Item:Item):<br/>    query = Item.query<br/>    search_result = qdrant.similarity_search(<br/>        query=query, k=10<br/>    )<br/>    i = 0<br/>    list_res = []<br/>    context = ""<br/>    mappings = {}<br/>    i = 0<br/>    for res in search_result:<br/>        context = context + str(i)+"\n"+res.page_content+"\n\n"<br/>        mappings[i] = res.metadata.get("path")<br/>        list_res.append({"id":i,"path":res.metadata.get("path"),"content":res.page_content})<br/>        i = i +1<br/><br/>    rolemsg = {"role": "system",<br/>               "content": "Answer user's question using documents given in the context. In the context are documents that should contain an answer. Please always reference document id (in squere brackets, for example [0],[1]) of the document that was used to make a claim. Use as many citations and documents as it is necessary to answer question."}<br/>    messages = [<br/>        rolemsg,<br/>        {"role": "user", "content": "Documents:\n"+context+"\n\nQuestion: "+query},<br/>    ]<br/>    if use_nvidia_api:<br/>        completion = client_ai.chat.completions.create(<br/>            model="meta/llama3-70b-instruct",<br/>            messages=messages,<br/>            temperature=0.5,<br/>            top_p=1,<br/>            max_tokens=1024,<br/>            stream=False<br/>        )<br/>        response = completion.choices[0].message.content<br/>    else:<br/>        input_ids = tokenizer.apply_chat_template(<br/>                messages,<br/>                add_generation_prompt=True,<br/>                return_tensors="pt"<br/>            ).to(model.device)<br/><br/><br/>        terminators = [<br/>            tokenizer.eos_token_id,<br/>            tokenizer.convert_tokens_to_ids("&lt;|eot_id|&gt;")<br/>            ]<br/><br/>        outputs = model.generate(<br/>            input_ids,<br/>            max_new_tokens=256,<br/>            eos_token_id=terminators,<br/>            do_sample=True,<br/>            temperature=0.2,<br/>            top_p=0.9,<br/>        )<br/>        response = tokenizer.decode(outputs[0][input_ids.shape[-1]:])<br/>    return {"context":list_res,"answer":response}</span></pre><p id="1107" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Both functions are POST methods, and we use our Item class to pass the query via JSON body. The first method returns the 10 most similar document chunks, with the path, and assigns document ID from 0–9. Therefore, it just performs the plain semantic search using dot product as similarity metric (this was defined during indexing in Qdrant — remember line containing distance=Distance.DOT).</p><p id="c5ee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The second function, called ask_localai is slightly more complex. It contains a search mechanism from the first method (therefore it may be easier to go through code there to understand semantic search), but adds a generative part. It creates a prompt for Llama 3, containing instructions in a system prompt message saying:</p><blockquote class="pr"><p id="30ad" class="ps pt fq bf pu pv pw px py pz qa ne dx">Answer the user’s question using the documents given in the context. In the context are documents that should contain an answer. Please always reference the document ID (in square brackets, for example [0],[1]) of the document that was used to make a claim. Use as many citations and documents as it is necessary to answer a question.</p></blockquote><p id="891c" class="pw-post-body-paragraph mj mk fq ml b go qb mn mo gr qc mq mr ms qd mu mv mw qe my mz na qf nc nd ne fj bk">The user’s message contains a list of documents structured as an ID (0–9) followed by the document chunk on the next line. To maintain the mapping between IDs and document paths, we create a list called list_res, which includes the ID, path, and content. The user prompt ends with the word “Question” followed by the user’s query.</p><p id="928d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The response contains context and generated answer. However, the answer is again generated by either the Llama 3 70B model (using NVIDIA NIM API), local Llama 3 8B, or local Llama 3 8B quantized depending on the passed parameters.</p><p id="ac7a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The API can be started from a separate file containing the following lines of code (given, that our generative component is in a file called api.py, as the first argument in Uvicorn maps to the file name):</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="9436" class="pm nh fq pj b bg pn po l pp pq">import uvicorn<br/><br/><br/>if __name__=="__main__":<br/>    uvicorn.run("api:app",host='0.0.0.0', port=8000, reload=False,  workers=3)</span></pre><h1 id="caf1" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Simple User Interface</h1><p id="08ba" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">The final component of our local generative search engine is the user interface. We will build a simple user interface using <a class="af nf" href="https://streamlit.io/" rel="noopener ugc nofollow" target="_blank">Streamlit</a>, which will include an input bar, a search button, a section for displaying the generated answer, and a list of referenced documents that can be opened or downloaded.</p><p id="c583" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The whole code for the user interface in Streamlit has less than 45 lines of code (44 to be exact):</p><pre class="os ot ou ov ow pi pj pk bp pl bb bk"><span id="01c8" class="pm nh fq pj b bg pn po l pp pq">import re<br/>import streamlit as st<br/>import requests<br/>import json<br/>st.title('_:blue[Local GenAI Search]_ :sunglasses:')<br/>question = st.text_input("Ask a question based on your local files", "")<br/>if st.button("Ask a question"):<br/>    st.write("The current question is \"", question+"\"")<br/>    url = "http://127.0.0.1:8000/ask_localai"<br/><br/>    payload = json.dumps({<br/>      "query": question<br/>    })<br/>    headers = {<br/>      'Accept': 'application/json',<br/>      'Content-Type': 'application/json'<br/>    }<br/><br/>    response = requests.request("POST", url, headers=headers, data=payload)<br/><br/>    answer = json.loads(response.text)["answer"]<br/>    rege = re.compile("\[Document\ [0-9]+\]|\[[0-9]+\]")<br/>    m = rege.findall(answer)<br/>    num = []<br/>    for n in m:<br/>        num = num + [int(s) for s in re.findall(r'\b\d+\b', n)]<br/><br/><br/>    st.markdown(answer)<br/>    documents = json.loads(response.text)['context']<br/>    show_docs = []<br/>    for n in num:<br/>        for doc in documents:<br/>            if int(doc['id']) == n:<br/>                show_docs.append(doc)<br/>    a = 1244<br/>    for doc in show_docs:<br/>        with st.expander(str(doc['id'])+" - "+doc['path']):<br/>            st.write(doc['content'])<br/>            with open(doc['path'], 'rb') as f:<br/>                st.download_button("Downlaod file", f, file_name=doc['path'].split('/')[-1],key=a<br/>                )<br/>                a = a + 1</span></pre><p id="7c3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It will all end up looking like this:</p><figure class="os ot ou ov ow ox op oq paragraph-image"><div role="button" tabindex="0" class="oy oz ed pa bh pb"><div class="op oq qg"><img src="../Images/a81808986db73b743ff6767665018ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1htZmxqFyL0sA0O32MaHA.png"/></div></div><figcaption class="pd pe pf op oq pg ph bf b bg z dx">An example of an answered question in the built user interface. Screenshot by author.</figcaption></figure><h1 id="5f3c" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Availability</h1><p id="703b" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">The entire code for the described project is available on GitHub, at <a class="af nf" href="https://github.com/nikolamilosevic86/local-genAI-search" rel="noopener ugc nofollow" target="_blank">https://github.com/nikolamilosevic86/local-genAI-search</a>. In the past, I have worked on several generative search projects, on which there have also been some publications. You can have a look at <a class="af nf" href="https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html" rel="noopener ugc nofollow" target="_blank">https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html</a> or <a class="af nf" href="https://arxiv.org/abs/2402.18589" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2402.18589</a>.</p><h1 id="7b7f" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Conclusion</h1><p id="1c7f" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">This article showed how one can leverage generative AI with semantic search using Qdrant. It is generally a Retrieval-Augmented Generation (RAG) pipeline over local files with instructions to reference claims to the local documents. The whole code is about 300 lines long, and we have even added complexity by giving a choice to the user between 3 different Llama 3 models. For this use case, both 8B and 70B parameter models work quite well.</p><p id="404b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I wanted to explain the steps I did, in case this can be helpful for someone in the future. However, if you want to use this particular tool, the easiest way to do so is by just getting it from <a class="af nf" href="https://github.com/nikolamilosevic86/local-genAI-search" rel="noopener ugc nofollow" target="_blank">GitHub</a>, it is all open source!</p></div></div></div></div>    
</body>
</html>