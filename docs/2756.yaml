- en: NER in Czech Documents with XLM-RoBERTa using 🤗 Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 🤗 Accelerate 进行捷克文档中的NER任务，基于XLM-RoBERTa模型
- en: 原文：[https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12](https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12](https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12)
- en: '*Decisions I made during the development of a document processing model that
    was successfully deployed*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*在开发一个成功部署的文档处理模型过程中，我做出的决策*'
- en: '[](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[![Bohumir
    Buso](../Images/751ba491a8f5aca31add3dbc850841c5.png)](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    [Bohumir Buso](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[![Bohumir
    Buso](../Images/751ba491a8f5aca31add3dbc850841c5.png)](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    [Bohumir Buso](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    ·9 min read·Nov 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    ·阅读时间：9分钟·2024年11月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/874a04ed0f94f4a2b3e255bd98e3ae03.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/874a04ed0f94f4a2b3e255bd98e3ae03.png)'
- en: Image generated by Dall-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由Dall-E生成
- en: Although I have over 8 years of experience with ML projects, this was my first
    NLP project. I initially searched for existing resources and code but found limited
    material, particularly for NER in Czech-language documents. This inspired me to
    compile everything I learned during development into one place, hoping to help
    future newcomers progress more efficiently. As such, **this article offers a practical
    introduction rather than an in-depth theoretical analysis**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我在机器学习项目中已有超过8年的经验，但这是我第一次从事NLP项目。我最初寻找现有的资源和代码，但发现相关材料非常有限，特别是关于捷克语文档中的NER任务。这激发我将开发过程中学到的所有知识整理到一个地方，希望能帮助未来的新人更高效地进步。因此，**本文提供的是一个实用的入门介绍，而非深入的理论分析**。
- en: Specific numerical results are omitted due to sensitivity considerations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于敏感性考虑，特定的数值结果已被省略。
- en: Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: '![](../Images/36c928f19c6a3344476a2804e0a82cf8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36c928f19c6a3344476a2804e0a82cf8.png)'
- en: 'Example of document with entities: *variable symbol of creditor* (red), surname
    (light green), given name (dark green), date of birth (blue). Sensitive information
    is blacked out.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 包含实体的文档示例：*债权人变量符号*（红色）、姓氏（浅绿色）、名字（深绿色）、出生日期（蓝色）。敏感信息已被屏蔽。
- en: '**Task** The **main objective was to identify the client(s) associated with
    each document** through one of the following identifiers:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务** 主要目标是通过以下任一标识符来识别与每个文档相关联的客户：'
- en: '*variable symbol of creditor* (present in about 20% of documents)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*债权人变量符号*（约20%的文档中出现）'
- en: '*birth ID* (present in about 60% of documents)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出生身份证明*（约60%的文档中出现）'
- en: combination *name* + *surname* + *birth date* (present in about 50% of documents)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合 *姓名* + *姓氏* + *出生日期*（约50%的文档中出现）
- en: Approximately 5% of the documents contained no identifying entities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大约5%的文档没有包含任何识别实体。
- en: '**Dataset** For development, I used 710 “true” PDF documents, dividing them
    into three sets: 600 for training, 55 for validation, and 55 for testing.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集** 在开发过程中，我使用了710个“真实”PDF文档，将其分为三个集：600个用于训练，55个用于验证，55个用于测试。'
- en: '**Labels** I received an Excel file with entities extracted as plain text,
    requiring manual labeling of the document text. Using the [BIO](https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition#BIO)
    tagging format, I followed these steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签** 我收到了一个包含实体提取为纯文本的Excel文件，需要手动标注文档文本。我使用了[BIO](https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition#BIO)标注格式，按照以下步骤进行操作：'
- en: Open each document (using `extract_text()` function from the `pdfminer.high_level`
    module)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开每个文档（使用`extract_text()`函数，来自`pdfminer.high_level`模块）
- en: Split text into words (using the SpaCy model “xx_sent_ud_sm” with adjustments,
    such as preventing splits on hyphens to handle birth number formats, e.g., ‘84–12–10/7869’)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文本拆分为单词（使用SpaCy模型“xx_sent_ud_sm”，并做一些调整，如防止在连字符上拆分，以处理出生号码格式，例如‘84–12–10/7869’）
- en: Identify entities within the text
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别文本中的实体
- en: Assign corresponding labels to entities, using the “O” label for all other words
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为实体分配相应的标签，使用“O”标签标注所有其他词汇
- en: '**Alternative Approach** Models like LayoutLM, which also consider bounding
    boxes for input tokens, might improve quality. However, I avoided this option
    since, as usual (😮‍💨), **I had already spent most of the project time on data
    preparation** (e.g., reformatting Excel files, correcting data errors, labeling).
    Pursuing bounding box-based models would have required even more time.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**替代方法** 类似LayoutLM的模型，考虑输入标记的边界框，可能会提高质量。然而，我避免了这个选项，因为和往常一样（😮‍💨），**我已经在数据准备上花费了大部分项目时间**（例如，重新格式化Excel文件、修正数据错误、标注）。追求基于边界框的模型会需要更多的时间。'
- en: While regex and heuristics could theoretically work for simple entities like
    these, I believe this approach would be ineffective, as it would require overly
    complex rules to accurately identify the correct ones amidst other potential candidates
    (lawyer name, case number, other participants in the proceedings, etc.). The model,
    on the other hand, is capable of learning to distinguish the relevant entities,
    making the use of heuristics unnecessary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正则表达式和启发式方法理论上可以处理这些简单的实体，我认为这种方法会无效，因为它需要过于复杂的规则才能准确地在其他潜在候选实体中识别出正确的实体（如律师名字、案件编号、其他诉讼参与者等）。另一方面，模型能够学习区分相关实体，因此不需要使用启发式方法。
- en: Model (training)
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型（训练）
- en: '**🤗 Accelerate**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**🤗 Accelerate**'
- en: Having started in a time when wrappers were less common, **I became accustomed
    to writing my own training loops, which I find easier to debug -** **an approach
    that 🤗 Accelerate supports effectively**. It proved beneficial in this project
    - I wasn’t entirely certain of the required data and label formats or shapes and
    my data didn’t match the well-organized examples often shown in tutorials, but
    having full access to intermediate computations during the training loop allowed
    me to iterate quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在封装器较不常见的时期开始工作，**我习惯于编写自己的训练循环，这让我更容易调试 -** **这一方法得到了🤗 Accelerate的有效支持**。这在这个项目中证明是有益的——我并不完全确定所需的数据和标签格式或形状，而我的数据与教程中常见的组织良好的示例不符，但在训练循环中完全访问中间计算使我能够快速迭代。
- en: '**Context Length** Most tutorials suggest using each sentence as a single training
    example. However, in this case, I decided **a longer context would be more suitable
    as documents typically contain references to multiple entities**, many of which
    are irrelevant (e.g. lawyers, other creditors, case numbers). This broader context
    enables the model to better identify the relevant client. I used 512 tokens from
    each document as one training example. This is a common maximum limit for models
    but comfortably accommodates all entities in most of my documents.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文长度** 大多数教程建议将每个句子作为一个单独的训练示例。然而，在这个案例中，我决定**更长的上下文更为合适，因为文档通常包含对多个实体的引用，其中许多是无关的（例如律师、其他债权人、案件编号）**。这种更广泛的上下文帮助模型更好地识别相关客户。我使用每个文档的512个标记作为一个训练示例。这是大多数模型的常见最大限制，但足以容纳我文档中的所有实体。'
- en: '**Labelling of Subtokens** In the 🤗 token classification tutorial [1], recommended
    approach is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**子标记的标注** 在🤗标记分类教程[1]中，推荐的方法是：'
- en: Only labeling the first token of a given word. Assign `*-100*` to other subtokens
    from the same word.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 只标注给定单词的第一个标记。对同一单词的其他子标记分配`*-100*`。
- en: 'However, I found that the following method suggested in the token classification
    tutorial in their NLP course [2] works much better:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我发现以下方法在他们的NLP课程中的标记分类教程[2]中效果更好：
- en: Each token gets the same label as the token that started the word it’s inside,
    since they are part of the same entity. For tokens inside a word but not at the
    beginning, we replace the `*B-*` with `*I-*`
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每个词元都得到与它所在单词开头词元相同的标签，因为它们是同一实体的一部分。对于单词内部但不是开头的词元，我们将`*B-*`替换为`*I-*`。
- en: 'Label “-100” is special label that is ignoredby loss function. Hence, I used
    their functions with minor changes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 标签“-100”是一个特殊标签，它会被损失函数忽略。因此，我对他们的函数做了小的修改：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I also used their `postprocess()`function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我还使用了他们的`postprocess()`函数：
- en: To simplify its evaluation part, we define this `*postprocess()*` function that
    takes predictions and labels and converts them to lists of strings.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了简化其评估部分，我们定义了这个`*postprocess()*`函数，它接收预测值和标签，并将它们转换为字符串列表。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Class Weights'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别权重**'
- en: 'Incorporating class weights into the loss function significantly improved model
    performance.** While this adjustment may seem straightforward — without it, the
    model overemphasized the majority “O” class — it’s surprisingly absent from most
    tutorials. I implemented a custom `compute_weights()` function to address this
    imbalance:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将类别权重纳入损失函数显著提高了模型性能。**虽然这个调整看起来很简单——没有它，模型过于强调了“O”类别这个多数类——但令人惊讶的是，大多数教程中都没有提到这一点。我实现了一个自定义的`compute_weights()`函数来解决这个不平衡问题：**
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Training Loop** I defined two additional functions: PyTorch `DataLoader()`
    to manage batch processing, and a `main()` function to set up distributed training
    objects and execute the training loop.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练循环** 我定义了两个额外的函数：PyTorch的`DataLoader()`来管理批处理，以及一个`main()`函数来设置分布式训练对象并执行训练循环。'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With everything prepared, the model is ready for training. I just need to initiate
    the process:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，模型已经可以开始训练了。我只需要启动这个过程：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**I find using** `notebook_launcher()` **convenient, as it allows me to run
    training in the console and easily work with results afterward.**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**我发现使用** `notebook_launcher()` **很方便，因为它允许我在控制台中运行训练，并且之后可以轻松处理结果。**'
- en: '**XLM-RoBERTa base vs large vs Small-E-Czech** I experimented with fine-tuning
    three models. The XLM-RoBERTa base model [3] delivered satisfactory performance,
    but the server capacity also allowed me to try the XLM-RoBERTa large model [3],
    which has twice the parameters.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLM-RoBERTa基础版 vs 大型版 vs Small-E-Czech** 我尝试了微调三种模型。XLM-RoBERTa基础版模型[3]表现令人满意，但服务器容量也允许我尝试XLM-RoBERTa大型版模型[3]，它的参数量是前者的两倍。'
- en: XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB
    of filtered CommonCrawl data containing 100 languages.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: XLM-RoBERTa是RoBERTa的多语言版本。它是在2.5TB过滤后的CommonCrawl数据上预训练的，包含100种语言。
- en: The large model showed a slight improvement in results, so I ultimately deployed
    it. I also tested Small-E-Czech [4], an Electra-small model pre-trained on Czech
    web data, but its performance was poor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型在结果上略有提升，所以我最终部署了它。我也测试了Small-E-Czech [4]，一个在捷克网页数据上预训练的Electra-small模型，但它的表现很差。
- en: '**Fine-tuning vs Transfer learning vs Training from scratch** In addition to
    fine-tuning (updating all model weights), I tested transfer learning, as it is
    sometimes suggested that training only the final (classification) layer may suffice..
    However, the performance difference was significant, favoring full fine-tuning.
    I also attempted training from scratch by importing only architecture of the model,
    initializing the weights randomly, and then training, but as expected, this approach
    was ineffective.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调 vs 迁移学习 vs 从头训练** 除了微调（更新所有模型权重）之外，我还测试了迁移学习，因为有时会建议只训练最后一层（分类层）就足够了。然而，性能差异显著，微调更为优越。我还尝试了从头训练，通过仅导入模型的架构，随机初始化权重，然后进行训练，但如预期的那样，这种方法效果不佳。'
- en: '**RoBERTa vs LLM (Claude 3.5 Sonnet)** I briefly explored zero-shot LLMs, though
    with minimal prompt engineering (so 🥱). The model struggled even with basic requests,
    such as (I used Czech in the actual prompt):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa vs LLM（Claude 3.5 Sonnet）** 我简要探索了零样本LLM，尽管我几乎没有进行提示工程（所以有点🥱）。模型在处理基本请求时表现困难，比如（我在实际提示中使用了捷克语）：'
- en: 'Find variable symbol of creditor.This number has exactly 9 consecutive digits
    0–9 without letters or other special characters. It is usually preceded by one
    of the following abbreviations: ‘ev.č.’, ‘zn. opr’, ‘VS. O’, ‘evid. č. opr.’.
    On the contrary, I’m not interested in a transaction number with the abbreviation
    ‘č.j.’. This number does not appear often in documents, it may happen that you
    will not be able to find it, then write ‘cannot find’. If you’re not sure, write
    ‘not sure’.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查找债权人变量符号。这个数字由恰好 9 位连续数字（0–9）组成，不包含字母或其他特殊字符。通常前面会有以下缩写之一：‘ev.č.’、‘zn. opr’、‘VS.
    O’、‘evid. č. opr.’。相反，我对缩写为 ‘č.j.’ 的交易号不感兴趣。这个数字在文档中出现的频率较低，可能你找不到它，若无法找到，写上‘cannot
    find’。如果不确定，写上‘not sure’。
- en: The model sometimes failed to output the 9-digit format accurately. Post-processing
    would filter out shorter numbers, but there were many false positives 9-digit
    numbers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 模型有时无法准确输出 9 位数字格式。后处理会过滤掉较短的数字，但也会出现很多误报的 9 位数字。
- en: Occasionally the model inferred incorrect *birth ID*s based solely on birth
    dates (even with temperature set to 0). On the other hand, it excelled at extracting
    *names*, *surnames*, and *birth dates*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有时模型仅基于出生日期推测错误的 *出生身份证号*（即使温度设置为 0）。另一方面，模型在提取 *姓名*、*姓氏* 和 *出生日期* 方面表现优秀。
- en: Overall, even in my previous experiments, **I found that LLMs** (at the time
    of writing) **perform better with general tasks but lack accuracy and reliability
    for specific or unconventional tasks.** The performance in identifying the client
    was fairly similar for both approaches. For internal reasons, the RoBERTa model
    was deployed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，即使在我之前的实验中，**我发现大语言模型（LLMs）**（在撰写时）**在一般任务上表现较好，但在特定或非常规任务上的准确性和可靠性较差**。在客户识别方面，两种方法的表现相差无几。由于内部原因，最终部署了
    RoBERTa 模型。
- en: Post-processing
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后处理
- en: 'Notably, implementing **post-processing can significantly reduce false positives**,
    enhancing overall performance. Each entity was subject to customized filtering
    and validation rules:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，实施 **后处理可以显著减少误报**，提高整体性能。每个实体都遵循定制的过滤和验证规则：
- en: '*variable symbol of debtor -* verify 9 digits format'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*债务人变量符号* - 验证 9 位数字格式'
- en: '*birth ID -* enforce XXXXXX/XXX(X) format and check divisibility by eleven'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出生身份证号* - 强制使用 XXXXXX/XXX(X) 格式，并检查是否能被 11 整除'
- en: '*name* and *surname -* apply lemmatization using MorphoDiTa [5]'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*姓名* 和 *姓氏* - 使用 MorphoDiTa [5] 进行词形还原'
- en: '*date of birth -* enforce DD.MM.YYYY format'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*出生日期* - 强制使用 DD.MM.YYYY 格式'
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The fine-tuned model was successfully deployed and **performs superbly**, **exceeding
    expectations given the modest dataset** of 710 documents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后的模型成功部署，**表现卓越**，**在仅有 710 篇文档的 modest 数据集上超出预期表现**。
- en: While LLMs show promise for general tasks, they lack the accuracy and reliability
    for specialized tasks. That said, it’s likely that in the near future, even fine-tuning
    will become unnecessary for all but highly specialized cases as LLMs continue
    to improve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型在一般任务中显示出潜力，但它们在专业任务上的准确性和可靠性不足。也就是说，随着大语言模型的持续改进，未来即便是微调，除非是高度专业化的任务，否则可能变得不再必要。
- en: '**Acknowledgments** I would like to thank [Martin](https://www.linkedin.com/in/martin-munch/),
    [Tomáš](https://www.linkedin.com/in/tomas-duricek/) and [Petr](https://www.linkedin.com/in/petr-petras-37b2b0160/)
    for their valuable suggestions for the improvement of this article.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**致谢** 我想感谢 [Martin](https://www.linkedin.com/in/martin-munch/)、[Tomáš](https://www.linkedin.com/in/tomas-duricek/)
    和 [Petr](https://www.linkedin.com/in/petr-petras-37b2b0160/) 对本文章提出的宝贵建议。'
- en: '**Sources** [1] Hugging Face, [Transformers - Token classification](https://huggingface.co/docs/transformers/tasks/token_classification#preprocess)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**来源** [1] Hugging Face，[Transformers - Token classification](https://huggingface.co/docs/transformers/tasks/token_classification#preprocess)'
- en: '[2] Hugging Face, [NLP Course — Token classification](https://huggingface.co/learn/nlp-course/chapter7/2)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hugging Face，[NLP Course — Token classification](https://huggingface.co/learn/nlp-course/chapter7/2)'
- en: '[3] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman,
    E. Grave, M. Ott, L. Zettlemoyer and V. Stoyanov, [Unsupervised Cross-lingual
    Representation Learning at Scale](https://arxiv.org/abs/1911.02116) (2019), CoRR
    abs/1911.02116'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman,
    E. Grave, M. Ott, L. Zettlemoyer 和 V. Stoyanov，[无监督跨语言表示学习 at Scale](https://arxiv.org/abs/1911.02116)（2019），CoRR
    abs/1911.02116'
- en: '[4] M. Kocián, J. Náplava, D. Štancl and V. Kadlec, [Siamese BERT-based Model
    for Web Search Relevance Ranking Evaluated on a New Czech Dataset](https://arxiv.org/abs/2112.01810)
    (2021)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] M. Kocián, J. Náplava, D. Štancl 和 V. Kadlec，[基于Siamese BERT的网络搜索相关性排序模型，在新的捷克数据集上进行评估](https://arxiv.org/abs/2112.01810)（2021）'
- en: '[5] J. Straková, M. Straka and J. Hajič . [Open-Source Tools for Morphology,
    Lemmatization, POS Tagging and Named Entity Recognition](http://www.aclweb.org/anthology/P/P14/P14-5003.pdf)
    (2014), In Proceedings of 52nd Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations, pages 13–18, Baltimore, Maryland, June 2014\.
    Association for Computational Linguistics.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. Straková, M. Straka 和 J. Hajič. [用于形态学、词形还原、词性标注和命名实体识别的开源工具](http://www.aclweb.org/anthology/P/P14/P14-5003.pdf)（2014），发表于《第52届计算语言学协会年会：系统展示论文集》，第13–18页，美国马里兰州巴尔的摩，2014年6月。计算语言学协会。'
