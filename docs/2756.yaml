- en: NER in Czech Documents with XLM-RoBERTa using ğŸ¤— Accelerate
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡Œæ·å…‹æ–‡æ¡£ä¸­çš„NERä»»åŠ¡ï¼ŒåŸºäºXLM-RoBERTaæ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12](https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12](https://towardsdatascience.com/ner-in-czech-documents-with-xlm-roberta-using-accelerate-32a6baf3e91e?source=collection_archive---------9-----------------------#2024-11-12)
- en: '*Decisions I made during the development of a document processing model that
    was successfully deployed*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*åœ¨å¼€å‘ä¸€ä¸ªæˆåŠŸéƒ¨ç½²çš„æ–‡æ¡£å¤„ç†æ¨¡å‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘åšå‡ºçš„å†³ç­–*'
- en: '[](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[![Bohumir
    Buso](../Images/751ba491a8f5aca31add3dbc850841c5.png)](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    [Bohumir Buso](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[![Bohumir
    Buso](../Images/751ba491a8f5aca31add3dbc850841c5.png)](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    [Bohumir Buso](https://medium.com/@bbuso?source=post_page---byline--32a6baf3e91e--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    Â·9 min readÂ·Nov 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--32a6baf3e91e--------------------------------)
    Â·é˜…è¯»æ—¶é—´ï¼š9åˆ†é’ŸÂ·2024å¹´11æœˆ12æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/874a04ed0f94f4a2b3e255bd98e3ae03.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/874a04ed0f94f4a2b3e255bd98e3ae03.png)'
- en: Image generated by Dall-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”±Dall-Eç”Ÿæˆ
- en: Although I have over 8 years of experience with ML projects, this was my first
    NLP project. I initially searched for existing resources and code but found limited
    material, particularly for NER in Czech-language documents. This inspired me to
    compile everything I learned during development into one place, hoping to help
    future newcomers progress more efficiently. As such, **this article offers a practical
    introduction rather than an in-depth theoretical analysis**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­å·²æœ‰è¶…è¿‡8å¹´çš„ç»éªŒï¼Œä½†è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡ä»äº‹NLPé¡¹ç›®ã€‚æˆ‘æœ€åˆå¯»æ‰¾ç°æœ‰çš„èµ„æºå’Œä»£ç ï¼Œä½†å‘ç°ç›¸å…³ææ–™éå¸¸æœ‰é™ï¼Œç‰¹åˆ«æ˜¯å…³äºæ·å…‹è¯­æ–‡æ¡£ä¸­çš„NERä»»åŠ¡ã€‚è¿™æ¿€å‘æˆ‘å°†å¼€å‘è¿‡ç¨‹ä¸­å­¦åˆ°çš„æ‰€æœ‰çŸ¥è¯†æ•´ç†åˆ°ä¸€ä¸ªåœ°æ–¹ï¼Œå¸Œæœ›èƒ½å¸®åŠ©æœªæ¥çš„æ–°äººæ›´é«˜æ•ˆåœ°è¿›æ­¥ã€‚å› æ­¤ï¼Œ**æœ¬æ–‡æä¾›çš„æ˜¯ä¸€ä¸ªå®ç”¨çš„å…¥é—¨ä»‹ç»ï¼Œè€Œéæ·±å…¥çš„ç†è®ºåˆ†æ**ã€‚
- en: Specific numerical results are omitted due to sensitivity considerations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ•æ„Ÿæ€§è€ƒè™‘ï¼Œç‰¹å®šçš„æ•°å€¼ç»“æœå·²è¢«çœç•¥ã€‚
- en: Data
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®
- en: '![](../Images/36c928f19c6a3344476a2804e0a82cf8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36c928f19c6a3344476a2804e0a82cf8.png)'
- en: 'Example of document with entities: *variable symbol of creditor* (red), surname
    (light green), given name (dark green), date of birth (blue). Sensitive information
    is blacked out.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…å«å®ä½“çš„æ–‡æ¡£ç¤ºä¾‹ï¼š*å€ºæƒäººå˜é‡ç¬¦å·*ï¼ˆçº¢è‰²ï¼‰ã€å§“æ°ï¼ˆæµ…ç»¿è‰²ï¼‰ã€åå­—ï¼ˆæ·±ç»¿è‰²ï¼‰ã€å‡ºç”Ÿæ—¥æœŸï¼ˆè“è‰²ï¼‰ã€‚æ•æ„Ÿä¿¡æ¯å·²è¢«å±è”½ã€‚
- en: '**Task** The **main objective was to identify the client(s) associated with
    each document** through one of the following identifiers:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä»»åŠ¡** ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ ‡è¯†ç¬¦æ¥è¯†åˆ«ä¸æ¯ä¸ªæ–‡æ¡£ç›¸å…³è”çš„å®¢æˆ·ï¼š'
- en: '*variable symbol of creditor* (present in about 20% of documents)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å€ºæƒäººå˜é‡ç¬¦å·*ï¼ˆçº¦20%çš„æ–‡æ¡£ä¸­å‡ºç°ï¼‰'
- en: '*birth ID* (present in about 60% of documents)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡ºç”Ÿèº«ä»½è¯æ˜*ï¼ˆçº¦60%çš„æ–‡æ¡£ä¸­å‡ºç°ï¼‰'
- en: combination *name* + *surname* + *birth date* (present in about 50% of documents)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»„åˆ *å§“å* + *å§“æ°* + *å‡ºç”Ÿæ—¥æœŸ*ï¼ˆçº¦50%çš„æ–‡æ¡£ä¸­å‡ºç°ï¼‰
- en: Approximately 5% of the documents contained no identifying entities.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§çº¦5%çš„æ–‡æ¡£æ²¡æœ‰åŒ…å«ä»»ä½•è¯†åˆ«å®ä½“ã€‚
- en: '**Dataset** For development, I used 710 â€œtrueâ€ PDF documents, dividing them
    into three sets: 600 for training, 55 for validation, and 55 for testing.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ•°æ®é›†** åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†710ä¸ªâ€œçœŸå®â€PDFæ–‡æ¡£ï¼Œå°†å…¶åˆ†ä¸ºä¸‰ä¸ªé›†ï¼š600ä¸ªç”¨äºè®­ç»ƒï¼Œ55ä¸ªç”¨äºéªŒè¯ï¼Œ55ä¸ªç”¨äºæµ‹è¯•ã€‚'
- en: '**Labels** I received an Excel file with entities extracted as plain text,
    requiring manual labeling of the document text. Using the [BIO](https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition#BIO)
    tagging format, I followed these steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ ‡ç­¾** æˆ‘æ”¶åˆ°äº†ä¸€ä¸ªåŒ…å«å®ä½“æå–ä¸ºçº¯æ–‡æœ¬çš„Excelæ–‡ä»¶ï¼Œéœ€è¦æ‰‹åŠ¨æ ‡æ³¨æ–‡æ¡£æ–‡æœ¬ã€‚æˆ‘ä½¿ç”¨äº†[BIO](https://natural-language-understanding.fandom.com/wiki/Named_entity_recognition#BIO)æ ‡æ³¨æ ¼å¼ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ“ä½œï¼š'
- en: Open each document (using `extract_text()` function from the `pdfminer.high_level`
    module)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰“å¼€æ¯ä¸ªæ–‡æ¡£ï¼ˆä½¿ç”¨`extract_text()`å‡½æ•°ï¼Œæ¥è‡ª`pdfminer.high_level`æ¨¡å—ï¼‰
- en: Split text into words (using the SpaCy model â€œxx_sent_ud_smâ€ with adjustments,
    such as preventing splits on hyphens to handle birth number formats, e.g., â€˜84â€“12â€“10/7869â€™)
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ï¼ˆä½¿ç”¨SpaCyæ¨¡å‹â€œxx_sent_ud_smâ€ï¼Œå¹¶åšä¸€äº›è°ƒæ•´ï¼Œå¦‚é˜²æ­¢åœ¨è¿å­—ç¬¦ä¸Šæ‹†åˆ†ï¼Œä»¥å¤„ç†å‡ºç”Ÿå·ç æ ¼å¼ï¼Œä¾‹å¦‚â€˜84â€“12â€“10/7869â€™ï¼‰
- en: Identify entities within the text
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“
- en: Assign corresponding labels to entities, using the â€œOâ€ label for all other words
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºå®ä½“åˆ†é…ç›¸åº”çš„æ ‡ç­¾ï¼Œä½¿ç”¨â€œOâ€æ ‡ç­¾æ ‡æ³¨æ‰€æœ‰å…¶ä»–è¯æ±‡
- en: '**Alternative Approach** Models like LayoutLM, which also consider bounding
    boxes for input tokens, might improve quality. However, I avoided this option
    since, as usual (ğŸ˜®â€ğŸ’¨), **I had already spent most of the project time on data
    preparation** (e.g., reformatting Excel files, correcting data errors, labeling).
    Pursuing bounding box-based models would have required even more time.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›¿ä»£æ–¹æ³•** ç±»ä¼¼LayoutLMçš„æ¨¡å‹ï¼Œè€ƒè™‘è¾“å…¥æ ‡è®°çš„è¾¹ç•Œæ¡†ï¼Œå¯èƒ½ä¼šæé«˜è´¨é‡ã€‚ç„¶è€Œï¼Œæˆ‘é¿å…äº†è¿™ä¸ªé€‰é¡¹ï¼Œå› ä¸ºå’Œå¾€å¸¸ä¸€æ ·ï¼ˆğŸ˜®â€ğŸ’¨ï¼‰ï¼Œ**æˆ‘å·²ç»åœ¨æ•°æ®å‡†å¤‡ä¸ŠèŠ±è´¹äº†å¤§éƒ¨åˆ†é¡¹ç›®æ—¶é—´**ï¼ˆä¾‹å¦‚ï¼Œé‡æ–°æ ¼å¼åŒ–Excelæ–‡ä»¶ã€ä¿®æ­£æ•°æ®é”™è¯¯ã€æ ‡æ³¨ï¼‰ã€‚è¿½æ±‚åŸºäºè¾¹ç•Œæ¡†çš„æ¨¡å‹ä¼šéœ€è¦æ›´å¤šçš„æ—¶é—´ã€‚'
- en: While regex and heuristics could theoretically work for simple entities like
    these, I believe this approach would be ineffective, as it would require overly
    complex rules to accurately identify the correct ones amidst other potential candidates
    (lawyer name, case number, other participants in the proceedings, etc.). The model,
    on the other hand, is capable of learning to distinguish the relevant entities,
    making the use of heuristics unnecessary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æ­£åˆ™è¡¨è¾¾å¼å’Œå¯å‘å¼æ–¹æ³•ç†è®ºä¸Šå¯ä»¥å¤„ç†è¿™äº›ç®€å•çš„å®ä½“ï¼Œæˆ‘è®¤ä¸ºè¿™ç§æ–¹æ³•ä¼šæ— æ•ˆï¼Œå› ä¸ºå®ƒéœ€è¦è¿‡äºå¤æ‚çš„è§„åˆ™æ‰èƒ½å‡†ç¡®åœ°åœ¨å…¶ä»–æ½œåœ¨å€™é€‰å®ä½“ä¸­è¯†åˆ«å‡ºæ­£ç¡®çš„å®ä½“ï¼ˆå¦‚å¾‹å¸ˆåå­—ã€æ¡ˆä»¶ç¼–å·ã€å…¶ä»–è¯‰è®¼å‚ä¸è€…ç­‰ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åŒºåˆ†ç›¸å…³å®ä½“ï¼Œå› æ­¤ä¸éœ€è¦ä½¿ç”¨å¯å‘å¼æ–¹æ³•ã€‚
- en: Model (training)
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹ï¼ˆè®­ç»ƒï¼‰
- en: '**ğŸ¤— Accelerate**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**ğŸ¤— Accelerate**'
- en: Having started in a time when wrappers were less common, **I became accustomed
    to writing my own training loops, which I find easier to debug -** **an approach
    that ğŸ¤— Accelerate supports effectively**. It proved beneficial in this project
    - I wasnâ€™t entirely certain of the required data and label formats or shapes and
    my data didnâ€™t match the well-organized examples often shown in tutorials, but
    having full access to intermediate computations during the training loop allowed
    me to iterate quickly.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨å°è£…å™¨è¾ƒä¸å¸¸è§çš„æ—¶æœŸå¼€å§‹å·¥ä½œï¼Œ**æˆ‘ä¹ æƒ¯äºç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™è®©æˆ‘æ›´å®¹æ˜“è°ƒè¯• -** **è¿™ä¸€æ–¹æ³•å¾—åˆ°äº†ğŸ¤— Accelerateçš„æœ‰æ•ˆæ”¯æŒ**ã€‚è¿™åœ¨è¿™ä¸ªé¡¹ç›®ä¸­è¯æ˜æ˜¯æœ‰ç›Šçš„â€”â€”æˆ‘å¹¶ä¸å®Œå…¨ç¡®å®šæ‰€éœ€çš„æ•°æ®å’Œæ ‡ç­¾æ ¼å¼æˆ–å½¢çŠ¶ï¼Œè€Œæˆ‘çš„æ•°æ®ä¸æ•™ç¨‹ä¸­å¸¸è§çš„ç»„ç»‡è‰¯å¥½çš„ç¤ºä¾‹ä¸ç¬¦ï¼Œä½†åœ¨è®­ç»ƒå¾ªç¯ä¸­å®Œå…¨è®¿é—®ä¸­é—´è®¡ç®—ä½¿æˆ‘èƒ½å¤Ÿå¿«é€Ÿè¿­ä»£ã€‚
- en: '**Context Length** Most tutorials suggest using each sentence as a single training
    example. However, in this case, I decided **a longer context would be more suitable
    as documents typically contain references to multiple entities**, many of which
    are irrelevant (e.g. lawyers, other creditors, case numbers). This broader context
    enables the model to better identify the relevant client. I used 512 tokens from
    each document as one training example. This is a common maximum limit for models
    but comfortably accommodates all entities in most of my documents.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸Šä¸‹æ–‡é•¿åº¦** å¤§å¤šæ•°æ•™ç¨‹å»ºè®®å°†æ¯ä¸ªå¥å­ä½œä¸ºä¸€ä¸ªå•ç‹¬çš„è®­ç»ƒç¤ºä¾‹ã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæˆ‘å†³å®š**æ›´é•¿çš„ä¸Šä¸‹æ–‡æ›´ä¸ºåˆé€‚ï¼Œå› ä¸ºæ–‡æ¡£é€šå¸¸åŒ…å«å¯¹å¤šä¸ªå®ä½“çš„å¼•ç”¨ï¼Œå…¶ä¸­è®¸å¤šæ˜¯æ— å…³çš„ï¼ˆä¾‹å¦‚å¾‹å¸ˆã€å…¶ä»–å€ºæƒäººã€æ¡ˆä»¶ç¼–å·ï¼‰**ã€‚è¿™ç§æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°è¯†åˆ«ç›¸å…³å®¢æˆ·ã€‚æˆ‘ä½¿ç”¨æ¯ä¸ªæ–‡æ¡£çš„512ä¸ªæ ‡è®°ä½œä¸ºä¸€ä¸ªè®­ç»ƒç¤ºä¾‹ã€‚è¿™æ˜¯å¤§å¤šæ•°æ¨¡å‹çš„å¸¸è§æœ€å¤§é™åˆ¶ï¼Œä½†è¶³ä»¥å®¹çº³æˆ‘æ–‡æ¡£ä¸­çš„æ‰€æœ‰å®ä½“ã€‚'
- en: '**Labelling of Subtokens** In the ğŸ¤— token classification tutorial [1], recommended
    approach is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**å­æ ‡è®°çš„æ ‡æ³¨** åœ¨ğŸ¤—æ ‡è®°åˆ†ç±»æ•™ç¨‹[1]ä¸­ï¼Œæ¨èçš„æ–¹æ³•æ˜¯ï¼š'
- en: Only labeling the first token of a given word. Assign `*-100*` to other subtokens
    from the same word.
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åªæ ‡æ³¨ç»™å®šå•è¯çš„ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚å¯¹åŒä¸€å•è¯çš„å…¶ä»–å­æ ‡è®°åˆ†é…`*-100*`ã€‚
- en: 'However, I found that the following method suggested in the token classification
    tutorial in their NLP course [2] works much better:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘å‘ç°ä»¥ä¸‹æ–¹æ³•åœ¨ä»–ä»¬çš„NLPè¯¾ç¨‹ä¸­çš„æ ‡è®°åˆ†ç±»æ•™ç¨‹[2]ä¸­æ•ˆæœæ›´å¥½ï¼š
- en: Each token gets the same label as the token that started the word itâ€™s inside,
    since they are part of the same entity. For tokens inside a word but not at the
    beginning, we replace the `*B-*` with `*I-*`
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¯ä¸ªè¯å…ƒéƒ½å¾—åˆ°ä¸å®ƒæ‰€åœ¨å•è¯å¼€å¤´è¯å…ƒç›¸åŒçš„æ ‡ç­¾ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€å®ä½“çš„ä¸€éƒ¨åˆ†ã€‚å¯¹äºå•è¯å†…éƒ¨ä½†ä¸æ˜¯å¼€å¤´çš„è¯å…ƒï¼Œæˆ‘ä»¬å°†`*B-*`æ›¿æ¢ä¸º`*I-*`ã€‚
- en: 'Label â€œ-100â€ is special label that is ignoredby loss function. Hence, I used
    their functions with minor changes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡ç­¾â€œ-100â€æ˜¯ä¸€ä¸ªç‰¹æ®Šæ ‡ç­¾ï¼Œå®ƒä¼šè¢«æŸå¤±å‡½æ•°å¿½ç•¥ã€‚å› æ­¤ï¼Œæˆ‘å¯¹ä»–ä»¬çš„å‡½æ•°åšäº†å°çš„ä¿®æ”¹ï¼š
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I also used their `postprocess()`function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜ä½¿ç”¨äº†ä»–ä»¬çš„`postprocess()`å‡½æ•°ï¼š
- en: To simplify its evaluation part, we define this `*postprocess()*` function that
    takes predictions and labels and converts them to lists of strings.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª`*postprocess()*`å‡½æ•°ï¼Œå®ƒæ¥æ”¶é¢„æµ‹å€¼å’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Class Weights'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç±»åˆ«æƒé‡**'
- en: 'Incorporating class weights into the loss function significantly improved model
    performance.** While this adjustment may seem straightforward â€” without it, the
    model overemphasized the majority â€œOâ€ class â€” itâ€™s surprisingly absent from most
    tutorials. I implemented a custom `compute_weights()` function to address this
    imbalance:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç±»åˆ«æƒé‡çº³å…¥æŸå¤±å‡½æ•°æ˜¾è‘—æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚**è™½ç„¶è¿™ä¸ªè°ƒæ•´çœ‹èµ·æ¥å¾ˆç®€å•â€”â€”æ²¡æœ‰å®ƒï¼Œæ¨¡å‹è¿‡äºå¼ºè°ƒäº†â€œOâ€ç±»åˆ«è¿™ä¸ªå¤šæ•°ç±»â€”â€”ä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¤§å¤šæ•°æ•™ç¨‹ä¸­éƒ½æ²¡æœ‰æåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘å®ç°äº†ä¸€ä¸ªè‡ªå®šä¹‰çš„`compute_weights()`å‡½æ•°æ¥è§£å†³è¿™ä¸ªä¸å¹³è¡¡é—®é¢˜ï¼š**
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Training Loop** I defined two additional functions: PyTorch `DataLoader()`
    to manage batch processing, and a `main()` function to set up distributed training
    objects and execute the training loop.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒå¾ªç¯** æˆ‘å®šä¹‰äº†ä¸¤ä¸ªé¢å¤–çš„å‡½æ•°ï¼šPyTorchçš„`DataLoader()`æ¥ç®¡ç†æ‰¹å¤„ç†ï¼Œä»¥åŠä¸€ä¸ª`main()`å‡½æ•°æ¥è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒå¯¹è±¡å¹¶æ‰§è¡Œè®­ç»ƒå¾ªç¯ã€‚'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With everything prepared, the model is ready for training. I just need to initiate
    the process:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæ¨¡å‹å·²ç»å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚æˆ‘åªéœ€è¦å¯åŠ¨è¿™ä¸ªè¿‡ç¨‹ï¼š
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**I find using** `notebook_launcher()` **convenient, as it allows me to run
    training in the console and easily work with results afterward.**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**æˆ‘å‘ç°ä½¿ç”¨** `notebook_launcher()` **å¾ˆæ–¹ä¾¿ï¼Œå› ä¸ºå®ƒå…è®¸æˆ‘åœ¨æ§åˆ¶å°ä¸­è¿è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä¹‹åå¯ä»¥è½»æ¾å¤„ç†ç»“æœã€‚**'
- en: '**XLM-RoBERTa base vs large vs Small-E-Czech** I experimented with fine-tuning
    three models. The XLM-RoBERTa base model [3] delivered satisfactory performance,
    but the server capacity also allowed me to try the XLM-RoBERTa large model [3],
    which has twice the parameters.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**XLM-RoBERTaåŸºç¡€ç‰ˆ vs å¤§å‹ç‰ˆ vs Small-E-Czech** æˆ‘å°è¯•äº†å¾®è°ƒä¸‰ç§æ¨¡å‹ã€‚XLM-RoBERTaåŸºç¡€ç‰ˆæ¨¡å‹[3]è¡¨ç°ä»¤äººæ»¡æ„ï¼Œä½†æœåŠ¡å™¨å®¹é‡ä¹Ÿå…è®¸æˆ‘å°è¯•XLM-RoBERTaå¤§å‹ç‰ˆæ¨¡å‹[3]ï¼Œå®ƒçš„å‚æ•°é‡æ˜¯å‰è€…çš„ä¸¤å€ã€‚'
- en: XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB
    of filtered CommonCrawl data containing 100 languages.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: XLM-RoBERTaæ˜¯RoBERTaçš„å¤šè¯­è¨€ç‰ˆæœ¬ã€‚å®ƒæ˜¯åœ¨2.5TBè¿‡æ»¤åçš„CommonCrawlæ•°æ®ä¸Šé¢„è®­ç»ƒçš„ï¼ŒåŒ…å«100ç§è¯­è¨€ã€‚
- en: The large model showed a slight improvement in results, so I ultimately deployed
    it. I also tested Small-E-Czech [4], an Electra-small model pre-trained on Czech
    web data, but its performance was poor.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹æ¨¡å‹åœ¨ç»“æœä¸Šç•¥æœ‰æå‡ï¼Œæ‰€ä»¥æˆ‘æœ€ç»ˆéƒ¨ç½²äº†å®ƒã€‚æˆ‘ä¹Ÿæµ‹è¯•äº†Small-E-Czech [4]ï¼Œä¸€ä¸ªåœ¨æ·å…‹ç½‘é¡µæ•°æ®ä¸Šé¢„è®­ç»ƒçš„Electra-smallæ¨¡å‹ï¼Œä½†å®ƒçš„è¡¨ç°å¾ˆå·®ã€‚
- en: '**Fine-tuning vs Transfer learning vs Training from scratch** In addition to
    fine-tuning (updating all model weights), I tested transfer learning, as it is
    sometimes suggested that training only the final (classification) layer may suffice..
    However, the performance difference was significant, favoring full fine-tuning.
    I also attempted training from scratch by importing only architecture of the model,
    initializing the weights randomly, and then training, but as expected, this approach
    was ineffective.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¾®è°ƒ vs è¿ç§»å­¦ä¹  vs ä»å¤´è®­ç»ƒ** é™¤äº†å¾®è°ƒï¼ˆæ›´æ–°æ‰€æœ‰æ¨¡å‹æƒé‡ï¼‰ä¹‹å¤–ï¼Œæˆ‘è¿˜æµ‹è¯•äº†è¿ç§»å­¦ä¹ ï¼Œå› ä¸ºæœ‰æ—¶ä¼šå»ºè®®åªè®­ç»ƒæœ€åä¸€å±‚ï¼ˆåˆ†ç±»å±‚ï¼‰å°±è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œæ€§èƒ½å·®å¼‚æ˜¾è‘—ï¼Œå¾®è°ƒæ›´ä¸ºä¼˜è¶Šã€‚æˆ‘è¿˜å°è¯•äº†ä»å¤´è®­ç»ƒï¼Œé€šè¿‡ä»…å¯¼å…¥æ¨¡å‹çš„æ¶æ„ï¼Œéšæœºåˆå§‹åŒ–æƒé‡ï¼Œç„¶åè¿›è¡Œè®­ç»ƒï¼Œä½†å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè¿™ç§æ–¹æ³•æ•ˆæœä¸ä½³ã€‚'
- en: '**RoBERTa vs LLM (Claude 3.5 Sonnet)** I briefly explored zero-shot LLMs, though
    with minimal prompt engineering (so ğŸ¥±). The model struggled even with basic requests,
    such as (I used Czech in the actual prompt):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**RoBERTa vs LLMï¼ˆClaude 3.5 Sonnetï¼‰** æˆ‘ç®€è¦æ¢ç´¢äº†é›¶æ ·æœ¬LLMï¼Œå°½ç®¡æˆ‘å‡ ä¹æ²¡æœ‰è¿›è¡Œæç¤ºå·¥ç¨‹ï¼ˆæ‰€ä»¥æœ‰ç‚¹ğŸ¥±ï¼‰ã€‚æ¨¡å‹åœ¨å¤„ç†åŸºæœ¬è¯·æ±‚æ—¶è¡¨ç°å›°éš¾ï¼Œæ¯”å¦‚ï¼ˆæˆ‘åœ¨å®é™…æç¤ºä¸­ä½¿ç”¨äº†æ·å…‹è¯­ï¼‰ï¼š'
- en: 'Find variable symbol of creditor.This number has exactly 9 consecutive digits
    0â€“9 without letters or other special characters. It is usually preceded by one
    of the following abbreviations: â€˜ev.Ä.â€™, â€˜zn. oprâ€™, â€˜VS. Oâ€™, â€˜evid. Ä. opr.â€™.
    On the contrary, Iâ€™m not interested in a transaction number with the abbreviation
    â€˜Ä.j.â€™. This number does not appear often in documents, it may happen that you
    will not be able to find it, then write â€˜cannot findâ€™. If youâ€™re not sure, write
    â€˜not sureâ€™.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æŸ¥æ‰¾å€ºæƒäººå˜é‡ç¬¦å·ã€‚è¿™ä¸ªæ•°å­—ç”±æ°å¥½ 9 ä½è¿ç»­æ•°å­—ï¼ˆ0â€“9ï¼‰ç»„æˆï¼Œä¸åŒ…å«å­—æ¯æˆ–å…¶ä»–ç‰¹æ®Šå­—ç¬¦ã€‚é€šå¸¸å‰é¢ä¼šæœ‰ä»¥ä¸‹ç¼©å†™ä¹‹ä¸€ï¼šâ€˜ev.Ä.â€™ã€â€˜zn. oprâ€™ã€â€˜VS.
    Oâ€™ã€â€˜evid. Ä. opr.â€™ã€‚ç›¸åï¼Œæˆ‘å¯¹ç¼©å†™ä¸º â€˜Ä.j.â€™ çš„äº¤æ˜“å·ä¸æ„Ÿå…´è¶£ã€‚è¿™ä¸ªæ•°å­—åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡è¾ƒä½ï¼Œå¯èƒ½ä½ æ‰¾ä¸åˆ°å®ƒï¼Œè‹¥æ— æ³•æ‰¾åˆ°ï¼Œå†™ä¸Šâ€˜cannot
    findâ€™ã€‚å¦‚æœä¸ç¡®å®šï¼Œå†™ä¸Šâ€˜not sureâ€™ã€‚
- en: The model sometimes failed to output the 9-digit format accurately. Post-processing
    would filter out shorter numbers, but there were many false positives 9-digit
    numbers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹æœ‰æ—¶æ— æ³•å‡†ç¡®è¾“å‡º 9 ä½æ•°å­—æ ¼å¼ã€‚åå¤„ç†ä¼šè¿‡æ»¤æ‰è¾ƒçŸ­çš„æ•°å­—ï¼Œä½†ä¹Ÿä¼šå‡ºç°å¾ˆå¤šè¯¯æŠ¥çš„ 9 ä½æ•°å­—ã€‚
- en: Occasionally the model inferred incorrect *birth ID*s based solely on birth
    dates (even with temperature set to 0). On the other hand, it excelled at extracting
    *names*, *surnames*, and *birth dates*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶æ¨¡å‹ä»…åŸºäºå‡ºç”Ÿæ—¥æœŸæ¨æµ‹é”™è¯¯çš„ *å‡ºç”Ÿèº«ä»½è¯å·*ï¼ˆå³ä½¿æ¸©åº¦è®¾ç½®ä¸º 0ï¼‰ã€‚å¦ä¸€æ–¹é¢ï¼Œæ¨¡å‹åœ¨æå– *å§“å*ã€*å§“æ°* å’Œ *å‡ºç”Ÿæ—¥æœŸ* æ–¹é¢è¡¨ç°ä¼˜ç§€ã€‚
- en: Overall, even in my previous experiments, **I found that LLMs** (at the time
    of writing) **perform better with general tasks but lack accuracy and reliability
    for specific or unconventional tasks.** The performance in identifying the client
    was fairly similar for both approaches. For internal reasons, the RoBERTa model
    was deployed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œå³ä½¿åœ¨æˆ‘ä¹‹å‰çš„å®éªŒä¸­ï¼Œ**æˆ‘å‘ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰**ï¼ˆåœ¨æ’°å†™æ—¶ï¼‰**åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸Šè¡¨ç°è¾ƒå¥½ï¼Œä½†åœ¨ç‰¹å®šæˆ–éå¸¸è§„ä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œå¯é æ€§è¾ƒå·®**ã€‚åœ¨å®¢æˆ·è¯†åˆ«æ–¹é¢ï¼Œä¸¤ç§æ–¹æ³•çš„è¡¨ç°ç›¸å·®æ— å‡ ã€‚ç”±äºå†…éƒ¨åŸå› ï¼Œæœ€ç»ˆéƒ¨ç½²äº†
    RoBERTa æ¨¡å‹ã€‚
- en: Post-processing
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åå¤„ç†
- en: 'Notably, implementing **post-processing can significantly reduce false positives**,
    enhancing overall performance. Each entity was subject to customized filtering
    and validation rules:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®æ–½ **åå¤„ç†å¯ä»¥æ˜¾è‘—å‡å°‘è¯¯æŠ¥**ï¼Œæé«˜æ•´ä½“æ€§èƒ½ã€‚æ¯ä¸ªå®ä½“éƒ½éµå¾ªå®šåˆ¶çš„è¿‡æ»¤å’ŒéªŒè¯è§„åˆ™ï¼š
- en: '*variable symbol of debtor -* verify 9 digits format'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å€ºåŠ¡äººå˜é‡ç¬¦å·* - éªŒè¯ 9 ä½æ•°å­—æ ¼å¼'
- en: '*birth ID -* enforce XXXXXX/XXX(X) format and check divisibility by eleven'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡ºç”Ÿèº«ä»½è¯å·* - å¼ºåˆ¶ä½¿ç”¨ XXXXXX/XXX(X) æ ¼å¼ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦èƒ½è¢« 11 æ•´é™¤'
- en: '*name* and *surname -* apply lemmatization using MorphoDiTa [5]'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å§“å* å’Œ *å§“æ°* - ä½¿ç”¨ MorphoDiTa [5] è¿›è¡Œè¯å½¢è¿˜åŸ'
- en: '*date of birth -* enforce DD.MM.YYYY format'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å‡ºç”Ÿæ—¥æœŸ* - å¼ºåˆ¶ä½¿ç”¨ DD.MM.YYYY æ ¼å¼'
- en: Conclusion
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The fine-tuned model was successfully deployed and **performs superbly**, **exceeding
    expectations given the modest dataset** of 710 documents.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¾®è°ƒåçš„æ¨¡å‹æˆåŠŸéƒ¨ç½²ï¼Œ**è¡¨ç°å“è¶Š**ï¼Œ**åœ¨ä»…æœ‰ 710 ç¯‡æ–‡æ¡£çš„ modest æ•°æ®é›†ä¸Šè¶…å‡ºé¢„æœŸè¡¨ç°**ã€‚
- en: While LLMs show promise for general tasks, they lack the accuracy and reliability
    for specialized tasks. That said, itâ€™s likely that in the near future, even fine-tuning
    will become unnecessary for all but highly specialized cases as LLMs continue
    to improve.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸€èˆ¬ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ï¼Œä½†å®ƒä»¬åœ¨ä¸“ä¸šä»»åŠ¡ä¸Šçš„å‡†ç¡®æ€§å’Œå¯é æ€§ä¸è¶³ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œéšç€å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­æ”¹è¿›ï¼Œæœªæ¥å³ä¾¿æ˜¯å¾®è°ƒï¼Œé™¤éæ˜¯é«˜åº¦ä¸“ä¸šåŒ–çš„ä»»åŠ¡ï¼Œå¦åˆ™å¯èƒ½å˜å¾—ä¸å†å¿…è¦ã€‚
- en: '**Acknowledgments** I would like to thank [Martin](https://www.linkedin.com/in/martin-munch/),
    [TomÃ¡Å¡](https://www.linkedin.com/in/tomas-duricek/) and [Petr](https://www.linkedin.com/in/petr-petras-37b2b0160/)
    for their valuable suggestions for the improvement of this article.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**è‡´è°¢** æˆ‘æƒ³æ„Ÿè°¢ [Martin](https://www.linkedin.com/in/martin-munch/)ã€[TomÃ¡Å¡](https://www.linkedin.com/in/tomas-duricek/)
    å’Œ [Petr](https://www.linkedin.com/in/petr-petras-37b2b0160/) å¯¹æœ¬æ–‡ç« æå‡ºçš„å®è´µå»ºè®®ã€‚'
- en: '**Sources** [1] Hugging Face, [Transformers - Token classification](https://huggingface.co/docs/transformers/tasks/token_classification#preprocess)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¥æº** [1] Hugging Faceï¼Œ[Transformers - Token classification](https://huggingface.co/docs/transformers/tasks/token_classification#preprocess)'
- en: '[2] Hugging Face, [NLP Course â€” Token classification](https://huggingface.co/learn/nlp-course/chapter7/2)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Hugging Faceï¼Œ[NLP Course â€” Token classification](https://huggingface.co/learn/nlp-course/chapter7/2)'
- en: '[3] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman,
    E. Grave, M. Ott, L. Zettlemoyer and V. Stoyanov, [Unsupervised Cross-lingual
    Representation Learning at Scale](https://arxiv.org/abs/1911.02116) (2019), CoRR
    abs/1911.02116'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman,
    E. Grave, M. Ott, L. Zettlemoyer å’Œ V. Stoyanovï¼Œ[æ— ç›‘ç£è·¨è¯­è¨€è¡¨ç¤ºå­¦ä¹  at Scale](https://arxiv.org/abs/1911.02116)ï¼ˆ2019ï¼‰ï¼ŒCoRR
    abs/1911.02116'
- en: '[4] M. KociÃ¡n, J. NÃ¡plava, D. Å tancl and V. Kadlec, [Siamese BERT-based Model
    for Web Search Relevance Ranking Evaluated on a New Czech Dataset](https://arxiv.org/abs/2112.01810)
    (2021)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] M. KociÃ¡n, J. NÃ¡plava, D. Å tancl å’Œ V. Kadlecï¼Œ[åŸºäºSiamese BERTçš„ç½‘ç»œæœç´¢ç›¸å…³æ€§æ’åºæ¨¡å‹ï¼Œåœ¨æ–°çš„æ·å…‹æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°](https://arxiv.org/abs/2112.01810)ï¼ˆ2021ï¼‰'
- en: '[5] J. StrakovÃ¡, M. Straka and J. HajiÄ . [Open-Source Tools for Morphology,
    Lemmatization, POS Tagging and Named Entity Recognition](http://www.aclweb.org/anthology/P/P14/P14-5003.pdf)
    (2014), In Proceedings of 52nd Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations, pages 13â€“18, Baltimore, Maryland, June 2014\.
    Association for Computational Linguistics.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] J. StrakovÃ¡, M. Straka å’Œ J. HajiÄ. [ç”¨äºå½¢æ€å­¦ã€è¯å½¢è¿˜åŸã€è¯æ€§æ ‡æ³¨å’Œå‘½åå®ä½“è¯†åˆ«çš„å¼€æºå·¥å…·](http://www.aclweb.org/anthology/P/P14/P14-5003.pdf)ï¼ˆ2014ï¼‰ï¼Œå‘è¡¨äºã€Šç¬¬52å±Šè®¡ç®—è¯­è¨€å­¦åä¼šå¹´ä¼šï¼šç³»ç»Ÿå±•ç¤ºè®ºæ–‡é›†ã€‹ï¼Œç¬¬13â€“18é¡µï¼Œç¾å›½é©¬é‡Œå…°å·å·´å°”çš„æ‘©ï¼Œ2014å¹´6æœˆã€‚è®¡ç®—è¯­è¨€å­¦åä¼šã€‚'
