- en: A Guide to Clustering Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06](https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[![Alex
    Davis](../Images/f773cce9438a68856cb8ba486ac8b051.png)](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    [Alex Davis](https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------)
    ·6 min read·Sep 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Clustering is a must-have skill set for any data scientist due to its utility
    and flexibility to real-world problems. This article is an overview of clustering
    and the different types of clustering algorithms.*'
  prefs: []
  type: TYPE_NORMAL
- en: What is Clustering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a popular unsupervised learning technique that is designed to
    group objects or observations together based on their similarities. Clustering
    has a lot of useful applications such as market segmentation, recommendation systems,
    exploratory analysis, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3aa1508082c9b47231972b9af15633a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: While clustering is a well-known and widely used technique in the field of data
    science, some may not be aware of the different types of clustering algorithms.
    While there are just a few, it is important to understand these algorithms and
    how they work to get the best results for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Centroid-Based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Centroid-based clustering is what most think of when it comes to clustering.
    It is the “traditional” way to cluster data by using a defined number of centroids
    (centers) to group data points based on their distance to each centroid. The centroid
    ultimately becomes the mean of it’s assigned data points. While centroid-based
    clustering is powerful, it is not robust against outliers, as outliers will need
    to be assigned to a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-Means is the most widely used clustering algorithm, and is likely the first
    one you will learn as a data scientist. As explained above, the objective is to
    minimize the sum of distances between the data points and the cluster centroid
    to identify the correct group that each data point should belong to. Here’s how
    it works:'
  prefs: []
  type: TYPE_NORMAL
- en: A defined number of centroids are randomly dropped into the vector space of
    the unlabeled data (initialization).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each data point measures itself to each centroid (usually using Euclidean distance)
    and assigns itself to the closest one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The centroids relocate to the mean of their assigned data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Steps 2–3 repeat until the ‘optimal’ clusters are produced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/228afd72b5c9d40421a55f06880992fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: K-Means ++
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-Means ++ is an improvement of the initialization step of K-Means. Since the
    centroids are randomly dropped in, there is a chance that more than one centroid
    might be initialized into the same cluster, resulting in poor results.
  prefs: []
  type: TYPE_NORMAL
- en: However K-Means ++ solves this by randomly assigning the first centroid that
    will eventually find the largest cluster. Then, the other centroids are placed
    a certain distance away from the initial cluster. The goal of K-Means ++ is to
    push the centroids as far as possible from one another. This results in high-quality
    clusters that are distinct and well-defined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Density-Based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Density-based algorithms are also a popular form of clustering. However, instead
    of measuring from randomly placed centroids, they create clusters by identifying
    high-density areas within the data. Density-based algorithms do not require a
    defined number of clusters, and therefore are less work to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: While centroid-based algorithms perform better with spherical clusters, density-based
    algorithms can take arbitrary shapes and are more flexible. They also do not include
    outliers in their clusters and therefore are robust. However, they can struggle
    with data of varying densities and high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/900c97181437cb771ab90dc1c2a7240b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'DBSCAN is the most popular density-based algorithm. DBSCAN works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN randomly selects a data point and checks if it has enough neighbors within
    a specified radius.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the point has enough neighbors, it is marked as part of a cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DBSCAN recursively checks if the neighbors also have enough neighbors within
    the radius until all points in the cluster have been visited.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1–3 until the remaining data point do not have enough neighbors
    in the radius.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remaining data points are marked as outliers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Hierarchical Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we have hierarchical clustering. This method starts off by computing a
    distance matrix from the raw data. This distance matrix is best and often visualized
    by a dendrogram (see below). Data points are linked together one by one by finding
    the nearest neighbor to eventually form one giant cluster. Therefore, a cut-off
    point to identify the clusters by stopping all data points from linking together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e180fc95c23f18fec38023cc80147e63.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: By using this method, the data scientist can build a robust model by defining
    outliers and excluding them in the other clusters. This method works great against
    hierarchical data, such as taxonomies. The number of clusters depends on the depth
    parameter and can be anywhere from 1-n.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Distribution Based Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lastly, distribution-based clustering considers a metric other than distance
    and density, and that is probability. Distribution-based clustering assumes that
    the data is made up of probabilistic distributions, such as normal distributions.
    The algorithm creates ‘bands’ that represent confidence intervals. The further
    away a data point is from the center of a cluster, the less confident we are that
    the data point belongs to that cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9c66c47eb6422b7014a43d6bd01a57b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Distribution-based clustering is very difficult to implement due to the assumptions
    it makes. It usually is not recommended unless rigorous analysis has been done
    to confirm its results. For example, using it to identify customer segments in
    a marketing dataset, and confirming these segments follow a distribution. This
    can also be a great method for exploratory analysis to see not only what the centers
    of clusters comprise of, but also the edges and outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is an unsupervised machine learning technique that has a growing
    utility in many fields. It can be used to support data analysis, segmentation
    projects, recommendation systems, and more. Above we have explored how they work,
    their pros and cons, code samples, and even some use cases. I would consider experience
    with clustering algorithms a must-have for data scientists due to their utility
    and flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: '*I hope you have enjoyed my article! Please feel free to comment, ask questions,
    or request other topics.*'
  prefs: []
  type: TYPE_NORMAL
