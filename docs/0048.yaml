- en: Solving Reasoning Problems with LLMs in 2023
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06](https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[![Zhaocheng
    Zhu](../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png)](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    [Zhaocheng Zhu](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    ¬∑17 min read¬∑Jan 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs the beginning of 2024 and ChatGPT just celebrated its one-year birthday.
    One year is a super long time for the community of large language models, where
    a myriad of interesting works have taken place. Let‚Äôs revisit the progress and
    discuss topics for the coming year.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d47fb72b435abfef88d39b07d7a27d8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The School of Agents: LLMs are retrieving knowledge from textbooks and performing
    reasoning. Image by authors & DALL¬∑E 3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *(Intel AI Lab),* [*Abulhair Saparov*](https://asaparov.org/) *(New York University),*
    [*Shibo Hao*](https://twitter.com/Ber18791531) *(UC San Diego) and* [*Yihong Chen*](https://twitter.com/yihong_thu)
    *(University College London & Meta AI Research). Many insights in this post were
    formed during the fruitful discussions with* [*Emily Xue*](https://www.linkedin.com/in/yuan-emily-xue-3483012/)
    *(Google),* [*Hanjun Dai*](https://twitter.com/hanjundai) *(Google DeepMind) and*
    [*Bruno Ribeiro*](https://twitter.com/brunofmr) *(Purdue University).*'
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#2669)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tool Use](#0b5d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. [In-context learning enables using more tools](#1acd)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2\. [Most used tools: code interpreters and retrievers](#89ee)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. [Let LLMs create their own tools](#9108)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Reasoning](#d8e6)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. [Planning](#d2e7)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. [Self series](#3582)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. [Evaluations and observations](#0f22)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What needs to be solved in 2024?](#7e91)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'üî• Large language models (LLMs) must be the hottest topic in 2023\. At the NeurIPS
    conference last month, the recurring topics in social events were: 1) what research
    are we doing with/for LLMs? 2) how can my research be integrated with LLMs? 3)
    what is the best strategy to shift from XXX to LLMs? 4) what research can we do
    as a [GPU-poor](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)
    group? The reason is that everyone was informed about the groundbreaking news
    of LLMs on X, Discord, Slack, and everywhere else.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a look at the language model papers on arXiv, there is a leap from
    2,837 to 11,033 in 2023, which breaks the linear trend from 2019 to 2022\. Papers
    in the past year can be roughly clustered into 3 major categories: 1Ô∏è‚É£ pretraining
    and alignment; 2Ô∏è‚É£ tool use and reasoning; 3Ô∏è‚É£ systems and serving. As the title
    indicates, **this post will focus on the progress of LLM research on tool use
    and reasoning.** We picked ~20 üëÄ mind-blowing üëÄ papers and summarized their insights
    and implications. By no means can this post be a comprehensive summary of all
    the achievements made by the community. Feel free to comment on any topics we
    missed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot made by authors & ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'This post is composed of two topics: **tool use** and **reasoning**.'
  prefs: []
  type: TYPE_NORMAL
- en: Tool use is more about how to solve reasoning problems by equipping LLMs with
    **external** tools, such as retrievers, search engines, and code interpreters.
    While tool use is not essential for building strong AI (see Yann‚Äôs classification
    below), tool use provides a practical solution to many applications when domain-specific
    tools are easily accessible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By contrast, reasoning focuses on solving complex problems with the **internal**
    reasoning capacities of LLMs. Research on reasoning tries to figure out the limit
    of the capabilities that LLMs possess and approaches to push that limit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There isn‚Äôt a strict dichotomy between the two topics, as we will see in the
    rest of this post.
  prefs: []
  type: TYPE_NORMAL
- en: Yann LeCun‚Äôs classification of retrieval and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Tool Use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In-context learning enables using more tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**‚û°Ô∏è** One limitation of LLM tool usage is the necessity of sufficient human
    annotations. Whenever we want to teach an LLM to use a tool, we need enough annotated
    tool calls to finetune the LLM. In the [Toolformer](https://arxiv.org/abs/2302.04761)
    paper by Meta, the authors use in-context learning to create a model that annotates
    tool calls for the input query. This model is then used to generate tool calls
    on an unllabeled dataset. While the generations may be far from perfect, incorrect
    calls can be filtered by executing the tools and filtering the outputs based on
    the ground truth answer. The correct calls are collected and used to finetune
    the model. In this way, we can teach Transformers to use any tool based on a conventional
    dataset and merely 5 additional annotations ‚Äî easy work for any engineer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2413879f08a9fefbd39a955d6044af62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Automatic annotation of tool calls. Source: [Schick et al.](https://arxiv.org/abs/2302.04761)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è [Lu et al.](https://arxiv.org/abs/2304.09842) proposed **Chameleon** ü¶é to
    compose tools for multi-step reasoning. The core idea is to use an LLM to decompose
    the question into a sequence of tools, and then generate the arguments for each
    tool call. Both steps are implemented by few-shot prompts. Such an idea is reminiscent
    of [Neural Module Networks (NMNs)](https://arxiv.org/abs/1511.02799) from 2016,
    which decompose a question into sub tasks and learn a module for each sub task.
    The main obstacle of NMNs is that they are hard to train without annotations of
    the decompositions (see [this study](https://arxiv.org/abs/1811.12889)). Fortunately,
    this is not a problem in pretrained LLMs. With in-context learning, Chameleon
    can generate different compositions of tool calls to solve a problem. [A similar
    idea on visual reasoning](https://arxiv.org/abs/2211.11559) got the best paper
    award in CVPR this year.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Chameleon for multi-step tool use. Source: [Lu et al.](https://arxiv.org/abs/2304.09842)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è While in-context learning is highly efficient compared to traditional methods,
    it faces certain limitations, such as difficulty in managing a large array of
    tools. Addressing this, [Hao et al.](https://arxiv.org/abs/2305.11554) introduced
    **ToolkenGPT**, which augments a frozen LLM with new token embeddings specifically
    for tools, termed ‚Äútoolkens‚Äù. This technique was originally used in [multilingual
    language models](https://arxiv.org/abs/1910.11856) to accomodate a new language.
    ToolkenGPT allows tool calling during inference in the same way as next word token
    prediction. It demonstrates the capacity to handle over 200 tools while being
    cost-efficient, establishing a new effectiveness-efficiency trade-off compared
    to LoRA finetuning. Similar ideas are also integrated into multi-modal LLMs for
    [robotic actions](https://arxiv.org/abs/2307.15818) and [image generation](https://arxiv.org/abs/2309.11499).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ToolkenGPT for massive tool use. Source: [Hao et al.](https://arxiv.org/abs/2305.11554)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most used tools: code interpreters and retrievers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you ask us which tools are most generally applicable to reasoning tasks,
    we would say they are **code interpreters** and **retrievers**. Code interpreters
    are probably the most expressive environment that humans have invented for logic
    and computation. Retrievers are a good complement to the parametric knowledge
    of LLMs when a question or assumed knowledge is out of their training distribution.
    Let‚Äôs see how these tools can be used by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è One common failure of [chain-of-thought (CoT)](https://arxiv.org/abs/2201.11903)
    is that LLMs fail to perform arithmetic operations. In [program-aided language
    modeling (PAL)](https://arxiv.org/abs/2211.10435) and [program-of-thoughts (PoT)](https://arxiv.org/abs/2211.12588)
    prompting, the authors prompt a code language model with programs to solve math
    problems. One may insert standard chain-of-thought texts as comments in the programs.
    The final answer is then generated by executing a Python interpreter. The insight
    behind these methods is that the code interpreter provides a perfect tool for
    all kinds of calculation, reducing failure cases to only incorrect reasoning.
    Code-style prompts are also commonly used in [planning tasks](https://arxiv.org/abs/2305.16653).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparison between CoT and PAL. Source: [Gao and Madaan et al.](https://arxiv.org/abs/2211.10435)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Retrievers are commonly used as preprocessing tools for LLMs to augment the
    question with relevant documents, often referred to as [retrieval-augmented generation
    (RAG)](https://arxiv.org/abs/2005.11401). However, when it comes to multi-step
    question answering, it is challenging to select the correct documents based on
    the question alone. In the **IRCoT** proposed by [Trivedi et al.](https://arxiv.org/abs/2212.10509),
    the authors interleave thought generation and knowledge retrieval. Whenever the
    LLM generates a thought sentence, IRCoT uses the sentence to retrieve documents
    from the corpus. The documents are prepended to the prompt to augment later generations.
    Even with a weak retriever like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25),
    IRCoT outperforms one-step RAG on several open-domain question answering benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)'
  prefs: []
  type: TYPE_IMG
- en: 'IRCoT that interleaves CoT and knowledge retrieval. Source: [Trivedi et al.](https://arxiv.org/abs/2212.10509)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è [Yang et al.](https://arxiv.org/abs/2306.15626) presented a novel usage of
    RAG for theorem proving. They built a gym-like environment **LeanDojo** üèØ based
    on the proof assistant Lean. [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))
    is an interactive programming environment where its compiler can verify whether
    the written proof proved the goal. It also includes numerous proved theorems in
    its standard libraries, similar to STL in C++. The cool thing is that because
    proofs are constructed by decomposing theorems into known premises, theorem proving
    can benefit from RAG. Given a theorem, we retrieve the relevant premises from
    the standard libraries and then ask an LLM to generate a proof step. The authors
    show that RAG requires much less training resources and generalizes better to
    novel premises.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Proof of a simple logical theorem in Lean. Source: [Xena Project](https://www.youtube.com/watch?v=POHVMMG7pqE)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Finally, [DSPy](https://github.com/stanfordnlp/dspy) by [Khattab et al.](https://arxiv.org/abs/2310.03714)
    presents a novel approach towards programming LLMs where the framework can actually
    improve the prompts over time and combine prompting techniques (CoT, PoT) with
    retrieval automatically. Further, DSPy introduces *teleprompters* for optimizing
    the prompts and bootstrapping new ones. It‚Äôs hard to fit a description of DSPy
    in one paragraph ‚Äî it‚Äôs not your average RAG technique, but rather an evolution
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: Let LLMs create their own tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Tool use has an inherent limitation: it relies on the presence of tools for
    a specific task. In nature, tool use is not an exclusive skill of humans, as many
    other animals can also use tools. However, what distinguishes humans from other
    animals is the ability to **create** tools. In 2023, we‚Äôve seen a few preliminary
    works exploring the ability of tool making in LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è In LLMs as tool makers (**LATM**) proposed by [Cai et al.](https://arxiv.org/abs/2305.17126),
    the authors prompt an LLM to craft tools in the form of Python functions for a
    given task. The tools are then verified on a few samples, similar to how engineers
    solve problems on LeetCode. Once some tools pass the verification test, they are
    wrapped with documentation strings generated by an LLM to describe their usage.
    At test time, an LLM is prompted to dispatch the question to one of the tools
    at hand, and execute the tool based on the usage. LATM significantly outperforms
    CoT on a wide range of reasoning tasks in BigBench.
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è [Voyager](https://arxiv.org/abs/2305.16291) brought the idea of tool making
    to the world of Minecraft, and came up with incredible results. The core idea
    of Voyager is to use an LLM to propose tasks based on existing skills and world
    state. Then the LLM is prompted to synthesize code (i.e. skills) to solve the
    tasks. The skills are refined based on environment feedback and mastered skills
    are committed to an external memory. Because new skills are built on top of existing
    skills, this significantly reduced the difficulty of learning a complex skill
    (e.g. building a diamond tool in Minecraft). While the idea of learning a library
    of skills can be traced back to [DreamCoder](https://arxiv.org/abs/2006.08381),
    Voyager demonstrates the superiority of GPT-4 in searching over skills in a challenging
    open-world game. Take a look at [the fancy demos](https://voyager.minedojo.org/)
    from the paper!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Minecraft items and skills discovered over time. Source: [Wang et al.](https://arxiv.org/abs/2305.16291)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚û°Ô∏è Both of the above works craft tools as code. In fact, tools can be in natural
    language, too. (shameless self-promotion) In the hypothesis-to-theories (**HtT**)
    work from [Zhu et al.](https://arxiv.org/abs/2310.07064), the authors show that
    we can use LLMs to induce a library of textual rules from a standard multi-step
    reasoning training set. The insight is that among all the rules that LLMs produce
    for different samples, rules that occur and lead to correct answers more often
    are likely to be correct. We then collect the rules and prepend them to a standard
    CoT prompt to perform deduction and get the answer. One interesting aspect about
    HtT is that it can be viewed as a novel way of learning: instead of learning model
    parameters, we learn a library of rules, which works well with black-box LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e30b407b2550861646906f5e0ad43916.png)'
  prefs: []
  type: TYPE_IMG
- en: 'HtT that learns textual rules for multi-hop reasoning. Source: [Zhu et al.](https://arxiv.org/abs/2310.07064)'
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One drawback for CoT-style reasoning is that LLMs have to greedily decode a
    path towards an answer. This is problematic for complex problems like math questions
    or games, since it is hard to predict a path without trial-and-error. In 2023,
    the community made some progress on this issue with new frameworks that enable
    planning with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '‚û°Ô∏è If we conceptualize CoT as ‚Äúsystem 1‚Äù reasoning ‚Äî characterized by its automatic,
    unconscious nature ‚Äî then a question arises: Is it feasible to replicate the more
    conscious ‚Äúsystem 2‚Äù reasoning of humans using LLMs? This query finds relevance
    in two methodologies: [reasoning-via-planning (RAP)](https://arxiv.org/abs/2305.14992)
    and [tree-of-thoughts (ToT)](https://arxiv.org/abs/2305.10601). Both empower LLMs
    to navigate through possible reasoning steps, and to search for the optimal reasoning
    chain based on specific evaluations. RAP additionally prompts an LLM as a ‚Äúworld
    model‚Äù, which predicts the next states following actions. This enables the LLM
    to operate within a self-simulated world, as opposed to interacting with an external
    environment. Both algorithms are available in the [LLM Reasoners](https://github.com/Ber666/llm-reasoners/)
    library now!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'RAP that repurposes LLMs as an agent and a world model. Source: [Hao et al.](https://arxiv.org/abs/2305.14992)'
  prefs: []
  type: TYPE_NORMAL
- en: Self series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self series are a family of techniques that replace human efforts with LLM predictions
    in the loop of LLM development. The year of 2023 has witnessed quite a few papers
    on this track. Let‚Äôs take a closer look at some representative works.
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Many people have the experience that ChatGPT doesn‚Äôt provide the desired
    output on the first trial, and this sometimes can be fixed by pointing out its
    mistake. [Self-debugging](https://arxiv.org/abs/2304.05128) and [self-refinement](https://arxiv.org/abs/2303.17651)
    automate this procedure by replacing human feedback with machine feedback. The
    feedback either comes from a program executor or an LLM that compares the generation
    with the explanation of the problem. One key observation is that the performance
    of self-refine depends on the quality of the feedback, where stronger base models
    that provide better feedback benefit more. Such iterative refinement methods have
    also been shown to be super effective in [pose estimation](https://arxiv.org/abs/1507.06550)
    and [protein structure prediction](https://www.nature.com/articles/s41586-021-03819-2),
    where it is difficult to predict the structure in a single run.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b3c4d3580bf9a786395a972b481534c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Illustration of Self-Debugging. Source: [Chen et al.](https://arxiv.org/abs/2304.05128)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è In the memory-of-thought (**MoT**) framework from [Li and Qiu](https://arxiv.org/abs/2305.05181),
    the authors ask an LLM to generate CoT rationales on an unlabelled dataset and
    use them for RAG. You may ask how this can be useful given that the generated
    rationales often contain errors. The key trick is to filter the rationales based
    on majority vote or entropy minimization (a similar idea is used in [Wan et al.](https://arxiv.org/abs/2305.14106)
    to filter rationales). Once we have good rationales on the unlabelled dataset,
    we dynamically retrieve few-shot examples based on the test question, which is
    shown to be much better than fixed few-shot examples. MoT can be interpreted as
    converting a parametric model to a non-parametric model without additional supervision.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07dcece1803feb415c68d2727bc2c037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'MoT that generates and recalls memory. Source: [Li and Qiu](https://arxiv.org/abs/2305.05181).'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Going beyond MoT, [Yasunaga et al.](https://arxiv.org/abs/2310.01714) proposed
    **analogical prompting** that eliminates the need of dumping rationales on an
    unlabeled dataset. Analogical prompting asks an LLM to recall relevant exemplars
    based on the question, and thereby generates dynamic few-shot exemplars from scratch.
    In fact, the authors found that analogical prompting is an emergent ability in
    large language models, similar to previous works on [open-domain question answering](https://arxiv.org/abs/2209.10063).
    Larger-scale LLMs can self-generate better exemplars compared to standard RAG
    solutions. Besides, this work provides a cool trick to fuse multi-step generations
    into a single prompt with markdown grammar ‚Äî a godsend for prompt engineers with
    a tight budget! üí°
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cff6c1216d04791bb9d00343530b5700.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Analogical prompting. Source: [Yasunaga et al.](https://arxiv.org/abs/2310.01714)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Are self-refine and self-generate the limit of LLM reasoning? [Yang et al.](https://arxiv.org/abs/2309.03409)
    show a more advanced usage of the reasoning abilities of LLMs ‚Äî to optimize a
    prompt based on the history of generated prompts. This is a cool reinvention of
    the famous meta-learning paper ‚Äú[Learning to learn by gradient descent by gradient
    descent](https://arxiv.org/abs/1606.04474)‚Äù, but all the steps here are performed
    by LLMs on text. At each step, an LLM is prompted with previous solutions and
    corresponding performance metrics and tries to predict a new solution. Notably,
    even without telling the LLM how to perform optimization, the LLM can gradually
    find better solutions that maximize the metric. Maybe this work brings prompt
    engineers one step closer to unemployment?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c7ff40222823804440ab08816082b7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Performance of prompts optimized by LLM. Source: [Yang et al.](https://arxiv.org/abs/2309.03409)'
  prefs: []
  type: TYPE_NORMAL
- en: üîÅ Probably the most eye-opening üëÄ work in self series is the self-taught optimizer
    (**STOP**) by [Zelikman et al.](https://arxiv.org/abs/2310.02304) We know LLMs
    are guided by textual prompts, take texts as input and output texts. While these
    these texts are usually separate variables, what will happen if we model them
    as a single variable? In STOP, the authors draw inspiration from [self-modifying
    code](https://en.wikipedia.org/wiki/Self-modifying_code) and use a self-improvement
    prompt to improve itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The seed improver that improves itself in STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  prefs: []
  type: TYPE_NORMAL
- en: While the seed prompt isn‚Äôt more complicated than a random search algorithm,
    with a strong LLM, one can discover many advanced meta-heuristic algorithms. Interestingly,
    GPT-4 discovers many prompting strategies that are published after the training
    cutoff for GPT-4, including [ToT](https://arxiv.org/abs/2305.10601) and [Parsel](https://arxiv.org/abs/2212.10561).
    It seems that the day when LLMs conduct research for themselves is approaching.
    One step in this direction is a recent work by [Huang et al.](https://arxiv.org/abs/2310.03302)
    showing that LLMs are capable of designing ML models for common benchmarks and
    even Kaggle challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e030d2aa8360e5a443b744b6852437af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Algorithms found by STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations and observations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ‚û°Ô∏è [Kandpal et al.](https://arxiv.org/abs/2211.08411) conducted a systematic
    study on the memorization ability of LLMs. They asked an LLM about factual questions
    from Wikipedia and found that the accuracy is highly correlated with the frequency
    of questioned entities in the pretraining documents, regardless of the scale of
    the model. By extrapolating the trend, the authors estimate that a model with
    10¬π‚Å∏ is needed to match human performance on long-tail entities ‚Äî which is way
    bigger than today‚Äôs LLMs. Hence an important takeaway is to use LLM reasoning
    for tasks related to frequent knowledge, and consider RAG or other tools for tasks
    related to long-tail knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7500b22b9573287416b537dcd301dcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LLMs can hardly memorize long-tail knowledge. Source: [Kandpal et al.](https://arxiv.org/abs/2211.08411)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è As the community tries to build bigger mixtures for training LLMs, one concern
    is that LLMs may not learn to actually reason but simply to memorize the solutions
    from the training distribution, just like humans in [teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test).
    [Wu et al.](https://arxiv.org/abs/2307.02477) answers this concern by comparing
    the performance of GPT-4 with zero-shot CoT on 11 different tasks, each with a
    default setting and a counterfactual setting. They observe that despite LLMs performing
    better than random in the counterfactual settings, their performance is consistently
    behind that in the default settings. It remains an open question how we can train
    models to focus more on reasoning rather than memorization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03f7be28616d30d4236ececb57c2aff3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GPT-4 underperforms on counterfactual variants. Source: [Wu et al.](https://arxiv.org/abs/2307.02477)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è [Saparov et al.](https://arxiv.org/abs/2305.15269) extended a synthetic dataset
    [PrOntoQA](https://arxiv.org/abs/2210.01240) to OOD setting to test generalization
    ability of LLMs on deductive reasoning with controlled depth, width, compositional
    structure, etc. The authors found that CoT can generalize to compositional and
    longer proofs. This is in contrast with previous conclusions on [compositional
    semantic parsing](https://arxiv.org/abs/2205.12253), possibly because deductive
    reasoning only requires composing deduction steps, while semantic parsing additionally
    deals with growing outputs. While LLMs are able to use most deduction rules, they
    require explicit demonstrations of [*proof by cases*](https://en.wikipedia.org/wiki/Disjunction_elimination)
    and [*proof by contradiction*](https://en.wikipedia.org/wiki/Proof_by_contradiction).
    There are also counterintuitive qualitative differences between in-context learning
    and supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37163127677f52ac47a0d2999eb0ebd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'OOD generalization over deductive reasoning. Source: [Saparov et al.](https://arxiv.org/abs/2305.15269)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚û°Ô∏è Regarding the parametric knowledge in LLMs, [Berglund et al.](https://arxiv.org/abs/2309.12288)
    found a phenomenon they called the *reversal curse*. That is, LLMs trained to
    memorize ‚ÄúA is B‚Äù do not know that ‚ÄúB is A‚Äù in closed-book question answering,
    despite the fact that they can be prompted to perform deductive reasoning. This
    indicates that LLMs lack certain kinds of symmetry in its parametric knowledge,
    and it is crucial to endow them with such symmetry to enable better generalization.
    Actually, the knowledge graph community has been a leader in this area, with works
    like [double permutation equivariance](https://arxiv.org/abs/2302.01313) and [relational
    rotation](https://arxiv.org/abs/1902.10197). It would be interesting to see how
    these ideas are adapted to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: What needs to be solved in 2024?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2023 has been an exciting year for tool use and reasoning, and we expect the
    new year to be more exciting. Let‚Äôs wrap up this post with predictions from the
    authors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhaocheng Zhu:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Reasoning with LLMs still requires ad-hoc engineering efforts for each specific
    task. By contrast, once humans acquire the skills for a task, they can quickly
    adapt the skills to similar tasks with very few or even no samples (e.g. from
    chess to poker). If we can create LLM solutions that generalize across tasks,
    it will save a lot of engineering efforts and boost the performance in low-resource
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Solving reasoning problems usually involves a lot of commonsense knowledge,
    ranging from math, physics to strategies like enumeration and proof by contradiction,
    if any. While LLMs may have obtained such knowledge from their training data,
    we lack precise control over the parameteric knowledge in LLMs. We would like
    to see new studies on the knowledge representations of LLMs, and techniques that
    verbalize, inject or delete knowledge in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Michael Galkin:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ In 2023, we saw an increasing effort to understand the basic principles
    of *what can be learned* by Transformer-based LLMs ‚Äî can we actually expect LLMs
    to be able to solve any arbitrary reasoning tasks? A few famous papers like [faith
    and fate](https://arxiv.org/abs/2305.18654) and [on length generalization](https://arxiv.org/abs/2310.16028)
    suggest that the autoregressive nature of LLMs might not be the optimal way to
    approach complex reasoning. In 2024, I‚Äôd expect more efforts on understanding
    the algorithmic alignment with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ It is likely that in 2024, most open and closed foundation models will be
    multi-modal supporting vision, text, audio, and other inputs. Incorporating other
    modalities into reasoning is the natural next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Abulhair Saparov:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ I anticipate there will be more efforts to find a more mechanistic understanding
    of reasoning in LLMs. What algorithms do they use when performing reasoning tasks?
    More precisely to what extent do they exploit shortcuts or heuristics that hurt
    robustness/generalization?
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Relatedly, I would expect researchers to make progress in answering whether
    increasing the scale of LLMs and/or their training will resolve their limitations
    in reasoning, or whether these limitations are fundamental, e.g. inherent to the
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shibo Hao:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Over the past year, the primary focus of LLM reasoning research has been
    on prompting and supervised fine-tuning, with some approaches, like [STaR](https://arxiv.org/abs/2203.14465),
    [Reflexion](https://arxiv.org/abs/2303.11366), and [RAP](https://arxiv.org/abs/2305.14992),
    already drawing inspiration from RL. However, we have yet to witness a breakthrough
    method that effectively employs RL to enhance an LLM‚Äôs reasoning capabilities,
    especially when compared to the advancements in RLHF for alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ On the flip side, in the future, language could become the primary medium
    of expression in RL systems. The key advantage lies in the rich information of
    language compared with the traditional scalar rewards / values. The prospect of
    an LLM agent that can autonomously improve its reasoning abilities with RL (no
    need for supervision data or prompting engineering), is not only exciting but
    may also indicate a significant leap towards AGI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yihong Chen:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1Ô∏è‚É£ Structured & unstructured. I assume LLMs will be gradually eating the pies
    of traditional products, which are mostly based on large databases, rules and
    piles of small classifiers. In this case, what we are referring as ‚ÄúLLM reasoning‚Äù
    is probably the hope that we will have a method ‚ÄúX‚Äù that can bridge the structured
    world, which most product data is currently living in, and the unstructured world,
    which most LLMs are living in. Knowledge graphs kind of champion the structured
    world and there have been fruity research on how to reason well on a knowledge
    graph, while LLMs are championing the unstructured world though we are still unclear
    about how they do the reasoning. They have different advantages and limitations.
    I‚Äôd expect that a nice bridge between the two would lead us to more pragmatic
    solutions for products.
  prefs: []
  type: TYPE_NORMAL
- en: 2Ô∏è‚É£ Sample efficiency. As Zhaocheng mentioned, current LLMs reasoning is hard
    at generalising across a large number of tasks. Instead of ad-hoc efforts, which
    are usually customised for specific problems, I would be interested in if we can
    simply pretrain a LLM that generalize with less data, similar to what‚Äôs done for
    [generalizing across multiple languages](https://arxiv.org/abs/2307.01163).
  prefs: []
  type: TYPE_NORMAL
- en: 3Ô∏è‚É£ Reasoning inside LLMs. As Abulhair and Michael mentioned, the community
    do not have a crystal understanding about how LLMs perform reasoning, if they
    indeed are reasoning. I‚Äôd expect more efforts on reverse-engineer LLMs‚Äô reasoning
    process, either in a [mechanistic way](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    or other interpretability approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Meme Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the tradition of [Michael Galkin](https://mgalkin.medium.com/), no
    blog post is truely complete without a meme. DALL¬∑E 3 is almost a meme wizard
    if it can spell words correctly. Guess what prompts are used for each panel?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c0de490cd0427c4127e390af9d93917.png)'
  prefs: []
  type: TYPE_IMG
- en: What an LLM learned and what it can reason about. Image by authors & DALL¬∑E
    3.
  prefs: []
  type: TYPE_NORMAL
- en: Read More
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If this blog left you wanting to learn more about LLM reasoning, take a look
    at the following awesome blog posts.
  prefs: []
  type: TYPE_NORMAL
- en: '[Towards Complex Reasoning: the Polaris of Large Language Models](https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21)
    by Yao Fu.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
    by Lilian Weng.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
