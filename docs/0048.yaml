- en: Solving Reasoning Problems with LLMs in 2023
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06](https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[![Zhaocheng
    Zhu](../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png)](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    [Zhaocheng Zhu](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)
    Â·17 min readÂ·Jan 6, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s the beginning of 2024 and ChatGPT just celebrated its one-year birthday.
    One year is a super long time for the community of large language models, where
    a myriad of interesting works have taken place. Letâ€™s revisit the progress and
    discuss topics for the coming year.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d47fb72b435abfef88d39b07d7a27d8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: 'The School of Agents: LLMs are retrieving knowledge from textbooks and performing
    reasoning. Image by authors & DALLÂ·E 3.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with* [*Michael Galkin*](https://twitter.com/michael_galkin)
    *(Intel AI Lab),* [*Abulhair Saparov*](https://asaparov.org/) *(New York University),*
    [*Shibo Hao*](https://twitter.com/Ber18791531) *(UC San Diego) and* [*Yihong Chen*](https://twitter.com/yihong_thu)
    *(University College London & Meta AI Research). Many insights in this post were
    formed during the fruitful discussions with* [*Emily Xue*](https://www.linkedin.com/in/yuan-emily-xue-3483012/)
    *(Google),* [*Hanjun Dai*](https://twitter.com/hanjundai) *(Google DeepMind) and*
    [*Bruno Ribeiro*](https://twitter.com/brunofmr) *(Purdue University).*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Introduction](#2669)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tool Use](#0b5d)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. [In-context learning enables using more tools](#1acd)
  id: totrans-12
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2\. [Most used tools: code interpreters and retrievers](#89ee)'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. [Let LLMs create their own tools](#9108)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Reasoning](#d8e6)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. [Planning](#d2e7)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. [Self series](#3582)
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3\. [Evaluations and observations](#0f22)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[What needs to be solved in 2024?](#7e91)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ğŸ”¥ Large language models (LLMs) must be the hottest topic in 2023\. At the NeurIPS
    conference last month, the recurring topics in social events were: 1) what research
    are we doing with/for LLMs? 2) how can my research be integrated with LLMs? 3)
    what is the best strategy to shift from XXX to LLMs? 4) what research can we do
    as a [GPU-poor](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)
    group? The reason is that everyone was informed about the groundbreaking news
    of LLMs on X, Discord, Slack, and everywhere else.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¿…å®šæ˜¯2023å¹´æœ€çƒ­é—¨çš„è¯é¢˜ã€‚åœ¨ä¸Šä¸ªæœˆçš„NeurIPSä¼šè®®ä¸Šï¼Œç¤¾äº¤æ´»åŠ¨ä¸­åå¤è®¨è®ºçš„è¯é¢˜æ˜¯ï¼š1ï¼‰æˆ‘ä»¬æ­£åœ¨åšä»€ä¹ˆå…³äºLLMsçš„ç ”ç©¶ï¼Ÿ
    2ï¼‰æˆ‘çš„ç ”ç©¶å¦‚ä½•ä¸LLMsç»“åˆï¼Ÿ 3ï¼‰ä»XXXè½¬å‘LLMsçš„æœ€ä½³ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ 4ï¼‰ä½œä¸ºä¸€ä¸ª[GPUä¸è¶³](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)çš„å›¢é˜Ÿï¼Œæˆ‘ä»¬èƒ½åšä»€ä¹ˆç ”ç©¶ï¼ŸåŸå› æ˜¯æ¯ä¸ªäººéƒ½é€šè¿‡Xã€Discordã€Slackå’Œå…¶ä»–åœ°æ–¹è·å¾—äº†LLMsçš„çªç ´æ€§æ–°é—»ã€‚
- en: 'If you take a look at the language model papers on arXiv, there is a leap from
    2,837 to 11,033 in 2023, which breaks the linear trend from 2019 to 2022\. Papers
    in the past year can be roughly clustered into 3 major categories: 1ï¸âƒ£ pretraining
    and alignment; 2ï¸âƒ£ tool use and reasoning; 3ï¸âƒ£ systems and serving. As the title
    indicates, **this post will focus on the progress of LLM research on tool use
    and reasoning.** We picked ~20 ğŸ‘€ mind-blowing ğŸ‘€ papers and summarized their insights
    and implications. By no means can this post be a comprehensive summary of all
    the achievements made by the community. Feel free to comment on any topics we
    missed.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æŸ¥çœ‹arXivä¸Šçš„è¯­è¨€æ¨¡å‹è®ºæ–‡ï¼Œ2023å¹´çš„æ•°é‡ä»2,837ç¯‡è·ƒå‡è‡³11,033ç¯‡ï¼Œè¿™æ‰“ç ´äº†2019åˆ°2022å¹´é—´çš„çº¿æ€§è¶‹åŠ¿ã€‚è¿‡å»ä¸€å¹´ä¸­çš„è®ºæ–‡å¤§è‡´å¯ä»¥åˆ†ä¸ºä¸‰å¤§ç±»ï¼š1ï¸âƒ£
    é¢„è®­ç»ƒå’Œå¯¹é½ï¼›2ï¸âƒ£ å·¥å…·ä½¿ç”¨å’Œæ¨ç†ï¼›3ï¸âƒ£ ç³»ç»Ÿå’ŒæœåŠ¡ã€‚æ­£å¦‚æ ‡é¢˜æ‰€ç¤ºï¼Œ**æœ¬æ–‡å°†é‡ç‚¹ä»‹ç»LLMç ”ç©¶åœ¨å·¥å…·ä½¿ç”¨å’Œæ¨ç†æ–¹é¢çš„è¿›å±•ã€‚** æˆ‘ä»¬æŒ‘é€‰äº†å¤§çº¦20ç¯‡
    ğŸ‘€ ä»¤äººéœ‡æƒŠçš„ ğŸ‘€ è®ºæ–‡ï¼Œå¹¶æ€»ç»“äº†å®ƒä»¬çš„è§è§£å’Œå½±å“ã€‚è¿™ç¯‡æ–‡ç« ç»ä¸æ˜¯å¯¹ç¤¾åŒºæ‰€æœ‰æˆå°±çš„å…¨é¢æ€»ç»“ã€‚å¦‚æœæˆ‘ä»¬é—æ¼äº†ä»»ä½•è¯é¢˜ï¼Œè¯·éšæ—¶è¯„è®ºã€‚
- en: '![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)'
- en: Plot made by authors & ChatGPT.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å’ŒChatGPTç»˜åˆ¶çš„å›¾è¡¨ã€‚
- en: 'This post is composed of two topics: **tool use** and **reasoning**.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ç”±ä¸¤ä¸ªä¸»é¢˜ç»„æˆï¼š**å·¥å…·ä½¿ç”¨**å’Œ**æ¨ç†**ã€‚
- en: Tool use is more about how to solve reasoning problems by equipping LLMs with
    **external** tools, such as retrievers, search engines, and code interpreters.
    While tool use is not essential for building strong AI (see Yannâ€™s classification
    below), tool use provides a practical solution to many applications when domain-specific
    tools are easily accessible.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·¥å…·ä½¿ç”¨æ›´å¤šçš„æ˜¯å…³äºå¦‚ä½•é€šè¿‡ä¸ºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é…å¤‡**å¤–éƒ¨**å·¥å…·æ¥è§£å†³æ¨ç†é—®é¢˜ï¼Œä¾‹å¦‚æ£€ç´¢å™¨ã€æœç´¢å¼•æ“å’Œä»£ç è§£é‡Šå™¨ã€‚è™½ç„¶å·¥å…·ä½¿ç”¨å¯¹äºæ„å»ºå¼ºå¤§çš„äººå·¥æ™ºèƒ½å¹¶éå¿…éœ€ï¼ˆè§ä¸‹æ–‡Yannçš„åˆ†ç±»ï¼‰ï¼Œä½†å½“é¢†åŸŸç‰¹å®šå·¥å…·æ˜“äºè·å–æ—¶ï¼Œå·¥å…·ä½¿ç”¨ä¸ºè®¸å¤šåº”ç”¨æä¾›äº†å®é™…çš„è§£å†³æ–¹æ¡ˆã€‚
- en: By contrast, reasoning focuses on solving complex problems with the **internal**
    reasoning capacities of LLMs. Research on reasoning tries to figure out the limit
    of the capabilities that LLMs possess and approaches to push that limit.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ¨ç†åˆ™é›†ä¸­åœ¨åˆ©ç”¨LLMçš„**å†…éƒ¨**æ¨ç†èƒ½åŠ›æ¥è§£å†³å¤æ‚é—®é¢˜ã€‚æ¨ç†ç ”ç©¶è¯•å›¾æ‰¾å‡ºLLMsæ‰€å…·å¤‡èƒ½åŠ›çš„æé™ï¼Œå¹¶æ¢ç´¢çªç ´è¿™ä¸€æé™çš„æ–¹æ³•ã€‚
- en: There isnâ€™t a strict dichotomy between the two topics, as we will see in the
    rest of this post.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤è€…ä¹‹é—´å¹¶æ²¡æœ‰ä¸¥æ ¼çš„äºŒåˆ†æ³•ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æœ¬æ–‡æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­å°†çœ‹åˆ°çš„é‚£æ ·ã€‚
- en: Yann LeCunâ€™s classification of retrieval and reasoning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCunå¯¹æ£€ç´¢ä¸æ¨ç†çš„åˆ†ç±»ã€‚
- en: Tool Use
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·¥å…·ä½¿ç”¨
- en: In-context learning enables using more tools
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿å¾—ä½¿ç”¨æ›´å¤šå·¥å…·æˆä¸ºå¯èƒ½
- en: '**â¡ï¸** One limitation of LLM tool usage is the necessity of sufficient human
    annotations. Whenever we want to teach an LLM to use a tool, we need enough annotated
    tool calls to finetune the LLM. In the [Toolformer](https://arxiv.org/abs/2302.04761)
    paper by Meta, the authors use in-context learning to create a model that annotates
    tool calls for the input query. This model is then used to generate tool calls
    on an unllabeled dataset. While the generations may be far from perfect, incorrect
    calls can be filtered by executing the tools and filtering the outputs based on
    the ground truth answer. The correct calls are collected and used to finetune
    the model. In this way, we can teach Transformers to use any tool based on a conventional
    dataset and merely 5 additional annotations â€” easy work for any engineer.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** LLMå·¥å…·ä½¿ç”¨çš„ä¸€ä¸ªé™åˆ¶æ˜¯éœ€è¦è¶³å¤Ÿçš„äººç±»æ ‡æ³¨ã€‚æ¯å½“æˆ‘ä»¬æƒ³æ•™ä¸€ä¸ªLLMä½¿ç”¨å·¥å…·æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è¶³å¤Ÿçš„æ ‡æ³¨å·¥å…·è°ƒç”¨æ¥å¯¹LLMè¿›è¡Œå¾®è°ƒã€‚åœ¨Metaçš„[Toolformer](https://arxiv.org/abs/2302.04761)è®ºæ–‡ä¸­ï¼Œä½œè€…ä½¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ åˆ›å»ºäº†ä¸€ä¸ªæ¨¡å‹ï¼Œèƒ½å¤Ÿä¸ºè¾“å…¥æŸ¥è¯¢æ ‡æ³¨å·¥å…·è°ƒç”¨ã€‚ç„¶åï¼Œä½¿ç”¨è¯¥æ¨¡å‹åœ¨æœªæ ‡è®°çš„æ•°æ®é›†ä¸Šç”Ÿæˆå·¥å…·è°ƒç”¨ã€‚è™½ç„¶ç”Ÿæˆçš„è°ƒç”¨å¯èƒ½è¿œéå®Œç¾ï¼Œä½†å¯ä»¥é€šè¿‡æ‰§è¡Œå·¥å…·å¹¶æ ¹æ®çœŸå®ç­”æ¡ˆè¿‡æ»¤è¾“å‡ºï¼Œç­›é€‰å‡ºé”™è¯¯çš„è°ƒç”¨ã€‚æ­£ç¡®çš„è°ƒç”¨è¢«æ”¶é›†å¹¶ç”¨äºå¾®è°ƒæ¨¡å‹ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºå¸¸è§„æ•°æ®é›†å’Œä»…ä»…5ä¸ªé¢å¤–æ ‡æ³¨æ¥æ•™ä¼šTransformerä½¿ç”¨ä»»ä½•å·¥å…·â€”â€”è¿™æ˜¯ä»»ä½•å·¥ç¨‹å¸ˆéƒ½èƒ½è½»æ¾å®Œæˆçš„å·¥ä½œã€‚'
- en: '![](../Images/2413879f08a9fefbd39a955d6044af62.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2413879f08a9fefbd39a955d6044af62.png)'
- en: 'Automatic annotation of tool calls. Source: [Schick et al.](https://arxiv.org/abs/2302.04761)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥å…·è°ƒç”¨çš„è‡ªåŠ¨æ³¨é‡Šã€‚æ¥æºï¼š[Schick et al.](https://arxiv.org/abs/2302.04761)
- en: â¡ï¸ [Lu et al.](https://arxiv.org/abs/2304.09842) proposed **Chameleon** ğŸ¦ to
    compose tools for multi-step reasoning. The core idea is to use an LLM to decompose
    the question into a sequence of tools, and then generate the arguments for each
    tool call. Both steps are implemented by few-shot prompts. Such an idea is reminiscent
    of [Neural Module Networks (NMNs)](https://arxiv.org/abs/1511.02799) from 2016,
    which decompose a question into sub tasks and learn a module for each sub task.
    The main obstacle of NMNs is that they are hard to train without annotations of
    the decompositions (see [this study](https://arxiv.org/abs/1811.12889)). Fortunately,
    this is not a problem in pretrained LLMs. With in-context learning, Chameleon
    can generate different compositions of tool calls to solve a problem. [A similar
    idea on visual reasoning](https://arxiv.org/abs/2211.11559) got the best paper
    award in CVPR this year.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ [Lu et al.](https://arxiv.org/abs/2304.09842) æå‡ºäº†**Chameleon** ğŸ¦ï¼Œç”¨äºç»„åˆå¤šæ­¥æ¨ç†çš„å·¥å…·ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å°†é—®é¢˜åˆ†è§£ä¸ºä¸€ç³»åˆ—å·¥å…·è°ƒç”¨ï¼Œç„¶åä¸ºæ¯ä¸ªå·¥å…·è°ƒç”¨ç”Ÿæˆå‚æ•°ã€‚è¿™ä¸¤ä¸ªæ­¥éª¤éƒ½æ˜¯é€šè¿‡å°‘é‡ç¤ºä¾‹æç¤ºæ¥å®ç°çš„ã€‚è¿™ä¸ªæƒ³æ³•è®©äººè”æƒ³åˆ°2016å¹´çš„[ç¥ç»æ¨¡å—ç½‘ç»œï¼ˆNMNsï¼‰](https://arxiv.org/abs/1511.02799)ï¼Œå®ƒå°†é—®é¢˜åˆ†è§£ä¸ºå­ä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡å­¦ä¹ ä¸€ä¸ªæ¨¡å—ã€‚NMNsçš„ä¸»è¦éšœç¢æ˜¯ï¼Œå®ƒä»¬åœ¨æ²¡æœ‰åˆ†è§£æ³¨é‡Šçš„æƒ…å†µä¸‹å¾ˆéš¾è®­ç»ƒï¼ˆè§[è¿™é¡¹ç ”ç©¶](https://arxiv.org/abs/1811.12889)ï¼‰ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨é¢„è®­ç»ƒçš„LLMä¸­ï¼Œè¿™ä¸æ˜¯é—®é¢˜ã€‚é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒChameleonå¯ä»¥ç”Ÿæˆä¸åŒçš„å·¥å…·è°ƒç”¨ç»„åˆæ¥è§£å†³é—®é¢˜ã€‚[ä¸€ç§ç±»ä¼¼çš„è§†è§‰æ¨ç†æ–¹æ³•](https://arxiv.org/abs/2211.11559)åœ¨ä»Šå¹´çš„CVPRä¸Šè·å¾—äº†æœ€ä½³è®ºæ–‡å¥–ã€‚
- en: '![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)'
- en: 'Chameleon for multi-step tool use. Source: [Lu et al.](https://arxiv.org/abs/2304.09842)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Chameleonç”¨äºå¤šæ­¥å·¥å…·ä½¿ç”¨ã€‚æ¥æºï¼š[Lu et al.](https://arxiv.org/abs/2304.09842)
- en: â¡ï¸ While in-context learning is highly efficient compared to traditional methods,
    it faces certain limitations, such as difficulty in managing a large array of
    tools. Addressing this, [Hao et al.](https://arxiv.org/abs/2305.11554) introduced
    **ToolkenGPT**, which augments a frozen LLM with new token embeddings specifically
    for tools, termed â€œtoolkensâ€. This technique was originally used in [multilingual
    language models](https://arxiv.org/abs/1910.11856) to accomodate a new language.
    ToolkenGPT allows tool calling during inference in the same way as next word token
    prediction. It demonstrates the capacity to handle over 200 tools while being
    cost-efficient, establishing a new effectiveness-efficiency trade-off compared
    to LoRA finetuning. Similar ideas are also integrated into multi-modal LLMs for
    [robotic actions](https://arxiv.org/abs/2307.15818) and [image generation](https://arxiv.org/abs/2309.11499).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å°½ç®¡ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ å…·æœ‰è¾ƒé«˜çš„æ•ˆç‡ï¼Œä½†å®ƒä¹Ÿé¢ä¸´æŸäº›é™åˆ¶ï¼Œä¾‹å¦‚ç®¡ç†å¤§é‡å·¥å…·çš„éš¾åº¦ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ[Hao et al.](https://arxiv.org/abs/2305.11554)
    æå‡ºäº†**ToolkenGPT**ï¼Œå®ƒé€šè¿‡ä¸ºå·¥å…·å¼•å…¥æ–°çš„ä»¤ç‰ŒåµŒå…¥ï¼ˆç§°ä¸ºâ€œtoolkensâ€ï¼‰æ¥å¢å¼ºä¸€ä¸ªå†»ç»“çš„LLMã€‚è¯¥æŠ€æœ¯æœ€åˆåœ¨[å¤šè¯­è¨€æ¨¡å‹](https://arxiv.org/abs/1910.11856)ä¸­ç”¨äºé€‚åº”æ–°çš„è¯­è¨€ã€‚ToolkenGPTå…è®¸åœ¨æ¨ç†è¿‡ç¨‹ä¸­åƒä¸‹ä¸€ä¸ªè¯é¢„æµ‹ä¸€æ ·è¿›è¡Œå·¥å…·è°ƒç”¨ã€‚å®ƒå±•ç¤ºäº†å¤„ç†è¶…è¿‡200ä¸ªå·¥å…·çš„èƒ½åŠ›ï¼ŒåŒæ—¶å…·æœ‰æˆæœ¬æ•ˆç›Šï¼Œç›¸è¾ƒäºLoRAå¾®è°ƒï¼Œå»ºç«‹äº†ä¸€ç§æ–°çš„æ•ˆèƒ½ä¸æ•ˆç‡çš„å¹³è¡¡ã€‚ç±»ä¼¼çš„æ€è·¯ä¹Ÿå·²æ•´åˆåˆ°å¤šæ¨¡æ€LLMä¸­ï¼Œç”¨äº[æœºå™¨äººåŠ¨ä½œ](https://arxiv.org/abs/2307.15818)å’Œ[å›¾åƒç”Ÿæˆ](https://arxiv.org/abs/2309.11499)ã€‚
- en: '![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)'
- en: 'ToolkenGPT for massive tool use. Source: [Hao et al.](https://arxiv.org/abs/2305.11554)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ToolkenGPTç”¨äºå¤§è§„æ¨¡å·¥å…·ä½¿ç”¨ã€‚æ¥æºï¼š[Hao et al.](https://arxiv.org/abs/2305.11554)
- en: 'Most used tools: code interpreters and retrievers'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€å¸¸ç”¨çš„å·¥å…·ï¼šä»£ç è§£é‡Šå™¨å’Œæ£€ç´¢å™¨
- en: If you ask us which tools are most generally applicable to reasoning tasks,
    we would say they are **code interpreters** and **retrievers**. Code interpreters
    are probably the most expressive environment that humans have invented for logic
    and computation. Retrievers are a good complement to the parametric knowledge
    of LLMs when a question or assumed knowledge is out of their training distribution.
    Letâ€™s see how these tools can be used by LLMs.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ é—®æˆ‘ä»¬å“ªäº›å·¥å…·æœ€å¹¿æ³›é€‚ç”¨äºæ¨ç†ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¼šè¯´å®ƒä»¬æ˜¯**ä»£ç è§£é‡Šå™¨**å’Œ**æ£€ç´¢å™¨**ã€‚ä»£ç è§£é‡Šå™¨å¯èƒ½æ˜¯äººç±»å‘æ˜çš„æœ€å…·è¡¨ç°åŠ›çš„é€»è¾‘ä¸è®¡ç®—ç¯å¢ƒã€‚æ£€ç´¢å™¨åœ¨LLMçš„å‚æ•°åŒ–çŸ¥è¯†æ— æ³•è¦†ç›–æŸäº›é—®é¢˜æˆ–å·²çŸ¥å‡è®¾çš„æƒ…å†µä¸‹ï¼Œæ˜¯ä¸€ç§å¾ˆå¥½çš„è¡¥å……ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™äº›å·¥å…·æ˜¯å¦‚ä½•è¢«LLMä½¿ç”¨çš„ã€‚
- en: â¡ï¸ One common failure of [chain-of-thought (CoT)](https://arxiv.org/abs/2201.11903)
    is that LLMs fail to perform arithmetic operations. In [program-aided language
    modeling (PAL)](https://arxiv.org/abs/2211.10435) and [program-of-thoughts (PoT)](https://arxiv.org/abs/2211.12588)
    prompting, the authors prompt a code language model with programs to solve math
    problems. One may insert standard chain-of-thought texts as comments in the programs.
    The final answer is then generated by executing a Python interpreter. The insight
    behind these methods is that the code interpreter provides a perfect tool for
    all kinds of calculation, reducing failure cases to only incorrect reasoning.
    Code-style prompts are also commonly used in [planning tasks](https://arxiv.org/abs/2305.16653).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ [æ€ç»´é“¾ï¼ˆCoTï¼‰](https://arxiv.org/abs/2201.11903)çš„ä¸€ä¸ªå¸¸è§å¤±è´¥æ˜¯LLMæ— æ³•æ‰§è¡Œç®—æœ¯è¿ç®—ã€‚åœ¨[ç¨‹åºè¾…åŠ©è¯­è¨€å»ºæ¨¡ï¼ˆPALï¼‰](https://arxiv.org/abs/2211.10435)å’Œ[æ€ç»´ç¨‹åºï¼ˆPoTï¼‰](https://arxiv.org/abs/2211.12588)çš„æç¤ºä¸­ï¼Œä½œè€…é€šè¿‡ç¨‹åºæç¤ºä»£ç è¯­è¨€æ¨¡å‹æ¥è§£å†³æ•°å­¦é—®é¢˜ã€‚å¯ä»¥åœ¨ç¨‹åºä¸­æ’å…¥æ ‡å‡†çš„æ€ç»´é“¾æ–‡æœ¬ä½œä¸ºæ³¨é‡Šã€‚æœ€ç»ˆçš„ç­”æ¡ˆæ˜¯é€šè¿‡æ‰§è¡ŒPythonè§£é‡Šå™¨ç”Ÿæˆçš„ã€‚è¿™äº›æ–¹æ³•èƒŒåçš„æ´è§æ˜¯ï¼Œä»£ç è§£é‡Šå™¨ä¸ºå„ç§è®¡ç®—æä¾›äº†å®Œç¾çš„å·¥å…·ï¼Œå°†å¤±è´¥æ¡ˆä¾‹å‡å°‘åˆ°ä»…ä»…æ˜¯æ¨ç†é”™è¯¯ã€‚ä»£ç é£æ ¼çš„æç¤ºä¹Ÿå¸¸ç”¨äº[è§„åˆ’ä»»åŠ¡](https://arxiv.org/abs/2305.16653)ä¸­ã€‚
- en: '![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)'
- en: 'Comparison between CoT and PAL. Source: [Gao and Madaan et al.](https://arxiv.org/abs/2211.10435)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CoTå’ŒPALçš„æ¯”è¾ƒã€‚æ¥æºï¼š[Gaoå’ŒMadaanç­‰](https://arxiv.org/abs/2211.10435)
- en: â¡ï¸ Retrievers are commonly used as preprocessing tools for LLMs to augment the
    question with relevant documents, often referred to as [retrieval-augmented generation
    (RAG)](https://arxiv.org/abs/2005.11401). However, when it comes to multi-step
    question answering, it is challenging to select the correct documents based on
    the question alone. In the **IRCoT** proposed by [Trivedi et al.](https://arxiv.org/abs/2212.10509),
    the authors interleave thought generation and knowledge retrieval. Whenever the
    LLM generates a thought sentence, IRCoT uses the sentence to retrieve documents
    from the corpus. The documents are prepended to the prompt to augment later generations.
    Even with a weak retriever like [BM25](https://en.wikipedia.org/wiki/Okapi_BM25),
    IRCoT outperforms one-step RAG on several open-domain question answering benchmarks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æ£€ç´¢å™¨é€šå¸¸ä½œä¸ºLLMçš„é¢„å¤„ç†å·¥å…·ï¼Œç”¨äºé€šè¿‡ç›¸å…³æ–‡æ¡£å¢å¼ºé—®é¢˜ï¼Œé€šå¸¸ç§°ä¸º[æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰](https://arxiv.org/abs/2005.11401)ã€‚ç„¶è€Œï¼Œå½“æ¶‰åŠåˆ°å¤šæ­¥éª¤é—®é¢˜è§£ç­”æ—¶ï¼Œä»…å‡­é—®é¢˜æœ¬èº«å°±å¾ˆéš¾é€‰æ‹©æ­£ç¡®çš„æ–‡æ¡£ã€‚åœ¨[Trivedi
    et al.](https://arxiv.org/abs/2212.10509)æå‡ºçš„**IRCoT**ä¸­ï¼Œä½œè€…å°†æ€ç»´ç”Ÿæˆä¸çŸ¥è¯†æ£€ç´¢äº¤æ›¿è¿›è¡Œã€‚æ¯å½“LLMç”Ÿæˆä¸€ä¸ªæ€ç»´å¥å­æ—¶ï¼ŒIRCoTå°±ä½¿ç”¨è¯¥å¥å­ä»è¯­æ–™åº“ä¸­æ£€ç´¢æ–‡æ¡£ã€‚è¿™äº›æ–‡æ¡£è¢«æ·»åŠ åˆ°æç¤ºä¸­ï¼Œä»¥å¢å¼ºåç»­ç”Ÿæˆã€‚å³ä½¿æ˜¯åƒ[BM25](https://en.wikipedia.org/wiki/Okapi_BM25)è¿™æ ·è¾ƒå¼±çš„æ£€ç´¢å™¨ï¼ŒIRCoTä¹Ÿåœ¨å¤šä¸ªå¼€æ”¾é¢†åŸŸçš„é—®ç­”åŸºå‡†æµ‹è¯•ä¸­è¶…è¶Šäº†ä¸€æ­¥RAGã€‚
- en: '![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)'
- en: 'IRCoT that interleaves CoT and knowledge retrieval. Source: [Trivedi et al.](https://arxiv.org/abs/2212.10509)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤æ›¿ä½¿ç”¨CoTå’ŒçŸ¥è¯†æ£€ç´¢çš„IRCoTã€‚æ¥æºï¼š[Trivediç­‰](https://arxiv.org/abs/2212.10509)
- en: â¡ï¸ [Yang et al.](https://arxiv.org/abs/2306.15626) presented a novel usage of
    RAG for theorem proving. They built a gym-like environment **LeanDojo** ğŸ¯ based
    on the proof assistant Lean. [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))
    is an interactive programming environment where its compiler can verify whether
    the written proof proved the goal. It also includes numerous proved theorems in
    its standard libraries, similar to STL in C++. The cool thing is that because
    proofs are constructed by decomposing theorems into known premises, theorem proving
    can benefit from RAG. Given a theorem, we retrieve the relevant premises from
    the standard libraries and then ask an LLM to generate a proof step. The authors
    show that RAG requires much less training resources and generalizes better to
    novel premises.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ [Yangç­‰](https://arxiv.org/abs/2306.15626)æå‡ºäº†ä¸€ç§RAGåœ¨å®šç†è¯æ˜ä¸­çš„æ–°é¢–åº”ç”¨ã€‚ä»–ä»¬åŸºäºè¯æ˜åŠ©æ‰‹Leanæ„å»ºäº†ä¸€ä¸ªç±»ä¼¼å¥èº«æˆ¿çš„ç¯å¢ƒ**LeanDojo**
    ğŸ¯ã€‚[Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))æ˜¯ä¸€ä¸ªäº’åŠ¨ç¼–ç¨‹ç¯å¢ƒï¼Œå…¶ä¸­çš„ç¼–è¯‘å™¨å¯ä»¥éªŒè¯æ‰€å†™çš„è¯æ˜æ˜¯å¦è¯æ˜äº†ç›®æ ‡ã€‚å®ƒè¿˜åŒ…å«äº†è®¸å¤šåœ¨æ ‡å‡†åº“ä¸­å·²è¯æ˜çš„å®šç†ï¼Œç±»ä¼¼äºC++ä¸­çš„STLã€‚å¾ˆé…·çš„ä¸€ç‚¹æ˜¯ï¼Œç”±äºè¯æ˜æ˜¯é€šè¿‡å°†å®šç†åˆ†è§£ä¸ºå·²çŸ¥å‰ææ„é€ çš„ï¼Œå› æ­¤å®šç†è¯æ˜å¯ä»¥ä»RAGä¸­å—ç›Šã€‚ç»™å®šä¸€ä¸ªå®šç†ï¼Œæˆ‘ä»¬ä»æ ‡å‡†åº“ä¸­æ£€ç´¢ç›¸å…³çš„å‰æï¼Œç„¶åè¯·æ±‚LLMç”Ÿæˆä¸€ä¸ªè¯æ˜æ­¥éª¤ã€‚ä½œè€…è¡¨æ˜ï¼ŒRAGéœ€è¦çš„è®­ç»ƒèµ„æºè¦å°‘å¾—å¤šï¼Œå¹¶ä¸”åœ¨æ–°é¢–çš„å‰æä¸‹å…·æœ‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: '![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)'
- en: 'Proof of a simple logical theorem in Lean. Source: [Xena Project](https://www.youtube.com/watch?v=POHVMMG7pqE)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Leanä¸­è¯æ˜ä¸€ä¸ªç®€å•çš„é€»è¾‘å®šç†ã€‚æ¥æºï¼š[Xenaé¡¹ç›®](https://www.youtube.com/watch?v=POHVMMG7pqE)
- en: â¡ï¸ Finally, [DSPy](https://github.com/stanfordnlp/dspy) by [Khattab et al.](https://arxiv.org/abs/2310.03714)
    presents a novel approach towards programming LLMs where the framework can actually
    improve the prompts over time and combine prompting techniques (CoT, PoT) with
    retrieval automatically. Further, DSPy introduces *teleprompters* for optimizing
    the prompts and bootstrapping new ones. Itâ€™s hard to fit a description of DSPy
    in one paragraph â€” itâ€™s not your average RAG technique, but rather an evolution
    of it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ æœ€åï¼Œ[DSPy](https://github.com/stanfordnlp/dspy)ç”±[Khattab et al.](https://arxiv.org/abs/2310.03714)æå‡ºäº†ä¸€ç§æ–°çš„LLMç¼–ç¨‹æ–¹æ³•ï¼Œåœ¨è¯¥æ¡†æ¶ä¸‹ï¼Œç³»ç»Ÿèƒ½å¤Ÿéšç€æ—¶é—´çš„æ¨ç§»è‡ªåŠ¨ä¼˜åŒ–æç¤ºï¼Œå¹¶ç»“åˆæç¤ºæŠ€æœ¯ï¼ˆCoT,
    PoTï¼‰ä¸æ£€ç´¢ã€‚è¿›ä¸€æ­¥åœ°ï¼ŒDSPyå¼•å…¥äº†*æè¯å™¨*æ¥ä¼˜åŒ–æç¤ºå¹¶å¼•å¯¼æ–°çš„æç¤ºç”Ÿæˆã€‚å¾ˆéš¾ç”¨ä¸€æ®µè¯æ¥æè¿°DSPyâ€”â€”å®ƒä¸æ˜¯æ™®é€šçš„RAGæŠ€æœ¯ï¼Œè€Œæ˜¯å…¶è¿›åŒ–ç‰ˆæœ¬ã€‚
- en: Let LLMs create their own tools
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©LLMåˆ›å»ºè‡ªå·±çš„å·¥å…·
- en: 'Tool use has an inherent limitation: it relies on the presence of tools for
    a specific task. In nature, tool use is not an exclusive skill of humans, as many
    other animals can also use tools. However, what distinguishes humans from other
    animals is the ability to **create** tools. In 2023, weâ€™ve seen a few preliminary
    works exploring the ability of tool making in LLMs.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å·¥å…·ä½¿ç”¨æœ‰ä¸€ä¸ªå›ºæœ‰çš„é™åˆ¶ï¼šå®ƒä¾èµ–äºç‰¹å®šä»»åŠ¡æ‰€éœ€å·¥å…·çš„å­˜åœ¨ã€‚åœ¨è‡ªç„¶ç•Œä¸­ï¼Œå·¥å…·ä½¿ç”¨å¹¶éäººç±»çš„ä¸“å±æŠ€èƒ½ï¼Œè®¸å¤šå…¶ä»–åŠ¨ç‰©ä¹Ÿèƒ½å¤Ÿä½¿ç”¨å·¥å…·ã€‚ç„¶è€Œï¼ŒåŒºåˆ«äººç±»ä¸å…¶ä»–åŠ¨ç‰©çš„ï¼Œæ˜¯**åˆ›é€ **å·¥å…·çš„èƒ½åŠ›ã€‚2023å¹´ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›åˆæ­¥çš„ç ”ç©¶ï¼Œæ¢ç´¢äº†LLMä¸­å·¥å…·åˆ¶ä½œçš„èƒ½åŠ›ã€‚
- en: â¡ï¸ In LLMs as tool makers (**LATM**) proposed by [Cai et al.](https://arxiv.org/abs/2305.17126),
    the authors prompt an LLM to craft tools in the form of Python functions for a
    given task. The tools are then verified on a few samples, similar to how engineers
    solve problems on LeetCode. Once some tools pass the verification test, they are
    wrapped with documentation strings generated by an LLM to describe their usage.
    At test time, an LLM is prompted to dispatch the question to one of the tools
    at hand, and execute the tool based on the usage. LATM significantly outperforms
    CoT on a wide range of reasoning tasks in BigBench.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ åœ¨ç”±[Cai et al.](https://arxiv.org/abs/2305.17126)æå‡ºçš„LLMä½œä¸ºå·¥å…·åˆ¶é€ è€…ï¼ˆ**LATM**ï¼‰ä¸­ï¼Œä½œè€…æç¤ºLLMä¸ºç»™å®šä»»åŠ¡ç¼–å†™Pythonå‡½æ•°å½¢å¼çš„å·¥å…·ã€‚è¿™äº›å·¥å…·éšååœ¨ä¸€äº›æ ·æœ¬ä¸Šè¿›è¡ŒéªŒè¯ï¼Œç±»ä¼¼äºå·¥ç¨‹å¸ˆåœ¨LeetCodeä¸Šè§£å†³é—®é¢˜çš„æ–¹å¼ã€‚ä¸€æ—¦æŸäº›å·¥å…·é€šè¿‡éªŒè¯æµ‹è¯•ï¼Œå®ƒä»¬ä¼šè¢«LLMç”Ÿæˆçš„æ–‡æ¡£å­—ç¬¦ä¸²åŒ…è£…ï¼Œä»¥æè¿°å…¶ä½¿ç”¨æ–¹æ³•ã€‚åœ¨æµ‹è¯•æ—¶ï¼ŒLLMä¼šè¢«æç¤ºå°†é—®é¢˜å‘é€åˆ°æ‰‹å¤´çš„æŸä¸ªå·¥å…·ï¼Œå¹¶æ ¹æ®å…¶ä½¿ç”¨æ–¹æ³•æ‰§è¡Œè¯¥å·¥å…·ã€‚LATMåœ¨BigBenchçš„å¹¿æ³›æ¨ç†ä»»åŠ¡ä¸­ï¼Œæ˜¾è‘—ä¼˜äºCoTã€‚
- en: â¡ï¸ [Voyager](https://arxiv.org/abs/2305.16291) brought the idea of tool making
    to the world of Minecraft, and came up with incredible results. The core idea
    of Voyager is to use an LLM to propose tasks based on existing skills and world
    state. Then the LLM is prompted to synthesize code (i.e. skills) to solve the
    tasks. The skills are refined based on environment feedback and mastered skills
    are committed to an external memory. Because new skills are built on top of existing
    skills, this significantly reduced the difficulty of learning a complex skill
    (e.g. building a diamond tool in Minecraft). While the idea of learning a library
    of skills can be traced back to [DreamCoder](https://arxiv.org/abs/2006.08381),
    Voyager demonstrates the superiority of GPT-4 in searching over skills in a challenging
    open-world game. Take a look at [the fancy demos](https://voyager.minedojo.org/)
    from the paper!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ [Voyager](https://arxiv.org/abs/2305.16291)å°†å·¥å…·åˆ¶ä½œçš„æ¦‚å¿µå¼•å…¥äº†Minecraftçš„ä¸–ç•Œï¼Œå¹¶å–å¾—äº†æƒŠäººçš„ç»“æœã€‚Voyagerçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨LLMæ ¹æ®ç°æœ‰çš„æŠ€èƒ½å’Œä¸–ç•ŒçŠ¶æ€æå‡ºä»»åŠ¡ã€‚æ¥ç€ï¼ŒLLMè¢«æç¤ºåˆæˆä»£ç ï¼ˆå³æŠ€èƒ½ï¼‰æ¥è§£å†³è¿™äº›ä»»åŠ¡ã€‚æŠ€èƒ½åŸºäºç¯å¢ƒåé¦ˆè¿›è¡Œç²¾ç‚¼ï¼ŒæŒæ¡çš„æŠ€èƒ½è¢«ä¿å­˜åˆ°å¤–éƒ¨è®°å¿†ä¸­ã€‚ç”±äºæ–°æŠ€èƒ½æ˜¯åœ¨ç°æœ‰æŠ€èƒ½çš„åŸºç¡€ä¸Šæ„å»ºçš„ï¼Œè¿™å¤§å¤§é™ä½äº†å­¦ä¹ å¤æ‚æŠ€èƒ½çš„éš¾åº¦ï¼ˆä¾‹å¦‚ï¼Œåœ¨Minecraftä¸­åˆ¶ä½œé’»çŸ³å·¥å…·ï¼‰ã€‚å°½ç®¡å­¦ä¹ æŠ€èƒ½åº“çš„æ¦‚å¿µå¯ä»¥è¿½æº¯åˆ°[DreamCoder](https://arxiv.org/abs/2006.08381)ï¼ŒVoyagerå±•ç¤ºäº†GPT-4åœ¨æŒ‘æˆ˜æ€§å¼€æ”¾ä¸–ç•Œæ¸¸æˆä¸­æœç´¢æŠ€èƒ½çš„ä¼˜åŠ¿ã€‚è¯·æŸ¥çœ‹[è®ºæ–‡ä¸­çš„ç²¾å½©æ¼”ç¤º](https://voyager.minedojo.org/)ï¼
- en: '![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)'
- en: 'Minecraft items and skills discovered over time. Source: [Wang et al.](https://arxiv.org/abs/2305.16291)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€æ—¶é—´çš„æ¨ç§»ï¼ŒMinecraftä¸­çš„ç‰©å“å’ŒæŠ€èƒ½è¢«å‘ç°ã€‚æ¥æºï¼š[Wang et al.](https://arxiv.org/abs/2305.16291)
- en: 'â¡ï¸ Both of the above works craft tools as code. In fact, tools can be in natural
    language, too. (shameless self-promotion) In the hypothesis-to-theories (**HtT**)
    work from [Zhu et al.](https://arxiv.org/abs/2310.07064), the authors show that
    we can use LLMs to induce a library of textual rules from a standard multi-step
    reasoning training set. The insight is that among all the rules that LLMs produce
    for different samples, rules that occur and lead to correct answers more often
    are likely to be correct. We then collect the rules and prepend them to a standard
    CoT prompt to perform deduction and get the answer. One interesting aspect about
    HtT is that it can be viewed as a novel way of learning: instead of learning model
    parameters, we learn a library of rules, which works well with black-box LLMs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ ä¸Šè¿°ä¸¤ç¯‡å·¥ä½œéƒ½å°†å·¥å…·è®¾è®¡ä¸ºä»£ç ã€‚äº‹å®ä¸Šï¼Œå·¥å…·ä¹Ÿå¯ä»¥æ˜¯è‡ªç„¶è¯­è¨€ã€‚ï¼ˆä¸å«Œå¼ƒçš„è‡ªæˆ‘æ¨é”€ï¼‰åœ¨ [Zhu et al.](https://arxiv.org/abs/2310.07064)
    çš„å‡è®¾åˆ°ç†è®º (**HtT**) å·¥ä½œä¸­ï¼Œä½œè€…å±•ç¤ºäº†æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ LLM ä»æ ‡å‡†çš„å¤šæ­¥éª¤æ¨ç†è®­ç»ƒé›†è¯±å¯¼å‡ºä¸€å¥—æ–‡æœ¬è§„åˆ™åº“ã€‚å…¶æ´å¯Ÿæ˜¯ï¼Œåœ¨ LLM ä¸ºä¸åŒæ ·æœ¬ç”Ÿæˆçš„æ‰€æœ‰è§„åˆ™ä¸­ï¼Œå‘ç”Ÿé¢‘ç‡è¾ƒé«˜å¹¶ä¸”å¯¼è‡´æ­£ç¡®ç­”æ¡ˆçš„è§„åˆ™å¯èƒ½æ˜¯æ­£ç¡®çš„ã€‚ç„¶åï¼Œæˆ‘ä»¬æ”¶é›†è¿™äº›è§„åˆ™å¹¶å°†å…¶é™„åŠ åˆ°æ ‡å‡†
    CoT æç¤ºä¸­ä»¥è¿›è¡Œæ¨ç†å¹¶å¾—å‡ºç­”æ¡ˆã€‚HtT çš„ä¸€ä¸ªæœ‰è¶£æ–¹é¢æ˜¯ï¼Œå®ƒå¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§æ–°çš„å­¦ä¹ æ–¹å¼ï¼šæˆ‘ä»¬ä¸æ˜¯å­¦ä¹ æ¨¡å‹å‚æ•°ï¼Œè€Œæ˜¯å­¦ä¹ ä¸€å¥—è§„åˆ™åº“ï¼Œè¿™ä¸é»‘ç›’ LLM é…åˆå¾—éå¸¸å¥½ã€‚
- en: '![](../Images/e30b407b2550861646906f5e0ad43916.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e30b407b2550861646906f5e0ad43916.png)'
- en: 'HtT that learns textual rules for multi-hop reasoning. Source: [Zhu et al.](https://arxiv.org/abs/2310.07064)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HtT å­¦ä¹ å¤šè·³æ¨ç†çš„æ–‡æœ¬è§„åˆ™ã€‚æ¥æºï¼š[Zhu et al.](https://arxiv.org/abs/2310.07064)
- en: Reasoning
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨ç†
- en: Planning
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§„åˆ’
- en: One drawback for CoT-style reasoning is that LLMs have to greedily decode a
    path towards an answer. This is problematic for complex problems like math questions
    or games, since it is hard to predict a path without trial-and-error. In 2023,
    the community made some progress on this issue with new frameworks that enable
    planning with LLMs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CoT é£æ ¼æ¨ç†çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼ŒLLM å¿…é¡»è´ªå©ªåœ°è§£ç ä¸€ä¸ªé€šå‘ç­”æ¡ˆçš„è·¯å¾„ã€‚å¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œå¦‚æ•°å­¦é¢˜æˆ–æ¸¸æˆï¼Œè¿™å°±æˆäº†ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºæ²¡æœ‰è¯•é”™å°±å¾ˆéš¾é¢„æµ‹å‡ºä¸€æ¡è·¯å¾„ã€‚2023
    å¹´ï¼Œç¤¾åŒºåœ¨è¿™ä¸ªé—®é¢˜ä¸Šå–å¾—äº†ä¸€äº›è¿›å±•ï¼Œæ¨å‡ºäº†æ–°çš„æ¡†æ¶ï¼Œä½¿å¾— LLM å¯ä»¥è¿›è¡Œè§„åˆ’ã€‚
- en: 'â¡ï¸ If we conceptualize CoT as â€œsystem 1â€ reasoning â€” characterized by its automatic,
    unconscious nature â€” then a question arises: Is it feasible to replicate the more
    conscious â€œsystem 2â€ reasoning of humans using LLMs? This query finds relevance
    in two methodologies: [reasoning-via-planning (RAP)](https://arxiv.org/abs/2305.14992)
    and [tree-of-thoughts (ToT)](https://arxiv.org/abs/2305.10601). Both empower LLMs
    to navigate through possible reasoning steps, and to search for the optimal reasoning
    chain based on specific evaluations. RAP additionally prompts an LLM as a â€œworld
    modelâ€, which predicts the next states following actions. This enables the LLM
    to operate within a self-simulated world, as opposed to interacting with an external
    environment. Both algorithms are available in the [LLM Reasoners](https://github.com/Ber666/llm-reasoners/)
    library now!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ å¦‚æœæˆ‘ä»¬å°† CoT æ¦‚å¿µåŒ–ä¸ºâ€œç³»ç»Ÿ 1â€æ¨ç†â€”â€”å…¶ç‰¹ç‚¹æ˜¯è‡ªåŠ¨çš„ã€æ— æ„è¯†çš„ç‰¹æ€§â€”â€”é‚£ä¹ˆå°±ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜ï¼šæ˜¯å¦å¯ä»¥ç”¨ LLM æ¥å¤åˆ¶æ›´å…·æ„è¯†çš„â€œç³»ç»Ÿ
    2â€æ¨ç†ï¼Ÿè¿™ä¸ªé—®é¢˜åœ¨ä¸¤ç§æ–¹æ³•ä¸­å¾—åˆ°äº†å›åº”ï¼š[æ¨ç†é€šè¿‡è§„åˆ’ (RAP)](https://arxiv.org/abs/2305.14992) å’Œ [æ€ç»´æ ‘ (ToT)](https://arxiv.org/abs/2305.10601)ã€‚è¿™ä¸¤è€…éƒ½ä½¿
    LLM èƒ½å¤Ÿé€šè¿‡å¯èƒ½çš„æ¨ç†æ­¥éª¤è¿›è¡Œæ¨ç†ï¼Œå¹¶æ ¹æ®ç‰¹å®šçš„è¯„ä¼°æœç´¢æœ€ä¼˜æ¨ç†é“¾ã€‚RAP è¿˜å°† LLM æç¤ºä¸ºä¸€ä¸ªâ€œä¸–ç•Œæ¨¡å‹â€ï¼Œå®ƒé¢„æµ‹åœ¨è¡ŒåŠ¨åçš„ä¸‹ä¸€çŠ¶æ€ã€‚è¿™ä½¿å¾— LLM
    å¯ä»¥åœ¨ä¸€ä¸ªè‡ªæˆ‘æ¨¡æ‹Ÿçš„ä¸–ç•Œä¸­æ“ä½œï¼Œè€Œä¸æ˜¯ä¸å¤–éƒ¨ç¯å¢ƒäº’åŠ¨ã€‚ç°åœ¨è¿™ä¸¤ä¸ªç®—æ³•éƒ½å¯ä»¥åœ¨ [LLM Reasoners](https://github.com/Ber666/llm-reasoners/)
    åº“ä¸­æ‰¾åˆ°ï¼
- en: '![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)'
- en: 'RAP that repurposes LLMs as an agent and a world model. Source: [Hao et al.](https://arxiv.org/abs/2305.14992)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: RAP å°† LLM é‡æ–°è®¾è®¡ä¸ºä¸€ä¸ªä»£ç†å’Œä¸–ç•Œæ¨¡å‹ã€‚æ¥æºï¼š[Hao et al.](https://arxiv.org/abs/2305.14992)
- en: Self series
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Self ç³»åˆ—
- en: Self series are a family of techniques that replace human efforts with LLM predictions
    in the loop of LLM development. The year of 2023 has witnessed quite a few papers
    on this track. Letâ€™s take a closer look at some representative works.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Self ç³»åˆ—æ˜¯ä¸€ç±»æŠ€æœ¯ï¼Œé€šè¿‡åœ¨ LLM å¼€å‘è¿‡ç¨‹ä¸­ç”¨ LLM é¢„æµ‹æ›¿ä»£äººå·¥åŠªåŠ›ã€‚2023 å¹´è§è¯äº†è¿™ä¸€é¢†åŸŸçš„è‹¥å¹²é‡è¦è®ºæ–‡ã€‚è®©æˆ‘ä»¬æ·±å…¥äº†è§£ä¸€äº›å…·æœ‰ä»£è¡¨æ€§çš„å·¥ä½œã€‚
- en: â¡ï¸ Many people have the experience that ChatGPT doesnâ€™t provide the desired
    output on the first trial, and this sometimes can be fixed by pointing out its
    mistake. [Self-debugging](https://arxiv.org/abs/2304.05128) and [self-refinement](https://arxiv.org/abs/2303.17651)
    automate this procedure by replacing human feedback with machine feedback. The
    feedback either comes from a program executor or an LLM that compares the generation
    with the explanation of the problem. One key observation is that the performance
    of self-refine depends on the quality of the feedback, where stronger base models
    that provide better feedback benefit more. Such iterative refinement methods have
    also been shown to be super effective in [pose estimation](https://arxiv.org/abs/1507.06550)
    and [protein structure prediction](https://www.nature.com/articles/s41586-021-03819-2),
    where it is difficult to predict the structure in a single run.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b3c4d3580bf9a786395a972b481534c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: 'Illustration of Self-Debugging. Source: [Chen et al.](https://arxiv.org/abs/2304.05128)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ In the memory-of-thought (**MoT**) framework from [Li and Qiu](https://arxiv.org/abs/2305.05181),
    the authors ask an LLM to generate CoT rationales on an unlabelled dataset and
    use them for RAG. You may ask how this can be useful given that the generated
    rationales often contain errors. The key trick is to filter the rationales based
    on majority vote or entropy minimization (a similar idea is used in [Wan et al.](https://arxiv.org/abs/2305.14106)
    to filter rationales). Once we have good rationales on the unlabelled dataset,
    we dynamically retrieve few-shot examples based on the test question, which is
    shown to be much better than fixed few-shot examples. MoT can be interpreted as
    converting a parametric model to a non-parametric model without additional supervision.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/07dcece1803feb415c68d2727bc2c037.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'MoT that generates and recalls memory. Source: [Li and Qiu](https://arxiv.org/abs/2305.05181).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ Going beyond MoT, [Yasunaga et al.](https://arxiv.org/abs/2310.01714) proposed
    **analogical prompting** that eliminates the need of dumping rationales on an
    unlabeled dataset. Analogical prompting asks an LLM to recall relevant exemplars
    based on the question, and thereby generates dynamic few-shot exemplars from scratch.
    In fact, the authors found that analogical prompting is an emergent ability in
    large language models, similar to previous works on [open-domain question answering](https://arxiv.org/abs/2209.10063).
    Larger-scale LLMs can self-generate better exemplars compared to standard RAG
    solutions. Besides, this work provides a cool trick to fuse multi-step generations
    into a single prompt with markdown grammar â€” a godsend for prompt engineers with
    a tight budget! ğŸ’¡
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cff6c1216d04791bb9d00343530b5700.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: 'Analogical prompting. Source: [Yasunaga et al.](https://arxiv.org/abs/2310.01714)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ Are self-refine and self-generate the limit of LLM reasoning? [Yang et al.](https://arxiv.org/abs/2309.03409)
    show a more advanced usage of the reasoning abilities of LLMs â€” to optimize a
    prompt based on the history of generated prompts. This is a cool reinvention of
    the famous meta-learning paper â€œ[Learning to learn by gradient descent by gradient
    descent](https://arxiv.org/abs/1606.04474)â€, but all the steps here are performed
    by LLMs on text. At each step, an LLM is prompted with previous solutions and
    corresponding performance metrics and tries to predict a new solution. Notably,
    even without telling the LLM how to perform optimization, the LLM can gradually
    find better solutions that maximize the metric. Maybe this work brings prompt
    engineers one step closer to unemployment?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: â¡ï¸ è‡ªæˆ‘ä¼˜åŒ–å’Œè‡ªæˆ‘ç”Ÿæˆæ˜¯LLMæ¨ç†çš„æé™å—ï¼Ÿ[Yang et al.](https://arxiv.org/abs/2309.03409)å±•ç¤ºäº†LLMæ¨ç†èƒ½åŠ›çš„æ›´é«˜çº§åº”ç”¨â€”â€”åŸºäºç”Ÿæˆçš„æç¤ºå†å²ä¼˜åŒ–æç¤ºã€‚è¿™æ˜¯å¯¹è‘—åå…ƒå­¦ä¹ è®ºæ–‡ã€Š[é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ å­¦ä¹ ](https://arxiv.org/abs/1606.04474)ã€‹çš„ä¸€ä¸ªé…·ç‚«é‡å¡‘ï¼Œä½†è¿™é‡Œçš„æ‰€æœ‰æ­¥éª¤éƒ½æ˜¯ç”±LLMåœ¨æ–‡æœ¬ä¸Šæ‰§è¡Œçš„ã€‚åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼ŒLLMéƒ½ä¼šæ¥æ”¶ä¹‹å‰çš„è§£å†³æ–¹æ¡ˆå’Œç›¸åº”çš„æ€§èƒ½æŒ‡æ ‡æç¤ºï¼Œå¹¶å°è¯•é¢„æµ‹ä¸€ä¸ªæ–°çš„è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿æ²¡æœ‰å‘Šè¯‰LLMå¦‚ä½•è¿›è¡Œä¼˜åŒ–ï¼ŒLLMä¹Ÿèƒ½é€æ­¥æ‰¾åˆ°æœ€å¤§åŒ–æŒ‡æ ‡çš„æ›´å¥½è§£å†³æ–¹æ¡ˆã€‚ä¹Ÿè®¸è¿™é¡¹å·¥ä½œè®©æç¤ºå·¥ç¨‹å¸ˆç¦»å¤±ä¸šæ›´è¿‘äº†ä¸€æ­¥ï¼Ÿ
- en: '![](../Images/2c7ff40222823804440ab08816082b7d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c7ff40222823804440ab08816082b7d.png)'
- en: 'Performance of prompts optimized by LLM. Source: [Yang et al.](https://arxiv.org/abs/2309.03409)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±LLMä¼˜åŒ–çš„æç¤ºæ€§èƒ½ã€‚æ¥æºï¼š[Yang et al.](https://arxiv.org/abs/2309.03409)
- en: ğŸ” Probably the most eye-opening ğŸ‘€ work in self series is the self-taught optimizer
    (**STOP**) by [Zelikman et al.](https://arxiv.org/abs/2310.02304) We know LLMs
    are guided by textual prompts, take texts as input and output texts. While these
    these texts are usually separate variables, what will happen if we model them
    as a single variable? In STOP, the authors draw inspiration from [self-modifying
    code](https://en.wikipedia.org/wiki/Self-modifying_code) and use a self-improvement
    prompt to improve itself.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ” å¯èƒ½æ˜¯è‡ªå­¦ç³»åˆ—ä¸­æœ€å…·å¯å‘æ€§çš„ğŸ‘€å·¥ä½œæ˜¯ç”±[Zelikman et al.](https://arxiv.org/abs/2310.02304)æå‡ºçš„è‡ªæˆ‘ä¼˜åŒ–å™¨(**STOP**)
    ã€‚æˆ‘ä»¬çŸ¥é“LLMæ˜¯é€šè¿‡æ–‡æœ¬æç¤ºæ¥å¼•å¯¼çš„ï¼Œæ¥å—æ–‡æœ¬ä½œä¸ºè¾“å…¥å¹¶è¾“å‡ºæ–‡æœ¬ã€‚è™½ç„¶è¿™äº›æ–‡æœ¬é€šå¸¸æ˜¯åˆ†å¼€çš„å˜é‡ï¼Œä½†å¦‚æœæˆ‘ä»¬å°†å®ƒä»¬å»ºæ¨¡ä¸ºå•ä¸€å˜é‡ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿåœ¨STOPä¸­ï¼Œä½œè€…ä»[è‡ªæˆ‘ä¿®æ”¹ä»£ç ](https://en.wikipedia.org/wiki/Self-modifying_code)ä¸­æ±²å–çµæ„Ÿï¼Œä½¿ç”¨è‡ªæˆ‘æ”¹è¿›çš„æç¤ºæ¥æå‡è‡ªèº«ã€‚
- en: '![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)'
- en: 'The seed improver that improves itself in STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨STOPä¸­æ”¹è¿›è‡ªèº«çš„ç§å­æ”¹è¿›å™¨ã€‚æ¥æºï¼š[Zelikman et al.](https://arxiv.org/abs/2310.02304)
- en: While the seed prompt isnâ€™t more complicated than a random search algorithm,
    with a strong LLM, one can discover many advanced meta-heuristic algorithms. Interestingly,
    GPT-4 discovers many prompting strategies that are published after the training
    cutoff for GPT-4, including [ToT](https://arxiv.org/abs/2305.10601) and [Parsel](https://arxiv.org/abs/2212.10561).
    It seems that the day when LLMs conduct research for themselves is approaching.
    One step in this direction is a recent work by [Huang et al.](https://arxiv.org/abs/2310.03302)
    showing that LLMs are capable of designing ML models for common benchmarks and
    even Kaggle challenges.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ç§å­æç¤ºå¹¶ä¸æ¯”éšæœºæœç´¢ç®—æ³•æ›´å¤æ‚ï¼Œä½†æœ‰äº†å¼ºå¤§çš„LLMï¼Œäººä»¬å¯ä»¥å‘ç°è®¸å¤šå…ˆè¿›çš„å…ƒå¯å‘å¼ç®—æ³•ã€‚æœ‰è¶£çš„æ˜¯ï¼ŒGPT-4å‘ç°äº†è®¸å¤šåœ¨å…¶è®­ç»ƒæˆªæ­¢æ—¥æœŸä¹‹åå‘å¸ƒçš„æç¤ºç­–ç•¥ï¼ŒåŒ…æ‹¬[ToT](https://arxiv.org/abs/2305.10601)å’Œ[Parsel](https://arxiv.org/abs/2212.10561)ã€‚çœ‹èµ·æ¥ï¼ŒLLMä¸ºè‡ªå·±è¿›è¡Œç ”ç©¶çš„é‚£ä¸€å¤©å³å°†åˆ°æ¥ã€‚æœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºçš„ä¸€æ­¥æ˜¯[Huang
    et al.](https://arxiv.org/abs/2310.03302)çš„æœ€æ–°ç ”ç©¶ï¼Œè¡¨æ˜LLMèƒ½å¤Ÿä¸ºå¸¸è§åŸºå‡†æµ‹è¯•ç”šè‡³KaggleæŒ‘æˆ˜è®¾è®¡æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚
- en: '![](../Images/e030d2aa8360e5a443b744b6852437af.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e030d2aa8360e5a443b744b6852437af.png)'
- en: 'Algorithms found by STOP. Source: [Zelikman et al.](https://arxiv.org/abs/2310.02304)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: STOPå‘ç°çš„ç®—æ³•ã€‚æ¥æºï¼š[Zelikman et al.](https://arxiv.org/abs/2310.02304)
- en: Evaluations and observations
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯„ä¼°ä¸è§‚å¯Ÿ
- en: â¡ï¸ [Kandpal et al.](https://arxiv.org/abs/2211.08411) conducted a systematic
    study on the memorization ability of LLMs. They asked an LLM about factual questions
    from Wikipedia and found that the accuracy is highly correlated with the frequency
    of questioned entities in the pretraining documents, regardless of the scale of
    the model. By extrapolating the trend, the authors estimate that a model with
    10Â¹â¸ is needed to match human performance on long-tail entities â€” which is way
    bigger than todayâ€™s LLMs. Hence an important takeaway is to use LLM reasoning
    for tasks related to frequent knowledge, and consider RAG or other tools for tasks
    related to long-tail knowledge.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7500b22b9573287416b537dcd301dcb.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'LLMs can hardly memorize long-tail knowledge. Source: [Kandpal et al.](https://arxiv.org/abs/2211.08411)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ As the community tries to build bigger mixtures for training LLMs, one concern
    is that LLMs may not learn to actually reason but simply to memorize the solutions
    from the training distribution, just like humans in [teaching to the test](https://en.wikipedia.org/wiki/Teaching_to_the_test).
    [Wu et al.](https://arxiv.org/abs/2307.02477) answers this concern by comparing
    the performance of GPT-4 with zero-shot CoT on 11 different tasks, each with a
    default setting and a counterfactual setting. They observe that despite LLMs performing
    better than random in the counterfactual settings, their performance is consistently
    behind that in the default settings. It remains an open question how we can train
    models to focus more on reasoning rather than memorization.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03f7be28616d30d4236ececb57c2aff3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'GPT-4 underperforms on counterfactual variants. Source: [Wu et al.](https://arxiv.org/abs/2307.02477)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ [Saparov et al.](https://arxiv.org/abs/2305.15269) extended a synthetic dataset
    [PrOntoQA](https://arxiv.org/abs/2210.01240) to OOD setting to test generalization
    ability of LLMs on deductive reasoning with controlled depth, width, compositional
    structure, etc. The authors found that CoT can generalize to compositional and
    longer proofs. This is in contrast with previous conclusions on [compositional
    semantic parsing](https://arxiv.org/abs/2205.12253), possibly because deductive
    reasoning only requires composing deduction steps, while semantic parsing additionally
    deals with growing outputs. While LLMs are able to use most deduction rules, they
    require explicit demonstrations of [*proof by cases*](https://en.wikipedia.org/wiki/Disjunction_elimination)
    and [*proof by contradiction*](https://en.wikipedia.org/wiki/Proof_by_contradiction).
    There are also counterintuitive qualitative differences between in-context learning
    and supervised learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/37163127677f52ac47a0d2999eb0ebd6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'OOD generalization over deductive reasoning. Source: [Saparov et al.](https://arxiv.org/abs/2305.15269)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: â¡ï¸ Regarding the parametric knowledge in LLMs, [Berglund et al.](https://arxiv.org/abs/2309.12288)
    found a phenomenon they called the *reversal curse*. That is, LLMs trained to
    memorize â€œA is Bâ€ do not know that â€œB is Aâ€ in closed-book question answering,
    despite the fact that they can be prompted to perform deductive reasoning. This
    indicates that LLMs lack certain kinds of symmetry in its parametric knowledge,
    and it is crucial to endow them with such symmetry to enable better generalization.
    Actually, the knowledge graph community has been a leader in this area, with works
    like [double permutation equivariance](https://arxiv.org/abs/2302.01313) and [relational
    rotation](https://arxiv.org/abs/1902.10197). It would be interesting to see how
    these ideas are adapted to LLMs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: What needs to be solved in 2024?
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2023 has been an exciting year for tool use and reasoning, and we expect the
    new year to be more exciting. Letâ€™s wrap up this post with predictions from the
    authors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhaocheng Zhu:'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ Reasoning with LLMs still requires ad-hoc engineering efforts for each specific
    task. By contrast, once humans acquire the skills for a task, they can quickly
    adapt the skills to similar tasks with very few or even no samples (e.g. from
    chess to poker). If we can create LLM solutions that generalize across tasks,
    it will save a lot of engineering efforts and boost the performance in low-resource
    domains.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ Solving reasoning problems usually involves a lot of commonsense knowledge,
    ranging from math, physics to strategies like enumeration and proof by contradiction,
    if any. While LLMs may have obtained such knowledge from their training data,
    we lack precise control over the parameteric knowledge in LLMs. We would like
    to see new studies on the knowledge representations of LLMs, and techniques that
    verbalize, inject or delete knowledge in LLMs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Michael Galkin:'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ In 2023, we saw an increasing effort to understand the basic principles
    of *what can be learned* by Transformer-based LLMs â€” can we actually expect LLMs
    to be able to solve any arbitrary reasoning tasks? A few famous papers like [faith
    and fate](https://arxiv.org/abs/2305.18654) and [on length generalization](https://arxiv.org/abs/2310.16028)
    suggest that the autoregressive nature of LLMs might not be the optimal way to
    approach complex reasoning. In 2024, Iâ€™d expect more efforts on understanding
    the algorithmic alignment with LLMs.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ It is likely that in 2024, most open and closed foundation models will be
    multi-modal supporting vision, text, audio, and other inputs. Incorporating other
    modalities into reasoning is the natural next step.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Abulhair Saparov:'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ I anticipate there will be more efforts to find a more mechanistic understanding
    of reasoning in LLMs. What algorithms do they use when performing reasoning tasks?
    More precisely to what extent do they exploit shortcuts or heuristics that hurt
    robustness/generalization?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ Relatedly, I would expect researchers to make progress in answering whether
    increasing the scale of LLMs and/or their training will resolve their limitations
    in reasoning, or whether these limitations are fundamental, e.g. inherent to the
    architecture.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Shibo Hao:'
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ Over the past year, the primary focus of LLM reasoning research has been
    on prompting and supervised fine-tuning, with some approaches, like [STaR](https://arxiv.org/abs/2203.14465),
    [Reflexion](https://arxiv.org/abs/2303.11366), and [RAP](https://arxiv.org/abs/2305.14992),
    already drawing inspiration from RL. However, we have yet to witness a breakthrough
    method that effectively employs RL to enhance an LLMâ€™s reasoning capabilities,
    especially when compared to the advancements in RLHF for alignment.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ On the flip side, in the future, language could become the primary medium
    of expression in RL systems. The key advantage lies in the rich information of
    language compared with the traditional scalar rewards / values. The prospect of
    an LLM agent that can autonomously improve its reasoning abilities with RL (no
    need for supervision data or prompting engineering), is not only exciting but
    may also indicate a significant leap towards AGI.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Yihong Chen:'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1ï¸âƒ£ Structured & unstructured. I assume LLMs will be gradually eating the pies
    of traditional products, which are mostly based on large databases, rules and
    piles of small classifiers. In this case, what we are referring as â€œLLM reasoningâ€
    is probably the hope that we will have a method â€œXâ€ that can bridge the structured
    world, which most product data is currently living in, and the unstructured world,
    which most LLMs are living in. Knowledge graphs kind of champion the structured
    world and there have been fruity research on how to reason well on a knowledge
    graph, while LLMs are championing the unstructured world though we are still unclear
    about how they do the reasoning. They have different advantages and limitations.
    Iâ€™d expect that a nice bridge between the two would lead us to more pragmatic
    solutions for products.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 2ï¸âƒ£ Sample efficiency. As Zhaocheng mentioned, current LLMs reasoning is hard
    at generalising across a large number of tasks. Instead of ad-hoc efforts, which
    are usually customised for specific problems, I would be interested in if we can
    simply pretrain a LLM that generalize with less data, similar to whatâ€™s done for
    [generalizing across multiple languages](https://arxiv.org/abs/2307.01163).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 3ï¸âƒ£ Reasoning inside LLMs. As Abulhair and Michael mentioned, the community
    do not have a crystal understanding about how LLMs perform reasoning, if they
    indeed are reasoning. Iâ€™d expect more efforts on reverse-engineer LLMsâ€™ reasoning
    process, either in a [mechanistic way](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide)
    or other interpretability approaches.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Meme Time
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following the tradition of [Michael Galkin](https://mgalkin.medium.com/), no
    blog post is truely complete without a meme. DALLÂ·E 3 is almost a meme wizard
    if it can spell words correctly. Guess what prompts are used for each panel?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c0de490cd0427c4127e390af9d93917.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: What an LLM learned and what it can reason about. Image by authors & DALLÂ·E
    3.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Read More
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If this blog left you wanting to learn more about LLM reasoning, take a look
    at the following awesome blog posts.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[Towards Complex Reasoning: the Polaris of Large Language Models](https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21)
    by Yao Fu.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
    by Lilian Weng.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
