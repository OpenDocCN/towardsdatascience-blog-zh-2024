- en: GPT from Scratch with MLX
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始使用 MLX 构建 GPT
- en: 原文：[https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15](https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15](https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e?source=collection_archive---------0-----------------------#2024-06-15)
- en: Define and train GPT-2 on your MacBook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你的 MacBook 上定义并训练 GPT-2
- en: '[](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)[![Pranav
    Jadhav](../Images/363dc9008e3e4d94a9566057cad59806.png)](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)
    [Pranav Jadhav](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)[![Pranav
    Jadhav](../Images/363dc9008e3e4d94a9566057cad59806.png)](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)
    [Pranav Jadhav](https://medium.com/@pranavj1?source=post_page---byline--acf2defda30e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)
    ·31 min read·Jun 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--acf2defda30e--------------------------------)
    ·31分钟阅读·2024年6月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e34fda6779460bc952923c13f0e9e939.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34fda6779460bc952923c13f0e9e939.png)'
- en: Photo by [Sergey Zolkin](https://unsplash.com/@szolkin?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sergey Zolkin](https://unsplash.com/@szolkin?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: My goal with this post is to walk you through defining and training GPT-2 from
    scratch with [MLX](https://github.com/ml-explore/mlx), Apple’s machine-learning
    library for Apple silicon. I want to leave no stone unturned from tokenizer to
    sampling. In the spirit of [Karpathy’s excellent GPT from scratch tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5978s),
    we will train a model on the works of Shakespeare [1]. We will start with a blank
    Python file and end with a piece of software that can write Shakespeare-like text.
    And we’ll build it all in MLX, which makes training on inference on Apple silicon
    much faster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是带领你一步步从头开始定义并训练 GPT-2，使用 [MLX](https://github.com/ml-explore/mlx)，苹果为
    Apple Silicon 提供的机器学习库。我希望在从分词器到采样的过程中，毫无遗漏地讲解每一步。沿袭 [Karpathy 的精彩 GPT 从零开始教程](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5978s)的精神，我们将训练一个基于莎士比亚作品的模型
    [1]。我们将从一个空白的 Python 文件开始，最终构建出一个能够写出莎士比亚风格文本的软件。所有这些都将在 MLX 中构建，这使得在 Apple Silicon
    上的训练和推理速度更快。
- en: This post is best experienced by following along. The code is contained in the
    following repo which I suggest opening and referencing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文最好通过跟随操作来体验。代码包含在以下仓库中，我建议打开并参考它。
- en: '[](https://github.com/pranavjad/mlx-gpt2?source=post_page-----acf2defda30e--------------------------------)
    [## GitHub - pranavjad/mlx-gpt2'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/pranavjad/mlx-gpt2?source=post_page-----acf2defda30e--------------------------------)
    [## GitHub - pranavjad/mlx-gpt2'
- en: Contribute to pranavjad/mlx-gpt2 development by creating an account on GitHub.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在 GitHub 上创建帐户，贡献于 pranavjad/mlx-gpt2 的开发。
- en: github.com](https://github.com/pranavjad/mlx-gpt2?source=post_page-----acf2defda30e--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/pranavjad/mlx-gpt2?source=post_page-----acf2defda30e--------------------------------)
- en: Table of Contents
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[Preparing the data](#c3a0)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[准备数据](#c3a0)'
- en: '[Coding GPT-2](#1d50)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[编码 GPT-2](#1d50)'
- en: '[Input Embeddings](#9d27)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[输入嵌入](#9d27)'
- en: '[Positional Embeddings](#5507)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[位置嵌入](#5507)'
- en: '[Self Attention](#b6bc)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自注意力](#b6bc)'
- en: '[Keys, Queries, and Values](#718c)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[键、查询和数值](#718c)'
- en: '[Multi-Head Attention](#38de)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多头注意力](#38de)'
- en: '[MLP](#43c1)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多层感知机（MLP）](#43c1)'
- en: '[Block](#91b3)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[模块](#91b3)'
- en: '[Layernorms and Skip Connections](#dc6e)'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[层归一化和跳跃连接](#dc6e)'
- en: '[Forward Pass](#c593)'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[前向传播](#c593)'
- en: '[Sampling](#c3cc)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[采样](#c3cc)'
- en: '[Initialization](#2077)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[初始化](#2077)'
- en: '[Training Loop](#56cf)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[训练循环](#56cf)'
- en: '[References](#0e89)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[参考文献](#0e89)'
- en: Preparing the data
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: Install mlx and run the following imports.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 mlx 并运行以下导入语句。
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The first step to training an LLM is collecting a large corpus of text data
    and then tokenizing it. Tokenization is the process of mapping text to integers,
    which can be fed into the LLM. Our training corpus for this model will be the
    works of Shakespeare concatenated into one file. This is roughly 1 million characters
    and looks like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLM的第一步是收集大量的文本数据，然后对其进行分词。分词是将文本映射到整数的过程，这些整数可以输入到LLM中。我们这个模型的训练语料库将是莎士比亚的作品，这些作品会被拼接成一个文件。大约有100万个字符，格式如下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: First, we read the file as a single long string into the `text` variable. Then
    we use the `set()` function to get all the unique characters in the text which
    will be our vocabulary. By printing `vocab` you can see all the characters in
    our vocabulary as one string, and we have a total of 65 characters which till
    be our tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将文件作为一个长字符串读取到`text`变量中。然后我们使用`set()`函数获取文本中的所有唯一字符，这些字符将构成我们的词汇表。通过打印`vocab`，你可以看到我们词汇表中的所有字符作为一个字符串，我们一共有65个字符，这些将作为我们的token。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Production models will use tokenization algorithms like byte-pair encoding to
    generate a larger vocabulary of sub-word chunks. Since our focus today is on the
    architecture, we will continue with character-level tokenization. Next, we will
    map our vocabulary to integers known as token IDs. Then we can encode our text
    into tokens and decode them back to a string.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生产模型将使用像字节对编码（byte-pair encoding）这样的分词算法来生成更大的子词词汇表。由于我们今天的重点是架构，因此我们将继续使用字符级别的分词。接下来，我们将把我们的词汇映射到称为token
    ID的整数。然后我们可以将文本编码为token并解码回字符串。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use the`enumerate()` function to iterate over all characters and their index
    in the vocabulary and create a dictionary `itos` which maps integers to characters
    and `stoi` which maps strings to integers. Then we use these mappings to create
    our encode and decode functions. Now we can encode the entire text and split training
    and validation data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`enumerate()`函数遍历词汇表中的所有字符及其索引，并创建一个字典`itos`，它将整数映射到字符，另一个字典`stoi`，它将字符串映射到整数。然后我们使用这些映射来创建我们的编码和解码函数。现在我们可以编码整个文本，并拆分训练数据和验证数据。
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Currently, our training data is just a very long string of tokens. However,
    we are trying to train our model to predict the next token some given previous
    tokens. Therefore our dataset should be comprised of examples where the input
    is some string of tokens and the label is the correct next token. We need to define
    a model parameter called *context length* which is the maximum number of tokens
    used to predict the next token. Our training examples will be the length of our
    context length.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，我们的训练数据只是一个非常长的token字符串。然而，我们正在训练模型以预测给定一些前置token后的下一个token。因此，我们的数据集应该包含输入是某个token字符串、标签是正确下一个token的示例。我们需要定义一个模型参数，称为*上下文长度*，即用于预测下一个token的最大token数量。我们的训练示例将是我们上下文长度的长度。
- en: Let’s look at the first `ctx_len+1` tokens.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下前`ctx_len+1`个token。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is one training example where the input is “18, 47, 56, 57, 58, 1, 15,
    47” and the desired output is “58”. This is 8 tokens of context. However, we also
    want to train the model to predict the next token given only 7, 6, 5 … 0 tokens
    as context which is needed during generation. Therefore we also consider the 8
    sub examples packed into this example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个训练示例，其中输入是“18, 47, 56, 57, 58, 1, 15, 47”，期望输出是“58”。这是8个token的上下文。然而，我们还希望训练模型在只给定7、6、5
    … 0个token作为上下文时预测下一个token，这在生成过程中是必要的。因此，我们还考虑了这个示例中包含的8个子示例：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that the labels are simply the inputs shifted left.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，标签只是输入左移后的结果。
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At index 0 the input is 18 and the label is 47\. At index 1 the input is everything
    before and including index 1 which is [18, 47] and the label is 56, etc. Now that
    we understand that the labels are simply the input sequence indexed one higher
    we can build our datasets.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引0处，输入是18，标签是47。 在索引1处，输入是包括索引1之前的所有内容，即[18, 47]，标签是56，依此类推。现在我们明白标签仅仅是将输入序列的索引增加1后得到的结果，我们可以构建我们的数据集。
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We loop through the data and take chunks of size `ctx_len` as the inputs (X)
    and then take the same chunks but at 1 higher index as the labels (y). Then we
    take these Python lists and create mlx array objects from them. The model internals
    will be written with mlx so we want our inputs to be mlx arrays.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历数据，将大小为`ctx_len`的块作为输入（X），然后将相同大小的块，但索引加1后的部分作为标签（y）。然后我们将这些Python列表转化为mlx数组对象。模型的内部会使用mlx编写，因此我们希望我们的输入是mlx数组。
- en: One more thing. During training we don’t want to feed the model one example
    at a time, we want to feed it multiple examples in parallel for efficiency. This
    group of examples is called our batch, and the number of examples in a group is
    our batch size. Thus we define a function to generate batches for training.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事。在训练过程中，我们不希望一次只输入一个样本，而是希望一次输入多个样本，以提高效率。这组样本被称为我们的批次，而每个批次中的样本数量就是我们的批量大小。因此，我们定义一个函数来生成训练用的批次。
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If shuffle=True, we shuffle the data by indexing it with a randomly shuffled
    index. Then we loop through our dataset and return batch-size chunks from input
    and label datasets. These chunks are known as mini-batches and are just stacked
    examples that we process in parallel. These mini-batches will be our input to
    the model during training.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`shuffle=True`，我们通过随机打乱的索引对数据进行打乱。然后我们遍历数据集，并从输入数据和标签数据集中返回批量大小的块。这些块被称为小批次，它们只是我们并行处理的堆叠样本。这些小批次将在训练过程中作为输入提供给模型。
- en: Here’s an example of a minibatch of 4 examples with context length 8.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个上下文长度为8的小批次示例，包含4个样本。
- en: '![](../Images/92f8cb95ed41ef0d51156a6ba2aad2f5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92f8cb95ed41ef0d51156a6ba2aad2f5.png)'
- en: A single minibatch (image by author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小批次（图由作者提供）
- en: This minibatch packs 32 next-token prediction problems. The model will predict
    the next token for each token in the input and the labels will be used to calculate
    the loss. Notice that the labels contain the next token for each index of the
    inputs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小批次包含了32个下一个标记预测问题。模型将为输入中的每个标记预测下一个标记，标签将用于计算损失。注意，标签包含了输入中每个索引对应的下一个标记。
- en: You’ll want to keep this picture in your mind because the shapes of these tensors
    will get hairy. For now, just remember that we will input a tensor of shape (batch_size,
    ctx_len) to the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你会希望将这个图像牢记在心，因为这些张量的形状会变得非常复杂。现在，只需要记住，我们将向模型输入一个形状为（batch_size, ctx_len）的张量。
- en: Coding GPT-2
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写GPT-2代码
- en: Let’s look at the GPT-2 architecture to get an overview of what we are trying
    to implement.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GPT-2架构，以便了解我们要实现的整体结构。
- en: '![](../Images/eefb8f6fd70479937d7b028ab70083a8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eefb8f6fd70479937d7b028ab70083a8.png)'
- en: GPT-2 Architecture (image by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2架构（图由作者提供）
- en: Don’t worry if this looks confusing. We will implement it step by step from
    bottom to top. Let’s start by implementing the input embeddings.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来有点困惑，不用担心。我们将从下到上一步步实现它。让我们从实现输入嵌入开始。
- en: Input Embeddings
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入嵌入
- en: The purpose of the input embedding layer is to map token IDs to vectors. Each
    token will be mapped to a vector which will be its representation as it is forwarded
    through the model. The vectors for each token will accumulate and exchange information
    as they pass through the model and eventually be used to predict the next token.
    These vectors are called embeddings.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输入嵌入层的目的是将标记ID映射到向量。每个标记将映射到一个向量，该向量将在模型中被传递并作为其表示。每个标记的向量将在模型中相互积累并交换信息，最终被用来预测下一个标记。这些向量被称为嵌入。
- en: The simplest way to map token IDs to vectors is through a lookup table. We create
    a matrix of size (vocab_size, n_emb) where each row is the embedding vector for
    the corresponding token. This matrix is known as the embedding weights.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 将标记ID映射到向量的最简单方法是通过查找表。我们创建一个大小为（vocab_size, n_emb）的矩阵，其中每一行是对应标记的嵌入向量。这个矩阵被称为嵌入权重。
- en: '![](../Images/bc9c1b7194ed45e12e28d4c3e95db59c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc9c1b7194ed45e12e28d4c3e95db59c.png)'
- en: Embedding Layer (image by author)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层（图由作者提供）
- en: The diagram shows an example embedding layer of size (65, 6). This means there
    are 65 tokens in the vocabulary and each one will be represented by a length 6
    embedding vector. The inputted sequence will be used to index the embedding weights
    to get the vector corresponding to each token. Remember the minibatches we input
    into the model? Originally the minibatch is size (batch_size, ctx_len). After
    passing through the embedding layer it is size (batch_size, ctx_len, n_emb). Instead
    of each token being a single integer, each token is now a vector of length n_emb.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了一个大小为（65, 6）的嵌入层示例。这意味着词汇表中有65个标记，每个标记将通过长度为6的嵌入向量表示。输入的序列将用于通过嵌入权重索引，以获取与每个标记对应的向量。记住我们输入到模型中的小批次吗？最初，小批次的大小是（batch_size,
    ctx_len）。经过嵌入层处理后，大小变为（batch_size, ctx_len, n_emb）。每个标记不再是一个单一的整数，而是一个长度为`n_emb`的向量。
- en: Let’s define the embedding layer in code now.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在代码中定义嵌入层。
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will define a class to organize our implementation. We subclass nn.Module
    to take advantage of mlx’s features. Then in the init function, we call the superclass
    constructor and initialize our token embedding layer called `wte` .
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个类来组织我们的实现。我们继承nn.Module，以便利用mlx的特性。然后在init函数中，我们调用父类构造函数并初始化我们的token嵌入层，命名为`wte`。
- en: Positional Embeddings
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置嵌入
- en: Next up is the positional embeddings. The purpose of positional embeddings is
    to encode information about the position of each token in the sequence. This can
    be added to our input embeddings to get a complete representation of each token
    that contains information about the token’s position in the sequence.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是位置嵌入。位置嵌入的目的是编码序列中每个token的位置相关信息。这些信息可以与我们的输入嵌入相加，从而获得一个完整的token表示，该表示包含了token在序列中的位置信息。
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The position embeddings work the same as token embeddings, except instead of
    having a row for each token we have a row for each possible position index. This
    means our embedding weights will be of shape (ctx_len, n_emb). Now we implement
    the __call__ function in our GPT class. This function will contain the forward
    pass of the model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 位置嵌入的工作方式与token嵌入相同，不同之处在于我们不是为每个token提供一行，而是为每个可能的位置索引提供一行。这意味着我们的嵌入权重的形状将是(ctx_len,
    n_emb)。现在我们在我们的GPT类中实现__call__函数。这个函数将包含模型的前向传播过程。
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First, we break out the dimensions of our input into variables B and T for easy
    handling. In sequence modeling contexts B and T are usually used as shorthand
    for “batch” and “time” dimensions. In this case, the “time” dimension of our sequence
    is the context length.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入的维度拆解为变量B和T，方便处理。在序列建模的上下文中，B和T通常是“batch”（批次）和“time”（时间）维度的简写。在这种情况下，序列的“时间”维度即为上下文长度。
- en: Next, we calculate token and position embeddings. Notice that for the position
    embeddings, our input is `mx.arange(T)` . This will output an array of consecutive
    integers from 0 to T-1 which is exactly what we want because those are the positions
    we want to embed. After passing that through the embedding layer we will have
    a tensor of shape (T, n_emb) because the embedding layer plucks out the n_emb
    length vector for each of the T positions. Note that even though pos_emb is not
    the same shape as tok_emb we can add the two because mlx will broadcast, or replicate
    pos_emb across the batch dimension to allow elementwise addition. Finally, we
    perform the addition to get the new representations of the tokens with positional
    information.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算token和位置的嵌入。注意，对于位置嵌入，我们的输入是`mx.arange(T)`。这个操作将输出一个从0到T-1的连续整数数组，正好符合我们的需求，因为这些正是我们要嵌入的位置。将其通过嵌入层后，我们将得到一个形状为(T,
    n_emb)的张量，因为嵌入层会为每个位置提取一个长度为n_emb的向量。注意，尽管pos_emb与tok_emb的形状不同，但我们仍然可以将它们相加，因为mlx会进行广播操作，即在批次维度上复制pos_emb，以便实现逐元素相加。最后，我们进行相加操作，得到包含位置信息的token新表示。
- en: Self-Attention
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自注意力
- en: So far the representation vectors for each token have been calculated independently.
    They have not had the opportunity to exchange any information. This is intuitively
    bad in language modeling because the meaning and usage of words depend on the
    surrounding context. Self-attention is how we incorporate information from previous
    tokens into a given token.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个token的表示向量是独立计算的。它们没有交换任何信息的机会。从语言建模的角度来看，这显然是有问题的，因为单词的含义和用法依赖于周围的上下文。自注意力机制是我们将前面token的信息融入到当前token的一种方式。
- en: First, let’s consider a naive approach. What if we simply represented each token
    as the average of its representation vector and the vectors of all the tokens
    before it? This achieves our goal of packing information from previous tokens
    into the representation for a given token. Here’s what it would look like.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们考虑一种简单的方法。如果我们仅仅将每个token表示为其表示向量和所有前面token的表示向量的平均值呢？这样就能达到将前面token的信息打包到当前token表示中的目标。下面是它的样子。
- en: '![](../Images/d1af7ef1560b69aaa50ff00af108fba9.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1af7ef1560b69aaa50ff00af108fba9.png)'
- en: image by author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: But self-attention doesn’t involve writing a for-loop. The key insight is we
    can achieve this previous token averaging with matrix multiplication!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是自注意力并不涉及编写for循环。关键的见解是，我们可以通过矩阵乘法来实现之前token的平均化！
- en: '![](../Images/4033f38082734c80ea099705718c3e00.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4033f38082734c80ea099705718c3e00.png)'
- en: image by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: By multiplying our input sequence on the left by a special matrix we get the
    desired result. This matrix is known as the attention weights. Notice that each
    row of the attention weight matrix specificies “how much” of each other token
    goes into the representation for any given token. For example in row two, we have
    [0.5, 0.5, 0, 0]. This means that row two of the result will be `0.5*token1 +
    0.5*token2 + 0*token3 + 0*token4` , or the average of token1 and token2\. Note
    that the attention weights are a lower-triangular matrix (zeros in upper right
    entries). This ensures that future tokens will not be included in the representation
    of a given token. This ensures that tokens can only communicate with the previous
    tokens because during generation the model will only have access to previous tokens.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将我们的输入序列左乘一个特殊矩阵，我们得到了期望的结果。这个矩阵被称为注意力权重。请注意，注意力权重矩阵的每一行都指定了“多少”每个其他 token
    进入给定 token 的表示。例如，在第二行中，我们有 [0.5, 0.5, 0, 0]。这意味着结果的第二行将是 `0.5*token1 + 0.5*token2
    + 0*token3 + 0*token4`，即 token1 和 token2 的平均值。请注意，注意力权重是一个下三角矩阵（右上角为零）。这确保了未来的
    token 不会被包含在某个给定 token 的表示中。这保证了 token 只能与之前的 token 进行通信，因为在生成过程中，模型只能访问到之前的 tokens。
- en: Let’s look at how we can construct the attention weight matrix.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何构建注意力权重矩阵。
- en: '![](../Images/03c45bdcb15092d5ba7d2ca01b63e5c1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03c45bdcb15092d5ba7d2ca01b63e5c1.png)'
- en: image by author
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Notice that if we create an array of zeros with -inf in the upper right entries
    and then perform row-wise softmax we get the desired attention weights. A good
    exercise is to step through the softmax calculation for a row to see how this
    works. The takeaway is that we can take some array of size (ctx_len, ctx_len)
    and softmax each row to get attention weights that sum to one.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们创建一个包含 -inf 的零数组，并将其放置在右上角位置，然后对每一行进行 softmax 操作，我们就能得到期望的注意力权重。一个好的练习是逐步进行某一行的
    softmax 计算，看看它是如何工作的。结论是，我们可以取一个大小为 (ctx_len, ctx_len) 的数组，并对每一行进行 softmax 操作，从而得到加和为一的注意力权重。
- en: Now we can leave the realm of naive self-attention. Instead of simply averaging
    previous tokens, we use arbitrary weighted sums over previous tokens. Notice what
    happens when we do row-wise softmax of an arbitrary matrix.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以离开朴素自注意力的领域了。我们不仅仅是对之前的 tokens 取平均值，而是对之前的 tokens 进行任意加权求和。注意，当我们对任意矩阵进行逐行
    softmax 时会发生什么。
- en: '![](../Images/11897ada690ad865eea3d6617a738316.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11897ada690ad865eea3d6617a738316.png)'
- en: image by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: We still get weights that sum to one on each row. During training, we can learn
    the numbers in the matrix on the left which will specify how much each token goes
    into the representation for another token. This is how tokens pay “attention”
    to each other. But we still haven’t understood where this matrix on the left came
    from. These pre-softmax attention weights are calculated from the tokens themselves,
    but indirectly through three linear projections.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然得到每行加和为一的权重。在训练过程中，我们可以学习左侧矩阵中的数值，这些数值将指定每个 token 如何影响其他 token 的表示。这就是 token
    如何彼此“关注”的方式。但我们仍然没有理解这个左侧矩阵是如何产生的。这些预 softmax 注意力权重是从 token 本身计算出来的，但通过三次线性投影间接获得的。
- en: '**Keys, Queries, and Values**'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**键、查询和值**'
- en: '![](../Images/ea6acf7b1fde813fde398d59348b6dbc.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea6acf7b1fde813fde398d59348b6dbc.png)'
- en: image by author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Each token in our sequence emits 3 new vectors. These vectors are called keys,
    queries, and values. We use the dot product of the query vector of one token and
    the key vector of another token to quantify the “affinity” those two tokens have.
    We want to calculate the pairwise affinities of each token with every other token,
    therefore we multiply the query vector (4x3) with the key vector transposed (3x4)
    to get the raw attention weights (4x4). Due to the way matrix multiplication works
    the (i,j) entry in the raw attention weights will be the query of token i dot
    the key of token j or the “affinity” between the two. Thus we have calculated
    interactions between every token. However, we don’t want past tokens interacting
    with future tokens so we apply a mask of -inf to the upper right entries to ensure
    they will zero out after softmax. Then we perform row-wise softmax to get the
    final attention weights. Instead of multiplying these weights directly with the
    input, we multiply them with the value projection. This results in the new representations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们序列中的每个令牌都会生成三个新的向量。这些向量被称为键（keys）、查询（queries）和值（values）。我们使用一个令牌的查询向量与另一个令牌的键向量的点积来量化这两个令牌之间的“亲和力”。我们希望计算每个令牌与其他每个令牌的成对亲和力，因此我们将查询向量（4x3）与转置后的键向量（3x4）相乘，以得到原始的注意力权重（4x4）。由于矩阵乘法的方式，原始注意力权重中（i,j）位置的值将是令牌i的查询向量与令牌j的键向量的点积，或者说是它们之间的“亲和力”。这样，我们就计算了每个令牌之间的相互作用。然而，我们不希望过去的令牌与未来的令牌发生交互，因此我们对右上部分的条目应用-∞的掩码，以确保它们在软最大化（softmax）之后会被归零。然后，我们执行按行的软最大化操作，以得到最终的注意力权重。我们并不直接将这些权重与输入相乘，而是将它们与值的投影相乘，这样就得到了新的表示。
- en: Now that we understand attention conceptually, let’s implement it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在概念上已经理解了注意力机制，让我们来实现它。
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We start by defining the key, query, and value projection layers. Note that
    instead of going from n_emb to n_emb, we project from n_emb to head_size. This
    doesn’t change anything, it just means the new representations calculated by attention
    will be dimension head_size.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义键、查询和值的投影层。请注意，我们并不是从n_emb映射到n_emb，而是从n_emb映射到head_size。这并不改变任何东西，只是意味着通过注意力计算得到的新表示将具有head_size维度。
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The forward pass begins by calculating the key, query, and value projections.
    We also break out the input shape into the variables B, T, and C for future convenience.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播开始时计算键、查询和值的投影。为了便于以后使用，我们还将输入的形状拆解为变量B、T和C。
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we calculate the attention weights. We only want to transpose the last
    two dimensions of the key tensor, because the batch dimension is just there so
    we can forward multiple training examples in parallel. The mlx transpose function
    expects the new order of the dimensions as input, so we pass it [0, 2, 1] to transpose
    the last two dimensions. One more thing: we scale the attention weights by the
    inverse square root of head_size. This is known as scaled attention and the purpose
    is to ensure that when Q and K are unit variance, attn_weights will be unit variance.
    If the variance of attn_weights is high, then the softmax will map these small
    and large values to 0 or 1which results in less complex representations.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算注意力权重。我们只需要转置键张量的最后两个维度，因为批量维度只是为了让我们可以并行处理多个训练示例。mlx转置函数期望输入的新维度顺序，因此我们传递[0,
    2, 1]来转置最后两个维度。还有一件事：我们通过head_size的平方根的倒数来缩放注意力权重。这被称为缩放注意力，其目的是确保当Q和K的方差为单位方差时，attn_weights的方差也是单位方差。如果attn_weights的方差很大，那么softmax将把这些小值和大值映射为0或1，从而导致表示变得不那么复杂。
- en: The next step is to apply the mask to ensure we are doing causal language modeling
    i.e. ensuring tokens cannot attend to future tokens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是应用掩码，以确保我们进行的是因果语言建模，即确保令牌无法关注未来的令牌。
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We create the mask with a clever broadcasting trick. Let’s say our ctx_len=4
    like in the diagrams above. First, we use mx.arange(4) to set the indices variable
    to [0, 1, 2, 3].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过巧妙的广播技巧创建掩码。假设我们的ctx_len=4，如上面的图示所示。首先，我们使用mx.arange(4)将索引变量设置为[0, 1, 2,
    3]。
- en: '![](../Images/776a8fa277964a2ae02319291108f577.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/776a8fa277964a2ae02319291108f577.png)'
- en: image by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: 'Then we can index like so `indices[:, None]` to generate a column vector with
    the values of indices. Similarly, we can get a row vector using `indices[None]`.
    Then when we do the < comparison, mlx broadcasts the vectors because they have
    mismatching shapes so they can’t be compared elementwise. Broadcasting means mlx
    will replicate the vectors along the lacking dimension. This results in an elementwise
    comparison of two (4, 4) matrices which makes sense. Side note: I recommend familiarizing
    yourself with the details of broadcasting by reading [this](https://pytorch.org/docs/stable/notes/broadcasting.html),
    it comes up all the time when dealing with tensors.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以像这样索引 `indices[:, None]` 来生成一个包含 `indices` 值的列向量。类似地，我们可以使用 `indices[None]`
    来获取一个行向量。然后，当我们进行 `<` 比较时，mlx 会广播这两个向量，因为它们的形状不匹配，无法进行元素级比较。广播意味着 mlx 会沿缺失的维度复制这些向量，从而实现两个
    (4, 4) 矩阵的逐元素比较，这就合理了。顺便说一下，我建议通过阅读 [这个](https://pytorch.org/docs/stable/notes/broadcasting.html)
    来熟悉广播的细节，它在处理张量时经常出现。
- en: 'After the elementwise comparison, we are left with the following tensor:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 元素级比较后，我们得到如下张量：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Multiplying this tensor by -1e9, we get:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个张量乘以 -1e9，我们得到：
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we have an additive mask. We can add this matrix to our attention weights
    to make all the upper right entries very large negative numbers. This will cause
    them to be zeroed out after the softmax operation. Also, note that we add “_”
    as a prefix to the attribute name `_causal_mask` which marks it as a private variable.
    This signals to mlx that it is not a parameter and should not be updated during
    training.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个加性掩码。我们可以将这个矩阵加到我们的注意力权重上，使得所有右上角的条目变得非常大的负数。这将导致它们在 softmax 操作后被置为零。此外，请注意，我们将“_”作为前缀添加到属性名
    `_causal_mask`，这将其标记为私有变量。这向 mlx 发出信号，表示它不是一个参数，训练过程中不应更新。
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we can softmax row-wise to get the final attention weights and multiply
    these weights by the values to get our output. Note we pass `axis=-1` to softmax
    which specifies that we want to softmax across the last dimension which are the
    rows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以按行应用 softmax，得到最终的注意力权重，并将这些权重与值相乘，得到我们的输出。注意，我们将 `axis=-1` 传递给 softmax，表示我们要在最后一个维度（即行）上进行
    softmax 操作。
- en: The final step is output linear projection and dropout.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是输出的线性投影和丢弃层。
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We add two new layers, `c_proj` and `resid_dropout` which are the output projection
    and residual dropout. The output projection is to return the vectors to their
    original dimension n_emb. The dropout is added for regularization and training
    stability which is important as we start layering the transformer blocks to get
    a deep network. And that’s it for implementing one attention head!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了两个新层，`c_proj` 和 `resid_dropout`，分别是输出投影和残差丢弃层。输出投影用于将向量恢复到原始维度 `n_emb`。丢弃层用于正则化和训练稳定性，尤其在我们开始堆叠
    Transformer 块以构建深度网络时，这非常重要。这就是实现一个注意力头的全部内容！
- en: Multi-Head Attention
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Instead of having just one attention head LLMs often use multiple attention
    heads in parallel and concatenate their outputs to create the final representation.
    For example, let’s say we had one attention head with head_size=64 so the vector
    it produced for each token was 64 dimensional. We could achieve the same thing
    with 4 parallel attention heads each with head_size=16 by concatenating their
    outputs to produce a 16x4 = 64 dimensional output. Multi-head attention allows
    the model to learn more complex representations because each head learns different
    projections and attention weights.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 通常使用多个并行的注意力头，而不仅仅是一个注意力头，并将它们的输出拼接起来生成最终表示。例如，假设我们有一个注意力头，其 `head_size=64`，那么它为每个标记生成的向量是
    64 维的。我们也可以通过使用 4 个并行的注意力头，每个头的 `head_size=16`，并将它们的输出拼接起来，得到一个 16x4=64 维的输出，来实现相同的效果。多头注意力使得模型能够学习更复杂的表示，因为每个头学习不同的投影和注意力权重。
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The straightforward implementation is to create a list of `n_heads` attention
    heads where each one has size equal to our final head size divided by n_heads.
    Then we concatenate the output of each head over the last axis. However, this
    implementation is inefficient and does not take advantage of the speed of tensors.
    Let’s implement multi-head attention with the power of tensors.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 直接的实现方式是创建一个 `n_heads` 的注意力头列表，其中每个头的大小等于最终头大小除以 `n_heads`。然后我们将每个头的输出沿最后一个维度进行拼接。然而，这种实现效率低下，并没有充分利用张量的计算速度。让我们利用张量的强大功能来实现多头注意力。
- en: '[PRE22]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We start with our single-head attention implementation. The `__init__()` function
    has not changed. The forward pass begins as normal with the creation of the key,
    query, and value projections.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从单头注意力的实现开始。`__init__()`函数没有改变。前向传播像往常一样开始，首先是创建键、查询和值的投影。
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next thing we need to do is introduce a new dimension for the number of
    heads `n_heads` . In the naive implementation, we had separate attention objects
    each with their own key, query, and value tensors but now we have them all in
    one tensor, therefore we need a dimension for the heads. We define the new shape
    we want in `mha_shape` . Then we use `mx.as_strided()` to reshape each tensor
    to have the head dimension. This function is equivalent to `view` from pytorch
    and tells mlx to treat this array as a different shape. But we still have a problem.
    Notice that we if try to multiply `Q @ K_t` (where K_t is K transposed over it’s
    last 2 dims) to compute attention weights as we did before, we will be multiplying
    the following shapes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是引入一个新的维度来表示头部的数量`n_heads`。在朴素实现中，我们有独立的注意力对象，每个都有自己的键、查询和值张量，但现在我们将它们放在一个张量中，因此我们需要一个用于头部的维度。我们在`mha_shape`中定义我们想要的新形状。然后，我们使用`mx.as_strided()`来重新塑形每个张量，使其包含头部维度。这个函数相当于pytorch中的`view`，它告诉mlx将该数组视为不同的形状。但我们仍然存在一个问题。注意，如果我们尝试像之前一样将`Q
    @ K_t`（其中K_t是K的最后两个维度转置）相乘来计算注意力权重，我们将会乘上以下形状：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This would result in a tensor of shape `(B, T, n_heads, n_heads)` which is
    incorrect. With one head our attention weights were shape `(B, T, T)` which makes
    sense because it gives us the interaction between each pair of tokens. So now
    our shape should be the same but with a heads dimension: `(B, n_heads, T, T)`
    . We achieve this by transposing the dimensions of keys, queries, and values after
    we reshape them to make `n_heads` dimension 1 instead of 2.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致一个形状为`(B, T, n_heads, n_heads)`的张量，这是不正确的。在单头情况下，我们的注意力权重的形状为`(B, T, T)`，这是合理的，因为它展示了每对token之间的交互。因此，现在我们的形状应该是相同的，只不过多了一个头部维度：`(B,
    n_heads, T, T)`。我们通过在重新塑形后转置键、查询和值的维度来实现这一点，从而使`n_heads`维度变为1而不是2。
- en: '[PRE25]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we can calculate the correction attention weights. Notice that we scale
    the attention weights by the size of an individual attention head rather than
    head_size which would be the size after concatenation. We also apply dropout to
    the attention weights.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算修正后的注意力权重。注意，我们通过单个注意力头的大小来缩放注意力权重，而不是使用拼接后大小的`head_size`。我们还对注意力权重应用了dropout。
- en: Finally, we perform the concatenation and apply the output projection and dropout.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们执行拼接，并应用输出投影和dropout。
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Since we have everything in one tensor, we can do some shape manipulation to
    do the concatenation. First, we move `n_heads` back to the second to last dimension
    with the transpose function. Then we reshape back to the original size to undo
    the splitting into heads we performed earlier. This is the same as concatenating
    the final vectors from each head. And that’s it for multi-head attention! We’ve
    gotten through the most intense part of our implementation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将所有内容都放在一个张量中，所以我们可以进行一些形状操作来实现拼接。首先，我们使用转置函数将`n_heads`移回到倒数第二个维度。然后，我们将其重新塑形为原始大小，以撤销我们之前对头部的拆分。这就相当于将每个头部的最终向量进行拼接。这就是多头注意力的全部内容！我们已经完成了实现中最复杂的部分。
- en: MLP
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP
- en: The next part of the architecture is the multilayer perception or MLP. This
    is a fancy way of saying 2 stacked linear layers. There’s not much to be said
    here, it is a standard neural network.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的下一部分是多层感知机（MLP）。这实际上是指堆叠的2个线性层。这里没什么好说的，它是一个标准的神经网络。
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We take the input and project it to a higher dimension with `c_fc` . Then we
    apply gelu nonlinearity and project it back down to the embedding dimension with
    `c_proj` . Finally, we apply dropout and return. The purpose of the MLP is to
    allow for some computation after the vectors have communicated during attention.
    We will stack these communication layers (attention) and computation layers (mlp)
    into a block.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入通过`c_fc`投影到更高的维度。然后，我们应用gelu非线性函数，并通过`c_proj`将其投影回嵌入维度。最后，我们应用dropout并返回。MLP的目的是在注意力层的向量交换之后，进行一些额外的计算。我们将这些通信层（注意力）和计算层（mlp）堆叠成一个块。
- en: Block
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块
- en: A GPT block consists of attention followed by an MLP. These blocks will be repeated
    to make the architecture deep.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GPT块由注意力和MLP组成。这些块会被重复使用，从而使架构变得深度。
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, we need to add two more features to improve training stability. Let’s take
    a look at the architecture diagram again.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要添加两个额外的功能来提高训练稳定性。让我们再次查看架构图。
- en: Layernorms and Skip Connections
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层归一化与跳跃连接
- en: '![](../Images/406a59dd5cb9bb6780808b2e94787f92.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406a59dd5cb9bb6780808b2e94787f92.png)'
- en: image by author
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：作者
- en: We still need to implement the components highlighted in red. The arrows are
    skip connections. Instead of the input being transformed directly, the effect
    of the attention and MLP layers is additive. Their result is added to the input
    instead of directly replacing it. This is good for the training stability of deep
    networks since in the backward pass, the operands of an addition operation will
    receive the same gradient as their sum. Gradients can thus flow backwards freely
    which prevents issues like vanishing/exploding gradients that plague deep networks.
    Layernorm also helps with training stability by ensuring activations are normally
    distributed. Here is the final implementation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然需要实现那些以红色高亮显示的组件。箭头表示跳跃连接。输入不会直接被转换，而是注意力和MLP层的效果是加性的。它们的结果会被加到输入中，而不是直接替换输入。这对深度网络的训练稳定性有好处，因为在反向传播中，加法操作的操作数会接收到与其和相同的梯度。因此，梯度可以自由地向后流动，从而防止了像梯度消失/爆炸这类困扰深度网络的问题。层归一化也有助于训练稳定性，确保激活值服从正态分布。下面是最终的实现。
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Layernorm is applied before multi-head attention and MLP. The skip connections
    are added with `x = x + ...` making the operations additive.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化应用于多头注意力和MLP之前。跳跃连接通过`x = x + ...`的方式加入，使得操作变为加性。
- en: Forward Pass
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: With the Block defined, we can finish the full GPT-2 forward pass.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完Block之后，我们可以完成完整的GPT-2前向传播。
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We create a container for the blocks using `nn.Sequential` which takes any input
    and passes it sequentially through the contained layers. Then we can apply all
    the blocks with `self.blocks(x)` . Finally, we apply a layer norm and then the
    lm_head. The lm_head or language modeling head is just a linear layer that maps
    from the embedding dimension to the vocab size. The model will output a vector
    containing some value for each word in our vocabulary, or the logits. We can softmax
    the logits to get a probability distribution over the vocabulary which we can
    sample from to get the next token. We will also use the logits to calculate the
    loss during training. There are just two more things we need to implement before
    we begin training.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`nn.Sequential`创建一个容器来包含这些模块，该容器接受任何输入并将其按顺序传递通过各个层。然后我们可以通过`self.blocks(x)`应用所有模块。最后，我们应用层归一化，然后是lm_head。lm_head或语言模型头只是一个线性层，它将嵌入维度映射到词汇表大小。模型将输出一个包含每个词汇表中单词的某些值的向量，或称为logits。我们可以对logits进行softmax操作，以获得词汇表上的概率分布，然后从中采样得到下一个标记。我们还会使用logits在训练过程中计算损失。在开始训练之前，我们只需要实现两个组件。
- en: Sampling
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采样
- en: We need to write a generate function to sample from the model once training
    is complete. The idea is that we start with some sequence of our choice, then
    we predict the next token and append this to our sequence. Then we feed the new
    sequence in and predict the next token again. This continues until we decide to
    stop.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要编写一个生成函数，以便在训练完成后从模型中进行采样。其思路是：我们从选择的某个序列开始，然后预测下一个标记并将其添加到序列中。接着我们将新的序列输入模型，再次预测下一个标记。这个过程会一直持续，直到我们决定停止。
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We prompt the model with a single token, zero. Zero is the newline character
    so it is a natural place to start the generation since we just want to see how
    Shakespeare-like our model can get. Note that we initialize the shape to (1, 1)
    to simulate a single batch with a sequence length of one.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个单独的标记零来提示模型。零是换行符，因此它是生成的自然起点，因为我们只是想看看我们的模型能生成多像莎士比亚的风格。请注意，我们将形状初始化为(1,
    1)，以模拟一个长度为1的单个批次。
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Then we get the logits for the next token by passing in the last ctx_len characters
    to the model. However, our model output is of shape `(B, T, vocab_size)` since
    it predicts the next token logits for each token in the input. We use all of that
    during training, but now we only want the logits for the last token because we
    can use this to sample a new token. Therefore we index the logits to get the last
    element in the first dimension which is the sequence dimension. Then we sample
    the next token using the `mx.random.categorical()` function which takes the logits
    and the number of samples we want as input. This function will softmax the logits
    to turn them into a probability distribution and then randomly sample a token
    according to the probabilities. Finally, we concatenate the new token to the context
    and repeat the process `max_new_tokens` number of times.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将最后的ctx_len个字符传递给模型来获得下一个token的logits。然而，我们的模型输出形状为`(B, T, vocab_size)`，因为它预测输入中每个token的下一个token的logits。在训练过程中我们会使用全部信息，但现在我们只想获取最后一个token的logits，因为我们可以利用这个信息来采样一个新的token。因此，我们索引logits，获取第一个维度中最后一个元素，即序列维度。接着，我们使用`mx.random.categorical()`函数来采样下一个token，该函数接受logits和我们希望的采样数量作为输入。这个函数会对logits进行softmax，将其转化为概率分布，然后根据概率随机采样一个token。最后，我们将新的token拼接到上下文中，并重复这个过程`max_new_tokens`次。
- en: Initialization
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化
- en: The last thing to do is handle weight initialization which is important for
    training dynamics.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要做的是处理权重初始化，这对训练动态非常重要。
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: First, we define two different `nn.init.normal` functions. The first one is
    for initializing all linear and embedding layers. The second one is for initializing
    linear layers that are specifically residual projections i.e. the last linear
    layer inside multi-head attention and MLP. The reason for this special initialization
    is that it checks accumulation along the residual path as model depth increases
    according to the GPT-2 paper [2].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了两个不同的`nn.init.normal`函数。第一个用于初始化所有线性层和嵌入层。第二个用于初始化专门的残差投影线性层，即多头注意力和MLP中的最后一个线性层。进行这种特殊初始化的原因是，它会根据GPT-2论文[2]检查随着模型深度增加，残差路径上的累积。
- en: In mlx we can change the parameters of the model using the `[mx.update()](https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.Module.update.html#mlx.nn.Module.update)`
    function. Checking the docs, it expects a complete or partial dictionary of the
    new model parameters. We can see what this dictionary looks like by printing out
    `self.parameters()` inside the GPT class.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在mlx中，我们可以使用`[mx.update()](https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.Module.update.html#mlx.nn.Module.update)`函数更改模型的参数。查看文档，它期望一个完整或部分的字典，包含新的模型参数。我们可以通过在GPT类中打印`self.parameters()`来查看这个字典的样子。
- en: '[PRE34]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It’s a nested dictionary containing each model weight as an mx.array. So to
    initialize the parameters of our model we need to build up a dictionary like this
    with our new params and pass them to `self.update()` . We can achieve this as
    follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个嵌套的字典，包含每个模型的权重，类型为mx.array。因此，为了初始化我们模型的参数，我们需要像这样构建一个包含新参数的字典，并将其传递给`self.update()`。我们可以通过以下方式实现：
- en: '[PRE35]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We maintain a list of tuples called `new_params` which will contain tuples
    of (parameter_name, new_value). Next, we loop through each nn.Module object in
    our model with `self.named_modules()` which returns tuples of (name, module).
    If we print out the module names within the loop we see that they look like this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们维护一个名为`new_params`的元组列表，其中包含(参数名, 新值)的元组。接下来，我们通过`self.named_modules()`循环遍历模型中的每个nn.Module对象，它返回(name,
    module)的元组。如果我们在循环中打印出模块名称，我们会看到它们像这样：
- en: '[PRE36]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We use the `isinstance()` function to find the linear and embedding layers and
    then add them to our list. For example, say we are looping and reach “blocks.layers.0.mlp.c_fc”
    which is the first linear layer in the MLP. This would trigger the first if statement,
    and the tuple `("block.layers.0.mlp.c_fc.weight", [<normally initialized weight
    here>])` would be added to our list. We have to add “.weight” to the name because
    we specifically want to initialize the weight in this way, not the bias. Now we
    need to handle the residual projection initialization.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`isinstance()`函数来查找线性层和嵌入层，然后将它们添加到我们的列表中。例如，假设我们正在循环并到达“blocks.layers.0.mlp.c_fc”，这是MLP中的第一个线性层。这将触发第一个if语句，并将元组`("block.layers.0.mlp.c_fc.weight",
    [<normally initialized weight here>])`添加到我们的列表中。我们必须在名称中添加“.weight”，因为我们特别想以这种方式初始化权重，而不是偏置。现在，我们需要处理残差投影的初始化。
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: After checking if the module is a linear layer, we check if “c_proj” is in the
    name because that’s how we named the residual projections. Then we can apply the
    special initialization. Finally, we need to initialize the biases to be zero.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查模块是否为线性层后，我们检查名称中是否包含“c_proj”，因为我们将残差投影命名为此。然后我们可以应用特殊的初始化方法。最后，我们需要将偏置初始化为零。
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We add another if statement under our linear branch to check if the nn.Module
    object has a bias attribute. If it does, we add it to the list initialized to
    zeros. Finally, we need to transform our list of tuples into a nested dictionary.
    Luckily mlx has some functions implemented for dealing with parameter dictionaries,
    and we can use `util.tree_unflatten()` to convert this list of tuples to a nested
    parameter dictionary. This is passed into the update method to initialize the
    parameters. Now we can call `_init_parameters()` in the constructor.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在线性分支下再添加一个if语句来检查`nn.Module`对象是否具有偏置属性。如果有，我们将其添加到初始化为零的列表中。最后，我们需要将元组列表转换为嵌套字典。幸运的是，mlx已经实现了一些函数来处理参数字典，我们可以使用`util.tree_unflatten()`将这个元组列表转换为嵌套的参数字典。这个字典会传递到更新方法中以初始化参数。现在我们可以在构造函数中调用`_init_parameters()`。
- en: '[PRE39]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We also add 2 lines of code in the constructor to print the total number of
    params. Finally, we are ready to build the training loop.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在构造函数中添加了两行代码来打印总的参数数量。最后，我们准备构建训练循环。
- en: Training Loop
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: To train the model we need a loss function. Since we are predicting classes
    (next token) we use cross-entropy loss.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要一个损失函数。由于我们预测的是类别（下一个令牌），所以我们使用交叉熵损失函数。
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: First, we get the logits from the model. Then we reshape logits to make a list
    of vocab_size length arrays. We also reshape y, the correct token ids, to have
    the same length. Then we use the built-in cross-entropy loss function to calculate
    the loss for each example and average them to get a single value.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从模型中获取logits。然后，我们重新调整logits的形状，形成一个词汇表大小长度的数组列表。我们还将正确的令牌ID（y）重新调整形状，以使其具有相同的长度。接着，我们使用内建的交叉熵损失函数来计算每个示例的损失，并将它们平均得到一个单一的值。
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Next, we instantiate the model, but since mlx is lazy evaluation it won’t allocate
    and create the parameters. We need to call mx.eval on the parameters to ensure
    they get created. Then we can use `[nn.value_and_grad()](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.nn.value_and_grad.html)`
    to get a function that returns the loss and gradient of model parameters w.r.t
    the loss. This is all we need to optimize. Finally, we initialize an AdamW optimizer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化模型，但由于mlx是惰性求值，它不会分配和创建参数。我们需要在参数上调用mx.eval以确保它们被创建。然后我们可以使用`[nn.value_and_grad()](https://ml-explore.github.io/mlx/build/html/python/_autosummary/mlx.nn.value_and_grad.html)`来获得一个函数，该函数返回模型参数相对于损失的损失值和梯度。这是我们优化所需的一切。最后，我们初始化一个AdamW优化器。
- en: A quick note on nn.value_and_grad(). If you are used to PyTorch you might expect
    us to use loss.backward() which goes through the computation graph and updates
    the .grad attribute of each tensor in our model. However, mlx automatic differentiation
    works on functions instead of computation graphs [3]. Therefore, mlx has built-ins
    that take in a function and return the gradient function such as `nn.value_and_grad()`
    .
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`nn.value_and_grad()`的简短说明。如果你习惯了PyTorch，可能会期望我们使用`loss.backward()`，这会遍历计算图并更新模型中每个张量的`.grad`属性。然而，mlx的自动微分是基于函数而非计算图的[3]。因此，mlx提供了内建函数，接受一个函数并返回该函数的梯度函数，例如`nn.value_and_grad()`。
- en: Now we define the training loop.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义训练循环。
- en: '[PRE42]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The outer loop runs through the epochs. We first set the model to training mode
    because some modules have different behaviors during training and testing such
    as dropout. Then we use our `get_batches` function from earlier to loop through
    batches of the training data. We get the loss over the batch and the gradient
    using `loss_and_grad` . Then we pass the model and gradients to the optimizer
    to update the model parameters. Finally we call mx.eval (remember mlx does lazy
    evaluation) to ensure the parameters and optimizer state get updated. Then we
    calculate the average train loss over the data to print later. This is one pass
    through the training data. Similarly, we calculate the validation loss and then
    print the average train and val loss over the epoch.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 外循环遍历各个训练轮次。我们首先将模型设置为训练模式，因为某些模块在训练和测试阶段的行为有所不同，例如 dropout。然后，我们使用之前的`get_batches`函数来遍历训练数据的批次。我们通过`loss_and_grad`获得批次的损失和梯度。接着，我们将模型和梯度传递给优化器，以更新模型的参数。最后，我们调用mx.eval（记住，mlx是懒惰求值）来确保参数和优化器的状态得到更新。然后，我们计算数据集的平均训练损失，以便稍后打印。这就是一次完整的训练数据遍历。类似地，我们计算验证损失，然后打印每个训练轮次的平均训练损失和验证损失。
- en: '[PRE43]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Finally, we add some code to generate from our model. Since the generation output
    is still in the (B, T) shape we have to index it at 0 to make it 1D and then convert
    it from an mlx array to a Python list. Then we can pass it to our decode function
    from earlier, and write it to a file.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加一些代码来从我们的模型生成数据。由于生成的输出仍然是（B，T）形状，我们需要在第0个索引位置进行切片，使其变为1D数组，然后将其从 mlx
    数组转换为 Python 列表。接着，我们可以将其传递给之前的解码函数，并将结果写入文件。
- en: 'These are the parameters we will use for training (you can play around with
    this):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将在训练中使用的参数（你可以自行尝试调整这些参数）：
- en: '[PRE44]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now we can run the file to start training. With the settings above training
    took around 10 minutes on my m2 MacBook. I achieved the following training loss
    last epoch.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行文件来开始训练。根据上述设置，训练大约花了10分钟，在我的 M2 MacBook 上训练完成。上一个训练轮次的损失如下：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Let’s look at some output.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下输出结果。
- en: '[PRE46]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Not bad for just 10 minutes of training with a tiny model that is predicting
    characters! It clearly has the form of Shakespeare, although it is nonsense. The
    only difference between our model and the real GPT-2 now is scale! Now I encourage
    you to experiment — try out different settings, maybe tinker with the architecture,
    and see how low of a loss you can achieve.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 经过短短10分钟的训练，这个小模型竟然能够预测字符！虽然输出内容是无意义的，但它显然具备了莎士比亚作品的风格。现在我们模型与真正的 GPT-2 之间唯一的区别就是规模！我现在鼓励你进行实验——尝试不同的设置，可能调整一下架构，看看你能达到多低的损失。
- en: References
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Karpathy A (2015).*Tiny Shakespeare* [Data set]. [https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)
    (MIT license)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Karpathy A (2015).*Tiny Shakespeare* [数据集]。[https://github.com/karpathy/char-rnn](https://github.com/karpathy/char-rnn)（MIT许可证）'
- en: '[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, [Language
    Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
    (2019), OpenAI'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever，[语言模型是无监督的多任务学习者](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)（2019），OpenAI'
- en: '[3] [Automatic Differentiation — mlx docs](https://ml-explore.github.io/mlx/build/html/usage/function_transforms.html#auto-diff)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [自动微分 — mlx 文档](https://ml-explore.github.io/mlx/build/html/usage/function_transforms.html#auto-diff)'
