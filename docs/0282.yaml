- en: Run LLM Inference Using Apple Hardware
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/run-llm-inference-using-apple-hardware-00a4a5d455b7?source=collection_archive---------8-----------------------#2024-01-30](https://towardsdatascience.com/run-llm-inference-using-apple-hardware-00a4a5d455b7?source=collection_archive---------8-----------------------#2024-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlock Apple GPU power for LLM inference with MLX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@christopher_karg?source=post_page---byline--00a4a5d455b7--------------------------------)[![Christopher
    Karg](../Images/9d163d59e0c3167732f55d497caf9db2.png)](https://medium.com/@christopher_karg?source=post_page---byline--00a4a5d455b7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--00a4a5d455b7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--00a4a5d455b7--------------------------------)
    [Christopher Karg](https://medium.com/@christopher_karg?source=post_page---byline--00a4a5d455b7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--00a4a5d455b7--------------------------------)
    ·15 min read·Jan 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43b051fdf06c8fc88b6ddbaf1bc274cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://www.pexels.com/photo/train-railway-near-trees-552779/](https://www.pexels.com/photo/train-railway-near-trees-552779/)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are in a position to run inference and fine-tune our own LLMs using Apple’s
    native hardware. This article will cover the setup for creating your own experiments
    and running inference. In the future I will be making an article on how to fine-tune
    these LLMs (again using Apple hardware).
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t checked out my previous articles, I suggest doing so as I make
    a case for why you should consider hosting (and [fine-tuning](https://medium.com/towards-data-science/how-to-efficiently-fine-tune-your-own-open-source-llm-using-novel-techniques-code-provided-03a4e67d1b48))
    your own open-source LLM. I also cover strategies as to how you can [optimise
    the process](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb)
    to reduce inference and training times. I will brush over topics such as quantisation
    as these are covered in depth in the aforementioned articles.
  prefs: []
  type: TYPE_NORMAL
- en: I will be using the [mlx](https://github.com/ml-explore) framework in combination
    with [Meta’s Llama2 model](https://ai.meta.com/llama/). In-depth information on
    how to access the models can be found in my [previous article](https://medium.com/towards-data-science/quantisation-and-co-reducing-inference-times-on-llms-by-80-671db9349bdb).
    However, I’ll briefly explain how to do so in this article as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-requisites:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A machine with an M-series chip (M1/M2/M3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OS >= 13.0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Python between 3.8–3.11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
