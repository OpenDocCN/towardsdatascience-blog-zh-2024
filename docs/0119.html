<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Efficient Feature Selection via Genetic Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Efficient Feature Selection via Genetic Algorithms</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274?source=collection_archive---------5-----------------------#2024-01-12">https://towardsdatascience.com/efficient-feature-selection-via-genetic-algorithms-d6d3c9aff274?source=collection_archive---------5-----------------------#2024-01-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="646e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Using evolutionary algorithms for fast feature selection with large datasets</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://florin-andrei.medium.com/?source=post_page---byline--d6d3c9aff274--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Florin Andrei" class="l ep by dd de cx" src="../Images/372ac3e80dbc03cbd20295ec1df5fa6f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ycaYBdB28ohxlpwWYSK1pA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d6d3c9aff274--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://florin-andrei.medium.com/?source=post_page---byline--d6d3c9aff274--------------------------------" rel="noopener follow">Florin Andrei</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d6d3c9aff274--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5069" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">This is the final part of a two-part series about feature selection. Read </em><a class="af ng" rel="noopener" target="_blank" href="/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173"><em class="nf">part 1 here</em></a><em class="nf">.</em></p><p id="782c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Brief recap: when fitting a model to a dataset, you may want to select a subset of the features (as opposed to using all features), for various reasons. But even if you have a clear objective function to search for the best combination of features, the search may take a long time if the number of features N is very large. Finding the best combination is not always easy. Brute-force search generally does not work beyond several dozens of features. Heuristic algorithms are needed to perform a more efficient search.</p><p id="06bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you have N features, what you’re looking for is an N-length vector <code class="cx nh ni nj nk b">[1, 1, 0, 0, 0, 1, ...]</code> with values from <code class="cx nh ni nj nk b">{0, 1}</code> . Each vector component corresponds to a feature. 0 means the feature is rejected, 1 means the feature is selected. You need to find the vector that minimizes the cost / objective function you’re using.</p><p id="2568" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the previous article, we’ve looked at a classic algorithm, SFS (sequential feature search), and compared it with an efficient evolutionary algorithm called CMA-ES. We’ve started with the House Prices dataset on Kaggle which, after some processing, yielded 213 features with 1453 observations each. The model we’ve tried to fit was <code class="cx nh ni nj nk b">statsmodels.api.OLS()</code> and the objective function was the model’s BIC — Bayesian Information Criterion, a measure of information loss. Lower BIC means a better fit, so we’re trying to minimize the objective.</p><p id="8223" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, we will look at another evolutionary technique: genetic algorithms. The context (dataset, model, objective) remains the same.</p><h1 id="6977" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">GA — Genetic Algorithms</h1><p id="3adf" class="pw-post-body-paragraph mj mk fq ml b go oh mn mo gr oi mq mr ms oj mu mv mw ok my mz na ol nc nd ne fj bk">Genetic algorithms are inspired by biological evolution and natural selection. In nature, living beings are (loosely speaking) selected for the genes (traits) that facilitate survival and reproductive success, in the context of the environment where they live.</p><p id="3c84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now think of feature selection. You have N features. You’re trying to find N-length binary vectors <code class="cx nh ni nj nk b">[1, 0, 0, 1, 1, 1, ...]</code> that select the features (0 = feature rejected, 1= feature included) so as to minimize a cost / objective function.</p><p id="3e8a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Each such vector can be thought of as an “individual”. Each vector component (value 0 or value 1) becomes a “gene”. By judiciously applying evolution and selection, it might be possible to evolve a population of individuals in such a way as to get near the best value for the objective function we’re interested in.</p><p id="45be" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s GA in a nutshell. Start by generating a population of individuals (vectors), each vector of length N. The vector component values (genes) are randomly chosen from {0, 1}. In the diagram below, N=12, and the population size is 8.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on oo"><img src="../Images/ddd592aacc4b7a015475db6501f619ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*REiA0OywiDi-Nz2oQtDiLg.png"/></div><figcaption class="ow ox oy om on oz pa bf b bg z dx">GA population</figcaption></figure><p id="bf0f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After the population is created, evaluate each individual via the objective function.</p><p id="8182" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now perform selection: keep the individuals with the best objective values, and discard those with the worst values. There are many possible strategies here, from naive ranking (which, counterintuitively, doesn’t work very well), to stochastic tournament selection, which is very efficient in the long run. If you remember the explore-exploit dilemma, with GA, it’s very easy to fall into naive exploit traps that slow exploration down. GA is all about exploration. <a class="af ng" href="https://www.tutorialspoint.com/genetic_algorithms/genetic_algorithms_parent_selection.htm" rel="noopener ugc nofollow" target="_blank">Here’s a short list of selection techniques</a>, and check the links at the end for more info.</p><p id="afff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once the best individuals have been selected, and the less fit ones have been discarded, it’s time to introduce variation in the gene pool via two techniques: crossover and mutation.</p><p id="8631" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Crossover works exactly like in nature, when two living creatures are mating and producing offspring: genetic material from both parents is “mixed” in the descendants, with some degree of randomness.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on oo"><img src="../Images/b218ad06454d32e0129d3dd0a38a808f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*EZx-Hk4Qx68ysajSJvynUA.png"/></div><figcaption class="ow ox oy om on oz pa bf b bg z dx">GA crossover</figcaption></figure><p id="6a2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mutation, again, is pretty much what happens in nature when random mutations occur in the genetic material, and new values are introduced in the gene pool, increasing its diversity.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on oo"><img src="../Images/8b3f6f5fc4776706f509b8ab8969d8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*o0-JV1HLnkzMmO3eZJqSoQ.png"/></div><figcaption class="ow ox oy om on oz pa bf b bg z dx">GA mutation</figcaption></figure><p id="a9a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After all that, the algorithm loops back: individuals are again evaluated via the objective function, selection occurs, then crossover, mutation, etc.</p><p id="f8e0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Various stopping criteria can be used: the loop may break if the objective function stops improving for some number of generations. Or you may set a hard stop for the total number of generations evaluated. Or do something time-based, or watch for an external signal, etc. Regardless, the individuals with the best objective values should be considered to be the solutions to the problem.</p><p id="3715" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A few words about elitism: with stochastic selection techniques such as tournament, the best, absolute top individuals in a generation may actually be discarded by pure chance — it’s unlikely, but it does happen. Elitism bypasses this, and simply decrees that the best must survive, no matter what. Elitism is an exploit technique. It may cause the algorithm to fall into local extremes, missing the global solution. Again, GA is all about exploration. My rather limited experience with GA seems to confirm the idea that an exploit bias is not beneficial for GA. But your mileage may vary; if you like to experiment with algorithm variants, GA gives you many opportunities to do so.</p><p id="177f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">GA has several hyperparameters you can tune:</p><ul class=""><li id="37ae" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pb pc pd bk">population size (number of individuals)</li><li id="ab7a" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pb pc pd bk">mutation probabilities (per individual, per gene)</li><li id="b7e8" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pb pc pd bk">crossover probability</li><li id="e846" class="mj mk fq ml b go pe mn mo gr pf mq mr ms pg mu mv mw ph my mz na pi nc nd ne pb pc pd bk">selection strategies, etc.</li></ul><p id="fd11" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Running trials by hand with various hyperparameter values is one way to figure out the best code. Or you could encapsulate GA in Optuna and let Optuna find the best hyperparameters — but this is computationally expensive.</p><h1 id="3132" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">GA for feature selection, in code</h1><p id="660f" class="pw-post-body-paragraph mj mk fq ml b go oh mn mo gr oi mq mr ms oj mu mv mw ok my mz na ol nc nd ne fj bk">Here’s a simple GA code that can be used for feature selection. It uses <a class="af ng" href="https://github.com/DEAP/deap" rel="noopener ugc nofollow" target="_blank">the deap library</a>, which is very powerful, but the learning curve may be steep. This simple version, however, should be clear enough.</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="c8d8" class="pm nm fq nk b bg pn po l pp pq"># to maximize the objective<br/># fitness_weights = 1.0<br/># to minimize the objective<br/>fitness_weights = -1.0<br/><br/># copy the original dataframes into local copies, once<br/>X_ga = X.copy()<br/>y_ga = y.copy()<br/><br/># 'const' (the first column) is not an actual feature, do not include it<br/>X_features = X_ga.columns.to_list()[1:]<br/><br/>try:<br/>    del creator.FitnessMax<br/>    del creator.Individual<br/>except Exception as e:<br/>    pass<br/><br/>creator.create("FitnessMax", base.Fitness, weights=(fitness_weights,))<br/>creator.create(<br/>    "Individual", array.array, typecode='b', fitness=creator.FitnessMax<br/>)<br/><br/>try:<br/>    del toolbox<br/>except Exception as e:<br/>    pass<br/><br/>toolbox = base.Toolbox()<br/># Attribute generator<br/>toolbox.register("attr_bool", random.randint, 0, 1)<br/># Structure initializers<br/>toolbox.register(<br/>    "individual",<br/>    tools.initRepeat,<br/>    creator.Individual,<br/>    toolbox.attr_bool,<br/>    len(X_features),<br/>)<br/>toolbox.register("population", tools.initRepeat, list, toolbox.individual)<br/><br/><br/>def evalOneMax(individual):<br/>    # objective function<br/>    # create True/False selector list for features<br/>    # and add True at the start for 'const'<br/>    cols_select = [True] + [i == 1 for i in list(individual)]<br/>    # fit model using the features selected from the individual<br/>    lin_mod = sm.OLS(y_ga, X_ga.loc[:, cols_select], hasconst=True).fit()<br/>    return (lin_mod.bic,)<br/><br/><br/>toolbox.register("evaluate", evalOneMax)<br/>toolbox.register("mate", tools.cxTwoPoint)<br/>toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)<br/>toolbox.register("select", tools.selTournament, tournsize=3)<br/><br/>random.seed(0)<br/>pop = toolbox.population(n=300)<br/>hof = tools.HallOfFame(1)<br/>pop, log = algorithms.eaSimple(<br/>    pop, toolbox, cxpb=0.5, mutpb=0.2, ngen=10, halloffame=hof, verbose=True<br/>)<br/><br/>best_individual_ga_small = list(hof[0])<br/>best_features_ga_small = [<br/>    X_features[i] for i, val in enumerate(best_individual_ga_small) if val == 1<br/>]<br/>best_objective_ga_small = (<br/>    sm.OLS(y_ga, X_ga[['const'] + best_features_ga_small], hasconst=True)<br/>    .fit()<br/>    .bic<br/>)<br/>print(f'best objective: {best_objective_ga_small}')<br/>print(f'best features:  {best_features_ga_small}')</span></pre><p id="6309" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The code creates the objects that define an individual and the whole population, along with the strategies used for evaluation (objective function), crossover / mating, mutation, and selection. It starts with a population of 300 individuals, and then calls <code class="cx nh ni nj nk b">eaSimple()</code> (a canned sequence of crossover, mutation, selection) which runs for only 10 generations, for simplicity. Hall of fame with a size of 1 is defined, where the absolute best individual is preserved against being accidentally mutated / skipped during selection, etc.</p><p id="3904" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hall of fame is not elitism. Hall of fame copies the best individual from the population, and only keeps an inactive copy in a tin can. Elitism preserves the best individual in the active population from one generation to the next.</p><p id="26cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This simple code is easy to understand, but inefficient. Check the notebook in <a class="af ng" href="https://github.com/FlorinAndrei/fast_feature_selection" rel="noopener ugc nofollow" target="_blank">the repository</a> for a more complex version of the GA code, which I am not going to quote here. However, running the more complex, optimized code from the notebook for 1000 generations produces these results:</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="9a3a" class="pm nm fq nk b bg pn po l pp pq">best objective:  33705.569572544795<br/>best generation: 787<br/>objective runs:  600525<br/>time to best:    158.027 sec</span></pre><p id="be2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Again, the baseline BIC before any feature selection is:</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="0181" class="pm nm fq nk b bg pn po l pp pq">baseline BIC: 34570.166173470934</span></pre><p id="b343" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And here’s the entire history of the full, optimized GA code from the notebook, running for 1000 generations, trying to find the best features. From left to right, the heatmap indicates the popularity of each feature within the population across generations (brighter shade = more popular). You can see how some features are always popular, others are rejected quickly, while yet others may become more popular or less popular as time goes by.</p></div></div><div class="ou"><div class="ab cb"><div class="lm pr ln ps lo pt cf pu cg pv ci bh"><figure class="op oq or os ot ou px py paragraph-image"><div role="button" tabindex="0" class="pz qa ed qb bh qc"><div class="om on pw"><img src="../Images/618e229be67fc6a6d949c05e038e3ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*17nQo2Ff_wUznu9zdMrIGw.png"/></div></div><figcaption class="ow ox oy om on oz pa bf b bg z dx">GA optimization history</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a409" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Comparison between methods</h1><p id="0ed6" class="pw-post-body-paragraph mj mk fq ml b go oh mn mo gr oi mq mr ms oj mu mv mw ok my mz na ol nc nd ne fj bk">We’ve tried three different techniques: SFS, CMA-ES, and GA. How do they compare in terms of the best objective found, and the time it took to find it?</p><p id="d497" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These tests were performed on an AMD Ryzen 7 5800X3D (8/16 cores) machine, running Ubuntu 22.04, and Python 3.11.7. SFS and GA are running the objective function via a multiprocessing pool with 16 workers. CMA-ES is single-process — running it multi-process did not seem to provide significant improvements, but I’m sure that could change if more work is dedicated to making the algorithm parallel.</p><p id="7f67" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These are the run times. For SFS it’s the total run time. For CMA-ES and GA it’s the time to the best solution. Less is better.</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="fef6" class="pm nm fq nk b bg pn po l pp pq">SFS:    42.448 sec<br/>GA:     158.027 sec<br/>CMA-ES: 48.326 sec</span></pre><p id="33bc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The number of times the objective function was invoked — less is better:</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="2ad3" class="pm nm fq nk b bg pn po l pp pq">SFS:    22791<br/>GA:     600525<br/>CMA-ES: 20000</span></pre><p id="7867" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The best values found for the objective function, compared to the baseline — less is better:</p><pre class="op oq or os ot pj nk pk bp pl bb bk"><span id="4444" class="pm nm fq nk b bg pn po l pp pq">baseline BIC: 34570.1662<br/>SFS:          33708.9860<br/>GA:           33705.5696<br/>CMA-ES:       33703.0705</span></pre><p id="0a3c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">GA was able to beat SFS at the objective function, running the objective function on as many CPU cores as were available, but it’s by far the slowest. It invoked the objective function more than an order of magnitude more times than the other methods. Further hyperparameter optimizations may improve the outcome.</p><p id="2984" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">SFS is quick (running on all CPU cores), but its performance is modest. It’s also the simplest algorithm by far.</p><p id="6c77" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you just want a quick estimate of the best feature set, using a simple algorithm, SFS is not too bad.</p><p id="71eb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">OTOH, if you want the optimal objective value, CMA-ES seems to be the best.</p><p id="6ce6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">This is the final part of a two-part series about feature selection. Read </em><a class="af ng" rel="noopener" target="_blank" href="/efficient-feature-selection-via-cma-es-covariance-matrix-adaptation-evolution-strategy-ee312bc7b173"><em class="nf">part 1 here</em></a><em class="nf">.</em></p><h1 id="a913" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Notes and links</h1><p id="095b" class="pw-post-body-paragraph mj mk fq ml b go oh mn mo gr oi mq mr ms oj mu mv mw ok my mz na ol nc nd ne fj bk">All images were created by the author.</p><p id="d3ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Repository with all code: <a class="af ng" href="https://github.com/FlorinAndrei/fast_feature_selection" rel="noopener ugc nofollow" target="_blank">https://github.com/FlorinAndrei/fast_feature_selection</a></p><p id="4993" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The House Prices dataset (MIT license): <a class="af ng" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a></p><p id="de85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The deap library: <a class="af ng" href="https://github.com/DEAP/deap" rel="noopener ugc nofollow" target="_blank">https://github.com/DEAP/deap</a></p><p id="d5b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A free tutorial for genetic algorithms: <a class="af ng" href="https://www.tutorialspoint.com/genetic_algorithms/index.htm" rel="noopener ugc nofollow" target="_blank">https://www.tutorialspoint.com/genetic_algorithms/index.htm</a></p></div></div></div></div>    
</body>
</html>