- en: Understanding Low Rank Adaptation (LoRA) in Fine-Tuning LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解低秩适应（LoRA）在微调大规模语言模型中的应用
- en: 原文：[https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24](https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24](https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24)
- en: 'This blog post will go into detail about how LoRA works to fine-tune LLMs,
    following the methodology set out in the “LoRA: Low-Rank Adaptation of Large Language
    Models” paper'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将详细讲解LoRA如何用于微调大规模语言模型，遵循[“LoRA：大规模语言模型的低秩适应”](https://arxiv.org/pdf/2106.09685)论文中提出的方法。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    ·6 min read·May 24, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    ·阅读时长：6分钟·2024年5月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fcdf35f66c36e62f303d26f31190e70b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcdf35f66c36e62f303d26f31190e70b.png)'
- en: Image by Author — generated by Stable Diffusion 2.1
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供 — 由Stable Diffusion 2.1生成
- en: Fine-Tuning is perhaps one of the most discussed technical aspects when it comes
    to Large Language Models. Most people understand that training these models is
    expensive and requires significant capital investment, so it is exciting to see
    that you can create a model that is somewhat unique by taking a pre-existing model
    and fine-tuning it with your own data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能是大规模语言模型（LLMs）中最常被讨论的技术方面之一。大多数人都知道训练这些模型是昂贵的，并且需要大量的资金投入，因此看到通过在已有模型上进行微调，结合自己的数据创建出一个具有一定独特性的模型，是令人兴奋的。
- en: 'There are multiple methods to fine-tune a model, but one of the most consistently
    popular currently is the LoRA method (short for Low Rank Adaptation) discussed
    in the [“LoRA: Low-Rank Adaptation of Large Language Models”](https://arxiv.org/pdf/2106.09685)
    paper.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以对模型进行微调，但目前最受欢迎的方式之一是LoRA方法（低秩适应，Low Rank Adaptation），它在[“LoRA：大规模语言模型的低秩适应”](https://arxiv.org/pdf/2106.09685)论文中有详细讨论。
- en: Before we dive into the mechanics behind LoRA, we need to understand some Matrix
    background and some of the basics of fine-tuning a machine learning model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨LoRA的工作原理之前，我们需要了解一些矩阵背景知识以及微调机器学习模型的一些基本概念。
- en: Matrix Background Terminology
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵背景术语
- en: Practically all machine learning models store their weights as matrices. Consequently,
    having some understanding of linear algebra is helpful to get intuition on what
    is…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的机器学习模型都会将其权重存储为矩阵。因此，理解一些线性代数的基本知识有助于直观理解...
