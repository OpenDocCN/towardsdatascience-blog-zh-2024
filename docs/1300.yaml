- en: Understanding Low Rank Adaptation (LoRA) in Fine-Tuning LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24](https://towardsdatascience.com/understanding-low-rank-adaptation-lora-in-fine-tuning-llms-d3dd283f1f0a?source=collection_archive---------3-----------------------#2024-05-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This blog post will go into detail about how LoRA works to fine-tune LLMs,
    following the methodology set out in the “LoRA: Low-Rank Adaptation of Large Language
    Models” paper'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d3dd283f1f0a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3dd283f1f0a--------------------------------)
    ·6 min read·May 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcdf35f66c36e62f303d26f31190e70b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — generated by Stable Diffusion 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning is perhaps one of the most discussed technical aspects when it comes
    to Large Language Models. Most people understand that training these models is
    expensive and requires significant capital investment, so it is exciting to see
    that you can create a model that is somewhat unique by taking a pre-existing model
    and fine-tuning it with your own data.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple methods to fine-tune a model, but one of the most consistently
    popular currently is the LoRA method (short for Low Rank Adaptation) discussed
    in the [“LoRA: Low-Rank Adaptation of Large Language Models”](https://arxiv.org/pdf/2106.09685)
    paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the mechanics behind LoRA, we need to understand some Matrix
    background and some of the basics of fine-tuning a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Background Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Practically all machine learning models store their weights as matrices. Consequently,
    having some understanding of linear algebra is helpful to get intuition on what
    is…
  prefs: []
  type: TYPE_NORMAL
