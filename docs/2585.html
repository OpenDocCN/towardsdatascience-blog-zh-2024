<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>ML Metamorphosis: Chaining ML Models for Optimized Results</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>ML Metamorphosis: Chaining ML Models for Optimized Results</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23">https://towardsdatascience.com/ml-metamorphosis-chaining-ml-models-for-optimized-results-d89d952627a9?source=collection_archive---------2-----------------------#2024-10-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="449b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The universal principle of knowledge distillation, model compression, and rule extraction</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Vadim Arzamasov" class="l ep by dd de cx" src="../Images/70ced2eafa6fc926052979875a0a4265.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*sLla8xXL0qQuaiNDqAnTiw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vadim.arzamasov?source=post_page---byline--d89d952627a9--------------------------------" rel="noopener follow">Vadim Arzamasov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d89d952627a9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5b5804cb68224d5d689f156155d59eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9O8CIlpfoiK_7FMx3DvmnQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf nc">Figure 1</strong>. This and other images were created by the author with the help of recraft.ai</figcaption></figure><p id="1bed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Machine learning (ML) model training typically follows a familiar pipeline: start with data collection, clean and prepare it, then move on to model fitting. But what if we could take this process further? Just as some insects undergo dramatic transformations before reaching maturity, ML models can evolve in a similar way (see Hinton et al. [1]) — what I will call the <strong class="nf fr">ML metamorphosis</strong>. This process involves chaining different models together, resulting in a final model that achieves significantly better quality than if it had been trained directly from the start.</p><p id="77de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s how it works:</p><ul class=""><li id="d270" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Start with some initial knowledge, <em class="oc">Data 1</em>.</li><li id="cd25" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk">Train an ML model, <em class="oc">Model A</em> (say, a neural network), on this data.</li><li id="1c3d" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk">Generate new data, <em class="oc">Data 2</em>, using <em class="oc">Model A</em>.</li><li id="251e" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk">Finally, use Data 2 to fit your target model, <em class="oc">Model B</em>.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oi"><img src="../Images/2645f7d1e0777997a8daebcec3b49575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSBS8VGVeS35bfItc_5fow.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf nc">Figure 2. </strong>An illustration of the ML metamorphosis</figcaption></figure><p id="bc4a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You may already be familiar with this concept from knowledge distillation, where a smaller neural network replaces a larger one. But ML metamorphosis goes beyond this, and neither the initial model (<em class="oc">Model A</em>) nor the final one (<em class="oc">Model B</em>) need be neural networks at all.</p><h2 id="0f4a" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">Example: ML metamorphosis on the MNIST Dataset</h2><p id="af46" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk"><em class="oc">Imagine you’re tasked with training a multi-class decision tree on the MNIST dataset of handwritten digit images, but only 1,000 images are labelled. You could train the tree directly on this limited data, but the accuracy would be capped at around 0.67. Not great, right? Alternatively, you could use ML metamorphosis to improve your results.</em></p><p id="ecb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But before we dive into the solution, let’s take a quick look at the techniques and research behind this approach.</p><h2 id="35ea" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">1. Knowledge distillation (2015)</h2><p id="c6c3" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Even if you haven’t used knowledge distillation, you’ve probably seen it in action. For example, Meta suggests distilling its Llama 3.2 model to adapt it to specific tasks [2]. Or take DistilBERT — a distilled version of BERT [3]— or the DMD framework, which distills Stable Diffusion to speed up image generation by a factor of 30 [4].</p><p id="0b0c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At its core, knowledge distillation transfers knowledge from a large, complex model (the <em class="oc">teacher</em>) to a smaller, more efficient model (the <em class="oc">student</em>). The process involves creating a <em class="oc">transfer set</em> that includes both the original training data and additional data (either original or synthesized) pseudo-labeled by the teacher model. The pseudo-labels are known as <em class="oc">soft labels</em> — derived from the probabilities predicted by the teacher across multiple classes. These soft labels provide richer information than <em class="oc">hard labels</em> (simple class indicators) because they reflect the teacher’s confidence and capture subtle similarities between classes. For instance, they might show that a particular “1” is more similar to a “7” than to a “5.”</p><p id="59c8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By training on this enriched transfer set, the student model can effectively mimic the teacher’s performance while being much lighter, faster, and easier to use.</p><blockquote class="pi pj pk"><p id="32f8" class="nd ne oc nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The student model obtained in this way is more accurate than it would have been if it had been trained solely on the original training set.</p></blockquote><h2 id="d870" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">2. Model compression (2007)</h2><p id="63db" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">Model compression [5] is often seen as a precursor to knowledge distillation, but there are important differences. Unlike knowledge distillation, model compression doesn’t seem to use soft labels, despite some claims in the literature [1,6]. I haven’t found any evidence that soft labels are part of the process. In fact, the method in the original paper doesn’t even rely on artificial neural networks (ANNs) as <em class="oc">Model A</em>. Instead, it uses an ensemble of models — such as SVMs, decision trees, random forests, and others.</p><p id="09e3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Model compression works by approximating the feature distribution <em class="oc">p(x)</em> to create a transfer set. This set is then labelled by <em class="oc">Model A</em>, which provides the conditional distribution <em class="oc">p(y|x)</em>. The key innovation in the original work is a technique called MUNGE to approximate <em class="oc">p(x)</em>. As with knowledge distillation, the goal is to train a smaller, more efficient <em class="oc">Model B</em> that retains the performance of the larger <em class="oc">Model A</em>.</p><blockquote class="pi pj pk"><p id="cb9c" class="nd ne oc nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As in knowledge distillation, the compressed model trained in this way can often outperform a similar model trained directly on the original data, thanks to the rich information embedded in the transfer set [5].</p></blockquote><p id="b794" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Often, “model compression” is used more broadly to refer to any technique that reduces the size of <em class="oc">Model A </em>[7,8]. This includes methods like knowledge distillation but also techniques that don’t rely on a transfer set, such as pruning, quantization, or low-rank approximation for neural networks.</p><h2 id="0a11" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">3. Rule extraction (1995)</h2><p id="0993" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">When the problem isn’t computational complexity or memory, but the opacity of a model’s decision-making, pedagogical rule extraction offers a solution [9]. In this approach, a simpler, more interpretable model (<em class="oc">Model B</em>) is trained to replicate the behavior of the opaque teacher model (<em class="oc">Model A</em>), with the goal of deriving a set of human-readable rules. The process typically starts by feeding unlabelled examples — often randomly generated — into <em class="oc">Model A</em>, which labels them to create a transfer set. This transfer set is then used to train the transparent student model. For example, in a classification task, the student model might be a decision tree that outputs rules such as: “If feature X1 is above threshold T1 and feature X2 is below threshold T2, then classify as positive”.</p><p id="a6c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The main goal of pedagogical rule extraction is to closely mimic the teacher model’s behavior, with <em class="oc">fidelity</em> — the accuracy of the student model relative to the teacher model — serving as the primary quality measure.</p><blockquote class="pi pj pk"><p id="6a7a" class="nd ne oc nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Interestingly, research has shown that transparent models created through this method can sometimes reach higher accuracy than similar models trained directly on the original data used to build Model A [10,11].</p></blockquote><p id="8451" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Pedagogical rule extraction belongs to a broader family of techniques known as “global” model explanation methods, which also include decompositional and eclectic rule extraction. See [12] for more details.</p><h2 id="4b65" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">4. Simulations as Model A</h2><p id="d02a" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk"><em class="oc">Model A</em> doesn’t have to be an ML model — it could just as easily be a computer simulation of an economic or physical process, such as the simulation of airflow around an airplane wing. In this case, <em class="oc">Data 1</em> consists of the differential or difference equations that define the process. For any given input, the simulation makes predictions by solving these equations numerically. However, when these simulations become computationally expensive, a faster alternative is needed: a surrogate model (<em class="oc">Model B</em>), which can accelerate tasks like optimization [13]. When the goal is to identify important regions in the input space, such as zones of system stability, an interpretable <em class="oc">Model B</em> is developed through a process known as scenario discovery [14]. To generate the transfer set (<em class="oc">Data 2</em>) for both surrogate modelling and scenario discovery, <em class="oc">Model A</em> is run on a diverse set of inputs.</p><h2 id="f74b" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">Back to our MNIST example</h2><p id="c8f9" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">In an insightful <a class="af pl" rel="noopener" target="_blank" href="/teaching-your-model-to-learn-from-itself-8b5ef13eb173">article</a> on TDS [15], <span class="ia"><span class="ia" aria-hidden="false"><a class="pm ib pn" href="https://medium.com/u/a3ecf86934da?source=post_page---user_mention--d89d952627a9--------------------------------" rel="noopener" target="_blank">Niklas von Moers</a></span></span> shows how semi-supervised learning can improve the performance of a convolutional neural network (CNN) on the same input data. This result fits into the first stage of the ML metamorphosis pipeline, where <em class="oc">Model A</em> is a trained CNN classifier. The transfer set, <em class="oc">Data 2</em>, then contains the originally labelled 1,000 training examples plus about 55,000 examples pseudo-labelled by <em class="oc">Model A</em> with high confidence predictions. I now train our target <em class="oc">Model B</em>, a decision tree classifier, on <em class="oc">Data 2</em> and achieve an accuracy of 0.86 — much higher than 0.67 when training on the labelled part of <em class="oc">Data 1</em> alone. This means that chaining the decision tree to the CNN solution reduces error rate of the decision tree from 0.33 to 0.14. Quite an improvement, wouldn’t you say?</p><p id="e930" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the full experimental code, check out the <a class="af pl" href="https://github.com/Arzik1987/medium/tree/main/metamorphosis" rel="noopener ugc nofollow" target="_blank">GitHub</a> repository.</p><h2 id="eace" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk"><strong class="al">Conclusion</strong></h2><p id="d231" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">In summary, ML metamorphosis isn’t always necessary — especially if accuracy is your only concern and there’s no need for interpretability, faster inference, or reduced storage requirements. But in other cases, chaining models may yield significantly better results than training the target model directly on the original data.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/1b04e87426dec01924b73226bb1df2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTRqyAJ9Y54fwZW6UH76kA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf nc">Figure 2</strong>: For easy reference, here’s the illustration again</figcaption></figure><p id="e57c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a classification task, the process involves:</p><ul class=""><li id="7149" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob bk"><em class="oc">Data 1</em>: The original, fully or partially labeled data.</li><li id="8ab2" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk"><em class="oc">Model A</em>: A model trained on <em class="oc">Data 1</em>.</li><li id="e03e" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk"><em class="oc">Data 2</em>: A transfer set that includes pseudo-labeled data.</li><li id="84be" class="nd ne fq nf b go od nh ni gr oe nk nl nm of no np nq og ns nt nu oh nw nx ny nz oa ob bk"><em class="oc">Model B</em>: The final model, designed to meet additional requirements, such as interpretability or efficiency.</li></ul><p id="e4ad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So why don’t we always use ML metamorphosis? The challenge often lies in finding the right transfer set, <em class="oc">Data 2</em> [9]. But that’s a topic for another story.</p><h2 id="7352" class="oj ok fq bf nc ol om on oo op oq or os nm ot ou ov nq ow ox oy nu oz pa pb pc bk">References</h2><p id="18c3" class="pw-post-body-paragraph nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny fj bk">[1] Hinton, Geoffrey. “<a class="af pl" href="https://arxiv.org/pdf/1503.02531" rel="noopener ugc nofollow" target="_blank">Distilling the Knowledge in a Neural Network</a>.” <em class="oc">arXiv preprint arXiv:1503.02531</em> (2015).</p><p id="fbb1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] <a class="af pl" href="https://llama.meta.com/?ref=engineering.fb.com" rel="noopener ugc nofollow" target="_blank">Introducing Llama 3.2</a></p><p id="98a2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] Sanh, Victor, et al. “<a class="af pl" href="https://arxiv.org/abs/1910.01108" rel="noopener ugc nofollow" target="_blank">DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter</a>. ” <em class="oc">arXiv preprint arXiv:1910.01108</em> (2019).</p><p id="2f0c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Yin, Tianwei, et al. “<a class="af pl" href="https://tianweiy.github.io/dmd/" rel="noopener ugc nofollow" target="_blank">One-step diffusion with distribution matching distillation</a>.” <em class="oc">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2024.</p><p id="f94b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Buciluǎ, Cristian, Rich Caruana, and Alexandru Niculescu-Mizil. “<a class="af pl" href="https://dl.acm.org/doi/pdf/10.1145/1150402.1150464?casa_token=CY8nr3ZTpi0AAAAA%3A6T9MQ4MKzDCllOBuaurkddgR67bKLt88tc-TtEf0MfzNCdncFzr0Q1ZQZSha2GkBBYdysx78qMxdwA" rel="noopener ugc nofollow" target="_blank">Model compression.</a>” <em class="oc">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 2006.</p><p id="63bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[6] <a class="af pl" href="https://en.wikipedia.org/wiki/Knowledge_distillation" rel="noopener ugc nofollow" target="_blank">Knowledge distillation</a>, Wikipedia</p><p id="5967" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[7] <a class="af pl" href="https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5" rel="noopener">An Overview of Model Compression Techniques for Deep Learning in Space</a>, on Medium</p><p id="6361" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[8] <a class="af pl" rel="noopener" target="_blank" href="/distilling-bert-using-unlabeled-qa-dataset-4670085cc18">Distilling BERT Using an Unlabeled Question-Answering Dataset</a>, on Towards Data Science</p><p id="784f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[9] Arzamasov, Vadim, Benjamin Jochum, and Klemens Böhm. “<a class="af pl" href="https://arxiv.org/pdf/2112.13285" rel="noopener ugc nofollow" target="_blank">Pedagogical Rule Extraction to Learn Interpretable Models — an Empirical Study</a>.” <em class="oc">arXiv preprint arXiv:2112.13285</em> (2021).</p><p id="d517" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[10] Domingos, Pedro. “<a class="af pl" href="https://citeseerx.ist.psu.edu/document?repid=rep1&amp;type=pdf&amp;doi=1a9a39da9d4fc937bc455705d508674a205620aa" rel="noopener ugc nofollow" target="_blank">Knowledge acquisition from examples via multiple models.</a>” <em class="oc">MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-</em>. MORGAN KAUFMANN PUBLISHERS, INC., 1997.</p><p id="6e49" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[11] De Fortuny, Enric Junque, and David Martens. “<a class="af pl" href="https://ieeexplore.ieee.org/abstract/document/7018925?casa_token=SU8qtZ-ZkGEAAAAA%3A7L_-_Sj-eZ7x0_f4oYQgH4kynTftYbJW4ytvcEQgaq-r4VWyOdmh0lD1jXYN1otMmKzt-dIRcA" rel="noopener ugc nofollow" target="_blank">Active learning-based pedagogical rule extraction</a>.” <em class="oc">IEEE transactions on neural networks and learning systems</em> 26.11 (2015): 2664–2677.</p><p id="f0f3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[12] Guidotti, Riccardo, et al. “<a class="af pl" href="https://dl.acm.org/doi/abs/10.1145/3236009" rel="noopener ugc nofollow" target="_blank">A survey of methods for explaining black box models</a>.” <em class="oc">ACM computing surveys (CSUR)</em> 51.5 (2018): 1–42.</p><p id="5000" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[13] <a class="af pl" href="https://en.wikipedia.org/wiki/Surrogate_model" rel="noopener ugc nofollow" target="_blank">Surrogate model</a>, Wikipedia</p><p id="3d0a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[14] <a class="af pl" href="https://waterprogramming.wordpress.com/2015/08/05/scenario-discovery-in-python/" rel="noopener ugc nofollow" target="_blank">Scenario discovery in Python</a>, blog post on <a class="af pl" href="https://waterprogramming.wordpress.com/" rel="noopener ugc nofollow" target="_blank">Water Programming</a></p><p id="fb47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[15] <a class="af pl" rel="noopener" target="_blank" href="/teaching-your-model-to-learn-from-itself-8b5ef13eb173">Teaching Your Model to Learn from Itself</a>, on Towards Data Science</p></div></div></div></div>    
</body>
</html>