- en: AI vs. Human Insight in Financial Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI 与人类洞察力在财务分析中的对比
- en: 原文：[https://towardsdatascience.com/ai-vs-human-insight-in-financial-analysis-89d3408eb6d5?source=collection_archive---------6-----------------------#2024-03-15](https://towardsdatascience.com/ai-vs-human-insight-in-financial-analysis-89d3408eb6d5?source=collection_archive---------6-----------------------#2024-03-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-vs-human-insight-in-financial-analysis-89d3408eb6d5?source=collection_archive---------6-----------------------#2024-03-15](https://towardsdatascience.com/ai-vs-human-insight-in-financial-analysis-89d3408eb6d5?source=collection_archive---------6-----------------------#2024-03-15)
- en: How the Bud Light boycott and SalesForce’s innovation plans confuse the best
    LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bud Light 抵制事件和 SalesForce 创新计划如何让最优秀的语言模型感到困惑
- en: '[](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)[![Misho
    Dungarov](../Images/c65eb57145dada4f02acbb3f145a9b77.png)](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)
    [Misho Dungarov](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)[![Misho
    Dungarov](../Images/c65eb57145dada4f02acbb3f145a9b77.png)](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)
    [Misho Dungarov](https://medium.com/@mihail.dungarov?source=post_page---byline--89d3408eb6d5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)
    ·10 min read·Mar 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89d3408eb6d5--------------------------------)
    ·阅读时间：10分钟·2024年3月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f3d40b0e75e91276eebdb39ec53680c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3d40b0e75e91276eebdb39ec53680c1.png)'
- en: Image by [Dall-E 3](https://platform.openai.com/docs/guides/images/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Dall-E 3](https://platform.openai.com/docs/guides/images/)
- en: Can the best AI models today, accurately pick up the most important message
    out of a company earnings call? They can certainly pick up SOME points but how
    do we know if those are the important ones? Can we *prompt* them into to doing
    a better job? To find those answers, we look at what the best journalists in the
    field have done and try to get as close to that with AI
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天最优秀的 AI 模型能否准确从公司财报电话会议中提取出最重要的信息？它们确实能捕捉到一些要点，但我们怎么知道这些要点是否真的重要呢？我们能否*提示*它们做得更好？为了找到这些答案，我们研究了该领域一些最优秀的记者所做的工作，并尝试用
    AI 尽可能接近这些水平。
- en: The Challenge
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: In this article, I look at 8 recent company earnings calls and ask the current
    contestants for smartest AIs ([Claude 3](https://www.anthropic.com/news/claude-3-family),
    [GPT-4](https://openai.com/gpt-4) and [Mistral Large](https://mistral.ai/news/mistral-large/))
    what they think is important. Then compare the results to what some of the best
    names in Journalism (Reuters, Bloomberg, and Barron’s) have said about those exact
    reports.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我分析了 8 场最近的公司财报电话会议，并询问了目前最聪明的 AI（[Claude 3](https://www.anthropic.com/news/claude-3-family)、[GPT-4](https://openai.com/gpt-4)
    和 [Mistral Large](https://mistral.ai/news/mistral-large/)）它们认为哪些内容最重要。然后，将这些结果与一些最顶尖新闻机构（路透社、彭博社和《巴伦周刊》）对这些报告的评价进行比较。
- en: Why care about this?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要关注这个？
- en: The Significance of Earnings Calls
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 盈利电话会议的重要性
- en: Earnings calls are quarterly events where senior management reviews the company’s
    financial results. They discuss the company’s performance, share commentary, and
    sometimes preview future plans. These discussions can significantly impact the
    company’s stock price. Management explains their future expectations and reasons
    for meeting or surpassing past forecasts. The management team offers invaluable
    insights into the company’s actual condition and future direction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 盈利电话会议是公司高层审视财务结果的季度活动。在会议中，他们会讨论公司业绩，分享评论，有时还会预告未来计划。这些讨论可能会显著影响公司的股价。管理层会解释他们对未来的预期，以及为什么能够实现或超越之前的预测。管理团队提供了关于公司实际状况和未来方向的宝贵见解。
- en: The Power of Automation in Earnings Analysis
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化在盈利分析中的力量
- en: Statista reports that there are just under [4000 companies listed on the NASDAQ](https://www.statista.com/statistics/1330817/nasdaq-number-of-listed-companies-by-domicile/)
    and about [58,000 globally](https://focus.world-exchanges.org/articles/number-listed-companies)
    according to one estimate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Statista报告称，根据一项估算，[纳斯达克上市公司数量](https://www.statista.com/statistics/1330817/nasdaq-number-of-listed-companies-by-domicile/)接近[4000家](https://www.statista.com/statistics/1330817/nasdaq-number-of-listed-companies-by-domicile/)，而全球约有[58,000家公司](https://focus.world-exchanges.org/articles/number-listed-companies)。
- en: A typical conference call lasts roughly 1 hour. To just listen to all NASDAQ
    companies, one would need at least 10 people working full-time for the entire
    quarter. And this doesn’t even include the more time-consuming tasks like analyzing
    and comparing financial reports.
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一场典型的电话会议大约持续1小时。要听完所有纳斯达克公司的财报电话会议，至少需要10个人在整个季度内全职工作。而且这还不包括分析和比较财务报告等更耗时的任务。
- en: Large brokerages might manage this workload, but it’s unrealistic for individual
    investors. Automation in this area could level the playing field, making it easier
    for everyone to understand quarterly earnings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型经纪公司可能能够处理这种工作量，但对个人投资者来说是不现实的。在这个领域的自动化可以使竞争环境更加平等，让每个人都更容易理解季度财报。
- en: While this may just be within reach of large brokerages, it is not feasible
    for private investors. Therefore, any reliable automation in this space will be
    a boon, especially for democratizing the understanding of quarterly earnings.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能只对大型经纪公司而言是触手可及的，但对于私人投资者来说并不可行。因此，在这个领域的任何可靠自动化都将是一大福音，尤其是在让更多人理解季度财报方面。
- en: The Process of Testing AI as a Financial Analyst
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试AI作为金融分析师的过程
- en: 'To test how well the best LLMs of the day can do this job. I decided to compare
    the main takeaways by humans and see how well AI can mimic that. Here are the
    steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试当今最好的LLM是否能够胜任这项工作，我决定比较人类的主要要点，并查看AI如何模仿这些要点。以下是步骤：
- en: Pick some companies with recent earnings call transcripts and matching news
    articles.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 挑选一些有近期财报电话会议记录和匹配新闻文章的公司。
- en: Provide the LLMs with the full transcript as context and ask them to provide
    **the top three bullet points** that seem most impactful for the value of the
    company. This is important as, providing a longer summary becomes progressively
    easier — there are only so many important things to say.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给LLM提供完整的会议记录作为上下文，并要求它们提供**三个最具影响力的要点**，这些要点对公司价值最为重要。这很重要，因为提供一个较长的摘要变得越来越容易——毕竟，重要的事情也就那么多。
- en: 'To ensure we maximise the quality of the output, I vary the way I phrase the
    problem to the AI (using different prompts): Ranging from simply asking for a
    summary, adding more detailed instructions, adding previous transcripts and some
    combinations of those.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保我们最大化输出质量，我会变化我向AI提出问题的方式（使用不同的提示）：从简单地请求摘要、添加更详细的指示、添加先前的转录内容到这些组合方式。
- en: Finally, compare those with the 3 most important points from the respective
    news article and use the overlap as a measure of success.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将这些要点与相应新闻文章中的3个最重要的要点进行比较，并用重叠部分作为衡量成功的标准。
- en: Summary of Results
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果总结
- en: GPT-4 shows best performance at 80% when providing it the previous quarter’s
    transcript and using a set of instructions on how to analyse transcripts well
    (Chain of Thought). Notably, just using correct instructions increases GPT-4 performance
    from 51% to 75%.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在提供上一季度的会议记录并使用一套分析记录的指示（链式思维）时表现最好，准确度达到80%。值得注意的是，仅使用正确的指示，就能将GPT-4的表现从51%提高到75%。
- en: '![](../Images/64789c98fb1056fbb549b6dbe7a3bd5a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64789c98fb1056fbb549b6dbe7a3bd5a.png)'
- en: GPT-4 shows the best results and responds best to prompting (80%) — i.e. adding
    previous results and dedicated instructions on how to analyse results. Without
    sophisticated prompting, Claude 3 Opus works best (67%). Image and data by the
    author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4表现最好，对提示反应最好（80%）——即添加先前的结果和专门的分析指示。而在没有复杂提示的情况下，Claude 3 Opus表现最好（67%）。图像和数据由作者提供
- en: 'Next best performers are:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名第二的表现较好的公司是：
- en: — Claude 3 Opus (67%) — Without sophisticated prompting, Claude 3 Opus works
    best.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — Claude 3 Opus（67%）——在没有复杂提示的情况下，Claude 3 Opus表现最佳。
- en: — Mistral Large (66%) when adding supporting instructions (i.e. Chain of Thought)
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — Mistral Large（66%）在加入支持性指示（即链式思维）后
- en: '**Chain-of-thought (CoT) and Think Step by Step (SxS) seem to work well for
    GPT-4 but are detrimental for other models.** This suggests there is still a lot
    to be learned about what prompts work for each LLM.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链式思维（CoT）和逐步思考（SxS）对GPT-4效果最好，但对其他模型则有害。**这表明我们仍然需要更多地了解哪些提示适用于每个LLM。'
- en: '**Chain-of-Thought (CoT) seems almost always outperforms Step-by-step (SxS)**.
    This means tailored financial knowledge of priorities for analysis helps. The
    specific instructions provided are listed at the bottom of the article.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chain-of-Thought (CoT) 几乎总是优于 Step-by-step (SxS)**。这意味着，定制的财务分析优先知识有所帮助。具体的指示已列在文章的底部。'
- en: '**More data-less sense:** Adding a previous period transcript to the model
    context seems to be at least slightly and at worst significantly detrimental to
    results across the board than just focusing on the latest results (except for
    GPT-4 + CoT). Potentially, there is much irrelevant information introduced from
    a previous transcript and a relatively small amount of specific facts to make
    a quarter-on-quarter comparison. Mistral Large’s performance drops significantly,
    note that its context window is just 32k tokens vs the significantly larger ones
    for the others (2 transcripts + prompt actually just barely fit under 32k tokens).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多数据——更少意义**：将先前一段的转录文本添加到模型上下文中，似乎至少对整体结果略有不利，最坏的情况下则可能显著降低结果，尤其是在专注于最新结果的情况下（除了
    GPT-4 + CoT）。可能是因为从先前的转录文本中引入了大量不相关的信息，而引入了相对较少的具体事实来进行季度比较。Mistral Large 的表现显著下降，注意它的上下文窗口仅为
    32k 个令牌，而其他模型的上下文窗口显著更大（2 个转录文本 + 提示词几乎刚好适应 32k 个令牌）。'
- en: '**Claude-3 Opus and Sonnet perform very closely**, with Sonnet actually outperforming
    Opus in some cases. However, this tends to be by a few %-age points and can therefore
    be attributed to the randomness of results.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Claude-3 Opus 和 Sonnet 的表现非常接近**，在某些情况下，Sonnet 实际上超过了 Opus。然而，这种差异通常只有几个百分点，因此可以归因于结果的随机性。'
- en: Note that, as mentioned, results show a high degree of variability and the **range
    of outcomes is within +/-6%**. For that reason, I have **rerun all analysis 3
    times** and am showing the averages. However, the +/-6% range is not sufficient
    to significantly upend any of the above conclusions
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，正如前面提到的，结果显示出高度的可变性，**结果范围在 +/-6% 之间**。因此，我**重新运行了所有分析 3 次**，并显示了平均值。然而，
    +/-6% 的范围不足以显著推翻上述结论。
- en: What do LLMs get right and wrong?
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 做对了什么，做错了什么？
- en: How the Bud Light Boycott and Salesforce’s AI plans confused the best AIs
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Bud Light Boycott 和 Salesforce 的 AI 计划如何让最好的 AI 感到困惑
- en: 'This task offers some easy wins: guessing that results are about the latest
    revenue numbers and next year’s projections is fairly on the nose. Unsurprisingly,
    this is where models get things right most of the time.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这项任务提供了一些简单的机会：猜测结果大约是最新的收入数字和明年的预测，这通常是相当准确的。不出所料，这也是模型最常做对的地方。
- en: The table below gives an overview of what was mentioned in the news and what
    LLMs chose differently when summarized in just a few words.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下表概述了新闻中提到的内容以及 LLMs 在仅用几个词进行总结时选择的不同内容。
- en: '![](../Images/fb1f72934bbfb2a64ec80c3656f2d44f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb1f72934bbfb2a64ec80c3656f2d44f.png)'
- en: '“Summarize each bullet with up to 3 words”: The top three themes in the news
    vs themes the LLMs picked that were not on that list. Each model was asked to
    provide a 2–3 word summary of the bullet points. A model will have 6 sets of top
    3 choices (i.e. 24) and these are the 3 that most often were not relevant when
    compared to news summaries. Note that in some cases, comparing the top and bottom
    table may feel like both sound the same, this is mostly because each bullet is
    actually significantly more detailed and may have a lot of additional / contradictory
    information missed in the 2–3 word summary'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “用最多 3 个词总结每个要点”：新闻中的前三个主题与 LLMs 选择的主题（这些主题不在该列表中）进行对比。每个模型被要求提供 2–3 个词的要点总结。每个模型将有
    6 组前三选择（即 24 个），这 3 个选择通常是与新闻摘要相比最不相关的。请注意，在某些情况下，对比顶部和底部的表格可能感觉两个总结相似，这主要是因为每个要点实际上更为详细，且在
    2–3 个词的总结中可能遗漏了大量附加/矛盾的信息。
- en: 'Next, I tried to look for any trends of what the models consistently miss.
    Those generally Fall into a few categories:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我尝试寻找模型一致遗漏的趋势。这些趋势通常属于几个类别：
- en: '**Making sense of changes:** In the above results, LLMs have been able to understand
    fairly reliably what to look for: earnings, sales, dividend, and guidance, however,
    making sense of what is significant is still very elusive. For instance, common-sense
    might suggest that Q4 2023 results will be a key topic for any company and this
    is what the LLMs pick. However, Nordstrom talks about muted revenue and demand
    expectations for 2024 which pushes Q4 2023 results aside in terms of importance'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解变化：** 在以上结果中，LLMs能够相对可靠地识别需要关注的内容：盈利、销售、股息和指导，但理解哪些内容具有重要意义依然非常困难。例如，常识可能认为2023年第四季度的财报结果会是任何公司的关键话题，而LLMs也确实选中了这一点。然而，Nordstrom讨论了2024年较低的收入和需求预期，这使得2023年第四季度的财报在重要性上被搁置。'
- en: '**Hallucinations:** as is well documented, LLMs tend to make up facts. In this
    case, despite having instructions to “only include facts and metrics from the
    context” some metrics and dates end up being made up. The models unfortunately
    will not be shy about talking about the Q4 2024 earnings by referring to them
    as already available and using the 2023 numbers for them.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉：** 如同广泛记录的那样，大语言模型倾向于编造事实。在这种情况下，尽管有指示要求“只包括上下文中的事实和数据”，但一些指标和日期最终还是被编造出来。不幸的是，这些模型并不回避谈论2024年第四季度的财报，并称其已公布，且使用了2023年的数据。'
- en: '**Significant one-off events:** Unexpected one-off events are surprisingly
    often missed by LLMs. For instance, **the boycott of Bud Light** drove sales of
    the best-selling beer in the US down by 15.9% for *Anheuser-Busch* and is discussed
    at length in the transcripts. The number alone should appear significant, however
    it was missed by all models in the sample.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重大的单次事件：** 出乎意料的单次事件往往被大语言模型（LLMs）忽视。例如，**百威啤酒的抵制**导致美国最畅销啤酒的销量下降了15.9%，对*安海斯·布希*公司影响深远，并在记录中有详细讨论。仅这一数字就应该显得非常重要，但所有样本中的模型都忽略了这一点。'
- en: '**Actions speak louder than words:** Both GPT and Claude highlight innovation
    and the commitment to AI as important.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行动胜于言辞：** GPT和Claude都强调创新和对AI的承诺为重要内容。'
- en: — Salesforce (CRM) talks at length about a heavy focus on AI and Data Cloud
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — Salesforce（CRM）详细讨论了公司在AI和数据云方面的重视。
- en: — Snowflake appointed their SVP of AI and former exec of Google Ads as CEO (Sridhar
    Ramaswamy), similarly signaling a focus on leveraging AI technology.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — Snowflake任命了他们的AI高级副总裁和前谷歌广告高管（Sridhar Ramaswamy）为CEO，同样表明了公司致力于利用AI技术。
- en: 'Both signal a shift to innovation & AI. **However, journalists and analysts
    are not as easily tricked into mistaking words for actions.** In the article analyzing
    CRM’s earnings, the subtitle reads Salesforce Outlook Disappoints as AI Fails
    to Spark Growth. However, Salesforce has been trying to tango with AI for a while
    and the forward-looking plans to use AI are not even mentioned. Salesforce’s transcript
    mentions AI 91 times while Snowflake’s less than half of that at 39\. However,
    humans can make the distinction in meaning: Bloomberg’s article [link] on the
    appointment of a new CEO: His elevation underscores a focus on AI for Snowflake.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两者都表明公司正在转向创新与AI。**然而，记者和分析师不会轻易被言辞所误导，将其当作行动。** 在分析CRM财报的文章中，副标题写道“Salesforce前景失望，AI未能激发增长”。然而，Salesforce已经在尝试与AI合作一段时间，且其未来使用AI的计划甚至没有被提及。Salesforce的记录提到AI91次，而Snowflake的只有不到一半，只有39次。然而，人类可以辨别其中的含义：彭博社的文章[链接]讨论了Snowflake任命新CEO的消息：他的升职强调了Snowflake对AI的关注。
- en: Experiment design and choices
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验设计与选择
- en: '**Why Earnings call transcripts?** The more intuitive choice may be company
    filings, however, I find transcripts to present a more natural and less formal
    discussion of events. I believe transcripts give the LLM as a reasoning engine
    a better chance to glean more natural commentary of events as opposed to the dry
    and highly regulated commentary of earnings. The calls are mostly management presentations,
    which might skew things toward a more positive view. However, my analysis has
    shown the performance of the LLMs seems similar between positive and negative
    narratives.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为什么选择财报电话会议记录？** 更直观的选择可能是公司文件，但我发现电话会议记录提供了一个更加自然、少形式化的事件讨论。我认为，记录能让LLM作为推理引擎，更好地提取事件的自然评论，而不像财报那样枯燥且高度规范化。电话会议大多是管理层的陈述，可能会将事物偏向于更积极的观点。然而，我的分析表明，在积极和消极的叙述中，LLMs的表现似乎相似。'
- en: '**Choice of Companies:** I chose stocks that have published Q4 2023 earnings
    reports between 25 Feb and 5 March and have been reported on by one of Reuters,
    Bloomberg, or Barron’s. This ensures that the results are timely and that the
    models have not been trained on that data yet. Plus, everyone always talks about
    AAPL and TSLA, so this is something different. Finally, the reputation of these
    journalistic houses ensures a meaningful comparison. The 8 stocks we ended up
    with are: *Autodesk (ADSK), BestBuy (BBY), Anheuser-Busch InBev (BUD), Salesforce
    (CRM), DocuSign (DOCU), Nordstrom (JWN), Kroger (KR), Snowflake (SNOW)*'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**公司选择：** 我选择了在2023年2月25日到3月5日之间发布了第四季度财报，并且已被路透社、彭博社或《巴伦周刊》报道的股票。这确保了结果是及时的，并且模型尚未在这些数据上进行过训练。此外，每个人总是谈论AAPL和TSLA，因此这是一个不同的选择。最后，这些新闻机构的声誉确保了有意义的比较。我们最终选定的8只股票是：*Autodesk
    (ADSK), BestBuy (BBY), Anheuser-Busch InBev (BUD), Salesforce (CRM), DocuSign
    (DOCU), Nordstrom (JWN), Kroger (KR), Snowflake (SNOW)*'
- en: '**Variability of results** LLM results can vary between runs so I have run
    all experiments 3 times and show an average. All analysis for all models was done
    using temperature 0 which is commonly used to minimize variation of results. In
    this case, I have observed different runs have as much as 10% difference in performance.
    This is due to the small sample (only 24 data points 8 stocks by 3 statements)
    and the fact that we are basically asking an LLM to choose one of many possible
    statements for the summary, so when this happens with some randomness it can naturally
    lead to picking some of them differently.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结果的可变性** LLM的结果在不同的运行之间可能有所不同，因此我进行了三次实验并展示了平均值。所有模型的分析均使用温度值0，这通常用于最小化结果的变化。在这种情况下，我观察到不同运行之间的性能差异最多可达10%。这是由于样本量较小（仅24个数据点，每只股票3条声明），并且我们基本上要求LLM从众多可能的声明中选择一个摘要，因此，当这种选择具有一定随机性时，自然会导致选择的差异。'
- en: '**Choice of Prompts:** For each of the 3 LLMs in comparison try out 4 different
    prompting approaches:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示词选择：** 对于三种比较的LLM，我尝试了四种不同的提示方法：'
- en: '**Naive** — The prompt simply asks the model to determine the most likely impact
    on the share price.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单模型** — 提示仅要求模型确定对股价最可能的影响。'
- en: '**Chain-of-Thought (CoT)** — where I provide a detailed list of steps to follow
    when choosing a summary. This is inspired and loosely follows [[Wei et. al. 2022]](https://arxiv.org/abs/2201.11903)
    work outlining the Chain of Thought approach, providing reasoning steps as part
    of the prompt dramatically improves results. These additional instructions, in
    the context of this experiment, include typical drivers of price movements: changes
    to expected performance in revenue, costs, earnings, litigation, etc.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**思维链（CoT）** — 我提供一个详细的步骤列表，在选择摘要时遵循这些步骤。这一方法的灵感来源并且大致遵循[[Wei等人 2022]](https://arxiv.org/abs/2201.11903)的工作，提出思维链方法，提示中提供推理步骤能够显著提高结果。在本实验的上下文中，这些附加指令包括影响价格波动的典型因素：预期收入、成本、收益、诉讼等方面的变化。'
- en: '**Step by Step (SxS)** aka Zero-shot CoT, inspired by [Kojima et.al (2022)](https://arxiv.org/abs/2205.11916)
    where they discovered that simply adding the phrase “Let’s think step by step”
    improves performance. I ask the LLMs to think step-by-step and describe their
    logic before answering.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步（SxS）** 又名零-shot Chain-of-Thought（CoT），灵感来源于[Kojima等人（2022）](https://arxiv.org/abs/2205.11916)，他们发现仅仅加入“让我们逐步思考”这一短语就能提高性能。我要求大型语言模型（LLMs）逐步思考并在回答之前描述他们的逻辑。'
- en: '**Previous transcript** — finally, I run all three of the above prompts once
    more by including the transcript from the previous quarter (in this case Q3)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先前的记录** — 最后，我再次运行以上三种提示方法，并包含上一季度的记录（在本例中为Q3）。'
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: From what we can see above, Journalists’ and Research Analysts’ jobs seem safe
    for now, as most LLMs struggle to get more than two of three answers correctly.
    In most cases, this just means guessing that the meeting was about the latest
    revenue and next year’s projections.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的内容来看，记者和研究分析师的工作似乎暂时是安全的，因为大多数LLMs在正确回答三分之二的问题上存在困难。在大多数情况下，这仅意味着他们猜测会议讨论的是最新的收入和明年的预测。
- en: 'However, despite all the limitations of this test, we can still see some clear
    conclusions:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管该测试存在所有局限性，我们仍然可以得出一些明确的结论：
- en: The accuracy level is fairly low for most models. Even GPT-4’s best performance
    of 80% will be problematic at scale without human supervision — giving wrong advice
    one in five times is not convincing.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大多数模型来说，准确率相当低。即使是GPT-4的最佳表现80%，在没有人工监督的情况下也会面临大规模应用的挑战——每五次中就有一次给出错误建议，这样的表现并不具有说服力。
- en: GPT4 seems to still be a clear leader in complex tasks it was not specifically
    trained for.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT-4似乎在其未专门训练的复杂任务中仍然是一个明确的领导者。
- en: There are significant gains when correctly prompt engineering the task
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确地进行任务提示工程会带来显著的收益
- en: Most models seem easily confused by extra information as adding the previous
    transcript generally reduces performance.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数模型似乎容易被额外的信息所困惑，因为加入之前的会议记录通常会降低性能。
- en: Where to from here?
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来该怎么做？
- en: 'We have all witnessed that LLM capabilities continuously improve. Will this
    gap be closed and how? We have observed three types of cognitive issues that have
    impacted performance: hallucinations, understanding what is important and what
    isn’t (e.g. really understanding what is *surprising* for a company), more complex
    company causality issues (e.g. like the Bud Light boycott and how important the
    US sales are relative to an overall business):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都已经见证了LLM能力的持续提升。这一差距是否能够弥合，又该如何弥合？我们观察到三种影响性能的认知问题：幻觉、理解什么是重要的而什么不重要（例如，真正理解什么对一家公司来说是*令人惊讶*的），更复杂的公司因果关系问题（例如，像Bud
    Light抵制事件，以及美国销售对整体业务的相对重要性）：
- en: '**Hallucinations** or scenarios where the LLM cannot correctly reproduce factual
    information are a major stumbling block in applications that require strict adherence
    to factuality. Advanced RAG approaches, combined with research in the area continue
    to make progress, [[Huang et al 2023]](https://arxiv.org/abs/2311.05232) give
    an overview of current progress'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**或LLM无法正确再现事实信息的情况，是要求严格遵守事实准确性的应用中的一大障碍。先进的RAG方法结合该领域的研究不断取得进展，[[Huang
    et al 2023]](https://arxiv.org/abs/2311.05232)概述了当前的进展'
- en: '**Understanding what is important** — fine-tuning LLM models for the specific
    use case should lead to some improvements. However, those come with much bigger
    requirements on team, cost, data, and infrastructure.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解什么是重要的** —— 针对特定应用场景对LLM模型进行微调应该会带来一定的改进。然而，这些改进伴随着对团队、成本、数据和基础设施的更大要求。'
- en: '**Complex Causality Links** — this one may be a good direction for AI Agents.
    For instance, in the Bud Light boycott case, the model might need to:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂的因果关系链** —— 这一方向可能是AI代理的一个良好发展方向。例如，在Bud Light抵制事件中，模型可能需要：'
- en: 1\. the importance of Bud Light to US sales, which is likely peppered through
    many presentations and management commentary
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1\. Bud Light对美国销售的重要性，可能在许多演示和管理层评论中有所提及
- en: 2\. The importance of US sales ot the overall company, which could be gleaned
    from company financials
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2\. 美国销售对整体公司的重要性，可以从公司财务中获取信息
- en: 3\. Finally stack those impacts to all other impacts mentioned
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3\. 最后将这些影响与所有其他提到的影响进行叠加
- en: Such causal logic is more akin to how a ReAct AI Agent might think instead of
    just a standalone LLM [[Yao, et al 2022]](https://arxiv.org/abs/2210.03629). Agent
    planning is a hot research topic [[Chen, et al 2024]](https://arxiv.org/abs/2402.10890)
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种因果逻辑更类似于ReAct AI代理的思考方式，而不是仅仅一个独立的LLM[[Yao, et al 2022]](https://arxiv.org/abs/2210.03629)。代理规划是一个热门的研究课题[[Chen,
    et al 2024]](https://arxiv.org/abs/2402.10890)
- en: '[Follow me on LinkedIn](https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=mihail-misho-dungarov-cfa-a0291a88)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[在LinkedIn关注我](https://linkedin.com/comm/mynetwork/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=mihail-misho-dungarov-cfa-a0291a88)'
- en: Disclaimers
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: '*The views, opinions, and conclusions expressed in this article are my own
    and do not reflect the views or positions of any of the entities mentioned or
    any other entities.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文中表达的观点、意见和结论仅代表我个人的看法，并不反映文中提到的任何实体或其他实体的观点或立场。*'
- en: '*No data was used to model training nor was systematically collected from the
    sources mentioned, all techniques were limited to prompt engineering.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*没有使用数据进行模型训练，也没有系统地从提到的来源收集数据，所有技术都仅限于提示工程。*'
- en: Resources
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: Earnings Call Transcripts (Motley Fool)
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 盈利电话会议记录（Motley Fool）
- en: '[Anheuser-Busch InBev (BUD) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/anheuser-busch-inbevnv-bud-q4-2023-earnings-call-t/)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[安海斯-布希英博（Anheuser-Busch InBev，BUD）Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/anheuser-busch-inbevnv-bud-q4-2023-earnings-call-t/)'
- en: '[Autodesk (ADSK) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/autodesk-adsk-q4-2024-earnings-call-transcript/)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Autodesk (ADSK) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/autodesk-adsk-q4-2024-earnings-call-transcript/)'
- en: '[Best Buy (BBY) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/best-buy-bby-q4-2024-earnings-call-transcript/)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Best Buy (BBY) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/best-buy-bby-q4-2024-earnings-call-transcript/)'
- en: '[DocuSign (DOCU) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/03/07/docusign-docu-q4-2024-earnings-call-transcript/)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DocuSign (DOCU) 2024 财年第四季度](https://www.fool.com/earnings/call-transcripts/2024/03/07/docusign-docu-q4-2024-earnings-call-transcript/)'
- en: '[Kroger (KR) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/03/07/kroger-kr-q4-2023-earnings-call-transcript/)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[克罗格 (KR) 2024 财年第四季度](https://www.fool.com/earnings/call-transcripts/2024/03/07/kroger-kr-q4-2023-earnings-call-transcript/)'
- en: '[Nordstrom (JWN) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/03/05/nordstrom-jwn-q4-2023-earnings-call-transcript/)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[诺德斯特龙 (JWN) 2024 财年第四季度](https://www.fool.com/earnings/call-transcripts/2024/03/05/nordstrom-jwn-q4-2023-earnings-call-transcript/)'
- en: '[Salesforce (CRM) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/salesforce-crm-q4-2024-earnings-call-transcript/)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Salesforce (CRM) 2024 财年第四季度](https://www.fool.com/earnings/call-transcripts/2024/02/29/salesforce-crm-q4-2024-earnings-call-transcript/)'
- en: '[Snowflake (SNOW) Q4 2024](https://www.fool.com/earnings/call-transcripts/2024/02/29/snowflake-snow-q4-2024-earnings-call-transcript/)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Snowflake (SNOW) 2024 财年第四季度](https://www.fool.com/earnings/call-transcripts/2024/02/29/snowflake-snow-q4-2024-earnings-call-transcript/)'
- en: News Articles
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新闻文章
- en: '[Anheuser-Busch InBev (Reuters)](https://www.reuters.com/business/retail-consumer/brewer-ab-inbev-hikes-annual-dividend-after-q4-sales-estimate-beat-2024-02-29/)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[安海斯-布希英博 (路透社)](https://www.reuters.com/business/retail-consumer/brewer-ab-inbev-hikes-annual-dividend-after-q4-sales-estimate-beat-2024-02-29/)'
- en: '[Autodesk (Reuters)](https://www.reuters.com/technology/autodesk-forecasts-annual-revenue-above-estimates-shares-jump-2024-02-29/)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[欧特克 (路透社)](https://www.reuters.com/technology/autodesk-forecasts-annual-revenue-above-estimates-shares-jump-2024-02-29/)'
- en: '[BestBuy (Bloomberg)](https://www.bloomberg.com/news/articles/2024-02-29/best-buy-sales-decline-at-slower-pace-as-demand-improves)'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[百思买 (彭博社)](https://www.bloomberg.com/news/articles/2024-02-29/best-buy-sales-decline-at-slower-pace-as-demand-improves)'
- en: '[DocuSign (Barron’s)](https://www.barrons.com/articles/docusign-shares-earnings-20862b9a)'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DocuSign (巴伦周刊)](https://www.barrons.com/articles/docusign-shares-earnings-20862b9a)'
- en: '[Kroger (Barron’s)](https://www.barrons.com/articles/kroger-stock-earnings-9b9bebd6)'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[克罗格 (巴伦周刊)](https://www.barrons.com/articles/kroger-stock-earnings-9b9bebd6)'
- en: '[Nordstrom (Bloomberg)](https://www.bloomberg.com/news/articles/2024-03-05/nordstrom-sees-muted-revenue-comparable-sales-in-current-year)'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[诺德斯特龙 (彭博社)](https://www.bloomberg.com/news/articles/2024-03-05/nordstrom-sees-muted-revenue-comparable-sales-in-current-year)'
- en: '[Salesforce (Bloomberg)](https://www.bloomberg.com/news/articles/2024-02-28/salesforce-outlook-disappoints-as-ai-fails-to-spark-growth)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Salesforce (彭博社)](https://www.bloomberg.com/news/articles/2024-02-28/salesforce-outlook-disappoints-as-ai-fails-to-spark-growth)'
- en: '[Snowflake (Bloomberg)](https://www.bloomberg.com/news/articles/2024-02-28/snowflake-misses-estimates-says-ceo-slootman-to-step-down)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Snowflake (彭博社)](https://www.bloomberg.com/news/articles/2024-02-28/snowflake-misses-estimates-says-ceo-slootman-to-step-down)'
