<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Estimate the Unobserved: Moving-Average Model Estimation with Maximum Likelihood in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Estimate the Unobserved: Moving-Average Model Estimation with Maximum Likelihood in Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/estimate-the-unobserved-moving-average-model-estimation-with-maximum-likelihood-in-python-5f372cec3652?source=collection_archive---------11-----------------------#2024-06-28">https://towardsdatascience.com/estimate-the-unobserved-moving-average-model-estimation-with-maximum-likelihood-in-python-5f372cec3652?source=collection_archive---------11-----------------------#2024-06-28</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f479" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How unobserved covariates’ coefficients can be estimated with MLE</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pollak.daniel?source=post_page---byline--5f372cec3652--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Pollak" class="l ep by dd de cx" src="../Images/a48f0aa944aeb4189e75cfc99949b4a7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*dn7WpqyK53Mbg_hxTfgFWg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5f372cec3652--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@pollak.daniel?source=post_page---byline--5f372cec3652--------------------------------" rel="noopener follow">Daniel Pollak</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5f372cec3652--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 28, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/300647241e2591ade3f90bbd02c13b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uF7hYKlIIIRNs80p"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@cjnaasz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Connor Naasz</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="88fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk ny"><span class="l nz oa ob bo oc od oe of og ed">F</span>or those experienced with time series data and forecasting, terms like regressions, AR, MA, and ARMA should be familiar. Linear Regression is a straightforward model with a closed-form parametric solution obtained through OLS. AR models can also be estimated using OLS. However, things become more complex with MA models, which form the second component of the more advanced ARMA and ARIMA models.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0f83" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The plan for this story:</p><ol class=""><li id="44a5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx op oq or bk">Introducing Moving Average models</li><li id="ed34" class="nc nd fq ne b go os ng nh gr ot nj nk nl ou nn no np ov nr ns nt ow nv nw nx op oq or bk">Discussing why there’s no closed solution for MA model</li><li id="fba7" class="nc nd fq ne b go os ng nh gr ot nj nk nl ou nn no np ov nr ns nt ow nv nw nx op oq or bk">Introducing Maximum Likelihood Estimation method</li><li id="991b" class="nc nd fq ne b go os ng nh gr ot nj nk nl ou nn no np ov nr ns nt ow nv nw nx op oq or bk">MA(1) ML estimation — Theory and Python code</li></ol><h1 id="1724" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Moving Average Model</h1><p id="b079" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">MA models can be described by the following formula:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj py"><img src="../Images/d8b50b75af780962a4fd28205f7e819b.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/0*HRtlyEyVvTTV80rZ"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Eq 1: MA(q) formula</figcaption></figure><p id="2d17" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, the thetas represent the model parameters, while the epsilons are the error terms, assumed to be mutually independent and normally distributed with constant variance. The intuition behind this formula is that our time series, X, can always be described by the last q shocks that occurred in the series. It is evident from the formula that each shock impacts only the subsequent q values of X, in contrast to AR models where the effect of a shock persists indefinitely, although gradually diminishing over time.</p><h2 id="2aa8" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Closed estimation of a simple model — Linear Regression</h2><p id="cfb0" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">As a reminder, the general form of a linear regression equation looks like this:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qq"><img src="../Images/05e531f5319c4f008477de2efa698218.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/0*EpJ3IlcGRP_YnOOT"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Eq 2: General linear regression formula</figcaption></figure><p id="8a5b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For forecasting tasks, we typically aim to estimate all the model’s parameters (the betas) using a sample set of x’s and y’s. Given a few assumptions about our model, the <strong class="ne fr">Gauss–Markov theorem</strong> states that the ordinary least squares (OLS) estimators for the betas have the lowest sampling variance among the class of linear unbiased estimators. In simpler terms, OLS provides us with the best possible estimation of the betas.</p><p id="101f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, what is OLS? It is a closed-form solution to a loss function minimization problem:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qr"><img src="../Images/44114c60ef9374efc409c10735aedd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/0*zjk4960mJixD6cpL"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Eq 3: OLS minimization equation</figcaption></figure><p id="4e8e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where the loss function, S, is defined as follows -</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qs"><img src="../Images/46e61adacddd09c1deaca8c27e0ac1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*U8mplByrVsIsSmcv"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Eq 4: OLS loss function</figcaption></figure><p id="3935" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this context, y and X are our sample data and are <strong class="ne fr">observable</strong> vectors of numbers (as in time series). Therefore, it is straightforward to calculate the function S, determine its derivative, and find the beta that solves the minimization problem.</p><h2 id="e88e" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Closed estimation of MA(q)</h2><p id="713c" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">It is should be clear why applying a method like OLS to estimate MA(q) models is problematic — the dependent variable, the time series values, are described by <strong class="ne fr">unobservable</strong> variables, the epsilons. This raises the question: how can these models be estimated at all?</p><h1 id="9ec7" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Maximum Likelihood (MLE)</h1><h2 id="4fb0" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Likelihood Function</h2><p id="e9fb" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Statistical distributions typically depend on one or more parameters. For instance, the normal distribution is characterized by its mean and variance, which define its “height” and “center of mass” —</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/ece1c8b44d8345a842f0ef08a6bfcb99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A5pa5iObkU2Q6xiE.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Normal distribution from <a class="af nb" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><p id="1c0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Suppose we have a dataset, X={x_1,…x_n}, comprising samples drawn from an unknown normal distribution, with its parameters unknown. Our objective is to determine the mean and variance values that would characterize the specific normal distribution from which our dataset X is most <strong class="ne fr">likely</strong> to have been sampled.</p></div></div></div><div class="ab cb oh oi oj ok" role="separator"><span class="ol by bm om on oo"/><span class="ol by bm om on oo"/><span class="ol by bm om on"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b3df" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">MLE provides a framework that precisely tackles this question. It introduces a likelihood function, which is a function that yields another function. This likelihood function takes a vector of parameters, often denoted as theta, and produces a <strong class="ne fr">probability density function</strong> (PDF) that depends on theta.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qu"><img src="../Images/90bb1564ef187f89653b8e970dcbf7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*pPB0Ztwol2yo8mXpO1G-Mw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The likelihood function general definition</figcaption></figure><p id="00f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The probability density function (PDF) of a distribution is a function that takes a value, x, and returns its probability within the distribution. Therefore, likelihood functions are typically expressed as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qv"><img src="../Images/456f9612c7bf6b89851084577631fd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*lvYaBDZsym9F1NMHGiz6CQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Likelihood as a function of theta given x</figcaption></figure><p id="1973" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The value of this function indicates the <strong class="ne fr">likelihood of observing x from the distribution defined by the PDF with theta as its parameters</strong>.</p><h2 id="9cef" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">The goal</h2><p id="fa2f" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">When constructing a forecast model, we have data samples and a parameterized model, and our goal is to estimate the model’s parameters. In our examples, such as Regression and MA models, these parameters are the coefficients in the respective model formulas.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qw"><img src="../Images/a686eaf98592b3337da346b90ecf6f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*LbIWNxvP21r23CuqRsdgRQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Statistic model estimation process</figcaption></figure><p id="ba5d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The equivalent in MLE is that we have observations and a PDF for a distribution defined over a set of parameters, theta, which are unknown and not directly observable. <strong class="ne fr">Our goal is to estimate theta</strong>.</p><p id="6e0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The MLE approach involves finding the set of parameters, theta, that maximizes the likelihood function given the observable data, x.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qx"><img src="../Images/6fa203b5d5c3e4913ca968a43172b6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*CHuru2bfaC1TQ0TQFjsTJw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Maximization of the likelihood function</figcaption></figure><p id="d8d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We assume our samples, x, are drawn from a distribution with a known PDF that depends on a set of parameters, theta. This implies that the likelihood (probability) of observing x under this PDF is essentially 1. Therefore, identifying the theta values that make our likelihood function value close to 1 on our samples, should reveal the true parameter values.</p><h2 id="7005" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Conditional likelihood</h2><p id="028f" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Notice that we haven’t made any assumptions about the distribution (PDF) on which the likelihood function is based. Now, let’s assume our observation X is a vector (x_1, x_2, …, x_n). We’ll consider a probability function that represents the probability of observing x_n conditional on that we have already observed (x_1, x_2, …, x_{n-1}) —</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qy"><img src="../Images/b628eb50e4fbb5553662921a658cd9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*ccfeR4oQ2eWvv9QGnsxqhg.png"/></div></figure><p id="eec5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This represents the likelihood of observing just x_n given the previous values (and theta, the set of parameters). Now, we define the conditional likelihood function as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qz"><img src="../Images/d432a00759163e07d59afd0e2c528354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*cUp-XBix_X6_eQepUfDEDQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Conditional likelihood function</figcaption></figure><p id="a3d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Later, we will see why it is useful to employ the conditional likelihood function rather than the exact likelihood function.</p><h2 id="99d1" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Log-Likelihood</h2><p id="a2ed" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">In practice, it is often convenient to use the natural logarithm of the likelihood function, referred to as the log-likelihood function:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ra"><img src="../Images/43e35a0a64b76e0493f71695bb840b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*9jKhHGQFsIyesZh0Ft6yYQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Maximize the log-likelihood function</figcaption></figure><p id="2295" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is more convenient because we often work with a likelihood function that is a joint probability function of independent variables, which translates to the product of each variable’s probability. Taking the logarithm converts this product into a sum.</p><h1 id="3cb3" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Estimating MA(1) with MLE</h1><p id="85e2" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">For simplicity, I’ll demonstrate how to estimate the most basic moving average model — MA(1):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rb"><img src="../Images/2cb70a6bc4250b8adbad548dd8b753bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*xb4DpbWDtIq1srmH_U5xUA.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">MA(1) model</figcaption></figure><p id="5809" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, x_t represents the time-series observations, alpha and beta are the model parameters to be estimated, and the epsilons are random noise drawn from a normal distribution with zero mean and some variance — sigma, which will also be estimated. Therefore, our “theta” is (alpha, beta, sigma), which we aim to estimate.</p><p id="cc12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s define our parameters and generate some synthetic data using Python:</p><pre class="ml mm mn mo mp rc rd re bp rf bb bk"><span id="d4aa" class="rg oy fq rd b bg rh ri l rj rk">import pandas as pd<br/>import numpy as np<br/><br/>STD = 3.3<br/>MEAN = 0<br/>ALPHA = 18<br/>BETA = 0.7<br/>N = 1000<br/><br/>df = pd.DataFrame({"et": np.random.normal(loc=MEAN, scale=STD, size=N)})<br/>df["et-1"] = df["et"].shift(1, fill_value=0)<br/>df["xt"] = ALPHA + (BETA*df["et-1"]) + df["et"]</span></pre><p id="9047" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that we have set the standard deviation of the error distribution to 3.3, with alpha at 18 and beta at 0.7. The data looks like this —</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rl"><img src="../Images/3db379dc985a0e08595c01c315be070e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qu_a3LpgzE2sCLsYrvFHtQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Simulation of MA(1) DGP</figcaption></figure><h2 id="6b8d" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Likelihood function for MA(1)</h2><p id="7fe9" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Our objective is to construct a likelihood function that addresses the question: how likely is it to observe our time series X=(x_1, …, x_n) assuming they are generated by the MA(1) process described earlier?</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qy"><img src="../Images/947345b1be99350bbb24f5c8ea3a1762.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*LhDwEfgkkKEehMRTKGjU_w.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Likelihood for observing X</figcaption></figure><p id="7b77" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The challenge in computing this probability lies in the mutual dependence among our samples — as evident from the fact that both x_t and x_{t-1} depend on e_{t-1) — making it non-trivial to determine the joint probability of observing all samples (referred to as the exact likelihood).</p><p id="83f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Therefore, as discussed previously, instead of computing the exact likelihood, we’ll work with a conditional likelihood. Let’s begin with the likelihood of observing a single sample given all previous samples:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rm"><img src="../Images/c0bb56d8d620460ed195d16df8b9ffc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*h4-Rll3vY8NSUqsD3tjPsg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Conditional likelihood for observing x_n given the rest</figcaption></figure><p id="f794" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is much simpler to calculate because —</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj rn"><img src="../Images/d6f46649792a3912f04dfbbfd1cdf268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*cXo0ip1IsGrARZuDf8nqRg.png"/></div></figure><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ro"><img src="../Images/be8c97e8745aec7ec37422597408f555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxL6ALIdvVEpm47OKWcXeQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">PDF of normal distribution</figcaption></figure><p id="d851" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All that remains is to calculate the conditional likelihood of observing all samples:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rp"><img src="../Images/90b57b8dca8c17a5890715d0796b95dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nY7r31Mn27gRmSAJuK82g.png"/></div></div></figure><p id="2c04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">applying a natural logarithm gives:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rq"><img src="../Images/83615610be05471262a0d06a459e9076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ww_c021bU6fWJbXagQjWtw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Final likelihood function to maximize</figcaption></figure><p id="1ad5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">which is the function we should maximize.</p><h2 id="0891" class="pz oy fq bf oz qa qb qc pc qd qe qf pf nl qg qh qi np qj qk ql nt qm qn qo qp bk">Code</h2><p id="f76d" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">We’ll utilize the <code class="cx rr rs rt rd b">GenericLikelihoodModel</code> class from statsmodels for our MLE estimation implementation. As outlined in the <a class="af nb" href="https://www.statsmodels.org/dev/examples/notebooks/generated/generic_mle.html" rel="noopener ugc nofollow" target="_blank">tutorial</a> on statsmodels’ website, we simply need to subclass this class and include our likelihood function calculation:</p><pre class="ml mm mn mo mp rc rd re bp rf bb bk"><span id="e4f3" class="rg oy fq rd b bg rh ri l rj rk">from scipy import stats<br/>from statsmodels.base.model import GenericLikelihoodModel<br/>import statsmodels.api as sm<br/><br/>class MovingAverageMLE(GenericLikelihoodModel):<br/>    def initialize(self):<br/>        super().initialize()<br/>        extra_params_names = ['beta', 'std']<br/>        self._set_extra_params_names(extra_params_names)<br/><br/>        self.start_params = np.array([0.1, 0.1, 0.1])<br/><br/>    def calc_conditional_et(self, intercept, beta):<br/>        df = pd.DataFrame({"xt": self.endog})<br/>        ets = [0.0]<br/>        for i in range(1, len(df)):<br/>            ets.append(df.iloc[i]["xt"] - intercept - (beta*ets[i-1]))<br/><br/>        return ets<br/><br/>    def loglike(self, params):<br/>        ets = self.calc_conditional_et(params[0], params[1])<br/>        return stats.norm.logpdf(<br/>            ets,<br/>            scale=params[2],<br/>        ).sum()</span></pre><p id="9b4d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The function <code class="cx rr rs rt rd b">loglike</code> is essential to implement. Given the iterated parameter values <code class="cx rr rs rt rd b">params</code>and the dependent variables (in this case, the time series samples), which are stored as class members <code class="cx rr rs rt rd b">self.endog</code>, it calculates the conditional log-likelihood value, as we discussed earlier.</p><p id="02f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s create the model and fit on our simulated data:</p><pre class="ml mm mn mo mp rc rd re bp rf bb bk"><span id="c4b6" class="rg oy fq rd b bg rh ri l rj rk">df = sm.add_constant(df) # add intercept for estimation (alpha)<br/>model = MovingAverageMLE(df["xt"], df["const"])<br/>r = model.fit()<br/>r.summary()</span></pre><p id="0eae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">and the output is:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ru"><img src="../Images/9e4512927b243a89efd88794d665358f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbYb_BCww_jFQTpZA7HG0Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">MLE results from python</figcaption></figure><p id="4842" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And that’s it! As demonstrated, MLE successfully estimated the parameters we selected for simulation.</p><h1 id="911b" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Wrapping Up</h1><p id="47e8" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">Estimating even a simple MA(1) model with maximum likelihood demonstrates the power of this method, which not only allows us to make efficient use of our data but also provides a solid statistical foundation for understanding and interpreting the dynamics of time series data.</p><p id="fade" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hope you liked it !</p><h1 id="1ebc" class="ox oy fq bf oz pa pb gq pc pd pe gt pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">References</h1><p id="2b4c" class="pw-post-body-paragraph nc nd fq ne b go pt ng nh gr pu nj nk nl pv nn no np pw nr ns nt px nv nw nx fj bk">[1] Andrew Lesniewski, <a class="af nb" href="https://mfe.baruch.cuny.edu/wp-content/uploads/2014/12/TS_Lecture1_2019.pdf" rel="noopener ugc nofollow" target="_blank">Time Series Analysis</a>, 2019, Baruch College, New York</p><p id="0f32" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] Eric Zivot, <a class="af nb" href="https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf" rel="noopener ugc nofollow" target="_blank">Estimation of ARMA Models</a>, 2005</p><p id="9789" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="rv">Unless otherwise noted, all images are by the author</em></p></div></div></div></div>    
</body>
</html>