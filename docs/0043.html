<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Inspecting Neural Network Model Performance for Edge Deployment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Inspecting Neural Network Model Performance for Edge Deployment</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/inspecting-neural-network-model-performance-for-edge-deployment-d4f8f18dbfd5?source=collection_archive---------11-----------------------#2024-01-05">https://towardsdatascience.com/inspecting-neural-network-model-performance-for-edge-deployment-d4f8f18dbfd5?source=collection_archive---------11-----------------------#2024-01-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="18da" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A detailed look at quantizing CNN- and transformer-based models and techniques to measure and understand their efficacy on edge hardware</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lindo.st.angel?source=post_page---byline--d4f8f18dbfd5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lindo St. Angel" class="l ep by dd de cx" src="../Images/e742a501401428448a235030414f851b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*gtrQTQeyq1eMppql."/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d4f8f18dbfd5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lindo.st.angel?source=post_page---byline--d4f8f18dbfd5--------------------------------" rel="noopener follow">Lindo St. Angel</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d4f8f18dbfd5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/02969585c26b65b1943f459ac7613f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lxe3CJnJJ6wuWGtH"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@gavla?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Gavin Allanwood</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="a4d6" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="f148" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">This article will show you how to convert and quantize neural network models for inference at the edge and how to inspect them for quantization efficacy, understand runtime latency, and model memory usage to optimize performance. Although focused on solving the non-intrusive load monitoring (NILM) problem using convolutional neural networks (CNN) and transformer-based neural networks as a way of illustrating the techniques introduced here, you can use the general approach to train, quantize, and analyze models to solve other problems.</p><p id="aa70" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The goal of NILM is to recover the energy consumption of individual appliances from the aggregate mains signal, which reflects the total electricity consumption of a building or house. NILM is also known as energy disaggregation, and you can use both terms interchangeably.</p><p id="b5e2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can find the code used to generate the results shown in this article on my GitHub, <a class="af nc" href="https://github.com/goruck/nilm" rel="noopener ugc nofollow" target="_blank">Energy Management Using Real-Time Non-Intrusive Load Monitoring</a>, and additional details omitted here for brevity.</p><h1 id="c27c" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">NILM Algorithm Selection and Models</h1><h2 id="0835" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">Algorithm Selection</h2><p id="a527" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Energy disaggregation is a highly under-determined and single-channel <a class="af nc" href="https://en.wikipedia.org/wiki/Signal_separation" rel="noopener ugc nofollow" target="_blank">Blind Source Separation</a> (BSS) problem, which makes it challenging to obtain accurate predictions. Let <em class="pr">M</em> be the number of household appliances, and <em class="pr">i</em> be the index referring to the i-th appliance. The aggregate power consumption <em class="pr">x</em> at a given time <em class="pr">t</em> is the sum of the power consumption of all appliances <em class="pr">M</em>, denoted by <em class="pr">yᵢ, </em>for all {i=1,…,M}. Therefore, the total power consumption <em class="pr">x</em> at a given time <em class="pr">t</em> can expressed by Equation 1, where <em class="pr">e</em> is a noise term.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ps"><img src="../Images/f3ed1d9a08d6d52b1366bd81d3527b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*-ZofJP1EWfVw__LWzbk5Fg.jpeg"/></div></figure><p id="0cec" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The goal is to solve the inverse problem and estimate the appliance power consumption <em class="pr">yᵢ</em>, given the aggregate power signal <em class="pr">x</em>, and to do so in a manner suitable for deployment at the edge.</p><p id="b480" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can solve the single-channel BSS problem by using sequence-to-point (seq2point) learning with neural networks, and it can applied to the NILM problem using transformers, convolutional (CNN), and recurrent neural networks. Seq2point learning involves training a neural network to map between an input time series, such as the aggregate power readings in the case of NILM, and an output signal. You use a sliding input window to train the network, which generates a corresponding single-point output at the window’s midpoint.</p><p id="9e58" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I selected the seq2point learning approach, and my implementation was inspired and guided by the work described by Michele D’Incecco, et al. ¹ and Zhenrui Yue et al. ². I developed various seq2point learning models but focused my work on the models based on transformer and CNN architectures.</p><h2 id="d714" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">Neural Network Models</h2><p id="3025" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">You can see the CNN model in Figure 1 for an input sequence length of 599 samples. You can view the complete model code <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/define_models.py#L72" rel="noopener ugc nofollow" target="_blank">here</a>. The model follows traditional CNN concepts from vision use cases where several convolutional layers extract features from the input power sequence at gradually finer details as the input traverses the network. These features are the appliances’ on-off patterns and power consumption levels. Max pooling manages the complexity of the model after each convolutional layer. Finally, dense layers output the window’s final single-point power consumption estimate, which is de-normalized before being used in downstream processing. There are about 40 million parameters in this model using the default values.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pt"><img src="../Images/a827065eb4ec90784e39e61fdc9ad106.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*IDjUNwaoO_4LTOt--ZDJ_A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1 — CNN Model</figcaption></figure><p id="8cd2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the transformer model in Figure 2 for an input sequence length of 599 samples where the transformer block is a Bert-style encoder. You can view the complete code <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/transformer_model.py" rel="noopener ugc nofollow" target="_blank">here</a>. The input sequence is first passed through a convolutional layer to expand into a latent space, analogous to the feature extraction in the CNN model case. Pooling and L2 normalization reduce model complexity and mitigate the effects of outliers. Next, a Bert-style transformer lineup processes the latent space sequence, which includes positional embedding and transformer blocks that apply importance weighting. Several layers process the output of the transformer blocks. These are relative position embedding, which uses symmetric weights around the mid-point of the signal; average pooling, which reduces the sequence to a single value per feature; and then finally, dense layers that output the final single point estimated power value for the window which again is de-normalized for downstream processing. There are about 1.6 million parameters in this model using the default values.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/ff11b2c840cad9aa37a4de2d0974fe03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qw5QiGTndN0vSaTxA2mAyQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 2 — Overall Transformer Model</figcaption></figure><p id="b5f1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the Bert-style transformer encoder in Figure 3, below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/87eb93bfafca90ab758c7f03e8bcae1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdk7nHkhzA6UosSMxQMATQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 3 — Transformer Encoder.</figcaption></figure><h2 id="eaee" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">NILM Datasets</h2><p id="ac56" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Several large-scale publicly available datasets specifically designed to address the NILM problem were captured in household buildings from various countries. The datasets generally include many 10s millions of active power, reactive power, current, and voltage samples but with different sampling frequencies, which require you to pre-process the data before use. Most NILM algorithms utilize only real (active or true) power data. Five appliances are usually considered for energy disaggregation research: a kettle, microwave, fridge, dishwasher, and washing machine. These are the appliances I used for this article, and I mainly focused on the <a class="af nc" href="https://pureportal.strath.ac.uk/en/datasets/refit-electrical-load-measurements-cleaned" rel="noopener ugc nofollow" target="_blank">REFIT</a>³ dataset.</p><p id="3c1a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Note that these datasets are typically very imbalanced because, most of the time, an appliance is in the off state.</p><h2 id="802d" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">Model Training and Results</h2><p id="fe5e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">I used <a class="af nc" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">TensorFlow</a> to train and test the model. You can find the code associated with this section <a class="af nc" href="https://github.com/goruck/nilm/tree/main/ml" rel="noopener ugc nofollow" target="_blank">here</a>. I trained the seq2point learning models for the appliances individually on z-score standardized REFIT data or normalized to [0, <em class="pr">Pₘ</em>], where <em class="pr">Pₘ</em> is the maximum power consumption of an appliance in its active state. Normalized data tends to give the best model performance, so I used it by default.</p><p id="6ff5" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I used the following metrics to evaluate the model’s performance. You can view the code that calculates these metrics <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/nilm_metric.py" rel="noopener ugc nofollow" target="_blank">here</a>.</p><ul class=""><li id="ce36" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Mean absolute error (<em class="pr">MAE)</em> evaluates the absolute difference between the prediction and the ground truth power at every time point and calculates the mean value, as defined by the equation below.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/d54421cc4c4a72796ba3155fbe7bac0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*sX9zYDsdRE6Athc3aMjQ_A.jpeg"/></div></figure><ul class=""><li id="82b7" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Normalized signal aggregate error (<em class="pr">SAE</em>) indicates the total energy’s relative error. Denote <em class="pr">r</em> as the total energy consumption of the appliance and <em class="pr">rₚ</em> as the predicted total energy, then <em class="pr">SAE</em> is defined per the equation below.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/76f14926ea0f57dd887be1e9276f353c.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*oK36RXREitIDRvwgV1SwRA.jpeg"/></div></figure><ul class=""><li id="ed52" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Energy per Day (<em class="pr">EpD</em>), which measures the predicted energy used in a day, is valuable when the household users are interested in the total energy consumed in a period. Denote <em class="pr">D</em> as the total number of days and <em class="pr">e</em> as the appliance energy consumed daily; then <em class="pr">EpD</em> is defined per the equation below.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/217353af2aac56c424094738cffb3f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*hdXvKcIzEjlpa6ADEr7z5g.jpeg"/></div></figure><ul class=""><li id="7517" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Normalized disaggregation error (<em class="pr">NDE</em>) measures the normalized error of the squared difference between the prediction and the ground truth power of the appliances, as defined by the equation below.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/4f221660b7809beb73473fa8862cbefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*UsPIGyYvL5qWRuHZpCrqoA.jpeg"/></div></figure><ul class=""><li id="214b" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">I also used accuracy (<em class="pr">ACC</em>), F1-score (<em class="pr">F1</em>), and Matthew’s correlation coefficient (<em class="pr">MCC</em>) to assess if the model can perform well with the severely imbalanced datasets used to train and test the model. These metrics depend on the computed on-off status of the appliance device. <em class="pr">ACC</em> equals the number of correctly predicted time points over the test dataset. The equations below define <em class="pr">F1</em> and <em class="pr">MCC</em>, where <em class="pr">TP</em> stands for true positives, <em class="pr">TN</em> stands for true negatives, <em class="pr">FP</em> stands for false positives, and <em class="pr">FN</em> stands for false negatives.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/be831bdf057836ca2fe1d178f535d5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*L9uSapG8_oTfqqkD_3FKoQ.jpeg"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/9ca4fb594b61a298bf23cf5eacb3dce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*h7NkCx-Vkxk_9WXNG0jWRw.jpeg"/></div></figure><p id="f01b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="pr">MAE, SAE, NDE</em>, and <em class="pr">EpDₑ, </em>defined as 100% times (predicted EpD — ground truth EpD) / ground truth EpD, reflect the model’s ability to predict the appliance energy consumption levels correctly. <em class="pr">F1</em> and <em class="pr">MCC</em> indicate the model’s ability to predict appliance on-off states using imbalanced classes correctly. <em class="pr">ACC </em>is less valuable than <em class="pr">F1</em> or <em class="pr">MCC</em> in this application because, most of the time, the model will accurately predict that the appliance, which dominates the dataset, is off.</p><p id="a532" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I used a sliding window of 599 samples of the aggregate real power consumption signal as inputs to the seq2point model, and I used the midpoints of the corresponding windows of the appliances as targets. You can see the code that generates these samples and targets by an instance of the WindowGenerator Class defined in the <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/window_generator.py" rel="noopener ugc nofollow" target="_blank">window_generator.py</a> module.</p><p id="8a0d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the code I used to train the model in <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/train.py" rel="noopener ugc nofollow" target="_blank">train.py</a>, which uses the <a class="af nc" href="https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy" rel="noopener ugc nofollow" target="_blank">tf.distribute.MirroredStrategy</a> distributed training strategy. I used the Keras Adam optimizer, with early stopping to reduce over-fitting.</p><p id="3e2f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The key hyper-parameters for training and the optimizer are summarized below.</p><ul class=""><li id="a0fa" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Input Window Size: 599 samples</li><li id="df94" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Global Batch size: 1024 samples.</li><li id="7047" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Learning Rate: 1e-04</li><li id="4274" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Adam Optimizer: beta_1=0.9, beta_2=0.999, epsilon=1e-08</li><li id="63c8" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Early Stopping Criteria: 6 epochs.</li></ul><p id="5ec8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I used the loss function shown in the equation below to compute training gradients and evaluate validation loss on a per-batch basis. It combines Mean Squared Error, Binary Cross-Entropy, and Mean Absolute Error losses, averaged over distributed model replica batches.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qk"><img src="../Images/dcb377666e4a69dc97bc991bb8afa72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRigaCh0KGVlHcWrT_CQ2g.jpeg"/></div></div></figure><p id="d05e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Where x, x_hat in [0, 1] is the ground truth and predicted power usage single point values divided by the maximum power limit per appliance and s, s_ hat in {0, 1} are the appliance state label and prediction. The absolute error term is only applied for the set of predictions when either the status label is on, or the prediction is incorrect. The hyper-parameter lambda tunes the absolute loss term on a per-appliance basis.</p><p id="0641" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see typical performance metrics for the CNN model in the table below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/852a5550cd715efaab21222cffa86c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6y2evUojM2jJ4kbl5OkHMA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 1 — CNN model performance</figcaption></figure><p id="34fb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see typical performance metrics for the transformer model in the table below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/084c86e0da169b59cf89ec8851372e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51V9aZVD5ucTT4SI5NH7og.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 2 — transformer model performance</figcaption></figure><p id="95c7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see that the CNN and transformer models have similar performance even though the latter has about 26 times fewer parameters than the former. However, each transformer training step takes about seven times longer than CNN due to the transformer model’s use of self-attention, which has O(<em class="pr">n</em>²) complexity compared to the CNN model’s O(<em class="pr">n</em>), where <em class="pr">n</em> is the input sequence length. Based on training (and inference) efficiency, you can see that CNN is preferable with little loss in model performance.</p><h1 id="8f05" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Model Conversion and Quantization</h1><p id="2491" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The steps involved in converting a model graph in floating point to a form suitable for inferencing on edge hardware, including those based on CPUs, MCUs, and specialized compute optimized for int8 operations, are as follows.</p><ul class=""><li id="f4bd" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk">Train the model in float32 or representation such as TensorFloat-32 using Nvidia GPUs. The output will be a complete network graph; I used the TensorFlow SavedModel format, a complete TensorFlow program including variables and computations.</li><li id="c510" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Convert the floating-point graph to a format optimized for the edge hardware using TensorFlow Lite or equivalent. The output will be a flat file that can run on a CPU, but all operations will still be in float32. Note that you cannot convert all TensorFlow operators into a TFLite equivalent. You can convert most layers and operators used in CNN networks can be converted, but I designed the transformer network carefully to avoid TFLite conversion issues. See <a class="af nc" href="https://www.tensorflow.org/lite/guide/ops_compatibility" rel="noopener ugc nofollow" target="_blank">TensorFlow Lite and TensorFlow operator compatibility.</a></li><li id="a9b7" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk">Quantize and optimize the converted model’s weights, biases, and activations. I used various quantization modes to partially or fully quantize the model to int8, int16, or combinations thereof, resulting in different inference latencies on the target hardware.</li></ul><p id="3465" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I performed <a class="af nc" href="https://www.tensorflow.org/lite/performance/post_training_quantization" rel="noopener ugc nofollow" target="_blank">Post-training quantization</a> on the CNN and transformer models using the <a class="af nc" href="https://www.tensorflow.org/lite/models/convert/" rel="noopener ugc nofollow" target="_blank">TensorFlow Lite (TFLite) converter AP</a>I with various quantization modes to improve inference speed on edge hardware, including the Raspberry Pi and the Google Edge TPU, while managing the impact on accuracy. You can see the quantization modes I used below.</p><ul class=""><li id="a7b8" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pw px py bk"><strong class="ob fr">convert_only</strong>:<strong class="ob fr"> </strong>Convert to tflite but keep all parameters in Float32 (no quantization).</li><li id="b794" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk"><strong class="ob fr">w8</strong>: Quantize weights from float32 to int8 and biases to int64. Leave activations in Float32.</li><li id="2ad2" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk"><strong class="ob fr">w8_a8_fallback</strong>: Same as w8 but quantize activations from float32 to int8. Fallback to float if an operator does not have an integer implementation.</li><li id="4382" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk"><strong class="ob fr">w8_a8</strong>: Same as w8 but quantize activations from float32 to int8. Enforce full int8 quantization for all operators.</li><li id="ccce" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou pw px py bk"><strong class="ob fr">w8_a16</strong>: Same as w8 but quantize activations to int16.</li></ul><p id="437f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The CNN model was quantized using all modes to understand the best tradeoff between latency and accuracy. Only the weights for the transformer model were quantized to int8 using mode w8; the activations needed to be kept in float32 to maintain acceptable accuracy. See <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/convert_keras_to_tflite.py" rel="noopener ugc nofollow" target="_blank">convert_keras_to_tflite.py</a> for the code that does this quantization, which also uses <a class="af nc" href="https://www.tensorflow.org/lite/performance/quantization_debugger" rel="noopener ugc nofollow" target="_blank">TensorFlow Lite’s quantization debugger</a> to check how well each layer in the model was quantized. I profiled the converted models using the <a class="af nc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark" rel="noopener ugc nofollow" target="_blank">TensorFlow Lite Model Benchmark Tool</a> to quantify inference latencies.</p><p id="37d5" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Fully quantizing a model requires calibration of the model’s activations via a dataset that is representative of the actual data used during training and testing of the floating point model. Calibration can be challenging with highly imbalanced data because a random selection of samples will likely lead to poor calibration and quantized model accuracy. To mitigate this, I used an algorithm to construct a representative dataset of the balanced appliance on- and off-states. You can find that code <a class="af nc" href="https://github.com/goruck/nilm/blob/main/ml/convert_model.py#L91" rel="noopener ugc nofollow" target="_blank">here</a> and in the snippet below.</p><figure class="mm mn mo mp mq mr"><div class="qn io l ed"><div class="qo qp l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 4 — Representative Generator Code Snippet</figcaption></figure><p id="44b4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can find the quantized inference results in the tables below, where Lx86 is the average inference latency on a 3.8 GHz x86 machine using eight TFlite interpreter threads, and Larm is the average inference latency on the ARM aarch-64-based Raspberry Pi 4 using four threads with both computers using the TensorFlow Lite <a class="af nc" href="https://github.com/google/XNNPACK" rel="noopener ugc nofollow" target="_blank">XNNPACK</a> CPU delegate. Ltpu is the average inference latency on the Google Coral Edge TPU. I kept the model inputs and outputs in float32 to maximize inference speed for the x86- and ARM-based machines. I set them to int8 for the edge TPU.</p><h2 id="2b28" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">CNN Model Results and Discussion</h2><p id="1f4d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">You can see the quantized results for the CNN models in the table below for quantization mode w8.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/08baf028bae667e2b026d41f53d38724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8k-2f7hFr7zGhuqP8YbICw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 4 — Quantized CNN Models for Mode w8</figcaption></figure><p id="549e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The quantized results for the CNN kettle model are shown below for the other quantization modes. You can see that latency on the edge TPU is much longer than other machines. Because of this, I focused my analysis on the x86 and ARM architectures.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/aefe2d6b5411a02c5049607cd7fbc6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61CsV8CQ2vVXfFtV8NLzbA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 5 — Quantized CNN Kettle Model for Other Modes</figcaption></figure><p id="3efb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Results for the other appliance models are omitted for brevity but show similar characteristics as a function of quantization mode.</p><p id="f2d7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the negative impact of activation quantization, but because of regularization effects, weight quantization has a moderate benefit on some model performance metrics. As expected, the full quantization modes lead to the lowest latencies. Quantizing activations to int16 by the w8_a16 mode results in the highest latencies because only non-optimized reference kernel implementations are presently available in TensorFlow Lite, but this scheme leads to the best model metrics given the regularization benefits from weight quantization and better preservation of activation numerics.</p><p id="a7f3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can also see that inference latency of the modes follows w8 &gt; convert_only &gt; w8_a8 for the x86 machine but convert_only &gt; w8 &gt; w8_a8 for the aarch64 machine, although the variation is more significant for x86. To understand this better, I profiled the converted models using the <a class="af nc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark%29" rel="noopener ugc nofollow" target="_blank">TFLite Model Benchmark Tool</a>. A summary of the profiling results for the CNN microwave model, which represents the other models, is shown below.</p><ol class=""><li id="dc17" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou qs px py bk">Model Profiling on x86 (slowest to fastest)</li></ol><p id="782e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see that the Fully Connected and Convolution operations are taking the longest to execute in all cases but are much faster in the fully quantized mode of w8_a8.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/454a66c12d5e6a88e1f4adbd7f18e8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d7Zwzx1yqkn0aYk6hiQmGg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 6 — CNN x86 Model Profiling for w8 Mode</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/c1a741b8e8eaf544abb2e94b3d4dcedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dzg1vW014ro-7JTGkugT-g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 7 — CNN x86 Model Profiling for convert_only Mode</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/1abeb28548f04843a2813f078bf5055c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a8PTW-hon5SAiaoI4ThUtQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 8 — CNN x86 Model Profiling for w8_a8 Mode</figcaption></figure><p id="5057" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">2. Model Profiling on aarch64 (slowest to fastest)</p><p id="0594" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The copy and Max Pooling operations are slower on x86 than on aarch64, probably due to memory bandwidth and micro-architecture differences.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/ef4254f43dbb042f9bb4385a15c80162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zd9Tq66koEoOold9F_CMHg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 9 — CNN aarch64 Model Profiling for convert_only Mode</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/8dbda2f2b7d89c47ba275bd718e2230a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nSm5gppaQz2h6jAJ5qsRQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 10 — CNN aarch64 Model Profiling for w8 Mode</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/a5dce1d80afb32bbeb6a126eaa9de516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d310d_l8rwJlFM7ybLBSkw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 11 — CNN aarch64 Model Profiling for w8_a8 Mode</figcaption></figure><p id="6e36" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">3. Quantization Efficacy</p><p id="58fb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The metric RMSE / scale is close to 1 / sqrt(12) (~ 0.289) when the quantized distribution is similar to the original float distribution, indicating a well-quantized model. The larger the value, the more likely the layer will not be quantized well. The tables below show the RMSE / Scale metric for the CNN kettle model and the Suspected? Column indicates a layer that significantly exceeds 0.289. Other models are omitted for brevity but show similar results. These layers can remain in float to generate a selectively quantized model that increases accuracy at the expense of inference performance, but doing so for the CNN models did not materially improve accuracy. See <a class="af nc" href="https://www.tensorflow.org/lite/performance/quantization_debugger" rel="noopener ugc nofollow" target="_blank">Inspecting Quantization Errors with Quantization Debugger</a>.</p><p id="e89e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can find layer quantization efficacy metrics for the CNN kettle model using mode w8_a8 below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/947f518302538d62fd6217b55dd5acde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UgSC3l5g9R7QjT1XOTHFAQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 12 — Layer quantization efficacy metrics for the CNN kettle model using mode w8_a8</figcaption></figure><p id="8619" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">4. Model Memory Footprint</p><p id="6adb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I used the <a class="af nc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark" rel="noopener ugc nofollow" target="_blank">TFLite Model Benchmark Tool</a> to get the approximate RAM consumption of the TFLite CNN microwave model at runtime, shown in the table below for each quantization mode, and the TFLite model disk space. The other CNN models show similar characteristics. The findings for the x86 architecture were identical to the arm architecture. Note that the Keras model consumes about 42.49 (MB) on disk. You can see that there is about a four times reduction in disk storage space due to the float32 to int8 weight conversions.</p><p id="43ea" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Interestingly, RAM runtime usage varies considerably due to the TFLite algorithms that optimize intermediate tensor usage. These are pre-allocated to reduce inference latency at the cost of memory space. See <a class="af nc" href="https://blog.tensorflow.org/2020/10/optimizing-tensorflow-lite-runtime.html" rel="noopener ugc nofollow" target="_blank">Optimizing TensorFlow Lite Runtime Memory</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ra"><img src="../Images/e8b191f426bc6728b7bc518f73a703e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*vWHmsKiEhXSiUYpZaAqzfw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 13 — CNN Model Memory Usage</figcaption></figure><h2 id="e040" class="pa ne fq bf nf pb pc pd ni pe pf pg nl oi ph pi pj om pk pl pm oq pn po pp pq bk">Transformer Model Results and Discussion</h2><p id="5b66" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Even though I enabled the XNNPACK delegate during the transformer model inference evaluation, nothing was accelerated because the transformer model contains dynamic tensors. I encountered the following warning when using the TFLite interpreter for inference:</p><blockquote class="rb rc rd"><p id="7977" class="nz oa pr ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors (tensor#94 is a dynamic-sized tensor).</p></blockquote><p id="455f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This warning means that all operators are unsupported by XNNPACK and will fall back to the default CPU kernel implementations. A future effort will involve refactoring the transformer model to use only static-size tensors. Note that a tensor could be marked dynamic when the TFLite runtime encounters a control-flow operation (e.g., if, while). In other words, even when the model graph doesn’t have any tensors of dynamic shapes, a model could have dynamic tensors at runtime. The current transformer model uses `if` control-flow operations.</p><p id="f8c6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the quantized results for the transformer model in the table below for quantization mode w8.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk re"><img src="../Images/9cd2b8c6568fcbc74589cb8cf0da2703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oSFSGr3O6uv4lAEtoBoerg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 14 — Quantized results for the transformer model for quantization mode w8</figcaption></figure><p id="9a4e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The quantized results for the transformer kettle and microwave models are shown in the table below for quantization mode convert_only.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rf"><img src="../Images/73188628808bc9f0b3f9cacb73817d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhH23fMUnlCO2VjM8cSWuw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 15 — Quantized results for the transformer kettle and microwave models for quantization mode convert_only</figcaption></figure><ol class=""><li id="8b77" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou qs px py bk">Model Profiling on x86 (slowest to fastest)</li></ol><p id="f154" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The FULLY_CONNECTED layers dominate the compute in w8 mode but less in convert_only mode. This behavior is probably due to x86 memory micro-architecture handling of int8 weights.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/c035ef48d43deca205ee762eb668c87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASlttVtm9QBBq2vg62X5_A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 16 — x86 transformer Model Profiling for Mode w8</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rg"><img src="../Images/cd0b839943f74f225583ded3eff9e304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCroyzwEIuIUhMkVo3DvTQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 17 — x86 transformer Model Profiling for Mode convert_only</figcaption></figure><p id="24ca" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">2. Model Profiling on aarch64 (slowest to fastest)</p><p id="2f6b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can see the arm architecture seems to be more efficient in computing the FULLY_CONNECTED layers in w8 mode than in the x86 case.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rh"><img src="../Images/21ab6a0cd821cc2ed517d7303be5898c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXJPcdh7viXF2EY7TlNLzQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 18 — aarch64 transformer Model Profiling for Mode convert_only</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rh"><img src="../Images/ea96ef58f5deba7021a479a4d562b7cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3OHFt5Z_SEpET6p4srpUg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 19 — aarch64 transformer Model Profiling for Mode w8</figcaption></figure><p id="0d03" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">3. Quantization Efficacy</p><p id="84fa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can find layer quantization efficacy metrics for the transformer kettle model using mode w8_a8 <a class="af nc" href="https://github.com/goruck/nilm?tab=readme-ov-file#quantization-efficacy-1" rel="noopener ugc nofollow" target="_blank">here</a>, although, as noted above, quantizing the transformer model’s activations results in inferior model performance. You can see that the RSQRT operator, in particular, does not quantize well; these operators are used in the Gaussian error linear activation functions, which helps explain the model’s poor performance. The other transformer appliance models show similar efficacy metrics.</p><p id="0419" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">4. Model Memory Footprint</p><p id="bbda" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Identical to the CNN case, I used the <a class="af nc" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark" rel="noopener ugc nofollow" target="_blank">TFLite Model Benchmark Tool</a> to get the approximate RAM consumption of the TFLite microwave model at runtime, shown in the table below for each relevant quantization mode and the TFLite model disk space. The other transformer models show similar characteristics. Note that the Keras model consumes about 6.02 (MB) on disk. You can see that there is about a three-times reduction in model size due to the weights being quantized from float32 to int8, which is less than the four-times reduction seen in the CNN case, likely because there are fewer layers with weights. You can also see that the x86 TFLite runtime is more memory efficient than its aarch64 counterpart for this model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ri"><img src="../Images/726685934870e28711b0d54476cad036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIcdrcZoIt9gVq3IHrUcyA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 20 — transformer Model Disk and RAM Usage</figcaption></figure><h1 id="2649" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusions</h1><p id="e6b1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">You can effectively develop and deploy models using TensorFlow and TensorFlow Lite at the edge. TensorFlow Lite offers tools useful in production to understand and modify the behavior of your models, including layer quantization inspection and runtime profiling.</p><p id="8072" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There is better support for the operators used in CNN-based models than the typical operators used in transformer-based models. You should carefully choose how to design your networks with these constraints and run a complete end-to-end training-conversion-quantization cycle before going too far in developing and training your models.</p><p id="3eaf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Post-training quantization works well to quantize CNN networks fully, but I could only quantize the transformer network weights to maintain acceptable performance. The transformer network should be trained using <a class="af nc" href="https://www.tensorflow.org/model_optimization/guide/quantization/training" rel="noopener ugc nofollow" target="_blank">Quantization-aware methods</a> for better integer performance.</p><p id="f1b0" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The CNN models used to solve the NILM problem in this article are many times larger than their transformer counterparts but train much faster and have lower latency due to linear complexity. The CNN models are a better solution if disk space and RAM are not your chief constraints.</p><h1 id="68be" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><ol class=""><li id="a941" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou qs px py bk"><a class="af nc" href="https://arxiv.org/abs/1902.08835" rel="noopener ugc nofollow" target="_blank">arXiv:1902.08835</a> | Transfer Learning for Non-Intrusive Load Monitoring by Michele D’Incecco, Stefano Squartini and Mingjun Zhong.</li><li id="99e9" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou qs px py bk"><a class="af nc" href="https://github.com/Yueeeeeeee/BERT4NILM" rel="noopener ugc nofollow" target="_blank">BERT4NILM</a>: A Bidirectional Transformer Model for Non-Intrusive Load Monitoring by Zhenrui Yue, et. al.</li><li id="339d" class="nz oa fq ob b go qf od oe gr qg og oh oi qh ok ol om qi oo op oq qj os ot ou qs px py bk">Available under the <a class="af nc" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution 4.0 International Public License</a>.</li></ol></div></div></div></div>    
</body>
</html>