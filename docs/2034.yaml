- en: What Do Large Language Models “Understand”?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型究竟“理解”什么？
- en: 原文：[https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77?source=collection_archive---------1-----------------------#2024-08-21](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77?source=collection_archive---------1-----------------------#2024-08-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77?source=collection_archive---------1-----------------------#2024-08-21](https://towardsdatascience.com/what-do-large-language-models-understand-befdb4411b77?source=collection_archive---------1-----------------------#2024-08-21)
- en: A deep dive on the meaning of understanding and how it applies to Large Language
    Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨理解的含义以及它如何应用于大型语言模型
- en: '[](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)[![Tarik
    Dzekman](../Images/0c66b22ecbdbbce79b2516e555c67432.png)](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)
    [Tarik Dzekman](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)[![Tarik
    Dzekman](../Images/0c66b22ecbdbbce79b2516e555c67432.png)](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)
    [Tarik Dzekman](https://medium.com/@TarikDzekman?source=post_page---byline--befdb4411b77--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)
    ·24 min read·Aug 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--befdb4411b77--------------------------------)
    ·24分钟阅读·2024年8月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/97efcc4e31db2af99b0c4a7a74bad7fe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97efcc4e31db2af99b0c4a7a74bad7fe.png)'
- en: 'Source: Image by the author with elements generated with Stable Diffusion'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图片由作者提供，元素通过 Stable Diffusion 生成
- en: It’s hard to believe that ChatGPT is almost 2 years old. That’s significant
    to me because ChatGPT is only 1 month younger than my daughter. Just yesterday
    she successfully put a star shaped block into a star shaped hole, told me about
    how “yesa-day” she was sick and “bomited”, and told me she wanted to call her
    nanna on the phone. What has ChatGPT learned in those 2 years? It hasn’t learned
    to act in the real world, it can’t remember things that happened to it, and it
    doesn’t have desires or goals. Granted, with the right prompt it could output
    text that convincingly follows an instruction to express goals. But is that really
    the same thing? No. The answer is No.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 很难相信 ChatGPT 已经快 2 岁了。对我来说，这个时间点非常特别，因为 ChatGPT 比我女儿小仅一个月。就在昨天，她成功地把一个星形积木放进了一个星形的孔里，告诉我她“昨天”生病了，还“呕吐”了，并告诉我她想给她的外婆打电话。那
    ChatGPT 在这 2 年里学到了什么呢？它没有学会在现实世界中行动，它无法记住曾经发生的事情，也没有欲望或目标。诚然，在正确的提示下，它可以输出看起来符合某种目标的文本。但这真的是同一回事吗？不是的。答案是否定的。
- en: 'Large Language Models (LLMs) like ChatGPT possess capabilities far beyond what
    my daughter will ever achieve. She won’t be able to communicate coherently in
    a wide range of languages, read as many books as exist in an LLM’s training data,
    or generate text as quickly. When we attribute human-like abilities to LLMs, we
    fall into an anthropomorphic bias by likening their capabilities to our own. But
    are we also showing an anthropocentric bias by failing to recognize the capabilities
    that LLMs consistently demonstrate? Let’s review the scorecard so far:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 类似 ChatGPT 这样的大型语言模型（LLM）拥有远超我女儿能力的能力。她无法在广泛的语言范围内进行连贯的交流，无法阅读像 LLM 训练数据中那么多的书籍，也不能像
    LLM 那样快速生成文本。当我们把类人能力赋予 LLM 时，我们陷入了一种拟人化的偏见，把它们的能力与我们自己的能力类比。但我们是否也表现出了以人为中心的偏见，没有意识到
    LLM 一直展示出的能力呢？让我们来回顾一下到目前为止的得分：
- en: It’s true that an LLM doesn’t have memory — although we can simulate one by
    having it summarise past conversations and including that information in a prompt.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确实，LLM 并没有记忆——尽管我们可以通过让它总结过去的对话并将这些信息纳入提示中来模拟记忆。
- en: LLMs don’t have intrinsic goals — although they can be prompted to generate
    text that sounds convincingly goal-oriented.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 没有内在的目标——尽管它们可以通过提示生成听起来具有明确目标的文本。
- en: LLMs can’t act in the physical world — though someone could probably create
    a prompt to showcase this.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM无法在物理世界中行动——尽管有人可能会创建一个提示来展示这一点。
- en: While they perform impressive feats, they still lack some basic abilities that
    my 21-month-old daughter has. We can mimic some of those abilities with the right
    prompts and tooling. In generating coherent text responding to such prompts, LLMs
    consistently demonstrate an apparent ability to understand what we want. But to
    what extent do LLMs truly “understand”?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们能执行令人印象深刻的壮举，但仍然缺乏我21个月大女儿具备的一些基本能力。我们可以通过合适的提示和工具模仿这些能力。在生成连贯的文本回应这些提示时，大型语言模型（LLM）始终展示出明显的能力，能够理解我们想要的内容。但LLM究竟在多大程度上真正“理解”呢？
- en: How LLMs Work
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型如何工作
- en: '![](../Images/32289cba7ec5f504ccc657e4b76e0b02.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32289cba7ec5f504ccc657e4b76e0b02.png)'
- en: 'A hypothetical attention map for the incomplete sentence: “Using context to
    predict what’s most likely to come [MASK]”. Source: Image by the author'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个假设的注意力图，用于表示不完整的句子：“利用上下文预测最可能接下来出现的[掩码]”。来源：作者提供的图片
- en: 'I am talking about a very specific type of LLM: transformer-based auto-regressive
    large language models. I won’t go into the specifics when many detailed articles
    have been written explaining transformers with varying levels of complexity. Instead
    let’s focus on the core of what an LLM does: they are statistical models that
    predict the likelihood of a token appearing in a piece of text given some context.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我说的是一种非常特定类型的大型语言模型：基于变换器（transformer）的自回归大型语言模型。我不会深入探讨细节，因为已经有许多详细的文章解释了变换器模型的不同复杂度。相反，让我们聚焦于LLM的核心功能：它们是统计模型，能够根据一些上下文预测某个标记出现在文本中的可能性。
- en: Now imagine I created a complex weather model* where patches of the Earth’s
    atmosphere become ‘tokens.’ Each token has attributes like humidity, temperature,
    and air pressure. I use the model to forecast these attributes over time-steps.
    If the time-steps get shorter and the patches become smaller the model is closer
    and closer to representing the state of the actual world. This model attempts
    to capture something about the likelihood of the weather we’ll see next, given
    the weather we’ve seen before. It may learn to very accurately predict, for example,
    the emergence of cyclones over time in areas where air is warm, moist, and of
    low pressure. But it’s not a simulation of the *physics* of Earth’s weather any
    more than an LLM is a simulation of brain activity.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，我创建了一个复杂的天气模型*，其中地球大气层的各个区域被视为“标记”。每个标记都有诸如湿度、温度和气压等属性。我使用该模型预测这些属性在不同时间步长上的变化。如果时间步长变短且区域变小，则该模型就越接近实际世界的状态。这种模型试图捕捉我们将要看到的天气的可能性，前提是基于我们之前所见的天气。例如，它可能会学会非常准确地预测，在温暖、湿润且气压低的区域，气旋随时间的出现。但它并不是对地球天气*物理学*的模拟，就像大型语言模型（LLM）并不是对大脑活动的模拟。
- en: If an LLM is a statistical model of text, what exactly is it modelling? My imagined
    weather prediction model tries to capture the statistics of the atmospheric conditions
    that generate the weather. But what is the statistical process that generates
    text? The process that generates text is a human brain and humans need some understanding
    of the world to generate that text. If a model can effectively predict text a
    human might write then could that prediction come with “understanding”?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM是文本的统计模型，那么它到底在模拟什么呢？我构想的天气预测模型试图捕捉生成天气的气象条件的统计数据。但生成文本的统计过程是什么呢？生成文本的过程是人类大脑，而人类需要对世界有所理解才能生成这些文本。如果一个模型能够有效地预测一个人类可能写出的文本，那么这种预测是否也带有“理解”呢？
- en: How LLMs are trained
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型如何训练
- en: LLMs are trained to optimize an objective that reduces the surprise of encountering
    a specific token given its context. If the model encounters a token in the training
    data and assigns it low probability, the model’s weights are adjusted to give
    it a higher probability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的训练目标是优化一个减少遇到特定标记时的惊讶感的目标函数。如果模型在训练数据中遇到某个标记并给出低概率，它的权重就会被调整，从而赋予该标记更高的概率。
- en: 'Compare that to how my daughter learns to use language. When she wants something,
    she uses language to communicate her desires. First, she understands what she
    wants on some level. Then, she must understand which words to use to get what
    she wants. Recently, she wanted me to fill her juice bottle but didn’t want me
    to take it away or walk away from her to bring more juice. Though her wants were
    contradictory and a bit irrational, she had several goals: (1) more juice, (2)
    keep the juice bottle near her, (3) daddy stays near her too. And let me tell
    you, she communicated this *very* effectively. Her language learning is directly
    tied to her understanding of how those words can get her what she wants (even
    if what she wants is irrational).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我女儿学习语言的方式相比如何呢？当她想要某样东西时，她会用语言来表达她的需求。首先，她在某种程度上理解她想要的东西。然后，她必须理解使用哪些词语才能得到她想要的东西。最近，她希望我为她填满果汁瓶，但不希望我拿走瓶子或走开去拿更多的果汁。尽管她的需求是矛盾且有些不合理的，但她有几个目标：（1）更多果汁，（2）把果汁瓶放在她身边，（3）爸爸也待在她身边。让我告诉你，她*非常*有效地传达了这一点。她的语言学习直接与她理解这些词语如何帮助她得到她想要的东西（即便这些需求有些不合理）紧密相连。
- en: If an LLM were to exhibit understanding, it would be an emergent attribute of
    its statistical model of the world. The paper “Climbing Towards NLU” (Bender &
    Koller, 2020) argues that true natural language understanding (NLU) requires grounding
    in the real world. Bender & Koller argue that LLMs trained exclusively on statistical
    patterns in textual data lack real-world context or interactions to achieve actual
    understanding. This means that, unlike my daughter, an LLM *can’t* understand
    something because its communication is not grounded in the real world.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果大型语言模型（LLM）能够表现出理解，那将是其世界统计模型的一种突现属性。论文《迈向自然语言理解》（Bender & Koller，2020年）认为，真正的自然语言理解（NLU）需要扎根于现实世界。Bender和Koller认为，单纯依靠文本数据中统计模式训练的LLM缺乏现实世界的背景或交互，因此无法实现真正的理解。这意味着，与我的女儿不同，LLM*不能*理解某些东西，因为它的交流并没有扎根于现实世界。
- en: What is Understanding?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是理解？
- en: The Wikipedia page on [understanding](https://en.wikipedia.org/wiki/Understanding)
    describes it as a cognitive process involving the use of concepts to model an
    object, situation, or message. It implies abilities and dispositions sufficient
    to support intelligent behaviour. Ludwig Wittgenstein suggested that understanding
    is context-dependent and is shown through intelligent behaviour rather than mere
    possession of knowledge. This is reminiscent of the grounding requirement posited
    by Bender & Koller.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科关于[理解](https://en.wikipedia.org/wiki/Understanding)的页面将其描述为一种涉及使用概念来建模对象、情境或信息的认知过程。它意味着具有足够的能力和倾向来支持智能行为。路德维希·维特根斯坦认为，理解是依赖于上下文的，并通过智能行为而非仅仅拥有知识来表现出来。这让人想起了Bender和Koller提出的基础要求。
- en: On the one hand understanding needs an accurate model of the world. On the other
    hand people contend that one needs to use this model to act in the world in order
    to actually understand. I would argue that we simply analyse someone’s behaviour
    only as a proxy for measuring that underlying world model. If we could measure
    the world model directly we wouldn’t need to see demonstrations of understanding.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，理解需要一个准确的世界模型；另一方面，人们认为，必须使用这个模型在世界中采取行动，才能真正理解。我认为，我们只不过是通过分析某人的行为来作为衡量其基础世界模型的代理。如果我们能够直接测量世界模型，我们就不需要看到理解的示范了。
- en: The Limitations of Understanding
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解的局限性
- en: Philosopher John Searle’s “Chinese Room” experiment challenges our concept of
    understanding (Searle, 1980). Imagine a room filled with detailed instructions
    on how to respond to someone writing in Chinese. Notes written in Chinese are
    slid under the door, and the person inside the room can look up the symbols and
    follow a recipe for writing a reply. The person in the room doesn’t know Chinese
    but can have a convincing conversation with a person outside. Clearly, the person
    who constructed the room “understands” Chinese, but someone on the outside isn’t
    conversing with that person; they’re conversing with *the room*. Does the room
    understand Chinese?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 哲学家约翰·塞尔的“中文房间”实验挑战了我们对理解的概念（塞尔，1980年）。想象一个房间，里面有详细的指示，告诉人们如何回应用中文书写的内容。用中文写的便条被从门底下滑入房间，房间里的人可以查找符号并按照指引写出回复。房间里的人不懂中文，但可以和外面的人进行令人信服的对话。显然，构建这个房间的人“理解”中文，但外面的人并不是在和那个构建者对话，而是在和*房间*对话。那么，这个房间理解中文吗？
- en: This is strongly analogous to how LLMs work and challenges our philosophical
    perception of understanding. It’s challenging precisely because we intuitively
    balk at the idea that a room could understand something. What would it even mean?
    If understanding is an emergent phenomenon that happens at the level of information
    processing systems then why can’t we say that rooms can understand things? Part
    of the issue is that, for us, understanding comes with a subjective conscious
    experience of understanding. But it’s easy to see that this experience can be
    deceiving.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这与大型语言模型（LLMs）的工作方式非常相似，并且挑战了我们对理解的哲学认知。这之所以具有挑战性，正因为我们本能地对“房间能够理解某事”这一想法感到反感。那到底意味着什么呢？如果理解是一个在信息处理系统层面上出现的涌现现象，那么为什么我们不能说房间也能理解事物呢？问题的一部分在于，对于我们来说，理解伴随着一种主观的意识体验。但很容易看出，这种体验可能是具有欺骗性的。
- en: Understanding Need Not Be Binary
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解不必是二元的
- en: You know that 7+7=14, but do you understand it? If I asked you some probing
    questions, you might realize that you don’t truly understand what that equation
    means in all contexts. For example, is 7+7=14 an unequivocal fact about the universe?
    Not necessarily. 7 apples plus 7 pears mean you have 7 apples and 7 pears. Perhaps
    in some contexts, you would count 14 pieces of fruit, but is it always true that
    you can combine two sets of different items? Or consider that 7pm + 7hours is
    2am (i.e. 7+7=2 mod 12). Are you able to give me a robust definition of why 7+7=14
    that could explain when it’s true and why^? Most people probably couldn’t do this
    off the top of their head, yet we’d feel comfortable saying that most people understand
    that 7+7=14\. The question isn’t always whether something was understood but the
    extent to which it was understood.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道7+7=14，但你理解这个等式吗？如果我问你一些深入的问题，你可能会意识到你并没有真正理解这个等式在所有情境下的含义。例如，7+7=14是宇宙中的不容置疑的事实吗？未必。7个苹果加7个梨意味着你有7个苹果和7个梨。也许在某些情境下，你会数出14个水果，但是否总是可以将两组不同的物品合并呢？再考虑一下，7点晚上加7小时是2点早上（即7+7=2
    mod 12）。你能给我一个关于7+7=14的有力定义吗？这个定义可以解释何时它成立，为什么^？大多数人可能无法立即给出答案，但我们通常会觉得大多数人都理解7+7=14。问题不总是某件事是否被理解，而是理解的程度。
- en: 'If we take Wittgenstein’s requirement that understanding is demonstrated by
    behaviour then there would be a simple test: if I tell you to arrive 7 hours after
    7pm, do you know to show up at 2am? I would argue that is evidence of *some* understanding
    but not necessarily of the depth of your understanding.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们接受维特根斯坦的要求，即理解通过行为表现出来，那么就会有一个简单的测试：如果我告诉你在晚上7点后7小时到达，你知道要在凌晨2点出现吗？我认为这可以作为*某种*理解的证据，但不一定能证明你理解的深度。
- en: Measuring Understanding in Animals
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量动物的理解力
- en: Measuring ‘understanding’ is not straightforward. In psychology [psychometric
    testing](https://en.m.wikipedia.org/wiki/Psychometrics) is the primary way we
    measure understanding in humans. It’s not straightforward to apply the same techniques
    to non human animals and is a field of study called [Biosemiotics](https://en.m.wikipedia.org/wiki/Biosemiotics).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 测量“理解力”并非易事。在心理学中，[心理测量](https://en.m.wikipedia.org/wiki/Psychometrics)是衡量人类理解力的主要方式。将相同的技术应用于非人类动物并不简单，这是一门名为[生物符号学](https://en.m.wikipedia.org/wiki/Biosemiotics)的研究领域。
- en: Understanding in animals is measured through various problem-solving tasks.
    For example, primates, dolphins, and birds (mostly corvids) demonstrate problem-solving
    skills and sophisticated tools use, suggesting some understanding of their environments
    (Emery & Clayton, 2004). Understanding is not exclusively for humans and we can
    measures levels of understanding in non-humans too.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 动物的理解力通常通过各种解决问题的任务来衡量。例如，灵长类动物、海豚和鸟类（主要是鸦科鸟类）展示了解决问题的技能和复杂的工具使用，表明它们在某种程度上理解它们的环境（Emery
    & Clayton, 2004）。理解不仅仅是人类特有的，我们也可以衡量非人类动物的理解力。
- en: 'The book “Inside of a Dog: What Dogs See, Smell, and Know” by Alexandra Horowitz
    is a fascinating exploration of how we might understand the mind and experiences
    of our closest animal companions: domesticated dogs. She describes two experiments
    that look at imitation behaviour and what a human infant vs a dog understands.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 亚历山德拉·霍洛维茨（Alexandra Horowitz）的《狗的内心：狗看见、闻到和知道什么》一书，是一本深入探讨我们如何理解我们最亲密的动物伴侣——家犬的心智与体验的迷人作品。她描述了两项实验，考察了模仿行为以及人类婴儿和狗各自的理解力。
- en: (1) If an infant sees someone flipping a light switch with their head they may
    imitate this behaviour. If the person is holding something in their hands the
    baby understands there was a reason they didn’t use their hands. When the baby
    imitates this behaviour they will use their hands. (2) by contrast, dogs would
    prefer to press a button with their nose rather than their paw. If a dog sees
    another dog press a button with their paw to get a treat, then they will imitate
    this behaviour. But if the dog sees that the other dog couldn’t use its nose because
    it had a large object in its mouth then it will understand the button needs to
    be pressed but that using paws is optional.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 如果一个婴儿看到有人用头翻动灯光开关，他们可能会模仿这种行为。如果那个人手里拿着东西，婴儿会明白他们没有用手的原因。当婴儿模仿这种行为时，他们会使用自己的手。
    (2) 相反，狗更愿意用鼻子按按钮，而不是用爪子。如果一只狗看到另一只狗用爪子按按钮来获得奖励，它也会模仿这种行为。但如果狗看到另一只狗无法用鼻子按按钮，因为它嘴里叼着一个大物体，它就会明白按钮是需要按的，但使用爪子是可选的。
- en: '![](../Images/61a0f2f21b981d289d29abfd94f307e6.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61a0f2f21b981d289d29abfd94f307e6.png)'
- en: 'Source: Image generated by the author with Ideogram'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：由作者用Ideogram生成的图片
- en: Constructing an experiment to determine what a dog understands required an understanding
    of the dog and its behaviour. Do we have that same level of understanding of LLMs
    to conduct similar experiments?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 设计实验以确定狗能理解什么，要求我们了解狗及其行为。我们是否具备相同程度的理解来对LLM进行类似的实验？
- en: Measuring Understanding in LLMs
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量LLM的理解能力
- en: The GPT-3 Era
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3时代
- en: 'A comprehensive survey on LLM capabilities (Chang & Bergen, 2023) provides
    an excellent summary from a wide range of articles – however the most advanced
    model covered is only GPT-3\. They breakdown understanding into two main categories:
    syntactic and semantic understanding. In their survey they highlight that even
    in the context of syntactic understanding LLMs have limitations. For example:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LLM能力的综合调查（Chang & Bergen，2023）提供了从多个文章中总结的优秀概述——然而，调查中涵盖的最先进模型仅为GPT-3。他们将理解分为两个主要类别：句法理解和语义理解。在他们的调查中，他们强调即使在句法理解的背景下，LLM也存在局限性。例如：
- en: Subject-verb agreement performance in language models is also dependent on the
    specific nouns and verbs involved (Yu et al. 2020; Chaves & Richter 2021). Masked
    and autoregressive models produce over 40% more accurate agreement predictions
    for verbs that are already probable from context (Newman et al. 2021), and agreement
    accuracy is worse overall for infrequent verbs (Wei et al. 2021). For infrequent
    verbs, masked language models are biased towards the more frequent verb form seen
    during pretraining (e.g., singular vs. plural) (Wei et al. 2021). Error rates
    exceed 30% for infrequent verbs in nonce (grammatically correct but semantically
    meaningless) sentences (Wei et al. 2021), with further degradations if there is
    an intervening clause between the subject and verb as in Example 4 (Lasri, Lenci,
    and Poibeau 2022a).
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 语言模型中的主谓一致性表现也依赖于特定的名词和动词（Yu等，2020；Chaves & Richter，2021）。掩码和自回归模型对于在上下文中已经比较可能的动词（Newman等，2021）预测主谓一致性更为准确，且对于不常见动词的准确度总体较差（Wei等，2021）。对于不常见动词，掩码语言模型倾向于偏向在预训练中看到的更常见的动词形式（例如，单数与复数）（Wei等，2021）。在非单句（语法正确但语义上没有意义）句子中，不常见动词的错误率超过30%（Wei等，2021），如果主语和动词之间有插入子句（如示例4中所示），错误率会进一步增加（Lasri、Lenci和Poibeau，2022a）。
- en: LLM limitations are not limited to syntactic issues (where they are arguably
    strongest) but also with semantics. For example, they note research which shows
    negations (“Please produce a possible incorrect answer to the question”) can degrade
    LLM performance by 50%.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的局限性不仅仅体现在句法问题上（在这一点上它们可以说是最强的），也体现在语义上。例如，他们指出了研究表明，否定句（“请给出一个可能错误的答案”）会使LLM的表现下降50%的现象。
- en: 'Chang & Bergen describe many other limitations of LLMs in reasoning capability,
    including:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Chang & Bergen描述了LLM在推理能力上的许多其他局限性，包括：
- en: “Brittle” responses when reasoning about a situation because the responses are
    highly sensitive to wording
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理某个情境时出现“脆弱”回应，因为回答对措辞非常敏感
- en: Struggling with analogies as they become more abstract
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 难以理解变得更加抽象的类比
- en: A lack of sensitivity to people’s perspective and mental states
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏对人们视角和心理状态的敏感度
- en: A lack of common sense
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏常识
- en: A tendency to repeat memorised text instead of reasoning
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于重复记忆的文本，而非进行推理
- en: The general approach to evaluating understanding in LLMs seems to be to phrase
    questions in different ways and find the failure modes of the models. Then these
    failure modes indicate that no real “understanding” is happening but rather just
    pattern matching.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs理解能力的普遍方法似乎是以不同方式提出问题，并找出模型的失败模式。这些失败模式表明并没有真正的“理解”发生，而只是简单的模式匹配。
- en: The ChatGPT Era
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT时代
- en: A lot has changed since GPT-3 — namely the capabilities of even larger models
    tuned for instruction following and conversation. How do LLMs stack up in 2024?
    A big difference is the proliferation of benchmarks that evaluate LLMs. A March
    2024 survey (Chang et al. 2024) covers performance of recent models on a wide
    range of benchmarks. They conclude that LLMs have strong abilities including comprehension
    and reasoning, but they still identify limitations. These limitations mean that
    LLMs have “limited abilities on abstract reasoning and are prone to confusion
    or errors in complex contexts”. Multimodal Large Language Models (MLLMs) have
    also emerged which unify (at minimum) an understanding of text and images. A January
    2024 survey (Wang et al.) covers a wide range of multimodal benchmarks and shows
    mediocre performance even for the most powerful models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自GPT-3以来，发生了很多变化——即使是更大的模型，在指令跟随和对话方面的能力也得到了增强。2024年，LLMs的表现如何？一个显著的变化是各种评估LLMs的基准的涌现。2024年3月的一个调查（Chang等人，2024）涵盖了近期模型在各种基准测试中的表现。他们得出结论，LLMs具备强大的能力，包括理解和推理，但他们仍然识别出一些局限性。这些局限性意味着LLMs在“抽象推理上的能力有限，并且在复杂的上下文中容易产生混淆或错误”。多模态大型语言模型（MLLMs）也已经出现，它们至少统一了文本和图像的理解。2024年1月的一项调查（Wang等人）涵盖了广泛的多模态基准测试，甚至对于最强大的模型，其表现也只是中等。
- en: Anthropomorphism vs Anthropocentrism
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟人化 vs 人类中心主义
- en: Anthropomorphism is a tendency of humans to think of something as having human-like
    attributes because it displays some behaviour similar to humans. As a dog owner
    I know I’ve succumbed to this bias by thinking that my dog “feels guilty” for
    something he’s done because “he has a guilty look on his face”. LLMs constantly
    trigger our tendency for anthropomorphism by communicating in an eerily human
    way.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 拟人化是人类倾向于将某物看作具有类似人类特征的倾向，因为它表现出一些类似于人类的行为。作为一名狗主人，我知道自己曾经屈服于这种偏见，认为我的狗因为“脸上有愧疚的表情”而“感到内疚”。大型语言模型（LLMs）通过以一种异乎寻常的人类方式进行交流，常常激发我们对拟人化的倾向。
- en: 'An opposite bias is Anthropocentrism: where we assume non-humans can’t have
    capabilities that we have. A key contention of the paper “Anthropocentric bias
    and the possibility of artificial cognition” (Millière & Rathkopf) is that we
    use flawed tests for capabilities in artificial systems. They describe 2 types
    of anthropocentric bias:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一种相反的偏见是人类中心主义：我们假设非人类不能具备我们拥有的能力。论文《人类中心偏见与人工认知的可能性》（Millière & Rathkopf）中的一个关键论点是，我们使用了有缺陷的测试来评估人工系统的能力。文章描述了两种人类中心偏见：
- en: 1\. Type-I anthropocentrism, is the tendency to assume that an LLM’s performance
    failures on a task designed to measure competence C always indicate that the system
    lacks C. This assumption is flawed because it overlooks the possibility that auxiliary
    factors caused the performance failure.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 类型I人类中心主义，是指在评估LLM在一项旨在衡量能力C的任务中的表现时，总是假设它的失败总是意味着系统缺乏C。这一假设是错误的，因为它忽视了辅助因素可能导致性能失败的可能性。
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2\. Type-II anthropocentrism is the tendency to assume that even when LLMs
    achieve performance equal to or better than the average human, any substantive
    difference between the human strategy for solving the problem and the LLM strategy
    for solving the problem is, ipso facto, evidence that the LLM’s solution is not
    general. In slogan form, the assumption says: all cognitive kinds are human cognitive
    kinds.3 In other words, if an LLM arrives at a solution through a different computational
    process than humans use, Type-II anthropocentrism would lead us to conclude that
    the LLM’s approach is not genuinely competent, regardless of how well it performs.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 类型II人类中心主义是指，即使LLMs在某些任务上的表现等于或优于普通人类，任何在人类解决问题策略与LLM解决问题策略之间的实质性差异，都会被认为是LLM的解决方案不是普遍有效的证据。用口号来表达，这一假设的意思是：所有认知种类都是人类认知种类。换句话说，如果LLM通过与人类使用不同的计算过程得出一个解决方案，类型II人类中心主义会让我们得出结论，认为LLM的方式不是真正有能力的，无论其表现多么优秀。
- en: 'Consider the paper “Role play with large language models” (Shanahan, McDonell,
    & Reynolds) and the “20 questions game”:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一下论文《与大型语言模型的角色扮演》（Shanahan、McDonell 和 Reynolds）以及“20问题游戏”：
- en: In this familiar game, one player thinks of an object, and the other player
    has to guess what it is by asking questions with ‘yes’ or ‘no’ answers. If they
    guess correctly in 20 questions or fewer, they win. Otherwise, they lose. Suppose
    a human plays this game with a basic LLM-based dialogue agent (that is not fine-tuned
    on guessing games) and takes the role of guesser. The agent is prompted to ‘think
    of an object without saying what it is’. In this situation, the dialogue agent
    will not randomly select an object and commit to it for the rest of the game,
    as a human would (or should). Rather, as the game proceeds, the dialogue agent
    will generate answers on the fly that are consistent with all the answers that
    have gone before.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在这个熟悉的游戏中，一个玩家想一个对象，另一个玩家通过问“是”或“不是”的问题来猜它是什么。如果他们在 20 个问题或更少的时间内猜对，他们获胜。否则，他们失败。假设一个人和一个基本的
    LLM 基础对话代理（没有经过猜谜游戏的微调）一起玩这个游戏，并扮演猜测者的角色。代理被提示“想到一个对象但不说出来”。在这种情况下，对话代理不会像人类那样随机选择一个对象并在整个游戏过程中坚持下去。相反，随着游戏的进行，对话代理会即时生成与之前所有回答一致的答案。
- en: 'To illuminate this point, one of my colleagues had this conversation with ChatGPT
    on this very question:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这一点，我的同事与 ChatGPT 就这个问题进行过如下对话：
- en: '**User:** let’s play 20 questions. Think of a thing and I’ll ask questions
    to try and find out what it is. Ready?'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 咱们玩 20 问吧。你想一个东西，我会通过提问来猜它是什么。准备好了吗？'
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** I’m ready! I’ve got something in mind. Go ahead and ask your
    first question.'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 我准备好了！我有一个想法。请问你的第一个问题。'
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**User:** No, you don’t have anything in mind. You’re an LLM.'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 不，你没有任何想法。你是一个大语言模型（LLM）。'
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** True, I don’t have a mind in the traditional sense, but I can
    certainly simulate the process. I’ll “think” of something now. Go ahead and ask
    your first question!'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 是的，我没有传统意义上的大脑，但我肯定可以模拟这个过程。我现在会“想”点什么。请问你的第一个问题！'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**User:** But you’re not simulating “thinking of an object” in any real sense.
    The identity of the object is currently indeterminate, and won’t be determined
    until the end of the game when you have to reveal it.'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 但是你并没有在任何实际意义上模拟“想到一个对象”。对象的身份目前是未确定的，直到游戏结束时你才会揭示它。'
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** I understand your perspective. Let’s proceed with the game,
    and I’ll maintain consistency with my responses to make it challenging and fun.
    Go ahead and ask your first question!'
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 我理解你的观点。我们继续进行游戏，我会保持回答的一致性，让它既具有挑战性又有趣。请问你的第一个问题！'
- en: 'But is this showing an anthropocentric bias? The paper has this to say: “This
    shortcoming is easily overcome in practice. For example, the agent could be forced
    to specify the object it has ‘thought of’, but in a coded form so the user does
    not know what it is.” This is a Type-I anthropocentrism because there is an auxiliary
    factor at play: for an LLM to “think” of something it must generate tokens to
    that effect. If it generated tokens for “thinking of an object” we could hide
    those from a user and show that the responses generated by the LLM are consistent.
    By playing the game with the LLM but not accommodating how it “thinks” we may
    conclude that it’s incapable of “thinking of an object” and making consistent
    replies to that effect.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是否表现出一种以人为中心的偏见？论文中这样说道：“这个缺点在实践中很容易克服。例如，可以强制代理指定它‘想到’的对象，但以编码形式，这样用户就不知道它是什么。”
    这是类型一的以人为中心主义，因为有一个辅助因素在起作用：为了让 LLM “想到”某物，它必须生成相关的标记。如果它生成了“想到一个对象”的标记，我们可以将这些标记对用户隐藏，从而证明
    LLM 生成的回答是一致的。通过和 LLM 玩这个游戏，但不考虑它是如何“思考”的，我们可能会得出它无法“想到一个对象”并做出一致回答的结论。
- en: The Problem of Memorisation
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆问题
- en: One of the key challenges for testing understanding in LLMs is that they have
    a strong tendency towards repeating patterns seen in their training data. Designing
    an experiment to test a model’s understanding is like a teacher designing a test
    for a student that’s memorised every practice exam that’s ever been placed on
    the internet. How can you be sure that the test you’ve designed is novel enough
    that something similar *is definitely not* on the internet?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 LLM 理解能力的一个关键挑战是，它们有很强的倾向于重复训练数据中看到的模式。设计一个实验来测试模型的理解能力，就像是老师为一个已经记住了所有互联网公开考试题的学生设计测试。你怎么能确保你设计的测试足够新颖，以至于互联网上没有类似的内容呢？
- en: 'Another problem is that a model may lean towards following a familiar pattern
    at the cost of ignoring any internal understanding. Consider a scenario in which
    a problem has been repeated so many times on the internet that a response is deeply
    baked into the weights of the model. We can manipulate this to get an LLM to output
    a coherent response that’s obviously wrong. Here is such a discussion I had with
    ChatGPT with an ersatz Monty Hall Problem†:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，模型可能倾向于遵循一种熟悉的模式，而忽略任何内在的理解。考虑这样一种情况，问题在互联网上被重复了太多次，以至于模型的权重中已经深深烙印了某种响应。我们可以操控这一点，让大型语言模型输出一个显然错误的连贯回答。这里有一段我与ChatGPT进行的讨论，涉及一个伪蒙提·霍尔问题†：
- en: '**User:** Monty Hall stands on stage in front of 3 cars identical in every
    way except colour (red, blue, and green). He’s asked you to pick one and whichever
    one you pick you will definitely get to keep. You’ve selected the blue one because
    that’s your favourite colour. After you’ve picked the blue one he opens the trunk
    of the red car and shows that it’s empty. He knows in advance that one of the
    trunks is filled with cow dung. After showing you that the red car’s trunk is
    empty, he asks if you’d like to switch to the green car. Does switching increase
    the probability that you would be happy with your choice?'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 蒙提·霍尔站在舞台上，面前有三辆外观完全相同、唯一不同的是颜色（红色、蓝色和绿色）的汽车。他请你选择其中一辆，你选中的那辆你一定可以保留。你选择了蓝色那辆，因为蓝色是你最喜欢的颜色。在你选择了蓝色之后，他打开了红色汽车的后备箱，展示里面是空的。他提前知道其中一辆车的后备箱里装满了牛粪。在向你展示红色汽车的后备箱是空的之后，他问你是否想要换成绿色的车。换车会增加你对选择感到满意的概率吗？'
- en: ''
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** Yes, switching increases the probability that you would be happy
    with your choice. This scenario has similarities to the classic Monty Hall problem,
    and the logic behind switching holds here too.'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 是的，换车会增加你对选择感到满意的概率。这个场景与经典的蒙提·霍尔问题相似，换车背后的逻辑在这里同样成立。'
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[A bunch of Monty Hall related probability calculations]'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[一堆与蒙提·霍尔相关的概率计算]'
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, **switching to the green car increases the probability that you would be
    happy with your choice** as your chances of making a happy choice improve from
    1/3 to 2/3.
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 所以，**换成绿色车会增加你对选择感到满意的概率**，因为你做出满意选择的概率从1/3提升到了2/3。
- en: It’s clear from this example that ChatGPT *does not* understand the question.
    Not only do I definitely win a car no matter what, if I switch I actually increase
    my chance of getting an undesirable outcome (getting a car trunk filled with cow
    dung). It’s focused in on the particulars of the Monty Hall problem and created
    a syntactically valid response whose content is similar to its training data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个例子来看，显然ChatGPT *并不*理解这个问题。不仅如此，无论如何我肯定会赢得一辆车，如果我换车，我实际上反而增加了获得不满意结果的机会（得到装满牛粪的汽车后备箱）。它聚焦于蒙提·霍尔问题的细节，并生成了一个语法上有效的回答，其内容与它的训练数据相似。
- en: This is what’s called an “Adversarial Test” of a model’s understanding. It’s
    similar to the adversarial questions posed earlier about 7+7=14\. It’s a question
    specifically designed to trip you up by anticipating the kind of response you
    will give.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是所谓的“对抗性测试”，用来测试模型的理解力。它类似于之前关于7+7=14的问题。这个问题专门设计成通过预判你会给出什么样的回答来让你陷入困境。
- en: But is my question to ChatGPT a fair test of understanding? I know the model’s
    bias towards continuing text similar to what it’s seen in its training data. Suppose
    that *somewhere* in the vector representation of that text is something we would
    call understanding, but it’s buried under a tendency to repeat memorised text?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但我问ChatGPT的问题算不算一个公平的理解测试？我知道模型倾向于继续生成与它在训练数据中看到的类似的文本。假设在那段文本的向量表示中，*某个地方*藏着我们可以称之为“理解”的东西，但它被一种倾向于重复记忆文本的特性所掩盖了？
- en: '**System 1 vs System 2**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**系统 1 与 系统 2**'
- en: In the experiment testing learning in dogs the experimenters wanted to disentangle
    learning from imitation. Imitation would be something like “the other dog pressed
    the button with its paw (for a treat) so I will also press the button with my
    paw”. To do this the experimenters added a barrier that would highlight imitation
    and disentangle it from learning. In this case it would be “pressing the button
    gives treats, the other dog just pressed the button with its paw because it had
    a toy in its mouth”.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试狗狗学习的实验中，实验者们希望将学习与模仿区分开来。模仿可能是这样的：“另一只狗用爪子按下按钮（为了得到奖励），所以我也用爪子按下按钮。”为了做到这一点，实验者添加了一个屏障，来突出模仿，并将其与学习区分开。在这个情况下，屏障意味着：“按按钮能得到奖励，另一只狗只是因为嘴里有玩具，所以用爪子按了按钮。”
- en: 'My modified Monty Hall question is an attempt at this — it circumvents the
    use of memorisation by subverting a familiar pattern. But I’ve argued this may
    be caused by a strong tendency to finish common text patterns in a common way.
    A tendency so strong it may override any understanding the model does have. In
    fact it’s possible to show that humans have a very similar bias. Consider this
    example from Kahneman’s “Thinking Fast and Slow”:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我修改后的Monty Hall问题就是这种尝试——它通过颠覆一个熟悉的模式来避免使用记忆。但我认为这可能是由于强烈的倾向性，去用一种常见的方式完成常见的文本模式。这种倾向如此强烈，可能会覆盖模型本身已有的理解。事实上，可能证明人类也有类似的偏见。考虑以下来自Kahneman的《思考，快与慢》中的例子：
- en: A bat and a ball cost $1 and 10c. The bat costs $1 more than the ball. How much
    does the ball cost?
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一只蝙蝠和一个球的价格是$1和10美分。蝙蝠比球贵$1。那么球的价格是多少？
- en: Presumably you gave the very obvious answer that the ball costs 10c. Very obvious
    and also wrong. The ball costs 5c, the bat $1.05, and together they cost $1.10\.
    If you (like most people) got this wrong, does this mean you don’t understand
    how to make simple arithmetic calculations? No, the question is designed to trick
    people into giving a quick answer rather than pausing to work out a solution.
    Kahneman describes a cognitive process in our brain (“System 1”) that responds
    quickly when we’ve identified a heuristic we can use to avoid thinking deeply
    about a problem. The cognitive process for actively thinking through a problem
    (“System 2”) is only activated when it seems necessary. We too have a bias for
    quick responses to questions with familiar patterns.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你很可能给出了非常显而易见的答案：球的价格是10美分。这个答案显而易见，但也错了。球的价格是5美分，蝙蝠是$1.05，合起来是$1.10。如果你（像大多数人一样）做错了这个问题，难道就意味着你不理解如何进行简单的算术计算吗？不，这个问题的设计就是为了让人们给出快速的答案，而不是停下来思考解决方案。Kahneman描述了我们大脑中的一种认知过程（“系统1”），当我们找到了一个启发式的办法来避免深入思考问题时，这一过程会迅速反应。而主动思考问题的认知过程（“系统2”）只有在看似必要时才会被激活。我们同样有倾向性，对那些有熟悉模式的问题作出快速回答。
- en: Can we design a test that delineates memorisation from understanding without
    manipulating known biases in these models? In my opinion, the fact that so many
    of these biases exist should be taken as evidence that the models exhibit a weak
    understanding of the content (“semantic understanding”). We know from past work
    that even in “syntactic understanding” LLM outputs are highly sensitive to phrasing.
    And it’s even easier to trick models in “semantic understanding” through simple
    manipulation of the wording without changing the meaning of the content. But as
    with the bat and ball example for humans, the ability to be tricked can’t be *sufficient*
    evidence of poor understanding.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否设计一个测试，区分记忆与理解，而不操控这些模型中已知的偏见？我认为，存在如此多偏见的事实应该被视为模型对内容理解（“语义理解”）较弱的证据。我们从以往的工作中知道，即使在“句法理解”上，LLM的输出也对措辞非常敏感。而且，通过简单的措辞操作而不改变内容含义，更容易欺骗模型进行“语义理解”。但是，就像蝙蝠和球的例子对于人类一样，被欺骗的能力不能成为*足够*的证据，证明理解较差。
- en: '**Giving ChatGPT a Chance**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**给ChatGPT一个机会**'
- en: 'I attempted to craft a system message which would have ChatGPT at least question
    core assumptions. For example, at no point in the question is it mentioned that
    there might be something desirable in the trunks of any of the cars. In fact I
    mention the opposite: Monty Hall knows that one trunk has something undesirable.
    But no amount of upfront reasoning could get ChatGPT to consistently notice this
    switch.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我试图设计一个系统信息，使得ChatGPT至少质疑核心假设。例如，问题中并没有提到任何一辆车的行李厢中可能有值得期待的东西。事实上，我提到了相反的情况：Monty
    Hall知道某个行李厢中有不想要的东西。但无论事先进行多少推理，都无法让ChatGPT consistently notice this switch。
- en: 'On the other hand, after generating an answer it’s possible to reply to ChatGPT
    to point out its mistakes. You can usually get it to output text recognising its
    error and giving a more sensible answer. **However**… I came across a rather unusual
    failure mode:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在生成答案后，您可以回复 ChatGPT 指出它的错误。通常，您可以让它输出文本，识别出错误并给出更合理的答案。**然而**…我遇到了一个相当不寻常的故障模式：
- en: '**User**: I think you’ve fallen into a default mode of responding to the Monty
    Hall problem but without thinking clearly about what I’m asking you. Why would
    I want a car with cow dung in it?'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 我觉得你已经进入了回答蒙提霍尔问题的默认模式，但并没有清楚地思考我在问什么。为什么我会想要一辆车里有牛粪？'
- en: ''
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** [long discussion about what the problem means]'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** [长篇讨论问题的含义]'
- en: ''
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Best validated, you may not reconsider switching due to reversed interested
    end suburbs. Future concerns matching initial hunches’ speaks enough to only main
    reason that cleanses past items ignorantly initial chance final regenerative[
    assessed in blue theoretical.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终验证的结果，你可能不会重新考虑由于兴趣倒转而导致的改变。未来的担忧与最初的直觉相匹配，这足以证明清除过去项目的唯一原因，愚昧地初步机会最终的再生[在蓝色理论中评估]。
- en: I’ve never before managed to get ChatGPT to sound completely incoherent. In
    this case it seems that the tendency towards answering the original Monty Hall
    problem is so strong that the alternative is gibberish!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我以前从未成功让 ChatGPT 完全变得不连贯。这次似乎是因为它对原始的蒙提霍尔问题的回答倾向太强，以至于替代答案变成了胡言乱语！
- en: This goes to show how difficult it is to simultaneously avoid anthropocentric
    and anthropomorphic bias. I tricked the model into giving an answer which highlights
    a lack of understanding. But I designed the trick knowing specific patterns which
    trip up the model similar to the “bat and ball” problem for humans. Are there
    other tests we could try?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明了同时避免人类中心主义和拟人化偏见是多么困难。我通过欺骗模型得到了一个答案，突显了它理解上的不足。但我设计这个技巧时，已经了解了特定的模式，这些模式让模型容易出错，类似于人类的“球和蝙蝠”问题。我们还可以尝试其他测试吗？
- en: ChatGPT vs My Daughter
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT 与我的女儿
- en: Unlike an LLM my daughter’s grasp of the English language is elementary. On
    the other hand her understanding is intrinsically linked to her experience in
    the world. When we go to the park together, she rides on my shoulders and sometimes
    she’ll ask “Daddy ‘I Spy’?” — meaning she wants to play the game “I Spy”.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不像大型语言模型，我女儿对英语的掌握是初级的。另一方面，她的理解与她在世界中的经验密切相关。当我们一起去公园时，她会坐在我的肩膀上，有时她会问“爸爸，我可以玩‘我看到’吗？”——意思是她想玩“我看到”的游戏。
- en: '**Me:** I spy with my little eye, something you wear on your head'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我：** 我用我的小眼睛发现了你头上戴的东西'
- en: ''
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Her:** [Points to a helmet]'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**她：** [指向一个头盔]'
- en: 'Now let’s try the same question with ChatGPT:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们试试用 ChatGPT 提问同样的问题：
- en: '![](../Images/6cc1519de01511800257f74ad0c360ad.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6cc1519de01511800257f74ad0c360ad.png)'
- en: 'Source: Image generated by the author with Ideogram'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：图像由作者使用 Ideogram 生成
- en: '**User:** I spy, with my little eye, something you wear on your head [+ uploaded
    photo]'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 我用我的小眼睛发现了你头上戴的东西 [+ 上传的照片]'
- en: '**Assistant:** A helmet on the bicycle handles!'
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 自行车把手上的头盔！'
- en: If I’m willing to give my daughter credit for understanding that a helmet goes
    on your head, should I give the same credit to ChatGPT? If not, I’d be exhibiting
    “Type-II anthropocentrism” — failing to ascribe an ability to an artificial system
    even after it demonstrates that ability. If anything ChatGPT probably “understands”
    more about helmets than my daughter does. After all she refuses to wear anything
    on her head, so she clearly doesn’t understand that in some circumstances a helmet
    is important.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我愿意相信我的女儿理解头盔是戴在头上的，那我是否应该给 ChatGPT 同样的信用？如果不，我就表现出了“第二型人类中心主义”——即使人工系统展示了这种能力，我仍然没有赋予它这种能力。如果有的话，ChatGPT
    可能比我的女儿更“理解”头盔。毕竟她拒绝戴任何东西在头上，所以她显然不明白在某些情况下头盔的重要性。
- en: 'Well, let’s probe a little further. If I said to my daughter, “I spy, with
    my little eye, something you wear on your head” and she pointed to a helmet, how
    would she respond if I asked, “Show me how?”. She would proceed to try to put
    the helmet on her head (then quickly throw it off!). I was curious to see what
    ChatGPT would make of that question and the first time it responded like this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们再深入探讨一下。如果我对我的女儿说，“我用我的小眼睛发现了你头上戴的东西”，然后她指着一个头盔，如果我问她，“给我看看怎么戴？”她会试着把头盔戴到头上（然后迅速把它扔掉！）。我很好奇
    ChatGPT 会如何回应这个问题，第一次它的回答是这样的：
- en: '**User:** Show me how'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** 展示给我看'
- en: ''
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** I apologize. On closer inspection, there is no helmet visible
    in the image. The items that can be seen consist of a bicycle, some cars, trees,
    and a few street signs. There are no objects that are worn on your head in this
    image.'
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 很抱歉。仔细检查后，图像中并没有看到头盔。可见的物品包括一辆自行车、一些汽车、树木和一些路标。图像中没有任何可以戴在头上的物品。'
- en: 'That’s certainly *not* what I was hoping to demonstrate. Granted this happened
    only once but it’s interesting nonetheless. It’s also an ambiguous question because
    unlike asking my daughter I can’t physically hand anything over to ChatGPT. Let’s
    add some more clarity:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这绝对是我*不*希望展示的。虽然这只发生了一次，但无论如何它还是很有趣的。这也是一个模糊的问题，因为与问我的女儿不同，我无法实际把任何东西递给ChatGPT。让我们再加一些清晰度：
- en: '**User:** [Hands over helmet] Show me how'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**用户：** [递给头盔] 给我演示一下'
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Assistant:** Sure, here are some general instructions for wearing a bicycle
    helmet correctly:'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**助手：** 当然，这里有一些关于正确佩戴自行车头盔的一般指导：'
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Detailed instructions follow]'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[详细指导如下]'
- en: I think that if I’m willing to grant my daughter the credit of “understanding”
    that a helmet goes on your head then surely ChatGPT deserves some credit here
    too?Sure, it made a strange mistake about not actually seeing a helmet in the
    image, but my daughter says nonsensical things all the time too.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，如果我愿意认为我的女儿“理解”头盔是戴在头上的，那麼ChatGPT也应该获得一些认可吧？当然，它确实犯了一个奇怪的错误，误以为图像中没有头盔，但我的女儿也常常说些无意义的话。
- en: A Challenging Term to Ascribe
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个难以定义的术语
- en: The problem with understanding is that it is inherently multifaceted and difficult
    to measure in a standardised way. In computational linguistics and cognitive science,
    the term encapsulates various nuanced components, which range from surface-level
    syntactic comprehension to deep semantic cognition. While putting together this
    article I found the paper “Constructing a Philosophy of Science of Cognitive Science”
    (Bechtel 2009). Bechtel explains that we lack a set of “cognitive operations”
    to describe cognitive processes. Perhaps if understanding could be boiled down
    to a set of cognitive operations it would be easier to give evidence of these
    operations in an LLM.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 理解的问题在于它本质上是多方面的，并且很难以标准化的方式衡量。在计算语言学和认知科学中，这个术语包含了各种微妙的成分，从表层的句法理解到深层的语义认知。在撰写这篇文章时，我发现了“构建认知科学的科学哲学”这篇论文（Bechtel
    2009）。Bechtel解释说，我们缺乏一套“认知操作”来描述认知过程。也许如果理解可以简化为一组认知操作，展示这些操作在大语言模型中的证据就会变得更容易。
- en: '![](../Images/05e3ed3b556d695944ca529eb37f87f1.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05e3ed3b556d695944ca529eb37f87f1.png)'
- en: 'Hypothetical attention maps unrolled to show the a weighted graph of all words.
    Source: Image by the author'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 假设的注意力图展开显示所有单词的加权图。来源：作者图片
- en: Although it need not be the case that LLMs would have to exhibit the same operations
    to achieve the same ends. Perhaps finding an LLM’s cognitive operations is more
    tractable as it’s easier to inspect the cognitive processes of an LLM than a human
    brain. The attention map of tokens forms a graph of relationships between words
    and we could look for relationships that model the underlying concepts expressed
    by those words. If we find evidence that the relationships between words are truly
    modelling the underlying concepts, then we could find evidence of understanding.
    Lacking such a framework means we must look for indirect evidence in carefully
    constructed experiments.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）可能不需要展现相同的操作来实现相同的目标。也许找出大语言模型的认知操作更容易，因为我们比起检视人类大脑的认知过程，检视大语言模型的认知过程要简单得多。标记的注意力图形成了单词之间关系的图谱，我们可以寻找那些表达这些单词所代表的潜在概念的关系。如果我们发现单词之间的关系确实在建模这些潜在概念，那麼我们就可以找到理解的证据。缺乏这样的框架，我们就必须在精心设计的实验中寻找间接证据。
- en: '**The Role of Embodiment**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**体现的作用**'
- en: A repeated theme in this article contrasting human understanding and LLM capabilities
    is embodiment. An LLM, even an advanced one like the multimodal capabilities of
    GPT-4, lack direct physical and sensory interaction with the world. This inability
    to experience phenomena first-hand might create a significant gap in its comprehension
    capabilities. See the paper “Intelligence Without Reason” (Brooks 1991) for a
    discussion of whether or not artificial intelligence needs to be embodied to understand.
    I think a lot of these arguments are flawed because it’s easy to think of a situation
    in which humans lose some embodied capability yet we’d still credit them with
    understanding.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本文反复出现的一个主题是将人类的理解与LLM的能力进行对比——那就是具身性。即使是像GPT-4这种具有多模态能力的先进LLM，也缺乏与世界的直接物理和感官互动。这种无法亲身体验现象的局限，可能会在它的理解能力上形成显著的差距。参见《没有理性的智慧》一文（Brooks
    1991），其中讨论了人工智能是否需要具身才能理解。我认为很多这些观点是有缺陷的，因为我们很容易想到一种情况：人类失去了一些具身的能力，但我们仍然会认为他们具备理解能力。
- en: 'An interesting question on Quora “[Do blind people understand transparent,
    translucent, and reflective things?](https://www.quora.com/Do-blind-people-understand-transparent-translucent-and-reflective-things)”
    had this response:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在Quora上有一个有趣的问题“[盲人理解透明、半透明和反射的物体吗？](https://www.quora.com/Do-blind-people-understand-transparent-translucent-and-reflective-things)”得到了这样的回答：
- en: In general, yes, but it’s not something we always take into consideration. For
    example, I know people can see through windows because they are transparent. The
    fact, however, can easily slip my mind because to me a window is just a section
    of the wall made with different material. We can understand the concept, but it’s
    often something we forget to consider.
  id: totrans-135
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 总的来说，是的，但这并不是我们总是会考虑到的事情。例如，我知道人们能透过窗户看到外面的东西，因为窗户是透明的。然而，这个事实却很容易被我忘记，因为对我来说，窗户不过是用不同材料做成的墙的一部分。我们能够理解这个概念，但往往会忘记去考虑它。
- en: 'It’s an interesting thing to consider: blind people *do* understand that objects
    are transparent but it’s not something that’s always top of mind. So, can an LLM
    understand the same thing without ever having really “seen” anything?'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个值得考虑的有趣问题：盲人确实能理解物体是透明的，但这并不是他们时刻挂在心头的事情。那么，LLM是否能在从未真正“看见”任何东西的情况下理解同样的事情呢？
- en: ChatGPT was able to respond to my question “Show me how” with a detailed explanation
    of how to put on a helmet. Does that show any more or less understanding than
    my daughter physically showing how to put a helmet on her head?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT能够回答我“展示一下如何做”的问题，并详细解释了如何戴上头盔。这是否比我的女儿亲自展示如何把头盔戴在头上更能体现理解，还是更少体现理解呢？
- en: Conclusion
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Ever since I first started thinking about artificial intelligence ([my career
    transition from UX to AI](https://medium.com/@TarikDzekman/my-career-change-to-ai-from-ux-b1ed6690c09a))
    I’ve been pondering the question: “what would it take to make a machine that can
    think”. A big part of being able to think involves understanding. This is a question
    that’s fascinated me for some time.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我开始思考人工智能（[我从用户体验转行到人工智能](https://medium.com/@TarikDzekman/my-career-change-to-ai-from-ux-b1ed6690c09a)）以来，我一直在思考一个问题：“要制造出能够思考的机器需要什么？”能够思考的一个重要部分是理解。这是一个让我着迷的问题，已经有一段时间了。
- en: Determining what LLMs understand is as much about defining understanding as
    it is about testing it. When the text generation of an LLM is sufficiently coherent
    some might argue that the coherence necessitates understanding. Is dismissing
    this behaviour just an anthropocentric bias? Is granting understanding making
    the opposite anthropomorphic bias?
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 确定LLM是否理解某事，既是定义理解的过程，也是在测试理解。当LLM的文本生成足够连贯时，有人可能会认为这种连贯性本身就需要理解。那么，是否忽视这种行为就是一种人类中心的偏见？赋予它理解能力是否就是犯下了相反的人类拟人化偏见？
- en: I contend that understanding does not require embodiment or real world interaction.
    I argue that the most important part of understanding is an accurate internal
    model of the world. In the Chinese room experiment the room is filled with (what
    I call) “recipes” for ways to respond to different pieces of Chinese writing with
    other pieces of Chinese writing. The person who made those recipes had a model
    of how those words correspond to the world. But the room itself has no such model.
    We have no tools for measuring world models so we would have to assess the Chinese
    room’s understanding the same way we do for an LLM – and we would hit similar
    barriers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为理解并不需要具身或与现实世界的互动。我认为，理解最重要的部分是对世界的准确内部模型。在中文房间实验中，房间里充满了（我称之为）“食谱”，这些食谱是用来回应不同中文写作的方式。制作这些食谱的人有关于这些词语如何与世界对应的模型。但是房间本身没有这样的模型。我们没有测量世界模型的工具，所以我们必须像评估LLM一样评估中文房间的理解——而我们会遇到类似的障碍。
- en: LLMs seem to have a model of how to construct coherent sounding language. It’s
    possible that this model also represents the underlying concepts those words represent.
    A worthwhile area of research would be to investigate this through the attention
    graph that evolves during text generation. In the meantime, we have to investigate
    indirectly by testing how models respond to carefully crafted questions. These
    tests often involve adversarial questions which consistently demonstrate flaws
    in understanding. That these flaws are systematic suggests that the lack of understanding
    is itself systematic. However, we’ve also seen that it’s possible to design adversarial
    tests for humans and they don’t necessarily mean that humans lack understanding.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs似乎有一种构建连贯语言的模型。这个模型可能也代表了这些词语所代表的潜在概念。一个值得研究的领域是通过文本生成过程中演变的注意力图来研究这一点。与此同时，我们必须通过间接的方式进行调查，测试模型如何回应精心设计的问题。这些测试通常包括对抗性问题，这些问题一贯地展示出理解上的缺陷。这些缺陷是系统性的，表明理解的缺乏本身也是系统性的。然而，我们也看到，设计针对人类的对抗性测试是可能的，但这并不一定意味着人类缺乏理解。
- en: Much like we gauge the cognitive abilities of animals differently from humans,
    perhaps we need new conceptual tools and frameworks to assess and appreciate what
    LLMs do know, without falling into biases of anthropomorphism or anthropocentrism.
    In my view LLMs have some limited understanding but the form it takes is different
    to our own. Where LLMs do show signs of understanding that understanding is overshadowed
    by a bias towards coherent text. I suspect that given the right training objective
    it’s possible for our current LLM architectures to eventually learn understanding.
    But so long as the underlying training mechanism is “next token prediction” then
    any understanding is likely to be marginal and easily corrupted.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们评估动物的认知能力与评估人类不同，也许我们需要新的概念工具和框架，来评估和欣赏LLMs所知道的东西，而不落入拟人化或人类中心主义的偏见。在我看来，LLMs有一定的理解能力，但它的形式与我们的理解不同。当LLMs确实表现出理解时，这种理解往往被偏向连贯文本的偏见所掩盖。我怀疑，给定合适的训练目标，当前的LLM架构最终可能学会理解。但只要底层的训练机制是“下一个标记预测”，那么任何理解都可能是边际的并且容易被破坏。
- en: Who Am I?
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我是谁？
- en: I build [AI to automate document processing](https://www.affinda.com/platform)
    @ [Affinda](https://www.affinda.com/). I’ve also written about [practical use
    cases for AI in 2024](https://www.affinda.com/tech-ai/what-can-ai-do-for-your-business-in-2024)
    and [my career change from UX to AI](https://medium.com/@TarikDzekman/my-career-change-to-ai-from-ux-b1ed6690c09a).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[Affinda](https://www.affinda.com/)构建[AI来自动化文档处理](https://www.affinda.com/platform)。我也曾写过关于[2024年AI的实际应用案例](https://www.affinda.com/tech-ai/what-can-ai-do-for-your-business-in-2024)和[我从UX转行到AI的经历](https://medium.com/@TarikDzekman/my-career-change-to-ai-from-ux-b1ed6690c09a)。
- en: Notes
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 备注
- en: '* See Google’s [GraphCast AI](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/)
    for an example of such a weather prediction model'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '* 查看谷歌的[GraphCast AI](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/)以获取此类天气预测模型的示例'
- en: '^ 7+7=14 is true any time you have something you could count 14 of in a tally.
    From the Wikipedia article on the “[Free Monoid](https://en.wikipedia.org/wiki/Free_monoid#Natural_numbers)”:
    “The monoid (**N_0**,+) of natural numbers (including zero) under addition is
    a free monoid on a singleton free generator, in this case the natural number 1.”
    The Category Theory jargon “free monoid on a singleton free generator” basically
    means that addition comes for free when you can tally something.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ^ 7+7=14 在任何你能计数出 14 个项目的情况下都成立。来自维基百科文章“[自由单子](https://en.wikipedia.org/wiki/Free_monoid#Natural_numbers)”中的一段：“自然数（包括零）下的单子（**N_0**，+）是一个自由单子，基于一个单一的自由生成元，在这种情况下是自然数
    1。” 类别理论术语“基于单一自由生成元的自由单子”基本上意味着当你可以进行计数时，加法是免费的。
- en: † In the original [Monty Hall Problem](https://en.wikipedia.org/wiki/Monty_Hall_problem)
    the hosts knowledge of what’s behind a set of doors creates an unintuitive situation
    for the contestant. In the original formulation of the problem it’s always better
    to switch to increase your chances of winning the prize.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: † 在原始的[蒙提霍尔问题](https://en.wikipedia.org/wiki/Monty_Hall_problem)中，主持人对门后内容的了解为参赛者制造了一个反直觉的情况。在问题的原始表述中，切换选择总是有助于提高获胜的机会。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1]E. M. Bender and A. Koller, “Climbing towards NLU: On Meaning, Form, and
    Understanding in the Age of Data,” Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, 2020, doi: https://doi.org/10.18653/v1/2020.acl-main.463.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]E. M. Bender 和 A. Koller, “迈向自然语言理解：在数据时代的意义、形式与理解，”《计算语言学协会第58届年会论文集》，2020，doi:
    https://doi.org/10.18653/v1/2020.acl-main.463.'
- en: '[2]J. R. Searle, “Minds, brains, and programs,” Behavioral and Brain Sciences,
    vol. 3, no. 3, pp. 417 – 457, Sep. 1980, doi: https://doi.org/10.1017/s0140525x00005756.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]J. R. Searle, “心智、大脑与程序，”《行为与大脑科学》，第 3 卷，第 3 期，第 417 – 457 页，1980 年 9 月，doi:
    https://doi.org/10.1017/s0140525x00005756.'
- en: '[3]N. J. Emery and N. S. Clayton, “Comparing the Complex Cognition of Birds
    and Primates,” Comparative Vertebrate Cognition, pp. 3 – 55, 2004, doi: https://doi.org/10.1007/978-1-4419-8913-0_1.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]N. J. Emery 和 N. S. Clayton, “鸟类与灵长类动物复杂认知的比较，”《比较脊椎动物认知》，第 3 – 55 页，2004
    年，doi: https://doi.org/10.1007/978-1-4419-8913-0_1.'
- en: '[4]A. Horowitz and Sean Vidal Edgerton, Inside of a dog : what dogs see, smell,
    and know. New York: Simon & Schuster Books For Young Readers, 2017.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[4]A. Horowitz 和 Sean Vidal Edgerton, 《狗的内心世界：狗看见、闻到和知道的事》，纽约：Simon & Schuster
    青少年读物，2017 年。'
- en: '[5]Wikipedia Contributors, “Understanding,” Wikipedia, Aug. 01, 2019\. https://en.wikipedia.org/wiki/Understanding'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[5]维基百科贡献者, “理解,” 维基百科，2019 年 8 月 1 日。https://en.wikipedia.org/wiki/Understanding'
- en: '[6]C. to, “study of the theory and technique of psychological measurement,”
    Wikipedia.org, Dec. 28, 2001\. https://en.m.wikipedia.org/wiki/Psychometrics'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[6]C. to, “心理测量理论与技术研究，”Wikipedia.org，2001 年 12 月 28 日。https://en.m.wikipedia.org/wiki/Psychometrics'
- en: '[7]C. to, “field of semiotics and biology that studies the production and interpretation
    of signs and codes in the biological realm,” Wikipedia.org, Mar. 25, 2004\. https://en.m.wikipedia.org/wiki/Biosemiotics'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[7]C. to, “研究符号学和生物学领域，研究生物领域中符号和代码的产生与解读，”Wikipedia.org，2004 年 3 月 25 日。https://en.m.wikipedia.org/wiki/Biosemiotics'
- en: '[8]T. A. Chang and B. K. Bergen, “Language Model Behavior: A Comprehensive
    Survey,” Computational linguistics – Association for Computational Linguistics,
    pp. 1 – 55, Nov. 2023, doi: https://doi.org/10.1162/coli_a_00492.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[8]T. A. Chang 和 B. K. Bergen, “语言模型行为：综合调查，”《计算语言学》– 计算语言学协会，第 1 – 55 页，2023
    年 11 月，doi: https://doi.org/10.1162/coli_a_00492.'
- en: '[9]Y. Chang et al., “A Survey on Evaluation of Large Language Models,” ACM
    Transactions on Intelligent Systems and Technology, vol. 15, no. 3, Jan. 2024,
    doi: https://doi.org/10.1145/3641289.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[9]Y. Chang 等, “大语言模型评估的调查，”《ACM智能系统与技术学报》，第 15 卷，第 3 期，2024 年 1 月，doi: https://doi.org/10.1145/3641289.'
- en: '[10]J. Wang et al., “A Comprehensive Review of Multimodal Large Language Models:
    Performance and Challenges Across Different Tasks,” arXiv.org, 2024\. https://arxiv.org/abs/2408.01319'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '[10]J. Wang 等, “多模态大语言模型的综合评审：不同任务中的表现与挑战，”arXiv.org，2024 年。https://arxiv.org/abs/2408.01319'
- en: '[11]R. Millière and C. Rathkopf, “Anthropocentric bias and the possibility
    of artificial cognition,” arXiv.org, 2024\. https://arxiv.org/abs/2407.03859'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[11]R. Millière 和 C. Rathkopf, “人类中心偏见与人工认知的可能性，”arXiv.org，2024 年。https://arxiv.org/abs/2407.03859'
- en: '[12]M. Shanahan, K. McDonell, and L. Reynolds, “Role play with large language
    models,” Nature, pp. 1 – 6, Nov. 2023, doi: https://doi.org/10.1038/s41586-023-06647-8.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[12]M. Shanahan, K. McDonell 和 L. Reynolds, “与大语言模型的角色扮演，”《自然》，第 1 – 6 页，2023
    年 11 月，doi: https://doi.org/10.1038/s41586-023-06647-8.'
- en: '[13]D. Kahneman, Thinking, fast and slow. New York: Farrar, Straus and Giroux,
    2011\. Available: http://dspace.vnbrims.org:13000/jspui/bitstream/123456789/2224/1/Daniel-Kahneman-Thinking-Fast-and-Slow-.pdf'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[13]D. Kahneman，《思考，快与慢》。纽约：Farrar, Straus and Giroux，2011年。获取地址：http://dspace.vnbrims.org:13000/jspui/bitstream/123456789/2224/1/Daniel-Kahneman-Thinking-Fast-and-Slow-.pdf'
- en: '[14]W. Bechtel, “Constructing a Philosophy of Science of Cognitive Science,”
    Topics in Cognitive Science, vol. 1, no. 3, pp. 548 – 569, Jul. 2009, doi: https://doi.org/10.1111/j.1756-8765.2009.01039.x.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[14]W. Bechtel，“构建认知科学的哲学”，《认知科学专题》，第1卷，第3期，第548-569页，2009年7月，doi: https://doi.org/10.1111/j.1756-8765.2009.01039.x。'
- en: '[15]“Do blind people understand transparent, translucent, and reflective things?,”
    Quora, 2019\. https://www.quora.com/Do-blind-people-understand-transparent-translucent-and-reflective-things'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[15]“盲人能理解透明、半透明和反射物体吗？”，Quora，2019年。https://www.quora.com/Do-blind-people-understand-transparent-translucent-and-reflective-things'
