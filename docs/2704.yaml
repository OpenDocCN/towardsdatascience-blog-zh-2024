- en: 'An Introduction to VLMs: The Future of Computer Vision Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VLM简介：计算机视觉模型的未来
- en: 原文：[https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06](https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06](https://towardsdatascience.com/an-introduction-to-vlms-the-future-of-computer-vision-models-5f5aeaafb282?source=collection_archive---------1-----------------------#2024-11-06)
- en: Building a 28% more accurate multimodal image search engine with VLMs.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用VLM构建一个准确度提高28%的多模态图像搜索引擎。
- en: '[](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[![Ro
    Isachenko](../Images/c2f1e41a389378cec8801e6eb8d8060c.png)](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)
    [Ro Isachenko](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[![Ro
    Isachenko](../Images/c2f1e41a389378cec8801e6eb8d8060c.png)](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)
    [Ro Isachenko](https://medium.com/@ro.isachenko?source=post_page---byline--5f5aeaafb282--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)
    ·12 min read·Nov 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5f5aeaafb282--------------------------------)
    ·12分钟阅读·2024年11月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Until recently, AI models were narrow in scope and limited to understanding
    either language or specific images, but rarely both.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，AI模型的范围较窄，只能理解语言或特定图像，但很少同时理解两者。
- en: In this respect, general language models like GPTs were a HUGE leap since we
    went from specialized models to general yet much more powerful models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，像GPT这样的通用语言模型是一次巨大的飞跃，因为我们从专业化的模型转向了通用但更强大的模型。
- en: But even as language models progressed, they remained separate from computer
    vision аreas, each domain advancing in silos without bridging the gap. Imagine
    what would happen if you could only listen but not see, or vice versa.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使语言模型有所进步，它们仍然与计算机视觉领域分离，每个领域在孤立的状态下发展，未能弥合这一鸿沟。试想一下，如果你只能听而不能看，或者只能看而不能听，会发生什么。
- en: My name is Roman Isachenko, and I’m part of the Computer Vision team at Yandex.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是Roman Isachenko，我是Yandex计算机视觉团队的一员。
- en: In this article, I’ll discuss visual language models (VLMs), which I believe
    are the future of compound AI systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将讨论视觉语言模型（VLM），我相信它们是复合AI系统的未来。
- en: I’ll explain the basics and training process for developing a multimodal neural
    network for image search and explore the design principles, challenges, and architecture
    that make it all possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将解释开发图像搜索多模态神经网络的基础知识和训练过程，并探讨使这一切成为可能的设计原则、挑战和架构。
- en: Towards the end, I’ll also show you how we used an AI-powered search product
    to handle images and text and what changed with the introduction of a VLM.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到最后，我还将展示我们如何使用AI驱动的搜索产品来处理图像和文本，以及引入VLM后发生了哪些变化。
- en: Let’s begin!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: What Are VLMs?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是VLM？
- en: LLMs with billions or even hundreds of billions of parameters are no longer
    a novelty.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有数十亿甚至数百亿参数的LLM已经不再是新鲜事物。
- en: We see them everywhere!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到处都能看到它们！
- en: The next key focus in LLM research has been more inclined towards developing
    multimodal models (omni-models) — models that can understand and process multiple
    data types.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: LLM研究的下一个重点是更多地倾向于开发多模态模型（全能模型）——能够理解和处理多种数据类型的模型。
- en: '![](../Images/e484761b3bcc1d96fbd1fbc82463d8a1.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e484761b3bcc1d96fbd1fbc82463d8a1.png)'
- en: Multimodal models (Image by Author)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态模型（图像由作者提供）
- en: As the name suggests, these models can handle more than just text. They can
    also analyze images, video, and audio.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，这些模型不仅能处理文本，还能分析图像、视频和音频。
- en: But why are we doing this?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们为什么要这么做呢？
- en: '*Jack of all trades, master of none, oftentimes better than master of one.*'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*万能工匠，什么都能做，什么都不精，往往比某一领域的专家更有优势。*'
- en: In recent years, we’ve seen a trend where general approaches dominate narrow
    ones.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们看到一个趋势，即通用方法正在主导特定的方法。
- en: Think about it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想。
- en: Today’s language-driven ML models have become relatively advanced and general-purpose.
    One model can translate, summarize, identify speech tags, and much more.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的基于语言的机器学习模型已经相对先进且具有通用性。一个模型可以进行翻译、摘要、识别语音标签，等等。
- en: '![](../Images/7b35063c0d265ee30383b449d2567afa.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b35063c0d265ee30383b449d2567afa.png)'
- en: General NLP model (Image by Author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通用NLP模型（图片由作者提供）
- en: But earlier, these models used to be task-specific (we have them now as well,
    but fewer than before).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但早些时候，这些模型往往是特定任务的（现在我们也有这种模型，但比以前少了）。
- en: A dedicated model for translating.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个专门用于翻译的模型。
- en: A dedicated model for summarizing, etc.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个专门用于摘要的模型，等等。
- en: In other words, today’s NLP models (LLMs, specifically) can serve multiple purposes
    that previously required developing highly specific solutions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，今天的自然语言处理模型（特别是大型语言模型）可以承担多种任务，这些任务以前需要开发高度特定的解决方案。
- en: 'Second, this approach allows us to exponentially scale the data available for
    model training, which is crucial given the finite amount of text data. Earlier,
    however, one would need task-specific data:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，这种方法让我们能够以指数级的速度扩展可用于模型训练的数据，这对于有限的文本数据量至关重要。然而，早期的做法是需要任务特定的数据：
- en: A dedicated translation labeled dataset.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个专门的翻译标注数据集。
- en: A dedicated summarization dataset, etc.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个专门的摘要数据集，等等。
- en: Third, we believe that training a multimodal model can enhance the performance
    of each data type, just like it does for humans.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们相信训练一个多模态模型能够提升每种数据类型的表现，就像它对人类的表现一样。
- en: For this article, we’ll simplify the “black box” concept to a scenario where
    the model receives an image and some text (which we call the “instruct”) as input
    and outputs only text (the response).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这篇文章，我们将“黑盒”概念简化为一种情景：模型接收一张图像和一些文本（我们称之为“指令”）作为输入，并仅输出文本（响应）。
- en: 'As a result, we end up with a much simpler process as shown below:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们得到了一个更简化的过程，如下所示：
- en: '![](../Images/ab10af2ac1a9743ce6deba3b47a7279a.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab10af2ac1a9743ce6deba3b47a7279a.png)'
- en: A simplified multimodal model (Image by Author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简化的多模态模型（图片由作者提供）
- en: We’ll discuss image-discriminative models that analyze and interpret what an
    image depicts.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论图像辨识模型，它们能够分析和解释图像所描绘的内容。
- en: Before delving into the technical details, consider the problems these models
    can solve.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入技术细节之前，请考虑这些模型能够解决的问题。
- en: 'A few examples are shown below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些示例：
- en: '![](../Images/6fa1a099ca302aa03dd704fbda095c2f.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fa1a099ca302aa03dd704fbda095c2f.png)'
- en: Examples of tasks (Image by Author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 任务示例（图片由作者提供）
- en: 'Top left image: We ask the model to describe the image. This is specified with
    text.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左上角图片：我们让模型描述这张图像。这个要求是用文本指定的。
- en: 'Top mid image: We ask the model to interpret the image.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中上部图片：我们让模型解释这张图像。
- en: 'Top right image: We ask the model to interpret the image and tell us what would
    happen if we followed the sign.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 右上角图片：我们让模型解释这张图像，并告诉我们如果我们遵循这个标志，会发生什么。
- en: 'Bottom image: This is the most complicated example. We give the model some
    math problems. From these examples, you can see that the range of tasks is vast
    and diverse.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底部图片：这是最复杂的例子。我们给模型一些数学问题。从这些例子中，你可以看到任务范围是如此广泛和多样化。
- en: VLMs are a new frontier in computer vision that can solve various [fundamental
    CV-related tasks](https://arxiv.org/abs/2405.18415) (classification, detection,
    description) in zero-shot and one-shot modes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: VLMs是计算机视觉领域的一个新前沿，它们能够在零样本和单样本模式下解决各种[基本的计算机视觉任务](https://arxiv.org/abs/2405.18415)（分类、检测、描述）。
- en: While VLMs may not excel in every standard task yet, they are advancing quickly.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然VLMs可能尚未在每个标准任务中表现得特别出色，但它们正在迅速进步。
- en: Now, let’s understand how they work.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解它们是如何工作的。
- en: VLM Architecture
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLM架构
- en: 'These models typically have three main components:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通常有三个主要组件：
- en: '![](../Images/b96ccc97b7f95acfec256feef7397cbc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b96ccc97b7f95acfec256feef7397cbc.png)'
- en: Simplified representation of VLM (Image by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: VLM的简化表示（图片由作者提供）
- en: LLM — a text model (YandexGPT, in our case) that doesn’t understand images.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM — 一种文本模型（在我们的例子中是YandexGPT），它不理解图像。
- en: Image encoder — an image model (CNN or Vision Transformer) that doesn’t understand
    text.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像编码器 — 一种图像模型（CNN或视觉变换器），它不理解文本。
- en: Adapter — a model that acts as a mediator to ensure that the LLM and image encoder
    get along well.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配器 — 一个充当中介的模型，确保LLM和图像编码器能够良好配合。
- en: 'The pipeline is pretty straightforward:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该流程相当直接：
- en: Feed an image into the image encoder.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像输入图像编码器。
- en: Transform the output of the image encoder into some representation using the
    adapter.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像编码器的输出转换为某种表示，供适配器使用。
- en: Integrate the adapter’s output into the LLM (more on that below).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将适配器的输出整合到 LLM 中（下面将详细介绍）。
- en: While the image is processed, convert the text instruct into a sequence of tokens
    and feed them into the LLM.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当图像被处理时，将文本指令转换为一系列标记并输入 LLM。
- en: More Information About Adapters
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于适配器的更多信息
- en: The adapter is the most exciting and important part of the model, as it precisely
    facilitates the communication/interaction between the LLM and the image encoder.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器是模型中最令人兴奋和最重要的部分，因为它准确地促进了 LLM 和图像编码器之间的通信/交互。
- en: 'There are two types of adapters:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的适配器：
- en: Prompt-based adapters
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于提示的适配器
- en: Cross-attention-based adapters
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于交叉注意力的适配器
- en: Prompt-based adapters were first proposed in [BLIP-2](https://arxiv.org/abs/2301.12597)
    and [LLaVa](https://arxiv.org/abs/2304.08485) models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的适配器最早在 [BLIP-2](https://arxiv.org/abs/2301.12597) 和 [LLaVa](https://arxiv.org/abs/2304.08485)
    模型中提出。
- en: The idea is simple and intuitive, as evident from the name itself.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单且直观，从名字本身就可以看出。
- en: We take the output of the image encoder (a vector, a sequence of vectors, or
    a tensor — depending on the architecture) and transform it into a sequence of
    vectors (tokens), which we feed into the LLM. You could take a simple MLP model
    with a couple of layers and use it as an adapter, and the results will likely
    be pretty good.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像编码器的输出（一个向量、一系列向量或一个张量——取决于架构）转换为一系列向量（标记），然后将其输入到 LLM 中。你可以采用一个简单的 MLP
    模型，带有几层，并将其作为适配器，结果可能会非常好。
- en: Cross-attention-based adapters are a bit more sophisticated in this respect.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基于交叉注意力的适配器在这方面要复杂一些。
- en: They were used in recent papers on [Llama 3.2](https://arxiv.org/abs/2407.21783)
    and [NVLM](https://arxiv.org/abs/2409.11402).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它们已被应用于最近关于 [Llama 3.2](https://arxiv.org/abs/2407.21783) 和 [NVLM](https://arxiv.org/abs/2409.11402)
    的论文中。
- en: These adapters aim to transform the image encoder’s output to be used in the
    LLM’s cross-attention block as key/value matrices. Examples of such adapters include
    transformer architectures like [perceiver resampler](https://arxiv.org/abs/2204.14198)
    or [Q‑former](https://arxiv.org/abs/2301.12597).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些适配器旨在将图像编码器的输出转换为可在 LLM 的交叉注意力块中用作键/值矩阵的形式。此类适配器的示例包括像 [感知器重采样器](https://arxiv.org/abs/2204.14198)
    或 [Q‑former](https://arxiv.org/abs/2301.12597) 这样的变换器架构。
- en: '![](../Images/0f4c28f4f6442ec22375f6141c9c62c1.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f4c28f4f6442ec22375f6141c9c62c1.png)'
- en: Prompt-based adapters (left) and Cross-attention-based adapters (right) (Image
    by Author)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的适配器（左）和基于交叉注意力的适配器（右）（图像来源：作者）
- en: Prompt-based adapters (left) and Cross-attention-based adapters (right)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的适配器（左）和基于交叉注意力的适配器（右）
- en: Both approaches have pros and cons.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法各有优缺点。
- en: Currently, prompt-based adapters [deliver better results](https://arxiv.org/abs/2409.11402)
    but take away a large chunk of the LLM’s input context, which is important since
    LLMs have limited context length (for now).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于提示的适配器 [提供更好的结果](https://arxiv.org/abs/2409.11402)，但它们会削弱 LLM 的大量输入上下文，而这很重要，因为
    LLM 的上下文长度有限（目前如此）。
- en: Cross-attention-based adapters don’t take away from the LLM’s context but require
    a large number of parameters to achieve good quality.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于交叉注意力的适配器不会削弱 LLM 的上下文，但需要大量的参数才能达到良好的质量。
- en: VLM Training
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLM 训练
- en: With the architecture sorted out, let’s dive into training.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 架构确定后，让我们深入探讨训练过程。
- en: Firstly, note that VLMs aren’t trained from scratch (although we think it’s
    only a matter of time) but are built on pre-trained LLMs and image encoders.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请注意，VLM 不是从头开始训练的（尽管我们认为这只是时间问题），而是基于预训练的 LLM 和图像编码器构建的。
- en: Using these pre-trained models, we fine-tune our VLM in multimodal text and
    image data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些预训练模型，我们在多模态文本和图像数据上对 VLM 进行微调。
- en: 'This process involves two steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程包括两个步骤：
- en: Pre-training
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练
- en: 'Alignment: SFT + RL (optional)'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对齐：SFT + RL（可选）
- en: '![](../Images/7ea2b804900e8b6219c18579ac3fc28a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ea2b804900e8b6219c18579ac3fc28a.png)'
- en: Training procedure of VLMs (Image by Author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: VLM 训练过程（图像来源：作者）
- en: Notice how these stages resemble LLM training?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些阶段如何类似于 LLM 的训练？
- en: This is because the two processes are similar in concept. Let’s take a brief
    look at these stages.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为这两个过程在概念上是相似的。让我们简要回顾一下这些阶段。
- en: VLM Pre-training
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLM 预训练
- en: 'Here’s what we want to achieve at this stage:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在这个阶段希望实现的目标：
- en: Link the text and image modalities together (remember that our model includes
    an adapter we haven’t trained before).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本和图像模态连接起来（记住，我们的模型包含一个我们之前没有训练过的适配器）。
- en: Load world knowledge into our model (the images have a lot of specifics, for
    one, OCR skills).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将世界知识加载到我们的模型中（这些图像包含许多细节，例如 OCR 技能）。
- en: 'There are three types of data used in pre-training VLMs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练 VLM 时使用了三种类型的数据：
- en: '**Interleaved Pre-training**: This mirrors the LLM pre-training phase, where
    we teach the model to perform the next token prediction task by feeding it web
    documents. With VLM pre-training, we pick web documents with images and train
    the model to predict text. The key difference here is that a VLM considers both
    the text and the images on the page. Such data is easy to come by, so this type
    of pre-training isn’t hard to scale up. However, the data quality isn’t great,
    and boosting it proves to be a tough job.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替预训练**：这与 LLM 的预训练阶段相似，在这个阶段，我们通过输入网络文档来训练模型执行下一个 token 预测任务。对于 VLM 的预训练，我们选择带有图像的网络文档，训练模型预测文本。这里的关键区别在于，VLM
    会同时考虑页面上的文本和图像。这样的数据很容易获得，因此这种类型的预训练不难扩展。然而，数据质量并不高，提升其质量证明是一项艰巨的任务。'
- en: '![](../Images/a9640b861d6f47a20c1846f5e1cabe28.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9640b861d6f47a20c1846f5e1cabe28.png)'
- en: Interleaved Pre-training dataset (Image by Author)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 交替预训练数据集（图片由作者提供）
- en: '**Image-Text Pairs Pre-training**: We train the model to perform one specific
    task: captioning images. You need a large corpus of images with relevant descriptions
    to do that. This approach is more popular because many such corpora are used to
    train other models (text-to-image generation, image-to-text retrieval).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**图像-文本配对预训练**：我们训练模型执行一个特定任务：为图像生成标题。你需要一个包含相关描述的大量图像数据集来完成这项任务。这种方法更为流行，因为许多此类数据集被用于训练其他模型（如文本生成图像、图像到文本检索）。'
- en: '![](../Images/eda08149f9e808bb81b4be9dac331113.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eda08149f9e808bb81b4be9dac331113.png)'
- en: Image-Text Pairs Pre-training dataset (Image by Author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图像-文本配对预训练数据集（图片由作者提供）
- en: '**Instruct-Based Pre-training**: During inference, we’ll feed the model images
    and text. Why not train the model this way from the start? This is precisely what
    instruct-based pre-training does: It trains the model on a massive dataset of
    image-instruct-answer triplets, even if the data isn’t always perfect.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于指令的预训练**：在推理过程中，我们将图像和文本输入模型。为什么不从一开始就用这种方式训练模型呢？这正是基于指令的预训练所做的：它在一个庞大的图像-指令-答案三元组数据集上训练模型，即使数据并不总是完美的。'
- en: '![](../Images/c5885ad9b34e987c993aabbdfe139b97.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5885ad9b34e987c993aabbdfe139b97.png)'
- en: Instruct-Based Pre-training dataset (Image by Author)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于指令的预训练数据集（图片由作者提供）
- en: How much data is needed to train a VLM model properly is a complex question.
    At this stage, the required dataset size can vary from a few million to several
    billion (thankfully, not a trillion!) samples.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确训练一个 VLM 模型需要多少数据是一个复杂的问题。在这一阶段，所需的数据集大小可以从几百万到几十亿（幸运的是，不是上万亿！）个样本不等。
- en: Our team used instruct-based pre-training with a few million samples. However,
    we believe interleaved pre-training has great potential, and we’re actively working
    in that direction.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们团队使用了基于指令的预训练，样本量为几百万。然而，我们相信交替预训练具有巨大的潜力，我们正在积极朝着这个方向努力。
- en: VLM Alignment
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VLM 对齐
- en: Once pre-training is complete, it’s time to start on alignment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预训练完成，就可以开始对齐阶段了。
- en: It comprises SFT training and an optional RL stage. Since we only have the SFT
    stage, I’ll focus on that.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括 SFT 训练和可选的 RL 阶段。由于我们只有 SFT 阶段，我将重点介绍这个。
- en: Still, recent papers (like [this](https://arxiv.org/abs/2405.17220) and [this](https://arxiv.org/abs/2407.21783))
    often include an RL stage on top of VLM, which uses the same methods as for LLMs
    (DPO and various modifications differing by the first letter in the method name).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，最近的论文（例如 [这篇](https://arxiv.org/abs/2405.17220) 和 [这篇](https://arxiv.org/abs/2407.21783)）通常在
    VLM 上加入 RL 阶段，使用与 LLM 相同的方法（DPO 和方法名称首字母的各种修改）。
- en: Anyway, back to SFT.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，回到 SFT。
- en: Strictly speaking, this stage is similar to instruct-based pre-training.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，这一阶段与基于指令的预训练相似。
- en: The distinction lies in our focus on high-quality data with proper response
    structure, formatting, and strong reasoning capabilities.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于我们注重高质量的数据，具备适当的响应结构、格式化和强大的推理能力。
- en: This means that the model must be able to understand the image and make inferences
    about it. Ideally, it should respond equally well to text instructs without images,
    so we’ll also add high-quality text-only data to the mix.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型必须能够理解图像并对其进行推断。理想情况下，它应对没有图像的文本指令做出同样好的响应，因此我们还将添加高质量的纯文本数据。
- en: Ultimately, this stage’s data typically ranges between hundreds of thousands
    to a few million examples. In our case, the number is somewhere in the six digits.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这个阶段的数据通常在数十万到几百万个样本之间。在我们的案例中，这个数字大约在六位数。
- en: Quality Evaluation
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 质量评估
- en: 'Let’s discuss the methods for evaluating the quality of VLMs. We use two approaches:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下评估VLM质量的方法。我们使用两种方法：
- en: Calculate metrics on open-source benchmarks.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算开源基准上的指标。
- en: Compare the models using side-by-side (SBS) evaluations, where an assessor compares
    two model responses and chooses the better one.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过并排（SBS）评估来比较模型，在这种评估中，评估者比较两个模型的响应并选择更好的一个。
- en: The first method allows us to measure surrogate metrics (like accuracy in classification
    tasks) on specific subsets of data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法允许我们在特定的数据子集上测量代理指标（例如分类任务中的准确性）。
- en: However, since most benchmarks are in English, they can’t be used to compare
    models trained in other languages, like German, French, Russian, etc.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于大多数基准测试都是英语的，因此它们不能用于比较其他语言（如德语、法语、俄语等）训练的模型。
- en: While translation can be used, the errors introduced by translation models make
    the results unreliable.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以使用翻译，但翻译模型引入的错误使得结果不可靠。
- en: The second approach allows for a more in-depth analysis of the model but requires
    meticulous (and expensive) manual data annotation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法可以更深入地分析模型，但需要细致（且昂贵）的手动数据标注。
- en: Our model is bilingual and can respond in both English and Russian. Thus, we
    can use English open-source benchmarks and run side-by-side comparisons.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型是双语的，能够用英语和俄语进行响应。因此，我们可以使用英语开源基准并进行并排比较。
- en: 'We trust this method and invest a lot in it. Here’s what we ask our assessors
    to evaluate:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们信任这种方法并在其中投入了大量资金。以下是我们要求评估者评估的内容：
- en: Grammar
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语法
- en: Readability
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可读性
- en: Comprehensiveness
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整性
- en: Relevance to the instruct
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与指令的相关性
- en: Errors (logical and factual)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误（逻辑错误和事实错误）
- en: Hallucinations
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉
- en: We strive to evaluate a complete and diverse subset of our model’s skills.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们力求评估模型技能的一个完整且多样的子集。
- en: The following pie chart illustrates the distribution of tasks in our SbS evaluation
    bucket.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下饼图展示了我们SbS评估任务的分配情况。
- en: '![](../Images/608427413661c3c7cc8a8694a629a64b.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/608427413661c3c7cc8a8694a629a64b.png)'
- en: Distribution of tasks for quality evaluation (Image by Author)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 质量评估任务分配（图片来源：作者）
- en: This summarizes the overview of VLM fundamentals and how one can train a model
    and evaluate its quality.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了VLM基础知识的概述，以及如何训练一个模型并评估其质量。
- en: Pipeline Architecture
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流水线架构
- en: This spring, we added multimodality to Neuro, an AI-powered search product,
    allowing users to ask questions using text and images.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 今年春天，我们为Neuro（一个由AI驱动的搜索产品）增加了多模态功能，允许用户通过文本和图像提问。
- en: Until recently, its underlying technology wasn’t truly multimodal.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，它的底层技术还不是真正的多模态。
- en: Here’s what this pipeline looked like before.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的是这个流水线之前的样子。
- en: '![](../Images/5d139ed87d58cc9543ac873ac974c51e.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d139ed87d58cc9543ac873ac974c51e.png)'
- en: Pipeline architecture (Image by Author)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线架构（图片来源：作者）
- en: This diagram seems complex, but it’s straightforward once you break it down
    into steps.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图看起来很复杂，但一旦将其分解成步骤后，实际上是直观的。
- en: Here’s what the process used to look like
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个过程之前的样子
- en: The user submits an image and a text query.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户提交一张图像和一个文本查询。
- en: We send the image to our visual search еngine, which would return a wealth of
    information about the image (tags, recognized text, information card).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将图像发送到我们的视觉搜索引擎，后者会返回关于该图像的大量信息（标签、识别的文本、信息卡片）。
- en: We formulate a text query using a rephraser (a fine-tuned LLM) with this information
    and the original query.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用改写器（一个微调的LLM）结合这些信息和原始查询来构造文本查询。
- en: With the rephrased text query, we use Yandex Search to retrieve relevant documents
    (or excerpts, which we call infocontext).
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用改写后的文本查询，我们使用Yandex搜索来检索相关文档（或摘录，我们称之为信息上下文）。
- en: Finally, with all this information (original query, visual search information,
    rephrased text query, and info context), we generate the final response using
    a generator model (another fine-tuned LLM).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，利用所有这些信息（原始查询、视觉搜索信息、重述的文本查询和信息上下文），我们通过生成器模型（另一个经过微调的LLM）生成最终响应。
- en: Done!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 完成！
- en: As you can see, we used to rely on two unimodal LLMs and our visual search engine.
    This solution worked well on a small sample of queries but had limitations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们过去依赖两个单模态LLM和我们的视觉搜索引擎。这个方案在少量查询样本中表现良好，但也有其局限性。
- en: Below is an example (albeit slightly exaggerated) of how things could go wrong.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个例子（虽然有些夸张），说明事情可能如何出错。
- en: '![](../Images/2d65efa3148887ebeeb8f1a47fac7994.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d65efa3148887ebeeb8f1a47fac7994.png)'
- en: The problem with two unimodal LLMs (Image by Author)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 两个单模态LLM的问题（图像来源：作者）
- en: Here, the rephraser receives the output of the visual search service and simply
    doesn’t understand the user’s original intent.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，重述器接收视觉搜索服务的输出，但根本无法理解用户的原始意图。
- en: In turn, the LLM model, which knows nothing about the image, generates an incorrect
    search query, getting tags about the pug and the apple simultaneously.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，LLM模型对图像一无所知，生成了一个错误的搜索查询，同时获取了关于哈巴狗和苹果的标签。
- en: To improve the quality of our multimodal response and allow users to ask more
    complex questions, we introduced a VLM into our architecture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们的多模态响应质量并允许用户提出更复杂的问题，我们将VLM引入了我们的架构中。
- en: 'More specifically, we made two major modifications:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们进行了两项主要修改：
- en: We replaced the LLM rephraser with a VLM rephraser. Essentially, we started
    feeding the original image to the rephraser’s input on top of the text from the
    visual search engine.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用VLM重述器替代了LLM重述器。从本质上讲，我们开始将原始图像与视觉搜索引擎的文本一起输入到重述器中。
- en: We added a separate VLM captioner to the pipeline. This model provides an image
    description, which we use as info context for the final generator.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在流程中添加了一个独立的VLM字幕生成器。这个模型提供了图像描述，我们将其作为最终生成器的信息上下文。
- en: You might wonder
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问
- en: Why not make the generator itself VLM-based?
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么不让生成器本身基于VLM呢？
- en: That’s a good idea!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个好主意！
- en: But there’s a catch.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一个问题。
- en: Our generator training inherits from Neuro’s text model, which is frequently
    updated.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的生成器训练继承自Neuro的文本模型，该模型经常更新。
- en: To update the pipeline faster and more conveniently, it was much easier for
    us to introduce a separate VLM block.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更快、更方便地更新流程，我们引入了一个独立的VLM模块，这样做要容易得多。
- en: 'Plus, this setup works just as well, which is shown below:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这个设置同样有效，下面展示了结果：
- en: '![](../Images/a122d37d30387dcc9720cbaae7ffc16d.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a122d37d30387dcc9720cbaae7ffc16d.png)'
- en: Using VLM in AI-powered search (Image by Author)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI驱动的搜索中使用VLM（图像来源：作者）
- en: Training VLM rephraser and VLM captioner are two separate tasks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练VLM重述器和VLM字幕生成器是两个独立的任务。
- en: For this, we use mentioned earlierse VLM, as mentioned e for thise-tuned it
    for these specific tasks.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们使用前面提到的VLM，并对其进行了针对这些特定任务的微调。
- en: Fine-tuning these models required collecting separate training datasets comprising
    tens of thousands of samples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 微调这些模型需要收集成千上万的独立训练数据集。
- en: We also had to make significant changes to our infrastructure to make the pipeline
    computationally efficient.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须对基础设施进行重大改进，以提高流程的计算效率。
- en: Gauging the Quality
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估质量
- en: 'Now for the grand question:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入重要的问题：
- en: Did introducing a VLM to a fairly complex pipeline improve things?
  id: totrans-177
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 引入VLM到这个相对复杂的流程中，真的有改善吗？
- en: In short, yes, it did!
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，是的，确实如此！
- en: We ran side-by-side tests to measure the new pipeline’s performance and compared
    our previous LLM framework with the new VLM one.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了并行测试，以衡量新流程的性能，并将我们之前的LLM框架与新的VLM框架进行了比较。
- en: This evaluation is similar to the one discussed earlier for the core technology.
    However, in this case, we use a different set of images and queries more aligned
    with what users might ask.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个评估与之前讨论的核心技术评估类似。然而，在这种情况下，我们使用了一组不同的图像和查询，这些查询更贴近用户可能提出的需求。
- en: Below is the approximate distribution of clusters in this bucket.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个桶中聚类的大致分布。
- en: '![](../Images/29abfb15a93da3ac9445af9b1a49ada7.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29abfb15a93da3ac9445af9b1a49ada7.png)'
- en: Cluster distribution (Image by Author)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分布（图像来源：作者）
- en: Our offline side-by-side evaluation shows that we’ve substantially improved
    the quality of the final response.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的离线并行评估显示，我们大大提高了最终响应的质量。
- en: The VLM pipeline noticeably increases the response quality and covers more user
    scenarios.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: VLM管道显著提高了响应质量，涵盖了更多用户场景。
- en: '![](../Images/9a70b5dbf5bb31451cad96af55767ae4.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a70b5dbf5bb31451cad96af55767ae4.png)'
- en: Accuracy of VLM vs LLM in Neuro (Image by Author)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: VLM与LLM在神经网络中的准确性对比（图源：作者）
- en: We also wanted to test the results on a live audience to see if our users would
    notice the technical changes that we believe would improve the product experience.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望在真实观众中测试这些结果，看看我们的用户是否会注意到我们认为能够改善产品体验的技术变化。
- en: 'So, we conducted an online split test, comparing our LLM pipeline to the new
    VLM pipeline. The preliminary results show the following change:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们进行了一个在线拆分测试，将我们的LLM管道与新的VLM管道进行了对比。初步结果显示了以下变化：
- en: The number of instructs that include an image increased by 17%.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含图片的指令数量增加了17%。
- en: The number of sessions (the user entering multiple queries in a row) saw an
    uptick of 4.5%.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会话数量（用户连续输入多个查询）增长了4.5%。
- en: To reiterate what was said above, we firmly believe that VLMs are the future
    of computer vision models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下之前的观点，我们坚信VLMs是计算机视觉模型的未来。
- en: VLMs are already capable of solving many out-of-the-box problems. With a bit
    of fine-tuning, they can absolutely deliver state-of-the-art quality.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: VLMs已经能够解决许多开箱即用的问题。经过一点微调，它们完全可以提供最先进的质量。
- en: Thanks for reading!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
