<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Running Large Language Models Privately</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Running Large Language Models Privately</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30">https://towardsdatascience.com/running-large-language-models-privately-a-comparison-of-frameworks-models-and-costs-ac33cfe3a462?source=collection_archive---------0-----------------------#2024-10-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8f2d" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A comparison of frameworks, models, and costs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Robert Corwin" class="l ep by dd de cx" src="../Images/ea7f47832ae17969ba4682c5b59129aa.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*86_Q3s_GaoiuqeaL"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@robert.corwin?source=post_page---byline--ac33cfe3a462--------------------------------" rel="noopener follow">Robert Corwin</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ac33cfe3a462--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="0d8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Robert Corwin, CEO, Austin Artificial Intelligence</p><p id="f8c0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">David Davalos, ML Engineer, Austin Artificial Intelligence</p><p id="4013" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Oct 24, 2024</p><p id="00c6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">Large Language Models (LLMs) have rapidly transformed the technology landscape, but security concerns persist, especially with regard to sending private data to external third parties. In this blog entry, we dive into the options for deploying Llama models locally and privately, that is, on one’s own computer. We get Llama 3.1 running locally and investigate key aspects such as speed, power consumption, and overall performance across different versions and frameworks. Whether you’re a technical expert or simply curious about what’s involved, you’ll find insights into local LLM deployment. For a quick overview, non-technical readers can skip to our summary tables, while those with a technical background may appreciate the deeper look into specific tools and their performance.</em></p><p id="abac" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="nf">All images by authors unless otherwise noted. The authors and Austin Artificial Intelligence, their employer, have no affiliations with any of the tools used or mentioned in this article.</em></p><h1 id="be79" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Key Points</h1><p id="cd0c" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk"><strong class="ml fr">Running LLMs:</strong> LLM models can be downloaded and run locally on private servers using tools and frameworks widely available in the community. While running the most powerful models require rather expensive hardware, smaller models can be run on a laptop or desktop computer.</p><p id="aa5b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Privacy and Customizability:</strong> Running LLMs on private servers provides enhanced privacy and greater control over model settings and usage policies.</p><p id="0ae6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Model Sizes:</strong> Open-source Llama models come in various sizes. For example, Llama 3.1 comes in 8 billion, 70 billion, and 405 billion parameter versions. A “parameter” is roughly defined as the weight on one node of the network. More parameters increase model performance at the expense of size in memory and disk.</p><p id="31bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Quantization:</strong> Quantization saves memory and disk space by essentially “rounding” weights to fewer significant digits — at the expense of accuracy. Given the vast number of parameters in LLMs, quantization is very valuable for reducing memory usage and speeding up execution.</p><p id="91af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Costs:</strong> Local implementations, referencing GPU energy consumption, demonstrate cost-effectiveness compared to cloud-based solutions.</p><h1 id="484e" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Privacy and Reliability as Motivations</h1><p id="3e44" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">In one of our <a class="af oh" href="https://www.austinai.io/blog/the-revolutions-behind-chatgpt-and-llms" rel="noopener ugc nofollow" target="_blank">previous entries</a> we explored the key concepts behind LLMs and how they can be used to create customized chatbots or tools with frameworks such as <strong class="ml fr">Langchain</strong> (see <strong class="ml fr">Fig. 1</strong>). In such schemes, while data can be protected by using synthetic data or obfuscation, we still must send data externally a third party and have no control over any changes in the model, its policies, or even its availability. A solution is simply to run an LLM on a private server (see <strong class="ml fr">Fig. 2</strong>). This approach ensures full privacy and mitigates the dependency on external service providers.</p><p id="09a9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Concerns about implementing LLMs privately include costs, power consumption, and speed. In this exercise, we get LLama 3.1 running while varying the 1. framework (tools) and 2. degrees of <strong class="ml fr">quantization</strong> and compare the ease of use of the frameworks, the resultant performance in terms of speed, and power consumption. Understanding these trade-offs is essential for anyone looking to harness the full potential of AI while retaining control over their data and resources.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj ok"><img src="../Images/4086a3837bbbe117393bc836e2bf9bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bcDvwniT5DAAD7Tr.png"/></div></div></figure><p id="ec8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 1</strong> Diagram illustrating a typical backend setup for chatbots or tools, with ChatGPT (or similar models) functioning as the natural language processing engine. This setup relies on prompt engineering to customize responses.”</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj ok"><img src="../Images/b7f1e437550e19bfc9ca68a0e1549779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DkjrV9LEqOeeLJlE.png"/></div></div></figure><p id="3fbd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 2</strong> Diagram of a fully private backend configuration where all components, including the large language model, are hosted on a secure server, ensuring complete control and privacy.</p><h1 id="f5ac" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Quantization and GGUF Files</h1><p id="9124" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Before diving into our impressions of the tools we explored, let’s first discuss quantization and the <em class="nf">GGUF</em> format.</p><p id="fe45" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Quantization is a technique used to reduce the size of a model by converting weights and biases from high-precision floating-point values to lower-precision representations. LLMs benefit greatly from this approach, given their vast number of parameters. For example, the largest version of Llama 3.1 contains a staggering 405 billion parameters. Quantization can significantly reduce both memory usage and execution time, making these models more efficient to run across a variety of devices. For an in-depth explanation and nomenclature of quantization types, check out this <a class="af oh" href="https://huggingface.co/docs/hub/en/gguf" rel="noopener ugc nofollow" target="_blank">great introduction</a>. A conceptual overview can also be found <a class="af oh" href="https://www.maartengrootendorst.com/blog/quantization/" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="9236" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <a class="af oh" href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md" rel="noopener ugc nofollow" target="_blank"><em class="nf">GGUF</em> format</a> is used to store LLM models and has recently gained popularity for distributing and running quantized models. It is optimized for fast loading, reading, and saving. Unlike tensor-only formats, GGUF also stores model metadata in a standardized manner, making it easier for frameworks to support this format or even adopt it as the norm.</p><h1 id="20cc" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Tools and Models Analyzed</h1><p id="ecdb" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We explored four tools to run Llama models locally:</p><ul class=""><li id="073b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ow ox oy bk"><a class="af oh" href="https://huggingface.co/docs/transformers/en/index" rel="noopener ugc nofollow" target="_blank">HuggingFace’s transformers library</a> and <a class="af oh" href="https://huggingface.co/docs/hub/en/index" rel="noopener ugc nofollow" target="_blank">Hub</a></li><li id="72eb" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk"><a class="af oh" href="https://docs.vllm.ai/en/latest/" rel="noopener ugc nofollow" target="_blank">vLLM</a></li><li id="99c1" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk"><a class="af oh" href="https://github.com/ggerganov/llama.cpp" rel="noopener ugc nofollow" target="_blank">llama.cpp</a></li><li id="706e" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk"><a class="af oh" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank">Ollama</a></li></ul><p id="facb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our primary focus was on llama.cpp and Ollama, as these tools allowed us to deploy models quickly and efficiently right out of the box. Specifically, we explored their speed, energy cost, and overall performance. For the models, we primarily analyzed the quantized 8B and 70B Llama 3.1 versions, as they ran within a reasonable time frame.</p><h1 id="5c7b" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">First Impressions and Installation</h1><h2 id="814b" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">HuggingFace</h2><p id="986c" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">HuggingFace’s transformers library and Hub are well-known and widely used in the community. They offer a wide range of models and tools, making them a popular choice for many developers. Its installation generally does not cause major problems once a proper environment is set up with Python. At the end of the day, the biggest benefit of Huggingface was its online hub, which allows for easy access to quantized models from many different providers. On the other hand, using the transformers library directly to load models, especially quantized ones, was rather tricky. Out of the box, the library seemingly directly dequantizes models, taking a great amount of ram and making it unfeasible to run in a local server.</p><p id="dd4a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Although Hugging Face <a class="af oh" href="https://huggingface.co/docs/transformers/en/main_classes/quantization" rel="noopener ugc nofollow" target="_blank">supports 4- and 8-bit</a> quantization and dequantization with <a class="af oh" href="https://github.com/bitsandbytes-foundation/bitsandbytes" rel="noopener ugc nofollow" target="_blank"><em class="nf">bitsandbytes</em></a>, our initial impression is that further optimization is needed. Efficient inference may simply not be its primary focus. Nonetheless, Hugging Face offers excellent documentation, a large community, and a robust framework for model training.</p><h2 id="9512" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">vLLM</h2><p id="fce4" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Similar to Hugging Face, vLLM is easy to install with a properly configured Python environment. However, support for GGUF files is still highly experimental. While we were able to quickly set it up to run 8B models, scaling beyond that proved challenging, despite the excellent documentation.</p><p id="ee9b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Overall, we believe vLLM has great potential. However, we ultimately opted for the llama.cpp and Ollama frameworks for their more immediate compatibility and efficiency. To be fair, a more thorough investigation could have been conducted here, but given the immediate success we found with other libraries, we chose to focus on those.</p><h2 id="ee57" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">Ollama</h2><p id="87f8" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We found Ollama to be fantastic. Our initial impression is that it is a user-ready tool for inferring Llama models locally, with an ease-of-use that works right out of the box. <a class="af oh" href="https://ollama.com/download" rel="noopener ugc nofollow" target="_blank">Installing it</a> for Mac and Linux users is straightforward, and a Windows version is currently in preview. Ollama automatically detects your hardware and manages model offloading between CPU and GPU seamlessly. It features its own model library, automatically downloading models and supporting GGUF files. Although its speed is slightly slower than llama.cpp, it performs well even on CPU-only setups and laptops.</p><p id="c363" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For a quick start, once installed, running <code class="cx pv pw px py b">ollama run llama3.1:latest</code> will load the latest 8B model in conversation mode directly from the command line.</p><p id="3a3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One downside is that customizing models can be somewhat impractical, especially for advanced development. For instance, even adjusting the temperature requires creating a new chatbot instance, which in turn loads an installed model. While this is a minor inconvenience, it does facilitate the setup of customized chatbots — including other parameters and roles — within a single file. Overall, we believe Ollama serves as an effective local tool that mimics some of the key features of cloud services.</p><p id="f104" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is worth noting that Ollama runs as a service, at least on Linux machines, and offers handy, simple commands for monitoring which models are running and where they’re offloaded, with the ability to stop them instantly if needed. One challenge the community has faced is configuring certain aspects, such as where models are stored, which requires technical knowledge of Linux systems. While this may not pose a problem for end-users, it perhaps slightly hurts the tool’s practicality for advanced development purposes.</p><h2 id="8f5b" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">llama.cpp</h2><p id="9af4" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">llama.cpp emerged as our favorite tool during this analysis. As stated in its <a class="af oh" href="https://github.com/ggerganov/llama.cpp/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">repository</a>, it is designed for running inference on large language models with minimal setup and cutting-edge performance. Like Ollama, it supports offloading models between CPU and GPU, though this is not available straight out of the box. To enable GPU support, you must compile the tool with the appropriate flags — specifically, <code class="cx pv pw px py b">GGML_CUDA=on</code>. We recommend using the latest version of the CUDA toolkit, as older versions may not be compatible.</p><p id="83a2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The tool can be installed as a standalone by pulling from the repository and compiling, which provides a convenient command-line client for running models. For instance, you can execute <code class="cx pv pw px py b">llama-cli -p 'you are a useful assistant' -m Meta-Llama-3-8B-Instruct.Q8_0.gguf -cnv</code>. Here, the final flag enables conversation mode directly from the command line. llama-cli offers various customization options, such as adjusting the context size, repetition penalty, and temperature, and it also supports GPU offloading options.</p><p id="647a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Similar to Ollama, llama.cpp has a Python binding which can be installed via <code class="cx pv pw px py b">pip install llama-cpp-python</code>. This Python library allows for significant customization, making it easy for developers to tailor models to specific client needs. However, just as with the standalone version, the Python binding requires compilation with the appropriate flags to enable GPU support.</p><p id="c2c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One minor downside is that the tool doesn’t yet support automatic CPU-GPU offloading. Instead, users need to manually specify how many layers to offload onto the GPU, with the remainder going to the CPU. While this requires some fine-tuning, it is a straightforward, manageable step.</p><p id="a07e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For environments with multiple GPUs, like ours, llama.cpp provides two split modes: <em class="nf">row mode</em> and <em class="nf">layer mode</em>. In row mode, one GPU handles small tensors and intermediate results, while in layer mode, layers are divided across GPUs. In our tests, both modes delivered comparable performance (see analysis below).</p><h1 id="e2c4" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Our Analysis</h1><p id="c850" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">► <em class="nf">From now on, results concern only llama.cpp and Ollama.</em></p><p id="15dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We conducted an analysis of the speed and power consumption of the 70B and 8B Llama 3.1 models using Ollama and llama.cpp. Specifically, we examined the speed and power consumption per token for each model across various quantizations available in <a class="af oh" href="https://huggingface.co/collections/QuantFactory/llama-31-66a36c08cd3aa07108e10dfe" rel="noopener ugc nofollow" target="_blank">Quant Factory</a>.</p><p id="7fba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To carry out this analysis, we developed a small application to evaluate the models once the tool was selected. During inference, we recorded metrics such as speed (tokens per second), total tokens generated, temperature, number of layers loaded on GPUs, and the quality rating of the response. Additionally, we measured the power consumption of the GPU during model execution. A script was used to monitor GPU power usage (via <code class="cx pv pw px py b">nvidia-smi</code>) immediately after each token was generated. Once inference concluded, we computed the average power consumption based on these readings. Since we focused on models that could fully fit into GPU memory, we only measured GPU power consumption.</p><p id="f712" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, the experiments were conducted with a variety of prompts to ensure different output sizes, thus, the data encompass a wide range of scenarios.</p><h1 id="659e" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Hardware and Software Setup</h1><p id="1595" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We used a pretty decent server with the following features:</p><ul class=""><li id="7baf" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ow ox oy bk">CPU: AMD Ryzen Threadripper PRO 7965WX 24-Cores @ 48x 5.362GHz.</li><li id="6a94" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">GPU: <strong class="ml fr">2x</strong> NVIDIA GeForce RTX 4090.</li><li id="ee3e" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">RAM: 515276MiB-</li><li id="ed23" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">OS: Pop 22.04 jammy.</li><li id="c746" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">Kernel: x86_64 Linux 6.9.3–76060903-generic.</li></ul><p id="6e15" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The retail cost of this setup was somewhere around $15,000 USD. We chose such a setup because it is a decent server that, while nowhere near as powerful as dedicated, high-end AI servers with 8 or more GPUs, is still quite functional and representative of what many of our clients might choose. We have found many clients hesitant to invest in high-end servers out of the gate, and this setup is a good compromise between cost and performance.</p><h1 id="b1cf" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Speed</h1><p id="87f3" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Let us first focus on speed. Below, we present several <a class="af oh" href="https://en.wikipedia.org/wiki/Box_plot" rel="noopener ugc nofollow" target="_blank">box-whisker plots</a> depicting speed data for several quantizations. The name of each model starts with its quantization level; so for example “Q4” means a 4-bit quantization. Again, a LOWER quantization level rounds more, reducing size and quality but increasing speed.</p><p id="f63d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">► <em class="nf">Technical Issue 1 (A Reminder of Box-Whisker Plots): Box-whisker plots display the median, the first and third quartiles, as well as the minimum and maximum data points. The whiskers extend to the most extreme points not classified as outliers, while outliers are plotted individually. Outliers are defined as data points that fall outside the range of Q1 − 1.5 × IQR and Q3 + 1.5 × IQR, where Q1 and Q3 represent the first and third quartiles, respectively. The interquartile range (IQR) is calculated as IQR = Q3 − Q1.</em></p><h2 id="1ef9" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">llama.cpp</h2><p id="18b3" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Below are the plots for llama.cpp. <strong class="ml fr">Fig. 3</strong> shows the results for all Llama 3.1 models with 70B parameters available in <a class="af oh" href="https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF-v2" rel="noopener ugc nofollow" target="_blank">QuantFactory</a>, while <strong class="ml fr">Fig. 4</strong> depicts some of the models with 8B parameters available <a class="af oh" href="https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF-v2" rel="noopener ugc nofollow" target="_blank">here</a>. 70B models can offload up to 81 layers onto the GPU while 8B models up to 33. For 70B, offloading all layers is not feasible for Q5 quantization and finer. Each quantization type includes the number of layers offloaded onto the GPU in parentheses. As expected, coarser quantization yields the best speed performance. Since row split mode performs similarly, we focus on layer split mode here.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj pz"><img src="../Images/dbb2c0ec9178da55140f4e44e00d4d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8p2mL7sNm3_wAky9.png"/></div></div></figure><p id="5130" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 3</strong> Llama 3.1 models with 70B parameters running under llama.cpp with split mode <em class="nf">layer</em>. As expected, coarser quantization provides the best speed. The number of layers offloaded onto the GPU is shown in parentheses next to each quantization type. Models with Q5 and finer quantizations do not fully fit into VRAM.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qa"><img src="../Images/ec621be986a8433692e2383f2b307f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YPnwDvkrpshTD2uy.png"/></div></div></figure><p id="5ed6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 4</strong> Llama 3.1 models with 8B parameters running under llama.cpp using split mode <em class="nf">layer</em>. In this case, the model fits within the GPU memory for all quantization types, with coarser quantization resulting in the fastest speeds. Note that high speeds are outliers, while the overall trend hovers around 20 tokens per second for Q2_K.</p><h2 id="78a1" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">Key Observations</h2><ul class=""><li id="7cdf" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne ow ox oy bk">During inference we observed some high speed events (especially in 8B Q2_K), this is where gathering data and understanding its distribution is crucial, as it turns out that those events are quite rare.</li><li id="9aec" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">As expected, coarser quantization types yield the best speed performance. This is because the model size is reduced, allowing for faster execution.</li><li id="525c" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">The results concerning 70B models that do not fully fit into VRAM must be taken with caution, as using the CPU too could cause a bottleneck. Thus, the reported speed may not be the best representation of the model’s performance in those cases.</li></ul><h2 id="f9d8" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">Ollama</h2><p id="77a8" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We executed the same analysis for Ollama. <strong class="ml fr">Fig. 5</strong> shows the results for the default Llama 3.1 and 3.2 models that Ollama automatically downloads. All of them fit in the GPU memory except for the 405B model.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qa"><img src="../Images/5cf70861d011baf06d6986701b776bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0m-wZmJEdJn-ySZk.png"/></div></div></figure><p id="2efc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 5</strong> Llama 3.1 and 3.2 models running under Ollama. These are the default models when using Ollama. All 3.1 models — specifically 405B, 70B, and 8B (labeled as “latest”) — use Q4_0 quantization, while the 3.2 models use Q8_0 (1B) and Q4_K_M (3B).</p><h2 id="68ee" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">Key Observations</h2><ul class=""><li id="250a" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne ow ox oy bk">We can compare the 70B Q4_0 model across Ollama and llama.cpp, with Ollama exhibiting a slightly slower speed.</li><li id="5379" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">Similarly, the 8B Q4_0 model is slower under Ollama compared to its llama.cpp counterpart, with a more pronounced difference — llama.cpp processes about five more tokens per second on average.</li></ul><h1 id="527f" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Summary of Analyzed Frameworks</h1><p id="e765" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">► Before discussing power consumption and rentability, let’s summarize the frameworks we analyzed up to this point.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qb"><img src="../Images/4523dc3a8ecb6b331bbae7feb7ad5a9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZ0MMI-5PAAPObYyUghiPA.png"/></div></div></figure><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qc"><img src="../Images/8cf1f9a4e23b4ddf59621e747fe2c680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ka35RKYkMZjN57HVAcqfyQ.png"/></div></div></figure><h1 id="2445" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Power Consumption and Rentability</h1><p id="5ff6" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">This analysis is particularly relevant to models that fit all layers into GPU memory, as we only measured the power consumption of two RTX 4090 cards. Nonetheless, it is worth noting that the CPU used in these tests has a <a class="af oh" href="https://www.techpowerup.com/cpu-specs/ryzen-threadripper-pro-7995wx.c3301" rel="noopener ugc nofollow" target="_blank">TDP of 350 W</a>, which provides an estimate of its power draw at maximum load. If the entire model is loaded onto the GPU, the CPU likely maintains a power consumption close to idle levels.</p><p id="22b4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To estimate energy consumption per token, we use the following parameters: <em class="nf">tokens per second</em> (NT) and <em class="nf">power drawn by both GPUs</em> (P) measured in watts. By calculating P/NT, we obtain the energy consumption per token in watt-seconds. Dividing this by 3600 gives the energy usage per token in Wh, which is more commonly referenced.</p><h2 id="6da5" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">llama.cpp</h2><p id="0078" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Below are the results for llama.cpp. <strong class="ml fr">Fig. 6</strong> illustrates the energy consumption for 70B models, while <strong class="ml fr">Fig. 7</strong> focuses on 8B models. These figures present energy consumption data for each quantization type, with average values shown in the legend.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qa"><img src="../Images/47545ef2848a549f6011535a1d58f92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LEdTRUnL0ahi7j5W.png"/></div></div></figure><p id="f29e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 6</strong> Energy per token for various quantizations of Llama 3.1 models with 70B parameters under llama.cpp. Both row and layer split modes are shown. Results are relevant only for models that fit all 81 layers in GPU memory.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qa"><img src="../Images/efa6856ace45a7e11318c546c90b8f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jFMW2a-gJttOOmpY.png"/></div></div></figure><p id="cca8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 7</strong> Energy per token for various quantizations of Llama 3.1 models with 8B parameters under llama.cpp. Both row and layer split modes are shown. All models exhibit similar average consumption.</p><h2 id="1d5b" class="pe nh fq bf ni pf pg ph nl pi pj pk no ms pl pm pn mw po pp pq na pr ps pt pu bk">Ollama</h2><p id="5a04" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We also analyzed the energy consumption for Ollama. <strong class="ml fr">Fig. 8</strong> displays results for Llama 3.1 8B (Q4_0 quantization) and Llama 3.2 1B and 3B (Q8_0 and Q4_K_M quantizations, respectively). <strong class="ml fr">Fig. 9</strong> shows separate energy consumption for the 70B and 405B models, both with Q4_0 quantization.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qa"><img src="../Images/672d7536032b24286d90785d4affa630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7abs3AEqKnXjSHYU.png"/></div></div></figure><p id="f4f4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 8</strong> Energy per token for Llama 3.1 8B (Q4_0 quantization) and Llama 3.2 1B and 3B models (Q8_0 and Q4_K_M quantizations, respectively) under Ollama.</p><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qd"><img src="../Images/056b3f9d748ce01c0acfc7af26168536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IAJEZ5eOTgfniVXh.png"/></div></div></figure><p id="5cab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Fig. 9</strong> Energy per token for Llama 3.1 70B (left) and Llama 3.1 405B (right), both using Q4_0 quantization under Ollama.</p><h1 id="35c4" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Summary of Costs</h1><p id="cdcf" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Instead of discussing each model individually, we will focus on those models that are comparable across llama.cpp and Ollama, as well of models with Q2_K quantization under llama.cpp, since it is the coarsest quantization explored here. To give a good idea of the costs, we show in the table below estimations of the energy consumption per one million generated tokens (1M) and the cost in USD. The cost is calculated based on the average electricity price in Texas, which is $0.14 per kWh according to this <a class="af oh" href="https://bkvenergy.com/blog/average-electricity-bill-texas/" rel="noopener ugc nofollow" target="_blank">source</a>. For a reference, the current <a class="af oh" href="https://openai.com/api/pricing/" rel="noopener ugc nofollow" target="_blank">pricing of GPT-4o</a> is at least of $5 USD per 1M tokens and $0.3 USD per 1M tokens for GPT-o mini.</p><h1 id="eb2a" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">llama.cpp</h1><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qe"><img src="../Images/dfbcafd4499fb5354d8e5832413108af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_i_gAsEaT9cZ9ExT19O7Q.png"/></div></div></figure><h1 id="b980" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Ollama</h1><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qf"><img src="../Images/6d75da0086d99efe5cb4edddfb50abba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjD8RdM-RP9K4xeFLivUFw.png"/></div></div></figure><h1 id="6afb" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Key Observations</h1><ul class=""><li id="4bee" class="mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne ow ox oy bk">Using Llama 3.1 70B models with Q4_0, there is not much difference in the energy consumption between llama.cpp and Ollama.</li><li id="b1d0" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">For the 8B model llama.cpp spends more energy than Ollama.</li><li id="20b1" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">Consider that the costs depicted here could be seen as a lower bound of the “bare costs” of running the models. Other costs, such as operation, maintenance, equipment costs and profit, are not included in this analysis.</li><li id="e77c" class="mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne ow ox oy bk">The estimations suggest that operating LLMs on private servers can be cost-effective compared to cloud services. In particular, comparing Llama 8B with GPT-45o mini and Llama 70B with GPT-4o models seem to be a potential good deal under the right circumstances.</li></ul><p id="19a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">► <em class="nf">Technical Issue 2 (Cost Estimation):</em> For most models, the estimation of energy consumption per 1M tokens (and its variability) is given by the “median ± IQR” prescription, where IQR stands for interquartile range. Only for the Llama 3.1 8B Q4_0 model do we use the “mean ± STD” approach, with STD representing standard deviation. These choices are not arbitrary; all models except for Llama 3.1 8B Q4_0 exhibit outliers, making the median and IQR more robust estimators in those cases. Additionally, these choices help prevent negative values for costs. In most instances, when both approaches yield the same central tendency, they provide very similar results.</p><h1 id="7052" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Final Word</h1><figure class="ol om on oo op oq oi oj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="oi oj qg"><img src="../Images/55c35bbe1cca9419622b9aa1539389d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1AHNXXRNR7f03IyS.png"/></div></div><figcaption class="qh qi qj oi oj qk ql bf b bg z dx">Image by Meta AI</figcaption></figure><p id="4a3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The analysis of speed and power consumption across different models and tools is only part of the broader picture. We observed that lightweight or heavily quantized models often struggled with reliability; hallucinations became more frequent as chat histories grew or tasks turned repetitive. This isn’t unexpected — smaller models don’t capture the extensive complexity of larger models. To counter these limitations, settings like repetition penalties and temperature adjustments can improve outputs. On the other hand, larger models like the 70B consistently showed strong performance with minimal hallucinations. However, since even the biggest models aren’t entirely free from inaccuracies, responsible and trustworthy use often involves integrating these models with additional tools, such as LangChain and vector databases. Although we didn’t explore specific task performance here, these integrations are key for minimizing hallucinations and enhancing model reliability.</p><p id="1cb5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In conclusion, running LLMs on private servers can provide a competitive alternative to LLMs as a service, with cost advantages and opportunities for customization. Both private and service-based options have their merits, and at <em class="nf">Austin Ai</em>, <strong class="ml fr">we specialize in implementing solutions that suit your needs</strong>, whether that means leveraging private servers, cloud services, or a hybrid approach.</p></div></div></div></div>    
</body>
</html>