<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Running a SOTA 7B Parameter Embedding Model on a Single GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Running a SOTA 7B Parameter Embedding Model on a Single GPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09">https://towardsdatascience.com/running-a-sota-7b-parameter-embedding-model-on-a-single-gpu-bb9b071e2238?source=collection_archive---------5-----------------------#2024-08-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="da96" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Running Qwen2 on SageMaker</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Szymon Palucha" class="l ep by dd de cx" src="../Images/37a33166ccb5b427d8aaf545e3376d59.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hjKCzYbs-m_KG7wmYibX1A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@paluchasz?source=post_page---byline--bb9b071e2238--------------------------------" rel="noopener follow">Szymon Palucha</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bb9b071e2238--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="b0ab" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this post I will explain how to run a state-of-the-art 7B parameter LLM based embedding model on just a single 24GB GPU. I will cover some theory and then show how to run it with the HuggingFace Transformers library in Python in just a few lines of code!</p><p id="fc94" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The model that we will run in the <a class="af ne" href="https://arxiv.org/abs/2407.10671" rel="noopener ugc nofollow" target="_blank">Qwen2 </a>open source model (<a class="af ne" href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct" rel="noopener ugc nofollow" target="_blank">Alibaba-NLP/gte-Qwen2–7B-instruct</a>) which was released in June 2024, and at the time of finishing this article is in 4th place on the <a class="af ne" href="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener ugc nofollow" target="_blank">Massive Text Embeddings Benchmark</a> on HuggingFace.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/ae9c63c85dcebf04f2477d219fbc7493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oiRvo-RSXDoyU8Teb0Ld5g.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">ScreenShot of the <a class="af ne" href="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener ugc nofollow" target="_blank">MTEB Leaderboard</a> on HuggingFace from July 2024. The model was in 1st place in June 2024 and has subsequently dropped to 4th place. This shows the pace of development in AI these days!</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="57f3" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Theoretical Memory Requirements</h1><h2 id="04bc" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Loading a Model</h2><p id="d739" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">The amount of memory required to load a machine learning model (e.g. LLM or Embedding model) can be calculated from the number of its parameters.</p><p id="d016" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For example, a 7B parameter model in fp32 (float32 precision), means that we need to store 7B numbers in 32-bit precision in order to initialise the model in memory and be able to use it. Therefore, recalling that there are 8 bits in one byte, the memory needed to load the model is</p><blockquote class="py pz qa"><p id="9998" class="mi mj qb mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Memory of a 7B param model in fp32 = 7B * 32 bits = 7B * 32 / 8 bytes = 28B bytes = <strong class="mk fr">28GB</strong>.</p></blockquote><p id="18fc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So in order to run this model we need at least 28GB of GPU memory. In fact there is also some additional overhead as described in <a class="af ne" href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm" rel="noopener ugc nofollow" target="_blank">this post</a>. As a result to run this model in full precision we cannot use smaller and cheaper GPU’s which have 16GB or 24GB of memory.</p><p id="c8df" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So without a more powerful GPU such as <a class="af ne" href="https://www.nvidia.com/en-gb/data-center/a100/" rel="noopener ugc nofollow" target="_blank">NVIDIA’s A100</a>, what alternatives do we have? It turns out there a few different techniques to reduce the memory requirement. The simplest one is to reduce the precision of the parameters. Most models can now be used in <strong class="mk fr">half precision</strong> without any significant loss in accuracy. The memory required to load a model in fp16 or <a class="af ne" href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format" rel="noopener ugc nofollow" target="_blank">bf16</a> is</p><blockquote class="py pz qa"><p id="842e" class="mi mj qb mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Memory of a 7B param model in fp16 = 7B * 16 bits = 7B * 16 / 8 bytes = 14B bytes = <strong class="mk fr">14GB</strong>.</p></blockquote><p id="420a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Whilst this is already good enough to load the model on a 24GB GPU, we would still struggle to run it on a 16GB GPU, due to the additional overheads and extra requirements during inference.</p><p id="8bbe" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Reducing the precision anymore than this naively would already start impacting performance but there is a technique called <strong class="mk fr">quantisation</strong> which is able to reduce the precision even further (such as into 8bits or 4bits) without a significant drop in accuracy. Recent research in LLMs has even shown the possibility of using 1 bit precision (actually, log_2(3) = 1.58 bits) known as <a class="af ne" href="https://arxiv.org/abs/2402.17764" rel="noopener ugc nofollow" target="_blank">1-bit LLMs</a>. The parameters of these models can only take the values of 1, -1 or 0!</p><p id="612a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To read up more on these topics I would recommend:</p><ul class=""><li id="66cc" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qc qd qe bk">Understanding floating point representations from the <a class="af ne" href="https://math.libretexts.org/Workbench/Numerical_Methods_with_Applications_(Kaw)/1%3A_Introduction/1.05%3A_Floating-Point_Binary_Representation_of_Numbers#:~:text=A%20machine%20stores%20floating%2Dpoint,the%20magnitude%20of%20the%20mantissa." rel="noopener ugc nofollow" target="_blank">following tutorial</a>.</li><li id="0097" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk"><a class="af ne" href="https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/" rel="noopener ugc nofollow" target="_blank">Quantisation Fundamentals with Huggingface</a> free course from DeepLearning.AI.</li></ul><h1 id="6717" class="og oh fq bf oi oj qk gq ol om ql gt oo op qm or os ot qn ov ow ox qo oz pa pb bk">Inference</h1><p id="f407" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">The above calculations only tell us how much memory we need to simply load the model! On top of this we need additional memory to actually run some input through the model. For the Qwen2 embedding model and LLMs in general the extra memory required depends on the size of the context window (ie. the length of the text passed into the model).</p><h2 id="3b15" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Older Models with Original Self-Attention Mechanism</h2><p id="1370" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">Before the release of <a class="af ne" href="https://arxiv.org/abs/2205.14135#" rel="noopener ugc nofollow" target="_blank">Flash Attention</a> which is now widely adopted, a lot of older Language Models were using the original self-attention from the Transformer architecture. This mechanism requires an additional memory which scales quadratically with the input sequence length. To illustrate why that’s the case below is a great visual reminder of self-attention.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qp"><img src="../Images/4a8c914370c08ec222bb4e23b4374895.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYvblCjDRiLGyeenbZJ4Bg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">A great visual of the self-attention mechanism. Source: Sebastian Raschka, <a class="af ne" href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention" rel="noopener ugc nofollow" target="_blank">https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</a>, reproduced with author’s permission.</figcaption></figure><p id="91cf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">From the diagram we can see that apart from model weights (the <em class="qb">Wq, Wk, Wv </em>matrices) which we accounted for when calculating the memory required to load the model, there are many additional calculations and their outputs which need to be stored. These include for example, the inputs X, the <em class="qb">Q, K, V </em>matrices, and the <strong class="mk fr">attention matrix</strong> <em class="qb">QK^T</em>. It turns out that as the size of the input sequence length, <em class="qb">n</em>, grows the attention matrix becomes the dominant factor in the extra memory required.</p><p id="ea30" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To understand this we can perform a few simple calculations. For example, in the original transformer the embedding dimension size, <em class="qb">d</em>, was 512. So for an input sequence of 512 tokens both the inputs X and the attention matrix would require an additional 1MB of memory each in fp32.</p><blockquote class="py pz qa"><p id="0130" class="mi mj qb mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">512² floats = 512² * 32 bits = 512² * 4 bytes = 1MB</p></blockquote><p id="70c9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If we increase the input sequence length to 8000 tokens, the extra memory requirements would be</p><blockquote class="py pz qa"><p id="e694" class="mi mj qb mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Inputs X = 512 * 8000 * 4 bytes = 16MB; Attention Matrix = 8000² * 4 bytes = 256MB.</p></blockquote><p id="bee7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and if we increase the input sequence length to 32k tokens the extra memory requirements would be</p><blockquote class="py pz qa"><p id="bec6" class="mi mj qb mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Inputs X = 512 * 32000 * 4 bytes = 65MB; Attention Matrix = 32000² * 4 bytes = 4GB!</p></blockquote><p id="ea4d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As you can see the extra memory required grows very quickly with the size of the context window, and is quickly dominated by the <em class="qb">n² </em>number of floats in the attention matrix!</p><p id="0635" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The above calculations are still very much a simplification as there a lot more details which contribute to even more memory usage. For instance, in the original transformer there is also multi-head attention — where the attention computation is computed in parallel with many different heads (8 in the original implementation). So we need to multiply the required memory by the number of heads. Similarly, the above calculations were for a batch size of 1, if we want to embed many different texts at once we can increase the batch size, but at the cost of additional memory. For a detailed breakdown of the different memory requirements see the following <a class="af ne" href="https://huggingface.co/blog/mayank-mishra/padding-free-transformer" rel="noopener ugc nofollow" target="_blank">article</a>.</p><h2 id="2367" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">More Recent Models like Qwen2</h2><p id="f3ea" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">Since the release of the Transformer in 2017, there has been a lot of research into alternative attention mechanisms to avoid the n² bottleneck. However, they came with the tradeoff of decreased accuracy. In 2022, an exact attention mechanism came out with specific GPU optimisations called <a class="af ne" href="https://arxiv.org/abs/2205.14135" rel="noopener ugc nofollow" target="_blank">Flash Attention</a> and has been widely adopted in LLMs. Since then theres been further iterations including the recent <a class="af ne" href="https://arxiv.org/abs/2407.08608" rel="noopener ugc nofollow" target="_blank">Flash Attention 3</a> released in July 2024. The most important takeaway for us is that Flash Attention scales linearly with the input sequence length!</p><p id="3456" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Below is a theoretical derivation which compares the memory requirements of a 20B parameter model with different sequence lengths of different attention mechanisms. The <code class="cx qq qr qs qt b">Padding-Free Transformer</code> is yet another optimisation which removes the need of <a class="af ne" href="https://huggingface.co/docs/transformers/en/pad_truncation" rel="noopener ugc nofollow" target="_blank">padding</a> — very useful if you have one long sequence and many short sequences in a batch.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qu"><img src="../Images/a941c0fa93eea419f3b66db2022ce8ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fEGr8wGylg8_l78b.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Theoretical estimates of memory requirements for a 20B parameter model with different Attention Mechanisms. The main takeaway is the quadratic vs linear scaling. Source: Mayank Mishra, <a class="af ne" href="https://huggingface.co/blog/mayank-mishra/padding-free-transformer" rel="noopener ugc nofollow" target="_blank">Saving Memory Using Padding-Free Transformer Layers during Finetuning</a>, reproduced with author’s permission.</figcaption></figure><p id="0d0d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The Qwen2 model uses both the Flash Attention and padding optimisations. Now with the theory covered let’s see how to actually run the Qwen2 model!</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="95f8" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Running a 7B Qwen2 Model with HuggingFace Transformers</h1><h2 id="b78d" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Set Up</h2><p id="ce7f" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">The model that we will experiment with is the <code class="cx qq qr qs qt b">Alibaba-NLP/gte-Qwen2-7B-instruct</code> from Transformers. The model card is <a class="af ne" href="https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="e85b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To perform this experiment, I have used Python 3.10.8 and installed the following packages:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="2320" class="qy oh fq qt b bg qz ra l rb rc">torch==2.3.0<br/>transformers==4.41.2<br/>xformers==0.0.26.post1<br/>flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl<br/>accelerate==0.31.0</span></pre><p id="ae3e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I ran into some <a class="af ne" href="https://github.com/Dao-AILab/flash-attention/issues/246" rel="noopener ugc nofollow" target="_blank">difficulty</a> in installing <code class="cx qq qr qs qt b">flash-attn</code> required to run this model and so had to install the specific version listed above. If anyone has a better workaround please let me know!</p><p id="483f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The Amazon SageMaker instance I used for this experiment is the <code class="cx qq qr qs qt b">ml.g5.2xlarge</code>. It has a 24GB NVIDIA A10G GPU and 32GB of CPU memory and it costs $1.69/hour. The below screenshot from AWS shows all the details of the instance</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rd"><img src="../Images/3a03efec36249f1333e76b131a3b3275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ri-6-2jRgKLQrTf5eSPx5Q.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">SageMaker g5 instance types from <a class="af ne" href="https://aws.amazon.com/sagemaker/pricing/" rel="noopener ugc nofollow" target="_blank">AWS docs</a>.</figcaption></figure><p id="f91b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Actually to be precise if you run <code class="cx qq qr qs qt b">nvidia-smi</code> you will see that the instance only has 23GB of GPU memory which is slightly less than advertised. The CUDA version on this GPU is 12.2.</p><h2 id="efbc" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">How to Run — In Detail</h2><p id="07a9" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">If you look at the model card, one of the suggested ways to use this model is via the <code class="cx qq qr qs qt b">sentence-transformers</code> library as show below</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="3d10" class="qy oh fq qt b bg qz ra l rb rc">from sentence_transformers import SentenceTransformer<br/><br/># This will not run on our 24GB GPU!<br/>model = SentenceTransformer("Alibaba-NLP/gte-Qwen2-7B-instruct", trust_remote_code=True)<br/>embeddings = model.encode(list_of_examples)</span></pre><p id="159c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Sentence-transformers is an extension of the Transformers package for computing embeddings and is very useful as you can get things working with two lines of code. The downside is that you have less control on how to load the model as it hides away tokenisation and pooling details. The above code <strong class="mk fr">will not run</strong> on our GPU instance because it attempts to load the model in full float32 precision which would take 28GB of memory. When the sentence transformer model is initialised it checks for available devices (cuda for GPU) and automatically shifts the Pytorch model onto the device. As a result it gets stuck after loading 5/7ths of the model and crashes.</p><p id="f0da" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Instead we need to be able to load the model in float16 precision before we move it onto the GPU. As such we need to use the lower level Transformers library. (I am not sure of a way to do it with sentence-transformers but let me know if one exists!) We do this as follows</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="bf3c" class="qy oh fq qt b bg qz ra l rb rc">import transformers<br/>import torch<br/><br/>model_path = "Alibaba-NLP/gte-Qwen2-7B-instruct"<br/>model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to("cuda")</span></pre><p id="2081" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">With the <code class="cx qq qr qs qt b">torch_dtype</code> parameter we specify that the model should be loaded in float16 precision straight away, thus only requiring 14GB of memory. We then need to move the model onto the GPU device which is achieved with the <code class="cx qq qr qs qt b">to</code> method. Using the above code, the model takes almost 2min to load!</p><p id="80cc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since we are using <code class="cx qq qr qs qt b">transformers</code> we need to separately load the tokeniser to tokenise the input texts as follows:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="07a4" class="qy oh fq qt b bg qz ra l rb rc">tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)</span></pre><p id="ed38" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The next step is to tokenise the input texts which is done as follows:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="8076" class="qy oh fq qt b bg qz ra l rb rc">texts = ["example text 1", "example text 2 of different length"]<br/>max_length = 32768<br/>batch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors="pt").to(DEVICE)</span></pre><p id="a9d9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The maximum length of the Qwen2 model is 32678, however as we will see later we are unable to run it with such a long sequence on our 24GB GPU due to the additional memory requirements. I would recommend reducing this to <em class="qb">no more than 24,000</em> to avoid out of memory errors. Padding ensures that all the inputs in the batch have the same length whilst truncation ensures that any inputs longer than the maximum length will be truncated. For more information please see the <a class="af ne" href="https://huggingface.co/docs/transformers/en/pad_truncation" rel="noopener ugc nofollow" target="_blank">docs</a>. Finally, we ensure that we return PyTorch tensors (default would be lists instead) and move these tensors onto the GPU to be available to pass to the model.</p><p id="9dfa" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The next step is to pass the inputs through our model and perform pooling. This is done as follows</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="d746" class="qy oh fq qt b bg qz ra l rb rc">with torch.no_grad():<br/>    outputs = model(**batch_dict)<br/>    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict["attention_mask"])</span></pre><p id="0b7b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">with the <code class="cx qq qr qs qt b">last_token_pool</code> which looks as follows:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="27ab" class="qy oh fq qt b bg qz ra l rb rc">def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:<br/>    # checks whether there is any padding (where attention mask = 0 for a given text)<br/>    no_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]<br/>    # if no padding - only would happen if batch size of 1 or all sequnces have the same length, then take the last tokens as the embeddings<br/>    if no_padding:<br/>        return last_hidden_states[:, -1]<br/>    # otherwise use the last non padding token for each text in the batch<br/>    sequence_lengths = attention_mask.sum(dim=1) - 1<br/>    batch_size = last_hidden_states.shape[0]<br/>    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengthsLet’s break down what happened in the above code snippets! </span></pre><ul class=""><li id="e8bc" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qc qd qe bk">The <code class="cx qq qr qs qt b">torch.no_grad()</code> context manager is used to disable gradient calculation, since we are not training the model and hence to speed up the inference.</li><li id="1669" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">We then pass the tokenised inputs into the transformer model.</li><li id="4b05" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">We retrieve the outputs from the last layer of the model with the <code class="cx qq qr qs qt b">last_hidden_state</code> attribute. This is a tensor of shape <em class="qb">(batch_size, max_sequence_length, embedding dimension)</em>. Essentially for each example in the batch the transformer outputs embeddings for all the tokens in the sequence.</li><li id="6ddc" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">We now need some way of combining all the token embeddings into a single embedding to represent the input text. This is called <code class="cx qq qr qs qt b">pooling</code> and it is done in the same way as during training of the model.</li><li id="26d2" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">In older BERT based models the first token was typically used (which represented the special classification [CLS] token). However, the Qwen2 model is LLM-based, i.e. transformer decoder based. In the decoder, the tokens are generated auto regressively (one after another) and so the last token contains all the information encoded about the sentence.</li><li id="2105" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">The goal of the <code class="cx qq qr qs qt b">last_token_pool</code> function is to therefore select the embedding of the last generated token (which was not the padding token) for each example in the batch.</li><li id="9b22" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">It uses the <code class="cx qq qr qs qt b">attention_mask</code> which tells the model which of the tokens are padding tokens for each example in the batch (see the <a class="af ne" href="https://huggingface.co/docs/transformers/en/glossary" rel="noopener ugc nofollow" target="_blank">docs</a>).</li></ul><h2 id="a973" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Annotated Example</h2><p id="556f" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">Let’s look at an example to understand it in a bit more detail. Let’s say we want to embed two examples in a single batch:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="227a" class="qy oh fq qt b bg qz ra l rb rc">texts = ["example text 1", "example text 2 of different length"]</span></pre><p id="0609" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The outputs of the tokeniser (the <code class="cx qq qr qs qt b">batch_dict</code> ) will look as follows:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="c265" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; batch_dict<br/>{'input_ids': tensor([[  8687,   1467,    220,     16, 151643, 151643, 151643],<br/>        [  8687,   1467,    220,     17,    315,   2155,   3084]],<br/>       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0],<br/>        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}</span></pre><p id="4b2f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">From this you can see that the first sentence gets split into four tokens (8687, 1467, 220, 16), while the second sentence get split into seven tokens. As a result, the first sentence is padded (with three padding tokens with id 151643) up to length seven — the maximum in the batch. The attention mask reflects this — it has three zeros for the first example corresponding to the location of the padding tokens. Both the tensors have the same size</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="67f0" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; batch_dict.input_ids.shape<br/>torch.Size([2, 7])<br/>&gt;&gt;&gt; batch_dict.attention_mask.shape<br/>torch.Size([2, 7])</span></pre><p id="ad0e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now passing the <code class="cx qq qr qs qt b">batch_dict</code> through the model we can retrieve the models last hidden state of shape:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="7f27" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; outputs.last_hidden_state.shape<br/>torch.Size([2, 7, 3584])</span></pre><p id="d0b8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can see that this is of shape <em class="qb">(batch_size, max_sequence_length, embedding dimension)</em>. Qwen2 has an embedding dimension of 3584!</p><p id="7eb7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now we are in the <code class="cx qq qr qs qt b">last_token_pool</code> function. The first line checks if padding exists, it does it by summing the last “column” of the attention_mask and comparing it to the batch_size (given by <code class="cx qq qr qs qt b">attention_mask.shape[0]</code>. This will only result in true if there exists a 1 in all of the attention mask, i.e. if all the examples are the same length or if we only have one example.</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="452a" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; attention_mask.shape[0]<br/>2<br/>&gt;&gt;&gt; attention_mask[:, -1]<br/>tensor([0, 1], device='cuda:0')</span></pre><p id="9ff2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If there was indeed no padding we would simply select the last token embedding for each of the examples with <code class="cx qq qr qs qt b">last_hidden_states[:, -1]</code>. However, since we have padding we need to select the last non-padding token embedding from each example in the batch. In order to pick this embedding we need to get its index for each example. This is achieved via</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="1787" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; sequence_lengths = attention_mask.sum(dim=1) - 1<br/>&gt;&gt;&gt; sequence_lengths<br/>tensor([3, 6], device='cuda:0')</span></pre><p id="23a3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So now we need to simply index into the tensor, with the correct indices in the first two dimensions. To get the indices for all the examples in the batch we can use <code class="cx qq qr qs qt b">torch.arange</code> as follows:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="b2d1" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; torch.arange(batch_size, device=last_hidden_states.device)<br/>tensor([0, 1], device='cuda:0')</span></pre><p id="4406" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Then we can pluck out the correct token embeddings for each example using this and the indices of the last non padding token:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="6cff" class="qy oh fq qt b bg qz ra l rb rc">&gt;&gt;&gt; embeddings = last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]<br/>&gt;&gt;&gt; embeddings.shape<br/>torch.Size([2, 3584])</span></pre><p id="23c3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">And we get two embeddings for the two examples passed in!</p><h2 id="9023" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">How to Run — TLDR</h2><p id="3fc4" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">The full code separated out into functions looks like</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="1999" class="qy oh fq qt b bg qz ra l rb rc">import numpy as np<br/>import numpy.typing as npt<br/>import torch<br/>import transformers<br/><br/><br/>DEVICE = torch.device("cuda")<br/><br/><br/>def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:<br/>    # checks whether there is any padding (where attention mask = 0 for a given text)<br/>    no_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]<br/>    # if no padding - only would happen if batch size of 1 or all sequnces have the same length, then take the last tokens as the embeddings<br/>    if no_padding:<br/>        return last_hidden_states[:, -1]<br/>    # otherwise use the last non padding token for each text in the batch<br/>    sequence_lengths = attention_mask.sum(dim=1) - 1<br/>    batch_size = last_hidden_states.shape[0]<br/>    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths<br/><br/><br/>def encode_with_qwen_model(<br/>    model: transformers.PreTrainedModel,<br/>    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,<br/>    texts: list[str],<br/>    max_length: int = 32768,<br/>) -&gt; npt.NDArray[np.float16]:<br/>    batch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors="pt").to(DEVICE)<br/><br/>    with torch.no_grad():<br/>        outputs = model(**batch_dict)<br/>        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict["attention_mask"])<br/>    return embeddings.cpu().numpy()<br/><br/><br/>def main() -&gt; None:<br/>    model_path = "Alibaba-NLP/gte-Qwen2-7B-instruct"<br/>    tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)<br/>    model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(DEVICE)<br/>    print("Loaded tokeniser and model")<br/>    <br/>    texts_to_encode = ["example text 1", "example text 2 of different length"]<br/>    embeddings = encode_with_qwen_model(model, tokenizer, texts_to_encode)<br/>    print(embeddings.shape)<br/><br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="613b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The <code class="cx qq qr qs qt b">encode_with_qwen_model</code> returns a numpy array. In order to convert a PyTorch tensor to a numpy array we first have to move it off the GPU back onto the CPU which is achieved with the <code class="cx qq qr qs qt b">cpu()</code> method. Please note that if you are planning to run long texts you should reduce the batch size to 1 and only embed one example at a time (thus reducing the list <code class="cx qq qr qs qt b">texts_to_encode</code> to length 1).</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a63d" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Empirical Memory Usage Tests with Context Length</h1><p id="74dd" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">Before we saw how the memory usage varies with the input text size from a theoretical standpoint. We can also measure how much memory the GPU actually uses when embedding texts of different length and verify the scaling empirically! I got the idea from this great HuggingFace tutorial: <a class="af ne" href="https://huggingface.co/docs/transformers/v4.35.0/en/llm_tutorial_optimization" rel="noopener ugc nofollow" target="_blank">Getting the most out of LLMs</a>.</p><p id="f882" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To do this we will make use of some extra functions</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="036b" class="qy oh fq qt b bg qz ra l rb rc">import gc<br/><br/>def flush() -&gt; None:<br/>    gc.collect()<br/>    torch.cuda.empty_cache()<br/>    torch.cuda.reset_peak_memory_stats()<br/><br/><br/>def bytes_to_giga_bytes(bytes_: float) -&gt; float:<br/>    return bytes_ / 1024 / 1024 / 1024</span></pre><p id="f553" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">as well the</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="e1a8" class="qy oh fq qt b bg qz ra l rb rc">torch.cuda.max_memory_allocated()</span></pre><p id="9303" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">function to measure the peak GPU usage. The <code class="cx qq qr qs qt b">flush</code> function will clear and reset the memory after each pass through the model. We will run texts of different lengths through the model and print out the peak and effective GPU usage. The effective GPU usage is the model size usage subtracted from the total usage which gives us an idea of how much extra memory we need to run the text through the model.</p><p id="1206" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The full code I used is below:</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="2897" class="qy oh fq qt b bg qz ra l rb rc">import gc<br/><br/>import numpy as np<br/>import numpy.typing as npt<br/>import torch<br/>import transformers<br/><br/>DEVICE = torch.device("cuda")<br/><br/><br/>def last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -&gt; torch.Tensor:<br/>    # checks whether there is any padding (where attention mask = 0 for a given text)<br/>    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]<br/>    # if no padding - only would happen if batch size of 1 or all sequences have the same length, then take the last tokens as the embeddings<br/>    if left_padding:<br/>        return last_hidden_states[:, -1]<br/>    # otherwise use the last non padding token for each text in the batch<br/>    sequence_lengths = attention_mask.sum(dim=1) - 1<br/>    batch_size = last_hidden_states.shape[0]<br/>    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]<br/><br/><br/>def encode_with_qwen_model(<br/>    model: transformers.PreTrainedModel,<br/>    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,<br/>    texts: list[str] | str,<br/>    max_length: int = 32768,<br/>) -&gt; npt.NDArray[np.float16]:<br/>    batch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors="pt").to(DEVICE)<br/><br/>    with torch.no_grad():<br/>        outputs = model(**batch_dict)<br/>        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict["attention_mask"])<br/>    return embeddings.cpu().numpy()<br/><br/><br/>def flush() -&gt; None:<br/>    gc.collect()<br/>    torch.cuda.empty_cache()<br/>    torch.cuda.reset_peak_memory_stats()<br/><br/><br/>def bytes_to_giga_bytes(bytes_: float) -&gt; float:<br/>    return bytes_ / 1024 / 1024 / 1024<br/><br/><br/>def memory_usage_experiments(<br/>    model: transformers.PreTrainedModel,<br/>    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,<br/>) -&gt; None:<br/>    model_size = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())<br/>    print(f"Most gpu usage on model loaded: {model_size} GB\n")<br/>    sentence = "This sentence should have minimum eight tokens. "<br/>    all_texts = [sentence, sentence * 100, sentence * 1000, sentence * 2000, sentence * 3000, sentence * 4000]<br/>    for texts in all_texts:<br/>        batch_dict = tokenizer(texts, max_length=32768, padding=True, truncation=True, return_tensors="pt")<br/>        encode_with_qwen_model(model, tokenizer, texts)<br/>        max_mem = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())<br/>        print(f"Sequence length: {batch_dict.input_ids.shape[-1]}. Most gpu usage: {max_mem} GB. Effective usage: {max_mem - model_size} GB\n")<br/>        flush()<br/><br/><br/>def main() -&gt; None:<br/>    model_path = "Alibaba-NLP/gte-Qwen2-7B-instruct"<br/>    tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)<br/>    model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(DEVICE)<br/>    print("Loaded tokeniser and model")<br/><br/>    memory_usage_experiments(model, tokenizer)<br/><br/><br/>if __name__ == "__main__":<br/>    main()</span></pre><p id="b541" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">and the traceback is</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="38a7" class="qy oh fq qt b bg qz ra l rb rc">Most gpu usage on model loaded: 14.958292961120605 GB<br/><br/>Sequence length: 9. Most gpu usage: 14.967926502227783 GB. Effective usage: 0.009633541107177734 GB<br/><br/>Sequence length: 801. Most gpu usage: 15.11520528793335 GB. Effective usage: 0.15691232681274414 GB<br/><br/>Sequence length: 8001. Most gpu usage: 16.45930576324463 GB. Effective usage: 1.5010128021240234 GB<br/><br/>Sequence length: 16001. Most gpu usage: 17.944651126861572 GB. Effective usage: 2.986358165740967 GB<br/><br/>Sequence length: 24001. Most gpu usage: 19.432421684265137 GB. Effective usage: 4.474128723144531 GB<br/><br/>torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU </span></pre><p id="40eb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can see from this that the Qwen2 model indeed scales linearly with the size of the input text. For example, as we double the number of tokens from 8000 to 16000, the effective memory usage roughly doubles as well. Unfortunately, trying to run a sequence of length 32000 through the model resulted in a CUDA OOM error so even with a 24GB GPU in float 16 precision we are still unable to utilise the full context window of the model.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c9f8" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Other Aspects</h1><h2 id="2544" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Running Qwen2 in fp32</h2><p id="7ee0" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">To run the Qwen2 model in full precision we have two options. Firstly we can get access to a bigger GPU — for example 40GB would be enough. However, this could be costly. Amazon SageMaker for instance does not have an instance with a single 40GB GPU, instead it has an instance with 8 of them! But that wouldn’t be useful as we do not need the other 7 sitting idly. Of course we may also look at other providers as well — there are quite a few now and offering competitive prices.</p><p id="e16d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The other option is to run the model on an instance with multiple smaller GPUs. The model can be sharded across different GPUs — i.e. different layers of the model are put on different GPUs and the data gets moved across the devices during inference. To do this with HuggingFace you can do</p><pre class="ni nj nk nl nm qv qt qw bp qx bb bk"><span id="6c7b" class="qy oh fq qt b bg qz ra l rb rc">model_path = "Alibaba-NLP/gte-Qwen2-7B-instruct"<br/>model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map="auto").to("cuda")</span></pre><p id="0a29" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For more information on how this works see the following <a class="af ne" href="https://huggingface.co/docs/accelerate/en/usage_guides/big_modeling" rel="noopener ugc nofollow" target="_blank">docs</a> and the <a class="af ne" href="https://huggingface.co/docs/accelerate/en/concept_guides/big_model_inference" rel="noopener ugc nofollow" target="_blank">conceptual guide</a>. The caveat is that this way of doing it is a lot slower — due to the overhead of inter-GPU communication, moving data across the different devices to perform inference. This implementation is also not optimised, meaning execution on each GPU happens sequentially whilst others are sitting idle. If you were embedding thousands of texts or training the model you would ideally have all the GPUs constantly doing some work.</p><h2 id="69aa" class="pc oh fq bf oi pd pe pf ol pg ph pi oo mr pj pk pl mv pm pn po mz pp pq pr ps bk">Running Qwen2 on smaller GPUs</h2><p id="2ac2" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">To run this model on even smaller GPUs you would need to quantise the model. Two popular options would be</p><ul class=""><li id="8014" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qc qd qe bk">Via HuggingFace which has various methods available (see <a class="af ne" href="https://huggingface.co/docs/transformers/main/en/quantization/overview" rel="noopener ugc nofollow" target="_blank">docs</a>).</li><li id="a8a9" class="mi mj fq mk b go qf mm mn gr qg mp mq mr qh mt mu mv qi mx my mz qj nb nc nd qc qd qe bk">Via the <a class="af ne" href="https://qwen.readthedocs.io/en/latest/deployment/vllm.html" rel="noopener ugc nofollow" target="_blank">vLLM</a> package.</li></ul></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bc6b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Conclusion</h1><p id="4dd9" class="pw-post-body-paragraph mi mj fq mk b go pt mm mn gr pu mp mq mr pv mt mu mv pw mx my mz px nb nc nd fj bk">In conclusion in this article we saw how to run a 7B LLM based Qwen2 Embedding model on a single 24GB GPU. We saw how the size of the model is calculated from the number of its parameters and that we need to load the model in float16 precision to fit it onto the 24GB GPU. We then saw that extra memory is required to actually run an example through the model which depends on the size of the context window and varies depending on the underlining attention mechanism used. Finally we saw how to do all of this in just a few lines of code with the Transformers library.</p></div></div></div></div>    
</body>
</html>