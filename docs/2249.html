<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Bayesian Linear Regression: A Complete Beginner’s guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Bayesian Linear Regression: A Complete Beginner’s guide</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-linear-regression-a-complete-beginners-guide-3a49bb252fdc?source=collection_archive---------1-----------------------#2024-09-14">https://towardsdatascience.com/bayesian-linear-regression-a-complete-beginners-guide-3a49bb252fdc?source=collection_archive---------1-----------------------#2024-09-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a720" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A workflow and code walkthrough for building a Bayesian regression model in <strong class="al">STAN</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samvardhanvishnoi2026?source=post_page---byline--3a49bb252fdc--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Samvardhan Vishnoi" class="l ep by dd de cx" src="../Images/a99d8db797d6ff346aed66cc84f0f32e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*X156krczAkyldogo"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3a49bb252fdc--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@samvardhanvishnoi2026?source=post_page---byline--3a49bb252fdc--------------------------------" rel="noopener follow">Samvardhan Vishnoi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3a49bb252fdc--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="eef4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note: Check out my previous <a class="af nf" href="https://medium.com/towards-data-science/a-practical-guide-to-becoming-a-bayesian-data-scientist-i-c4f7a1844825" rel="noopener">article</a> for a practical discussion on why Bayesian modeling may be the right choice for your task.</p><p id="0b2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This tutorial will focus on a workflow + code walkthrough for building a Bayesian regression model in <strong class="ml fr">STAN</strong>, a probabilistic programming language. STAN is widely adopted and interfaces with your language of choice (R, Python, shell, MATLAB, Julia, Stata). See the <a class="af nf" href="https://mc-stan.org/users/interfaces/" rel="noopener ugc nofollow" target="_blank">installation</a> guide and <a class="af nf" href="https://mc-stan.org/users/documentation/" rel="noopener ugc nofollow" target="_blank">documentation</a>.</p><p id="d51a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will use <a class="af nf" href="https://pystan.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">Pystan</a> for this tutorial, simply because I code in Python. Even if you use another language, the general Bayesian practices and STAN language syntax I will discuss here doesn’t vary much.</p><p id="6f0d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the more hands-on reader, here is a link to the <a class="af nf" href="https://github.com/samvardhan/stan_workshop/blob/main/stan_workshop.ipynb" rel="noopener ugc nofollow" target="_blank">notebook</a> for this tutorial, part of my Bayesian modeling workshop at Northwestern University (April, 2024).</p><p id="b3d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s dive in!</p><h1 id="1737" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Bayesian Linear Regression</h1><p id="2086" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Lets learn how to build a simple linear regression model, the bread and butter of any statistician, the Bayesian way. Assuming a dependent variable <strong class="ml fr">Y</strong> and covariate <strong class="ml fr">X</strong>, I propose the following simple model-</p><p id="d9dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Y</strong> = α + β * <strong class="ml fr">X</strong> + ϵ</p><p id="d3ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where ⍺ is the intercept, β is the slope, and ϵ is some random error. Assuming that,</p><p id="ab85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">ϵ ~ Normal(0, σ)</p><p id="db62" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">we can show that</p><p id="dd72" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Y</strong> ~ Normal(α + β * <strong class="ml fr">X, </strong>σ)</p><p id="7cca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We will learn how to code this model form in STAN.</p><h2 id="2983" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk"><strong class="al">Generate Data</strong></h2><p id="0011" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">First, let’s generate some fake data.</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="1954" class="ph nh fq pe b bg pi pj l pk pl">#Model Parameters<br/>alpha = 4.0  #intercept<br/>beta = 0.5 #slope<br/>sigma = 1.0 #error-scale</span></pre><pre class="pm pd pe pf bp pg bb bk"><span id="157f" class="ph nh fq pe b bg pi pj l pk pl">#Generate fake data<br/>x =  8 * np.random.rand(100)<br/>y = alpha + beta * x<br/>y = np.random.normal(y, scale=sigma) #noise<br/>#visualize generated data<br/>plt.scatter(x, y, alpha = 0.8)</span></pre><figure class="oy oz pa pb pc pq pn po paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="pn po pp"><img src="../Images/6164a06655df22cfc765b4dc8df23e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbZnaCuhuanIMvm7esJemA.png"/></div></div><figcaption class="pw px py pn po pz qa bf b bg z dx">Generated data for Linear Regression (Image from code by Author)</figcaption></figure><h1 id="9dba" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Model String</h1><p id="7697" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Now that we have some data to model, let’s dive into how to structure it and pass it to STAN along with modeling instructions. This is done via the <em class="qb">model</em> string, which typically contains 4 (occasionally more) blocks- <em class="qb">data</em>, <em class="qb">parameters</em>, <em class="qb">model</em>, and <em class="qb">generated</em> <em class="qb">quantities</em>. Let’s discuss each of these blocks in detail.</p><h2 id="7391" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk"><strong class="al">DATA block</strong></h2><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="fef7" class="ph nh fq pe b bg pi pj l pk pl">data {                    //input the data to STAN<br/>    int&lt;lower=0&gt; N;<br/>    vector[N] x;<br/>    vector[N] y;<br/>}</span></pre><p id="abe6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <em class="qb">data</em> block is perhaps the simplest, it tells STAN internally what data it should expect, and in what format. For instance, here we pass-</p><p id="ba12" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">N</strong>: the size of our dataset as type <em class="qb">int</em>. The <em class="qb">&lt;lower=0&gt; </em>part declares that N≥0. (Even though it is obvious here that data length cannot be negative, stating these bounds is good standard practice that can make STAN’s job easier.)</p><p id="42cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">x</strong>: the covariate as a vector of length N.</p><p id="7d58" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">y</strong>: the dependent as a vector of length N.</p><p id="6bcf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">See <a class="af nf" href="https://mc-stan.org/docs/reference-manual/types.html#:~:text=Stan%20provides%20three%20real%2Dvalued,vectors%2C%20and%20matrix%20for%20matrices." rel="noopener ugc nofollow" target="_blank">docs here</a> for a full range of supported data types. STAN offers support for a wide range of types like arrays, vectors, matrices etc. As we saw above, STAN also has support for encoding <strong class="ml fr">limits</strong> on variables. Encoding limits is recommended! It leads to better specified models and simplifies the probabilistic sampling processes operating under the hood.</p><h2 id="6f37" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk">Model Block</h2><p id="400c" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Next is the <em class="qb">model</em> block, where we tell STAN the structure of our model.</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="966f" class="ph nh fq pe b bg pi pj l pk pl">//simple model block <br/>model {                   <br/>    //priors<br/>    alpha ~ normal(0,10);<br/>    beta ~ normal(0,1); <br/> <br/>    //model<br/>    y ~ normal(alpha + beta * x, sigma);<br/>}</span></pre><p id="2b68" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The model block also contains an important, and often confusing, element: <em class="qb">prior</em> specification. Priors are a <strong class="ml fr">quintessential</strong> part of Bayesian modeling, and must be specified suitably for the sampling task.</p><p id="c2c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">See my previous <a class="af nf" href="https://medium.com/towards-data-science/a-practical-guide-to-becoming-a-bayesian-data-scientist-i-c4f7a1844825" rel="noopener">article</a> for a primer on the role and intuition behind priors. To summarize, the <em class="qb">prior</em> is a presupposed functional form for the distribution of parameter values — often referred to, simply, as <em class="qb">prior belief</em>. Even though priors <strong class="ml fr">don’t</strong> have to exactly <strong class="ml fr">match</strong> the final solution, they must allow us to <strong class="ml fr">sample</strong> from it.</p><p id="9e16" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In our example, we use Normal priors of mean 0 with different variances, depending on how sure we are of the supplied mean value: 10 for alpha (very unsure), 1 for beta (somewhat sure). Here, I supplied the general <em class="qb">belief</em> that while alpha can take a wide range of different values, the slope is generally more contrained and won’t have a large magnitude.</p><blockquote class="qc qd qe"><p id="871a" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hence, in the example above, the prior for alpha is ‘weaker’ than beta.</p></blockquote><p id="4abc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As models get more complicated, the sampling solution space expands, and supplying beliefs gains importance. Otherwise, if there is no strong intuition, it is good practice to just supply less belief into the model i.e. use a <strong class="ml fr">weakly informative </strong>prior, and remain flexible to incoming data.</p><p id="da84" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The form for y, which you might have recognized already, is the standard linear regression equation.</p><h2 id="d294" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk">Generated Quantities</h2><p id="dee6" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Lastly, we have our block for <em class="qb">generated quantities</em>. Here we tell STAN what quantities we want to calculate and receive as output.</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="21b9" class="ph nh fq pe b bg pi pj l pk pl">generated quantities {    //get quantities of interest from fitted model<br/>    vector[N] yhat;<br/>    vector[N] log_lik;<br/>    for (n in 1:N){<br/>        yhat[n] = normal_rng(alpha + x[n] * beta, sigma);             <br/>        //generate samples from model<br/>        log_lik[n] = normal_lpdf( y[n] | alpha + x[n] * beta, sigma); <br/>        //probability of data given the model and parameters<br/>        }<br/>}</span></pre><p id="efe8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note: STAN supports vectors to be passed either directly into equations, or as iterations 1:N for each element n. In practice, I’ve found this support to change with different versions of STAN, so it is good to try the iterative declaration if the vectorized version fails to compile.</p><p id="c886" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the above example-</p><p id="1858" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">yhat:</strong> generates samples for y from the fitted parameter values.</p><p id="57a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">log_lik: </strong>generates probability of data given the model and fitted parameter value.</p><p id="09dd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The purpose of these values will be clearer when we talk about model evaluation.</p><h1 id="c7de" class="ng nh fq bf ni nj nk gq nl nm nn gt no np nq nr ns nt nu nv nw nx ny nz oa ob bk">Putting it all together</h1><p id="d829" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Altogether, we have now fully specified our first simple Bayesian regression model:</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="2583" class="ph nh fq pe b bg pi pj l pk pl">model = """<br/>data {                    //input the data to STAN<br/>    int&lt;lower=0&gt; N;<br/>    vector[N] x;<br/>    vector[N] y;<br/>}</span></pre><pre class="pm pd pe qf qg ay qh bk"><span id="3cb1" class="oh nh fq pe b hw qi qj l im pl">parameters {              <br/>    real alpha;<br/>    real beta;<br/>    real&lt;lower=0&gt; sigma;<br/>}</span><span id="84a3" class="oh nh fq pe b hw qk qj l im pl">model {                   <br/>    alpha ~ normal(0,10);<br/>    beta ~ normal(0,1);   <br/>    y ~ normal(alpha + beta * x, sigma);<br/>}</span><span id="f013" class="oh nh fq pe b hw qk qj l im pl">generated quantities {    <br/>    vector[N] yhat;<br/>    vector[N] log_lik;<br/>    <br/>    for (n in 1:N){</span><span id="21ea" class="oh nh fq pe b hw qk qj l im pl">        yhat[n] = normal_rng(alpha + x[n] * beta, sigma);             <br/>        log_lik[n] = normal_lpdf(y[n] | alpha + x[n] * beta, sigma);</span><span id="06e1" class="oh nh fq pe b hw qk qj l im pl">        }<br/>}<br/>"""</span></pre><p id="af33" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All that remains is to compile the model and run the sampling.</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="6f0c" class="ph nh fq pe b bg pi pj l pk pl">#STAN takes data as a dict<br/>data = {'N': len(x), 'x': x, 'y': y}</span></pre><p id="d858" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">STAN takes input data in the form of a dictionary. It is important that this dict contains all the variables that we told STAN to expect in the model-data block, otherwise the model won’t compile.</p><pre class="oy oz pa pb pc pd pe pf bp pg bb bk"><span id="06f3" class="ph nh fq pe b bg pi pj l pk pl">#parameters for STAN fitting<br/>chains = 2<br/>samples = 1000<br/>warmup = 10<br/># set seed</span></pre><pre class="pm pd pe pf bp pg bb bk"><span id="b2fb" class="ph nh fq pe b bg pi pj l pk pl"># Compile the model<br/>posterior = stan.build(model, data=data, random_seed = 42)<br/># Train the model and generate samples<br/>fit = posterior.sample(num_chains=chains, num_samples=samples)The .sample() method parameters control the Hamiltonian Monte Carlo (HMC) sampling process, where —</span></pre><ul class=""><li id="672b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ql qm qn bk"><strong class="ml fr">num_chains:</strong> is the number of times we repeat the sampling process.</li><li id="f5bd" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk"><strong class="ml fr">num_samples:</strong> is the number of samples to be drawn in each chain.</li><li id="9b75" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk"><strong class="ml fr">warmup: </strong>is the number of initial samples that we discard (as it takes some time to reach the general vicinity of the solution space).</li></ul><blockquote class="qc qd qe"><p id="6486" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Knowing the right values for these parameters depends on both the complexity of our model and the resources available.</p></blockquote><p id="80af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Higher sampling sizes are of course ideal, yet for an ill-specified model they will prove to be just waste of time and computation. Anecdotally, I’ve had large data models I’ve had to wait a week to finish running, only to find that the model didn’t converge. Is is important to start slowly and sanity check your model before running a full-fledged sampling.</p><h2 id="b86a" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk">Model Evaluation</h2><p id="c201" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">The generated quantities are used for</p><ul class=""><li id="0404" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ql qm qn bk">evaluating the goodness of fit i.e. convergence,</li><li id="2e1c" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk">predictions</li><li id="d0f2" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk">model comparison</li></ul><p id="76af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Convergence</strong></p><p id="44f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first step for evaluating the model, in the Bayesian framework, is visual. We observe the sampling draws of the <a class="af nf" href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">Hamiltonian Monte Carlo </a>(HMC) sampling process.</p><figure class="oy oz pa pb pc pq pn po paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="pn po qt"><img src="../Images/b5a5120fa7102b295945615ecfe1bbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-Cyksdle_QDn9qn4g08aA.png"/></div></div><figcaption class="pw px py pn po pz qa bf b bg z dx">Model Convergence: visually evaluating the overlap of independent sampling chains (Image from code by Author)</figcaption></figure><p id="e350" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In simplistic terms, STAN iteratively draws samples for our parameter values and evaluates them (HMC does <em class="qb">way</em> more, but that’s beyond our current scope). For a good fit, the sample draws must <strong class="ml fr">converge</strong> to some common general area which would, ideally, be the global <strong class="ml fr">optima</strong>.</p><p id="0e07" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The figure above shows the sampling draws for our model across 2 independent chains (red and blue).</p><ul class=""><li id="1b37" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ql qm qn bk">On the left, we plot the overall distribution of the fitted parameter value i.e. the <strong class="ml fr">posteriors</strong>. We expect a <strong class="ml fr">normal</strong> distribution if the model, and its parameters, are <strong class="ml fr">well specified</strong>. (<em class="qb">Why</em> is that? Well, a normal distribution just implies that there exist a certain range of best fit values for the parameter, which speaks in support of our chosen model form). Furthermore, we should expect a considerable <strong class="ml fr">overlap</strong> across chains <strong class="ml fr">IF</strong> the model is converging to an optima.</li><li id="244d" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk">On the right, we plot the actual samples drawn in each iteration (just to be <em class="qb">extra</em> sure). Here, again, we wish to see not only a <strong class="ml fr">narrow</strong> range but also a lot of <strong class="ml fr">overlap</strong> between the draws.</li></ul><p id="21e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Not all evaluation metrics are visual. Gelman et al. [1] also propose the <strong class="ml fr">Rhat</strong> diagnostic which essentially is a mathematical measure of the sample similarity across chains. Using Rhat, one can define a cutoff point beyond which the two chains are judged too dissimilar to be converging. The cutoff, however, is hard to define due to the iterative nature of the process, and the variable warmup periods.</p><blockquote class="qc qd qe"><p id="b82f" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Visual comparison is hence a crucial component, regardless of diagnostic tests</p></blockquote><p id="fab9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A frequentist thought you may have here is that, “well, if all we have is chains and distributions, what is the actual parameter value?” This is exactly the point. The Bayesian formulation only deals in <strong class="ml fr">distributions</strong>, NOT <strong class="ml fr">point</strong> estimates with their hard-to-interpret test statistics.</p><p id="538b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That said, the posterior can still be summarized using <strong class="ml fr">credible</strong> intervals like the <strong class="ml fr">High Density Interval (HDI</strong>), which includes all the x% highest probability density points.</p><figure class="oy oz pa pb pc pq pn po paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="pn po qu"><img src="../Images/59d883677a32fba505646b0a126d59da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCd-9LXQxbVeF5nOO944vw.png"/></div></div><figcaption class="pw px py pn po pz qa bf b bg z dx">95% HDI for beta (Image from code by Author)</figcaption></figure><p id="b078" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is important to contrast Bayesian <strong class="ml fr">credible</strong> intervals with frequentist <strong class="ml fr">confidence</strong> intervals.</p><ul class=""><li id="f412" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne ql qm qn bk">The credible interval gives a <strong class="ml fr">probability</strong> distribution on the <strong class="ml fr">possible values</strong> for the <strong class="ml fr">parameter </strong>i.e. the probability of the parameter assuming each value in some interval, given the data.</li><li id="823f" class="mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne ql qm qn bk">The confidence interval regards the <strong class="ml fr">parameter</strong> value as <strong class="ml fr">fixed</strong>, and estimates instead the confidence that <strong class="ml fr">repeated</strong> random <strong class="ml fr">samplings</strong> of the data would <strong class="ml fr">match</strong>.</li></ul><p id="8833" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hence the</p><blockquote class="qc qd qe"><p id="7a08" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bayesian approach lets the parameter values be fluid and takes the data at face value, while the frequentist approach demands that there exists the one true parameter value… if only we had access to all the data ever</p></blockquote><p id="58de" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Phew. Let that sink in, read it again until it does.</p><p id="101d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another important implication of using credible intervals, or in other words, allowing the parameter to be <strong class="ml fr">variable</strong>, is that the predictions we make capture this <strong class="ml fr">uncertainty </strong>with<strong class="ml fr"> transparency</strong>, with a certain HDI % informing the best fit line.</p><figure class="oy oz pa pb pc pq pn po paragraph-image"><div role="button" tabindex="0" class="pr ps ed pt bh pu"><div class="pn po qv"><img src="../Images/1dffd2b876d486a412c8f23896cfd326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQOpNM6gV8ZzOnyBGvQVbw.png"/></div></div><figcaption class="pw px py pn po pz qa bf b bg z dx">95% HDI line of best fit (Image from code by Author)</figcaption></figure><p id="2e00" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Model comparison</strong></p><p id="3136" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the Bayesian framework, the Watanabe-Akaike Information Metric (<strong class="ml fr">WAIC</strong>) score is the widely accepted choice for model comparison. A simple explanation of the WAIC score is that it estimates the model <strong class="ml fr">likelihood</strong> while <strong class="ml fr">regularizing</strong> for the number of model parameters. In simple words, it can account for overfitting. This is also major draw of the Bayesian framework — one does <strong class="ml fr">not</strong> necessarily <strong class="ml fr">need</strong> to hold-out a model <strong class="ml fr">validation</strong> dataset. Hence,</p><blockquote class="qc qd qe"><p id="ee21" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Bayesian modeling offers a crucial advantage when data is scarce.</p></blockquote><p id="ac3f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The WAIC score is a <strong class="ml fr">comparative</strong> measure i.e. it only holds meaning when compared across different models that attempt to explain the same underlying data. Thus in practice, one can keep adding more complexity to the model as long as the WAIC increases. If at some point in this process of adding maniacal complexity, the WAIC starts dropping, one can call it a day — any more complexity will not offer an informational advantage in describing the underlying data distribution.</p><h2 id="4d91" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk"><strong class="al">Conclusion</strong></h2><p id="5a86" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">To summarize, the STAN model block is simply a string. It explains to STAN what you are going to give to it (model), what is to be found (parameters), what you think is going on (model), and what it should give you back (generated quantities).</p><p id="21bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When turned on, STAN simple turns the crank and gives its output.</p><blockquote class="qc qd qe"><p id="c702" class="mj mk qb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The real challenge lies in defining a proper model (refer priors), structuring the data appropriately, asking STAN exactly what you need from it, and evaluating the sanity of its output.</p></blockquote><p id="68cc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once we have this part down, we can delve into the real power of STAN, where specifying increasingly complicated models becomes just a simple syntactical task. In fact, in our next tutorial we will do exactly this. We will build upon this simple regression example to explore Bayesian <strong class="ml fr">Hierarchical</strong> models: an industry standard, state-of-the-art, defacto… you name it. We will see how to add group-level radom or fixed effects into our models, and marvel at the ease of adding complexity while maintaining comparability in the Bayesian framework.</p><p id="3ac4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Subscribe if this article helped, and to stay-tuned for more!</p><h2 id="9702" class="oh nh fq bf ni oi oj ok nl ol om on no ms oo op oq mw or os ot na ou ov ow ox bk">References</h2><p id="55e7" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">[1] Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari and Donald B. Rubin (2013). <em class="qb">Bayesian Data Analysis, Third Edition</em>. Chapman and Hall/CRC.</p></div></div></div></div>    
</body>
</html>