- en: Run Mixtral-8x7B on Consumer Hardware with Expert Offloading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/run-mixtral-8x7b-on-consumer-hardware-with-expert-offloading-bd3ada394688?source=collection_archive---------1-----------------------#2024-01-11](https://towardsdatascience.com/run-mixtral-8x7b-on-consumer-hardware-with-expert-offloading-bd3ada394688?source=collection_archive---------1-----------------------#2024-01-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finding the right trade-off between memory usage and inference speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--bd3ada394688--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--bd3ada394688--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bd3ada394688--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bd3ada394688--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--bd3ada394688--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bd3ada394688--------------------------------)
    ·8 min read·Jan 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0878942c81ee58eeef1c04a5a585e7d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Activation pattern of Mixtral-8x7B’s expert sub-networks — [source](https://arxiv.org/pdf/2312.17238.pdf)
    (CC-BY)
  prefs: []
  type: TYPE_NORMAL
- en: While Mixtral-8x7B is one of the best open large language models (LLM), it is
    also a huge model with 46.7B parameters. Even when quantized to 4-bit, the model
    can’t be fully loaded on a consumer GPU (e.g., an RTX 3090 with 24 GB of VRAM
    is not enough).
  prefs: []
  type: TYPE_NORMAL
- en: Mixtral-8x7B is a [mixture of experts (MoE)](https://kaitchup.substack.com/p/mixtral-8x7b-understanding-and-running).
    It is made of 8 expert sub-networks of 6 billion parameters each.
  prefs: []
  type: TYPE_NORMAL
- en: Since only 2 experts among 8 are effective during decoding, the 6 remaining
    experts can be moved, or *offloaded*, to another device, e.g., the CPU RAM, to
    free up some of the GPU VRAM. In practice, this offloading is complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing which one of the experts to activate is a decision taken at inference
    time for each input token and each layer of the model. Naively moving some parts
    of the model to the CPU RAM, as with [Accelerate’s device_map](https://kaitchup.substack.com/p/device-map-avoid-out-of-memory-errors-when-running-large-language-models-af7de5076f9d),
    would create a communication bottleneck between the CPU and the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[Mixtral-offloading](https://github.com/dvmazur/mixtral-offloading) (MIT license)
    is a project that proposes a much more efficient solution to reduce VRAM consumption
    while preserving a reasonable inference speed.'
  prefs: []
  type: TYPE_NORMAL
