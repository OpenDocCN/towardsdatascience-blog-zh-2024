<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Line-By-Line, Let’s Reproduce GPT-2: Section 2 — Hardware Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Line-By-Line, Let’s Reproduce GPT-2: Section 2 — Hardware Optimization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31">https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-2-hardware-optimization-86e71c91d9bb?source=collection_archive---------15-----------------------#2024-07-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2849" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post will go line-by-line through the hardware optimizations in Section 2 of Andrej Karpathy’s “Let’s reproduce GPT-2 (124M)”</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Gunton" class="l ep by dd de cx" src="../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F8sHS2ai6w95qbGIZ9qM_g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mgunton7?source=post_page---byline--86e71c91d9bb--------------------------------" rel="noopener follow">Matthew Gunton</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--86e71c91d9bb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/6eaae334a9023076271e9cf731d1385a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*J2bpqgJai7a8PiVLVv60FA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — SDXL</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c270" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">As a quick recap, <a class="af oe" href="https://medium.com/towards-data-science/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492" rel="noopener">in Section 1</a> we went line-by-line through the code written by Karpathy to naively train GPT-2. Now that we have our setup, Karpathy shows us how we can make the model train fast on our NVIDIA GPU! While we know that it takes a lot of time to train good models, by optimizing each run we can shave days or even weeks off our training time. This naturally gives us more iterations to improve our model. By the end of this blog post, you will see how to radically speed up your training (by 10x) using an Ampere-series Nvidia GPU.</p><p id="4120" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To do this blog post, I ran the optimizations both on the NVIDIA T4 GPU that Google Colab gives you for free and on a NVIDIA A100 GPU 40GB SXM4 from Lambda Labs. Most of the optimizations Karpathy goes over are specifically for an A100 or better, but there are still some gains to be made on less powerful GPUs.</p><p id="917d" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Let’s dive in!</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4623" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Timing Our Code</h1><p id="d354" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">To begin, we want to create a way to see how effective our optimizations are. To do so, we will add in to our training loop the below code:</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="315b" class="ps oo fq pp b bg pt pu l pv pw">for i in range(50):<br/>    t0 = time.time() # start timer<br/>    x, y = train_loader.next_batch()<br/>    x, y = x.to(device), y.to(device)<br/>    optimizer.zero_grad() <br/>    logits, loss = model(x, y)<br/>    loss.backward() <br/>    optimizer.step()<br/>    torch.cuda.synchronize() # synchronize with GPU<br/>    t1 = time.time() # end timer<br/>    dt = (t1-t0)*1000 # milliseconds difference<br/>    print(f"loss {loss.item()}, step {i}, dt {dt:.2f}ms")</span></pre><p id="65f4" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We start off by capturing the time at the beginning of the loop, but before we capture the end time we run <code class="cx px py pz pp b">torch.cuda.synchronize()</code>. By default we are only paying attention to when the CPU stops. Because we have moved most of the major calculations to GPU, we need to make sure that our timer here takes into account when the GPU stops its calculations. Synchronize will have the CPU wait to progress until the GPU has completed its work queue, giving us an accurate time for when the loop was completed. Once we have an accurate time, we naturally calculate the difference between the start and the end.</p><h1 id="8816" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Batch Sizing</h1><p id="d1fb" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">We also want to make sure we are putting as much data as possible through each round. The way we achieve this is by setting batch sizes. In our <code class="cx px py pz pp b">DataLoaderLite</code> class, we can adjust our 2 parameters (B and T) so that we use the most amount of memory in our GPU without going out of bounds.</p><p id="66c0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">With the A100 GPU, you can follow Karpathy’s example, where we set T equal to the max <code class="cx px py pz pp b">block_size</code> of 1024 and we set B equal to 16 because it’s a “nice” number (easily divisible by powers of 2) and it’s the largest such “nice” number we can fit in memory.</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="1d88" class="ps oo fq pp b bg pt pu l pv pw">train_loader = DataLoaderLite(B=16, T=1024)</span></pre><p id="70f7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If you try to put in a value that is too large, you’ll wind up seeing a <code class="cx px py pz pp b">OutOfMemoryError</code> from CUDA in your terminal. I found the best values for a T4 GPU I could get was B =4 and T =1024 (<em class="qf">when trying different B values in Google Colab, be aware you may need to restart the session to ensure you’re not getting false positive </em><code class="cx px py pz pp b">OutOfMemoryError</code><em class="qf">s</em>)</p><p id="aa15" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Running on the A100 and T4 below, I get the following graphs showing training time to start (on average roughly 1100ms on the T4 and 1040ms on the A100)</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qg"><img src="../Images/4982d21ba78b861a4b97a097448bf3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_6b53elSPbAmkMWL7wR8FA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — A100 Training with no optimizations</figcaption></figure><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qh"><img src="../Images/ffb306ee31f999c89ad7ab83e1e33a44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRwgzJSSb-O4nuw0LPravg.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — T4 training with no optimizations</figcaption></figure><h1 id="6014" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Floating Point Optimizations</h1><p id="44ed" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Now we’re going to focus on changes we make to the internal representation of data within the model.</p><p id="adca" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If you look at the <code class="cx px py pz pp b">dtype</code> of the weights in our code from section 1, you will see we use Floating Point 32 (fp32) by default. Fp32 means that we represent the numbers using 32 bits following the IEEE floating point standard below:</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qi"><img src="../Images/dc819a07b517ac882a19fa1a53a3de13.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*LchRH5weHqdn73_L3_gZ5g.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — IEEE Representation of Floating Point 32 (FP32)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9e6c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">As Karpathy says in the video, we have seen empirically that fp32 isn’t necessary to train quality models — we can use less data to represent each weight and still have quality outputs. One way to speed up the calculations is to use NVIDIA’s TensorCore instruction. This will handle the matrix multiplications by converting the operands to the form Tensor Float 32 (TF32) laid out below:</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qj"><img src="../Images/b15aba27d3e3263698eb170290bd1844.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*sjvH1bR3J5wPVZvhp9TQKQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — Tensor Float 32 (TF32)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qk"><img src="../Images/eddcc5b41eeeae51b530a74cb8e2ec35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ncielZqhaY8k6qKwJ9tUA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — TF32 data flow through a Tensor Core post optimization</figcaption></figure><p id="d361" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">From the code point of view, all of our variables (input, output) are in FP32, but the NVIDIA GPU will convert the intermediary matrices to TF32 for speedup. This according to NVIDIA drives an 8x speed up<a class="af oe" href="https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedflops.htm" rel="noopener ugc nofollow" target="_blank"> versus a FFMA instruction.</a> To enable TF32 in PyTorch, we only need to add the below line (high = TF32, highest = FP32, medium=BF16 (more on that later)):</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="6fa6" class="ps oo fq pp b bg pt pu l pv pw">torch.set_float32_matmul_precision("high")</span></pre><p id="b2bb" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">TensorCore is unique to NVIDIA and you can only run TF32 on an A100 GPU or better, so some developers have used Floating Point 16 (FP16) as a way to train. The problem with this representation is that the range of data that FP16 can capture is smaller than FP32, leading to problems representing the same data range needed for training. While you can get around this using gradient expansion, this requires more calculations so you wind up in a situation where you take 1 step forwards, 2 steps back.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp ql"><img src="../Images/c607218da80ec3a96abac4629db209e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*6UB-fAaUQAOoGApcmCczTQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — IEEE Representation of Floating Point 16 (FP16)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2cf7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Instead, the data optimization Karpathy uses in his video is brain floating point (BF16). Here we have the same number of exponent bits as FP32, so we can represent the same range, but we have fewer mantissa bits. This means that while we have fewer bits, our precision in representing numbers is lower. Empirically, this has not caused major reduction in performance, so it’s a tradeoff we’re willing to make. To use this on NVIDIA chips, you need to have an A100.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qm"><img src="../Images/0d7891f96b11cdaa642777ec0cf19edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*c1ffREihw-d8sB8ZO9ONXQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — Brain Floating Point 16 (BF16)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a272" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Using PyTorch, we don’t need to change our code dramatically to use the new data type. The documentation advises us to only use these during the forward pass of your model and loss calculation. As our code does both of these in 1 line, we can modify our code as below:</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="b716" class="ps oo fq pp b bg pt pu l pv pw">for i in range(50):<br/>    t0 = time.time() <br/>    x, y = train_loader.next_batch()<br/>    x, y = x.to(device), y.to(device)<br/>    optimizer.zero_grad() <br/>    with torch.autocast(device_type=device, dtype=torch.bfloat16): # bf16 change<br/>        logits, loss = model(x, y)<br/>    loss.backward() <br/>    optimizer.step()<br/>    torch.cuda.synchronize() <br/>    t1 = time.time()<br/>    dt = (t1-t0)*1000 <br/>    print(f"loss {loss.item()}, step {i}, dt {dt:.2f}ms")<br/>    loss_arr.append(loss.item())</span></pre><p id="8287" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Just like that, our code is now running using BF16.</p><p id="84ed" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Running on our A100, we now see that the average step takes about 330ms! We’ve already reduced our runtime by about 70%, and we’re just getting started!</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qn"><img src="../Images/a2fd83867ed991a7df45314ca29e3492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxSEgLBTMqv8FhgtOMkfww.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — A100 Training after data type optimizations</figcaption></figure><h1 id="cd2d" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Torch Compile</h1><p id="d43f" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">We can further improve our training time by utilizing the PyTorch Compile feature. This will give us fairly big performance increases without having to adjust our code one bit.</p><p id="9817" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To come at it from a high-level, every computer program is executed in binary. Because most people find it difficult to code in binary, we have created higher-level languages that let us code in forms that are easier for people to think in. When we compile these languages, they are transformed back into binary that we actually run. Sometimes in this translation, we can figure out faster ways to do the same calculation — such as reusing a certain variable or even simply not doing one to begin with.</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="bdfe" class="ps oo fq pp b bg pt pu l pv pw"># ...<br/>model = GPT(GPTConfig(vocab_size=50304))<br/>model.to(device)<br/>model = torch.compile(model) # new line here<br/># ...</span></pre><p id="aa3e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This brings us now to machine learning and PyTorch. Python is a high-level language but we’re still doing computationally intense calculations with it. When we run <code class="cx px py pz pp b">torch compile</code> we are spending more time compiling our code, but we wind up seeing our runtime (the training for us here) go a lot faster because of that extra work we did to find those optimizations.</p><p id="733c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Karpathy gives the following example of how PyTorch may improve the calculations. Our GELU activation function can be written out like below:</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="6166" class="ps oo fq pp b bg pt pu l pv pw">class TanhGELU(nn.Module):<br/>    def forward(self, input):<br/>        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0/math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))</span></pre><p id="e4c7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">For each calculation you see in the above function, we have to dispatch a kernel in the GPU. This means that when we start off by taking input to the third power, we pull input from high-bandwidth memory (HBM) into the GPU cores and do our calculation. We then write back to HBM before we start our next calculation and begin the whole process over again. Naturally, this sequencing is causing us to spend a lot of time waiting for memory transfers to occur.</p><p id="bcd9" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">PyTorch compile allows us to see an inefficiency like this and be more careful with when we are spinning up new kernels, resulting in dramatic speed ups. This is called kernel fusion.</p><p id="b436" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">While on this topic, I’d like to point out an excellent open-source project called Luminal that takes this idea a little further. <a class="af oe" href="https://github.com/jafioti/luminal" rel="noopener ugc nofollow" target="_blank">Luminal is a separate framework that you write your training / inferencing in</a>. By using this framework, you get access to its compiler which finds many more optimizations for you by nature of having a more limited number of computations to consider. If you like the idea of improving runtime by compiling fast GPU code, give the project a look.</p><p id="1bea" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">When we run the above code now we see that we see each step takes roughly 145 ms (cutting by 50% from before and ~86% from the original). We pay for this with the first iteration which took roughly 40,000ms to run! As most training sequences have many more steps than 50, this tradeoff is one that we are willing to make.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qo"><img src="../Images/77e1cd818db8c6ec28081e44b6f4e121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-HnQLtKL52tm_z8k5RrwtQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — A100 Training run after the Torch Compile optimizations</figcaption></figure><h1 id="c269" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Flash Attention</h1><p id="f60b" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Another optimization we make is using Flash Attention (<a class="af oe" href="https://arxiv.org/pdf/2205.14135" rel="noopener ugc nofollow" target="_blank">see the paper here</a>). The code change itself is very simple for us, but the thinking behind it is worth exploring.</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="8281" class="ps oo fq pp b bg pt pu l pv pw">y = F.scaled_dot_product_attention(q, k, v, is_causal=True)</span></pre><p id="2f49" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Similar to how we condensed the <code class="cx px py pz pp b">TanhGELU</code> class into as few kernels as we could, we apply the same thinking to attention. In their paper, <a class="af oe" href="https://arxiv.org/pdf/2205.14135" rel="noopener ugc nofollow" target="_blank">“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”</a>, the authors show how you can achieve a 7.6x speed up by fusing the kernel. While in theory torch compile should be able to find optimizations like this, in practice we haven’t seen it find this yet.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qp"><img src="../Images/0a620953fcf8999cd84bae98c7f64114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rH-TmS5rR-qvc8wYpJWqZw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Figure 1 from <a class="af oe" href="https://arxiv.org/pdf/2205.14135" rel="noopener ugc nofollow" target="_blank">“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”</a></figcaption></figure><p id="907f" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The paper is worth doing a deep dive on, but to give a quick synopsis, FlashAttention is written to be IO-aware, thus preventing unnecessary (and time-consuming) calls to memory. By reducing these, they can radically speed up the calculations.</p><p id="c082" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">After implementing this, we find that we now have an average step of about 104ms.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qn"><img src="../Images/a1f72e2c0244950e414977351ca9932a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2oq1mlVXnGqzBBX7ZPEO0g.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — A100 Training after Flash Attention Optimization</figcaption></figure><h1 id="3043" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Vocab Size Change</h1><p id="75fa" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Finally, we can go through all of the numbers we have hard-coded and evaluate how “nice” they are. When we do this, we find that the vocabulary size is not divisible by many powers of 2 and so will be more time-consuming for our GPU’s memory to load in. We fix this by going from the 50,257 vocab size to the next “nice” number, which is 50,304. This is a nice number as it’s cleanly divisible by 2, 4, 8, 16, 32, 64, and 128.</p><pre class="mr ms mt mu mv po pp pq bp pr bb bk"><span id="8ebb" class="ps oo fq pp b bg pt pu l pv pw">model = GPT(GPTConfig(vocab_size=50304))</span></pre><p id="485e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now you may remember from the last blog post that our vocab size is not an arbitrary value — it is determined by the tokenizer we are using. Thus begs the question, When we arbitrarily add in more values to our vocab size, what happens? During the training, the model will notice that these new vocab never appear, so it will start to push the probabilities of these tokens to 0 — thus our performance is safe. That does not mean that there is no tradeoff though. By loading into memory vocab that is never used, we are wasting time. However, empirically we can see that loading in “nice” numbers more than compensates for this cost.</p><p id="172e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">With our last optimization, we now have an average of about 100 ms per step.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qq"><img src="../Images/6e684ff36d2019e35a382dc8544b893e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7j_mIM---rhVLvwe__TxBA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — A100 Training after Vocab Size Optimization</figcaption></figure><p id="4389" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">With this final optimization, we find that our training has improved ~10x from the beginning!</p><h1 id="4cb9" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">What optimizations work on T4 GPU?</h1><p id="404a" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">If you’ve been following along but only have access to the consumer-grade T4 GPU, you may wonder which optimizations you can use. To recap, we cannot use the BF16 representation, but we can use the vocabulary size change, flash attention, and torch compile. <a class="af oe" href="https://colab.research.google.com/drive/1NhX0dDHF8mpzQGG9vjUJ16qiMbL7QI5Y" rel="noopener ugc nofollow" target="_blank">To see this code in action, check out my Google Colab notebook, which is optimized just for T4 usage</a>.</p><p id="8904" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We can see from the graph below that while the torch compile does take a lot of time for the first round, the next rounds are not significantly better than the unoptimized versions (roughly an 8% drop on T4 vs 90% drop on A100).</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qr"><img src="../Images/f95034c6a9cd38614158cf644977592e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLSXzvYuQvYg3zcQDjMh7A.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — Optimized run on T4 GPU</figcaption></figure><p id="82ec" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Nevertheless, when OpenAI was training GPT-2 it was running on far more advanced hardware than the T4. The fact that we can run this workload on a T4 today suggests that hardware requirements are becoming less onerous, helping create a future where hardware is not a barrier to ML work.</p><h1 id="4a94" class="on oo fq bf op oq qa gq os ot qb gt ov ow qc oy oz pa qd pc pd pe qe pg ph pi bk">Closing</h1><p id="50a1" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">By optimizing our code, we’ve seen major speed ups and also learned a bit about where the big bottlenecks for training happen. First and foremost, datatypes are critically important for speed, as this change by itself contributed majorly to the speed ups. Second, we see that hardware optimizations can play a major role in speeding up calculations — so GPU hardware is worth its weight in gold. Finally, compiler optimizations have a major role to play here as well.</p><p id="c32e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To see the code I ran in the A100, <a class="af oe" href="https://gist.github.com/matthewjgunton/3e2d9c9be60499cf6460d197917b976e" rel="noopener ugc nofollow" target="_blank">check out this gist here</a>. If you have any suggestions for how to optimize the hardware further, I would love to see them in the comments!</p><p id="0ceb" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">It’s an exciting time to be building!</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b598" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[1] Karpathy, A., <a class="af oe" href="https://youtu.be/l8pRSuU81PU?feature=shared" rel="noopener ugc nofollow" target="_blank">“Let’s reproduce GPT-2 (124M)”</a> (2024), YouTube</p><p id="4cb7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[2] Dao, T., et al. <a class="af oe" href="https://arxiv.org/pdf/2205.14135" rel="noopener ugc nofollow" target="_blank">“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”</a> (2022), arXiv</p><p id="93a4" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[3] Krashinsky, R., et al “<a class="af oe" href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/" rel="noopener ugc nofollow" target="_blank">NVIDIA Ampere Architecture In-Depth</a>” (2020), NVIDIA Developer</p></div></div></div></div>    
</body>
</html>