<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand ✍︎</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand ✍︎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02">https://towardsdatascience.com/deep-dive-into-soras-diffusion-transformer-dit-by-hand-%EF%B8%8E-1e4d84ec865d?source=collection_archive---------3-----------------------#2024-04-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ac65" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the secret behind Sora’s state-of-the-art videos</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--1e4d84ec865d--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1e4d84ec865d--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/5fbe86c67353e44883ee9150b7907a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uaVWQu6H49BlVl6OAsM83g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure><p id="f641" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">“In the ancient land of DiTharos, there once lived a legend, called Sora. A legend that embodied the very essence of unlimited potential, encompassing the vastness and the magnificence of the skies.</em></p><p id="6572" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">When it soared high with its iridescent wings spanning vast expanses and light bouncing off its striking body, one could hear the words ‘Sora IS Sky’ echoing into the heavens. What made it a legend was not just its epic enormity but its power to harness the elements of light scattered in the swirling clouds. With its mighty strength, the magic that Sora created with a single twirl, was a sight to behold!</em></p><p id="a23e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">They say, Sora lives on, honing its skills and getting stronger with each passing day, ready to fly when the hour is golden. When you see a splash of crimson red in the sky today, you would know it’s a speck of the legend flying into the realms of light!”</em></p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8d34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is a story I told my son about a mythical dragon that lived in a far away land. We called it ‘The Legend of Sora’. He really enjoyed it because Sora is big and strong, and illuminated the sky. Now of course, he doesn’t understand the idea of transformers and diffusion yet, he’s only four, but he does understand the idea of a magnanimous dragon that uses the power of light and rules over DiTharos.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a3017fb7070ef36c0941deefb160ee3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiyuPP6RULEoJhftjwGoBw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author (The powerful Sora by my son — the color choices and the bold strokes are all his work.)</figcaption></figure><h1 id="ceb1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">Sora by Open AI</strong></h1><p id="e05e" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">And that story very closely resembles how our world’s Sora, Open AI’s text-to-video model emerged in the realm of AI and has taken the world by storm. In principle, Sora is a diffusion transformer (DiT) developed by<a class="af pi" href="https://www.linkedin.com/in/william-peebles-a980a212a/" rel="noopener ugc nofollow" target="_blank"> William Peebles</a> and<a class="af pi" href="https://www.linkedin.com/in/sainxie/" rel="noopener ugc nofollow" target="_blank"> Saining Xie</a> in 2023.</p><p id="0caf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In other words, it uses the idea of diffusion for predicting the videos and the strength of transformers for next-level scaling. To understand this further, let’s try to find the answer to these two questions:</p><ul class=""><li id="9074" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk">What does Sora do when given a prompt to work on?</li><li id="58a5" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">How does it combine the diffusion-transformer ideas?</li></ul><p id="2db0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Talking about the videos made by Sora, here is my favorite one of an adorable Dalmatian in the streets of Italy. How natural is its movement!</p><figure class="mm mn mo mp mq mr"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="a0e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The prompt used for the video : “The camera directly faces colorful buildings in Burano Italy. An adorable dalmation looks through a window on a building on the ground floor. Many people are walking and cycling along the canal streets in front of the buildings.”</p><p id="e890" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How did Sora do this?</p><p id="9abf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Without any further ado, let’s dive into the details and look at how Sora creates these super-realistic videos based on text-prompts.</p><h1 id="be17" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">How does Sora work?</strong></h1><p id="a38d" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Thanks once again to Prof. Tom Yeh’s wonderful AI by Hand Series, we have this <a class="af pi" href="https://www.linkedin.com/feed/update/urn:li:share:7165412130224033793/" rel="noopener ugc nofollow" target="_blank">great piece on Sora</a> for our discussion. (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned LinkedIn post, which I have edited with his permission.)</p><p id="a984" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, here we go:</p><p id="e2af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Our goal</strong> — Generate a video based on a text-prompt.</p><p id="f046" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We are given:</p><ul class=""><li id="69de" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk">Training video</li><li id="f966" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">Text-prompt</li><li id="1f45" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">Diffusion step <em class="ny">t</em> = 3</li></ul><p id="b13f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For our example, can you guess what our text-prompt is going to be? You are right. It is “Sora is sky”. A diffusion step of<em class="ny"> t</em> = 3 means we are adding noise or diffusing the model in three steps but for illustration we will stick to one in this example.</p><blockquote class="pu pv pw"><p id="55b4" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">What is diffusion?</strong></p><p id="86ae" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Diffusion mainly refers to the phenomenon of scattering of particles — think how we enjoy the soft sun rays making a peek from behind the clouds. This soft glow can be attributed to the scattering of sunlight as it passes through the cloud layer causing the rays to spread out in different directions.</p><p id="2207" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The random motion of the particles drives this diffusion. And that is exactly what happens for diffusion models used in image generation. Random noise is added to the image causing the elements in the image to deviate from the original and thus making way for creating more refined images.</p><p id="11f1" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we talk about diffusion in regards to image-models, the key idea to remember is ‘noise’.</p></blockquote><p id="f781" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The process begins here:</p><p id="c39e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[1]<strong class="ne fr"> Convert video into patches</strong></p><p id="2c1a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When working with text-generation, the models break down the large corpus into small pieces called tokens and use these tokens for all the calculations. Similarly, Sora breaks down the video into smaller elements called visual patches to make the work simpler.</p><p id="75f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we are talking about a video, we are talking about images in multiple frames. In our example, we have four frames. Each of the four frames or matrices contain the pixels that create the image.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/f4692e227bf767ec00f7bee6b9feb7e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*SAxjwPEfvNwlkcZTxB82LA.png"/></div></figure><p id="17c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The first step here is to convert this training video into 4 spacetime patches as below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/f6acaf339dcea5ba8aad7eda0a89d8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*PJX0W_jhfoUqHP8OGjiQmw.gif"/></div></figure><p id="049e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2]<strong class="ne fr"> Reduce the dimension of these visual patches : Encoder</strong></p><p id="eaee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, dimension reduction. The idea of dimension reduction has existed for over a century now <em class="ny">(Trivia : Principal Component Analysis, also known as PCA was introduced by Karl Pearson in 1901)</em>, but its significance hasn’t faded over time.</p><p id="5d20" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And Sora uses it too!</p><p id="75c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When we talk about Neural Networks, one of the fundamental ideas for dimension reduction is the encoder. Encoder, by its design, transforms high-dimensional data into lower-dimension by focusing on capturing the most relevant features of the data. Win-win on both sides: it increases the efficiency and speed of the computations while the algorithm gets useful data to work on.</p><p id="12d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Sora uses the same idea for converting the high-dimensional pixels into a lower-dimensional latent space. To do so, we multiply the patches with weights and biases, followed by ReLU.</p><blockquote class="pu pv pw"><p id="e025" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Note</strong>:</p><p id="44db" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Linear transformation : The input embedding vector is multiplied by the weight matrix <strong class="ne fr">W </strong>and</p><p id="f2de" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">then added with the bias vector <strong class="ne fr">b</strong>,</p><p id="0de8" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">z = <strong class="ne fr">W</strong>x+<strong class="ne fr">b</strong>, where <strong class="ne fr">W</strong> is the weight matrix, x is our word embedding and <strong class="ne fr">b</strong> is the bias vector.</p><p id="7406" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ReLU activation function : Next, we apply the ReLU to this intermediate z.</p><p id="4fdb" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ReLU returns the element-wise maximum of the input and zero. Mathematically, h = max{0,z}.</p></blockquote><ul class=""><li id="f576" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk">The weight matrix here is a 2x4 matrix [ [1, 0, -1, 0], [0, 1, 0, 1] ] with the bias being [0,1].</li><li id="cdd1" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">The patches matrix here is 4x4.</li></ul><p id="e31c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The product of the transpose of the weight matrix <strong class="ne fr">W</strong> and bias <strong class="ne fr">b</strong> with the patches followed by <strong class="ne fr">ReLU</strong> gives us a latent space which is only a 2x4 matrix. Thus, by using the visual encoder the dimension of the ‘model’ is reduced from 4 (2x2x1) to 2 (2x1).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/133c33cc9f5459d066951674310d428e.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*68tKXg6QZfJo85bV38weTQ.gif"/></div></figure><p id="e8c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the original DiT paper, this reduction is from 196,608 (256x256x3) to 4096 (32x32x4), which is huge. Imagine working with 196,608 pixels against working with 4096 — a 48 times reduction!</p><p id="31a5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Right after this dimension reduction, we have one of the most significant steps in the entire process — <strong class="ne fr">diffusion</strong>.</p><p id="e862" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3]<strong class="ne fr"> Diffuse the model with noise</strong></p><p id="f6d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To introduce diffusion, we add sampled noise to the obtained latent features in the previous step to find the Noised Latent. The goal here is to <strong class="ne fr">ask the model to detect what the noise is</strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/9a4857023e2bc12e838a6824b7b12128.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*AkykcFSkG6a3T-XxLhdUfg.gif"/></div></figure><p id="e83e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is in essence the idea of diffusion for image generation.</p><p id="c785" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By adding noise to the image, the model is asked to guess what the noise is and what it looks like. In return, the model can generate a completely new image based on what it guessed and learnt from the noisy image.</p><blockquote class="pu pv pw"><p id="79ab" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It can also be seen relative to deleting a word from the language model and asking it to guess what the deleted word was.</p></blockquote><p id="3d2e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that the training video has been reduced and diffused with noise, the next steps are to make use of the text-prompt to get a video as advocated by the prompt. We do this by conditioning with the adaptive norm layer.</p><h2 id="52d4" class="qa oi fq bf oj qb qc qd om qe qf qg op nl qh qi qj np qk ql qm nt qn qo qp qq bk">[4]-[6] <strong class="al">Conditioning by Adaptive Norm Layer</strong></h2><p id="a317" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">What ‘conditioning’ essentially means is we try to influence the behavior of the model using the additional information we have available. For eg: since our prompt is ‘Sora is sky’, we would like for the model to focus on elements such as sky or clouds rather attaching importance on other concepts like a hat or a plant. Thus, an adaptive norm layer massages, in better terms — <strong class="ne fr">dynamically scales and shifts the data</strong> in the network based on the input it receives.</p><blockquote class="pu pv pw"><p id="8731" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is scale and shift?</p><p id="23dc" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Scale occurs when we multiply, for e.g. we may start with a variable A. When we multiply it with 2 suppose, we get 2*A which amplifies or scales the value of A up by 2. If we multiply it by ½, the value is scaled down by 0.5.</p><p id="77ad" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Shift is denoted by addition, for e.g. we may be walking on the number line. We start with 1 and we are asked to shift to 5. What do we do? We can either add 4 and get 1+4=5 or we could add a hundred 0.4s to get to 5, 1+(100*0.04 )= 5. It all depends on if we want to take bigger steps (4) or smaller steps (0.04) to reach our goal.</p></blockquote><p id="dadc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4]<strong class="ne fr"> Encode Conditions</strong></p><p id="cf8a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To make use of the conditions, in our case the information we have for building the model, first we translate it into a form the model understands, i.e., <strong class="ne fr">vectors</strong>.</p><ul class=""><li id="7799" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pj pk pl bk">The first step in the process is to <strong class="ne fr">translate the prompt into a text embedding vector</strong>.</li><li id="54dc" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">The next step is to translate step <em class="ny">t </em>= 3 into a binary vector.</li><li id="792f" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx pj pk pl bk">The third step is to concatenate these vectors together.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qr"><img src="../Images/877ebac8fcd40947dfe7735ccbe9ebdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*BBGGwoBtHQVE2arRc1teyA.gif"/></div></figure><p id="2789" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5]<strong class="ne fr"> Estimate Scale/Shift</strong></p><p id="0390" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Remember that here we use an ‘adaptive’ layer norm which implies that it adapts its values based on what the current conditions of the model are. Thus, to capture the correct essence of the data, we need to include the importance of each element in the data. And it is done by estimating the scale and shift.</p><p id="d9b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For estimating these values for our model, we multiply the concatenated vector of prompt and diffusion step with the weight and add the bias to it. These weights and biases are <strong class="ne fr">learnable parameters </strong>which the model learns and updates.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qs"><img src="../Images/20a96c4d027e0dcb74a4b43e95edcd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*Lgn3lmWnU1xoQE5ezdkMGg.gif"/></div></figure><p id="b016" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(Remark: The third element in the resultant vector, according to me, should be 1. It could be a small error in the original post but as humans we are allowed a bit of it, aren’t we? To maintain uniformity, I continue here with the values from the original post.)</em></p><p id="44a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The goal here is to estimate the scale [2,-1] and the shift [-1,5] (since our model size is 2, we have two scale and two shift parameters). We keep them under ‘X’ and ‘+’ respectively.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qt"><img src="../Images/e1afe750aa12e3c8589b8e306aace95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/1*8pHF6-AdrPbn3-3V_rqRWg.gif"/></div></figure><p id="e454" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[6] <strong class="ne fr">Apply Scale/Shift</strong></p><p id="886e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To apply the scale and shift obtained in the previous step, we multiply the noised latent in Step 3 by [2, -1] and shift it by adding [-1,5].</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/b07c1988760b5d313712184f1a634222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1jFIrL4X-p7iSoTWkInfA.png"/></div></div></figure><p id="48dd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The result is the <strong class="ne fr">‘conditioned’</strong> noise latent.</p><h2 id="69cf" class="qa oi fq bf oj qb qc qd om qe qf qg op nl qh qi qj np qk ql qm nt qn qo qp qq bk"><strong class="al">[7]-[9] Transformer</strong></h2><p id="5edd" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The last three steps consist of adding the transformer element to the above diffusion and conditioning steps. This step help us find the noise as predicted by the model.</p><p id="fb68" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[7] <strong class="ne fr">Self-Attention</strong></p><p id="fc0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is the critical idea behind transformers that make them so phenomenal!</p><blockquote class="pu pv pw"><p id="b5a2" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What is self-attention?</p><p id="2563" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is a mechanism by which each word in a sentence analyzes every other word and measures how important they are to each other, making sense of the context and relationships in the text.</p></blockquote><p id="aeb7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To enable self-attention, the conditioned noise latent is fed into the Query-Key function to obtain a self-attention matrix. The QK-values are omitted here for simplicity.</p><p id="f8c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[8] <strong class="ne fr">Attention Pooling</strong></p><p id="e3c5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we multiply the conditioned noised latent with the self-attention matrix to obtain the attention weighted features.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/f1c33717362b9918f946a25cb164dbee.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*FLIwrZPvgdBYWc0ZWJ0lyw.gif"/></div></figure><p id="92ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[9] <strong class="ne fr">Point-wise Feed Forward Network</strong></p><p id="3873" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once again returning back to the basics, we multiply the attention-weighted features with weights and biases to obtain the predicted noise.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qw"><img src="../Images/c6887a55e18a4f9d1bf622f42f11cee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*N_Z4wAxaN03sjhRbSH9LoA.gif"/></div></figure><h2 id="4e73" class="qa oi fq bf oj qb qc qd om qe qf qg op nl qh qi qj np qk ql qm nt qn qo qp qq bk"><strong class="al">Training</strong></h2><p id="f14c" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The last bit now is to train the model using <strong class="ne fr">Mean Square Error</strong> between the <strong class="ne fr">predicted noise </strong>and<strong class="ne fr"> the sampled noise</strong> (ground truth).</p><p id="5610" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[10] <strong class="ne fr">Calculate the MSE loss gradients and update learnable parameters</strong></p><p id="296c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using the MSE loss gradients, we use backpropagation to update all the parameters that are learnable (for e.g. the weights and biases in the adaptive norm layer).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qx"><img src="../Images/3a7a6dabeab8658d307d4307e6c19845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RAAErECQUGKpdex2NtPa7Q.gif"/></div></figure><p id="25e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The encoder and decoder parameters are frozen and not learnable.</p><p id="f9fb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(Remark: The second element in the second row should be -1, a tiny error which makes things better).</em></p><h2 id="2272" class="qa oi fq bf oj qb qc qd om qe qf qg op nl qh qi qj np qk ql qm nt qn qo qp qq bk">[11]-[13] <strong class="al">Generate New Samples</strong></h2><p id="ca76" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">[11] <strong class="ne fr">Denoise</strong></p><p id="89c1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that we are ready to generate new videos (yay!), we first need to remove the noise we had introduced. To do so, we subtract the predicted noise from the noise-latent to obtain noise-free latent.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qy"><img src="../Images/3391e576b58b059eef9272423a126735.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*WsJhh08Fw5lkgTHc23LvRg.gif"/></div></figure><p id="dbc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Mind you, this is not the same as our original latent. Reason being we went through multiple conditioning and attention steps in between that included the context of our problem into the model. Thus, allowing the model a better feel for what its target should be while generating the video.</p><p id="240f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[12] <strong class="ne fr">Convert the latent space back to the pixels : Decoder</strong></p><p id="130c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Just like we did for encoders, we multiply the latent space patches with weight and biases while followed by ReLU. We can observe here that after the work of the decoder, the model is back to the original dimension of 4 which was lowered to 2 when we had used the encoder.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/ed3cb93df31d1aa408d4a5f3263997b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*QdbH5c1EI4Z_uBMsylLzHg.gif"/></div></figure><p id="6525" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[13] <strong class="ne fr">Time for the video!</strong></p><p id="4537" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The last step is to arrange the result from the above matrix into a sequence of frames which finally gives us our new video. Hooray!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/e6535b8b94e1c013d089ba5d25dc378d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcW2F3IdufU5p_Eqkbf7Cw.gif"/></div></div></figure><p id="7702" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And with that we come to the end of this supremely powerful technique. <strong class="ne fr">Congratulations, you have created a Sora video!</strong></p><p id="e341" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To summarize all that was said and done above, here are the 5 key points:</p><ol class=""><li id="96f4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ra pk pl bk">Converting the videos into visual patches and then reducing their dimension is essential. A visual encoder is our friend here.</li><li id="6884" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx ra pk pl bk">As the name suggests, diffusion is the name of the game in this method. Adding noise to the video and then working with it at each of the subsequent steps (in different ways) is what this technique relies on.</li><li id="4534" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx ra pk pl bk">Next up is the transformer architecture that enhances the abilities of the diffusion process along with amplifying the scale of the model.</li><li id="6b87" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx ra pk pl bk">Once the model is trained and ready to converge to a solution, the two D’s — denoiser and decoder come in handy. One by removing the noise and the other by projecting the low-dimensional space to its original dimension.</li><li id="e351" class="nc nd fq ne b go pm ng nh gr pn nj nk nl po nn no np pp nr ns nt pq nv nw nx ra pk pl bk">Finally, the resultant pixels from the decoder are rearranged to generate the desired video.</li></ol><p id="d56e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">(<em class="ny">Once you are done with the article, I suggest you to read the story at the beginning once more. Can you spot the similarities between Sora of DiTharos and Sora of our world?</em>)</p><h1 id="dc48" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk"><strong class="al">The Diffusion-Transformer (DiT) Combo</strong></h1><p id="76e2" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The kind of videos Sora has been able to produce, it is worth saying that the Diffusion-Transformer duo is lethal. Along with it, the idea of visual patches opens up an avenue for tinkering with a range of image resolutions, aspect ratios and durations, which allows for utmost experimentation.</p><p id="667b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Overall, it would not be wrong to say that this idea is seminal and without a doubt is here to stay. According to this <a class="af pi" href="https://www.nytimes.com/2024/02/15/technology/openai-sora-videos.html" rel="noopener ugc nofollow" target="_blank">New York Times article </a>, Sora was named after the Japanese word for sky and to evoke the idea of limitless potential. And having witnessed its initial promise, it is true that Sora has definitely set a new frontier in AI. Now it remains to see how well it stands the test of safety and time.</p><p id="d19a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the legend of <strong class="ne fr">DiTharos</strong> goes — “Sora lives on, honing its skills and getting stronger with each passing day, ready to fly when the hour is golden!”</p><p id="2780" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">P.S. If you would like to work through this exercise on your own, here is a blank template for you to use.</p><p id="d246" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af pi" href="https://bit.ly/3vvj6a6" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="aa94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now go have some fun with Sora in <strong class="ne fr">the land of ‘DiTharos’</strong>!</p></div></div></div></div>    
</body>
</html>