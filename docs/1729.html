<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Advanced Retrieval Techniques in a World of 2M Token Context Windows, Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Advanced Retrieval Techniques in a World of 2M Token Context Windows, Part 1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15">https://towardsdatascience.com/advanced-retrieval-techniques-in-a-world-of-2m-token-context-windows-pt-1-2edc0266aabe?source=collection_archive---------10-----------------------#2024-07-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ce43" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring RAG techniques to improve retrieval accuracy</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Meghan Heintz" class="l ep by dd de cx" src="../Images/9eaae6d3d8168086d83ff7100329c51f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Tespb9SFbU5QAxy8f7bhnA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@meghanheintz?source=post_page---byline--2edc0266aabe--------------------------------" rel="noopener follow">Meghan Heintz</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2edc0266aabe--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj mk"><img src="../Images/06308257f080607db3569ca0a65e320f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*4K3_I6r8vJXB4tE_llkibw.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">Visualising AI project launched by Google DeepMind. From <a class="af mx" href="https://unsplash.com/photos/a-bunch-of-different-colored-sprinkles-on-a-pink-background-QhDs9x7o9Jc" rel="noopener ugc nofollow" target="_blank">Unsplash</a> image.</figcaption></figure><h1 id="4368" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">First of all, do we still care about RAG (Retrieval Augmented Generation)?</h1><p id="b8d6" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">Gemini Pro can handle an astonishing 2M token context compared to the paltry 15k we were amazed by when GPT-3.5 landed. Does that mean we no longer care about retrieval or RAG systems? Based on <a class="af mx" href="https://huggingface.co/papers/2407.01370" rel="noopener ugc nofollow" target="_blank">Needle-in-a-Haystack benchmarks</a>, the answer is that while the need is diminishing, especially for Gemini models, advanced retrieval techniques still significantly improve performance for most LLMs. Benchmarking results show that long context models perform well at surfacing specific insights. However, they struggle when a citation is required. <strong class="nw fr">That makes retrieval techniques especially important for use cases where citation quality is important (think law, journalism, and medical applications among others).</strong> These tend to be higher-value applications where lacking a citation makes the initial insight much less useful. Additionally, while the cost of long context models will likely decrease, augmenting shorter content window models with retrievers can be a cost-effective and lower latency path to serve the same use cases. It’s safe to say that RAG and retrieval will stick around a while longer but maybe you won’t get much bang for your buck implementing a naive RAG system.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="mi mj oq"><img src="../Images/6ae67db7cd66874b13651235affe7c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vfkGsF-QuyyaV-gh2l0-mw.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx">From <a class="af mx" href="https://huggingface.co/papers/2407.01370" rel="noopener ugc nofollow" target="_blank">Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems</a> by Laban, Fabbri, Xiong, Wu in 2024. “Summary of a Haystack results of human performance, RAG systems, and Long-Context LLMs. Results are reported using three metrics: Coverage (left), Citation (center), and Joint (right) scores. Full corresponds to model performance when inputting the entire Haystack, whereas Rand, Vect, LongE, KWs, RR3, Orac correspond to retrieval components RAG systems. Models ranked by Oracle Joint Score. For each model, #Wb report the average number of words per bullet point.”</figcaption></figure><h1 id="04c4" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">So what are the retrieval techniques we should be implementing?</h1><p id="7dfc" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">Advanced RAG covers a range of techniques but broadly they fall under the umbrella of pre-retrieval query rewriting and post-retrieval re-ranking. Let’s dive in and learn something about each of them.</p><h1 id="8cad" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Pre-Retrieval — Query Rewriting</h1><p id="b24f" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">Q: “What is the meaning of life?”</p><p id="e606" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">A: “42”</p><p id="0ab9" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">Question and answer asymmetry is a huge issue in RAG systems. A typical approach to simpler RAG systems is to compare the cosine similarity of the query and document embedding. This works when the question is nearly restated in the answer, “What’s Meghan’s favorite animal?”, “Meghan’s favorite animal is the giraffe.”, but we are rarely that lucky.</p><p id="384a" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">Here are a few techniques that can overcome this:</p><h1 id="bde8" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Rewrite-Retrieve-Read</h1><p id="02e4" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">The nomenclature “Rewrite-Retrieve-Read” originated from a <a class="af mx" href="https://arxiv.org/pdf/2305.14283" rel="noopener ugc nofollow" target="_blank">paper</a> from the Microsoft Azure team in 2023 (although given how intuitive the technique is it had been used for a while). In this study, an LLM would rewrite a user query into a search engine-optimized query before fetching relevant context to answer the question.</p><p id="0991" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">The key example was how this query, <em class="pa">“What profession do Nicholas Ray and Elia Kazan have in common?”</em> should be broken down into two queries, <em class="pa">“Nicholas Ray profession”</em> and <em class="pa">“Elia Kazan profession”</em>. This allows for better results because it’s unlikely that a single document would contain the answer to both questions. By splitting the query into two the retriever can more effectively retrieve relevant documents.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="mi mj pb"><img src="../Images/f37621b4a81d6edc33b6da094587402c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_XZohrABVf_Xq-mg7fXAdA.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx"><a class="af mx" href="https://arxiv.org/pdf/2305.14283" rel="noopener ugc nofollow" target="_blank">From Query Rewriting for Retrieval-Augmented Large Language Models</a> by Ma, Gong, He, Zhao, &amp; Duan in 2023 “(a) standard retrieve-then-read method, (b) LLM as a query rewriter for rewrite-retrieve-read pipeline, and (c ) trainable rewriter.”</figcaption></figure><p id="b853" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">Rewriting can also help overcome issues that arise from “distracted prompting”. Or instances where the user query has mixed concepts in their prompt and taking an embedding directly would result in nonsense. For example, “<em class="pa">Great, thanks for telling me who the Prime Minister of the UK is. Now tell me who the President of France is”</em> would be rewritten like <em class="pa">“current French president”.</em> This can help make your application more robust to a wider range of users as some will think a lot about how to optimally word their prompts, while others might have different norms.</p><h1 id="2685" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Query Expansion</h1><p id="3afb" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">In query expansion with LLMs, the initial query can be rewritten into multiple reworded questions or decomposed into subquestions. Ideally, by expanding the query into several options, the chances of lexical overlap increase between the initial query and the correct document in your storage component.</p><p id="207d" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">Query expansion is a concept that predates the widespread usage of LLMs. Pseudo Relevance Feedback (PRF) is a technique that inspired some LLM researchers. In PRF, the top-ranked documents from an initial search to identify and weight new query terms. With LLMs, we rely on the creative and generative capabilities of the model to find new query terms. This is beneficial because LLMs are not restricted to the initial set of documents and can generate expansion terms not covered by traditional methods.</p><p id="966a" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk"><a class="af mx" href="https://arxiv.org/abs/2402.18031" rel="noopener ugc nofollow" target="_blank">Corpus-Steered Query Expansion (CSQE)</a> is a method that marries the traditional PRF approach with the LLMs’ generative capabilities. The initially retrieved documents are fed back to the LLM to generate new query terms for the search. This technique can be especially performant for queries for which LLMs lacks subject knowledge.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pc"><img src="../Images/f059b785162916053ff89d6ad95c4a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*3RjHSnrEd8LOHbuB_2ob0Q.png"/></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx"><a class="af mx" href="https://arxiv.org/abs/2402.18031" rel="noopener ugc nofollow" target="_blank">From Corpus-Steered Query Expansion with Large Language Models</a> by Lei , Cao, Zhou , Shen, Yates in 2024. “Overview of CSQE. Given a query Biology definition and the top-2 retrieved documents, CSQE utilizes an LLM to identify relevant document 1 and extract the key sentences from document 1 that contribute to the relevance. The query is then expanded by both these corpus-originated texts and LLM-knowledge empowered expansions (i.e., hypothetical documents that answer the query) to obtain the final results.”</figcaption></figure><p id="ea73" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">There are limitations to both LLM-based query expansion and its predecessors like PRF. The most glaring of which is the assumption that the LLM generated terms are relevant or that the top ranked results are relevant. God forbid I am trying to find information about the Australian journalist Harry Potter instead of the famous boy wizard. Both techniques would further pull my query away from the less popular query subject to the more popular one making edge case queries less effective.</p><h1 id="2d6a" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Hypothetical Query Indexes</h1><p id="b471" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk">Another way to reduce the asymmetry between questions and documents is to index documents with a set of LLM-generated hypothetical questions. For a given document, the LLM can generate questions that <em class="pa">could</em> be answered by the document. Then during the retrieval step, the user’s query embedding is compared to the hypothetical question embeddings versus the document embeddings.</p><p id="dfeb" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">This means that we don’t need to embed the original document chunk, instead, we can assign the chunk a document ID and store that as metadata on the hypothetical question document. Generating a document ID means there is much less overhead when mapping many questions to one document.</p><p id="2708" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">The clear downside to this approach is your system will be limited by the creativity and volume of questions you store.</p><h1 id="7c05" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Hypothetical Document Embeddings — HyDE</h1><p id="7f6e" class="pw-post-body-paragraph nu nv fq nw b go nx ny nz gr oa ob oc od oe of og oh oi oj ok ol om on oo op fj bk"><a class="af mx" href="https://arxiv.org/abs/2212.10496" rel="noopener ugc nofollow" target="_blank">HyDE</a> is the opposite of Hypothetical Query Indexes. Instead of generating hypothetical questions, the LLM is asked to generate a hypothetical document that <em class="pa">could</em> answer the question, and the embedding of that generated document is used to search against the real documents. The real document is then used to generate the response. This method showed strong improvements over other contemporary retriever methods when it was first introduced in 2022.</p><p id="ec62" class="pw-post-body-paragraph nu nv fq nw b go ov ny nz gr ow ob oc od ox of og oh oy oj ok ol oz on oo op fj bk">We use this concept at Dune for our natural language to SQL product. By rewriting user prompts as a possible caption or title for a chart that would answer the question, we are better able to retrieve SQL queries that can serve as context for the LLM to write a new query.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="or os ed ot bh ou"><div class="mi mj pd"><img src="../Images/84cc7146e1c968806f8ef8f6a712c95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQSJNBGjLG5tRhjmykPOCw.png"/></div></div><figcaption class="ms mt mu mi mj mv mw bf b bg z dx"><a class="af mx" href="https://arxiv.org/abs/2212.10496" rel="noopener ugc nofollow" target="_blank">From Precise Zero-Shot Dense Retrieval without Relevance Labels</a> by Gao, Ma, Lin, Callan in 2022. “An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever/mContriever models.”</figcaption></figure><h1 id="8f53" class="my mz fq bf na nb nc gq nd ne nf gt ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Stay tuned for part 2 on reranking methods.</h1></div></div></div></div>    
</body>
</html>