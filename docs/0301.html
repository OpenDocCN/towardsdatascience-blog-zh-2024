<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Revolutionizing Culinary Experiences with AI: Introducing FIRE (Food Image to REcipe generation) 🔥</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Revolutionizing Culinary Experiences with AI: Introducing FIRE (Food Image to REcipe generation) 🔥</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31">https://towardsdatascience.com/revolutionizing-culinary-experiences-with-ai-introducing-fire-food-image-to-recipe-generation-d7b010ee2eb2?source=collection_archive---------9-----------------------#2024-01-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="d95e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">From Visual Delights to Culinary Recipes: How AI Transforms Food Images into Recipes?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Prateek Chhikara" class="l ep by dd de cx" src="../Images/4cabb40cbab34038c0f762b45d58bbba.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F6Bov2ToQ_oaiAcr8eYWpg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@prateekchhikara?source=post_page---byline--d7b010ee2eb2--------------------------------" rel="noopener follow">Prateek Chhikara</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d7b010ee2eb2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/28ec61f09782221ff6bb62c3825991f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0e89EmBTlirxWF3OAF0vXQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1. Given a potentially unseen image, our method FIRE generates a corresponding recipe consisting of a title, ingredients, and cooking instructions. (Source: Image by the author)</figcaption></figure><h1 id="fc0a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction:</h1><p id="08b0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Food is an essential source of nutrition and also an integral part of our cultural identity, describing our lifestyle, traditions, and social relations [1]. A person’s physical appearance and cognitive abilities usually contain evidence of their dietary habits because selecting nutritious food contributes to the overall well-being of the body and mind of a person [2]. The rapid growth of social media enables everyone to share stunning visuals of the delicious food they consume. A simple search for hashtags like #food or #foodie yields millions of posts, emphasizing the immense value of food in our society [3]. The importance of food, followed by large amounts of publicly available food datasets, has encouraged food computing applications that associate visual depictions of dishes with symbolic knowledge. An ambitious goal of food computing is to produce the recipe for a given food image, with applications such as food recommendation according to user preferences, recipe customization to accommodate cultural or religious factors, and automating cooking execution for higher efficiency and precision [4].</p><blockquote class="ou ov ow"><p id="6d35" class="ny nz ox oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">“Tell me what you eat, and I will tell you who you are.” This saying emphasizes the idea that an individual’s dietary choices reflect their identity.</p></blockquote><p id="3b81" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">Before delving deep into the proposed work, I would like to mention that this work is published and available at the<em class="ox"> </em><strong class="oa fr"><em class="ox">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) — 2024</em></strong>, a conference recognized for its contributions and advancements in computer vision. ✨✨✨</p><blockquote class="ou ov ow"><p id="b9cf" class="ny nz ox oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Poster: </strong><a class="af pd" href="https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view" rel="noopener ugc nofollow" target="_blank">https://drive.google.com/file/d/1zf2NA6ga8PWndZAgu5QjSwO8EPsvt-yt/view</a></p><p id="2b4b" class="ny nz ox oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Paper: </strong><a class="af pd" href="https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html" rel="noopener ugc nofollow" target="_blank">https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html</a></p></blockquote><h1 id="e8b3" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">The Inspiration Behind this Work:</h1><p id="d6ed" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Our motivation behind creating an end-to-end method for recipe generation from food images came from the interest of the Computer Vision (CV) community in food computing. CV has been used for food quality assurance for around three decades [5]. Despite advances in deep learning techniques for food image processing, existing methods have achieved limited performance in extracting ingredients from a given food image [6, 7]. Moreover, recipe generation from a set of ingredients could be treated as a language-generation task or, more precisely, as a seq-to-seq use case, which is also unexplored in the current literature. Additionally, previous methods have not thoroughly combined CV and NLP research to devise a comprehensive system that translates food images into complete recipes. Therefore, the current food computing methods have yet to leverage recent breakthroughs in NLP and CV, such as vision transformers and advanced language modeling. This gap in technology and application motivated us to develop an end-to-end pipeline, which we name FIRE (🔥), desiring to join these dots and push the limits of food computing. FIRE is a multimodal model that is designed to generate comprehensive recipe, including food titles, ingredients, and cooking instructions, based on given input food images, as shown in Figure 1.</p><p id="e99a" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">The contributions of our paper are as follows:</p><ol class=""><li id="db14" class="ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot pe pf pg bk">Our approach uses Vision Transformers (ViT) to extract detailed embeddings from food images, which are then used as an input by an attention-based decoder which further identifies recipe ingredients.</li><li id="d51a" class="ny nz fq oa b go ph oc od gr pi of og oh pj oj ok ol pk on oo op pl or os ot pe pf pg bk">We have developed a detailed design to generate recipe titles and cooking instructions, using state-of-the-art (SotA) vision (BLIP) and language (T5) models.</li><li id="30db" class="ny nz fq oa b go ph oc od gr pi of og oh pj oj ok ol pk on oo op pl or os ot pe pf pg bk">Our multimodal approach surpasses existing models in performance based on ingredient extraction accuracy and the quality of generated cooking instructions.</li><li id="07e6" class="ny nz fq oa b go ph oc od gr pi of og oh pj oj ok ol pk on oo op pl or os ot pe pf pg bk">We demonstrate FIRE’s versatility via two innovative applications, (i) <em class="ox">Recipe Customization</em> and (ii) <em class="ox">Recipe to Code Generation</em>, displaying its significance in integrating large language models (LLMs) using few-shot prompting.</li></ol><h1 id="674c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Unveiling FIRE: Breaking Down the Process</h1><p id="ecdf" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">FIRE contains three components: (1)<strong class="oa fr"> title generation</strong> from food images by using SotA image captioning, (2) <strong class="oa fr">ingredient extraction</strong> from images using vision transformers and decoder layers with attention, and (3) <strong class="oa fr">cooking instruction generation</strong> based on the generated title and extracted ingredients using an encoder-decoder model. Please refer to Figure 2 for more details about the proposed architecture.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pm ln pn lo po cf pp cg pq ci bh"><figure class="mm mn mo mp mq mr ps pt paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/32c90148e7f474d6e6cd5fe4b0b3a8cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*_N39Zsd59dw_q2CL2zLjBg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 2. Proposed architecture to extract ingredients, and generate the recipe title and cooking instructions from a food image. (Ingredients with quantity is passed during the train time only) (Source: Image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="946d" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk"><strong class="al">1. Title Generation</strong></h2><p id="6304" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Using the BLIP model, we generate recipe titles from food images [8]. In our initial experiments with the off-the-shelf BLIP model, we observed BLIP’s prediction accuracy was lower because of the domain shift between its training data and the food domain. BLIP tends to capture extraneous details impertinent to our goal because it was originally designed to provide a comprehensive image caption for various settings. As an illustration, when presented with an image of a muffin, BLIP produced the description ‘<em class="ox">a muffin positioned atop a wooden cutting board</em>’. To better align the generated captions with recipe titles, we fine-tune the BLIP model using a subset of the Recipe1M dataset. We observe that the fine-tuned version of BLIP shows promising improvements in generating accurate, aligned, and pertinent titles for food images. For the same example image the fine-tuned BLIP model provides a shorter string ‘<em class="ox">muffin</em>’, removing the additional extraneous information.</p><h2 id="c03c" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk">2. Ingredient Extraction</h2><p id="23c0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Extracting ingredients from a given food image is a challenging task due to the inherent complexity and variability of food compositions. We develop an ingredient extraction pipeline (shown in Figure 2) that is built on top of the one proposed by [7].</p><p id="1155" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Feature Extractor:</strong> We extract the image’s features using ViT [9]. ViT’s attention mechanism handles the feature representations with stable and notably high resolution. This capability precisely meets the requirements of dense prediction tasks such as ingredient extraction from food images.</p><p id="286e" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Ingredient Decoder:</strong> The feature extractor produces image embeddings. We pass these image embeddings through three normalization layers (layerNorm) and subsequently feed the output into our ingredient decoder responsible for extracting ingredients. The decoder consists of four consecutive blocks, each having multiple sequential layers: self-attention, conditional attention, two fully connected layers, and three normalization layers. In the last step, the decoder output is processed by a fully connected layer with a node count equivalent to the vocabulary size, resulting in a predicted set of ingredients.</p><h2 id="2e47" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk">3. Cooking Instruction Generation</h2><p id="1670" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Considering the remarkable accomplishments of LMs in natural language applications like text generation and question answering [10], we pose cooking instruction generation as a language modeling task.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/b4d3515db1cb21ea3c9c5eb24b6ea211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y95cu4KLXuFh-5c9Votb5w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 3. Generating cooking instructions for a title and a set of ingredients. (ingredients with quantity is present only during the fine-tuning of T5) (Source: Image by the author)</figcaption></figure><p id="a683" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">Refining the LMs for downstream tasks has demonstrated remarkable outcomes in various NLP tasks. While we expect that large LMs would be capable of generating cooking instructions after fine-tuning, they require high computational resources given their large number of parameters. Given the available resources and our research objective, we adopt the popular encoder-decoder model, T5 [11], for generating cooking instructions. During fine-tuning, we pass the title and ingredients of the recipe as a formatted string (see Figure 3), inspired by prior work [12].</p><p id="d750" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">The T5 is fine-tuned on three inputs: title, ingredients, and ingredients with quantity to incorporate maximum information from the dataset. However, we do not have ingredients with quantities at the inference time; hence, we pass only the title and ingredients. Moreover, excluding the quantity information from our model ensures a fair comparison with previous approaches. It investigates whether our model’s advantage stems from a well-structured architecture rather than relying solely on augmenting additional knowledge. By removing the influence of quantity information during inference, we aim to highlight the inherent capabilities of T5 and its ability to generate high-quality cooking instructions.</p><h1 id="b4c0" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">FIRE Results:</h1><p id="3843" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The results in Table 1 show that FIRE performs better than the SotA baselines, InverseCooking [7] and Chef Transformer [13]. These results demonstrate our proposed pipeline’s ability to generate precise and coherent recipes, corroborating the effectiveness of FIRE and emphasizing the value of language generation models for high-quality recipe generation. These results also support our expectation that the FIRE method can generalize well without ingredient quantity information given at inference time, even when they were present during training. Meanwhile, training with extra information results in fewer hallucinations, especially regarding ingredients quantity (e.g., 2 tablespoons of salt) and cooking time (e.g., heat for 10–12 minutes).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/eb3500ec8e3e77538caf3a9d4332d770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMWDbEs0pB0Vpra0p3ng5Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Table 1. Recipe generation comparison on the test dataset. We report mean with one standard deviation of 10 experiments. Bold represents the best model. (+) represents the model tested on the ground truth title and ingredients to generate the recipe. (Source: Image by the author)</figcaption></figure><p id="932b" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">In this article, we mostly focus on the critical aspects of our proposed work, considering the article length constraints. We encourage readers to check out our detailed paper for a deeper dive into our experimental results and comprehensive analysis. 😁</p><h2 id="bf68" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk">Error Analysis</h2><p id="9bd0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To gain further insight into the performance of our recipe generation method, we inspected its performance on individual images. FIRE is often able to generate a correct recipe for dishes similar to those present in the Recipe1M dataset. For Pav Bhaji (a popular Indian dish not present in Recipe1M), it gave a result that is unrelated to the intended dish, as illustrated in Figure. 4. Therefore, we want to highlight the importance of developing better evaluation metrics because conventional evaluation metrics such as SacreBLEU and ROUGE failed to capture the accuracy of the recipes generated and detect specific text hallucinations.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/efe451638ebff8231061c973651d9421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-ywL4RiQ-H3yxBLfGxCMg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 4. Recipe prediction by FIRE for Pav Bhaji image. (Source: Image by the author)</figcaption></figure><h1 id="684a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Beyond the Kitchen: The Future of Food Computing with FIRE:</h1><p id="ff22" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">While FIRE achieves SotA performance on the ambitious task of generating recipes from images, we go a step further and investigate its integration into larger pipelines for food computing applications. Namely, considering the promise of few-shot prompting of large language models, we describe how FIRE and large LMs can be integrated to support <strong class="oa fr"><em class="ox">recipe customization</em></strong> and <strong class="oa fr"><em class="ox">recipe-to-machine-code generation</em></strong>.</p></div></div><div class="mr"><div class="ab cb"><div class="lm pm ln pn lo po cf pp cg pq ci bh"><figure class="mm mn mo mp mq mr ps pt paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/085bb19ddc9aebf21cb6f74efd267815.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*FSZU6YSWeDq98qR7tDRGvg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 5. Applications of FIRE: Recipe Customization and Recipe to Code Generation. (Source: Image by the author)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="64e2" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk">1. Recipe Customization</h2><p id="44cc" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Recipe customization is crucial due to the connection between food, customs, and individual preferences. Additionally, it becomes essential when addressing allergies or dietary restrictions. Surprisingly, despite the evident demand, existing literature lacks dedicated efforts in recipe customization. Our work aims to bridge the research gap by enabling personalized recipe customization, considering individual taste profiles and dietary restrictions.</p><p id="9679" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">To guide future research in this area, we showcase the ability of FIRE to support a recipe customization approach that focuses on a wide range of topics (e.g., ingredient replacement, taste adjustment, calorie adjustment, cooking time adaptation) to test few-shot performance thoroughly. As shown in the purple part of Figure 5, we remove ingredients to trim the potatoes from the recipe. Two sentences related to potatoes are deleted in the modified version, and one sentence is changed to ensure consistency. Specifically, we perform ingredient addition to replace ‘<em class="ox">cheese</em>’ with ‘<em class="ox">cheddar cheese</em>’ and recognize that it should be added before baking, resulting in the modified sentence ‘<em class="ox">Sprinkle half each of cheddar cheese and onions</em>.’</p><h2 id="7929" class="pu nd fq bf ne pv pw px nh py pz qa nk oh qb qc qd ol qe qf qg op qh qi qj qk bk">2. Generating Machine Code for Image-based Recipes</h2><p id="e484" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Converting recipes to machine code enables automation, scalability, and integration with various existing systems, thus reducing manual intervention, saving labor costs, and reducing human errors while preparing the food. To facilitate this task, we combine FIRE’s recipe generation strength with the ability of large LMs to manipulate code-style prompts for structural tasks [14]. We show an example approach for generating Python-style code representations of recipes developed by FIRE, by prompting GPT-3 (please refer to orange part in Figure 5).</p><h1 id="5861" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion &amp; Future Work:</h1><p id="3e02" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We introduced FIRE, a methodology tailored for food computing, focusing on generating food titles, extracting ingredients, and generating cooking instructions solely from image inputs. We leveraged recent CV and language modeling advancements to achieve superior performance against solid baselines. Furthermore, we demonstrated practical applications of FIRE for <em class="ox">recipe customization</em> and <em class="ox">recipe-to-code generation</em>, showcasing the adaptability and automation potential of our approach.</p><p id="1946" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">We list three challenges that should be addressed in future research:</p><ol class=""><li id="4b7c" class="ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot pe pf pg bk">Existing and proposed recipe generation models lack a reliable mechanism to verify the accuracy of the generated recipes. Conventional evaluation metrics fall short in this aspect. Hence, we would like to create a new metric that assesses the coherence and plausibility of recipes, providing a more thorough evaluation.</li><li id="229b" class="ny nz fq oa b go ph oc od gr pi of og oh pj oj ok ol pk on oo op pl or os ot pe pf pg bk">The diversity and availability of recipes are influenced by geographical, climatic, and religious factors, which may limit their applicability. Incorporating knowledge graphs that account for these contextual factors and ingredient relationships can offer alternative ingredient suggestions, addressing this issue.</li><li id="2d33" class="ny nz fq oa b go ph oc od gr pi of og oh pj oj ok ol pk on oo op pl or os ot pe pf pg bk">Hallucination in recipe generation using language and vision models poses a significant challenge. Future work would explore the state-tracking methods to improve the generation process, ensuring the production of more realistic and accurate recipes.</li></ol><h1 id="5e6b" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Call-to-Action:</h1><p id="6c4e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">I hope this overview has provided you the insight into the inspiration and development of FIRE, our innovative tool for converting food images into detailed recipes. For a more in-depth exploration of our approach, I invite you to check out our full paper, which is published in the<em class="ox"> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) — 2024</em>. If our research contribute to your work, we would be happy if you cite it. 😊</p><blockquote class="ou ov ow"><p id="a1bb" class="ny nz ox oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk"><strong class="oa fr">Paper Link:</strong> <a class="af pd" href="https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html" rel="noopener ugc nofollow" target="_blank">https://openaccess.thecvf.com/content/WACV2024/html/Chhikara_FIRE_Food_Image_to_REcipe_Generation_WACV_2024_paper.html</a></p></blockquote><pre class="mm mn mo mp mq qp qq qr bp qs bb bk"><span id="339a" class="qt nd fq qq b bg qu qv l qw qx">@InProceedings{Chhikara_2024_WACV,<br/>    author    = {Chhikara, Prateek and Chaurasia, Dhiraj and Jiang, Yifan and Masur, Omkar and Ilievski, Filip},<br/>    title     = {FIRE: Food Image to REcipe Generation},<br/>    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},<br/>    month     = {January},<br/>    year      = {2024},<br/>    pages     = {8184-8194}<br/>}</span></pre><h1 id="aee4" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">References:</h1><p id="9d00" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[1] Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh Jain. A survey on food computing. ACM Comput. Surv., 52(5), sep 2019.</p><p id="3541" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[2] Sutter Health. Eating Well for Mental Health. <a class="af pd" href="https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health." rel="noopener ugc nofollow" target="_blank">https://www.sutterhealth.org/health/nutrition/eating-wellfor-mental-health.</a> Accessed on March 24, 2023.</p><p id="9d9b" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[3] Kiely Kuligowski. 12 Reasons to Use Instagram for Your Business. <a class="af pd" href="https://www.business.com/articles/10-reasons-touse-instagram-for-business/." rel="noopener ugc nofollow" target="_blank">https://www.business.com/articles/10-reasons-touse-instagram-for-business/.</a> Accessed on May 12, 2023.</p><p id="7db0" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[4] Dim P. Papadopoulos, Enrique Mora, Nadiia Chepurko, Kuan Wei Huang, Ferda Ofli, and Antonio Torralba. Learning program representations for food images and cooking recipes, 2022.</p><p id="f5d3" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[5] Sundaram Gunasekaran. Computer vision technology for food quality assurance. Trends in Food Science &amp; Technology, 7(8):245–256, 1996.</p><p id="8201" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[6] Yoshiyuki Kawano and Keiji Yanai. Food image recognition with deep convolutional features. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication, pages 589– 593, 2014</p><p id="3f53" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[7] Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and ´ Adriana Romero. Inverse cooking: Recipe generation from food images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10453– 10462, 2019.</p><p id="1fef" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[8] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888– 12900. PMLR, 2022.</p><p id="bfd5" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p><p id="439f" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[10] Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, and Shweta Kumari. Privacy aware questionanswering system for online mental health risk assessment. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 215– 222, Toronto, Canada, July 2023. Association for Computational Linguistics.</p><p id="226d" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[11] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.</p><p id="a2c9" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[12] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzman, Luke Zettlemoyer, and Marjan ´ Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404, 2021.</p><p id="4fd6" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[13] Mehrdad Farahani and Kartik Godawat and Haswanth Aekula and Deepak Pandian and Nicholas Broad. Chef Transformer. <a class="af pd" href="https://huggingface.co/flax-community/t5-" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/flax-community/t5-</a> recipe-generation. Accessed on April 12, 2023.</p><p id="b78b" class="pw-post-body-paragraph ny nz fq oa b go oy oc od gr oz of og oh pa oj ok ol pb on oo op pc or os ot fj bk">[14] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Findings of the Association for Computational Linguistics: EMNLP 2022, 2022</p></div></div></div></div>    
</body>
</html>