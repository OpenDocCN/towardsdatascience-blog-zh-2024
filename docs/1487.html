<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Understanding Buffer of Thoughts (BoT) — Reasoning with Large Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Understanding Buffer of Thoughts (BoT) — Reasoning with Large Language Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14">https://towardsdatascience.com/understanding-buffer-of-thoughts-bot-reasoning-with-large-language-models-391919d2f76f?source=collection_archive---------1-----------------------#2024-06-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8302" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A new prompting tool for complex reasoning, compared with Chain of thought (CoT) and Tree of Thought (ToT)</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hesam Sheikh" class="l ep by dd de cx" src="../Images/b8d5f4f285eef77634e4c1d4321580ed.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*hEouYBx-IeJIslDqS20BjQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@itshesamsheikh?source=post_page---byline--391919d2f76f--------------------------------" rel="noopener follow">Hesam Sheikh</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--391919d2f76f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mj mk ml"><p id="0462" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">If you’re not a member, <a class="af nj" href="https://hesamsheikh.substack.com/" rel="noopener ugc nofollow" target="_blank"><strong class="mp fr">read for free!</strong></a><strong class="mp fr"> ✨</strong></p></blockquote><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/1dd8cf331a7b47649dd087344982ff56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Sc-px7DsflTWJAPU"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Photo by <a class="af nj" href="https://unsplash.com/@etiennegirardet?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Etienne Girardet</a> on <a class="af nj" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1ec0" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Squeezing proficiency in complex reasoning tasks and avoiding hallucinations remains a major research topic in Large Language Models (LLMs). Despite the effort, LLMs need help with generalized reasoning capabilities. Traditional methods such as <strong class="mp fr">Chain-of-Thought (CoT)</strong> or <strong class="mp fr">Tree-of-Thought (ToT)</strong> often require multiple assumptions or numerous back-and-forth prompting which means intensive computation.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl od"><img src="../Images/acb9be520c3976344727f77d1cbdfb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOBApb9eUG3QB3uGzYIQyA.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Buffer of Thoughts (BoT) vs other prompting methods. (source: <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="6e06" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The new proposed method in the paper, <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank"><strong class="mp fr">Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models</strong></a><strong class="mp fr"> [1]</strong>, combats these limitations with a dynamic, adaptive repository of high-level thought templates<strong class="mp fr"> </strong>called <strong class="mp fr">meta-buffer. </strong>In BoT, once the user presents a new problem, it is first simplified and analyzed to extract key elements, which then guide the retrieval of a relevant thought template from a dynamic dataset. This allows adaptive and efficient problem-solving through modified and complex reasoning patterns. According to the original paper, this is so effective that <em class="mo">“</em><strong class="mp fr"><em class="mo">Llama3–8B+BoT </em></strong><em class="mo">has the potential to surpass </em><strong class="mp fr"><em class="mo">Llama3–70B model.”</em></strong></p><p id="2c74" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">BoT </strong>achieves efficient reasoning across problems that are similar to its templates as it:</p><ul class=""><li id="cd94" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni oe of og bk">(1) leverages previous solutions on new challenges,</li><li id="b835" class="mm mn fq mp b go oh mr ms gr oi mu mv mw oj my mz na ok nc nd ne ol ng nh ni oe of og bk">(2) boosts efficiency by eliminating the need for multiple query iterations (as we see in Graph-of-Thoughts (GoT) or ToT), and</li><li id="fafe" class="mm mn fq mp b go oh mr ms gr oi mu mv mw oj my mz na ok nc nd ne ol ng nh ni oe of og bk">(3) dynamically updates its template repository to ensure it evolves as it encounters new tasks.</li></ul><p id="f8fa" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In this article, we will first go through the general outline of how BoT works, understand the function of each key part, and test the procedure with an example.</p><h1 id="035b" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">How does BoT work?</h1><p id="3ed0" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">The general thought-augmented reasoning process (as shown in the figure below) starts with <strong class="mp fr">Problem Distillation</strong>, which analyzes and condenses the incoming task into essential elements and constraints and then creates a simplified problem statement.</p><p id="cbb5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This distilled information is then used to query the <strong class="mp fr">Meta-Buffer</strong>, a dynamic repository that contains high-level thought templates. From the thought templates, one that is most similar to the distilled problem is retrieved. Then, during the <strong class="mp fr">Instantiation Process</strong>, it is instantiated with specific requirements and information about the distilled problem.</p><p id="052a" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Throughout this process, the <strong class="mp fr">Buffer Manager</strong> actively keeps an eye on the <strong class="mp fr">Meta-Buffer. </strong>Once it detects a new insight not included in the meta-buffer, <strong class="mp fr">Buffer Manager </strong>updates it to ensure a continual evolution of the thought template repository.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pn"><img src="../Images/32292bd0be51f32c6798de545021c314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMSmql9X-whVPYKijsTNVQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">The BoT process. (source: <a class="af nj" href="http://Paper" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="c5df" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s go through each of these key parts to gain a more detailed look:</p><h2 id="a1f4" class="po on fq bf oo pp pq pr or ps pt pu ou mw pv pw px na py pz qa ne qb qc qd qe bk">Problem Distiller</h2><p id="938e" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">Problem Distiller can be thought of as a <strong class="mp fr">preprocess </strong>on the input tasks in order to…</p><ul class=""><li id="5072" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni oe of og bk">(1) extract essential information of the problem, and</li><li id="d6b1" class="mm mn fq mp b go oh mr ms gr oi mu mv mw oj my mz na ok nc nd ne ol ng nh ni oe of og bk">(2) simplify complex tasks for a better search and retrieval of thought templates.</li></ul><p id="eb04" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Problem Distiller takes the burden off of LLM to identify and extract vital information and constraints of the problem. This is done by a meta prompt ϕ:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl qf"><img src="../Images/944bc53eb01b77fdf82f8608b1a19b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*MC3YflNqZSP3q4lMmgqVEA.png"/></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">(source: <a class="af nj" href="http://Paper" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="7a00" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The prompt used by the authors to distill key information about a task is as follows:</p><pre class="nn no np nq nr qg qh qi bp qj bb bk"><span id="f2b0" class="qk on fq qh b bg ql qm l qn qo">[Problem Distiller]:<br/>As a highly professional and intelligent expert in information distillation, you excel at<br/>extracting essential information to solve problems from user input queries. You adeptly<br/>transform this extracted information into a suitable format based on the respective type of the<br/>issue.<br/>Please categorize and extract the crucial information required to solve the problem from the<br/>user’s input query, the distilled information should include.<br/>1. Key information:<br/>Values and information of key variables extracted from user input, which will be handed over<br/>to the respective expert for task resolution, ensuring all essential information required to solve<br/>the problem is provided.<br/>2. Restrictions:<br/>The objective of the problem and corresponding constraints.<br/>3. Distilled task:<br/>Extend the problem based on 1 and 2, summarize a meta problem that can address the user<br/>query and handle more input and output variations. Incorporate the real-world scenario of the<br/>extended problem along with the types of key variables and information constraints from the<br/>original problem to restrict the key variables in the extended problem. After that, use the user<br/>query input key information as input to solve the problem as an example.</span></pre><h2 id="b8ee" class="po on fq bf oo pp pq pr or ps pt pu ou mw pv pw px na py pz qa ne qb qc qd qe bk">Meta-Buffer</h2><p id="57ac" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">The meta-buffer is a central database that stores high-level thought templates. These templates are high-level abstractions representing various problem-solving processes. The idea is that LLM can leverage past problems and insights to solve current challenges. The best part is that the Meta-Buffer dynamically updates to ensure new unseen problems are also included. The Meta-Buffer doesn’t enforce thought templates to follow a specific instruction.</p><p id="ddf9" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">Template Retrieval</strong>: Once a task is distilled, BoT would go through the thought templates and grab the one most similar to the task. This is done by calculating the embedding similarity between the task and the thought templates.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div class="nk nl qp"><img src="../Images/3877ad60c789b210ee2cd5f42b642b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*_F34rCJ41VnsLqlBBvkG4A.png"/></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">The retriever would calculate the similarity between the embedding of the input task f(xd), and the embedding of templates f(DTi ). (source: <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="8202" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">the retriever would calculate the similarity between the embedding of the input task <strong class="mp fr">f(xd)</strong>, and the embedding of templates <strong class="mp fr">f(D<em class="mo">Ti</em> ). </strong>This is only done if the similarity is above a certain threshold δ (0.5–0.7). If none of the thought templates have a similarity score with the task above the δ threshold, then the <strong class="mp fr">xd </strong>is identified as a new task. Depending on if the task is new or not, one of the two paths would be taken:</p><ul class=""><li id="d7ed" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni oe of og bk">If the task is similar to one of the thought templates, the template would be instantiated with the distilled information using an instantiation prompt (which you can check in the paper). This instantiation process can be denoted as</li></ul><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qq"><img src="../Images/872661a6c9986487d31d5d362f937a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vsrdxG7bXs5HAx1YdApGMA.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Instantiated Reasoning. (source: <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><ul class=""><li id="2e76" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni oe of og bk">If the task is new, a general thought template that is designed to address a broad set of problems is used. As the task is processed, the <strong class="mp fr">Buffer Manager </strong>observes and learns and potentially creates a new, more specific thought template and pushes it to the meta-buffer.</li></ul><h2 id="0ae5" class="po on fq bf oo pp pq pr or ps pt pu ou mw pv pw px na py pz qa ne qb qc qd qe bk">Buffer Manager</h2><p id="e614" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">The Buffer Manager serves as a crucial player in maintaining and enhancing the meta-buffer. Based on the new insights and outcomes from the tasks that are solved, it updates the thought templates. Also, whenever a new or drastically different problem is solved, the buffer manager assesses whether or not to create a new thought template. This is to ensure thought templates remain to the point and are not redundant.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qr"><img src="../Images/64359637991b99ea155bceb95391d9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4A8XsjDczL31_1DXOXpZSA.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">checking whether or not a newly generated template is similar to the existing ones. (source: Paper)</figcaption></figure><p id="b606" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">By employing the above formulation, the Buffer Manager checks whether or not the meta-buffer already has the necessary knowledge to tackle a problem.</p><h2 id="fdd1" class="po on fq bf oo pp pq pr or ps pt pu ou mw pv pw px na py pz qa ne qb qc qd qe bk">BoT vs. Single-Query vs. Multi-Query</h2><p id="33bd" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">How does the BoT stand out compared to previous methods? The authors of the paper evaluate various methods on different datasets of various tasks, such as Data Understanding, Python Programming Puzzles, Multilingual Grade School Math<strong class="mp fr"> </strong>(MGSM), etc. The results show a surprising advantage of <strong class="mp fr">BoT </strong>in almost all the tasks.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qs"><img src="../Images/498b49fc3dd24dbe76978a7fb2099b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ht4i0Hf9zMEHkfFKkGysXQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">BoT vs previous methods. The best results (marked in blue) are all achieved by BoT. (source: <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><p id="782b" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">One of the key advantages of BoT is its efficiency — requiring <strong class="mp fr">only 12%</strong> of the computational cost on average when compared to multi-query prompting methods. This high computational cost and latency of multi-query methods such as ToT often renders them impractical in real-world use cases.</p><blockquote class="mj mk ml"><p id="bb1d" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">BoT+Llama3–8B has the potential to surpass single Llama3–70B model</p></blockquote><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl qt"><img src="../Images/d343e715b3bfa85f8ddd6a75569f32f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rgW6uKTyvHkf9AK-Gh9Mw.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">comparing the effect of BoT on Llama3–8B and 70B. Annotated. (source: <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">Paper</a>)</figcaption></figure><h1 id="29d1" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Buffer of Thoughts in Practice</h1><p id="4763" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">The demo code for Buffer of Thoughts is published on <a class="af nj" href="https://github.com/YangLing0818/buffer-of-thought-llm" rel="noopener ugc nofollow" target="_blank">GitHub</a> [2]. To test out the functionality in practice, I will use this method on a custom task: Word Reordering. In this task, the LLM must take a scrambled sentence, such as <em class="mo">“Sam name is my”</em>, and return a permutation of these words that is semantically meaningful, which in this example would be <em class="mo">“my name is Sam”, </em>(this is not a benchmark with baseline performance). Some examples of the scrambled sentences and the correct ones are as follows:</p><pre class="nn no np nq nr qg qh qi bp qj bb bk"><span id="45c5" class="qk on fq qh b bg ql qm l qn qo">{"input": "&lt;start&gt; life plan and families to for social hospital workers outside with patients work the &lt;end&gt;", <br/>"target": "&lt;start&gt; social workers work with patients and families to plan for life outside the hospital &lt;end&gt;"}<br/>{"input": "&lt;start&gt; yield plant refers dry total to production biological matter &lt;end&gt;", <br/>"target": "&lt;start&gt; biological yield refers to total plant dry matter production &lt;end&gt;"}<br/>{"input": "&lt;start&gt; the bloodstream into alcohol from directly stomach goes the &lt;end&gt;", <br/>"target": "&lt;start&gt; alcohol goes directly from the stomach into the bloodstream &lt;end&gt;"}</span></pre><p id="150a" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">I will also use the following user prompt to explain the task:</p><pre class="nn no np nq nr qg qh qi bp qj bb bk"><span id="e438" class="qk on fq qh b bg ql qm l qn qo">Reorder a list of words to make them semantically meaningful, <br/>for example "Sam name is my" -&gt; "my name is Sam". <br/>Please provide only the reordered sentence without any explanations. <br/>Start the ordered sentence with &lt;start&gt; and end it with &lt;end&gt;</span></pre><p id="7589" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Provided this task, the BoT framework first distills my task and extracts the goal, the restriction, and how I would like my output along with other key information. The full procedure of BoT in detail, is the following terminal output:</p><pre class="nn no np nq nr qg qh qi bp qj bb bk"><span id="f9f8" class="qk on fq qh b bg ql qm l qn qo">User prompt:<br/>Reorder a list of words to make them semantically meaningful, for example "Sam name is my" -&gt; "my name is Sam". Please provide only the reordered sentence without any explanations. Start the ordered sentence with &lt;start&gt; and end it with &lt;end&gt;<br/>Input:<br/>&lt;start&gt; the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other &lt;end&gt;<br/>Distilled information:Meta distiller Respond:<br/><br/>Distilled Information:<br/><br/>1. Key information:<br/>- Input: "&lt;start&gt; the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other &lt;end&gt;"<br/><br/>2. Restriction:<br/>- Reorder the list of words such that the sentence is semantically meaningful<br/>- The output should begin with "&lt;start&gt;" and end with "&lt;end&gt;"<br/><br/>3. Distilled task:<br/>- The task is to reorder the words to create a meaningful sentence from the provided mixed words inside the tags "&lt;start&gt;" and "&lt;end&gt;"<br/><br/>4. Python transformation:<br/>   (Optional, skip when Python tag is Not for Python) Input parameters:<br/>     input_sentence = "&lt;start&gt; the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other &lt;end&gt;"<br/><br/>5. Answer form: (Optional, skip when there is no specific answer form)<br/>     The output sentence should be in the form of "&lt;start&gt; ... &lt;end&gt;"<br/><br/>  **Note: The generation ends here. Do not show this message in your answer !**<br/>Instantiated reasoning result: ```python<br/>def ReorderWords(input_sentence):<br/>    # Remove the start and end tags, and split the sentence into words<br/>    words = input_sentence.replace("&lt;start&gt;", "").replace("&lt;end&gt;", "").strip().split()<br/><br/>    # Tentative reordering to form a meaningful sentence<br/>    # Based on the context provided in the task, the following reordering seems logical<br/>    reordered_words = ["the", "melting", "in", "solid", "metals", "is", "achieved", "by", "mixing", "gold", "leaf", "and", "desired", "other", "color", "gold"]<br/><br/>    # Join the words into a single sentence and add start and end tags<br/>    reordered_sentence = "&lt;start&gt; " + " ".join(reordered_words) + " &lt;end&gt;"<br/><br/>    return reordered_sentence<br/><br/># Input sentence provided in the task<br/>input_sentence = "&lt;start&gt; the melting in solid to gold leaf metals is achieve made by desired gold and mixing color other &lt;end&gt;"<br/>print(ReorderWords(input_sentence))<br/>```<br/><br/><br/>The result of code execution: &lt;start&gt; the melting in solid metals is achieved by mixing gold leaf and desired other color gold &lt;end&gt;</span></pre><p id="1df5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Some of the examples of the reordered sentences using BoT:</p><pre class="nn no np nq nr qg qh qi bp qj bb bk"><span id="1477" class="qk on fq qh b bg ql qm l qn qo">{"input": "&lt;start&gt; life plan and families to for social hospital workers outside with patients work the &lt;end&gt;", <br/>"result": "&lt;start&gt; Hospital workers work outside with social patients and plan to the families for life &lt;end&gt;\n"}<br/>{"input": "&lt;start&gt; yield plant refers dry total to production biological matter &lt;end&gt;", <br/>"result": "&lt;start&gt; Plant yield refers to total dry matter biological production &lt;end&gt;\n"}<br/>{"input": "&lt;start&gt; the bloodstream into alcohol from directly stomach goes the &lt;end&gt;", <br/>"result": "&lt;start&gt; the alcohol goes directly from the stomach into the bloodstream &lt;end&gt;\n"}</span></pre><p id="33c5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Note, that the BoT repository I used is a demo code and lacks some of the features mentioned in the original paper, such as a general thought template, the dynamic updating of the Meta-Buffer, or finding the closest template embedding to the user task. These are important aspects of the framework and without them, we cannot conclude the performance of Buffer of Thoughts in practice.</p><h1 id="9353" class="om on fq bf oo op oq gq or os ot gt ou ov ow ox oy oz pa pb pc pd pe pf pg ph bk">Final Words</h1><p id="2827" class="pw-post-body-paragraph mm mn fq mp b go pi mr ms gr pj mu mv mw pk my mz na pl nc nd ne pm ng nh ni fj bk">In conclusion, BoT shows promising results in both accuracy and efficiency in various domains and tasks. It’s an interesting approach to breaking down a reasoning problem into its fundamental restrictions and key information and building on top of previous solutions and templates to better formulate the task for an LLM to understand.</p><p id="a665" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">By addressing some of the limitations in other prompting techniques, Buffer of Thoughts allows LLM to have more complex thinking patterns, potentially making smaller lightweight models perform at the level of larger ones.</p><p id="5ec1" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Allowing small LLMs to achieve results close to large LLMs is a key topic addressed in many of the current research papers. The goal is to employ various prompting and fine-tuning techniques to extract accurate AI outputs with low computation and cost.</p><p id="8566" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Buffer of Thoughts is a novel and promising prompting framework that leverages a domain of techniques to guide LLM step by step, in a reasoning process. A complete practical implementation of the Buffer of Thoughts technique is yet to come, but in the meanwhile, test out the provided benchmarks in the demo GitHub repository [2].</p></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="84c7" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">If you have made it this far, consider this for a<strong class="mp fr"> further read</strong>:</p><div class="rc rd re rf rg rh"><a rel="noopener follow" target="_blank" href="/platonic-representation-hypothesis-c812813d7248?source=post_page-----391919d2f76f--------------------------------"><div class="ri ab ig"><div class="rj ab co cb rk rl"><h2 class="bf fr hw z io rm iq ir rn it iv fp bk">Platonic Representation: Are AI Deep Network Models Converging?</h2><div class="ro l"><h3 class="bf b hw z io rm iq ir rn it iv dx">Are Artificial Intelligence models evolving towards a unified representation of reality? The Platonic Representation…</h3></div><div class="rp l"><p class="bf b dy z io rm iq ir rn it iv dx">towardsdatascience.com</p></div></div><div class="rq l"><div class="rr l rs rt ru rq rv lr rh"/></div></div></a></div><p id="3aaf" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">🌟 Join +1000 people learning about </strong>Python, ML / MLOps / AI, Data Science, and LLM. <a class="af nj" href="https://medium.com/@itshesamsheikh/subscribe" rel="noopener"><strong class="mp fr">follow me</strong></a><strong class="mp fr"> </strong>and check out my<a class="af nj" href="https://twitter.com/itsHesamSheikh" rel="noopener ugc nofollow" target="_blank"><strong class="mp fr"> X/Twitter</strong></a>, where I keep you updated Daily<strong class="mp fr">.</strong></p><p id="e78e" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Thanks for reading,</p><p id="8a7c" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">— Hesam</p></div></div></div><div class="ab cb qu qv qw qx" role="separator"><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra rb"/><span class="qy by bm qz ra"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5771" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[1] Yang, L., Yu, Z., Zhang, T., Cao, S., Xu, M., Zhang, W., Gonzalez, J. E., &amp; Cui, B. (2024). Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. <em class="mo">arXiv</em>. Retrieved from <a class="af nj" href="https://arxiv.org/abs/2406.04271" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2406.04271</a></p><p id="8153" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[2] buffer-of-thought-llm, <a class="af nj" href="https://github.com/YangLing0818/buffer-of-thought-llm" rel="noopener ugc nofollow" target="_blank">https://github.com/YangLing0818/buffer-of-thought-llm</a></p></div></div></div></div>    
</body>
</html>