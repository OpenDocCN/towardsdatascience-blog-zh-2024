# 强化学习锦标赛：DDPG、SAC、PPO、I2A、决策变换器

> 原文：[https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23](https://towardsdatascience.com/the-tournament-of-reinforcement-learning-ddpg-sac-ppo-i2a-decision-transformer-6c1e42f394f0?source=collection_archive---------6-----------------------#2024-08-23)

## 训练仿真类人机器人进行对抗，使用五篇新的强化学习论文

[](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[![Anand Majmudar](../Images/4840cb28e81326221cebef9f540c8e12.png)](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------) [Anand Majmudar](https://medium.com/@almond.maj?source=post_page---byline--6c1e42f394f0--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6c1e42f394f0--------------------------------) ·阅读时间13分钟·2024年8月23日

--

![](../Images/b2543b4652d2b4021438fd08240435f9.png)

由GPT-4生成

最近我想起了旧电视节目《机器人战斗》（Battlebots），并且想为它加上一些自己的特色。因此，我训练了仿真类人机器人，通过五篇新的强化学习论文让它们进行对抗。

阅读下文，你将学习这五种强化学习算法的理论和数学，看到我如何实现它们，并见证它们一较高下，决出冠军！

1.  深度确定性策略梯度（DDPG）

1.  决策变换器

1.  软演员-评论家（SAC）

1.  带有近端策略优化（PPO）的想象增强智能体（I2A）

**设置仿真环境：**

我使用了Unity机器学习代理模拟器，并为每个机器人身体构建了21个执行器，分布在9个关节上，通过虚拟摄像头在其头部实现10×10的RGB视觉，此外还配备了剑和盾。我随后编写了C#代码，定义了它们的奖励和物理交互。智能体可以通过三种主要方式获得奖励：

1.  用剑击中对手（‘击败’对手）

1.  保持头部的y位置高于身体（以鼓励它们站立起来）

1.  比之前更接近对手（以鼓励智能体收敛并进行对抗）

智能体在1000个时间步后重置，并且我对环境进行了大规模并行化以进行训练。

![](../Images/384f5e631a16dd1220b37f6485276dc9.png)

大规模并行化的训练环境，我的截图

然后是编写算法的时刻。为了理解我使用的算法，了解什么是Q学习至关重要，所以让我们来探究一下！

**Q学习** *(如果你已经熟悉，可以跳过)*

在强化学习中，我们让智能体采取行动以探索其环境，并根据与目标的接近程度给予正向或负向奖励。智能体如何调整其决策标准以获得更好的奖励？

Q学习提供了一种解决方案。在Q学习中，我们追踪Q函数Q(s,a)，它追踪从状态s_t执行动作a_t后的期望回报。

Q(s, a) = R(s, a) + γ * E[Q(s_t + 1, a_t + 1)] + γ² * E[Q(s_t + 2, a_t + 2) + …]

其中R(s,a)是当前状态和动作的奖励，y是折扣因子（一个超参数），E[]是期望值。

如果我们正确地学习了这个Q函数，我们就可以简单地选择返回最高Q值的动作。

我们如何学习这个Q函数？

从回合的结束处开始，在那里我们可以确定Q值（就是我们当前的奖励），我们可以使用递归来填充之前的Q值，更新公式如下：

Q(s,a) ← (1 — α) Q(s,a) + α * [r + γ * max_a’ Q(s’,a’)]

其中α是学习率，r是即时奖励，γ是折扣因子（权重参数），s’是下一个状态，max_a’ Q(s’,a’)是下一个状态下所有可能动作的最大Q值

本质上，我们的新Q值等于旧Q值加上当前奖励与下一个最大Q值之间差异的小部分百分比。现在，当智能体想要选择一个动作时，它们可以选择返回最大Q值（期望奖励）的动作。

你可能会注意到一个潜在的问题：我们在每个时间步长上评估每个可能的动作的Q函数。如果我们在离散空间中有有限的动作数，这没有问题，但在连续动作空间中，无法高效地评估Q函数，因为可能的动作数是无限的。这使得我们需要考虑第一个竞争算法：（DDPG）

**深度确定性策略梯度（DDPG）**

DDPG尝试以一种新颖的方式在连续动作空间中使用Q网络。

*创新 1：演员和评论员*

我们不能直接使用Q网络来做决策，但我们可以用它来训练另一个独立的决策函数。这就是演员-评论员设置：演员是决定动作的策略，评论员则基于这些动作来确定未来的期望奖励

目标评论员：Q_target(s,a) = r + γ * Q’(s’, μ’(s’))

其中r是即时奖励，γ是折扣因子，s’是下一个状态，μ’(s’)是目标策略网络对下一个状态的动作，Q’是目标评论员网络，目标演员：相对于策略的期望回报的梯度≈ 1/N * Σ ∇a Q(s, a)|a=μ(s) * ∇θ_μ μ(s)

本质上，如何通过N个样本，衡量策略选择的动作的Q值（相对于策略变化，策略变化又与策略参数相关）

为了更新两者，我们使用随机梯度上升更新，其中学习率 * 当前 Q 和目标 Q 的 MSE 损失的梯度。请注意，演员和评论家都被实现为神经网络。

*创新 2：确定性动作策略*

我们的策略可以是确定性的（为每个状态保证一个动作）或随机性的（根据概率分布为每个状态抽取一个动作）。确定性动作策略有助于高效评估 Q 函数（每个状态只有一个动作，因此进行单次递归评估）。

那么，如何在确定性策略下进行探索呢？我们不会被困在一遍又一遍地执行相同的动作吗？这种情况确实可能发生，然而，我们可以通过添加随机生成的噪声来增加智能体的探索性，以鼓励探索（有点像突变如何通过允许探索独特的遗传可能性来促进进化）

*创新 3：交互式环境中的批量学习*

我们还希望在每个观察到的时间步上获得更多收益（它由状态、动作、奖励和下一状态组成）：因此我们可以存储之前的时间步数据元组，并将其用于未来的训练

这使我们能够离线使用批量学习（即使用之前收集的数据，而不是通过环境交互），并且允许我们通过 GPU 并行化以提高训练速度。现在，我们还拥有独立同分布的数据，而不是我们通常得到的有偏的顺序数据（在这种数据中，一个数据点的值依赖于之前的数据点）

*创新 4：目标网络*

通常，使用神经网络的 Q 学习过于不稳定，且不容易收敛到最优解，因为更新过于敏感/强大

因此，我们使用目标演员和评论家网络，它们与环境互动，并在训练过程中部分接近真实的演员和评论家，但不是完全接近（大因子）目标 + （小因子）新

*算法演练与代码*

1.  初始化评论家、演员、目标评论家和目标演员、重放缓冲区

1.  对于视觉，我在任何其他层之前使用 CNN（这样视觉的最重要特征会被算法利用）

1.  对于每一集

1.  观察状态，选择并执行动作 mu + 噪声

1.  获取奖励，下一状态

1.  将 (s_t, a_t, r_t, s_(t+1)) 存储在重放缓冲区中

1.  从缓冲区中随机抽取一个小批次

1.  更新 y_i = reward_i + gamma Q(s 给定 theta)

1.  递归评估

1.  更新评论家以最小化 L = y_i — Q(s,a|theta)

1.  使用策略梯度 J 更新演员，期望递归 Q 给定策略

1.  更新目标为大因子 * 目标 + (1 — 大因子) * 实际

[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------) [## Knights-of-Papers/src/DDPG/DDPG.py at main · AlmondGod/Knights-of-Papers

### DDPG、决策变换器、I2A 与 PPO、SAC 自我博弈模拟战斗类人形机器人……

[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/DDPG/DDPG.py?source=post_page-----6c1e42f394f0--------------------------------)

**软演员评论员（SAC）**

DDPG确实存在一些问题。特别是，评论员更新包括贝尔曼方程：Q(s,a) = r + max Q(s’a’)，但作为Q网络近似器的神经网络会产生很多噪声，噪声的最大值意味着我们过高估计，因此我们对我们的策略过于乐观，奖励了平庸的动作。众所周知，DDPG还需要广泛的超参数调整（包括噪声添加），并且除非其超参数在一个狭窄的范围内，否则无法保证收敛到最优解。

*创新1：最大熵强化学习*

不再让演员纯粹地最大化奖励，演员现在最大化奖励 + 熵：

为什么使用熵？

熵本质上是我们对某个结果的不确定性（例如，硬币的最大熵偏向硬币，较少的熵硬币始终是正面时熵为0：展示公式）。

通过将熵作为最大化因子，我们鼓励广泛探索，从而提高对局部最优解的敏感性，允许对高维空间进行更一致、稳定的探索（为什么这比随机噪声更好）。Alpha：用于加权熵优先级的参数，自动调整（怎么调整？）

*创新2：两个Q函数*

这一变化旨在通过独立训练两个Q网络并在策略改进步骤中使用两个中的最小值，来解决Q函数的贝尔曼过高估计偏差。

*算法运行与代码*

1.  初始化演员，2个Q函数，2个目标Q函数，回放缓冲区，alpha

1.  重复直到收敛：

1.  对于每个环境步骤：

1.  从策略中采样动作，观察下一个状态和奖励

1.  将（s_t, a_t, r_t, s_t+1）存储在回放缓冲区

1.  对于每个更新步骤：

1.  采样批次

1.  更新 Q 值：

1.  计算目标y = 奖励加上策略的最小Q值 + alpha 熵

1.  最小化 Q 预测 — y

1.  更新策略以最大化策略的Q值 + alpha奖励

1.  更新alpha以满足目标熵

1.  更新目标Q网络（软更新目标，使目标 = 大因子 * 目标 + （1 — 大因子） * 实际值）

[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------) [## Knights-of-Papers/src/SAC/SAC.py at main · AlmondGod/Knights-of-Papers

### DDPG、决策变换器、I2A与PPO以及SAC自对弈于模拟战斗类人形……

[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/SAC/SAC.py?source=post_page-----6c1e42f394f0--------------------------------)

**I2A与PPO**

这里有两个算法（附加算法层可以在任何算法之上工作）

**近端策略优化（PPO）**

我们的方法不同于DDPG和SAC，我们的目标是一个可扩展、数据高效、具有鲁棒性收敛性的算法（对超参数的定义不敏感）。

*创新1：代理目标函数*

这个代理目标允许离策略训练，因此我们可以使用更广泛的数据（这对于现实世界场景尤其有利，因为现有的数据集庞大）。

在讨论代理目标之前，理解优势的概念至关重要。优势是：在状态 s 执行行动 s 后的期望奖励与状态 s 的期望奖励之间的差异。实质上，它量化了行动 a 相较于“平均”行动的好坏。

我们将其估算为 A = Q(a,s) — V(a)，其中 Q 是行动价值（在行动 a 后的期望回报），V 是状态价值（从当前状态出发的期望回报），两者都通过学习获得

现在，代理目标是：

J(θ) = Ê_t [ r_t(θ) Â_t ]

其中：

+   J(θ) 是代理目标

+   Ê_t […] 表示在有限样本批次上的经验平均值

+   r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t) 是新策略下行动的似然度 / 旧策略下行动的似然度

+   Â_t 是时间步 t 的估计优势

这相当于量化新策略如何提高高回报行动的似然性，并降低低回报行动的似然性。

*创新2：剪切目标函数*

这是另一种解决过大策略更新问题的方法，从而实现更稳定的学习。

L_CLIP(θ) = E[ min( r(θ) * A, clip(r(θ), 1-ε, 1+ε) * A ) ]

剪切目标是实际代理目标和剪切比例在 1 — epsilon 与 1 + epsilon 之间的代理目标的最小值（基本上是未修改比例的信任区域）。epsilon 通常为 ~0.1/0.2。

它本质上选择剪切比例和正常比例中较为保守的那一个。

实际的PPO目标：

L^{PPO}(θ) = Ê_t [ L^{CLIP}(θ) — c_1 * L^{VF}(θ) + c_2 * Sπ_θ ]

其中：

1.  L^{VF}(θ) = (V_θ(s_t) — V^{target}_t)²

1.  Sπ_θ 是策略 π_θ 在状态 s_t 下的熵

本质上，我们优先考虑较高的熵、较低的价值函数和较高的剪切优势

PPO还使用小批量处理，并交替进行数据训练。

*算法执行和代码*

1.  每次迭代

1.  对于每个 N 个智能体

1.  运行策略 T 个时间步

1.  计算优势

1.  针对策略优化代理函数，进行 K 次迭代，并且小批量大小 M < NT

1.  更新策略

[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------) [## Knights-of-Papers/src/I2A-PPO/gpuI2APPO.py at main · AlmondGod/Knights-of-Papers

### DDPG、决策变换器、结合PPO的I2A，以及SAC自我对战在模拟战斗类人型机器人上的应用……

github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/I2A-PPO/gpuI2APPO.py?source=post_page-----6c1e42f394f0--------------------------------)

**想象增强智能体**

我们的目标是为任何其他算法创建一个额外的嵌入向量输入，以提供关键的有价值信息，并充当环境的“心理模型”

*创新：想象向量*

想象向量使我们能够向代理的观察中添加一个额外的嵌入向量，以编码多个“想象的未来运行”及其奖励评估（目标是“看到未来”并“在行动前思考”）。

我们如何计算它？我们使用一个学习到的环境逼近函数，试图模拟环境（这被称为基于模型的学习，因为我们尝试学习环境的模型）。我们将它与一个 rollout 策略配合使用，rollout 策略通常是一个非常简单且执行速度快的策略（通常是随机的），用来决定行动，从而“探索未来”。通过在 rollout 策略上运行环境逼近器，我们可以探索未来的行动及其奖励，然后找到一种方法将这些所有的想象未来的行动和奖励表示为一个向量。一个值得注意的缺点是：正如你所预期的，它增加了大量的训练工作，并且需要更多的数据。

*结合的 I2A-PPO 算法流程与代码*

1.  每次我们收集 PPO 的观察数据时：

1.  初始化环境模型和 rollout 策略

1.  对于多个“想象的运行”：

1.  从当前状态开始运行环境模型，并使用 rollout 策略决策，直到达到某个时间范围，从而生成一个想象轨迹（s，a，r 序列）

1.  想象编码器：将这些想象轨迹转换成单一的输入嵌入，以供实际的决策网络使用

**决策 Transformer**

我们在这里的目标是利用 Transformer 架构的优势来进行强化学习。通过决策 Transformer，我们可以在稀疏/分散的奖励中识别出重要奖励，享受更广泛的分布建模以获得更好的泛化和知识迁移，并从预先获得的有限次次优数据中学习（称为离线学习）。

对于决策 Transformer，我们实际上是将强化学习视为序列建模问题。

*创新点 1：Transformer*

如果你想真正理解 Transformer，我推荐观看 Karpathy 从零构建 GPT-2 的视频。这里有一个关于 Transformer 的简要回顾，适用于 DT：

我们有表示状态、动作、期望获得的未来奖励之和（返回奖励），以及时间步的 token 序列。我们的目标是现在输入一系列 token 并预测下一步的动作：这将作为我们的策略。

这些 token 都有键、值和查询，我们通过复杂的网络将它们结合起来，以表达各个元素之间的关系。然后，我们将这些关系结合成一个“嵌入”向量，用以编码输入之间的关系。这个过程被称为注意力机制（Attention）。

请注意，“因果自注意力掩码”确保嵌入只能与序列中之前的嵌入相关联，因此我们不能使用未来的信息来预测未来，而是使用过去的信息来预测未来（因为我们的目标是预测下一步的动作）。

一旦我们获得了这个嵌入向量，就将其通过神经网络层（Karpathy使用的类比是，我们在这里‘推理token之间的关系’）。

这两者的结合（通过Attention找到tokens之间的关系，通过我们的神经网络层推理关系）是Transformers的一种头部，我们将其堆叠多次。在这些头部的末尾，我们使用一个学习到的神经网络层将输出转换为我们的动作空间大小和要求。

*顺便说一下，在推理时，我们预定义回报以作为我们期望的总奖励。*

*算法演示和代码*

1.  对于dataloader中的(R,s,a,t)

1.  预测动作

1.  模型将obs、视觉（通过卷积网络层）、rtg和时间步长转换为唯一的嵌入，并将时间步长嵌入添加到其他嵌入中

1.  所有三个输入都作为输入提供给Transformer层，最后使用动作嵌入

1.  计算MSEloss（a_pred-a）**2

1.  对决策Transformer模型执行SGD，通过这个损失的梯度更新参数

[](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------) [## Knights-of-Papers/src/Decision-Transformer/DecisionTransformer.py 在主分支下·…

### DDPG、决策Transformer、I2A与PPO，以及SAC自对战模拟类人战斗机器人…

[github.com](https://github.com/AlmondGod/Knights-of-Papers/blob/main/src/Decision-Transformer/DecisionTransformer.py?source=post_page-----6c1e42f394f0--------------------------------)

**结果**

为了训练这些模型，我将算法运行在NVIDIA RTX 4090上，利用这些算法的GPU加速创新。感谢[vast.ai](http://vast.ai)！以下是损失曲线：

*DDPG损失（2000个回合）*

![](../Images/dac56145748bf3104d9f46fef457ddc0.png)

Matplotlib损失图表，由我制作

*I2APPO损失（3500个回合）*

![](../Images/e3551e86cfeb767b73048b843d6f103e.png)

Matplotlib损失图表，由我制作

*SAC损失（5000个回合）*

![](../Images/cb81f1216c1bbde245f0a961454a3976.png)

Matplotlib损失图表，由我制作

*决策Transformer损失（1600个回合，每40个回合记录一次损失）*

![](../Images/2f1780c8ad215f4e2521a1f752613ae9.png)

Matplotlib损失图表，由我制作

通过比较算法的结果（主观比较并根据训练时间加权），我发现决策Transformer表现最好！考虑到DT是专门为充分利用GPU而设计的，这一点是有道理的。请观看我制作的[视频](https://www.youtube.com/watch?v=kpDfXqX7h1U)，查看算法的实际表现。模型学会了爬行并停止摔倒，但仍有一段路要走，才能成为专家级战斗者。

**改进的领域：**

我深刻体会到了训练类人机器人有多么困难。我们在一个高维输入空间（包括视觉RGB和执行器的位置/速度）中操作，同时还结合了一个极其高维的输出空间（27维连续空间）。

从一开始，我最希望的只是它们能爬到彼此面前并碰触剑锋，尽管即便如此也充满挑战。大多数训练运行甚至没有经历到触剑的高奖励，因为光是走路本身就太难了。

主要的改进维度就是简单地增加训练时间和使用的计算量。正如我们在现代AI革命中所见，这些增加的计算量和数据趋势似乎没有上限！

最重要的是，我学到了很多！下次，我会使用NVIDIA的技能嵌入或终身学习方法，让机器人在学习对战之前，先学会走路！

要查看我制作的展示创建这个项目过程的视频，并看到机器人对战，请查看下面的视频：

我尝试使用新的强化学习论文来让模拟机器人进行对战，

感谢你看到最后！如果你对更多内容感兴趣，可以在Twitter上找到我[@AlmondGodd](https://x.com/Almondgodd)！
