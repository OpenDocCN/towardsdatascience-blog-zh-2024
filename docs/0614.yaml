- en: 'Navigating Cost-Complexity: Mixture of Thought LLM Cascades Illuminate a Path
    to Efficient Large Language Model Deployment'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/navigating-cost-complexity-mixture-of-thought-llm-cascades-illuminate-a-path-to-efficient-large-23291d1eda41?source=collection_archive---------5-----------------------#2024-03-06](https://towardsdatascience.com/navigating-cost-complexity-mixture-of-thought-llm-cascades-illuminate-a-path-to-efficient-large-23291d1eda41?source=collection_archive---------5-----------------------#2024-03-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@yuval_domino?source=post_page---byline--23291d1eda41--------------------------------)[![Yuval
    Zukerman](../Images/b04d3068659ed79643398dc39f6ce950.png)](https://medium.com/@yuval_domino?source=post_page---byline--23291d1eda41--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--23291d1eda41--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--23291d1eda41--------------------------------)
    [Yuval Zukerman](https://medium.com/@yuval_domino?source=post_page---byline--23291d1eda41--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--23291d1eda41--------------------------------)
    ·5 min read·Mar 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/535098db554b3f659cfd241166a5b1b7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino) on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What if I told you that you could save 60% or more off of the cost of your LLM
    API spending without compromising on accuracy? Surprisingly, now you can.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are now part of our everyday lives. Companies use
    the technology to automate processes, improve customer experiences, build better
    products, save money, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Hosting your own LLMs is very challenging. They offer broad capabilities but
    are often expensive to run. They often require complex infrastructure and massive
    amounts of data. Cost and complexity are why you use prompt engineering. You may
    even use retrieval-augmented generation (RAG) to improve context and reduce hallucinations.
    With both techniques, you offload running LLMs to the likes of OpenAI, Cohere,
    or Google. Yet, scaling LLM adoption to new use cases, especially with the latest
    powerful models, can drive up a new cost that was previously unaccounted for.
    Weaker models may be cheaper, but can you trust them with complex questions? Now,
    new research shows us how to save money and get as good, sometimes better, LLM
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '**Get to Know LLM Cascades**'
  prefs: []
  type: TYPE_NORMAL
- en: In the search for lower LLM costs, researchers turned to the concept of LLM
    Cascades. In the dark ages, before the launch of ChatGPT, [a team from Google
    and The University of Toronto defined this term](https://arxiv.org/pdf/2207.10342.pdf)
    as programs that use probability calculations to get the best results using multiple
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the [FrugalGPT paper](https://arxiv.org/abs/2305.05176) defined
    cascades as sending a user query to a list of LLMs, one after the other, from
    weaker to stronger LLMs, until the answer is good enough. FrugalGPT Cascades uses
    a dedicated model to determine when the answer is good enough against a quality
    threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent paper titled [‘Large Language Model Cascades With Mixture of Thought
    Representations for Cost-Efficient Reasoning’](https://arxiv.org/pdf/2310.03094.pdf)
    from George Mason University, Microsoft, and Virginia Tech offers an alternative:
    a function that can determine whether the answer is good enough without fine-tuning
    another model.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mixture of Thought LLM Cascades**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using several LLMs, ‘Mixture of thought’ (MoT) reasoning uses just
    two — GPT 3.5 Turbo and GPT 4\. The former model is regarded as the ‘weaker’ LLM,
    while the latter is the ‘strong’ LLM. The authors harnessed LLM ‘answer consistency’
    to flag whether an LLM’s response is good enough. LLMs produce consistent answers
    to similar prompts when they are confident the answers are correct. Therefore,
    when weaker LLM answers are consistent, there is no need to call the stronger
    LLM. Conversely, these LLMs produce inconsistent answers when they lack confidence.
    That’s when you need a stronger LLM to answer the prompt. (Note: you can use a
    weaker/stronger LLM pair of your choice as well.)'
  prefs: []
  type: TYPE_NORMAL
- en: The prompts themselves use few-shot in-context prompting to improve LLM answer
    quality. Such prompts guide the LLM’s response by giving examples of similar questions
    and answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To improve model reasoning and simplify consistency measurement, the researchers
    introduce a new prompting technique for reasoning tasks by ‘mixing’ two prompting
    techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chain of Thought](https://openreview.net/forum?id=_VjQlMeSB_J) (CoT) Prompting
    encourages LLMs to generate intermediate steps or reasonings before arriving at
    a final answer. Generating these steps helps the model improve complicated task
    results. It also increases answer accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Program of Thought](https://arxiv.org/abs/2211.12588) (PoT) extends Chain
    of Thought prompting and uses the model’s output as a new input for further prompts.
    Prompts using this technique often request the model to answer with code instead
    of human language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper also introduces two methods to determine answer consistency:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Voting: This method samples multiple answers from LLM queries with similar
    prompts or by varying the response temperature option. It then measures how similar
    the LLM’s answers are to each other. The answer that agrees the most with all
    the other answers is assumed to be correct. The team also defined a flexible ‘threshold’
    value that aligns answer consistency and budget constraints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verification: This approach compares the LLM’s most consistent answers across
    two distinct thought representations (e.g., CoT and PoT). The algorithm accepts
    the weaker LLM’s answer if the two prompt responses are identical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since voting requires multiple prompts, it may be more suitable when a budget
    exists to guide the threshold number.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Bottom Line: Mixture of Thought Saves You Money**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how much money the MoT technique saves and its impact on answer
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The researchers used the following sum to calculate prompt cost:'
  prefs: []
  type: TYPE_NORMAL
- en: The cost of prompting the weaker model (because we may prompt it several times)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of the answer evaluation process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the evaluation process rejects the answer, we add the cost of prompting the
    strong model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results were dramatic:'
  prefs: []
  type: TYPE_NORMAL
- en: Using MoT variants — combining voting and verification with CoT and PoT — can
    lead to comparable performance at 40% of the cost of solely using GPT-4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In testing against the [CREPE](https://github.com/velocityCavalry/CREPE) Q&A
    dataset, MoT outperformed GPT-4 at 47% of its cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixing PoT and CoT improves decision-making compared to using one of the techniques
    alone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the threshold when using the voting method did not significantly
    impact quality despite the additional cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consistency model proved itself in reliably identifying correct LLM answers.
    It successfully predicted when to resort to using the strong model to obtain the
    optimal results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hosting and managing Large Language Models (LLMs) in-house comes with significant
    challenges. They bring complexity, high costs, and the need for extensive infrastructure
    and data resources. As a result, LLMs present substantial hurdles for organizations
    seeking to harness their broad capabilities. That may lead you to turn to hosted
    LLMs. Yet, this approach presents companies with unforeseen cost increases and
    budget challenges as they expand to new use cases. That is particularly evident
    when integrating the latest powerful models. To avoid that fate, you face a new
    dilemma: Can you trust weaker, more affordable models? Can you overcome concerns
    about their accuracy in handling complex questions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Cascades with Mixture of Thought (MoT) offers two significant steps forward:'
  prefs: []
  type: TYPE_NORMAL
- en: Substantial cost savings over exclusively using the latest models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demonstrable results on par with the latest models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This breakthrough provides organizations with a practical and efficient approach
    to navigating the delicate balance between the powerful capabilities of LLMs and
    the imperative to manage costs effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Domino Staff Software Engineer Subir Mansukhani contributed to this post.
  prefs: []
  type: TYPE_NORMAL
