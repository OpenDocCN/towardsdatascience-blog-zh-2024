<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Guide to Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Guide to Clustering Algorithms</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06">https://towardsdatascience.com/a-guide-to-clustering-algorithms-e28af85da0b7?source=collection_archive---------4-----------------------#2024-09-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Alex Davis" class="l ep by dd de cx" src="../Images/f773cce9438a68856cb8ba486ac8b051.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*T0N-gqDlACyH39JH8hcOdQ@2x.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@adavis08?source=post_page---byline--e28af85da0b7--------------------------------" rel="noopener follow">Alex Davis</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e28af85da0b7--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">4</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="735b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="mv">Clustering is a must-have skill set for any data scientist due to its utility and flexibility to real-world problems. This article is an overview of clustering and the different types of clustering algorithms.</em></p><h1 id="d96a" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">What is Clustering?</h1><p id="740f" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Clustering is a popular unsupervised learning technique that is designed to group objects or observations together based on their similarities. Clustering has a lot of useful applications such as market segmentation, recommendation systems, exploratory analysis, and more.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa ob"><img src="../Images/3aa1508082c9b47231972b9af15633a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k8HVaIcy0DnJtklThYeINg.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Image by Author</figcaption></figure><p id="c097" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">While clustering is a well-known and widely used technique in the field of data science, some may not be aware of the different types of clustering algorithms. While there are just a few, it is important to understand these algorithms and how they work to get the best results for your use case.</p><h1 id="287b" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Centroid-Based Clustering</h1><p id="3df9" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Centroid-based clustering is what most think of when it comes to clustering. It is the “traditional” way to cluster data by using a defined number of centroids (centers) to group data points based on their distance to each centroid. The centroid ultimately becomes the mean of it’s assigned data points. While centroid-based clustering is powerful, it is not robust against outliers, as outliers will need to be assigned to a cluster.</p><h2 id="3af5" class="os mx fq bf my ot ou ov nc ow ox oy ng mi oz pa pb mm pc pd pe mq pf pg ph pi bk">K-Means</h2><p id="0bfa" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">K-Means is the most widely used clustering algorithm, and is likely the first one you will learn as a data scientist. As explained above, the objective is to minimize the sum of distances between the data points and the cluster centroid to identify the correct group that each data point should belong to. Here’s how it works:</p><ol class=""><li id="466e" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pj pk pl bk">A defined number of centroids are randomly dropped into the vector space of the unlabeled data (initialization).</li><li id="e78a" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">Each data point measures itself to each centroid (usually using Euclidean distance) and assigns itself to the closest one.</li><li id="7c6a" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">The centroids relocate to the mean of their assigned data points.</li><li id="0a5e" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">Steps 2–3 repeat until the ‘optimal’ clusters are produced.</li></ol><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa pr"><img src="../Images/228afd72b5c9d40421a55f06880992fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*GcwPY7k_7dtEp04uBMBbsg.gif"/></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Image by Author</figcaption></figure><pre class="oc od oe of og ps pt pu bp pv bb bk"><span id="4ff5" class="pw mx fq pt b bg px py l pz qa">from sklearn.cluster import KMeans<br/>import numpy as np<br/><br/>#sample data<br/>X = np.array([[1, 2], [1, 4], [1, 0],<br/>              [10, 2], [10, 4], [10, 0]])<br/><br/>#create k-means model<br/>kmeans = KMeans(n_clusters = 2, random_state = 0, n_init = "auto").fit(X)<br/><br/>#print the results, use to predict, and print centers<br/>kmeans.labels_<br/>kmeans.predict([[0, 0], [12, 3]])<br/>kmeans.cluster_centers_</span></pre><h2 id="3ca3" class="os mx fq bf my ot ou ov nc ow ox oy ng mi oz pa pb mm pc pd pe mq pf pg ph pi bk">K-Means ++</h2><p id="6d85" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">K-Means ++ is an improvement of the initialization step of K-Means. Since the centroids are randomly dropped in, there is a chance that more than one centroid might be initialized into the same cluster, resulting in poor results.</p><p id="f805" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">However K-Means ++ solves this by randomly assigning the first centroid that will eventually find the largest cluster. Then, the other centroids are placed a certain distance away from the initial cluster. The goal of K-Means ++ is to push the centroids as far as possible from one another. This results in high-quality clusters that are distinct and well-defined.</p><pre class="oc od oe of og ps pt pu bp pv bb bk"><span id="773b" class="pw mx fq pt b bg px py l pz qa">from sklearn.cluster import KMeans<br/>import numpy as np<br/><br/>#sample data<br/>X = np.array([[1, 2], [1, 4], [1, 0],<br/>              [10, 2], [10, 4], [10, 0]])<br/><br/>#create k-means model<br/>kmeans = KMeans(n_clusters = 2, random_state = 0, n_init = "k-means++").fit(X)<br/><br/>#print the results, use to predict, and print centers<br/>kmeans.labels_<br/>kmeans.predict([[0, 0], [12, 3]])<br/>kmeans.cluster_centers_</span></pre><h1 id="59c2" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Density-Based Clustering</h1><p id="d8f4" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Density-based algorithms are also a popular form of clustering. However, instead of measuring from randomly placed centroids, they create clusters by identifying high-density areas within the data. Density-based algorithms do not require a defined number of clusters, and therefore are less work to optimize.</p><p id="82c6" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">While centroid-based algorithms perform better with spherical clusters, density-based algorithms can take arbitrary shapes and are more flexible. They also do not include outliers in their clusters and therefore are robust. However, they can struggle with data of varying densities and high dimensions.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div class="nz oa qb"><img src="../Images/900c97181437cb771ab90dc1c2a7240b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*dLFCCoKbaR4_bVUWgSyi0Q.jpeg"/></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Image by Author</figcaption></figure><h2 id="f3aa" class="os mx fq bf my ot ou ov nc ow ox oy ng mi oz pa pb mm pc pd pe mq pf pg ph pi bk">DBSCAN</h2><p id="623f" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">DBSCAN is the most popular density-based algorithm. DBSCAN works as follows:</p><ol class=""><li id="3918" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu pj pk pl bk">DBSCAN randomly selects a data point and checks if it has enough neighbors within a specified radius.</li><li id="b3c6" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">If the point has enough neighbors, it is marked as part of a cluster.</li><li id="ee03" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">DBSCAN recursively checks if the neighbors also have enough neighbors within the radius until all points in the cluster have been visited.</li><li id="99b7" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">Repeat steps 1–3 until the remaining data point do not have enough neighbors in the radius.</li><li id="41bf" class="lx ly fq lz b ma pm mc md me pn mg mh mi po mk ml mm pp mo mp mq pq ms mt mu pj pk pl bk">Remaining data points are marked as outliers.</li></ol><pre class="oc od oe of og ps pt pu bp pv bb bk"><span id="9b82" class="pw mx fq pt b bg px py l pz qa">from sklearn.cluster import DBSCAN<br/>import numpy as np<br/><br/>#sample data<br/>X = np.array([[1, 2], [2, 2], [2, 3],<br/>              [8, 7], [8, 8], [25, 80]])<br/><br/>#create model<br/>clustering = DBSCAN(eps=3, min_samples=2).fit(X)<br/><br/>#print results<br/>clustering.labels_</span></pre><h1 id="dccc" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Hierarchical Clustering</h1><p id="e14c" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Next, we have hierarchical clustering. This method starts off by computing a distance matrix from the raw data. This distance matrix is best and often visualized by a dendrogram (see below). Data points are linked together one by one by finding the nearest neighbor to eventually form one giant cluster. Therefore, a cut-off point to identify the clusters by stopping all data points from linking together.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa qc"><img src="../Images/e180fc95c23f18fec38023cc80147e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dkURHzaoD9JtEtFwEwsD4w.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Image by Author</figcaption></figure><p id="ad96" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By using this method, the data scientist can build a robust model by defining outliers and excluding them in the other clusters. This method works great against hierarchical data, such as taxonomies. The number of clusters depends on the depth parameter and can be anywhere from 1-n.</p><pre class="oc od oe of og ps pt pu bp pv bb bk"><span id="6e43" class="pw mx fq pt b bg px py l pz qa">from scipy.cluster.hierarchy import dendrogram, linkage<br/>from sklearn.cluster import AgglomerativeClustering<br/>from scipy.cluster.hierarchy import fcluster<br/><br/>#create distance matrix<br/>linkage_data = linkage(data, method = 'ward', metric = 'euclidean', optimal_ordering = True)<br/><br/>#view dendrogram<br/>dendrogram(linkage_data)<br/>plt.title('Hierarchical Clustering Dendrogram') <br/>plt.xlabel('Data point') <br/>plt.ylabel('Distance') <br/>plt.show()<br/><br/>#assign depth and clusters<br/>clusters = fcluster(linkage_data, 2.5, criterion = 'inconsistent', depth = 5)</span></pre><h1 id="dc57" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Distribution Based Clustering</h1><p id="a210" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Lastly, distribution-based clustering considers a metric other than distance and density, and that is probability. Distribution-based clustering assumes that the data is made up of probabilistic distributions, such as normal distributions. The algorithm creates ‘bands’ that represent confidence intervals. The further away a data point is from the center of a cluster, the less confident we are that the data point belongs to that cluster.</p><figure class="oc od oe of og oh nz oa paragraph-image"><div role="button" tabindex="0" class="oi oj ed ok bh ol"><div class="nz oa qd"><img src="../Images/b9c66c47eb6422b7014a43d6bd01a57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r2w63oyDRGSs1hH_pS-9Ng.png"/></div></div><figcaption class="on oo op nz oa oq or bf b bg z dx">Image by Author</figcaption></figure><p id="b104" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Distribution-based clustering is very difficult to implement due to the assumptions it makes. It usually is not recommended unless rigorous analysis has been done to confirm its results. For example, using it to identify customer segments in a marketing dataset, and confirming these segments follow a distribution. This can also be a great method for exploratory analysis to see not only what the centers of clusters comprise of, but also the edges and outliers.</p><h1 id="93bc" class="mw mx fq bf my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt bk">Conclusion</h1><p id="e457" class="pw-post-body-paragraph lx ly fq lz b ma nu mc md me nv mg mh mi nw mk ml mm nx mo mp mq ny ms mt mu fj bk">Clustering is an unsupervised machine learning technique that has a growing utility in many fields. It can be used to support data analysis, segmentation projects, recommendation systems, and more. Above we have explored how they work, their pros and cons, code samples, and even some use cases. I would consider experience with clustering algorithms a must-have for data scientists due to their utility and flexibility.</p><p id="9ae5" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="mv">I hope you have enjoyed my article! Please feel free to comment, ask questions, or request other topics.</em></p></div></div></div></div>    
</body>
</html>