- en: Pytorch Introduction — Enter NonLinear Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12](https://towardsdatascience.com/pytorch-introduction-enter-nonlinear-functions-4dd893845592?source=collection_archive---------7-----------------------#2024-01-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Continuing the Pytorch series, in this post we’ll learn about how non-linearities
    help solve complex problems in the context of neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page---byline--4dd893845592--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4dd893845592--------------------------------)
    ·8 min read·Jan 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee5af60923412013e898de28da50645c.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural Networks are Powerful Architectures able to Solve Complex Problems —
    Image generated by AI
  prefs: []
  type: TYPE_NORMAL
- en: In the last blog posts of the PyTorch Introduction series, we spoke about [introduction
    to tensor objects](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)
    and building a [simple linear model using PyTorch](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa).
    The first two blog posts of the series were the start of a larger objective where
    we understand deep learning at a deeper level (pun intended). To do that, we are
    using one of the most famous libraries in the machine learning world, PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: When building our simple linear model, we’ve understood that PyTorch is able
    to solve simple regression problems — but it wouldn’t be a deep learning library
    if these would be the only problems that it could solve, right? In this blog post,
    we are going to go a bit deeper into the complexities of Neural Networks and learn
    a bit about how to implement a neural network that deals with non-linear patterns
    and solve complex problems by introducing the concept of activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: This blog post (and series) is loosely based on the structure of [https://www.learnpytorch.io/](https://www.learnpytorch.io/),
    an excellent resource to learn PyTorch that I recommend you to check out!
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, in this blog post, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand how activation functions in PyTorch work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore how we can solve a non-linear problem using Neural Networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this blog post, we’ll use the Heart Failure prediction dataset available
    at [Kaggle](https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data).
    The dataset contains data from 299 patients with heart failure and specifies different
    variables about their health status. The goal is to predict if the patients died
    (column named DEATH_EVENT) and understand if there’s any signal in the patient’s
    Age, Anaemia level, ejection fraction or other health data that can predict the
    death outcome.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by loading our data using `pandas` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the `head` of our DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/32ee9e510675e85bbb1586d824350020.png)'
  prefs: []
  type: TYPE_IMG
- en: Head of the heart_failure_data — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to predict the `DEATH_EVENT` binary column, available at the end
    of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d3cf4c22ebc09f761de3d755612c93f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Head of the heart_failure_data, extra columns — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s standardize our data using `StandardScaler` — although not as
    important as in distance algorithms, standardizing the data will be extremely
    helpful to improve the gradient descent algorithm we’ll use during the training
    process. We’ll want to scale all but the last column (the target):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can perform a simple train-test split. We’ll use `sklearn` to do that
    and leave 20% of our dataset for testing purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to transform our data into `torch.tensor` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Having our data ready, time to fit our Neural Network!
  prefs: []
  type: TYPE_NORMAL
- en: Training a Vanilla Linear Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our data in-place, it’s time to train our first Neural Network. We’ll
    use a similar architecture to what we’ve done in the last blog post of the series,
    using a Linear version of our Neural Network with the ability to handle [linear
    patterns](/pytorch-introduction-building-your-first-linear-model-d868a8681a41?sk=152e552c4338cd1f06c64aedde5838fa):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This neural network uses the `nn.Linear`module from `pytorch` to create a Neural
    Network with 1 deep layer (one input layer, a deep layer and an output layers).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we can create our own class inheriting from `nn.Module` , we can also
    use (more elegantly) the `nn.Sequential` constructor to do the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fd150d50d7f417cd8b3d8cfafc4c0811.png)'
  prefs: []
  type: TYPE_IMG
- en: model_0 Neural Network Architecture — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Cool! So our Neural Network contains a single inner layer with 5 neurons (this
    can be seen by the `out_features=5` on the first layer).
  prefs: []
  type: TYPE_NORMAL
- en: This inner layer receives the same number of connections from each input neuron.
    The 12 in `in_features` in the first layer reflects the number of features and
    the 1 in `out_features` of the second layer reflects the output (a single value
    raging from 0 to 1).
  prefs: []
  type: TYPE_NORMAL
- en: To train our Neural Network, we’ll define a loss function and an optimizer.
    We’ll define `BCEWithLogitsLoss` ([PyTorch 2.1 documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html))
    as this loss function (torch implementation of Binary Cross-Entropy, appropriate
    for classification problems) and Stochastic Gradient Descent as the optimizer
    (using `torch.optim.SGD` ).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as I’ll also want to calculate the accuracy for every epoch of training
    process, we’ll design a function to calculate that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Time to train our model! Let’s train our model for 1000 epochs and see how
    a simple linear network is able to deal with this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately the neural network we’ve just built is not good enough to solve
    this problem. Let’s see the evolution of training and test accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4aab993292ac6a0ee659bab76fbf33e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test Accuracy through the Epochs — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '*(I’m plotting accuracy instead of loss as it is easier to interpret in this
    problem)*'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, our Neural Network isn’t able improve much of the test set accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the knowledge have from previous blog posts, we can try to add more layers
    and neurons to our neural network. Let’s try to do both and see the outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/de168d8143ebbf2f0d3a4fa97a7200fe.png)'
  prefs: []
  type: TYPE_IMG
- en: deeper_model Neural Network Architecture — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Although our deeper model is a bit more complex with an extra layer and more
    neurons, that doesn’t translate into more performance in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dbe817cbe76b1f309b8e83ecab499f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test Accuracy through the Epochs for deeper model— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Even though our model is more complex, that doesn’t really bring more accuracy
    to our classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: To be able to achieve more performance, we need to unlock a new feature of Neural
    Networks — activation functions!
  prefs: []
  type: TYPE_NORMAL
- en: Enter NonLinearities!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If making our model wider and larger didn’t bring much improvement, there must
    be something else that we can do with Neural Networks that will be able to improve
    its performance, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s where activation functions can be used! In our example, we’ll return
    to our simpler model, but this time with a twist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'What’s the difference between this model and the first one? The difference
    is that we added a new block to our neural network — `nn.ReLU` . The [rectified
    linear unit](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is an
    activation function that will change the calculation in each of the weights of
    the Neural Network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16289c0bd3c32ce0f22f2dc2cc5c1dab.png)'
  prefs: []
  type: TYPE_IMG
- en: ReLU Illustrative Example — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Every value that goes through our weights in the Neural Network will be computed
    against this function. If the value of the feature times the weight is negative,
    the value is set to 0, otherwise the calculated value is assumed. Just this small
    change adds a lot of power to a Neural Network architecture — in `torch` we have
    different activation functions we can use such as `nn.ReLU` , `nn.Tanh` or `nn.ELU`
    . For an overview of all activation functions, check this [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-other).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our neural network architecture contains a small twist, at the moment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a74076b4998dd59c65d9b8f984198f6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural Network Architecture — ReLU — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With this small twist in the Neural Network, every value coming from the first
    layer (represented by `nn.Linear(in_features=12, out_features=5)` ) will have
    to go through the “ReLU” test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the impact of fitting this architecture on our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ceae486f686af1f4c222a79cf48aea0.png)'
  prefs: []
  type: TYPE_IMG
- en: Train and Test Accuracy through the Epochs for non-linear model — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Cool! Although we see some of the performance degrading after 800 epochs, this
    model doesn’t exhibit overfitting as the previous ones. Keep in mind that our
    dataset is very small, so there’s a chance that our results are better just by
    randomness. Nevertheless, adding activation functions to your `torch` models definitely
    has a huge impact in terms of performance, training and generalization, particularly
    when you have a lot of data to train on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know the power of non-linear activation functions, it’s also relevant
    to know:'
  prefs: []
  type: TYPE_NORMAL
- en: You can add activation functions to every layer of the Neural Network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different activation functions have [different effects on your performance and
    training process](https://www.v7labs.com/blog/neural-networks-activation-functions).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` elegantly gives you the ability to add activation functions in-between
    layers by leveraging the `nn` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this post! In this blog post, we’ve checked
    how to incorporate activation functions inside `torch` Neural Network paradigm.
    **Another important concept that we’ve understood is that larger and wider networks
    are not a synonym of better performance.**
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions help us deal with problems that are solved with more complex
    architectures (again, more complex is different than larger/wider). They help
    with generalization power and help us converge our solution faster, being one
    of the major features of neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: And because of their widespread use on a variety of neural models, `torch` has
    got our back with its cool implementation of different functions inside the `nn.Sequential`
    modules!
  prefs: []
  type: TYPE_NORMAL
- en: Hope you’ve enjoyed and see you on the next PyTorch post! You can check the
    first PyTorch blog posts [here](/pytorch-introduction-tensors-and-tensor-calculations-412ff818bd5b?sk=2cf4d44549664fc647baa3455e9d78e8)
    and [here](/pytorch-introduction-building-your-first-linear-model-d868a8681a41).
    I also recommend that you visit [PyTorch Zero to Mastery Course](https://www.learnpytorch.io/01_pytorch_workflow/),
    an amazing free resource that inspired the methodology behind this post.
  prefs: []
  type: TYPE_NORMAL
- en: Also, I would love to see you on my newly created YouTube Channel — the [Data
    Journey](https://www.youtube.com/@TheDataJourney42) where I’ll be adding content
    on Data Science and Machine Learning.
  prefs: []
  type: TYPE_NORMAL
- en: '*[The dataset used in this blog post is under licence Creative Commons* [*https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2*](https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1023-5#Sec2)*]*'
  prefs: []
  type: TYPE_NORMAL
