- en: BERT — Intuitively and Exhaustively Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23](https://towardsdatascience.com/bert-intuitively-and-exhaustively-explained-48a24ecc1c8a?source=collection_archive---------2-----------------------#2024-08-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Baking General Understanding into Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page---byline--48a24ecc1c8a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--48a24ecc1c8a--------------------------------)
    ·46 min read·Aug 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a936f980e2bca9d843e27da4b08d8e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: “Baking” by Daniel Warfield using MidJourney. All images by the author unless
    otherwise specified. Article originally made available on [Intuitively and Exhaustively
    Explained](https://iaee.substack.com/).
  prefs: []
  type: TYPE_NORMAL
- en: In this article we’ll discuss “Bidirectional Encoder Representations from Transformers”
    (BERT), a model designed to understand language. While BERT is similar to models
    like GPT, the focus of BERT is to understand text rather than generate it. This
    is useful in a variety of tasks like ranking how positive a review of a product
    is, or predicting if an answer to a question is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into BERT we’ll briefly discuss the transformer architecture,
    which is the direct inspiration of BERT. Using that understanding we’ll dive into
    BERT and discuss how it’s built and trained to solve problems by leveraging a
    general understanding of language. Finally, we’ll create a BERT model ourselves
    from scratch and use it to predict if product reviews are positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Who is this useful for?** Anyone who wants to form a complete understanding
    of the state of the art of AI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**How advanced is this post?** Early parts of this article are accessible to
    readers of all levels, while later sections concerning the from-scratch implementation
    are fairly advanced. Supplemental resources are provided as necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-requisites:** I would highly recommend understanding fundamental ideas
    about…'
  prefs: []
  type: TYPE_NORMAL
