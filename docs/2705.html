<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Predict Housing Price using Linear Regression in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Predict Housing Price using Linear Regression in Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06">https://towardsdatascience.com/predict-housing-price-using-linear-regression-in-python-bfc0fcfff640?source=collection_archive---------2-----------------------#2024-11-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="e89e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A walk-through of cost computation, gradient descent, and regularization using Boston Housing dataset</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Elisa Yao" class="l ep by dd de cx" src="../Images/bf38cf250ae51db4f9880cb907b2f854.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IS2tUmtvNtk3E-IPXJA0lw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@yqelisa?source=post_page---byline--bfc0fcfff640--------------------------------" rel="noopener follow">Elisa Yao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bfc0fcfff640--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e7bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Linear Regression seems old and naive when Large Language Models (LLMs) dominate people’s attention through their sophistication recently. Is there still a point of understanding it?</p><p id="355c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">My answer is “Yes”, because it’s a building block of more complex models, including LLMs.</p><p id="96f0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Creating a Linear Regression model can be as easy as running 3 lines of code:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="ea8d" class="no np fq nl b bg nq nr l ns nt">from sklearn.linear_model import LinearRegression<br/>regressor = LinearRegression()<br/>regressor.fit(X_train, y_train)</span></pre><p id="757a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, this doesn’t show us the structure of the model. To produce optimal modeling results, we need to <strong class="ml fr">understand what goes on behind the scenes</strong>. In this article, I’ll break down the <strong class="ml fr">process of implementing Linear Regression in Python </strong>using a simple dataset known as “Boston Housing”, step by step.</p><h1 id="2178" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">What is Linear Regression</h1><p id="3975" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk"><strong class="ml fr">Linear </strong>— when plotted in a 2-dimensional space, if the dots showing the relationship of predictor <em class="ou">x</em> and predicted variable <em class="ou">y</em> scatter along a straight line, then we think this relationship can be represented by this line.</p><p id="e948" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Regression </strong>— a statistical method for estimating the relationship between one or more predictors (independent variables) and a predicted (dependent variable).</p><p id="3cf9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Linear Regression</strong> describes the predicted variable as a linear combination of the predictors. The line that abstracts this relationship is called <strong class="ml fr">line of best fit</strong>, see the red straight line in the below figure as an example.</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow ox"><img src="../Images/f5b975352a38e52b607f7ad54a6f9c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*MXP0q7fM2KiJQcjJhbTLyw.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Example of Linear Relationship and Line of Best Fit (Image by author)</figcaption></figure><h1 id="e289" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Data Description</h1><p id="9d79" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">To keep our goal focused on illustrating the Linear Regression steps in Python, I picked the Boston Housing dataset, which is:</p><ul class=""><li id="a5aa" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk"><strong class="ml fr">Small </strong>— makes debugging easy</li><li id="8bfe" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk"><strong class="ml fr">Simple </strong>— so we spend less time in understanding the data or feature engineering</li><li id="5f93" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk"><strong class="ml fr">Clean </strong>— requires minimum data cleaning</li></ul><p id="c1b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The dataset was first curated in <a class="af pn" href="https://www.law.berkeley.edu/files/Hedonic.PDF" rel="noopener ugc nofollow" target="_blank">Harrison and Rubinfeld’s (1978) study of Hedonic Housing Prices</a>. It originally has:</p><ul class=""><li id="6853" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">13 predictors — including demographic attributes, environmental attributes, and economics attributes</li></ul><blockquote class="po pp pq"><p id="23d2" class="mj mk ou ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">- CRIM — per capita crime rate by town <br/>- ZN — proportion of residential land zoned for lots over 25,000 sq.ft. <br/>- INDUS — proportion of non-retail business acres per town. <br/>- CHAS — Charles River dummy variable (1 if tract bounds river; 0 otherwise) <br/>- NOX — nitric oxides concentration (parts per 10 million) <br/>- RM — average number of rooms per dwelling <br/>- AGE — proportion of owner-occupied units built prior to 1940 <br/>- DIS — weighted distances to five Boston employment centres <br/>- RAD — index of accessibility to radial highways <br/>- TAX — full-value property-tax rate per $10,000 <br/>- PTRATIO — pupil-teacher ratio by town <br/>- LSTAT — % lower status of the population</p></blockquote><ul class=""><li id="4393" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">1 target (with variable name “MEDV”) — median value of owner-occupied homes in $1000's, at a specific location</li></ul><p id="533f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can download the raw data <a class="af pn" href="https://faculty.tuck.dartmouth.edu/images/uploads/faculty/business-analytics/Boston_Housing.xlsx" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="0a8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Load data into Python using <code class="cx pr ps pt nl b">pandas</code>:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="8108" class="no np fq nl b bg nq nr l ns nt">import pandas as pd<br/><br/># Load data<br/>data = pd.read_excel("Boston_Housing.xlsx")</span></pre><p id="b163" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">See the dataset’s number of rows (observations) and columns (variables):</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="cff9" class="no np fq nl b bg nq nr l ns nt">data.shape<br/># (506, 14)</span></pre><p id="afd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <strong class="ml fr">modeling problem</strong> of our exercise is: given the attributes of a location, try to predict the <strong class="ml fr">median housing price of this location</strong>.</p><p id="b858" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We store the target variable and predictors using 2 separate objects, <em class="ou">x</em> and <em class="ou">y</em>, following math and ML notations.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="8cb4" class="no np fq nl b bg nq nr l ns nt"># Split up predictors and target<br/>y = data['MEDV']<br/>X = data.drop(columns=['MEDV'])</span></pre><p id="5d45" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Visualize </strong>the dataset by histogram and scatter plot:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="5422" class="no np fq nl b bg nq nr l ns nt">import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/># Distribution of predictors and relationship with target<br/>for col in X.columns:<br/>    fig, ax = plt.subplots(1, 2, figsize=(6,2))<br/>    ax[0].hist(X[col])<br/>    ax[1].scatter(X[col], y)<br/>    fig.suptitle(col)<br/>    plt.show()</span></pre><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pu"><img src="../Images/55a4db43502733e11e9da189a5afd375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*FBktqPnc2vbDizuE43bnJA.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Example output of histogram and scatter plot (Image by author)</figcaption></figure><p id="96ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The point of visualizing the variables is to see if any <strong class="ml fr">transformation </strong>is needed for the variables, and identify the <strong class="ml fr">type of relationship</strong> between individual variables and target. For example, the target may have a linear relationship with some predictors, but polynomial relationship with others. This further <strong class="ml fr">infers which models to use</strong> for solving the problem.</p><h1 id="e60d" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Cost Computation</h1><p id="0d5e" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">How well the model captures the relationship between the predictors and the target can be <strong class="ml fr">measured by</strong> how much the predicted results <strong class="ml fr">deviate</strong> from the ground truth. The function that quantifies this deviation is called <strong class="ml fr">Cost Function</strong>.</p><p id="e064" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The smaller the <strong class="ml fr">cost </strong>is, the better the model captures the relationship the predictors and the target. This means, mathematically, the <strong class="ml fr">model training</strong> process aims to <strong class="ml fr">minimize </strong>the result of cost function.</p><p id="47f3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are different cost functions that can be used for regression problems: Sum of Squared Errors (SSE), Mean Squared Error (MSE), Mean Absolute Error (MAE)…</p><p id="d599" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">MSE</strong> is the most popular cost function used for Linear Regression, and is the default cost function in many statistical packages in R and Python. Here’s its math expression:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pv"><img src="../Images/97705ac06e7953532935e51ec78ceb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*8MnKaX2tF3unnwYJInFBKA.png"/></div></figure><p id="29fd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note: The 2 in the denominator is there to make calculation neater.</p><p id="a314" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To use MSE as our cost function, we can create the following function in Python:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="b740" class="no np fq nl b bg nq nr l ns nt">def compute_cost(X, y, w, b): <br/>    m = X.shape[0] <br/>    <br/>    f_wb = np.dot(X, w) + b<br/>    cost = np.sum(np.power(f_wb - y, 2))<br/>    <br/>    total_cost = 1 / (2 * m) * cost<br/><br/>    return total_cost</span></pre><h1 id="d4e8" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Gradient Descent</h1><p id="b643" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk"><strong class="ml fr">Gradient </strong>— the slope of the tangent line at a certain point of the function. In multivariable calculus, gradient is a vector that points in the direction of the steepest ascent at a certain point.</p><p id="6c9d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Descent</strong> — moving towards the minimum of the cost function.</p><p id="5ff3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Gradient Descent </strong>— a method that iteratively adjusts the parameters in small steps, guided by the gradient, to reach the lowest point of a function. It is a way to <strong class="ml fr">numerically </strong>reach the desired parameters for Linear Regression.</p><p id="a68b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In contrast, there’s a way to <strong class="ml fr">analytically </strong>solve for the optimal parameters — Ordinary Least Squares (OLS). See <a class="af pn" href="https://www.geeksforgeeks.org/linear-regression-python-implementation/" rel="noopener ugc nofollow" target="_blank">this GeekforGeeks article</a> for details of how to implement it in Python. In practice, it does not scale as well as the Gradient Descent approach because of higher computational complexity. Therefore, we use Gradient Descent in our case.</p><p id="6658" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In each iteration of the Gradient Descent process:</p><ul class=""><li id="2dac" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">The <strong class="ml fr">gradients </strong>determine the <strong class="ml fr">direction </strong>of the descent</li><li id="3319" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">The <strong class="ml fr">learning rate</strong> determines the <strong class="ml fr">scale </strong>of the descent</li></ul><p id="1324" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To calculate the gradients, we need to understand that there are 2 parameters that alter the value of the cost function:</p><ul class=""><li id="5b4a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk"><strong class="ml fr"><em class="ou">w</em></strong> — the vector of each predictor’s weight</li><li id="8872" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk"><strong class="ml fr"><em class="ou">b</em></strong> — the bias term</li></ul><p id="d0f2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note: because the values of all the observations (<em class="ou">xⁱ</em>) don’t change over the training process, they contribute to the computation result, but are constants, not variables.</p><p id="a61e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Mathematically, the gradients are:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pw"><img src="../Images/00ac51d4afef3f89656324302f167105.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*RKXuTP-KQk6P3NBBO7iiBQ.png"/></div></figure><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow px"><img src="../Images/0e59a669b46d0a40d02f8220f06f2760.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*H7FF8hZSftOWpDF5-pm0hA.png"/></div></figure><p id="b09b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Correspondingly, we create the following function in Python:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="0c00" class="no np fq nl b bg nq nr l ns nt">def compute_gradient(X, y, w, b):<br/>    m, n = X.shape<br/>    dj_dw = np.zeros((n,))<br/>    dj_db = 0.<br/>    <br/>    err = (np.dot(X, w) + b) - y<br/>    dj_dw = np.dot(X.T, err)    # dimension: (n,m)*(m,1)=(n,1)<br/>    dj_db = np.sum(err)<br/>    <br/>    dj_dw = dj_dw / m<br/>    dj_db = dj_db / m<br/>    <br/>    return dj_db, dj_dw</span></pre><p id="34bb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using this function, we get the gradients of the cost function, and with a set learning rate, update the parameters iteratively.</p><p id="765f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Since it’s a loop logically, we need to define the stopping condition, which could be any of:</p><ul class=""><li id="a22d" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">We reach the set <strong class="ml fr">number of iterations</strong></li><li id="13b8" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">The <strong class="ml fr">cost </strong>gets to below a certain threshold</li><li id="fc04" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">The <strong class="ml fr">improvement </strong>drops below a certain threshold</li></ul><p id="0718" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we choose the number of iterations as the stopping condition, we can write the Gradient Descent process to be:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="5aa3" class="no np fq nl b bg nq nr l ns nt">def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):<br/>    J_history = []<br/>    w = copy.deepcopy(w_in)<br/>    b = b_in<br/>    <br/>    for i in range(num_iters):<br/>        dj_db, dj_dw = gradient_function(X, y, w, b)<br/>        <br/>        w = w - alpha * dj_dw<br/>        b = b - alpha * dj_db<br/>        <br/>        cost = cost_function(X, y, w, b)<br/>        J_history.append(cost)<br/>        <br/>        if i % math.ceil(num_iters/10) == 0:<br/>            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}")<br/>        <br/>    return w, b, J_history</span></pre><p id="28ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Apply it to our dataset:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="1f46" class="no np fq nl b bg nq nr l ns nt">iterations = 1000<br/>alpha = 1.0e-6<br/><br/>w_out, b_out, J_hist = gradient_descent(X_train, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, iterations)</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="fcc7" class="no np fq nl b bg nq nr l ns nt">Iteration    0: Cost   169.76<br/>Iteration  100: Cost   106.96<br/>Iteration  200: Cost   101.11<br/>Iteration  300: Cost    95.90<br/>Iteration  400: Cost    91.26<br/>Iteration  500: Cost    87.12<br/>Iteration  600: Cost    83.44<br/>Iteration  700: Cost    80.15<br/>Iteration  800: Cost    77.21<br/>Iteration  900: Cost    74.58</span></pre><p id="5172" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can visualize the process of cost decreases as the number iteration increases using the below function:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="936c" class="no np fq nl b bg nq nr l ns nt">def plot_cost(data, cost_type):<br/>    plt.figure(figsize=(4,2))<br/>    plt.plot(data)<br/>    plt.xlabel("Iteration Step")<br/>    plt.ylabel(cost_type)<br/>    plt.title("Cost vs. Iteration")<br/>    plt.show()    </span></pre><p id="a081" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s the the plot for our training process:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pz"><img src="../Images/d76a91de9d10c5e4c3987e31c90b8dbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*pMNcWb71dZqwWbQRcT1Itg.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">How the value of cost function changes over the iterations (Image by author)</figcaption></figure><h1 id="58ae" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Prediction</h1><p id="33a4" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">Making predictions is essentially applying the model to our dataset of interest to get the output values. These values are <strong class="ml fr">what the model “thinks”</strong> <strong class="ml fr">the target value should be, given a set of predictor values</strong>.</p><p id="7683" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In our case, we apply the linear function:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="33c9" class="no np fq nl b bg nq nr l ns nt">def predict(X, w, b):<br/>    p = np.dot(X, w) + b<br/>    return p</span></pre><p id="8690" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Get the prediction results using:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="3173" class="no np fq nl b bg nq nr l ns nt">y_pred = predict(X_test, w_out, b_out)</span></pre><h1 id="d923" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Result Evaluation</h1><p id="1736" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">How do we get an idea of the model performance?</p><p id="9213" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">One way is through the cost function, as stated earlier:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="a5f3" class="no np fq nl b bg nq nr l ns nt">def compute_mse(y1, y2):<br/>    return np.mean(np.power((y1 - y2),2))</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="ae7a" class="no np fq nl b bg nq nr l ns nt">mse = compute_mse(y_test, y_pred)<br/>print(mse)</span></pre><p id="ada1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s the MSE on our test dataset:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="0505" class="no np fq nl b bg nq nr l ns nt">132.83636802687786</span></pre><p id="a999" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another way is more intuitive — visualizing the predicted values against the actual values. If the model makes perfect predictions, then each element of <code class="cx pr ps pt nl b">y_test</code> should always equal to the corresponding element of <code class="cx pr ps pt nl b">y_pred</code>. If we plot <code class="cx pr ps pt nl b">y_test</code> on <em class="ou">x</em> axis, <code class="cx pr ps pt nl b">y_pred</code> on <em class="ou">y</em> axis, the dots will form a <strong class="ml fr">diagonal</strong> straight line.</p><p id="6e42" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s our custom plotting function for the comparison:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="f815" class="no np fq nl b bg nq nr l ns nt">def plot_pred_actual(y_actual, y_pred):<br/>    x_ul = int(math.ceil(max(y_actual.max(), y_pred.max()) / 10.0)) * 10<br/>    y_ul = x_ul<br/><br/>    plt.figure(figsize=(4,4))<br/>    plt.scatter(y_actual, y_pred)<br/>    plt.xlim(0, x_ul)<br/>    plt.ylim(0, y_ul)<br/>    plt.xlabel("Actual values")<br/>    plt.ylabel("Predicted values")<br/>    plt.title("Predicted vs Actual values")<br/>    plt.show()</span></pre><p id="5b60" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After applying to our training result, we find that the dots look nothing like a straight line:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qa"><img src="../Images/d13f49c7be107a1c26dfcda35ddf3ade.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*m7kE20acpLfOeNx6ZXGR-Q.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Scatter plot of predicted values against actual values (Image by author)</figcaption></figure><p id="b796" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This should get us thinking: how can we improve the model’s performance?</p><h1 id="9194" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Feature Scaling</h1><p id="acd4" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">The Gradient Descent process is sensitive to the scale of features. As shown in the contour plot on the left, when the learning rate of different features are kept the same, then if the features are in different scales, the path of reaching global minimum may jump back and forth along the cost function.</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div role="button" tabindex="0" class="qc qd ed qe bh qf"><div class="ov ow qb"><img src="../Images/cc27bf98766041306bdc54fa915c5edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYWO4Q-fzYY_9-Mim6_vXA.png"/></div></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">The path towards global minimum of the cost function when features are not-scaled vs scaled (Source: <a class="af pn" href="https://www.youtube.com/watch?v=CFA7OFYDBQY" rel="noopener ugc nofollow" target="_blank">DataMListic</a>)</figcaption></figure><p id="8b7d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After scaling all the features to the same ranges, we can observe a smoother and more straight-forward path to global minimum of the cost function.</p><p id="8647" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are multiple ways to conduct feature scaling, and here we choose <strong class="ml fr">Standardization </strong>to turn all the features to have mean of 0 and standard deviation of 1.</p><p id="6333" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s how to standardize features in Python:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="435f" class="no np fq nl b bg nq nr l ns nt">from sklearn.preprocessing import StandardScaler<br/><br/>standard_scaler = StandardScaler()<br/>X_train_norm = standard_scaler.fit_transform(X_train)<br/>X_test_norm = standard_scaler.transform(X_test)</span></pre><p id="85c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now we conduct Gradient Descent on the standardized dataset:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="8fe7" class="no np fq nl b bg nq nr l ns nt">iterations = 1000<br/>alpha = 1.0e-2<br/><br/>w_out, b_out, J_hist = gradient_descent(X_train_norm, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, iterations)<br/><br/>print(f"Training result: w = {w_out}, b = {b_out}")<br/>print(f"Training MSE = {J_hist[-1]}")</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="2b5d" class="no np fq nl b bg nq nr l ns nt">Training result: w = [-0.87200786  0.83235112 -0.35656148  0.70462672 -1.44874782  2.69272839<br/> -0.12111304 -2.55104665  0.89855827 -0.93374049 -2.151963   -3.7142413 ], b = 22.61090500500162<br/>Training MSE = 9.95513733581214</span></pre><p id="08af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We get a steeper and smoother decline of cost before 200 iterations, compared to the previous round of training:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pz"><img src="../Images/66636c59252b21acb1e9bde42290616f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*2L-pJZ-_JB_QszkD7PRnTA.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Cost by each iteration on the standardized dataset (Image by author)</figcaption></figure><p id="28af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we plot the predicted versus actual values again, we see the dots look much closer to a straight line:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qa"><img src="../Images/db67ab25d03c0b01f74ae4f4b25ecdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*eKx-V23U3riPwv0j49YiDQ.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Scatter plot of predicted values against actual values on standardized dataset (Image by author)</figcaption></figure><p id="052d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To quantify the model performance on the test set:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="f13b" class="no np fq nl b bg nq nr l ns nt">mse = compute_mse(y_test, y_pred)<br/>print(f"Test MSE = {mse}")</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="6ff9" class="no np fq nl b bg nq nr l ns nt">Test MSE = 35.66317674147827</span></pre><p id="189d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We see an improvement from MSE of 132.84 to 35.66! Can we do more to improve the model?</p><h1 id="2492" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Regularization — Ridge Regression</h1><p id="0691" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">We notice that in the last round of training, the training MSE is 9.96, and the testing MSE is 35.66. Can we push the test set performance to be closer to training set?</p><p id="7ef0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here comes <strong class="ml fr">Regularization</strong>. It penalizes large parameters to prevent the model from being too specific to the training set.</p><p id="68d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are mainly 2 popular ways of regularization:</p><ul class=""><li id="cc77" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk"><strong class="ml fr">L1 Regularization</strong> — uses the L1 norm (<strong class="ml fr">absolute values</strong>, a.k.a. “Manhattan norm”) of the weights as the penalty term.</li><li id="cb8a" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk"><strong class="ml fr">L2 Regularization</strong> — uses the L2 norm (<strong class="ml fr">squared values</strong>, a.k.a. “Euclidean norm”) of the weights as the penalty term.</li></ul><p id="fbfd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s first try <strong class="ml fr">Ridge Regression</strong> which uses L2 regularization as our new version of model. Its Gradient Descent process is easier to understand than <strong class="ml fr">LASSO Regression</strong>, which uses L1 regularization.</p><p id="ede9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The cost function with L1 regularization looks like this:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qg"><img src="../Images/30ba14f3eb405c8a1f5fbf3276207f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*56aVEOu6ks0aEhaOa_FrZw.png"/></div></figure><p id="a728" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Lambda </strong>controls the degree of penalty. When lambda is high, the level of penalty is high, then the model leans to underfitting.</p><p id="83cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can turn the calculation into the following function:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="2512" class="no np fq nl b bg nq nr l ns nt">def compute_cost_ridge(X, y, w, b, lambda_ = 1): <br/>    m = X.shape[0] <br/>    <br/>    f_wb = np.dot(X, w) + b<br/>    cost = np.sum(np.power(f_wb - y, 2))    <br/><br/>    reg_cost = np.sum(np.power(w, 2))<br/><br/>    total_cost = 1 / (2 * m) * cost + (lambda_ / (2 * m)) * reg_cost<br/><br/>    return total_cost</span></pre><p id="e1b1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the Gradient Descent process, we use the following function to compute the gradients with regularization:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="42a1" class="no np fq nl b bg nq nr l ns nt">def compute_gradient_ridge(X, y, w, b, lambda_):<br/>    m = X.shape[0]<br/><br/>    err = np.dot(X, w) + b - y<br/>    dj_dw = np.dot(X.T, err) / m + (lambda_ / m) * w<br/>    dj_db = np.sum(err) / m<br/><br/>    return dj_db, dj_dw</span></pre><p id="b1d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Combine the two steps together, we get the following Gradient Descent function for Ridge Regression:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="1b4b" class="no np fq nl b bg nq nr l ns nt">def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, lambda_=0.7, num_iters=1000):<br/>    J_history = []<br/>    w = copy.deepcopy(w_in)<br/>    b = b_in<br/>    <br/>    for i in range(num_iters):<br/>        dj_db, dj_dw = gradient_function(X, y, w, b, lambda_)<br/>        <br/>        w = w - alpha * dj_dw<br/>        b = b - alpha * dj_db<br/>        <br/>        cost = cost_function(X, y, w, b, lambda_)<br/>        J_history.append(cost)<br/>        <br/>        if i % math.ceil(num_iters/10) == 0:<br/>            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}")<br/>        <br/>    return w, b, J_history</span></pre><p id="052b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Train the model on our standardized dataset:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="fd9a" class="no np fq nl b bg nq nr l ns nt">iterations = 1000<br/>alpha = 1.0e-2<br/>lambda_ = 1<br/><br/>w_out, b_out, J_hist = gradient_descent(X_train_norm, y_train, w_init, b_init, compute_cost_ridge, compute_gradient_ridge, alpha, lambda_, iterations)</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="be2e" class="no np fq nl b bg nq nr l ns nt">print(f"Training result: w = {w_out}, b = {b_out}")<br/>print(f"Training MSE = {J_hist[-1]}")</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="0d2f" class="no np fq nl b bg nq nr l ns nt">Training result: w = [-0.86996629  0.82769399 -0.35944104  0.7051097  -1.43568137  2.69434668<br/> -0.12306667 -2.53197524  0.88587909 -0.92817437 -2.14746836 -3.70146378], b = 22.61090500500162<br/>Training MSE = 10.005991756561285</span></pre><p id="bcca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The training cost is slightly higher than our previous version of model.</p><p id="19c1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The learning curve looks very similar to the one from the previous round:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow pz"><img src="../Images/1d49ce9629259ad7e2859125a46332d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*icWut5oYRssRjUrjju0pxg.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Cost by each iteration for Ridge Regression (Image by author)</figcaption></figure><p id="7c9a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The predicted vs actual values plot looks almost identical to what we got from the previous round:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qa"><img src="../Images/c3083a8f8327e8491a9dafdf20291a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*NA7LUPLnN1PhwC4WYoUUaw.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Scatter plot of predicted values against actual values for Ridge Regression (Image by author)</figcaption></figure><p id="2664" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We got test set MSE of 35.69, which is slightly higher than the one without regularization.</p><h1 id="cb10" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Regularization — LASSO Regression</h1><p id="f89f" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">Finally, let’s try out LASSO Regression! LASSO stands for <strong class="ml fr">Least Absolute Shrinkage and Selection Operator</strong>.</p><p id="c1e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is the cost function with L2 regularization:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qh"><img src="../Images/3b3bf962171d9ca08e93bb2c114abaed.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*XHpJQp3IjkR94dnFHE2mxA.png"/></div></figure><p id="b97e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What’s tricky about the training process of LASSO Regression, is that the derivative of the absolute function is undefined at <em class="ou">w=0</em>. Therefore, <strong class="ml fr">Coordinate Descent</strong> is used in practice for LASSO Regression. It focuses on one coordinate at a time to find the minimum, and then switch to the next coordinate.</p><p id="acdc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s how we implement it in Python, inspired by <a class="af pn" href="https://xavierbourretsicotte.github.io/lasso_implementation.html" rel="noopener ugc nofollow" target="_blank">Sicotte (2018)</a> and <a class="af pn" href="https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook" rel="noopener ugc nofollow" target="_blank">D@Kg’s notebook (2022)</a>.</p><p id="ee2f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First, we define the soft threshold function, which is the solution to the single variable optimization problem:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="4c81" class="no np fq nl b bg nq nr l ns nt">def soft_threshold(rho, lamda_):<br/>    if rho &lt; - lamda_:<br/>        return (rho + lamda_)<br/>    elif rho &gt;  lamda_:<br/>        return (rho - lamda_)<br/>    else: <br/>        return 0</span></pre><p id="4dab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Second, calculate the residuals of the prediction:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="6e65" class="no np fq nl b bg nq nr l ns nt">def compute_residuals(X, y, w, b):<br/>    return y - (np.dot(X, w) + b)</span></pre><p id="8a23" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Use the residual to calculate rho, which is the subderivative:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="4935" class="no np fq nl b bg nq nr l ns nt">def compute_rho_j(X, y, w, b, j):<br/>    X_k = np.delete(X, j, axis=1)    # remove the jth element<br/>    w_k = np.delete(w, j)    # remove the jth element<br/><br/>    err = compute_residuals(X_k, y, w_k, b)<br/><br/>    X_j = X[:,j]<br/>    rho_j = np.dot(X_j, err)<br/>    <br/>    return rho_j</span></pre><p id="723a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Put everything together:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="d06c" class="no np fq nl b bg nq nr l ns nt">def coordinate_descent_lasso(X, y, w_in, b_in, cost_function, lambda_, num_iters=1000, tolerance=1e-4):<br/>    J_history = []<br/>    w = copy.deepcopy(w_in)<br/>    b = b_in<br/>    n = X.shape[1]<br/><br/>    for i in range(num_iters):<br/>        # Update weights<br/>        for j in range(n):<br/>            X_j = X[:,j]<br/>            rho_j = compute_rho_j(X, y, w, b, j)<br/>            w[j] = soft_threshold(rho_j, lambda_) / np.sum(X_j ** 2)<br/><br/>        # Update bias<br/>        b = np.mean(y - np.dot(X, w))<br/>        err = compute_residuals(X, y, w, b)<br/><br/>        # Calculate total cost<br/>        cost = cost_function(X, y, w, b, lambda_)<br/>        J_history.append(cost)<br/><br/>        if i % math.ceil(num_iters/10) == 0:<br/>            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}")<br/><br/>        # Check convergence<br/>        if np.max(np.abs(err)) &lt; tolerance:<br/>            break<br/><br/>    return w, b, J_history</span></pre><p id="b51c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Apply it to our training set:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="bb0d" class="no np fq nl b bg nq nr l ns nt">iterations = 1000<br/>lambda_ = 1e-4<br/>tolerance = 1e-4<br/><br/>w_out, b_out, J_hist = coordinate_descent_lasso(X_train_norm, y_train, w_init, b_init, compute_cost_lasso, lambda_, iterations, tolerance)</span></pre><p id="a9e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The training process converged drastically, compared to Gradient Descent on Ridge Regression:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qi"><img src="../Images/2954df2b315ee435c1882247a520c4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*ym7fqSWz9r403DC0LsHA7Q.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Cost by each iteration for LASSO Regression (Image by author)</figcaption></figure><p id="a066" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, the training result is not significantly improved:</p><figure class="nf ng nh ni nj oy ov ow paragraph-image"><div class="ov ow qa"><img src="../Images/0d0a3769b84987abd051000bb9c5c56a.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*4RTRAviq_i5lTvOZ4hpvDw.png"/></div><figcaption class="pa pb pc ov ow pd pe bf b bg z dx">Scatter plot of predicted values against actual values for LASSO Regression (Image by author)</figcaption></figure><p id="f30b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Eventually, we achieved MSE of 34.40, which is the lowest among the methods we tried.</p><h1 id="db02" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Interpreting the Results</h1><p id="dc1c" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">How do we interpret the model training results using human language? Let’s use the result of LASSO Regression as an example, since it has the best performance among the model variations we tried out.</p><p id="f659" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can get the <strong class="ml fr">weights </strong>and the <strong class="ml fr">bias </strong>by printing the <code class="cx pr ps pt nl b">w_out</code> and <code class="cx pr ps pt nl b">b_out</code> we got in the previous section:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="b9ed" class="no np fq nl b bg nq nr l ns nt">print(f"Training result: w = {w_out}, b = {b_out}")</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="1ec0" class="no np fq nl b bg nq nr l ns nt">Training result: w = [-0.86643384  0.82700157 -0.35437324  0.70320366 -1.44112303  2.69451013<br/> -0.11649385 -2.53543865  0.88170899 -0.92308699 -2.15014264 -3.71479811], b = 22.61090500500162</span></pre><p id="07d6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In our case, there are 13 predictors, so this dataset has 13 dimensions. In each dimension, we can plot the predictor <code class="cx pr ps pt nl b">x_i</code> against the target <code class="cx pr ps pt nl b">y</code> as a scatterplot. The regression line’s <strong class="ml fr">slope</strong> is the weight <code class="cx pr ps pt nl b">w_i</code>.</p><p id="0fd3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In details, the first dimension is <em class="ou">“CRIM — per capita crime rate by town”</em>, and our <code class="cx pr ps pt nl b">w_1</code> is -0.8664. This means, each unit of <strong class="ml fr">increase </strong>in <code class="cx pr ps pt nl b">x_i</code>, <code class="cx pr ps pt nl b">y</code> is expected to <strong class="ml fr">decrease </strong>by -0.8664 unit.</p><p id="674a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that we have <strong class="ml fr">scaled </strong>our dataset before we run the training process, so now we need to <strong class="ml fr">reverse </strong>that process to get the intuitive relationship between the predictor <em class="ou">“per capita crime rate by town”</em> and our target variable <em class="ou">“median value of owner-occupied homes in $1000’s, at a specific location”</em>.</p><p id="82ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To reverse the scaling, we need to get the vector of scales:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="8743" class="no np fq nl b bg nq nr l ns nt">print(standard_scaler.scale_)</span></pre><pre class="py nk nl nm bp nn bb bk"><span id="9e13" class="no np fq nl b bg nq nr l ns nt">[8.12786482e+00 2.36076347e+01 6.98435113e+00 2.53975353e-01<br/> 1.15057872e-01 6.93831576e-01 2.80721481e+01 2.07800639e+00<br/> 8.65042138e+00 1.70645434e+02 2.19210336e+00 7.28999160e+00]</span></pre><p id="8584" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here we find the scale we used for our first predictor: 8.1278. We divide the weight of -0.8664 by scale or 8.1278 to get <strong class="ml fr">-0.1066</strong>.</p><p id="c544" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">This means:</strong> when all other factors remains the same, if the per capita crime rate <strong class="ml fr">increases </strong>by 1 percentage point, the medium housing price of that location <strong class="ml fr">drops </strong>by $1000 * (-0.1066) = $106.6 in value.</p><h1 id="c5a6" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">Summary</h1><p id="b0e9" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">This article unveiled the details of implementing Linear Regression in Python, going beyond just calling high level <code class="cx pr ps pt nl b">scikit-learn</code> functions.</p><ul class=""><li id="5190" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">We looked into the target of regression — minimizing the cost function, and wrote the cost function in Python.</li><li id="2419" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We broke down Gradient Descent process step by step.</li><li id="020e" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We created plotting functions to visualize the training process and assessing the results.</li><li id="dade" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We discussed ways to improve model performance, and found out that LASSO Regression achieved the lowest test MSE for our problem.</li><li id="090a" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">Lastly, we used one predictor as an example to illustrate how the training result should be interpreted.</li></ul><h1 id="0614" class="nu np fq bf nv nw nx gq ny nz oa gt ob oc od oe of og oh oi oj ok ol om on oo bk">References</h1><p id="fbc8" class="pw-post-body-paragraph mj mk fq ml b go op mn mo gr oq mq mr ms or mu mv mw os my mz na ot nc nd ne fj bk">[1] A. Ng, <em class="ou">Supervised Machine Learning: Regression and Classification</em> (2022), <a class="af pn" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a></p><p id="71f7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] D. Harrison and D. L. Rubinfeld, <em class="ou">Hedonic Housing Prices and the Demand for Clean Air</em> (1978), <a class="af pn" href="https://www.law.berkeley.edu/files/Hedonic.PDF" rel="noopener ugc nofollow" target="_blank">https://www.law.berkeley.edu/files/Hedonic.PDF</a></p><p id="6ec5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] <em class="ou">Linear Regression (Python Implementation)</em> (2024), <a class="af pn" href="https://www.geeksforgeeks.org/linear-regression-python-implementation/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/linear-regression-python-implementation/</a></p><p id="e0f2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] <em class="ou">Why We Perform Feature Scaling In Machine Learning</em> (2022), <a class="af pn" href="https://www.youtube.com/watch?v=CFA7OFYDBQY" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=CFA7OFYDBQY</a></p><p id="3413" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] X. Sicotte, <em class="ou">Lasso regression: implementation of coordinate descent</em> (2018), <a class="af pn" href="https://xavierbourretsicotte.github.io/lasso_implementation.html" rel="noopener ugc nofollow" target="_blank">https://xavierbourretsicotte.github.io/lasso_implementation.html</a></p><p id="a4f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[6] D@Kg, <em class="ou">Coordinate Descent for LASSO &amp; Normal Regression</em> (2022), <a class="af pn" href="https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/code/ddatad/coordinate-descent-for-lasso-normal-regression/notebook</a></p><p id="f3b6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[7] Fairlearn, <em class="ou">Revisiting the Boston Housing Dataset</em>, <a class="af pn" href="https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset" rel="noopener ugc nofollow" target="_blank">https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html#revisiting-the-boston-housing-dataset</a></p><p id="4dad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[8] V. Rathod, <em class="ou">All about Boston Housing</em> (2020), <a class="af pn" href="https://rpubs.com/vidhividhi/LRversusDT" rel="noopener ugc nofollow" target="_blank">https://rpubs.com/vidhividhi/LRversusDT</a></p><p id="0fde" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[9] A. Gupta, <em class="ou">Regularization in Machine Learning</em> (2023), <a class="af pn" href="https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/</a></p><p id="7910" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[10] The University of Melbourne, <em class="ou">Rescaling explanatory variables in linear regression</em>, <a class="af pn" href="https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression" rel="noopener ugc nofollow" target="_blank">https://scc.ms.unimelb.edu.au/resources/reporting-statistical-inference/rescaling-explanatory-variables-in-linear-regression</a></p></div></div></div></div>    
</body>
</html>