# 卷积神经网络在图像分类中的历史（1989年至今）

> 原文：[https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28](https://towardsdatascience.com/the-history-of-convolutional-neural-networks-for-image-classification-1989-today-5ea8a5c5fe20?source=collection_archive---------5-----------------------#2024-06-28)

## 深度学习和计算机视觉领域最伟大创新的视觉之旅。

[](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Avishek Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------) [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--5ea8a5c5fe20--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5ea8a5c5fe20--------------------------------) ·阅读时间 15 分钟 ·2024年6月28日

--

在卷积神经网络出现之前，训练神经网络对图像进行分类的标准方法是将图像展平成像素列表，并通过前馈神经网络来输出图像的类别。展平图像的问题在于，它丢失了图像中至关重要的空间信息。

1989年，Yann LeCun 和团队推出了卷积神经网络——过去15年来计算机视觉研究的基石！与前馈网络不同，卷积神经网络保持了图像的二维特性，并能够空间处理信息！

在本文中，我们将回顾卷积神经网络（CNN）在图像分类任务中的历史——从90年代的早期研究开始，到2010年代中期的黄金时代，那时许多最杰出的深度学习架构应运而生，最后讨论当前卷积神经网络研究的最新趋势，它们与注意力机制和视觉变换器展开竞争。

*查看此* [*YouTube 视频*](https://youtu.be/N_PocrMHWbw) *，视频通过动画形象地解释了本文中的所有概念。除非另有说明，本文中使用的所有图像和插图均由我在制作视频版本时自行生成。*

![](../Images/6f788fc5c6f6bbd352b7b138f5198dd1.png)

今天我们将讨论的论文！

# **卷积神经网络的基础**

CNN的核心是卷积操作。我们扫描滤波器穿过图像，并在每个重叠位置计算滤波器与图像的点积。**这个输出结果称为特征图，它捕捉了滤波器模式在图像中出现的程度和位置。**

![](../Images/6a88bb9a19c81a4788e265c2bdfb7177.png)

卷积是如何工作的 — 卷积核在输入图像上滑动，并在每个位置计算重叠部分（点积）— 最终输出一个特征图！

在卷积层中，我们训练多个滤波器，从输入图像中提取不同的特征图。当我们将多个卷积层按顺序堆叠，并加入非线性激活函数时，就得到了卷积神经网络（CNN）。

所以，每一层卷积层同时完成两项任务 —

1\. **空间滤波**，即图像和卷积核之间的卷积操作，和

2\. **结合多个输入通道**并输出一组新的通道。

CNN研究的90％集中在修改或改进这两项内容。

![](../Images/7a53ab3d99f44347e87950b7143ae2f8.png)

CNN的两大核心任务

## 1989年论文

[这篇1989年论文](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)教我们如何通过反向传播从头开始训练非线性CNN。它们输入16x16的灰度手写数字图像，并通过两层卷积层，每层有12个5x5的滤波器。滤波器在扫描过程中还以步长2进行移动。步长卷积对于降采样输入图像非常有用。经过卷积层后，输出的特征图会被展平并传递到两个全连接网络，输出10个数字的概率。通过softmax交叉熵损失函数，网络被优化以预测手写数字的正确标签。每一层后，还使用了tanh非线性激活函数——使得学习到的特征图更加复杂和富有表现力。这个网络仅有9760个参数，相比今天那些包含数亿个参数的网络，它是一个非常小的网络。

![](../Images/5f6ce141fce07f95a9408e58d031bf95.png)

1989年的原始CNN架构

## **偏向性归纳**

偏向性归纳（Inductive Bias）是机器学习中的一个概念，指的是我们故意在学习过程中引入特定的规则和限制，以使我们的模型避免过度泛化，更加接近符合人类理解的解决方案。

当人类进行图像分类时，我们也会进行空间过滤*以寻找共同的模式，形成多个表示*，然后*将它们结合起来做出预测*。CNN架构正是为了复制这一过程而设计的。在前馈网络中，每个像素都被视为独立的特征，因为层中的每个神经元都与所有像素相连——而在CNN中，由于相同的滤波器扫描整个图像，因此参数共享更多。归纳偏置使得CNN在数据需求上也更为节省，因为它们通过网络设计能够免费获得局部模式识别，而前馈网络则需要从头开始通过训练周期学习这些模式。

# **Le-Net 5（1998年）**

![](../Images/d625a2448a44d6150887d3d8a598512d.png)

Lenet-5架构（来源：[Le-Net-5论文](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)）

1998年，Yann LeCun及其团队发布了[Le-Net 5](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)——一个更深、更大的7层CNN模型网络。他们还使用了最大池化（Max Pooling），通过从2x2滑动窗口中提取最大值来对图像进行下采样。

![](../Images/59cfe79ef5bedf3c1b0dd5b33ae17f44.png)![](../Images/1f847d9fc9f1eab0131f8b91092d006d.png)

Max Pooling如何工作（左侧）以及随着CNN增加更多层，局部感受野如何增大（右侧）

## 局部感受野

注意，当你训练一个3x3卷积层时，每个神经元都与原始图像中的一个3x3区域相连——这就是神经元的局部感受野——该神经元从中提取模式的图像区域。

当我们通过另一个3x3层传递这个特征图时，新的特征图会间接地创建一个更大的5x5区域的感受野，来源于原始图像。此外，当我们通过最大池化或步幅卷积对图像进行下采样时，感受野也会增加——使得更深层的网络能够更加全局地访问输入图像。

因此，CNN中的早期层只能捕捉到低级细节，如特定的边缘或角落，而后续层则捕捉到更为广泛的全局级别的模式。

# The Draught (1998–2012)

尽管Le-Net-5非常令人印象深刻，但2000年代初期的研究人员仍然认为神经网络在计算上非常昂贵且训练数据需求量大。另一个问题是过拟合——复杂的神经网络可能只是记住整个训练数据集，而无法对新的未见数据集进行泛化。研究人员因此转向传统的机器学习算法，如支持向量机，这些算法在当时较小的数据集上表现出了更好的性能，且计算需求远低于神经网络。

## **ImageNet数据集（2009年）**

[ImageNet数据集](https://www.image-net.org/index.php)在2009年开源——当时包含了320万张标注图像，覆盖了1000多个不同类别。如今，它已经拥有超过1400万张图像和超过2万个标注的不同类别。从2010年到2017年，每年都会举行一个叫做[ILSVRC](https://www.image-net.org/challenges/LSVRC/)的大型比赛，研究小组们会发布模型，以打破ImageNet数据集子集上的基准。在2010年和2011年，传统的机器学习方法如支持向量机（SVM）获胜——但从2012年开始，比赛的焦点就转向了卷积神经网络（CNN）。排名不同网络的指标通常是top-5错误率——即衡量真实类别标签未能出现在网络预测的前5个类别中的比例。

# **AlexNet (2012)**

[AlexNet](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)，由Geoffrey Hinton博士及其团队提出，是2012年ILSVRC的冠军，测试集的top-5错误率为17%。以下是AlexNet的三大主要贡献。

## 1\. 多尺度卷积核

AlexNet在224x224的RGB图像上进行训练，并在网络中使用了多种卷积核大小——分别是11x11、5x5和3x3卷积核。像Le-Net 5这样的模型只使用了5x5卷积核。较大的卷积核计算开销更大，因为它们训练了更多的权重，但也能从图像中捕捉到更多的全局模式。由于这些大卷积核，AlexNet拥有超过6000万个可训练参数。然而，所有这些复杂性可能会导致过拟合。

![](../Images/3dbfe9a516bc0af4053955d3cbd0ae81.png)

AlexNet从较大的卷积核（11x11）开始，并为更深的层次减少卷积核大小（至5x5和3x3）（图片来源：作者）

## 2\. Dropout

为了缓解过拟合，AlexNet采用了一种叫做Dropout的正则化技术。在训练过程中，每一层的部分神经元会被设为零。这可以防止网络过度依赖某些特定的神经元或神经元组来进行预测，从而鼓励所有神经元学习对分类有用的通用有意义特征。

## 3\. RELU

AlexNet还将tanh非线性函数替换为ReLU。ReLU是一种激活函数，它将负值变为零，保留正值不变。由于当x值过高或过低时，tanh函数的梯度会变得很小，导致优化过程变慢，因此它通常会在深度网络中饱和。相比之下，ReLU提供了稳定的梯度信号，使得训练速度比tanh快约6倍。

![](../Images/46bebdc63f8f31a04b7b17236f3a5c49.png)![](../Images/9853cded8ce78082f5567793b0d22c30.png)![](../Images/40d276fedfc70960ef639c45f76d221a.png)

RELU、TANH，以及RELU带来的差异有多大（图片来源：中间：[大数据中的人工智能](https://learning.oreilly.com/library/view/artificial-intelligence-for/9781788472173/)，右侧：[Alex-Net论文](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)）  

AlexNet还引入了局部响应归一化（Local Response Normalization）以及分布式CNN训练策略。  

# **GoogleNet / Inception（2014）**  

2014年，GoogleNet的论文在ImageNet上取得了6.67%的Top-5错误率。GoogLeNet的核心组件是**Inception模块**。每个Inception模块由不同滤波器大小（1x1、3x3、5x5）和最大池化层的并行卷积层组成。Inception将这些卷积核应用于相同的输入，然后将它们连接起来，结合了低级特征和中级特征。  

![](../Images/997501a2e4dd36f4519c2d85b202386a.png)  

一个Inception模块  

## 1x1卷积  

它们还使用了1x1卷积层。每个1x1卷积核首先对输入通道进行缩放，然后将它们结合在一起。1x1卷积核通过与固定值相乘每个像素——这也是它被称为点卷积的原因。  

虽然像3x3和5x5这样的更大卷积核同时进行空间过滤和通道组合，但1x1卷积核仅适用于通道混合，而且在权重数量较少的情况下非常高效。例如，一个3x4的1x1卷积层网格只需训练*(1x1 x 3x4 =)* 12个权重——但是如果使用3x3卷积核，则需要训练*(3x3 x 3x4 =)* 108个权重。  

![](../Images/6f234f8f634cf640af57b563c47cb50d.png)![](../Images/65f6501cf00fd0cbb64593fff021f896.png)  

1x1卷积核与更大卷积核（左）和使用1x1卷积核的降维（右）  

## 降维  

GoogleNet使用1x1卷积层作为降维方法，在对这些低维特征图进行3x3和5x5卷积的空间过滤之前，先降低通道数量。这有助于减少可训练权重的数量，相比于AlexNet，它大大减少了权重数量。  

# **VGGNet（2014）**  

[VGG网络](https://arxiv.org/abs/1409.1556)指出，我们不需要像5x5或7x7这样的更大卷积核，所需的只是3x3卷积核。两个3x3卷积层的感受野与单个5x5卷积层相同。三个3x3卷积层的感受野与单个7x7卷积层相同。  

**深度3x3卷积层捕捉到的感受野与更大卷积核相同，但参数更少！**

一个5x5滤波器训练25个权重——而两个3x3滤波器只需训练18个权重。类似地，一个7x7滤波器训练49个权重，而三个3x3卷积层只需训练27个权重。使用深度的3x3卷积层长期以来成为了CNN架构的标准做法。  

![](../Images/cfed290a6af3ba3dee066dc01e5f4633.png)  

# **批量归一化（2015）**  

深度神经网络在训练过程中可能会遇到一个问题，称为**“内部协方差偏移”**。由于网络的早期层在不断训练，后续层需要持续适应从前面层接收到的不断变化的输入分布。

[批归一化](https://arxiv.org/pdf/1502.03167)旨在通过在训练过程中将每层的输入归一化为零均值和单位标准差，从而抵消这个问题。批归一化或 BN 层可以应用于任何卷积层之后。在训练过程中，它会减去特征图在小批量维度上的均值，并除以标准差。这意味着每一层在训练过程中将看到一个更加平稳的单位高斯分布。

## 批归一化的优势

1.  收敛速度约为原来的 14 倍

1.  让我们使用更高的学习率，并且

1.  使网络对初始权重具有鲁棒性。

# **ResNets (2016)**

## 深度网络在做恒等映射时遇到困难

想象你有一个浅层神经网络，它在分类任务上表现出色。结果发现，如果我们在这个网络上加上 100 个新的卷积层，模型的训练准确性可能会下降！

这相当反直觉，因为这些新层只需要做的就是复制浅层网络每层的输出——至少能够匹配原始的准确性。实际上，深度网络在训练时往往非常难以调优，因为在通过许多层反向传播时，梯度可能会饱和或变得不稳定。在 Relu 和批归一化的帮助下，我们当时能够训练 22 层深的 CNN —— 微软的优秀团队在 2015 年推出了[ResNets](https://arxiv.org/abs/1512.03385)，使我们能够稳定地训练 150 层的 CNN。他们是如何做到的？

## 残差学习

输入通常会通过一个或多个 CNN 层，但在最后，原始输入会被加回到最终输出。这些块被称为残差块，因为它们不需要像传统意义上那样学习最终的输出特征图——它们只是必须加到输入上的残差特征，以便得到最终的特征图。**如果中间层的权重变为零，那么残差块就会返回恒等函数——意味着它将能够轻松复制输入 X。**

![](../Images/b67371d5f87b5198e3e1a03c3a62ef15.png)

残差网络

## 简单的梯度流

在反向传播过程中，梯度可以通过这些快捷路径直接流向模型的早期层，从而更快地到达，有助于防止梯度消失问题。ResNet 将许多这样的块堆叠在一起，形成真正深的网络，而不会损失准确性！

![](../Images/55b293233faca9c8f22ed808e1dd8c4f.png)

[来自 ResNet 论文](https://arxiv.org/abs/1512.03385)

凭借这一显著的改进，ResNet成功训练了一个152层的模型，创造了打破所有纪录的top-5错误率！

# **DenseNet（2017）**

![](../Images/ef9fa851e3acd1c08106018c2f39b471.png)

[Dense-Nets](https://arxiv.org/abs/1608.06993) 也增加了连接早期层与后期层的捷径路径。DenseNet模块训练了一系列卷积层，每一层的输出都会与模块中每个先前层的特征图拼接，之后再传递给下一层。每一层仅向网络的“集体知识”中添加少量的新特征图，随着图像在网络中的流动，DenseNet的网络信息流和梯度流得到了改善，因为每一层可以直接访问来自损失函数的梯度。

![](../Images/164f7f2c97a282693b45bcb96e2b21d7.png)

Dense Nets

# 压缩与激励网络（2017）

[SEN-NET](https://arxiv.org/abs/1709.01507) 是ILSVRC比赛的最终获胜者，提出了将压缩与激励层引入到卷积神经网络（CNN）中。SE模块旨在显式地建模特征图中所有通道之间的依赖关系。在普通的CNN中，特征图的每个通道是相互独立计算的；而SEN-Net采用类似自注意力的方法，使得特征图中的每个通道能够感知输入图像的全局特性。SEN-Net赢得了2017年ILSVRC比赛的最终胜利，且其中一个包含154层的SenNet + ResNet模型，创下了令人惊讶的top-5错误率4.47%。

![](../Images/6b871751ded64eef0b9db11f9795ad28.png)

[SEN-NET](https://arxiv.org/abs/1709.01507)

## 压缩操作

压缩操作通过全局平均池化将输入特征图的空间维度压缩成一个通道描述符。由于每个通道包含捕捉图像局部特性的神经元，压缩操作将关于每个通道的全局信息进行积累。

## 激励操作

激励操作通过与来自压缩操作获得的通道描述符按通道进行乘法，重新调整输入特征图的尺度。这有效地将全局级别的信息传播到每个通道——使每个通道能够理解特征图中其他通道的上下文。

![](../Images/c37db341a2da8c50a202b6e36a3c93a6.png)

[压缩与激励模块](https://arxiv.org/abs/1709.01507)

# **MobileNet（2017）**

卷积层有两个功能——1）*过滤空间信息*，2）*按通道合并信息*。MobileNet 论文使用了**深度可分离卷积**，这是一种将这两种操作分开到两个不同层次的技术——使用深度卷积进行过滤，使用逐点卷积进行通道合并。

## 深度卷积

![](../Images/1b885d58c9a73b3b1a51592e2d9e1966.png)

深度可分离卷积

给定一个具有M个通道的特征图集，首先，它们使用深度卷积层训练M个3x3卷积核。与普通卷积层对所有特征图进行卷积不同，深度卷积层训练的滤波器仅对每个特征图单独进行卷积。其次，它们使用1x1逐点卷积滤波器来混合所有这些特征图。**像这样分开滤波和组合步骤极大地减少了权重数量**，使得网络变得非常轻量化，同时仍保持性能。

![](../Images/66344afbabc2e1473b31832405fdda81.png)

为什么深度可分离层减少训练权重

# **MobileNetV2 (2019)**

2018年，MobileNetV2通过引入两个创新——*线性瓶颈*（Linear Bottlenecks）和*倒残差*（Inverted residuals）——改进了MobileNet架构。

## 线性瓶颈

MobileNetV2使用1x1逐点卷积进行降维，然后是深度卷积层进行空间滤波，接着再通过一个1x1逐点卷积层扩展通道回原来的维度。**这些瓶颈不经过RELU，而是保持线性。** RELU会将降维步骤中产生的所有负值归零，这可能导致网络丢失重要信息，尤其是在这些低维空间大部分为负值时。线性层防止了在这一瓶颈过程中丢失过多信息。

![](../Images/8950a4c49cce3be0d309a0c1db0f9c82.png)

每个特征图的宽度旨在显示相对的通道维度。

## 倒残差（Inverted Residuals）

第二个创新被称为倒残差。通常，残差连接发生在具有最高通道数的层之间，但作者在瓶颈层之间添加了捷径。瓶颈层捕捉了低维潜在空间中的相关信息，而这些层之间信息和梯度的自由流动是至关重要的。

![](../Images/c2e4c4eae0d3d37c24af81e463c38f2b.png)

# **视觉变换器 (2020)**

视觉变换器（Vision Transformers，简称ViTs）证明了变换器在图像分类任务中确实能够超越最先进的卷积神经网络（CNN）。变换器和注意力机制提供了一种高度可并行化、可扩展的通用架构，用于建模序列。神经注意力是深度学习中的一个完全不同的领域，本文不会涉及，但你可以在这个YouTube视频中了解更多。

## ViTs使用图块嵌入和自注意力

输入图像首先被分割成一系列固定大小的图块。每个图块都通过卷积神经网络（CNN）或经过一个线性层独立地嵌入成一个固定大小的向量。然后，这些图块嵌入和它们的位置编码作为一个令牌序列输入到基于自注意力的变换器编码器中。自注意力模型会建模所有图块之间的关系，并输出新的更新后的图块嵌入，这些嵌入能够理解整个图像的上下文。

![](../Images/fc228d2bbe95cf7993f257be0bc535bb.png)

视觉变换器。每个自注意力层通过图像的全局上下文进一步对每个块的嵌入进行上下文化。

## 归纳偏置与普适性

当CNN引入了多个关于图像的归纳偏置时，变换器则相反——没有局部化，没有滑动卷积核——它们依赖于普适性和原始计算来建模图像中所有块之间的关系。自注意力层允许图像中所有块之间的全局连接，无论它们在空间上相距多远。归纳偏置在较小的数据集上表现良好，但变换器的优势在于大规模训练数据集，最终，一个通用的框架将战胜CNN所提供的归纳偏置。

![](../Images/6a450fb97b290d4419671ac36c27f673.png)

卷积层与自注意力层

# **ConvNext —** 2020年代的卷积网络 **(2022)**

在这篇文章中加入Swin Transformers是个很好的选择，但那是另一个话题！因为这是关于CNN的文章，让我们集中讨论最后一篇CNN相关的论文。

![](../Images/3a0f9822f909de4a7b52dfe04861be4e.png)

## 像VITs一样对图像进行分块

ConvNext的输入遵循一种受视觉转换器启发的图像分块策略。一个4x4的卷积核，步幅为4，创建了一个下采样的图像，并输入到网络的其余部分。

## 深度可分离卷积

受MobileNet启发，ConvNext使用深度可分离卷积层。作者还假设深度卷积类似于自注意力中的加权和操作，这种操作在每个通道上进行，仅在空间维度上混合信息。此外，1x1的逐点卷积类似于自注意力中的通道混合步骤。

## 更大的卷积核尺寸

自VGG以来，CNN一直在使用3x3的卷积核，而ConvNext提出了更大的7x7滤波器，以捕捉更广泛的空间上下文，尽量接近ViTs所捕捉的完全全局上下文，同时保留CNN的本地化特性。

还有一些其他的改进，比如使用受MobileNetV2启发的反向瓶颈、GELU激活函数、使用层归一化而非批量归一化等，这些都塑造了ConvNext架构的其余部分。

## 可扩展性

ConvNext通过深度可分离卷积提供了更高的计算效率，并且在高分辨率图像上比变换器更具可扩展性——这是因为自注意力在序列长度上按平方级别扩展，而卷积则不受此限制。

![](../Images/273120ccbef227363f48781e3fc84edf.png)

# 最终思考！

卷积神经网络（CNN）的历史教会了我们很多关于深度学习、归纳偏置以及计算本质的知识。最终谁会胜出是很有趣的——是卷积网络的归纳偏置，还是变换器的普适性。一定要查看配套的YouTube视频，以便直观地了解这篇文章及下列列出的相关论文。

## 参考文献

带反向传播的卷积神经网络（CNN with Backprop）（1989年）：[http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf)

LeNet-5：[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)

AlexNet：[https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)

GoogleNet：[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)

VGG：[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)

批量归一化（Batch Norm）：[https://arxiv.org/pdf/1502.03167](https://arxiv.org/pdf/1502.03167)

ResNet：[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)

DenseNet：[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)

MobileNet：[https://arxiv.org/abs/1704.04861](https://arxiv.org/abs/1704.04861)

MobileNet-V2：[https://arxiv.org/abs/1801.04381](https://arxiv.org/abs/1801.04381)

视觉变换器（Vision Transformers）：[https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)

ConvNext：[https://arxiv.org/abs/2201.03545](https://arxiv.org/abs/2201.03545)

挤压与激励网络（Squeeze-and-Excitation Network）：[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)

Swin Transformers：[https://arxiv.org/abs/2103.14030](https://arxiv.org/abs/2103.14030)
