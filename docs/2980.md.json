["```py\nclass Actor(nn.Module):\n    \"\"\"\n    Actor network for the DDPG algorithm.\n    \"\"\"\n    def __init__(self, state_dim, action_dim, max_action,use_batch_norm):\n        \"\"\"\n        Initialise the Actor's Policy network.\n\n        :param state_dim: Dimension of the state space\n        :param action_dim: Dimension of the action space\n        :param max_action: Maximum value of the action\n        \"\"\"\n        super(Actor, self).__init__()\n        self.bn1 = nn.LayerNorm(HIDDEN_LAYERS_ACTOR) if use_batch_norm else nn.Identity()\n        self.bn2 = nn.LayerNorm(HIDDEN_LAYERS_ACTOR) if use_batch_norm else nn.Identity()\n\n        self.l1 = nn.Linear(state_dim, HIDDEN_LAYERS_ACTOR)\n        self.l2 = nn.Linear(HIDDEN_LAYERS_ACTOR, HIDDEN_LAYERS_ACTOR)\n        self.l3 = nn.Linear(HIDDEN_LAYERS_ACTOR, action_dim)\n        self.max_action = max_action\n\n    def forward(self, state):\n        \"\"\"\n        Forward propagation through the network.\n\n        :param state: Input state\n        :return: Action\n        \"\"\"\n\n        a = torch.relu(self.bn1(self.l1(state)))\n        a = torch.relu(self.bn2(self.l2(a)))\n        return self.max_action * torch.tanh(self.l3(a))\n\nclass Critic(nn.Module):\n    \"\"\"\n    Critic network for the DDPG algorithm.\n    \"\"\"\n    def __init__(self, state_dim, action_dim,use_batch_norm):\n        \"\"\"\n        Initialise the Critic's Value network.\n\n        :param state_dim: Dimension of the state space\n        :param action_dim: Dimension of the action space\n        \"\"\"\n        super(Critic, self).__init__()\n        self.bn1 = nn.BatchNorm1d(HIDDEN_LAYERS_CRITIC) if use_batch_norm else nn.Identity()\n        self.bn2 = nn.BatchNorm1d(HIDDEN_LAYERS_CRITIC) if use_batch_norm else nn.Identity()\n        self.l1 = nn.Linear(state_dim + action_dim, HIDDEN_LAYERS_CRITIC)\n\n        self.l2 = nn.Linear(HIDDEN_LAYERS_CRITIC, HIDDEN_LAYERS_CRITIC)\n        self.l3 = nn.Linear(HIDDEN_LAYERS_CRITIC, 1)\n\n    def forward(self, state, action):\n        \"\"\"\n        Forward propagation through the network.\n\n        :param state: Input state\n        :param action: Input action\n        :return: Q-value of state-action pair\n        \"\"\"\n        q = torch.relu(self.bn1(self.l1(torch.cat([state, action], 1))))\n        q = torch.relu(self.bn2(self.l2(q)))\n        return self.l3(q)\n```", "```py\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        return random.sample(self.buffer, batch_size)\n\n    def __len__(self):\n        return len(self.buffer)\n```", "```py\n\"\"\"\nTaken from https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n\"\"\"\nclass OUNoise(object):\n    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n        self.mu           = mu\n        self.theta        = theta\n        self.sigma        = max_sigma\n        self.max_sigma    = max_sigma\n        self.min_sigma    = min_sigma\n        self.decay_period = decay_period\n        self.action_dim   = action_space.shape[0]\n        self.low          = action_space.low\n        self.high         = action_space.high\n        self.reset()\n\n    def reset(self):\n        self.state = np.ones(self.action_dim) * self.mu\n\n    def evolve_state(self):\n        x  = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n        self.state = x + dx\n        return self.state\n\n    def get_action(self, action, t=0): \n        ou_state = self.evolve_state()\n        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n        return np.clip(action + ou_state, self.low, self.high)\n```", "```py\nclass DDPG():\n    \"\"\"\n    Deep Deterministic Policy Gradient (DDPG) agent.\n    \"\"\"\n    def __init__(self, state_dim, action_dim, max_action,use_batch_norm):\n        \"\"\"\n        Initialise the DDPG agent.\n\n        :param state_dim: Dimension of the state space\n        :param action_dim: Dimension of the action space\n        :param max_action: Maximum value of the action\n        \"\"\"\n        # [STEP 0]\n        # Initialise Actor's Policy network\n        self.actor = Actor(state_dim, action_dim, max_action,use_batch_norm)\n        # Initialise Actor target network with same weights as Actor's Policy network\n        self.actor_target = Actor(state_dim, action_dim, max_action,use_batch_norm)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n\n        # Initialise Critic's Value network\n        self.critic = Critic(state_dim, action_dim,use_batch_norm)\n        # Initialise Crtic's target network with same weights as Critic's Value network\n        self.critic_target = Critic(state_dim, action_dim,use_batch_norm)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n\n        # Initialise the Replay Buffer\n        self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n```", "```py\n def select_action(self, state):\n        \"\"\"\n        Select an action given the current state.\n\n        :param state: Current state\n        :return: Selected action\n        \"\"\"\n        state = torch.FloatTensor(state.reshape(1, -1))\n        action = self.actor(state).cpu().data.numpy().flatten()\n        return action\n```", "```py\n def train(self, use_target_network,use_batch_norm):\n        \"\"\"\n        Train the DDPG agent.\n\n        :param use_target_network: Whether to use target networks or not\n        :param use_batch_norm: Whether to use batch normalisation or not\n        \"\"\"\n        if len(self.replay_buffer) < BATCH_SIZE:\n            return\n\n        # [STEP 4]. Sample a batch from the replay buffer\n        batch = self.replay_buffer.sample(BATCH_SIZE)\n        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n\n        state = torch.FloatTensor(state)\n        action = torch.FloatTensor(action)\n        next_state = torch.FloatTensor(next_state)\n        reward = torch.FloatTensor(reward.reshape(-1, 1))\n        done = torch.FloatTensor(done.reshape(-1, 1))\n\n        # Critic Network update #\n        if use_target_network:\n            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n        else:\n            target_Q = self.critic(next_state, self.actor(next_state))\n\n        # [STEP 5]. Calculate target Q-value (y_i)\n        target_Q = reward + (1 - done) * GAMMA * target_Q\n        current_Q = self.critic(state, action)\n        critic_loss = nn.MSELoss()(current_Q, target_Q.detach())\n\n        # [STEP 6]. Use gradient descent to update weights of the Critic network \n        # to minimise loss function\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n\n        # Actor Network update #\n        actor_loss = -self.critic(state, self.actor(state)).mean()\n\n        # [STEP 7]. Use gradient descent to update weights of the Actor network\n        # to minimise loss function and maximise the Q-value => choose the action that yields the highest cumulative reward\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n\n        # [STEP 8]. Update target networks\n        if use_target_network:\n            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n\n            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)\n```", "```py\ndef train_ddpg(use_target_network, use_batch_norm, num_episodes=NUM_EPISODES):\n    \"\"\"\n    Train the DDPG agent.\n\n    :param use_target_network: Whether to use target networks\n    :param use_batch_norm: Whether to use batch normalization\n    :param num_episodes: Number of episodes to train\n    :return: List of episode rewards\n    \"\"\"\n    agent = DDPG(state_dim, action_dim, 1,use_batch_norm)\n\n    episode_rewards = []\n    noise = OUNoise(env.action_space)\n\n    for episode in range(num_episodes):\n        state= env.reset()\n        noise.reset()\n        episode_reward = 0\n        done = False\n        step=0\n        while not done:\n            action_actor = agent.select_action(state)\n            action = noise.get_action(action_actor,step) # Add noise for exploration\n            next_state, reward, done,_= env.step(action)\n            done = float(done) if isinstance(done, (bool, int)) else float(done[0])\n            agent.replay_buffer.push(state, action, reward, next_state, done)\n\n            if len(agent.replay_buffer) > BATCH_SIZE:\n                agent.train(use_target_network,use_batch_norm)\n\n            state = next_state\n            episode_reward += reward\n            step+=1\n\n        episode_rewards.append(episode_reward)\n\n        if (episode + 1) % 10 == 0:\n            print(f\"Episode {episode + 1}: Reward = {episode_reward}\")\n\n    return agent, episode_rewards\n```"]