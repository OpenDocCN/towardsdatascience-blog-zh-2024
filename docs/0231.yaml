- en: Why Retraining Can Be Harder Than Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么再训练比训练更难
- en: 原文：[https://towardsdatascience.com/why-retraining-can-be-harder-than-training-489b3bc6ae02?source=collection_archive---------11-----------------------#2024-01-23](https://towardsdatascience.com/why-retraining-can-be-harder-than-training-489b3bc6ae02?source=collection_archive---------11-----------------------#2024-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-retraining-can-be-harder-than-training-489b3bc6ae02?source=collection_archive---------11-----------------------#2024-01-23](https://towardsdatascience.com/why-retraining-can-be-harder-than-training-489b3bc6ae02?source=collection_archive---------11-----------------------#2024-01-23)
- en: A neural network perspective on learning, unlearning and relearning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从神经网络的角度看学习、去学习和再学习
- en: '[](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)[![Christian
    Koch](../Images/6daf756236838069bf79f1078b03ae6d.png)](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)
    [Christian Koch](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)[![Christian
    Koch](../Images/6daf756236838069bf79f1078b03ae6d.png)](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)
    [Christian Koch](https://medium.com/@c4ristian?source=post_page---byline--489b3bc6ae02--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)
    ·12 min read·Jan 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--489b3bc6ae02--------------------------------)
    ·12分钟阅读·2024年1月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f1f726a7f843bd800db8f9f08a90fd96.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f726a7f843bd800db8f9f08a90fd96.png)'
- en: Photo by [Mary Blackwey](https://unsplash.com/@belokonenko?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mary Blackwey](https://unsplash.com/@belokonenko?utm_source=medium&utm_medium=referral)
    提供，来源 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In a rapidly changing world, humans are required to quickly adapt to a new environment.
    Neural networks show why this is easier said than done. Our article uses a perceptron
    to demonstrate why unlearning and relearning can be costlier than learning from
    scratch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个快速变化的世界里，人类需要快速适应新的环境。神经网络展示了为什么这一点说起来容易做起来难。我们的文章通过感知机展示了为什么“去学习”和“再学习”可能比从零开始学习更具成本。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: One of the positive side effects of artificial intelligence (AI) is that it
    can help us to better understand our own human intelligence. Ironically, AI is
    also one of the technologies seriously challenging our cognitive abilities. Together
    with other innovations, it transforms modern society at a breathtaking speed.
    In his book “Think Again”, Adam Grant points out that in a volatile environment
    rethinking and unlearning may be more important than thinking and learning [1].
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）的一个积极副作用是，它可以帮助我们更好地理解我们自己的人的智能。具有讽刺意味的是，AI也是挑战我们认知能力的技术之一。与其他创新技术一起，它以惊人的速度转变现代社会。在他的书《Think
    Again》中，亚当·格兰特指出，在动荡的环境中，重新思考和去学习可能比思考和学习更为重要[1]。
- en: 'Especially for aging societies this can be a challenge. In Germany, there is
    a saying “Was Hänschen nicht lernt, lernt Hans nimmermehr.” English equivalents
    are: “A tree must be bent while it is young”, or less charmingly: “You can’t teach
    an old dog new tricks.” In essence, all these sayings suggest that younger people
    learn more easily than older persons. But is this really true, and if so, what
    are the reasons behind it?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于老龄化社会来说，这可能是一个挑战。在德国有一句话：“Was Hänschen nicht lernt, lernt Hans nimmermehr。”英语中类似的说法是：“A
    tree must be bent while it is young” 或者更不客气地说：“You can’t teach an old dog new
    tricks。”本质上，这些说法都暗示着年轻人比老年人更容易学习。但这真的正确吗？如果是，背后的原因是什么？
- en: Obviously, the brain structure of young people is different to that of older
    persons from a physiological standpoint. At an individual level, however, these
    differences vary considerably [2]. According to Creasy and Rapoport, the “overall
    functions [of the brain] can be maintained at high and effective levels“ even
    in an older age [3]. Aside from physiology, motivation and emotion seem to play
    vital roles in the learning process [4][5]. A study by Kim and Marriam at a retirement
    institution shows that cognitive interest and social interaction are strong learning
    motivators [6].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，从生理学角度来看，年轻人与年长者的大脑结构是不同的。然而，在个体层面上，这些差异变化很大[2]。根据 Creasy 和 Rapoport 的研究，"即使在年老时，大脑的整体功能也可以保持在高效的水平"[3]。除了生理学因素，动机和情感似乎在学习过程中扮演着至关重要的角色[4][5]。Kim
    和 Marriam 在一家退休机构的研究表明，认知兴趣和社会互动是强大的学习动力[6]。
- en: Our article discusses the question from the perspective of mathematics and computer
    science. Inspired by Hinton and Sejnowski [7], we conduct an experiment with an
    artificial neural network (ANN). Our test shows why retraining can be harder than
    training from scratch in a changing environment. The reason is that a network
    must first unlearn previously learned concepts before it can adapt to new training
    data. Assuming that AI has similarities with human intelligence, we can draw some
    interesting conclusions from this insight.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的文章从数学和计算机科学的角度讨论了这个问题。受到Hinton和Sejnowski [7]的启发，我们进行了一项关于人工神经网络（ANN）的实验。我们的测试表明，在变化的环境中，重新训练比从头开始训练更具挑战性。原因在于，网络必须先忘记之前学到的概念，才能适应新的训练数据。假设人工智能与人类智能有相似之处，我们可以从这一洞察中得出一些有趣的结论。
- en: Artificial neural networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Artificial neural networks resemble the structure and behavior of the nerve
    cells of our brain, known as neurons. Typically, an ANN consists of input cells
    that receive signals from the outside world. By processing these signals, the
    network is able to make a decision in response to the received input. A perceptron
    is a simple variant of an ANN [8]. It was introduced in 1958 by Rosenblatt [9].
    Figure 1 outlines the basic structure of a perceptron. In recent decades, more
    advanced types of ANNs have been developed. Yet for our experiment, a perceptron
    is well suited as it is easy to explain and interpret.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络类似于我们大脑神经元的结构和行为。通常，人工神经网络由接收外界信号的输入单元组成。通过处理这些信号，网络能够对接收到的输入做出响应。感知器是人工神经网络的一个简单变体[8]。它由
    Rosenblatt 于 1958 年提出[9]。图 1 概述了感知器的基本结构。在最近几十年里，已经开发出了更为先进的人工神经网络类型。然而，针对我们的实验，感知器非常适用，因为它容易解释和理解。
- en: '![](../Images/121e879b0940369cca18609f272d31a3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/121e879b0940369cca18609f272d31a3.png)'
- en: 'Figure 1: Structure of a single-layer perceptron. Own representation based
    on [8, p. 284].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：单层感知器的结构。基于[8，第284页]的自定义表示。
- en: Figure 1 shows the architecture of a single-layer perceptron. As input, the
    network receives *n n*umbers (*i₁..iₙ*). Together with learned weights (*w₁..wₙ*),
    the inputs are transmitted to a threshold logic unit (TLU). This TLU calculates
    a weighted sum (*z*) by multiplying the inputs (*i*) and the weights (*w*). In
    the next step, an activation function (*f*) determines the output (*o)* based
    on the weighted sum (*z*). Finally, the output (*o*) allows the network to make
    a decision as a response to the received input. Rosenblatt has shown that this
    simple form of ANN can solve a variety of problems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1 显示了单层感知器的架构。作为输入，网络接收*n n*umbers（*i₁..iₙ*）。与学习到的权重（*w₁..wₙ*）一起，这些输入被传递到一个阈值逻辑单元（TLU）。该
    TLU 通过将输入（*i*）与权重（*w*）相乘来计算加权和（*z*）。在下一步，激活函数（*f*）根据加权和（*z*）来确定输出（*o*）。最后，输出（*o*）使得网络能够做出响应接收到的输入的决策。Rosenblatt
    已证明，这种简单形式的人工神经网络能够解决多种问题。
- en: Perceptrons can use different activation functions to determine their output
    (*o*). Common functions are the *binary step function* and the *sign* function,
    presented in Figure 2\. As the name indicates, the binary function generates a
    binary output *{0,1*} that can be used to make yes/no decisions. For this purpose,
    the binary function checks whether the weighted sum (*z*) of a given input is
    less or equal to zero. If this is the case, the output (*o*) is zero, otherwise
    one. In comparison, the sign function distinguishes between three different output
    values *{-1,0,+1*}.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器可以使用不同的激活函数来决定其输出（*o*）。常见的函数有*二进制阶跃函数*和*符号*函数，如图 2 所示。顾名思义，二进制函数生成一个二进制输出*{0,1*}，可以用来做是/否决策。为此，二进制函数检查给定输入的加权和（*z*）是否小于或等于零。如果是这种情况，则输出（*o*）为零，否则为一。相比之下，符号函数区分三个不同的输出值*{-1,0,+1*}。
- en: '![](../Images/5aebab4258528b304d9599b8e9629ac2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aebab4258528b304d9599b8e9629ac2.png)'
- en: 'Figure 2: Examples of activation functions. Own representation based on [8,
    p. 285].'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：激活函数的示例。根据[8, p. 285]的内容制作。
- en: To train a perceptron based on a given dataset, we need to provide a sample
    that includes input signals (features) linked to the desired output (target).
    During the training process, an algorithm repeatedly processes the input to learn
    the best fitting weights to generate the output. The number of iterations required
    for training is a measure of the learning effort. For our experiment, we train
    a perceptron to decide whether a customer will buy a certain mobile phone. The
    source code is available on [GitHub](https://github.com/c4ristian/retrain) [10].
    For the implementation, we used Python v3.10 and scikit-learn v1.2.2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基于给定数据集训练感知器，我们需要提供一个样本，其中包括与期望输出（目标）相关联的输入信号（特征）。在训练过程中，算法反复处理输入数据，以学习最适合的权重，从而生成输出。训练所需的迭代次数是衡量学习努力的标准。对于我们的实验，我们训练一个感知器来判断客户是否会购买某款手机。源代码可在[GitHub](https://github.com/c4ristian/retrain)
    [10]上找到。我们使用的是Python v3.10和scikit-learn v1.2.2版本。
- en: Learning customer preferences
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习客户偏好
- en: Our experiment is inspired by a well-known case of (failed) relearning. Let
    us imagine we work for a mobile phone manufacturer in the year 2000\. Our goal
    is to train a perceptron that learns whether customers will buy a certain phone
    model. In 2000, touchscreens are still an immature technology. Therefore, clients
    prefer devices with a keypad instead. Moreover, customers pay attention to the
    price and opt for low-priced models compared to more expensive phones. Features
    like these made the Nokia 3310 the world’s best-selling mobile phone in 2000 [11].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验灵感来源于一个著名的（失败的）重新学习案例。假设我们在2000年为一家手机制造商工作。我们的目标是训练一个感知器，学习客户是否会购买某款手机型号。在2000年，触摸屏仍然是一项不成熟的技术。因此，客户更喜欢带有键盘的设备。此外，客户关注价格，并选择价格较低的型号，而不是更昂贵的手机。这些特征使得诺基亚3310成为2000年全球销量最高的手机[11]。
- en: '![](../Images/c7ef4fbe0879c2ab85d775e07a317441.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7ef4fbe0879c2ab85d775e07a317441.png)'
- en: 'Figure 3: Nokia 3310, Image by LucaLuca, CC BY-SA 3.0, [Wikimedia Commons](https://commons.wikimedia.org/w/index.php?curid=6349715)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：诺基亚 3310，由LucaLuca拍摄，CC BY-SA 3.0，[维基共享资源](https://commons.wikimedia.org/w/index.php?curid=6349715)
- en: For the training of the perceptron, we use the hypothetical dataset shown in
    Table 1\. Each row represents a specific phone model and the columns “keypad”,
    “touch” and “low_price” its features. For the sake of simplicity, we use binary
    variables. Whether a customer will buy a device is defined in the column “sale.”
    As described above, clients will buy phones with keypads and a low price (*keypad=1*
    and *low_price=1*). In contrast, they will reject high-priced models (*low_price=0*)
    and phones with touchscreens (*touch=1*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练感知器时，我们使用如表 1 所示的假设数据集。每一行代表一个特定的手机型号，“keypad”、“touch”和“low_price”列表示其特征。为了简化，我们使用二进制变量。客户是否会购买设备在“sale”列中定义。如上所述，客户会购买带有键盘且价格低的手机（*keypad=1*
    和 *low_price=1*）。相反，他们会拒绝高价的型号（*low_price=0*）和带触摸屏的手机（*touch=1*）。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In order to train the perceptron, we feed the above dataset several times. In
    terms of scikit-learn, we repeatedly call the function `partial_fit` (source code
    see [here](https://github.com/c4ristian/retrain/blob/master/retrain_phones.ipynb)).
    In each iteration, an algorithm tries to gradually adjust the weights of the network
    to minimize the error in predicting the variable “sale.” Figure 4 illustrates
    the training process over the first ten iterations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练感知器，我们多次输入上述数据集。从 scikit-learn 的角度来看，我们反复调用函数 `partial_fit`（源代码见[这里](https://github.com/c4ristian/retrain/blob/master/retrain_phones.ipynb)）。在每次迭代中，算法会尝试逐渐调整网络的权重，以最小化预测“sale”变量的误差。图
    4 展示了前十次迭代中的训练过程。
- en: '![](../Images/a493715c4df1a69080ffa8508fee5fcc.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a493715c4df1a69080ffa8508fee5fcc.png)'
- en: 'Figure 4: Training the phone sales perceptron with data from 2000'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用来自 2000 年的数据训练手机销售感知器
- en: As the above diagram shows, the weights of the perceptron are gradually optimized
    to fit the dataset. In the sixth iteration, the network learns the best fitting
    weights, subsequently the numbers remain stable. Figure 5 visualizes the perceptron
    after the learning process.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上述图示所示，感知器的权重逐渐被优化，以适应数据集。在第六次迭代中，网络学习到了最佳的拟合权重，随后数字保持稳定。图 5 可视化了学习过程后的感知器。
- en: '![](../Images/00ae300c640a69751c8e6eac04a1bf06.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00ae300c640a69751c8e6eac04a1bf06.png)'
- en: 'Figure 5: Phone sales perceptron trained with data from 2000'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用来自 2000 年的数据训练的手机销售感知器
- en: Let us consider some examples based on the trained perceptron. A low-priced
    phone with a keypad leads to a weighted sum of *z=-1*1–3*0+2*1=1*. Applying the
    binary step function generates the output *sale=1*. Consequently, the network
    predicts clients to buy the phone. In contrast, a high-priced device with a keypad
    leads to the weighted sum *z=-1*1–3*0+2*0=1=-1*. This time, the network predicts
    customers to reject the device. The same is true, for a phone having a touchscreen.
    (In our experiment, we ignore the case where a device has neither a keypad nor
    a touchscreen, as customers have to operate it somehow.)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们基于训练好的感知器考虑一些例子。一款带有按键的低价手机，其加权和为 *z=-1*1–3*0+2*1=1*。应用二值步进函数后，生成的输出为 *sale=1*。因此，网络预测客户会购买该手机。相反，一款带有按键的高价设备，其加权和为
    *z=-1*1–3*0+2*0=1=-1*。这一次，网络预测客户会拒绝购买该设备。对于一款带有触摸屏的手机也是如此。（在我们的实验中，我们忽略了设备既没有按键也没有触摸屏的情况，因为客户必须以某种方式操作设备。）
- en: Retraining with changed preferences
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用变化的偏好进行重新训练
- en: Let us now imagine that customer preferences have changed over time. In 2007,
    technological progress has made touchscreens much more user-friendly. As a result,
    clients now prefer touchscreens instead of keypads. Customers are also willing
    to pay higher prices as mobile phones have become status symbols. These new preferences
    are reflected in the hypothetical dataset shown in Table 2.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们设想，随着时间的推移，客户的偏好发生了变化。到 2007 年，技术进步使触摸屏变得更加易于使用。因此，客户现在更倾向于选择触摸屏而不是按键。随着手机成为身份象征，客户也愿意支付更高的价格。这些新的偏好体现在表
    2 所示的假设数据集中。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: According to Table 2, clients will buy a phone with a touchscreen (*touch=1*)
    and do not pay attention to the price. Instead, they refuse to buy devices with
    keypads. In reality, Apple entered the mobile phone market in 2007 with its iPhone.
    Providing a high-quality touchscreen, it challenged established brands. By 2014,
    the iPhone eventually became the best-selling mobile phone, pushing Nokia out
    of the market [11].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表 2，客户会购买带有触摸屏的手机（*touch=1*），并且不太关注价格。相反，他们拒绝购买带有按键的设备。实际上，苹果公司于 2007 年进入了手机市场，推出了其
    iPhone。凭借高质量的触摸屏，它挑战了传统品牌。到 2014 年，iPhone 最终成为了全球销量最高的手机，推动诺基亚退出市场[11]。
- en: '![](../Images/cc56991bfe684ce975172b5d802d2c1e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc56991bfe684ce975172b5d802d2c1e.png)'
- en: 'Figure 6: iPhone 1st generation, Carl Berkeley — CC BY-SA 2.0, [Wikimedia Commons](https://commons.wikimedia.org/w/index.php?curid=41718431)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：iPhone 第一代，Carl Berkeley — CC BY-SA 2.0，[维基共享资源](https://commons.wikimedia.org/w/index.php?curid=41718431)
- en: In order to adjust the previously trained perceptron to the new customer preferences,
    we have to retrain it with the 2007 dataset. Figure 7 illustrates the retraining
    process over the first ten iterations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将之前训练的感知器调整到新的客户偏好，我们必须使用 2007 年的数据集重新训练它。图 7 展示了在前十次迭代中的重新训练过程。
- en: '![](../Images/f01fbfe2b3051af16a4777388f5f7582.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f01fbfe2b3051af16a4777388f5f7582.png)'
- en: 'Figure 7: Retraining the phones sales perceptron with data from 2007'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用来自 2007 年的数据重新训练手机销售感知器
- en: As Figure 7 shows, the retraining requires three iterations. Then, the best
    fitting weights are found and the network has learned the new customer preferences
    of 2007\. Figure 8 illustrates the network after relearning.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 7 所示，重新训练需要三次迭代。然后，找到最佳的拟合权重，网络就学习到了 2007 年的新客户偏好。图 8 展示了重新学习后的网络。
- en: '![](../Images/b1c5ebf96aeaa5c6a4f9ef4d8e80a05a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c5ebf96aeaa5c6a4f9ef4d8e80a05a.png)'
- en: 'Figure 8: Phone sales perceptron after retraining with data from 2007'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：经过 2007 年数据重新训练后的手机销售感知机
- en: Let us consider some examples based on the retrained perceptron. A phone with
    a touchscreen (*touch=1*) and a low price (*low_price=1*) now leads to the weighted
    sum *z=-3*0+1*1+1*1=2*. Accordingly, the network predicts customers to buy a phone
    with these features. The same applies to a device having a touchscreen (*touch=1*)
    and a high price (*low_price=0*). In contrast, the network now predicts that customers
    will reject devices with keypads.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些基于重新训练的感知机的例子。一部具有触摸屏（*touch=1*）且价格低廉（*low_price=1*）的手机，现在的加权和为 *z=-3*0+1*1+1*1=2*。因此，网络预测客户会购买具有这些特征的手机。同样，对于一部具有触摸屏（*touch=1*）但价格较高（*low_price=0*）的设备，网络也做出类似预测。相反，网络现在预测客户会拒绝配有键盘的设备。
- en: From Figure 7, we can see that the retraining with the 2007 data requires three
    iterations. But what if we train a new perceptron from scratch instead? Figure
    9 compares the retraining of the old network with training a completely new perceptron
    on basis of the 2007 dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 7 中我们可以看出，使用 2007 年数据进行重新训练需要三次迭代。但是如果我们从零开始训练一个新的感知机呢？图 9 比较了使用 2007 年数据集重新训练旧网络与完全从零开始训练一个新感知机的情况。
- en: '![](../Images/0d58217634d34fc040eb2b82230d4139.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d58217634d34fc040eb2b82230d4139.png)'
- en: 'Figure 9: Retraining vs training from scratch with data from 2007'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于 2007 年数据的重新训练与从零开始训练的对比
- en: In our example, training a new perceptron from scratch is much more efficient
    than retraining the old network. According to Figure 9, training requires only
    one iteration, while retraining takes three times as many steps. Reason for this
    is that the old perceptron must first unlearn previously learned weights from
    the year 2000\. Only then is it able to adjust to the new training data from 2007\.
    Consider, for example, the weight of the feature “touch.” The old network must
    adjust it from -3 to +1\. Instead, the new perceptron can start from scratch and
    increase the weight directly from 0 to +1\. As a result, the new network learns
    faster and arrives at a slightly different setting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，从零开始训练一个新的感知机比重新训练旧网络更加高效。根据图 9，训练只需要一次迭代，而重新训练需要三倍的步骤。原因在于，旧感知机必须首先“遗忘”2000年时学习的权重，只有这样，它才能调整到2007年新的训练数据。以“触摸”特征的权重为例，旧网络必须将其从
    -3 调整到 +1。相比之下，新的感知机可以从零开始，直接将权重从 0 增加到 +1。因此，新的网络学习速度更快，并且最终会达到一个略有不同的设置。
- en: Discussion of results
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果讨论
- en: Our experiment shows from a mathematical perspective why retraining an ANN can
    be more costly than training a new network from scratch. When data has changed,
    old weights must be unlearned before new weights can be learned. If we assume
    that this also applies to the structure of the human brain, we can transfer this
    insight to some real-world problems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验从数学角度展示了为什么重新训练一个人工神经网络（ANN）可能比从零开始训练一个新网络更加昂贵。当数据发生变化时，旧的权重必须先被“遗忘”，才能学习新的权重。如果我们假设这同样适用于人类大脑的结构，那么我们可以将这一见解应用于一些实际问题。
- en: In his book “The Innovator’s Dilemma”, Christensen studies why companies that
    once were innovators in their sector failed to adapt to new technologies [12].
    He underpins his research with examples from the hard disk and the excavator market.
    In several cases, market leaders struggled to adjust to radical changes and were
    outperformed by market entrants. According to Christensen, new companies entering
    a market could adapt faster and more successfully to the transformed environment.
    As primary causes for this he identifies economic factors. Our experiment suggests
    that there may also be mathematical reasons. From an ANN perspective, market entrants
    have the advantage of learning from scratch, while established providers must
    first unlearn their traditional views. Especially in the case of disruptive innovations,
    this can be a major drawback for incumbent firms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Radical change is not only a challenge for businesses, but also for society
    as a whole. In their book “The Second Machine Age”, Brynjolfsson and McAfee point
    out that disruptive technologies can trigger painful social adjustment processes
    [13]. The authors compare the digital age of our time with the industrial revolution
    of the 18th and 19th centuries. Back then, radical innovations like the steam
    engine and electricity led to a deep transformation of society. Movements such
    as the Luddites tried to resist this evolution by force. Their struggle to adapt
    may not only be a matter of will, but also of ability. As we have seen above,
    unlearning and relearning can require a considerable effort compared to learning
    from scratch.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clearly, our experiment builds on a simplified model of reality. Biological
    neural networks are more complicated than perceptrons. The same is true for customer
    preferences in the mobile phone market. Nokia’s rise and fall has many reasons
    aside from the features included in our dataset. As we have only discussed one
    specific scenario, another interesting research question is in which cases retraining
    is actually harder than training. Authors like Hinton and Sejnowski [7] as well
    as Chen et. al [14] offer a differentiated view of the topic. Hopefully our article
    provides a starting point to these more technical publications.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledging the limitations of our work, we can draw some key lessons from
    it. When people fail to adapt to a changing environment, it is not necessarily
    due to a lack of intellect or motivation. We should keep this in mind when it
    comes to the digital transformation. Unlike digital natives, the older generation
    must first unlearn “analog” concepts. This requires effort and time. Putting too
    much pressure on them can lead to an attitude of denial, which translates into
    conspiracy theories and calls for strong leaders to stop progress. Instead, we
    should develop concepts for successful unlearning and relearning. Teaching technology
    is at least as important as developing it. Otherwise, we leave the society behind
    that we aim to support.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the authors.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
- en: About the authors
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: '*Christian Koch* is an Enterprise Lead Architect at BWI GmbH and Lecturer at
    the Nuremberg Institute of Technology Georg Simon Ohm.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Christian Koch* 是BWI GmbH的企业首席架构师，并且是纽伦堡理工学院乔治·西蒙·欧姆学院的讲师。'
- en: '*Markus Stadi* is a Senior Cloud Data Engineer at Dehn SE working in the field
    of Data Engineering, Data Science and Data Analytics for many years.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*Markus Stadi* 是Dehn SE的高级云数据工程师，多年来一直在数据工程、数据科学和数据分析领域工作。'
- en: References
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Grant, A. (2023). *Think again: The power of knowing what you don’t know*.
    Penguin.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Grant, A. (2023). *再思考：了解你不知道的事情的力量*. Penguin。
- en: 'Reuter-Lorenz, P. A., & Lustig, C. (2005). Brain aging: reorganizing discoveries
    about the aging mind. *Current opinion in neurobiology*, *15*(2), 245–251.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Reuter-Lorenz, P. A., & Lustig, C. (2005). 脑部衰老：重组关于衰老心智的发现。*神经生物学当前意见*, *15*(2),
    245–251.
- en: 'Creasey, H., & Rapoport, S. I. (1985). The aging human brain. *Annals of Neurology:
    Official Journal of the American Neurological Association and the Child Neurology
    Society*, *17*(1), 2–10.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Creasey, H., & Rapoport, S. I. (1985). 衰老的人脑。*神经学年鉴：美国神经学会与儿童神经学学会的官方期刊*, *17*(1),
    2–10.
- en: Welford AT. Motivation, Capacity, Learning and Age. *The International Journal
    of Aging and Human Development*. 1976;7(3):189–199.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Welford AT. Motivation, Capacity, Learning and Age. *国际老龄化与人类发展期刊*. 1976;7(3):189–199.
- en: Carstensen, L. L., Mikels, J. A., & Mather, M. (2006). Aging and the intersection
    of cognition, motivation, and emotion. In *Handbook of the psychology of aging*
    (pp. 343–362). Academic Press.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Carstensen, L. L., Mikels, J. A., & Mather, M. (2006). 衰老与认知、动机、情感的交集。见 *老年心理学手册*
    (第343–362页)。学术出版社。
- en: Kim, A., & Merriam, S. B. (2004). Motivations for learning among older adults
    in a learning in retirement institute. *Educational gerontology*, *30*(6), 441–455.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kim, A., & Merriam, S. B. (2004). 退休学习机构中老年人学习动机。*教育老年学*, *30*(6), 441–455.
- en: 'Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Boltzmann
    machines. *Parallel distributed processing: Explorations in the microstructure
    of cognition*, *1*(282–317), 2.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hinton, G. E., & Sejnowski, T. J. (1986). 学习与重学在玻尔兹曼机中的应用。*并行分布式处理：认知微观结构的探索*,
    *1*(282–317), 2.
- en: Géron, A. (2022). *Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow*.
    O’Reilly Media, Inc.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Géron, A. (2022). *动手学习机器学习：使用Scikit-Learn、Keras和TensorFlow*. O'Reilly Media,
    Inc.
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, *65*(6), 386.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *心理学评论*, *65*(6), 386.'
- en: 'Koch, C. (2024). Retrain Python Project. URL: [https://github.com/c4ristian/retrain](https://github.com/c4ristian/retrain).
    Accessed 11 January 2024.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Koch, C. (2024). Retrain Python项目。网址：[https://github.com/c4ristian/retrain](https://github.com/c4ristian/retrain)。访问时间：2024年1月11日。
- en: 'Wikipedia. List of best-selling mobile phones. URL: [https://en.wikipedia.org/wiki/List_of_best-selling_mobile_phones](https://en.wikipedia.org/wiki/List_of_best-selling_mobile_phones).
    Accessed 11 January 2024.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wikipedia. 热销手机列表。网址：[https://en.wikipedia.org/wiki/List_of_best-selling_mobile_phones](https://en.wikipedia.org/wiki/List_of_best-selling_mobile_phones)。访问时间：2024年1月11日。
- en: 'Christensen, C. M. (2013). *The innovator’s dilemma: when new technologies
    cause great firms to fail*. Harvard Business Review Press.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Christensen, C. M. (2013). *创新者的窘境：当新技术导致大公司失败时*. 哈佛商业评论出版社。
- en: 'Brynjolfsson, E., & McAfee, A. (2014). *The second machine age: Work, progress,
    and prosperity in a time of brilliant technologies*. WW Norton & Company.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brynjolfsson, E., & McAfee, A. (2014). *第二次机器时代：在辉煌技术时代的工作、进步与繁荣*. WW Norton
    & Company。
- en: Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., & Zhang, Y. (2022, November).
    Graph unlearning. In *Proceedings of the 2022 ACM SIGSAC Conference on Computer
    and Communications Security* (pp. 499–513).
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, M., Zhang, Z., Wang, T., Backes, M., Humbert, M., & Zhang, Y. (2022, November).
    图形反学习。见 *2022年ACM SIGSAC计算机与通信安全大会论文集* (第499–513页)。
