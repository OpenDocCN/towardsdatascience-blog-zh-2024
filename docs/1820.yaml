- en: AI Models Have an Expiry Date — Continual Learning May Be an Answer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI模型有过期日期——持续学习可能是答案
- en: 原文：[https://towardsdatascience.com/ai-models-have-expiry-date-9a6e2c9c0a9f?source=collection_archive---------5-----------------------#2024-07-26](https://towardsdatascience.com/ai-models-have-expiry-date-9a6e2c9c0a9f?source=collection_archive---------5-----------------------#2024-07-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-models-have-expiry-date-9a6e2c9c0a9f?source=collection_archive---------5-----------------------#2024-07-26](https://towardsdatascience.com/ai-models-have-expiry-date-9a6e2c9c0a9f?source=collection_archive---------5-----------------------#2024-07-26)
- en: Why, in a world where the only constant is change, we need a **Continual Learning**
    approach to AI models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么在这个唯一不变的就是变化的世界里，我们需要一种**持续学习**的方法来处理AI模型。
- en: '[](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[![Alicja
    Dobrzeniecka](../Images/b731eb2bb8fde56e84273af8050b59e4.png)](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)
    [Alicja Dobrzeniecka](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[![Alicja
    Dobrzeniecka](../Images/b731eb2bb8fde56e84273af8050b59e4.png)](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)
    [Alicja Dobrzeniecka](https://alicjadobrzeniecka.medium.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)
    ·7 min read·Jul 26, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9a6e2c9c0a9f--------------------------------)
    ·阅读时长7分钟·2024年7月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a30f4c8693a444a23e937f1d298ffc5c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a30f4c8693a444a23e937f1d298ffc5c.png)'
- en: Image by the author generated in Midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者在Midjourney生成
- en: Imagine you have a small robot that is designed to walk around your garden and
    water your plants. Initially, you spend a few weeks collecting data to train and
    test the robot, investing considerable time and resources. The robot learns to
    navigate the garden efficiently when the ground is covered with grass and bare
    soil.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你有一个小型机器人，它被设计用来在你的花园里走动并浇水。最初，你花费了几周的时间收集数据来训练和测试机器人，投入了相当多的时间和资源。当花园的地面覆盖着草和裸土时，机器人学会了高效地导航。
- en: However, as the weeks go by, flowers begin to bloom and the appearance of the
    garden changes significantly. The robot, trained on data from a different season,
    now fails to recognise its surroundings accurately and struggles to complete its
    tasks. To fix this, you need to add new examples of the blooming garden to the
    model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，花朵开始盛开，花园的外观发生了显著变化。这个根据不同季节的数据训练出来的机器人，现在无法准确识别周围的环境，并且难以完成任务。为了修复这个问题，你需要将盛开的花园的新例子添加到模型中。
- en: Your first thought is to add new data examples to the training and retrain the
    model from scratch. But this is expensive and you do not want to do this every
    time the environment changes. In addition, you have just realised that you do
    not have all the historical training data available.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你最初的想法是向训练中添加新的数据样本，并重新从头开始训练模型。但这是昂贵的，而且你不想每次环境变化时都这样做。此外，你刚刚意识到，你没有所有历史训练数据可用。
- en: Now you consider just fine-tuning the model with new samples. But this is risky
    because the model may lose some of its previously learned capabilities, leading
    to **catastrophic forgetting** (a situation where the model loses previously acquired
    knowledge and skills when it learns new information).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你考虑只用新样本微调模型。但这样做是有风险的，因为模型可能会失去之前学到的一些能力，从而导致**灾难性遗忘**（即模型在学习新信息时，失去之前获得的知识和技能）。
- en: ..so is there an alternative? Yes, using Continual Learning!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ..那么有没有替代方法呢？有的，使用持续学习！
- en: Of course, the robot watering plants in a garden is only an illustrative example
    of the problem. In the later parts of the text you will see more realistic applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，机器人在花园里浇水只是问题的一个示例。在接下来的部分，你将看到更现实的应用场景。
- en: Learn adaptively with Continual Learning (CL)
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用持续学习（CL）进行适应性学习
- en: It is not possible to foresee and prepare for all the possible scenarios that
    a model may be confronted with in the future. Therefore, in many cases, adaptive
    training of the model as new samples arrive can be a good option.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无法预见并为模型未来可能遇到的所有场景做好准备。因此，在许多情况下，随着新样本的到来，对模型进行自适应训练可能是一个不错的选择。
- en: In CL we want to find a balance between the **stability** of a model and its
    **plasticity**. Stability is the ability of a model to retain previously learned
    information, and plasticity is its ability to adapt to new information as new
    tasks are introduced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在持续学习（CL）中，我们希望找到模型**稳定性**与**可塑性**之间的平衡。稳定性是指模型保持之前学习的信息的能力，而可塑性是指模型随着新任务的引入能够适应新信息的能力。
- en: '*“(…) in the Continual Learning scenario, a learning model is required to incrementally
    build and dynamically update internal representations as the distribution of tasks
    dynamically changes across its lifetime*.” [2]'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“(…) 在持续学习场景中，需要一个学习模型能够随着任务分布在其生命周期中动态变化，逐步建立和动态更新内部表示*。” [2]'
- en: But how to control for the stability and plasticity?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，如何控制稳定性与可塑性呢？
- en: 'Researchers have identified a number of ways to build adaptive models. In [3]
    the following categories have been established:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经确定了多种构建自适应模型的方法。在[3]中，已建立了以下几类方法：
- en: '**Regularisation-based approach**'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于正则化的方法**'
- en: In this approach we add a regularisation term that should balance the effects
    of old and new tasks on the model structure.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种方法中，我们添加一个正则化项，用以平衡新任务与旧任务对模型结构的影响。
- en: '**For example, weight regularisation** aims to control the variation of the
    parameters, by adding a penalty term to the loss function, which penalises the
    change of the parameter by taking into account how much it contributed to the
    previous tasks.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例如，权重正则化**旨在通过向损失函数添加一个惩罚项来控制参数的变化，该惩罚项会根据参数对之前任务的贡献程度惩罚其变化。'
- en: '**2\. Replay-based approach**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 基于重放的方法**'
- en: This group of methods focuses on recovering some of the historical data so that
    the model can still reliably solve previous tasks. One of the limitations of this
    approach is that we need access to historical data, which is not always possible.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一组方法的重点是恢复一些历史数据，使得模型仍然能够可靠地解决以前的任务。该方法的一个局限性是我们需要访问历史数据，而这并非总是可能的。
- en: '**For example, experience replay**, where we preserve and replay a sample of
    old training data. When training a new task, some examples from previous tasks
    are added to expose the model to a mixture of old and new task types, thereby
    limiting catastrophic forgetting.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例如，经验重放**，即我们保存并重放一部分旧的训练数据。在训练新任务时，添加一些来自旧任务的样本，以将模型暴露于旧任务与新任务类型的混合，从而限制灾难性遗忘。'
- en: '**3\. Optimisation based approach**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**3. 基于优化的方法**'
- en: Here we want to manipulate the optimisation methods to maintain performance
    for all tasks, while reducing the effects of catastrophic forgetting.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们希望调整优化方法，以保持所有任务的性能，同时减少灾难性遗忘的影响。
- en: '**For example, gradient projection** is a method where gradients computed for
    new tasks are projected so as not to affect previous gradients.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例如，梯度投影**是一种方法，其中计算的新任务的梯度会被投影，以避免影响先前的梯度。'
- en: '**4\. Representation-based approach**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**4. 基于表示的方法**'
- en: This group of methods focuses on obtaining and using robust feature representations
    to avoid catastrophic forgetting.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一组方法的重点是获得并使用强健的特征表示，以避免灾难性遗忘。
- en: '**For example, self-supervised learning**, where a model can learn a robust
    representation of the data before being trained on specific tasks. The idea is
    to learn high-quality features that reflect good generalisation across different
    tasks that a model may encounter in the future.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例如，自监督学习**，即模型可以在专门训练特定任务之前，学习数据的强健表示。其理念是学习能够反映良好泛化能力的高质量特征，以应对模型未来可能遇到的不同任务。'
- en: '**5\. Architecture-based approach**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**5. 基于架构的方法**'
- en: The previous methods assume a single model with a single parameter space, but
    there are also a number of techniques in CL that exploit model’s architecture.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前的方法假设使用单一模型和单一参数空间，但在持续学习中，也有许多技术可以利用模型的架构。
- en: '**For example, parameter allocation**, where, during training, each new task
    is given a dedicated subspace in a network, which removes the problem of parameter
    destructive interference. However, if the network is not fixed, its size will
    grow with the number of new tasks.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**例如，参数分配，** 在训练过程中，每个新任务都会在网络中分配一个专用的子空间，这解决了参数冲突干扰的问题。然而，如果网络不是固定的，其规模会随着新任务数量的增加而增长。'
- en: And how to evaluate the performance of the CL models?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，如何评估CL模型的性能呢？
- en: 'The basic performance of CL models can be measured from a number of angles
    [3]:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CL模型的基本性能可以从多个角度来衡量[3]：
- en: '**Overall performance evaluation:** average performance across all tasks'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总体性能评估：** 所有任务的平均性能'
- en: '**Memory stability evaluation:** calculating the difference between maximum
    performance for a given task before and its current performance after continual
    training'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆稳定性评估：** 计算给定任务在持续训练前的最大性能与当前性能之间的差异'
- en: '**Learning plasticity evaluation:** measuring the difference between joint
    training performance (if trained on all data) and performance when trained using
    CL'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习可塑性评估：** 衡量联合训练性能（如果在所有数据上训练）与使用CL训练时的性能差异'
- en: So why don’t all AI researchers switch to Continual Learning right away?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，为什么并不是所有AI研究人员都立刻转向持续学习呢？
- en: If you have access to the historical training data and are not worried about
    the computational cost, it may seem easier to just train from scratch.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你可以访问历史训练数据并且不担心计算成本，那么从头开始训练似乎更容易。
- en: One of the reasons for this is that the interpretability of what happens in
    the model during continual training is still limited. If training from scratch
    gives the same or better results than continual training, then people may prefer
    the easier approach, i.e. retraining from scratch, rather than spending time trying
    to understand the performance problems of CL methods.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个原因是，模型在持续训练过程中发生的事情的可解释性仍然有限。如果从头开始训练能够得到与持续训练相同或更好的结果，那么人们可能更倾向于选择更简单的方法，即从头开始重新训练，而不是花时间去理解CL方法的性能问题。
- en: In addition, current research tends to focus on the evaluation of models and
    frameworks, which may not reflect well the real use cases that the business may
    have. As mentioned in [6], there are many synthetic incremental benchmarks that
    do not reflect well real-world situations where there is a natural evolution of
    tasks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当前的研究往往集中在模型和框架的评估上，这可能无法很好地反映出商业可能面临的实际使用案例。正如[6]中所提到的，有许多合成增量基准测试并没有很好地反映出现实世界中任务自然演变的情况。
- en: Finally, as noted in [4], many papers on the topic of CL focus on storage rather
    than computational costs, and in reality, storing historical data is much less
    costly and energy consuming than retraining the model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如在[4]中所指出的，许多关于CL的论文关注的是存储问题而非计算成本，实际上，存储历史数据比重新训练模型要便宜且耗能更少。
- en: If there were more focus on the inclusion of computational and environmental
    costs in model retraining, more people might be interested in improving the current
    state of the art in CL methods as they would see measurable benefits. For example,
    as mentioned in [4], model re-training can exceed **10 000 GPU days** of training
    for recent large models.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果更多地关注模型重新训练中计算和环境成本的包含，可能会有更多的人对改进当前的CL方法感兴趣，因为他们会看到可衡量的收益。例如，正如[4]中提到的，模型重新训练可能超过**10,000
    GPU天**的训练时间，尤其是在最近的大型模型中。
- en: Why should we work on improving CL models?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要致力于改进CL模型？
- en: Continual learning seeks to address one of the most challenging bottlenecks
    of current AI models — the fact that data distribution changes over time. Retraining
    is expensive and requires large amounts of computation, which is not a very sustainable
    approach from both an economic and environmental perspective. Therefore, in the
    future, well-developed CL methods may allow for models that are more accessible
    and reusable by a larger community of people.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 持续学习旨在解决当前AI模型最具挑战性的瓶颈之一——数据分布随时间变化的事实。重新训练成本高，且需要大量计算，这在经济和环境角度来看并不是一种可持续的方法。因此，在未来，发展完善的CL方法可能使得模型变得更加可接入且可重用，供更广泛的社区使用。
- en: 'As found and summarised in [4], there is a list of applications that inherently
    require or could benefit from the well-developed CL methods:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[4]中发现并总结的那样，有一系列应用天生需要或能够从完善的CL方法中受益：
- en: '**Model Editing**'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型编辑**'
- en: Selective editing of an error-prone part of a model without damaging other parts
    of the model. Continual Learning techniques could help to continuously correct
    model errors at much lower computational cost.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不破坏模型其他部分的情况下，选择性地编辑模型中易出错的部分。持续学习技术可以帮助我们在较低的计算成本下持续纠正模型错误。
- en: '**2\. Personalisation and specialisation**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 个性化与专业化**'
- en: General purpose models sometimes need to be adapted to be more personalised
    for specific users. With Continual Learning, we could update only a small set
    of parameters without introducing catastrophic forgetting into the model.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用模型有时需要进行调整，以便更好地个性化满足特定用户的需求。通过持续学习，我们可以只更新一小部分参数，而不会引入灾难性遗忘。
- en: '**3\. On-device learning**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 设备端学习**'
- en: Small devices have limited memory and computational resources, so methods that
    can efficiently train the model in real time as new data arrives, without having
    to start from scratch, could be useful in this area.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小型设备具有有限的内存和计算资源，因此能够在新数据到达时有效地实时训练模型，而无需从头开始的方法，在这一领域可能会非常有用。
- en: '**4\. Faster retraining with warm start**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 使用热启动进行更快速的再训练**'
- en: Models need to be updated when new samples become available or when the distribution
    shifts significantly. With Continual Learning, this process can be made more efficient
    by updating only the parts affected by new samples, rather than retraining from
    scratch.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当新的样本可用或分布发生显著变化时，模型需要进行更新。通过持续学习，这一过程可以更高效地进行，只更新受新样本影响的部分，而不是从头开始重新训练。
- en: '**5\. Reinforcement learning**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 强化学习**'
- en: Reinforcement learning involves agents interacting with an environment that
    is often non-stationary. Therefore, efficient Continual Learning methods and approaches
    could be potentially useful for this use case.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习涉及智能体与通常是非静态的环境进行交互。因此，效率较高的持续学习方法和策略可能对这种使用场景有很大帮助。
- en: Learn more
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解更多
- en: 'As you can see, there is still **a lot of room for improvement in the area
    of Continual Learning methods**. If you are interested you can start with the
    materials below:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，**持续学习方法领域仍然有很大的提升空间**。如果你感兴趣，可以从以下材料开始：
- en: 'Introduction course: *[Continual Learning Course] Lecture #1: Introduction
    and Motivation* from ContinualAI on YouTube [https://youtu.be/z9DDg2CJjeE?si=j57_qLNmpRWcmXtP](https://youtu.be/z9DDg2CJjeE?si=j57_qLNmpRWcmXtP)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入门课程：*【持续学习课程】第1讲：介绍与动机* 来自ContinualAI在YouTube上的视频 [https://youtu.be/z9DDg2CJjeE?si=j57_qLNmpRWcmXtP](https://youtu.be/z9DDg2CJjeE?si=j57_qLNmpRWcmXtP)
- en: 'Paper about the motivation for the Continual Learning: *Continual Learning:
    Application and the Road Forward* [4]'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讲述持续学习动机的论文：*持续学习：应用与前进的道路* [4]
- en: 'Paper about the state of the art techniques in Continual Learning: *Comprehensive
    Survey of Continual Learning: Theory, Method and Application* [](https://arxiv.org/pdf/2302.00487)
    [3]'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讲述持续学习最新技术的论文：*持续学习的综合调查：理论、方法与应用* [](https://arxiv.org/pdf/2302.00487) [3]
- en: '**If you have any questions or comments, please feel free to share them in
    the comments section.**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有任何问题或评论，请随时在评论区分享。**'
- en: Cheers!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯！
- en: '![](../Images/e1e87ef455f4ecbab117a49cda5e832d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1e87ef455f4ecbab117a49cda5e832d.png)'
- en: Image by the author generated in Midjourney
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者在Midjourney生成
- en: References
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Awasthi, A., & Sarawagi, S. (2019). *Continual Learning with Neural Networks:
    A Review*. In Proceedings of the ACM India Joint International Conference on Data
    Science and Management of Data (pp. 362–365). Association for Computing Machinery.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Awasthi, A., & Sarawagi, S. (2019). *使用神经网络的持续学习：综述*。发表于ACM印度联合国际数据科学与数据管理会议论文集（第362-365页）。计算机协会。'
- en: '[2] Continual AI Wiki *Introduction to Continual Learning* [https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning](https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Continual AI Wiki *持续学习简介* [https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning](https://wiki.continualai.org/the-continualai-wiki/introduction-to-continual-learning)'
- en: '[3] Wang, L., Zhang, X., Su, H., & Zhu, J. (2024). A Comprehensive Survey of
    Continual Learning: Theory, Method and Application*. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, 46(8), 5362–5383.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wang, L., Zhang, X., Su, H., & Zhu, J. (2024). **持续学习的综合调查：理论、方法与应用**。IEEE模式分析与机器智能学报,
    46(8), 5362–5383。'
- en: '[4] Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu,
    Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha
    Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu,
    Andreas S. Tolias, Joost van de Weĳer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars,
    & Gido M. van de Ven. (2024). *Continual Learning: Applications and the Road Forward*
    [https://arxiv.org/abs/2311.11908](https://arxiv.org/abs/2311.11908)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Eli Verwimp, Rahaf Aljundi, Shai Ben-David, Matthias Bethge, Andrea Cossu,
    Alexander Gepperth, Tyler L. Hayes, Eyke Hüllermeier, Christopher Kanan, Dhireesha
    Kudithipudi, Christoph H. Lampert, Martin Mundt, Razvan Pascanu, Adrian Popescu,
    Andreas S. Tolias, Joost van de Weĳer, Bing Liu, Vincenzo Lomonaco, Tinne Tuytelaars,
    & Gido M. van de Ven. (2024). *持续学习：应用与未来发展之路* [https://arxiv.org/abs/2311.11908](https://arxiv.org/abs/2311.11908)'
- en: '[5] Awasthi, A., & Sarawagi, S. (2019). Continual Learning with Neural Networks:
    A Review. In *Proceedings of the ACM India Joint International Conference on Data
    Science and Management of Data* (pp. 362–365). Association for Computing Machinery.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Awasthi, A., & Sarawagi, S. (2019). 神经网络的持续学习：综述。收录于 *ACM印度联合国际数据科学与数据管理会议论文集*（第362–365页）。计算机协会。'
- en: '[6] Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli,
    Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, & Fartash Faghri. (2024). TiC-CLIP:
    Continual Training of CLIP Models.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli,
    Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, & Fartash Faghri. (2024). TiC-CLIP：CLIP模型的持续训练。'
