- en: Track Your ML Experiments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/track-your-ml-experiments-9d2ea8cbeb02?source=collection_archive---------8-----------------------#2024-03-29](https://towardsdatascience.com/track-your-ml-experiments-9d2ea8cbeb02?source=collection_archive---------8-----------------------#2024-03-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A guide to Neptune.ai for tracking your experiments in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pelletierhaden?source=post_page---byline--9d2ea8cbeb02--------------------------------)[![Haden
    Pelletier](../Images/8f73fc8222e783883c4ebcaee14513e0.png)](https://medium.com/@pelletierhaden?source=post_page---byline--9d2ea8cbeb02--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d2ea8cbeb02--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d2ea8cbeb02--------------------------------)
    [Haden Pelletier](https://medium.com/@pelletierhaden?source=post_page---byline--9d2ea8cbeb02--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d2ea8cbeb02--------------------------------)
    ·7 min read·Mar 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c33444e15f12e4cf5a5edd7d2b09982.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Alex Kondratiev](https://unsplash.com/@alexkondratiev?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Every data scientist is familiar with experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: You know the drill. You get a dataset, load it into a Jupyter notebook, explore
    it, preprocess the data, fit a baseline model or two, and then train an initial
    final model, such as XGBoost. The first time around, maybe you don’t tune the
    hyperparameters and include 20 features. Then, you check your error metrics.
  prefs: []
  type: TYPE_NORMAL
- en: They look okay, but perhaps your model is overfitting a bit. So you decide to
    tune some regularization parameters (eg max depth) to reduce the complexity of
    the model and run it again.
  prefs: []
  type: TYPE_NORMAL
- en: 'You see a little improvement from the last run, but perhaps you want to also:'
  prefs: []
  type: TYPE_NORMAL
- en: Add more features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform feature selection and remove some features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try a different scaler for your features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune different/more hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the different kinds of tests you want to run increases, the more difficult
    it is to remember which combinations of your “experiments” actually yielded the
    best results. You can only run a notebook so many times, print out the results,
    and copy/paste them to a Google doc before you get frustrated.
  prefs: []
  type: TYPE_NORMAL
