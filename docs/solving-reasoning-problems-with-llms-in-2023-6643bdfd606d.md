# 解决 2023 年大语言模型推理问题

> 原文：[https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06](https://towardsdatascience.com/solving-reasoning-problems-with-llms-in-2023-6643bdfd606d?source=collection_archive---------1-----------------------#2024-01-06)

[](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[![Zhaocheng Zhu](../Images/80d09cfe902ca99c97fd6cfd6e387c2f.png)](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------) [Zhaocheng Zhu](https://medium.com/@kiddozhu?source=post_page---byline--6643bdfd606d--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6643bdfd606d--------------------------------) ·阅读时间17分钟·2024年1月6日

--

现在是2024年初，ChatGPT刚刚庆祝了一周年。对于大语言模型社区来说，一年是非常漫长的时间，这一年里发生了无数有趣的工作。让我们回顾一下这一年的进展，并讨论未来一年的话题。

![](../Images/d47fb72b435abfef88d39b07d7a27d8c.png)

智能体学院：大语言模型正在从教科书中提取知识并进行推理。图像由作者和DALL·E 3生成。

*这篇文章由* [*Michael Galkin*](https://twitter.com/michael_galkin) *(英特尔AI实验室)，* [*Abulhair Saparov*](https://asaparov.org/) *(纽约大学)，* [*Shibo Hao*](https://twitter.com/Ber18791531) *(加州大学圣地亚哥分校) 和* [*Yihong Chen*](https://twitter.com/yihong_thu) *(伦敦大学学院与Meta AI研究) 合著。文章中的许多见解是在与* [*Emily Xue*](https://www.linkedin.com/in/yuan-emily-xue-3483012/) *(谷歌)，* [*Hanjun Dai*](https://twitter.com/hanjundai) *(谷歌DeepMind) 和* [*Bruno Ribeiro*](https://twitter.com/brunofmr) *(普渡大学) 的富有成果的讨论中形成的。*

# 目录

1.  [简介](#2669)

1.  [工具使用](#0b5d)

    1\. [上下文学习使得使用更多工具成为可能](#1acd)

    2\. [最常用的工具：代码解释器和检索工具](#89ee)

    3\. [让大语言模型创建自己的工具](#9108)

1.  [推理](#d8e6)

    1\. [规划](#d2e7)

    2\. [自我系列](#3582)

    3\. [评估与观察](#0f22)

1.  [2024年需要解决的问题](#7e91)

# 简介

🔥 大语言模型（LLMs）必定是2023年最热门的话题。在上个月的NeurIPS会议上，社交活动中反复讨论的话题是：1）我们正在做什么关于LLMs的研究？ 2）我的研究如何与LLMs结合？ 3）从XXX转向LLMs的最佳策略是什么？ 4）作为一个[GPU不足](https://www.semianalysis.com/p/google-gemini-eats-the-world-gemini)的团队，我们能做什么研究？原因是每个人都通过X、Discord、Slack和其他地方获得了LLMs的突破性新闻。

如果你查看arXiv上的语言模型论文，2023年的数量从2,837篇跃升至11,033篇，这打破了2019到2022年间的线性趋势。过去一年中的论文大致可以分为三大类：1️⃣ 预训练和对齐；2️⃣ 工具使用和推理；3️⃣ 系统和服务。正如标题所示，**本文将重点介绍LLM研究在工具使用和推理方面的进展。** 我们挑选了大约20篇 👀 令人震惊的 👀 论文，并总结了它们的见解和影响。这篇文章绝不是对社区所有成就的全面总结。如果我们遗漏了任何话题，请随时评论。

![](../Images/2b76e17e1ac1e676a1b1776c4af794e4.png)

作者和ChatGPT绘制的图表。

本文由两个主题组成：**工具使用**和**推理**。

+   工具使用更多的是关于如何通过为大语言模型（LLMs）配备**外部**工具来解决推理问题，例如检索器、搜索引擎和代码解释器。虽然工具使用对于构建强大的人工智能并非必需（见下文Yann的分类），但当领域特定工具易于获取时，工具使用为许多应用提供了实际的解决方案。

+   相比之下，推理则集中在利用LLM的**内部**推理能力来解决复杂问题。推理研究试图找出LLMs所具备能力的极限，并探索突破这一极限的方法。

这两者之间并没有严格的二分法，正如我们在本文接下来的部分中将看到的那样。

Yann LeCun对检索与推理的分类。

# 工具使用

## 上下文学习使得使用更多工具成为可能

**➡️** LLM工具使用的一个限制是需要足够的人类标注。每当我们想教一个LLM使用工具时，我们需要足够的标注工具调用来对LLM进行微调。在Meta的[Toolformer](https://arxiv.org/abs/2302.04761)论文中，作者使用上下文学习创建了一个模型，能够为输入查询标注工具调用。然后，使用该模型在未标记的数据集上生成工具调用。虽然生成的调用可能远非完美，但可以通过执行工具并根据真实答案过滤输出，筛选出错误的调用。正确的调用被收集并用于微调模型。通过这种方式，我们可以基于常规数据集和仅仅5个额外标注来教会Transformer使用任何工具——这是任何工程师都能轻松完成的工作。

![](../Images/2413879f08a9fefbd39a955d6044af62.png)

工具调用的自动注释。来源：[Schick et al.](https://arxiv.org/abs/2302.04761)

➡️ [Lu et al.](https://arxiv.org/abs/2304.09842) 提出了**Chameleon** 🦎，用于组合多步推理的工具。其核心思想是使用大型语言模型（LLM）将问题分解为一系列工具调用，然后为每个工具调用生成参数。这两个步骤都是通过少量示例提示来实现的。这个想法让人联想到2016年的[神经模块网络（NMNs）](https://arxiv.org/abs/1511.02799)，它将问题分解为子任务，并为每个子任务学习一个模块。NMNs的主要障碍是，它们在没有分解注释的情况下很难训练（见[这项研究](https://arxiv.org/abs/1811.12889)）。幸运的是，在预训练的LLM中，这不是问题。通过上下文学习，Chameleon可以生成不同的工具调用组合来解决问题。[一种类似的视觉推理方法](https://arxiv.org/abs/2211.11559)在今年的CVPR上获得了最佳论文奖。

![](../Images/be14f7112ad6ccee215eb810ecf2b688.png)

Chameleon用于多步工具使用。来源：[Lu et al.](https://arxiv.org/abs/2304.09842)

➡️ 尽管与传统方法相比，上下文学习具有较高的效率，但它也面临某些限制，例如管理大量工具的难度。为了解决这个问题，[Hao et al.](https://arxiv.org/abs/2305.11554) 提出了**ToolkenGPT**，它通过为工具引入新的令牌嵌入（称为“toolkens”）来增强一个冻结的LLM。该技术最初在[多语言模型](https://arxiv.org/abs/1910.11856)中用于适应新的语言。ToolkenGPT允许在推理过程中像下一个词预测一样进行工具调用。它展示了处理超过200个工具的能力，同时具有成本效益，相较于LoRA微调，建立了一种新的效能与效率的平衡。类似的思路也已整合到多模态LLM中，用于[机器人动作](https://arxiv.org/abs/2307.15818)和[图像生成](https://arxiv.org/abs/2309.11499)。

![](../Images/52ecf8ab7d5d6708483fb644e9601483.png)

ToolkenGPT用于大规模工具使用。来源：[Hao et al.](https://arxiv.org/abs/2305.11554)

## 最常用的工具：代码解释器和检索器

如果你问我们哪些工具最广泛适用于推理任务，我们会说它们是**代码解释器**和**检索器**。代码解释器可能是人类发明的最具表现力的逻辑与计算环境。检索器在LLM的参数化知识无法覆盖某些问题或已知假设的情况下，是一种很好的补充。让我们来看看这些工具是如何被LLM使用的。

➡️ [思维链（CoT）](https://arxiv.org/abs/2201.11903)的一个常见失败是LLM无法执行算术运算。在[程序辅助语言建模（PAL）](https://arxiv.org/abs/2211.10435)和[思维程序（PoT）](https://arxiv.org/abs/2211.12588)的提示中，作者通过程序提示代码语言模型来解决数学问题。可以在程序中插入标准的思维链文本作为注释。最终的答案是通过执行Python解释器生成的。这些方法背后的洞见是，代码解释器为各种计算提供了完美的工具，将失败案例减少到仅仅是推理错误。代码风格的提示也常用于[规划任务](https://arxiv.org/abs/2305.16653)中。

![](../Images/dd4f4150c2e2a642854d8791b55c4a09.png)

CoT和PAL的比较。来源：[Gao和Madaan等](https://arxiv.org/abs/2211.10435)

➡️ 检索器通常作为LLM的预处理工具，用于通过相关文档增强问题，通常称为[检索增强生成（RAG）](https://arxiv.org/abs/2005.11401)。然而，当涉及到多步骤问题解答时，仅凭问题本身就很难选择正确的文档。在[Trivedi et al.](https://arxiv.org/abs/2212.10509)提出的**IRCoT**中，作者将思维生成与知识检索交替进行。每当LLM生成一个思维句子时，IRCoT就使用该句子从语料库中检索文档。这些文档被添加到提示中，以增强后续生成。即使是像[BM25](https://en.wikipedia.org/wiki/Okapi_BM25)这样较弱的检索器，IRCoT也在多个开放领域的问答基准测试中超越了一步RAG。

![](../Images/0e047cefb88b0636fb931ccbe6ca4277.png)

交替使用CoT和知识检索的IRCoT。来源：[Trivedi等](https://arxiv.org/abs/2212.10509)

➡️ [Yang等](https://arxiv.org/abs/2306.15626)提出了一种RAG在定理证明中的新颖应用。他们基于证明助手Lean构建了一个类似健身房的环境**LeanDojo** 🏯。[Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant))是一个互动编程环境，其中的编译器可以验证所写的证明是否证明了目标。它还包含了许多在标准库中已证明的定理，类似于C++中的STL。很酷的一点是，由于证明是通过将定理分解为已知前提构造的，因此定理证明可以从RAG中受益。给定一个定理，我们从标准库中检索相关的前提，然后请求LLM生成一个证明步骤。作者表明，RAG需要的训练资源要少得多，并且在新颖的前提下具有更好的泛化能力。

![](../Images/c56af9f2fb5c435c163c807a4fa4ffd3.png)

在Lean中证明一个简单的逻辑定理。来源：[Xena项目](https://www.youtube.com/watch?v=POHVMMG7pqE)

➡️ 最后，[DSPy](https://github.com/stanfordnlp/dspy)由[Khattab et al.](https://arxiv.org/abs/2310.03714)提出了一种新的LLM编程方法，在该框架下，系统能够随着时间的推移自动优化提示，并结合提示技术（CoT, PoT）与检索。进一步地，DSPy引入了*提词器*来优化提示并引导新的提示生成。很难用一段话来描述DSPy——它不是普通的RAG技术，而是其进化版本。

## 让LLM创建自己的工具

工具使用有一个固有的限制：它依赖于特定任务所需工具的存在。在自然界中，工具使用并非人类的专属技能，许多其他动物也能够使用工具。然而，区别人类与其他动物的，是**创造**工具的能力。2023年，我们看到了一些初步的研究，探索了LLM中工具制作的能力。

➡️ 在由[Cai et al.](https://arxiv.org/abs/2305.17126)提出的LLM作为工具制造者（**LATM**）中，作者提示LLM为给定任务编写Python函数形式的工具。这些工具随后在一些样本上进行验证，类似于工程师在LeetCode上解决问题的方式。一旦某些工具通过验证测试，它们会被LLM生成的文档字符串包装，以描述其使用方法。在测试时，LLM会被提示将问题发送到手头的某个工具，并根据其使用方法执行该工具。LATM在BigBench的广泛推理任务中，显著优于CoT。

➡️ [Voyager](https://arxiv.org/abs/2305.16291)将工具制作的概念引入了Minecraft的世界，并取得了惊人的结果。Voyager的核心思想是利用LLM根据现有的技能和世界状态提出任务。接着，LLM被提示合成代码（即技能）来解决这些任务。技能基于环境反馈进行精炼，掌握的技能被保存到外部记忆中。由于新技能是在现有技能的基础上构建的，这大大降低了学习复杂技能的难度（例如，在Minecraft中制作钻石工具）。尽管学习技能库的概念可以追溯到[DreamCoder](https://arxiv.org/abs/2006.08381)，Voyager展示了GPT-4在挑战性开放世界游戏中搜索技能的优势。请查看[论文中的精彩演示](https://voyager.minedojo.org/)！

![](../Images/3dc3a1462663cecf5c056fab3a88b4d8.png)

随着时间的推移，Minecraft中的物品和技能被发现。来源：[Wang et al.](https://arxiv.org/abs/2305.16291)

➡️ 上述两篇工作都将工具设计为代码。事实上，工具也可以是自然语言。（不嫌弃的自我推销）在 [Zhu et al.](https://arxiv.org/abs/2310.07064) 的假设到理论 (**HtT**) 工作中，作者展示了我们可以利用 LLM 从标准的多步骤推理训练集诱导出一套文本规则库。其洞察是，在 LLM 为不同样本生成的所有规则中，发生频率较高并且导致正确答案的规则可能是正确的。然后，我们收集这些规则并将其附加到标准 CoT 提示中以进行推理并得出答案。HtT 的一个有趣方面是，它可以被看作是一种新的学习方式：我们不是学习模型参数，而是学习一套规则库，这与黑盒 LLM 配合得非常好。

![](../Images/e30b407b2550861646906f5e0ad43916.png)

HtT 学习多跳推理的文本规则。来源：[Zhu et al.](https://arxiv.org/abs/2310.07064)

# 推理

## 规划

CoT 风格推理的一个缺点是，LLM 必须贪婪地解码一个通向答案的路径。对于复杂的问题，如数学题或游戏，这就成了一个问题，因为没有试错就很难预测出一条路径。2023 年，社区在这个问题上取得了一些进展，推出了新的框架，使得 LLM 可以进行规划。

➡️ 如果我们将 CoT 概念化为“系统 1”推理——其特点是自动的、无意识的特性——那么就会出现一个问题：是否可以用 LLM 来复制更具意识的“系统 2”推理？这个问题在两种方法中得到了回应：[推理通过规划 (RAP)](https://arxiv.org/abs/2305.14992) 和 [思维树 (ToT)](https://arxiv.org/abs/2305.10601)。这两者都使 LLM 能够通过可能的推理步骤进行推理，并根据特定的评估搜索最优推理链。RAP 还将 LLM 提示为一个“世界模型”，它预测在行动后的下一状态。这使得 LLM 可以在一个自我模拟的世界中操作，而不是与外部环境互动。现在这两个算法都可以在 [LLM Reasoners](https://github.com/Ber666/llm-reasoners/) 库中找到！

![](../Images/76d337122e2af90fdd867f1c7ab0e5a8.png)

RAP 将 LLM 重新设计为一个代理和世界模型。来源：[Hao et al.](https://arxiv.org/abs/2305.14992)

## Self 系列

Self 系列是一类技术，通过在 LLM 开发过程中用 LLM 预测替代人工努力。2023 年见证了这一领域的若干重要论文。让我们深入了解一些具有代表性的工作。

➡️ 许多人都有过ChatGPT在第一次尝试时未能提供期望输出的经历，而这有时可以通过指出其错误来修复。[自我调试](https://arxiv.org/abs/2304.05128)和[自我完善](https://arxiv.org/abs/2303.17651)通过用机器反馈替代人类反馈来自动化这个过程。反馈来自于程序执行器或一个LLM，它比较生成的内容与问题的解释。一项关键的观察是，自我完善的表现取决于反馈的质量，提供更好反馈的强大基础模型能带来更大的帮助。这类迭代完善方法在[姿态估计](https://arxiv.org/abs/1507.06550)和[蛋白质结构预测](https://www.nature.com/articles/s41586-021-03819-2)中也被证明非常有效，因为在单次运行中很难预测结构。

![](../Images/2b3c4d3580bf9a786395a972b481534c.png)

自我调试的示意图。来源：[Chen et al.](https://arxiv.org/abs/2304.05128)

➡️ 在[Li and Qiu](https://arxiv.org/abs/2305.05181)提出的记忆-思维（**MoT**）框架中，作者要求LLM在未标注数据集上生成CoT推理，并将其用于RAG。你可能会问，考虑到生成的推理通常包含错误，这如何有用呢？关键的技巧是根据多数投票或熵最小化来过滤推理（[Wan et al.](https://arxiv.org/abs/2305.14106)中也使用了类似的思想来过滤推理）。一旦我们在未标注数据集上获得了良好的推理，我们会基于测试问题动态地检索少样本示例，这被证明比固定的少样本示例要好得多。MoT可以解释为将一个参数化模型转化为一个非参数化模型，而无需额外的监督。

![](../Images/07dcece1803feb415c68d2727bc2c037.png)

生成和回忆记忆的MoT。来源：[Li and Qiu](https://arxiv.org/abs/2305.05181)。

➡️ 超越MoT，[Yasunaga et al.](https://arxiv.org/abs/2310.01714)提出了**类比提示**，它消除了在未标注数据集上倾倒推理的需求。类比提示要求LLM根据问题回忆相关的示例，从而从头生成动态的少样本示例。事实上，作者发现类比提示是大语言模型的一个涌现能力，类似于之前在[开放领域问答](https://arxiv.org/abs/2209.10063)中的研究。更大规模的LLM可以自我生成比标准RAG解决方案更好的示例。此外，这项工作为将多步生成合并为一个单一提示提供了一个很酷的技巧，使用Markdown语法——这对于预算紧张的提示工程师来说是一个天赐之物！💡

![](../Images/cff6c1216d04791bb9d00343530b5700.png)

类比提示。来源：[Yasunaga et al.](https://arxiv.org/abs/2310.01714)

➡️ 自我优化和自我生成是LLM推理的极限吗？[Yang et al.](https://arxiv.org/abs/2309.03409)展示了LLM推理能力的更高级应用——基于生成的提示历史优化提示。这是对著名元学习论文《[通过梯度下降学习学习](https://arxiv.org/abs/1606.04474)》的一个酷炫重塑，但这里的所有步骤都是由LLM在文本上执行的。在每个步骤中，LLM都会接收之前的解决方案和相应的性能指标提示，并尝试预测一个新的解决方案。值得注意的是，即使没有告诉LLM如何进行优化，LLM也能逐步找到最大化指标的更好解决方案。也许这项工作让提示工程师离失业更近了一步？

![](../Images/2c7ff40222823804440ab08816082b7d.png)

由LLM优化的提示性能。来源：[Yang et al.](https://arxiv.org/abs/2309.03409)

🔁 可能是自学系列中最具启发性的👀工作是由[Zelikman et al.](https://arxiv.org/abs/2310.02304)提出的自我优化器(**STOP**) 。我们知道LLM是通过文本提示来引导的，接受文本作为输入并输出文本。虽然这些文本通常是分开的变量，但如果我们将它们建模为单一变量，会发生什么呢？在STOP中，作者从[自我修改代码](https://en.wikipedia.org/wiki/Self-modifying_code)中汲取灵感，使用自我改进的提示来提升自身。

![](../Images/3da91a00d4241277814ce2cfc36e8dda.png)

在STOP中改进自身的种子改进器。来源：[Zelikman et al.](https://arxiv.org/abs/2310.02304)

虽然种子提示并不比随机搜索算法更复杂，但有了强大的LLM，人们可以发现许多先进的元启发式算法。有趣的是，GPT-4发现了许多在其训练截止日期之后发布的提示策略，包括[ToT](https://arxiv.org/abs/2305.10601)和[Parsel](https://arxiv.org/abs/2212.10561)。看起来，LLM为自己进行研究的那一天即将到来。朝着这个方向迈出的一步是[Huang et al.](https://arxiv.org/abs/2310.03302)的最新研究，表明LLM能够为常见基准测试甚至Kaggle挑战设计机器学习模型。

![](../Images/e030d2aa8360e5a443b744b6852437af.png)

STOP发现的算法。来源：[Zelikman et al.](https://arxiv.org/abs/2310.02304)

## 评估与观察

➡️ [Kandpal等人](https://arxiv.org/abs/2211.08411)对LLMs的记忆能力进行了系统研究。他们向LLM提出了来自维基百科的事实性问题，并发现准确性与预训练文档中被提问实体的频率高度相关，无论模型的规模如何。通过推测这一趋势，作者估计，需要一个拥有10¹⁸参数的模型才能在长尾实体上与人类表现相匹配——这远远超出了当前的LLMs。因此，一个重要的启示是，对于与频繁知识相关的任务，应使用LLM推理，而对于与长尾知识相关的任务，可以考虑使用RAG或其他工具。

![](../Images/d7500b22b9573287416b537dcd301dcb.png)

LLMs几乎无法记住长尾知识。来源：[Kandpal等人](https://arxiv.org/abs/2211.08411)

➡️ 随着社区尝试构建更大规模的混合数据集来训练LLMs，一个担忧是LLMs可能无法真正学习推理，而只是简单地记住训练分布中的解决方案，就像人类在[应试教育](https://en.wikipedia.org/wiki/Teaching_to_the_test)中一样。[Wu等人](https://arxiv.org/abs/2307.02477)通过比较GPT-4在11个不同任务上的零样本CoT表现来回应这一担忧，每个任务都有默认设置和反事实设置。他们观察到，尽管LLMs在反事实设置中表现优于随机结果，但其表现始终低于默认设置。如何训练模型更多地关注推理而非记忆，仍然是一个悬而未决的问题。

![](../Images/03f7be28616d30d4236ececb57c2aff3.png)

GPT-4在反事实变体上的表现不佳。来源：[Wu等人](https://arxiv.org/abs/2307.02477)

➡️ [Saparov等人](https://arxiv.org/abs/2305.15269)将一个合成数据集[PrOntoQA](https://arxiv.org/abs/2210.01240)扩展到OOD设置，以测试LLMs在控制深度、宽度、组合结构等方面的推理泛化能力。作者发现，CoT能够推广到组合性和更长的证明。这与之前关于[组合语义解析](https://arxiv.org/abs/2205.12253)的结论相反，可能是因为推理只需要组合推理步骤，而语义解析还需要处理不断增长的输出。虽然LLMs能够使用大多数推理规则，但它们需要显式演示[*分情况证明*](https://en.wikipedia.org/wiki/Disjunction_elimination)和[*反证法证明*](https://en.wikipedia.org/wiki/Proof_by_contradiction)。在上下文学习和监督学习之间也存在一些违反直觉的定性差异。

![](../Images/37163127677f52ac47a0d2999eb0ebd6.png)

关于推理的OOD泛化能力。来源：[Saparov等人](https://arxiv.org/abs/2305.15269)

➡️ 关于LLMs中的参数化知识，[Berglund等人](https://arxiv.org/abs/2309.12288)发现了一种他们称之为*逆转诅咒*的现象。也就是说，在闭卷问答中，被训练记住“ A是B”的LLMs并不知道“B是A”，尽管它们可以被提示进行演绎推理。这表明LLMs在其参数化知识中缺乏某些对称性，而赋予它们这种对称性对于更好的泛化至关重要。实际上，知识图谱领域一直是这一领域的领先者，像[双重排列等变性](https://arxiv.org/abs/2302.01313)和[关系旋转](https://arxiv.org/abs/1902.10197)等工作便是例证。看看这些想法如何适应LLMs将会很有趣。

# 2024年需要解决什么问题？

2023年对于工具使用和推理来说是令人兴奋的一年，我们预计新的一年将更加精彩。让我们通过作者的预测来总结这篇文章。

> Zhaocheng Zhu:

1️⃣ 使用LLMs进行推理仍然需要针对每个特定任务进行临时的工程努力。相比之下，一旦人类掌握了某项任务的技能，他们可以迅速将这些技能适应到类似的任务上，几乎不需要样本，甚至不需要样本（例如，从国际象棋到扑克牌）。如果我们能够创建跨任务泛化的LLM解决方案，这将节省大量的工程努力，并提升在低资源领域的表现。

2️⃣ 解决推理问题通常涉及大量的常识性知识，从数学、物理到像枚举法和反证法这样的策略，如果有的话。虽然LLMs可能从其训练数据中获得了这些知识，但我们对LLMs中参数化知识的精确控制仍然缺乏。我们希望看到关于LLMs知识表示的新研究，以及可以在LLMs中表达、注入或删除知识的技术。

> Michael Galkin:

1️⃣ 在2023年，我们看到对*Transformer基础的LLMs可以学习什么*基本原理的理解力度不断加大——我们是否真的可以期望LLMs能够解决任何任意的推理任务？一些著名的论文，比如[信仰与命运](https://arxiv.org/abs/2305.18654)和[关于长度泛化](https://arxiv.org/abs/2310.16028)，表明LLMs的自回归特性可能不是处理复杂推理的最佳方式。在2024年，我预计会有更多的努力理解LLMs与算法的对齐。

2️⃣ 很可能在2024年，大多数开放和闭合的基础模型将是多模态的，支持视觉、文本、音频等多种输入。将其他模态融入推理是自然的下一步。

> Abulhair Saparov:

1️⃣ 我预期会有更多的努力去寻找LLMs推理机制的更深层次理解。在执行推理任务时，它们使用什么算法？更准确地说，它们在多大程度上利用了会影响鲁棒性/泛化能力的捷径或启发式方法？

2️⃣ 与此相关，我预计研究人员将会取得进展，回答一个问题：是否增加 LLM 的规模和/或其训练将解决其在推理方面的局限性，或者这些局限性是否是根本性的，例如架构固有的问题。

> Shibo Hao:

1️⃣ 在过去的一年里，LLM 推理研究的主要焦点集中在提示和监督微调上，像 [STaR](https://arxiv.org/abs/2203.14465)、[Reflexion](https://arxiv.org/abs/2303.11366) 和 [RAP](https://arxiv.org/abs/2305.14992) 等方法，已经开始从强化学习（RL）中汲取灵感。然而，我们尚未见证一种突破性的方法，能够有效地利用强化学习提升 LLM 的推理能力，特别是与 RLHF 在对齐方面的进展相比。

2️⃣ 另一方面，未来语言可能成为强化学习系统中的主要表达媒介。其关键优势在于与传统的标量奖励/价值相比，语言蕴含着丰富的信息。一个能够通过强化学习自主提高推理能力的 LLM 代理（无需监督数据或提示工程），不仅令人兴奋，还可能意味着朝着通用人工智能（AGI）迈出了重要一步。

> Yihong Chen：

1️⃣ 结构化与非结构化。我认为，大型语言模型（LLMs）将逐渐吞噬传统产品的市场份额，而这些传统产品大多依赖于大型数据库、规则和大量小型分类器。在这种情况下，我们所说的“LLM 推理”可能指的是我们期望能够有一种方法“X”，它可以弥合结构化世界（目前大多数产品数据所处的世界）与非结构化世界（大多数 LLM 所处的世界）之间的鸿沟。知识图谱在某种程度上代表了结构化世界，关于如何在知识图谱上进行有效推理的研究也很多，而 LLM 则代表了非结构化世界，尽管我们仍不清楚它们是如何进行推理的。它们各有优劣和局限性。我预计，结构化世界与非结构化世界之间的一个有效桥梁将能为产品提供更具务实性的解决方案。

2️⃣ 样本效率。正如 Zhaocheng 所提到的，目前 LLM 的推理在跨大量任务的泛化方面存在困难。我对是否能够简单地预训练一个能够用更少数据泛化的 LLM 感兴趣，类似于 [跨多种语言的泛化](https://arxiv.org/abs/2307.01163)所做的那样。

3️⃣ LLM 内部的推理。正如 Abulhair 和 Michael 所提到的，社区目前尚未清楚了解 LLM 是如何进行推理的，甚至是否真正在进行推理。我预计，未来会有更多的努力，去逆向工程 LLM 的推理过程，可能是通过 [机械性方法](https://www.lesswrong.com/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide) 或其他可解释性方法。

# Meme Time

延续[Michael Galkin](https://mgalkin.medium.com/)的传统，任何博客文章都不算完整，没有一个表情包。DALL·E 3简直是表情包魔法师，如果它能正确拼写单词的话。猜猜每个面板上用的是什么提示？

![](../Images/1c0de490cd0427c4127e390af9d93917.png)

一个LLM学到了什么，它能推理什么。图片来源：作者与DALL·E 3。

# 阅读更多

如果这篇博客让你想了解更多关于LLM推理的内容，看看以下这些精彩的博客文章。

+   [面向复杂推理：大语言模型的北极星](https://www.notion.so/c2b4a51355b44764975f88e6a42d4e75?pvs=21) 作者：姚福。

+   [LLM驱动的自主智能体](https://lilianweng.github.io/posts/2023-06-23-agent/) 作者：Lilian Weng。
