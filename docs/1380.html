<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>PRISM-Rules in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>PRISM-Rules in Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/prism-rules-in-python-14d2cfd801a3?source=collection_archive---------3-----------------------#2024-06-02">https://towardsdatascience.com/prism-rules-in-python-14d2cfd801a3?source=collection_archive---------3-----------------------#2024-06-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ec9a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A simple python rules-induction system</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--14d2cfd801a3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--14d2cfd801a3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--14d2cfd801a3--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--14d2cfd801a3--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="li k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lj an ao ap ig lk ll lm" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ln cn"><div class="l ae"><div class="ab cb"><div class="lo lp lq lr ls lt ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lj an ao ap ig lu lv lg lw lx ly lz ma s mb mc md me mf mg mh u mi mj mk"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="2213" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This article is part of a series covering interpretable predictive models. Previous articles covered <a class="af nh" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a> and <a class="af nh" href="https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223" rel="noopener">Additive Decision Trees</a>. PRISM is an existing algorithm (though I did create a python implementation), and the focus in this series is on original algorithms, but I felt it was useful enough to warrant it’s own article as well. Although it is an old idea, I’ve found it to be competitive with most other interpretable models for classification and have used it quite a number of times.</p><p id="dc5d" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">PRISM is relatively simple, but in machine learning, sometimes the most complicated solutions work best and sometimes the simplest. Where we wish for interpretable models, though, there is a strong benefit to simplicity.</p><p id="20c4" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">PRISM is a rules-induction tool. That is, it creates a set of rules to predict the target feature from the other features.</p><p id="e027" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Rules have at least a couple very important purposes in machine learning. One is prediction. Similar to Decision Trees, linear regression, GAMs, <a class="af nh" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a>, <a class="af nh" href="https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223" rel="noopener">Additive Decision Trees</a>, Decision Tables, and small number of other tools, they can provide interpretable classification models.</p><p id="93ad" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Rules can also be used simply as a technique to understand the data. In fact, even without labels, they can be used in an unsupervised manner, creating a set of rules to predict each feature from the others (treating each feature in the table in turns as a target column), which can highlight any strong patterns in the data.</p><p id="eee2" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">There are other tools for creating rules in python, including the very strong <a class="af nh" href="https://github.com/csinva/imodels" rel="noopener ugc nofollow" target="_blank">imodels library</a>. However, it can still be challenging to create a set of rules that are both accurate and comprehensible. Often rules induction systems are unable to create reasonably accurate models, or, if they are able, only by creating many rules and rules with many terms. For example, rules such as:</p><p id="b7df" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">IF color=’blue’ AND height &lt; 3.4 AND width &gt; 3.2 AND length &gt; 33.21 AND temperature &gt; 33.2 AND temperature &lt; 44.2 AND width &lt; 5.1 AND weight &gt; 554.0 AND … THEN…</p><p id="83c0" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Where rules have more than about five or ten terms, they can become difficult to follow. Given enough terms, rules can eventually become effectively uninterpretable. And where the set of rules includes more than a moderate number of rules, the rule set as a whole becomes difficult to follow (more so if each rule has many terms).</p><h1 id="0e6e" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">PRISM Rules</h1><p id="6939" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">PRISM is a rules-induction system first proposed by Chendrowska <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules#references" rel="noopener ugc nofollow" target="_blank">[1]</a> <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules#references" rel="noopener ugc nofollow" target="_blank">[2]</a> and described in Principles of Data Mining <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules#references" rel="noopener ugc nofollow" target="_blank">[3]</a>.</p><p id="b9fd" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">I was unable to find a python implementation and so created one. The main page for PRISM rules is: <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules" rel="noopener ugc nofollow" target="_blank">https://github.com/Brett-Kennedy/PRISM-Rules</a>.</p><p id="44f2" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">PRISM supports generating rules both as a descriptive model: to describe patterns within a table (in the form of associations between the features); and as a predictive model. It very often produces a very concise, clean set of interpretable rules.</p><p id="64c2" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">As a predictive model, it provides both what are called <em class="oj">global </em>and <em class="oj">local </em>explanations (in the terminology used in Explainable AI (XAI)). That is, it is fully-interpretable and allows both understanding the model as a whole and the individual predictions.</p><p id="89f0" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Testing multiple rules-induction systems, I very often find PRISM produces the cleanest set of rules. Though, no one system works consistently the best, and it’s usually necessary to try a few rules induction tools.</p><p id="1a5a" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The rules produced are in disjunctive normal form (an OR of ANDs), with each individual rule being the AND of one or more terms, with each term of the form Feature = Value, for some Value within the set of values for that feature. For example: the rules produced may be of the form:</p><p id="c833" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Rules for target value: ‘blue’:</p><ul class=""><li id="e3c7" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng ok ol om bk">IF feat_A = ‘hot’ AND feat_C = ‘round’ THEN ‘blue’</li><li id="028c" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">IF feat_A = ‘warm’ AND feat_C = ‘square’ THEN ‘blue’</li></ul><p id="67a9" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Rules for target value: ‘red’:</p><ul class=""><li id="bc97" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng ok ol om bk">IF feat_A = ‘cold’ AND feat_C = ‘triangular’ THEN ‘red’</li><li id="6388" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">IF feat_A = ‘cool’ AND feat_C = ‘triangular’ THEN ‘red’</li></ul><p id="fecb" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The algorithm works strictly with categorical features, in both the X and Y columns. This implementation will, therefore, automatically bin any numeric columns to support the algorithm. By default, three equal-count bins (representing low, medium, and high values for the feature) are used, but this is configurable through the nbins parameter (more or less bins may be used).</p><h2 id="f53b" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">PRISM Algorithm</h2><p id="6bd0" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">For this section, we assume we are using PRISM as a predictive model, specifically as a classifier.</p><p id="d47e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The algorithm works by creating a set of rules for each class in the target column. For example, if executing on the Iris dataset, where there are three values in the target column (Setosa, Versicolour, and Virginica), there would be a set of rules related to Setosa, a set related to Versicolour, and a set related to Virginica.</p><p id="63d8" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The generated rules should be read in a first-rule-to-fire manner, and so all rules are generated and presented in a sensible order (from most to least relevant for each target class). For example, examining the set of rules related to Setosa, we would have a set of rules that predict when an iris is Setosa, and these would be ordered from most to least predictive. Similarly for the sets of rules for the other two classes.</p><h2 id="8fd2" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">Generating the Rules</h2><p id="f6f2" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">We’ll describe here the algorithm PRISM uses to generate a set of rules for one class. With the Iris dataset, lets say we’re about to generate the rules for the Setosa class.</p><p id="41ff" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">To start, PRISM finds the best rule available to predict that target value. This first rule for Setosa would predict as many of the Setosa records as possible. That is, we find the unique set of values in some subset of the other features that best predicts when a record will be Setosa. This is the first rule for Setosa.</p><p id="d9f1" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The first rule will, however, not cover all Setosa records, so we create additional rules to cover the remaining rows for Setosa (or as many as we can).</p><p id="b2f3" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">As each rule is discovered, the rows matching that rule are removed, and the next rule is found to best describe the remaining rows for that target value.</p><p id="7a6e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The rules may each have any number of terms.</p><p id="ac6f" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">For each other value in the target column, we start again with the full dataset, removing rows as rules are discovered, and generating additional rules to explain the remaining rows for this target class value. So, after finding the rules for Setosa, PRISM would generate the rules for Versicolour, and then for Virginica.</p><h2 id="417e" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">Coverage and Support</h2><p id="b7cf" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">This implementation enhances the algorithm as described in Principles of Data Mining by outputting statistics related to each rule, as many induced rules can be much more relevant or, the opposite, of substantially lower significance than other rules induced.</p><p id="ba3c" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">As well, tracking simple statistics about each rule allows providing parameters to specify the minimum <em class="oj">coverage </em>for each rule (the minimum number of rows in the training data for which it applies); and the minimum <em class="oj">support</em> (the minimum probability of the target class matching the desired value for rows matching the rule). These help reduce noise (extra rules that add only small value to the descriptive or predictive power of the model), though can result in some target classes having few or no rules, potentially not covering all rows for one or more target column values. In these cases, users may wish to adjust these parameters.</p><h1 id="3ff7" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Comparison to Decision Trees</h1><p id="acbb" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">Decision trees are among the most common interpretable models, quite possibly the most common. When sufficiently small, they can be reasonably interpretable, perhaps as interpretable as any model type, and they can be reasonably accurate for many problems (though certainly not all). They do, though, have limitations as interpretable models, which PRISM was designed to address.</p><p id="374b" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Decision trees were not specifically designed to be interpretable; it is a convenient property of decision trees that they are as interpretable as they are. They, for example, often grow much larger than is easily comprehensible, often with repeated sub-trees as relationships to features have to be repeated many times within the trees to be properly captured.</p><p id="dc17" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">As well, the decision paths for individual predictions may include nodes that are irrelevant, or even misleading, to the final predictions, further reducing compressibility.</p><p id="5239" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The Cendrowska paper provides examples of simple sets of rules that cannot be represented easily by trees. For example:</p><ul class=""><li id="f896" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng ok ol om bk">Rule 1: IF a = 1 AND b = 1 THEN class = 1</li><li id="5188" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">Rule 2: IF c = 1 AND d = 1 THEN class = 1</li></ul><p id="ba8b" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">These lead to a surprisingly complex tree. In fact, this is a common pattern that results in overly-complex decision trees: “where there are two (underlying) rules with no attribute in common, a situation that is likely to occur frequently in practice” <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules#references" rel="noopener ugc nofollow" target="_blank">[3]</a></p><p id="03ae" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Rules can often generate more interpretable models than can decision trees (though the opposite is also often true) and are useful to try with any project where interpretable models are beneficial. And, where the goal is not building a predictive model, but understanding the data, using multiple models may be advantageous to capture different elements of the data.</p><h2 id="b068" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">Installation</h2><p id="818a" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">The project consists of a single <a class="af nh" href="https://github.com/Brett-Kennedy/PRISM-Rules/blob/main/prism_rules.py" rel="noopener ugc nofollow" target="_blank">python file</a> which may be downloaded and included in any project using:</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="5a7d" class="ps nj fq pp b bg pt pu l pv pw">from prism_rules import PrismRules</span></pre><h1 id="60bb" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Example using the Wine dataset from sklearn</h1><p id="e6db" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">The github page provides two example notebooks that provide simple, but thorough examples of using the tool. The tool is, though, quite straight-forward. To use the tool to generate rules, simply create a PrismRules object and call get_prism_rules() with a dataset, specifying the target column:</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="4d38" class="ps nj fq pp b bg pt pu l pv pw">import pandas as pd<br/>from sklearn.datasets import load_wine<br/><br/>data = datasets.load_wine()<br/>df = pd.DataFrame(data.data, columns=data.feature_names)<br/>df['Y'] = data['target']<br/>display(df.head())<br/><br/>prism = PrismRules()<br/>_ = prism.get_prism_rules(df, 'Y')</span></pre><h2 id="68c4" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">Results</h2><p id="c91f" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">This dataset has three values in the target column, so will generate three sets of rules:</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="6ca4" class="ps nj fq pp b bg pt pu l pv pw">................................................................<br/>Target: 0<br/>................................................................<br/><br/>proline = High AND alcohol = High<br/>   Support:  the target has value: '0' for 100.000% of the 39 rows matching the rule <br/>   Coverage: the rule matches: 39 out of 59 rows for target value: 0. This is:<br/>      66.102% of total rows for target value: 0<br/>      21.910% of total rows in data<br/><br/>proline = High AND alcalinity_of_ash = Low<br/>   Support:  The target has value: '0' for 100.000% of the 10 remaining rows matching the rule <br/>   Coverage: The rule matches: 10 out of 20 rows remaining for target value: '0'. This is:<br/>      50.000% of remaining rows for target value: '0'<br/>      16.949% of total rows for target value: 0<br/>      5.618% of total rows in data0<br/><br/><br/>................................................................<br/>Target: 1<br/>................................................................<br/>color_intensity = Low AND alcohol = Low<br/>   Support:  the target has value: '1' for 100.000% of the 46 rows matching the rule <br/>   Coverage: the rule matches: 46 out of 71 rows for target value: 1. This is:<br/>      64.789% of total rows for target value: 1<br/>      25.843% of total rows in data<br/><br/>color_intensity = Low<br/>   Support:  The target has value: '1' for 78.571% of the 11 remaining rows matching the rule <br/>   Coverage: The rule matches: 11 out of 25 rows remaining for target value: '1'. This is:<br/>      44.000% of remaining rows for target value: '1'<br/>      15.493% of total rows for target value: 1<br/>      6.180% of total rows in data<br/><br/>................................................................<br/>Target: 2<br/>................................................................<br/>flavanoids = Low AND color_intensity = Med<br/>   Support:  the target has value: '2' for 100.000% of the 16 rows matching the rule <br/>   Coverage: the rule matches: 16 out of 48 rows for target value: 2. This is:<br/>      33.333% of total rows for target value: 2<br/>      8.989% of total rows in data<br/><br/>flavanoids = Low AND alcohol = High<br/>   Support:  The target has value: '2' for 100.000% of the 10 remaining rows matching the rule <br/>   Coverage: The rule matches: 10 out of 32 rows remaining for target value: '2'. This is:<br/>      31.250% of remaining rows for target value: '2'<br/>      20.833% of total rows for target value: 2<br/>      5.618% of total rows in data<br/><br/>flavanoids = Low AND color_intensity = High AND hue = Low<br/>   Support:  The target has value: '2' for 100.000% of the 21 remaining rows matching the rule <br/>   Coverage: The rule matches: 21 out of 22 rows remaining for target value: '2'. This is:<br/>      95.455% of remaining rows for target value: '2'<br/>      43.750% of total rows for target value: 2<br/>      11.798% of total rows in data</span></pre><p id="b343" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">For each rule, we see both the support and the coverage.</p><p id="ccb3" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The support indicates how many rows support the rule; that is: of the rows where the rule can be applied, in how many is it true. The first rule here is:</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="8bf6" class="ps nj fq pp b bg pt pu l pv pw">proline = High AND alcohol = High<br/>   Support:  the target has value: '0' for 100.000% of the 39 rows matching the rule</span></pre><p id="0ae1" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This indicates that of the 39 rows where proline = High (the feature proline has a high numeric value) and alcohol is High (the features alcohol has a high numeric value), for 100% of them, the target it 0.</p><p id="8290" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">The coverage indicates how many rows the rule covers. For the first rule, this is:</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="efeb" class="ps nj fq pp b bg pt pu l pv pw">Coverage: the rule matches: 39 out of 59 rows for target value: 0. This is:<br/>      66.102% of total rows for target value: 0<br/>      21.910% of total rows in data</span></pre><p id="4b6e" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">This indicates the coverage both in terms of row count and as a percent of the rows in the data.</p><h1 id="32f3" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Example Generating Predictions</h1><p id="1eb3" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">To create predictions, we simply call predict() passing a dataframe with the same features as the dataframe used to fit the model (though the target column may optionally be omitted, as in this example).</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="793c" class="ps nj fq pp b bg pt pu l pv pw">y_pred = prism.predict(df.drop(columns=['Y']))</span></pre><p id="34e3" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">In this way, PRISM rules may be used equivalently to any other predictive model, such as Decision Trees, Random Forests, XGBoost, and so on.</p><p id="f6f7" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">However, while generating predictions, some rows may match no rules. In this case, by default, the most common value in the target column during training (which can be seen accessing prism.default_target) will be used. The predict() method also supports a parameter, leave_unknown. If this is set to True, then any records not matching any rules will be set to “NO PREDICTION”. In this case, the predictions will be returned as a string type, even if the original target column was numeric.</p><p id="8910" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Further examples are provided in the sample notebooks.</p><h1 id="372c" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Example with Numeric Data</h1><p id="e877" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">In this example, we use sklearn’s make_classification() method to create numeric data (other than the target column), which is then binned. This uses the default of three bins per numeric feature.</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="83f5" class="ps nj fq pp b bg pt pu l pv pw">x, y = make_classification(<br/>    n_samples=1000, <br/>    n_features=20,    <br/>    n_informative=2,<br/>    n_redundant=2,<br/>    n_repeated=0,<br/>    n_classes=2,<br/>    n_clusters_per_class=1,<br/>    class_sep=2,<br/>    flip_y=0, <br/>    random_state=0<br/>    )<br/><br/>df = pd.DataFrame(x)<br/>df['Y'] = y<br/>prism = PrismRules()<br/>_ = prism.get_prism_rules(df, 'Y')</span></pre><h2 id="4e5a" class="os nj fq bf nk ot ou ov nn ow ox oy nq mu oz pa pb my pc pd pe nc pf pg ph pi bk">Results</h2><p id="e712" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">The data is binned into low, medium, and high values for each column. The results are a set of rules per target class.</p><pre class="pj pk pl pm pn po pp pq bp pr bb bk"><span id="5014" class="ps nj fq pp b bg pt pu l pv pw">Target: 0<br/>1 = High<br/>   Support:  the target has value: '0' for 100.000% of the 333 rows matching the rule <br/>   Coverage: the rule matches: 333 out of 500 rows for target value: 0. This is:<br/>      66.600% of total rows for target value: 0<br/>      33.300% of total rows in data<br/><br/>15 = Low AND 4 = Med<br/>   Support:  The target has value: '0' for 100.000% of the 63 remaining rows matching the rule <br/>   Coverage: The rule matches: 63 out of 167 rows remaining for target value: '0'. This is:<br/>      37.725% of remaining rows for target value: '0'<br/>      12.600% of total rows for target value: 0<br/>      6.300% of total rows in data<br/><br/>4 = High AND 1 = Med<br/>   Support:  The target has value: '0' for 100.000% of the 47 remaining rows matching the rule <br/>   Coverage: The rule matches: 47 out of 104 rows remaining for target value: '0'. This is:<br/>      45.192% of remaining rows for target value: '0'<br/>      9.400% of total rows for target value: 0<br/>      4.700% of total rows in data</span></pre><h1 id="81f2" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Example from the book Principles of Data Mining</h1><p id="e47f" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">This example is provided in one of the sample notebooks on the github page.</p><figure class="pj pk pl pm pn qa px py paragraph-image"><div class="px py pz"><img src="../Images/dc7522d6dae2c461205a8af9a8c2fb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*iJBev63jHBjThyHFvnxVJg.png"/></div></figure><p id="2871" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">PRISM generated three rules:</p><ul class=""><li id="43c1" class="ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng ok ol om bk">IF tears = 1 THEN Target=3</li><li id="033b" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">IF astig = 1 AND tears = 2 and specRX = 2 THEN Target=2</li><li id="69dd" class="ml mm fq mn b go on mp mq gr oo ms mt mu op mw mx my oq na nb nc or ne nf ng ok ol om bk">If astig = 2 and tears = 2 AND specRX =1 THEN Target =1</li></ul><h1 id="3bb2" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Execution Time</h1><p id="c8dc" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">The algorithm is generally able to produce a set of rules in seconds or minutes, but if it is necessary to decrease the execution time of the algorithm, a sample of the data may be used in lieu of the full dataset. The algorithm generally works quite well on samples of the data, as the model is looking for general patterns as opposed to exceptions, and the patterns will be present in any sufficiently large sample.</p><p id="9f27" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">Further notes on tuning the model are provided on the github page.</p><h1 id="20d5" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Conclusions</h1><p id="fc5c" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">Unfortunately, there are relatively few options available today for interpretable predictive models. As well, no one interpretable model will be sufficiently accurate or sufficiently interpretable for all datasets. Consequently, where interpretability is important, it can be worth testing multiple interpretable models, including Decision Trees, other rules-induction tools, GAMs, <a class="af nh" rel="noopener" target="_blank" href="/interpretable-knn-iknn-33d38402b8fc">ikNN</a>, <a class="af nh" href="https://medium.com/towards-data-science/additive-decision-trees-85f2feda2223" rel="noopener">Additive Decision Trees</a>, and PRISM rules.</p><p id="f5c8" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">PRISM Rules very often generates clean, interpretable rules, and often a high level of accuracy, though this will vary from project to project. Some tuning is necessary, though similar to other predictive models.</p><h1 id="6ec4" class="ni nj fq bf nk nl nm gq nn no np gt nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">References</h1><p id="c08f" class="pw-post-body-paragraph ml mm fq mn b go oe mp mq gr of ms mt mu og mw mx my oh na nb nc oi ne nf ng fj bk">[1] Chendrowska, J. (1987) PRISM: An Algorithm for Inducing Modular Rules. International Journal of Man-Machine Studies, vol 27, pp. 349–370.</p><p id="60fd" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">[2] Chendrowska, J. (1990) Knowledge Acquisition for Expert Systems: Inducing Modular Rules from Examples. PhD Thesis, The Open University.</p><p id="7c8a" class="pw-post-body-paragraph ml mm fq mn b go mo mp mq gr mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng fj bk">[3] Bramer, M. (2007) Principles of Data Mining, Springer Press.</p></div></div></div></div>    
</body>
</html>