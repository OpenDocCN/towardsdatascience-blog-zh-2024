["```py\nimport pandas as pd\nimport numpy as np\n\ndef get_random_subspaces(features_arr, num_base_detectors,\n                         num_feats_per_detector):\n    num_feats = len(features_arr)\n    feat_sets_arr = []\n    ft_used_counts = np.zeros(num_feats) \n    ft_pair_mtx = np.zeros((num_feats, num_feats))  \n\n    # Each loop generates one subspace, which is one set of features\n    for _ in range(num_base_detectors):  \n        # Get the set of features with the minimum count      \n        min_count = ft_used_counts.min() \n        idxs = np.where(ft_used_counts == min_count)[0]    \n\n        # Pick one of these randomly and add to the current set\n        feat_set = [np.random.choice(idxs)]   \n\n        # Find the remaining set of features\n        while len(feat_set) < num_feats_per_detector: \n            mtx_with_set = ft_pair_mtx[:, feat_set]\n            sums = mtx_with_set.sum(axis=1)\n            min_sum = sums.min()\n            min_idxs = np.where(sums==min_sum)[0]\n            new_feat = np.random.choice(min_idxs)\n            feat_set.append(new_feat)\n            feat_set = list(set(feat_set))\n\n            # Updates ft_pair_mtx\n            for c in feat_set: \n                ft_pair_mtx[c][new_feat] += 1\n                ft_pair_mtx[new_feat][c] += 1\n\n        # Updates ft_used_counts\n        for c in feat_set: \n            ft_used_counts[c] += 1\n\n        feat_sets_arr.append(feat_set)\n\n    return feat_sets_arr\n\nnp.random.seed(0)\nfeatures_arr = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'] \nnum_base_detectors = 4\nnum_feats_per_detector = 5\n\nfeat_sets_arr = get_random_subspaces(features_arr, \n                                     num_base_detectors, \n                                     num_feats_per_detector)\nfor feat_set in feat_sets_arr:    \n    print([features_arr[x] for x in feat_set])\n```", "```py\n['A', 'E', 'F', 'G', 'H']\n['B', 'C', 'D', 'F', 'H']\n['A', 'B', 'C', 'D', 'E']\n['B', 'D', 'E', 'F', 'G']\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\n\n# Function to find the pair of features remaining in the matrix with the \n# highest correlation\ndef get_highest_corr(): \n    return np.unravel_index(\n        np.argmax(corr_matrix.values, axis=None), \n        corr_matrix.shape)\n\ndef get_correlated_subspaces(corr_matrix, num_base_detectors, \n                                num_feats_per_detector):\n    sets = []\n\n    # Loop through each subspace to be created\n    for _ in range(num_base_detectors): \n        m1, m2 = get_highest_corr()\n\n        # Start each subspace as the two remaining features with \n        # the highest correlation\n        curr_set = [m1, m2] \n        for _ in range(2, num_feats_per_detector):\n            # Get the other remaining correlations\n            m = np.unravel_index(np.argsort(corr_matrix.values, axis=None), \n                                 corr_matrix.shape) \n            m0 = m[0][::-1]\n            m1 = m[1][::-1]\n            for i in range(len(m0)):\n                d0 = m0[i]\n                d1 = m1[i]\n                # Add the pair if either feature is already in the subset\n                if (d0 in curr_set) or (d1 in curr_set): \n                    curr_set.append(d0)\n                    curr_set = list(set(curr_set))\n                    if len(curr_set) < num_feats_per_detector:\n                        curr_set.append(d1)\n                        # Remove duplicates\n                        curr_set = list(set(curr_set)) \n                if len(curr_set) >= num_feats_per_detector:\n                    break\n\n            # Update the correlation matrix, removing the features now used \n            # in the current subspace\n            for i in curr_set: \n                i_idx = corr_matrix.index[i]\n                for j in curr_set:\n                    j_idx = corr_matrix.columns[j]\n                    corr_matrix.loc[i_idx, j_idx] = 0\n            if len(curr_set) >= num_feats_per_detector:\n                break\n\n        sets.append(curr_set)\n    return sets\n\ndata = fetch_openml('baseball', version=1)\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\ncorr_matrix = abs(df.corr(method='spearman'))\ncorr_matrix = corr_matrix.where(\n    np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\ncorr_matrix = corr_matrix.fillna(0)\n\nfeat_sets_arr = get_correlated_subspaces(corr_matrix, num_base_detectors=5, \n                                         num_feats_per_detector=4)\nfor feat_set in feat_sets_arr:    \n    print([df.columns[x] for x in feat_set]) \n```", "```py\n['Games_played', 'At_bats', 'Runs', 'Hits']\n['RBIs', 'At_bats', 'Hits', 'Doubles']\n['RBIs', 'Games_played', 'Runs', 'Doubles']\n['Walks', 'Runs', 'Games_played', 'Triples']\n['RBIs', 'Strikeouts', 'Slugging_pct', 'Home_runs']\n```", "```py\npip install pyod\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom pyod.models.sod import SOD\n\nnp.random.seed(0)\nd = np.random.randn(100, 35)\nd = pd.DataFrame(d)\n\n#A Ensure features 8 and 9 are correlated, while all others are irrelevant\nd[9] = d[9] + d[8] \n\n# Insert a single outlier\nd.loc[99, 8] = 3.5 \nd.loc[99, 9] = -3.8\n\n#C Execute SOD, flagging only 1 outlier\nclf = SOD(ref_set=3, contamination=0.01) \nd['SOD Scores'] = clf.fit (d)\nd['SOD Scores'] = clf.labels_\n```"]