["```py\nmountains = np.load(os.path.join(figure_path, 'mountains.npy'))\n\nH = mountains.shape[0]\nW = mountains.shape[1]\nprint('Mountain at Dusk is H =', H, 'and W =', W, 'pixels.')\nprint('\\n')\n\nfig = plt.figure(figsize=(10,6))\nplt.imshow(mountains, cmap='Purples_r')\nplt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))\nplt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))\nplt.clim([0,1])\ncbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])\nplt.clim([0, 1])\nplt.colorbar(cax=cbar_ax);\n#plt.savefig(os.path.join(figure_path, 'mountains.png'))\n```", "```py\nMountain at Dusk is H = 60 and W = 100 pixels.\n```", "```py\nP = 20\nN = int((H*W)/(P**2))\nprint('There will be', N, 'patches, each', P, 'by', str(P)+'.')\nprint('\\n')\n\nfig = plt.figure(figsize=(10,6))\nplt.imshow(mountains, cmap='Purples_r')\nplt.hlines(np.arange(P, H, P)-0.5, -0.5, W-0.5, color='w')\nplt.vlines(np.arange(P, W, P)-0.5, -0.5, H-0.5, color='w')\nplt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))\nplt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))\nx_text = np.tile(np.arange(9.5, W, P), 3)\ny_text = np.repeat(np.arange(9.5, H, P), 5)\nfor i in range(1, N+1):\n    plt.text(x_text[i-1], y_text[i-1], str(i), color='w', fontsize='xx-large', ha='center')\nplt.text(x_text[2], y_text[2], str(3), color='k', fontsize='xx-large', ha='center');\n#plt.savefig(os.path.join(figure_path, 'mountain_patches.png'), bbox_inches='tight'\n```", "```py\nThere will be 15 patches, each 20 by 20.\n```", "```py\nprint('Each patch will make a token of length', str(P**2)+'.')\nprint('\\n')\n\npatch12 = mountains[40:60, 20:40]\ntoken12 = patch12.reshape(1, P**2)\n\nfig = plt.figure(figsize=(10,1))\nplt.imshow(token12, aspect=10, cmap='Purples_r')\nplt.clim([0,1])\nplt.xticks(np.arange(-0.5, 401, 50), labels=np.arange(0, 401, 50))\nplt.yticks([]);\n#plt.savefig(os.path.join(figure_path, 'mountain_token12.png'), bbox_inches='tight')\n```", "```py\nEach patch will make a token of length 400.\n```", "```py\nclass Patch_Tokenization(nn.Module):\n    def __init__(self,\n                img_size: tuple[int, int, int]=(1, 1, 60, 100),\n                patch_size: int=50,\n                token_len: int=768):\n\n        \"\"\" Patch Tokenization Module\n            Args:\n                img_size (tuple[int, int, int]): size of input (channels, height, width)\n                patch_size (int): the side length of a square patch\n                token_len (int): desired length of an output token\n        \"\"\"\n        super().__init__()\n\n        ## Defining Parameters\n        self.img_size = img_size\n        C, H, W = self.img_size\n        self.patch_size = patch_size\n        self.token_len = token_len\n        assert H % self.patch_size == 0, 'Height of image must be evenly divisible by patch size.'\n        assert W % self.patch_size == 0, 'Width of image must be evenly divisible by patch size.'\n        self.num_tokens = (H / self.patch_size) * (W / self.patch_size)\n\n        ## Defining Layers\n        self.split = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size, padding=0)\n        self.project = nn.Linear((self.patch_size**2)*C, token_len)\n\n    def forward(self, x):\n        x = self.split(x).transpose(1,0)\n        x = self.project(x)\n        return x\n```", "```py\nx = torch.from_numpy(mountains).unsqueeze(0).unsqueeze(0).to(torch.float32)\ntoken_len = 768\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of input channels:', x.shape[1], '\\n\\timage size:', (x.shape[2], x.shape[3]))\n\n# Define the Module\npatch_tokens = Patch_Tokenization(img_size=(x.shape[1], x.shape[2], x.shape[3]),\n                                    patch_size = P,\n                                    token_len = token_len)\n```", "```py\nInput dimensions are\n   batchsize: 1 \n   number of input channels: 1 \n   image size: (60, 100)\n```", "```py\nx = patch_tokens.split(x).transpose(2,1)\nprint('After patch tokenization, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nAfter patch tokenization, dimensions are\n   batchsize: 1 \n   number of tokens: 15 \n   token length: 400\n```", "```py\nx = patch_tokens.project(x)\nprint('After projection, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nAfter projection, dimensions are\n   batchsize: 1 \n   number of tokens: 15 \n   token length: 768\n```", "```py\n# Define an Input\nnum_tokens = 175\ntoken_len = 768\nbatch = 13\nx = torch.rand(batch, num_tokens, token_len)\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n\n# Append a Prediction Token\npred_token = torch.zeros(1, 1, token_len).expand(batch, -1, -1)\nprint('Prediction Token dimensions are\\n\\tbatchsize:', pred_token.shape[0], '\\n\\tnumber of tokens:', pred_token.shape[1], '\\n\\ttoken length:', pred_token.shape[2])\n\nx = torch.cat((pred_token, x), dim=1)\nprint('Dimensions with Prediction Token are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nInput dimensions are\n   batchsize: 13 \n   number of tokens: 175 \n   token length: 768\nPrediction Token dimensions are\n   batchsize: 13 \n   number of tokens: 1 \n   token length: 768\nDimensions with Prediction Token are\n   batchsize: 13 \n   number of tokens: 176 \n   token length: 768\n```", "```py\ndef get_sinusoid_encoding(num_tokens, token_len):\n    \"\"\" Make Sinusoid Encoding Table\n\n        Args:\n            num_tokens (int): number of tokens\n            token_len (int): length of a token\n\n        Returns:\n            (torch.FloatTensor) sinusoidal position encoding table\n    \"\"\"\n\n    def get_position_angle_vec(i):\n        return [i / np.power(10000, 2 * (j // 2) / token_len) for j in range(token_len)]\n\n    sinusoid_table = np.array([get_position_angle_vec(i) for i in range(num_tokens)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) \n\n    return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n\nPE = get_sinusoid_encoding(num_tokens+1, token_len)\nprint('Position embedding dimensions are\\n\\tnumber of tokens:', PE.shape[1], '\\n\\ttoken length:', PE.shape[2])\n\nx = x + PE\nprint('Dimensions with Position Embedding are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nPosition embedding dimensions are\n   number of tokens: 176 \n   token length: 768\nDimensions with Position Embedding are\n   batchsize: 13 \n   number of tokens: 176 \n   token length: 768\n```", "```py\nclass Encoding(nn.Module):\n\n    def __init__(self,\n       dim: int,\n       num_heads: int=1,\n       hidden_chan_mul: float=4.,\n       qkv_bias: bool=False,\n       qk_scale: NoneFloat=None,\n       act_layer=nn.GELU, \n       norm_layer=nn.LayerNorm):\n\n        \"\"\" Encoding Block\n\n            Args:\n                dim (int): size of a single token\n                num_heads(int): number of attention heads in MSA\n                hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Layers\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim=dim,\n                            chan=dim,\n                            num_heads=num_heads,\n                            qkv_bias=qkv_bias,\n                            qk_scale=qk_scale)\n        self.norm2 = norm_layer(dim)\n        self.neuralnet = NeuralNet(in_chan=dim,\n                                hidden_chan=int(dim*hidden_chan_mul),\n                                out_chan=dim,\n                                act_layer=act_layer)\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.neuralnet(self.norm2(x))\n        return x\n```", "```py\n# Define an Input\nnum_tokens = 176\ntoken_len = 768\nbatch = 13\nheads = 4\nx = torch.rand(batch, num_tokens, token_len)\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n\n# Define the Module\nE = Encoding(dim=token_len, num_heads=heads, hidden_chan_mul=1.5, qkv_bias=False, qk_scale=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm)\nE.eval();\n```", "```py\nInput dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token length: 768\n```", "```py\ny = E.norm1(x)\nprint('After norm, dimensions are\\n\\tbatchsize:', y.shape[0], '\\n\\tnumber of tokens:', y.shape[1], '\\n\\ttoken size:', y.shape[2])\ny = E.attn(y)\nprint('After attention, dimensions are\\n\\tbatchsize:', y.shape[0], '\\n\\tnumber of tokens:', y.shape[1], '\\n\\ttoken size:', y.shape[2])\ny = y + x\nprint('After split connection, dimensions are\\n\\tbatchsize:', y.shape[0], '\\n\\tnumber of tokens:', y.shape[1], '\\n\\ttoken size:', y.shape[2])\n```", "```py\nAfter norm, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\nAfter attention, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\nAfter split connection, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\n```", "```py\nz = E.norm2(y)\nprint('After norm, dimensions are\\n\\tbatchsize:', z.shape[0], '\\n\\tnumber of tokens:', z.shape[1], '\\n\\ttoken size:', z.shape[2])\nz = E.neuralnet(z)\nprint('After neural net, dimensions are\\n\\tbatchsize:', z.shape[0], '\\n\\tnumber of tokens:', z.shape[1], '\\n\\ttoken size:', z.shape[2])\nz = z + y\nprint('After split connection, dimensions are\\n\\tbatchsize:', z.shape[0], '\\n\\tnumber of tokens:', z.shape[1], '\\n\\ttoken size:', z.shape[2])\n```", "```py\nAfter norm, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\nAfter neural net, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\nAfter split connection, dimensions are\n   batchsize: 13 \n   number of tokens: 176 \n   token size: 768\n```", "```py\nclass NeuralNet(nn.Module):\n    def __init__(self,\n       in_chan: int,\n       hidden_chan: NoneFloat=None,\n       out_chan: NoneFloat=None,\n       act_layer = nn.GELU):\n        \"\"\" Neural Network Module\n\n            Args:\n                in_chan (int): number of channels (features) at input\n                hidden_chan (NoneFloat): number of channels (features) in the hidden layer;\n                                        if None, number of channels in hidden layer is the same as the number of input channels\n                out_chan (NoneFloat): number of channels (features) at output;\n                                        if None, number of output channels is same as the number of input channels\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Number of Channels\n        hidden_chan = hidden_chan or in_chan\n        out_chan = out_chan or in_chan\n\n        ## Define Layers\n        self.fc1 = nn.Linear(in_chan, hidden_chan)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_chan, out_chan)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n```", "```py\n# Define an Input\nnum_tokens = 176\ntoken_len = 768\nbatch = 1\nx = torch.rand(batch, num_tokens, token_len)\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nInput dimensions are\n   batchsize: 1 \n   number of tokens: 176 \n   token length: 768\n```", "```py\nnorm = nn.LayerNorm(token_len)\nx = norm(x)\nprint('After norm, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n```", "```py\nAfter norm, dimensions are\n   batchsize: 1 \n   number of tokens: 1001 \n   token size: 768\n```", "```py\npred_token = x[:, 0]\nprint('Length of prediction token:', pred_token.shape[-1])\n```", "```py\nLength of prediction token: 768\n```", "```py\nhead = nn.Linear(token_len, 1)\npred = head(pred_token)\nprint('Length of prediction:', (pred.shape[0], pred.shape[1]))\nprint('Prediction:', float(pred))\n```", "```py\nLength of prediction: (1, 1)\nPrediction: -0.5474240779876709\n```", "```py\nclass ViT_Backbone(nn.Module):\n    def __init__(self,\n                preds: int=1,\n                token_len: int=768,\n                num_heads: int=1,\n                Encoding_hidden_chan_mul: float=4.,\n                depth: int=12,\n                qkv_bias=False,\n                qk_scale=None,\n                act_layer=nn.GELU,\n                norm_layer=nn.LayerNorm):\n\n        \"\"\" VisTransformer Backbone\n            Args:\n                preds (int): number of predictions to output\n                token_len (int): length of a token\n                num_heads(int): number of attention heads in MSA\n                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module\n                depth (int): number of encoding blocks in the model\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                 if None, queries and keys are scaled by ``head_dim ** -0.5``\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n        \"\"\"\n\n        super().__init__()\n\n        ## Defining Parameters\n        self.num_heads = num_heads\n        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul\n        self.depth = depth\n\n        ## Defining Token Processing Components\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.token_len))\n        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(num_tokens=self.num_tokens+1, token_len=self.token_len), requires_grad=False)\n\n        ## Defining Encoding blocks\n        self.blocks = nn.ModuleList([Encoding(dim = self.token_len, \n                                               num_heads = self.num_heads,\n                                               hidden_chan_mul = self.Encoding_hidden_chan_mul,\n                                               qkv_bias = qkv_bias,\n                                               qk_scale = qk_scale,\n                                               act_layer = act_layer,\n                                               norm_layer = norm_layer)\n             for i in range(self.depth)])\n\n        ## Defining Prediction Processing\n        self.norm = norm_layer(self.token_len)\n        self.head = nn.Linear(self.token_len, preds)\n\n        ## Make the class token sampled from a truncated normal distrobution \n        timm.layers.trunc_normal_(self.cls_token, std=.02)\n\n    def forward(self, x):\n        ## Assumes x is already tokenized\n\n        ## Get Batch Size\n        B = x.shape[0]\n        ## Concatenate Class Token\n        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n        ## Add Positional Embedding\n        x = x + self.pos_embed\n        ## Run Through Encoding Blocks\n        for blk in self.blocks:\n            x = blk(x)\n        ## Take Norm\n        x = self.norm(x)\n        ## Make Prediction on Class Token\n        x = self.head(x[:, 0])\n        return x\n```", "```py\nclass ViT_Model(nn.Module):\n def __init__(self,\n    img_size: tuple[int, int, int]=(1, 400, 100),\n    patch_size: int=50,\n    token_len: int=768,\n    preds: int=1,\n    num_heads: int=1,\n    Encoding_hidden_chan_mul: float=4.,\n    depth: int=12,\n    qkv_bias=False,\n    qk_scale=None,\n    act_layer=nn.GELU,\n    norm_layer=nn.LayerNorm):\n\n  \"\"\" VisTransformer Model\n\n   Args:\n    img_size (tuple[int, int, int]): size of input (channels, height, width)\n    patch_size (int): the side length of a square patch\n    token_len (int): desired length of an output token\n    preds (int): number of predictions to output\n    num_heads(int): number of attention heads in MSA\n    Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module\n    depth (int): number of encoding blocks in the model\n    qkv_bias (bool): determines if the qkv layer learns an addative bias\n    qk_scale (NoneFloat): value to scale the queries and keys by; \n         if None, queries and keys are scaled by ``head_dim ** -0.5``\n    act_layer(nn.modules.activation): torch neural network layer class to use as activation\n    norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n  \"\"\"\n  super().__init__()\n\n  ## Defining Parameters\n  self.img_size = img_size\n  C, H, W = self.img_size\n  self.patch_size = patch_size\n  self.token_len = token_len\n  self.num_heads = num_heads\n  self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul\n  self.depth = depth\n\n  ## Defining Patch Embedding Module\n  self.patch_tokens = Patch_Tokenization(img_size,\n           patch_size,\n           token_len)\n\n  ## Defining ViT Backbone\n  self.backbone = ViT_Backbone(preds,\n         self.token_len,\n         self.num_heads,\n         self.Encoding_hidden_chan_mul,\n         self.depth,\n         qkv_bias,\n         qk_scale,\n         act_layer,\n         norm_layer)\n  ## Initialize the Weights\n  self.apply(self._init_weights)\n\n def _init_weights(self, m):\n  \"\"\" Initialize the weights of the linear layers & the layernorms\n  \"\"\"\n  ## For Linear Layers\n  if isinstance(m, nn.Linear):\n   ## Weights are initialized from a truncated normal distrobution\n   timm.layers.trunc_normal_(m.weight, std=.02)\n   if isinstance(m, nn.Linear) and m.bias is not None:\n    ## If bias is present, bias is initialized at zero\n    nn.init.constant_(m.bias, 0)\n  ## For Layernorm Layers\n  elif isinstance(m, nn.LayerNorm):\n   ## Weights are initialized at one\n   nn.init.constant_(m.weight, 1.0)\n   ## Bias is initialized at zero\n   nn.init.constant_(m.bias, 0)\n\n @torch.jit.ignore ##Tell pytorch to not compile as TorchScript\n def no_weight_decay(self):\n  \"\"\" Used in Optimizer to ignore weight decay in the class token\n  \"\"\"\n  return {'cls_token'}\n\n def forward(self, x):\n  x = self.patch_tokens(x)\n  x = self.backbone(x)\n  return x\n```"]