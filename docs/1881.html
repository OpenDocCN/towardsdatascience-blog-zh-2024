<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building a RAG Pipeline with MongoDB: Vector Search for Personalized Picks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building a RAG Pipeline with MongoDB: Vector Search for Personalized Picks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-rag-pipeline-with-mongodb-vector-search-for-personalized-movie-picks-46a58a2aaac9?source=collection_archive---------8-----------------------#2024-08-01">https://towardsdatascience.com/building-a-rag-pipeline-with-mongodb-vector-search-for-personalized-movie-picks-46a58a2aaac9?source=collection_archive---------8-----------------------#2024-08-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pablomerchanrivera?source=post_page---byline--46a58a2aaac9--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Pablo Merchán-Rivera, Ph.D." class="l ep by dd de cx" src="../Images/a560330911c7ba23fd4839e33e528f5a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YfALD8ls3hFu_qlSpKRACA.jpeg"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--46a58a2aaac9--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@pablomerchanrivera?source=post_page---byline--46a58a2aaac9--------------------------------" rel="noopener follow">Pablo Merchán-Rivera, Ph.D.</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--46a58a2aaac9--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx kq kr ab q ee ks kt" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="kp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ko kp">1</span></p></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="ku k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al kv an ao ap hr kw kx ky" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep kz cn"><div class="l ae"><div class="ab cb"><div class="la lb lc ld le lf ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al kv an ao ap hr lg lh kt li lj lk ll lm s ln lo lp lq lr ls lt u lu lv lw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="4f83" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This article explores the construction of a movie recommendation system using a Retrieval-Augmented Generation (RAG) pipeline. The objective is to learn how to harness the power of MongoDB’s vector search capabilities, transform data descriptions into searchable digital fingerprints, and create a system that understands the nuances of your preferences and your communication. In other words, we will aim to build a recommendation system that’s not just smart, but also efficient.</p><p id="68a7" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">By the end of this article, you’ll have built a functional movie recommendation system. This system will be able to take a user’s query, such as <em class="mv">“I want to watch a good sci-fi movie that explores artificial intelligence”</em> or <em class="mv">“What is a good animated film that adults would enjoy too? What makes your suggestion a good fit?”</em> and return relevant movie suggestions and the choice reasoning.</p><figure class="mz na nb nc nd ne mw mx paragraph-image"><div role="button" tabindex="0" class="nf ng ed nh bh ni"><div class="mw mx my"><img src="../Images/a1cde82c3c844d5926591d0148cf5a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U8J7k6d8SxIc3C-H"/></div></div><figcaption class="nk nl nm mw mx nn no bf b bg z dx">Photo by <a class="af np" href="https://unsplash.com/@irrabagon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alexandr Popadin</a> on <a class="af np" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9896" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">What is a RAG Pipeline?</h1><p id="ad5b" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">A RAG pipeline refers to the sequential flow of data through a series of processing steps that combines the strengths of large language models (LLMs) with structured data retrieval. It works by first retrieving relevant information from a knowledge base, and then using this information to augment the input of a large language model, which generates the final output. The primary objective of such a pipeline is to generate more accurate, contextually appropriate, and personalised responses to user-specific queries from vast databases.</p><h1 id="fdda" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Why MongoDB?</h1><p id="424b" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">MongoDB is a open-source NoSQL database that stores data in flexible, JSON-like documents, allowing for easy scalability and handling of diverse data types and structures. MongoDB plays a significant role in this project. Its document model aligns well with our movie data, while its vector search capabilities enable similarity searches on our embeddings (i.e., the numerical representations of movie content). We can also take advantage of indexing and query optimisation features to maintain quick data retrieval even as the dataset expands.</p><h1 id="4253" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Our Project</h1><p id="800a" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Here’s what our pipeline will look like:</p><ol class=""><li id="74ae" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ot ou ov bk">Set up the environment and load movie data from Hugging Face</li><li id="7021" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Model the data using Pydantic</li><li id="14e6" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Generate embeddings for the movies information</li><li id="0472" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Ingest the data into a MongoDB database</li><li id="2ba6" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Create a Vector Search Index in MongoDB Atlas</li><li id="b96e" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Perform vector search operations to find relevant movies</li><li id="3ab6" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Handle user queries with an LLM model</li><li id="6872" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Use the RAG pipeline to get a movie recommendation</li></ol><h2 id="2a91" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 1: Setting Up the Environment and Loading the Dataset</h2><p id="8636" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">First, we need to import the necessary libraries and set up our environment. This also involves setting up our API keys and the connection string that the application uses to connect to a MongoDB database:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="f1ec" class="pw nr fq pt b bg px py l pz qa">import warnings<br/>warnings.filterwarnings('ignore')<br/><br/>import os<br/>from dotenv import load_dotenv, find_dotenv<br/>from datasets import load_dataset<br/>import pandas as pd<br/>from typing import List, Optional<br/>from pydantic import BaseModel<br/>from datetime import datetime<br/>from pymongo.mongo_client import MongoClient<br/>import openai<br/>import time<br/><br/>_ = load_dotenv(find_dotenv())<br/>MONGO_URI = os.environ.get("MONGO_URI")<br/>OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")<br/>openai.api_key = OPENAI_API_KEY</span></pre><p id="6a13" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Next, we load our movie dataset:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="b391" class="pw nr fq pt b bg px py l pz qa">dataset = load_dataset("Pablinho/movies-dataset", streaming=True, split="train")<br/>dataset = dataset.take(200)  # 200 movies for the sake of simplicity<br/>dataset_df = pd.DataFrame(dataset)</span></pre><p id="f756" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The dataset contains more than 9000 entries. However, for this exercise, we’re limiting our dataset to 200 movies using <code class="cx qb qc qd pt b">dataset.take(200)</code>. In a real-world scenario, you’d likely use a much larger dataset.</p><h2 id="9496" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 2: Modeling the Data with Pydantic</h2><p id="3b1e" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Data modeling is crucial for ensuring consistency and type safety in our application. Hence, we use Pydantic for this purpose:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="19eb" class="pw nr fq pt b bg px py l pz qa">class Movie(BaseModel):<br/>    Release_Date: Optional[str]<br/>    Title: str<br/>    Overview: str<br/>    Popularity: float<br/>    Vote_Count: int<br/>    Vote_Average: float<br/>    Original_Language: str<br/>    Genre: List[str]<br/>    Poster_Url: str<br/>    text_embeddings: List[float]</span></pre><p id="ff7a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Using Pydantic provides several benefits, such as automatic data validation, type checking, and easy serialization/deserialization. Notice that we also created a <code class="cx qb qc qd pt b">text_embeddings</code> field that will store our generated embeddings as a list of floats</p><h2 id="0f1d" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 3: Embedding Generation</h2><p id="8bcb" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Now, we can use the OpenAI API and write a function for generating embeddings, as follows:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="a06b" class="pw nr fq pt b bg px py l pz qa">def get_embedding(text):<br/>    if not text or not isinstance(text, str):<br/>        return None<br/>    try:<br/>        embedding = openai.embeddings.create(<br/>            input=text,<br/>            model="text-embedding-3-small", dimensions=1536).data[0].embedding<br/>        return embedding<br/>    except Exception as e:<br/>        print(f"Error in get_embedding: {e}")<br/>        return None</span></pre><p id="2e2e" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">In the previous code lines, we first check if the input is valid (non-empty string). Then, we use OpenAI’s embeddings.create method to generate the embedding using the “text-embedding-3-small” model, which generates 1536-dimensional embeddings.</p><p id="4d2d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">Now, we can process each record and generate embeddings with the previous function. We also add some lines to process the <code class="cx qb qc qd pt b">'Genre'</code> field, converting it from a string (if it exists) to a list of genres.</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="8d57" class="pw nr fq pt b bg px py l pz qa">def process_and_embed_record(record):<br/>    for key, value in record.items():<br/>        if pd.isnull(value):<br/>            record[key] = None<br/><br/>    if record['Genre']:<br/>        record['Genre'] = record['Genre'].split(', ')<br/>    else:<br/>        record['Genre'] = []<br/><br/>    text_to_embed = f"{record['Title']} {record['Overview']}"<br/>    embedding = get_embedding(text_to_embed)<br/>    record['text_embeddings'] = embedding<br/>    return record<br/><br/>records = [process_and_embed_record(record) for record in dataset_df.to_dict(orient='records')]</span></pre><p id="fc12" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">These embeddings will allow us to perform semantic searches later, finding movies that are conceptually similar to a given query. Notice that this process might take some time, especially for larger datasets, as we’re making an API call for each movie.</p><h2 id="344a" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 4: Data Ingestion into MongoDB</h2><p id="8c4e" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">We establish a connection to our MongoDB database:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="33d6" class="pw nr fq pt b bg px py l pz qa">def get_mongo_client(mongo_uri):<br/>    client = MongoClient(mongo_uri, appname="pmr.movie.python")<br/>    print("Connection to MongoDB successful")<br/>    return client<br/><br/>mongo_client = get_mongo_client(MONGO_URI)<br/>database_name = "movies_dataset"<br/>collection_name = "movies"<br/>db = mongo_client.get_database(database_name)<br/>collection = db.get_collection(collection_name)<br/><br/>collection.delete_many({})</span></pre><p id="d67c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">We insert our processed and embedded data into MongoDB, which allows us to efficiently store and query our movie data, including the high-dimensional embeddings:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="24d3" class="pw nr fq pt b bg px py l pz qa">movies = [Movie(**record).dict() for record in records]<br/>collection.insert_many(movies)</span></pre><h2 id="d47d" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 5: Creating a Vector Search Index in MongoDB Atlas</h2><p id="619e" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Before we can perform vector search operations, we need to create a vector search index. This step can be done directly in the MongoDB Atlas platform:</p><ol class=""><li id="3d19" class="lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu ot ou ov bk">Log in to your MongoDB Atlas account</li><li id="fdb8" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Navigate to your cluster</li><li id="24aa" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Go to the “Search &amp; Vector Search” tab</li><li id="0b3b" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Click on “Create Search Index”</li><li id="35aa" class="lx ly fq lz b ma ow mc md me ox mg mh mi oy mk ml mm oz mo mp mq pa ms mt mu ot ou ov bk">Choose “JSON Editor” in the “Atlas Vector Search” section and use the following configuration:</li></ol><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="95f2" class="pw nr fq pt b bg px py l pz qa">{<br/>  "fields": [<br/>    {<br/>      "numDimensions": 1536,<br/>      "path": "text_embeddings",<br/>      "similarity": "cosine",<br/>      "type": "vector"<br/>    }<br/>  ]<br/>}</span></pre><p id="e42f" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The idea is to create a vector search index named <code class="cx qb qc qd pt b">"vector_index_text"</code> on the <code class="cx qb qc qd pt b">"text_embeddings"</code> field. We use cosine similarity because it helps us find movies with similar themes or content by comparing the direction of their embedding vectors, ignoring differences in length or amount of detail, which is really good for matching a user’s query to movie descriptions.</p><h2 id="18ca" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 6: Implementing Vector Search</h2><p id="9a2d" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Now, we implement the vector search function. The following function is meant to perform a vector search in our MongoDB collection. It first generates an embedding for the user’s query. It then constructs a MongoDB aggregation pipeline using the $vectorSearch operator. The search looks for the 20 nearest neighbors among 150 candidates.</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="e22e" class="pw nr fq pt b bg px py l pz qa">def vector_search(user_query, db, collection, vector_index="vector_index_text", max_retries=3):<br/>    query_embedding = get_embedding(user_query)<br/>    if query_embedding is None:<br/>        return "Invalid query or embedding generation failed."<br/><br/>    vector_search_stage = {<br/>        "$vectorSearch": {<br/>            "index": vector_index,<br/>            "queryVector": query_embedding,<br/>            "path": "text_embeddings",<br/>            "numCandidates": 150,<br/>            "limit": 20<br/>        }<br/>    }<br/><br/>    pipeline = [vector_search_stage]<br/><br/>    for attempt in range(max_retries):<br/>        try:<br/>            results = list(collection.aggregate(pipeline))<br/>            if results:<br/>                explain_query_execution = db.command(<br/>                    'explain', {<br/>                        'aggregate': collection.name,<br/>                        'pipeline': pipeline,<br/>                        'cursor': {}<br/>                    },<br/>                    verbosity='executionStats')<br/>                vector_search_explain = explain_query_execution['stages'][0]['$vectorSearch']<br/>                millis_elapsed = vector_search_explain['explain']['collectStats']['millisElapsed']<br/>                print(f"Total time for the execution to complete on the database server: {millis_elapsed} milliseconds")<br/>                return results<br/>            else:<br/>                print(f"No results found on attempt {attempt + 1}. Retrying...")<br/>                time.sleep(2)<br/>        except Exception as e:<br/>            print(f"Error on attempt {attempt + 1}: {str(e)}")<br/>            time.sleep(2)<br/>    <br/>    return "Failed to retrieve results after multiple attempts."</span></pre><p id="592c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">We implement a retry mechanism (up to 3 attempts) to handle potential transient issues. The function executes the <code class="cx qb qc qd pt b">explain</code> command as well, which provides detailed information about the query execution.</p><h2 id="9a5a" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">Step 7: Handling User Queries with a LLM</h2><p id="9180" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Finally, we can handle user queries. First, we define a <code class="cx qb qc qd pt b">SearchResultItem</code> class to structure our search results. Then, the <code class="cx qb qc qd pt b">handle_user_query</code> function ties everything together: it performs a vector search based on the user’s query, formats the search results into a pandas DataFrame, and then uses OpenAI’s GPT model (i.e., gpt-3.5-turbo) to generate a response based on the search results and the user’s query, and displays the results and the generated response:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="7e4f" class="pw nr fq pt b bg px py l pz qa">class SearchResultItem(BaseModel):<br/>    Title: str<br/>    Overview: str<br/>    Genre: List[str]<br/>    Vote_Average: float<br/>    Popularity: float<br/><br/>def handle_user_query(query, db, collection):<br/>    get_knowledge = vector_search(query, db, collection)<br/><br/>    if isinstance(get_knowledge, str):<br/>        return get_knowledge, "No source information available."<br/>        <br/>    search_results_models = [SearchResultItem(**result) for result in get_knowledge]<br/>    search_results_df = pd.DataFrame([item.dict() for item in search_results_models])<br/><br/>    completion = openai.chat.completions.create(<br/>        model="gpt-3.5-turbo",<br/>        messages=[<br/>            {"role": "system", "content": "You are a movie recommendation system."},<br/>            {"role": "user", "content": f"Answer this user query: {query} with the following context:\n{search_results_df}"}<br/>        ]<br/>    )<br/><br/>    system_response = completion.choices[0].message.content<br/><br/>    print(f"- User Question:\n{query}\n")<br/>    print(f"- System Response:\n{system_response}\n")<br/><br/>    return system_response</span></pre><p id="f94a" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">This function actually demonstrates the core value of this RAG: we generate a contextually appropriate response by retrieving relevant information from our database.</p><h2 id="2679" class="pb nr fq bf ns pc pd pe nw pf pg ph oa mi pi pj pk mm pl pm pn mq po pp pq pr bk">8. Using the RAG Pipeline</h2><p id="b377" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">To use this RAG pipeline, you can now make queries like this:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="1157" class="pw nr fq pt b bg px py l pz qa">query = """<br/>I'm in the mood for a highly-rated action movie. Can you recommend something popular?<br/>Include a reason for your recommendation.<br/>"""<br/>handle_user_query(query, db, collection)</span></pre><p id="2c6c" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">The system would give a respond similar to this:</p><pre class="mz na nb nc nd ps pt pu bp pv bb bk"><span id="b9b9" class="pw nr fq pt b bg px py l pz qa">I recommend "Spider-Man: No Way Home" as a popular and highly-rated action <br/>movie for you to watch. With a vote average of 8.3 and a popularity score <br/>of 5083.954, this film has garnered a lot of attention and positive <br/>reviews from audiences. <br/><br/>"Spider-Man: No Way Home" is a thrilling action-packed movie that brings <br/>together multiple iterations of Spider-Man in an epic crossover event. It <br/>offers a blend of intense action sequences, emotional depth, and nostalgic<br/>moments that fans of the superhero genre will surely enjoy. So, if you're<br/>in the mood for an exciting action movie with a compelling storyline and<br/>fantastic visual effects, "Spider-Man: No Way Home" is an excellent choice<br/>for your movie night.</span></pre><h1 id="935e" class="nq nr fq bf ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om on bk">Conclusion</h1><p id="d819" class="pw-post-body-paragraph lx ly fq lz b ma oo mc md me op mg mh mi oq mk ml mm or mo mp mq os ms mt mu fj bk">Building a RAG pipeline involves several steps, from data loading and modeling to embedding generation and vector search. This example showcases how a RAG pipeline can provide informative, context-aware responses by combining the specific movie data in our database with the natural language understanding and generation capabilities of the language model. On top of this, we use MongoDB because it is well-suited for this type of workflow due to its native vector search capabilities, flexible document model, and scalability.</p><p id="a02b" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk">You can expand on this system by adding more data, fine-tuning your embeddings, or implementing more complex recommendation algorithms.</p><p id="245d" class="pw-post-body-paragraph lx ly fq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu fj bk"><em class="mv">For the complete code and additional resources, check out the </em><a class="af np" href="https://github.com/mr-pablinho/rag-mongodb-moviepl" rel="noopener ugc nofollow" target="_blank"><em class="mv">GitHub repository</em></a><em class="mv">. The dataset used in this project is sourced from </em><a class="af np" href="https://www.kaggle.com/datasets/disham993/9000-movies-dataset/data" rel="noopener ugc nofollow" target="_blank"><em class="mv">Kaggle</em></a><em class="mv"> and has been granted CC0 1.0 Universal (CC0 1.0) Public Domain Dedication by the original author. You can find the dataset and more information </em><a class="af np" href="https://huggingface.co/datasets/Pablinho/movies-dataset" rel="noopener ugc nofollow" target="_blank"><em class="mv">here</em></a><em class="mv">.</em></p></div></div></div></div>    
</body>
</html>