- en: 'Discretization, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?source=collection_archive---------5-----------------------#2024-10-22](https://towardsdatascience.com/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?source=collection_archive---------5-----------------------#2024-10-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DATA PREPROCESSING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6 fun ways to categorize numbers into bins!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--f056af9102fa--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--f056af9102fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f056af9102fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f056af9102fa--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--f056af9102fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f056af9102fa--------------------------------)
    ·10 min read·Oct 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6561ef28778421d0abf32c496d9cc832.png)'
  prefs: []
  type: TYPE_IMG
- en: '`⛳️ More [DATA PREPROCESSING](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4),
    explained: · [Missing Value Imputation](/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb)
    · [Categorical Encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    · [Data Scaling](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    ▶ [Discretization](/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86)
    · [Oversampling & Undersampling](/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091)
    · [Data Leakage in Preprocessing](/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7)`'
  prefs: []
  type: TYPE_NORMAL
- en: Most machine learning model requires the data to be numerical — all object or
    categorical data has to be in numerical format first. But, actually, there are
    times when categorical data comes in handy (it’s more useful to us human than
    to the machines most of the time). Discretization (or binning) does exactly that
    — converting numerical data into categorical ones!
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your goal, there are numerous way to categorize your data. Here,
    we’ll use a simple dataset to show through six different binning methods. From
    equal-width to clustering-based approaches, we’ll sweep those numerical values
    into some categorical bins!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/637e519d208064455494fb6145230db2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Discretization?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discretization, also known as binning, is the process of transforming continuous
    numerical variables into discrete categorical features. It involves dividing the
    range of a continuous variable into intervals (bins) and assigning data points
    to these bins based on their values.
  prefs: []
  type: TYPE_NORMAL
- en: Why Do We Need Binning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Handling Outliers**: Binning can reduce the impact of outliers without removing
    data points.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Improving Model Performance**: Some algorithms perform better with categorical
    inputs (such as [Bernoulli Naive Bayes](https://medium.com/towards-data-science/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Simplifying Visualization**: Binned data can be easier to visualize and interpret.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reducing Overfitting**: It can prevent models from fitting to noise in high-precision
    data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Data Needs Binning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data That Often Benefits from Binning:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Continuous variables with wide ranges**: Variables with a large spread of
    values can often benefit from grouping.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Skewed distributions**: Binning can help normalize heavily skewed data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Variables with outliers**: Binning can handle the effect of extreme values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**High-cardinality numerical data**: Variables with many unique values can
    be simplified through binning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data That Usually Doesn’t Need Binning:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Already categorical data**: Variables that are already in discrete categories
    don’t need further binning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Discrete numerical data with few unique values**: If a variable only has
    a small number of possible values, binning might not provide additional benefit.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Numeric IDs or codes**: These are meant to be unique identifiers, not for
    analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time series data**: While you can bin time series data, it often requires
    specialized techniques and careful consideration, but less common overall.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/aee9adcaa9403a05e391d55f4a08f509.png)'
  prefs: []
  type: TYPE_IMG
- en: The Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate these binning techniques, we’ll be using this artificial dataset.
    Say, this is the weather condition in some golf course, collected on 15 different
    days.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d331773e1d05bd2af57d2579c62304b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**UV Index** (a scale from 0–11), **Humidity** (in %), **Wind Speed** (in mph),
    **Rainfall Amount** (in mm), **Temperature (**in Fahrenheit), **Crowdedness**
    (0 (empty) to 1 (full))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Using this dataset, let’s see how various binning techniques can be applied
    to our columns!
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Equal-Width Binning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equal-width binning divides the range of a variable into a specified number
    of intervals, all with the same width.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method works well for data with a roughly uniform
    distribution and when the minimum and maximum values are meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply equal-width binning to our UV Index variable.
    We’ll create four bins: Low, Moderate, High, and Very High. We chose this method
    for UV Index because it gives us a clear, intuitive division of the index range,
    which could be useful for understanding how different index ranges affect golfing
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d852fd9590a7de9d5af5d3b3aa020b08.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 2: Equal-Frequency Binning (Quantile Binning)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Equal-frequency binning creates bins that contain approximately the same number
    of observations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method is particularly useful for skewed data or
    when you want to make sure a balanced representation across categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply equal-frequency binning to our Humidity variable,
    creating three bins: Low, Medium, and High. We chose this method for Humidity
    because it ensures we have an equal number of observations in each category, which
    can be helpful if humidity values are not evenly distributed across their range.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49950ea927ed58b57b5cfcf244bd9d46.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 3: Custom Binning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Custom binning allows you to define your own bin edges based on domain knowledge
    or specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method is ideal when you have specific thresholds
    that are meaningful in your domain or when you want to focus on particular ranges
    of values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply custom binning to our Rainfall Amount. We chose
    this method for this column because there are standardized categories for rain
    (such as described [in this site](https://www.researchgate.net/figure/Classification-of-rainfall-events-based-on-daily-rainfall-amount_tbl1_289849694))
    that are more meaningful than arbitrary divisions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c211f20ccf1f74df9d51d93058fbdf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 4: Logarithmic Binning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logarithmic binning creates bins that grow exponentially in size. The method
    basically applies log transformation first then performs equal-width binning.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method is particularly useful for data that spans
    several orders of magnitude or follows a power law distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply logarithmic binning to our Wind Speed variable.
    We chose this method for Wind Speed because the effect of wind on a golf ball’s
    trajectory might not be linear. A change from 0 to 5 mph might be more significant
    than a change from 20 to 25 mph.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/447e1400db2cdde81998ef29e1c3bb00.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 5: Standard Deviation-based Binning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Standard Deviation based binning creates bins based on the number of standard
    deviations away from the mean. This approach is useful when working with normally
    distributed data or when you want to bin data based on how far values deviate
    from the central tendency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variations**: The exact number of standard deviations used for binning can
    be adjusted based on the specific needs of the analysis. The number of bins is
    typically odd (to have a central bin). Some implementations might use unequal
    bin widths, with narrower bins near the mean and wider bins in the tails.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method is well-suited for data that follows a normal
    distribution or when you want to identify outliers and understand the spread of
    your data. May not be suitable for highly skewed distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply this binning method scaling to our Temperature
    variable. We chose this method for Temperature because it allows us to categorize
    temperatures based on how they deviate from the average, which can be particularly
    useful in understanding weather patterns or climate trends.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c740e47c406b24440f075ca4a5884dd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 6: K Means Binning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-Means binning uses the K-Means clustering algorithm to create bins. It groups
    data points into clusters based on how similar the data points are to each other,
    with each cluster becoming a bin.
  prefs: []
  type: TYPE_NORMAL
- en: '**Common Data Type**: This method is great for finding groups in data that
    might not be obvious at first. It works well with data that has one peak or several
    peaks, and it can adjust to the way the data is organized.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In our Case**: Let’s apply K-Means binning to our Crowdedness variable. We
    chose this method for Crowdedness because it might reveal natural groupings in
    how busy the golf course gets, which could be influenced by various factors not
    captured by simple threshold-based binning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd0981955f858214f737ef76dc4980ef.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We tried six different ways to ‘discretize’ the numbers in our golf data. So,
    the final dataset now looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c8083b55c6b1d171f7f5adc0523d5c6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s review how each binning technique transformed our weather data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equal-Width Binning (UVIndex)**: Divided our UV Index scale into four equal
    ranges, categorizing exposure levels from ‘Low’ to ‘Very High’. This gives a straightforward
    interpretation of UV intensity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Equal-Frequency Binning (Humidity)**: Sorted our Humidity readings into ‘Low’,
    ‘Medium’, and ‘High’ categories, each containing an equal number of data points.
    This approach makes sure a balanced representation across humidity levels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logarithmic Binning (WindSpeed)**: Applied to our Wind Speed data, this method
    accounts for the non-linear impact of wind on weather conditions, categorizing
    speeds as ‘Light’, ‘Moderate’, or ‘Strong’.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Custom Binning (RainfallAmount)**: Used domain knowledge to classify rainfall
    into meaningful categories from ‘No Rain’ to ‘Heavy Rain’. This method directly
    translates measurements into practical weather descriptions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Standard Deviation-Based Binning (Temperature)**: Segmented our Temperature
    data based on its distribution, ranging from ‘Very Low’ to ‘Very High’. This approach
    highlights how temperatures deviate from the average.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**K-Means Binning (Crowdedness)**: Showed natural groupings in our Crowdedness
    data, potentially showing patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s important to avoid applying binning techniques with no thought. The nature
    of each variable and your analytical goals are always varied and it’s good to
    keep that in mind when selecting a binning method. In many cases, trying out multiple
    techniques and comparing their outcomes can provide the most insights into your
    data!
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ The Risks of Binning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While performing binning sounds easy, it comes with its own risks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Information Loss**: When you bin data, you’re essentially smoothing over
    the details. This can be great for spotting trends, but you might miss out on
    subtle patterns or relationships within the bins.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Arbitrary Boundaries**: The choice of bin edges can sometimes feel like more
    art than science. A slight shift in these boundaries can lead to different interpretations
    of your data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model Impact**: Some models, particularly tree-based ones like [Decision
    Tree](https://medium.com/towards-data-science/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e),
    might actually perform worse with binned data. They’re pretty good at finding
    their own ‘bins’, so to speak.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**False Sense of Security**: Binning can make your data look neater and more
    manageable, but the underlying complexity is still there. Just hidden.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Difficulty in Interpretation**: While binning can simplify analysis, it can
    also make it harder to interpret the magnitude of effects. “High” temperature
    could mean very different things in different contexts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, what’s a data scientist to do? Here’s my advice:'
  prefs: []
  type: TYPE_NORMAL
- en: Always keep a copy of your unbinned data. You might need to go back to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try different binning strategies and compare the results. Don’t settle for the
    first method you try.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look up if there’s already a standard way in the domain of the dataset to categorize
    the data (like our “Rainfall Amount” example above.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 🌟 Discretization Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘿𝙖𝙩𝙖 𝙋𝙧𝙚𝙥𝙧𝙤𝙘𝙚𝙨𝙨𝙞𝙣𝙜 𝙢𝙚𝙩𝙝𝙤𝙙𝙨 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----f056af9102fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----f056af9102fa--------------------------------)6
    stories![](../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png)![Cartoon illustration
    of two figures embracing, with letters ‘A’, ‘B’, ‘C’ and numbers ‘1’, ‘2’, ‘3’
    floating around them. A pink heart hovers above, symbolizing affection. The background
    is a pixelated pattern of blue and green squares, representing data or encoding.
    This image metaphorically depicts the concept of encoding categorical data, where
    categories (ABC) are transformed into numerical representations (123).](../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png)![A
    cartoon illustration representing data scaling in machine learning. A tall woman
    (representing a numerical feature with a large range) is shown shrinking into
    a child (representing the same feature after scaling to a smaller range). A red
    arrow indicates the shrinking process, and yellow sparkles around the child signify
    the positive impact of scaling.](../Images/d261b2c52a3cafe266d1962d4dbabdbd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----f056af9102fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----f056af9102fa--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----f056af9102fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----f056af9102fa--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This “dummy” doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
