<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Dog Poop Compass</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Dog Poop Compass</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dog-poop-compass-bayesian-analysis-of-canine-business-f95a4b9f2bf9?source=collection_archive---------3-----------------------#2024-11-25">https://towardsdatascience.com/dog-poop-compass-bayesian-analysis-of-canine-business-f95a4b9f2bf9?source=collection_archive---------3-----------------------#2024-11-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="cc0b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Bayesian analysis of canine business</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://datawondering.com/?source=post_page---byline--f95a4b9f2bf9--------------------------------" rel="noopener  ugc nofollow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dima Sergeev" class="l ep by dd de cx" src="../Images/62e582badeef3041a535414ac5b79048.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*c0DjxFL5Y8AA-swblPKU6w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f95a4b9f2bf9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://datawondering.com/?source=post_page---byline--f95a4b9f2bf9--------------------------------" rel="noopener  ugc nofollow">Dima Sergeev</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f95a4b9f2bf9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">22 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="7f3b" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">tl;dr</h1><p id="a2b9" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Do dogs poop facing North and South? Turns out they do! Want to learn how to measure this at home using a compass app, Bayesian statistics, and one dog (dog not included)? Jump in!</p><h1 id="5fe3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Introduction</h1><p id="c2ea" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This is my dog. His name is Auri and he is a 5 year old Cavalier King Charles Spaniel.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc od"><img src="../Images/5f8a4dec752dfd712c3586dc50a6a50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mxiykmeTKxYM5DI35unz9A.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Auri (image by author)</figcaption></figure><p id="8cfa" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">As many other dog owners, during our walks I noticed that Auri has a very particular ritual when he needs to go to the bathroom. When he finds a perfect spot he starts circling around something like a compass.</p><p id="f0b0" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">At first I was simply amused by this behaviour. After all, who knows what goes through a dog’s mind? But after a while I remembered reading a research paper from 2013 titled <a class="af oz" href="https://frontiersinzoology.biomedcentral.com/counter/pdf/10.1186/1742-9994-10-80.pdf" rel="noopener ugc nofollow" target="_blank">“Dogs are sensitive to small variations of the Earth’s magnetic field”</a>. The research was conducted on a relatively large sample of dogs and confirmed that “dogs preferred to excrete with the body being aligned along the north-south axis”.</p><p id="1cd7" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk"><em class="pa">Now that could be an interesting research to try out!</em> I thought to myself. And how lucky that I have a perfect test subject, a lovely canine of my own. I decided to replicate the findings and confirm (or debunk!) the hypothesis with Auri, my N=1 unsuspecting research participant.</p><p id="1858" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And so began my long journey of data collection spanning multiple months and capturing over 150 of “alignment sessions” if you catch my drift.</p><h1 id="e800" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Data Collection</h1><p id="21c1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">For my study, I needed to have a compass measurement for every time Auri pooped. Thank to modern technological advancements we not only have <a class="af oz" href="https://www.theverge.com/2024/6/10/24175487/ipad-calculator-app-ipados18-pencil-apple-wwdc2024" rel="noopener ugc nofollow" target="_blank">a calculator app for iPad</a> but also a <a class="af oz" href="https://www.simplymac.com/apps/how-accurate-is-the-iphone-compass" rel="noopener ugc nofollow" target="_blank">fairly accurate</a> compass on a phone. And that’s what I decided to use.</p><p id="6180" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The approach was very simple. Every time my dog settled to have some private time I opened the compass app, aligned my phone along Auri’s body and took a screenshot. In the original paper the authors eloquently called the alignment as <em class="pa">a compass direction of the thoracic spine (between scapulae) towards the head</em>. Very scientific. All it means is that the compass arrow is supposed to point at the same direction as dog’s head.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pb"><img src="../Images/40d6a8344373312ad0c80743e791b191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qO7h79BEV0uyxIvoT5Lrjw.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx"><a class="af oz" href="https://frontiersinzoology.biomedcentral.com/counter/pdf/10.1186/1742-9994-10-80.pdf" rel="noopener ugc nofollow" target="_blank">Dogs are sensitive to small variations of the Earth’s magnetic field, Vlastimil Hart et al.</a></figcaption></figure><p id="72ed" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Anyways, that’s what I did in total 150 something times over the course of a few months. I could almost sense the passer bys’ confusion mixed with curiosity as I was seemingly taking pictures of all my dog’s relief acts. But was it worth it? Let’s find out!</p><h1 id="20b7" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Analysis</h1><p id="d33d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I’ll briefly discuss the data extraction and preprocessing here and then jump straight to working with circular distributions and hypothesis testing.</p><p id="def2" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">As always, all the code can be found on my GitHub here: <a class="af oz" href="https://github.com/DmitrySerg/data-wondering/tree/main/bayesian-dog-poop" rel="noopener ugc nofollow" target="_blank">Data Wondering</a>.</p><h2 id="b5ea" class="pc mk fq bf ml pd pe pf mo pg ph pi mr no pj pk pl ns pm pn po nw pp pq pr ps bk">How to process app screenshots?</h2><p id="e5d7" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">After data collection I had a series of screenshots of the compass app:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc pt"><img src="../Images/d2b8c3a46070b0eb2047d7317029992d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCwOrF0r21QUjk_nTd6QEg.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Compass app screenshot (image by author)</figcaption></figure><p id="3da1" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Being lazy as I am I didn’t feel like scrolling through all of them patiently writing down the compass degrees. So I decided to send all those images to my notebook and automate the process.</p><figure class="oe of og oh oi oj"><div class="pu io l ed"><div class="pv pw l"/></div></figure><p id="79fb" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The task was simple — all I needed from the images was the big numbers at the bottom of the screen. Fortunately, there are a lot of small pretrained networks out there that can do basic <a class="af oz" href="https://en.wikipedia.org/wiki/Optical_character_recognition" rel="noopener ugc nofollow" target="_blank">OCR</a> (stands for Optical character recognition). I opted for an <code class="cx px py pz qa b"><a class="af oz" href="https://github.com/JaidedAI/EasyOCR" rel="noopener ugc nofollow" target="_blank">easyocr</a></code> package which was a bit on a slower end but free and easy to work with.</p><p id="5bbe" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">I’ll walk you through a quick example of using <code class="cx px py pz qa b">easyocr</code> alongside with <code class="cx px py pz qa b"><a class="af oz" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank">opencv</a></code> to extract the numbers from a single screenshot.</p><p id="07d9" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">First, let’s load the image and display it:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="46ba" class="qe mk fq qa b bg qf qg l qh qi">import cv2<br/>import os<br/><br/>image_dir = '../data/raw/'<br/>img = cv2.imread(image_dir + 'IMG_6828.png')<br/><br/>plt.imshow(img)<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qj"><img src="../Images/c42af7627123c9f8bf71f71b333cbb6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hcfvy4TC7l2UEIrgqfyFvQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author</figcaption></figure><p id="fc12" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Next, I convert the image to greyscale to remove any color information and make it less noisy:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="612c" class="qe mk fq qa b bg qf qg l qh qi">gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/>plt.imshow(gray, cmap='gray')<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qk"><img src="../Images/eb2cd69c076f290f6deee122198215a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5dlTROsaCEEh60UnhOHgA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author</figcaption></figure><p id="c2c6" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And then I zoom in on the area of interest:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ql"><img src="../Images/6e9f14b9749c3719bfde09111822d570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXJVM05CNoB27BWTB8CbmA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author</figcaption></figure><p id="9e23" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Finally, I use <em class="pa">easyocr</em> to extract the numbers from the image. It does extract both the number and the confidence score but I am only interested in the number.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="c035" class="qe mk fq qa b bg qf qg l qh qi">import easyocr<br/><br/>reader = easyocr.Reader(['en'])<br/>result = reader.readtext(gray[1850:2100, 200:580])<br/>for bbox, text, prob in result:<br/>    print(f"Detected text: {text} with confidence {prob}")</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="5712" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Detected text: 340 with confidence 0.999995182215476</span></pre><p id="e8ed" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And that’s it! I wrote a simple for loop to iterate through all the screenshots and saved the results to a CSV file.</p><p id="777c" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Here’s a link to the full preprocessing notebook: <a class="af oz" href="https://github.com/DmitrySerg/data-wondering/blob/main/bayesian-dog-poop/notebooks/0-data-preprocessing.ipynb" rel="noopener ugc nofollow" target="_blank">Data Preprocessing</a>.</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qn"><img src="../Images/a068fb6851bf4c3f1511a667403b7d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3UuaZTPoY2aNayAET-RLg.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Image by author</figcaption></figure><h1 id="3a58" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Spinning the wheel: Circular Distributions</h1><p id="0e7e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">I don’t usually work with circular distributions so I had to do some reading. Unlike regular data that we are used to, circular data has a peculiar property: the “ends” of the distribution are connected.</p><p id="89c4" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">For example, if you take the distribution of hours in a day, you would find that the distance between 23:00 and 00:00 is the same as between 00:00 and 01:00. Or, in the case of compass degrees, the distance between 359° and 0° is the same as between 0° and 1°.</p><p id="9840" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Even calculating a sample mean is not straightforward. A standard arithmetic mean between 360° and 0° would give 180° although both 360° and 0° point to the exact same direction.</p><p id="6675" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Similarly in my case, I got almost perfectly opposite estimates when calculating the mean in an arithmetic and in correct way. I converted the degrees to radians using a helper function from this nice library: <a class="af oz" href="https://pingouin-stats.org/build/html/generated/pingouin.convert_angles.html#pingouin.convert_angles" rel="noopener ugc nofollow" target="_blank">pingouin</a> and calculated the mean using the <code class="cx px py pz qa b">circ_mean</code> function.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="b3ab" class="qe mk fq qa b bg qf qg l qh qi">from pingouin import circ_mean<br/><br/>arithmetic_mean = data['radians'].mean()<br/>circular_mean = circ_mean(data['radians'])<br/><br/>print(f"Arithmetic mean: {arithmetic_mean:.3f}; Circular mean: {circular_mean:.3f}")</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="a301" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Arithmetic mean: 0.082; Circular mean: 2.989</span></pre><p id="bff2" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Next, I wanted to visualize the compass distribution. I used the <a class="af oz" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.vonmises.html" rel="noopener ugc nofollow" target="_blank">Von Mises distribution</a> to model the circular data and drew the polar plot using <a class="af oz" href="https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_demo.html" rel="noopener ugc nofollow" target="_blank">matplotlib</a>.</p><blockquote class="qo qp qq"><p id="cd8b" class="nf ng pa nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The <strong class="nh fr">Von Mises</strong> distribution is the circular analogue of the normal distribution. It is defined by two parameters: the mean location μ and the concentration κ. The concentration parameter controls the spread and is analogous to the inverse of the variance. When κ is 0 the distribution is uniform, as κ increases the distribution contracts around the mean.</p></blockquote><p id="8aef" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let’s import the necessary libraries and define the helper functions:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="aa14" class="qe mk fq qa b bg qf qg l qh qi">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/><br/>from scipy.stats import vonmises<br/>from pingouin import convert_angles<br/>from typing import Tuple, List<br/><br/><br/>def vonmises_kde(series: np.ndarray, kappa: float, n_bins: int = 100) -&gt; Tuple[np.ndarray, np.ndarray]:<br/>    """<br/>    Estimate a von Mises kernel density estimate (KDE) over circular data using scipy.<br/>    <br/>    Parameters:<br/>    series: np.ndarray <br/>        The input data in radians, expected to be a 1D array.<br/>    kappa: float<br/>        The concentration parameter for the von Mises distribution.<br/>    n_bins: int<br/>        The number of bins for the KDE estimate (default is 100).<br/>    <br/>    Returns:<br/>    bins: np.ndarray<br/>        The bin edges (x-values) used for the KDE.<br/>    kde: np.ndarray<br/>        The estimated density values (y-values) for each bin.<br/>    """<br/>    bins = np.linspace(-np.pi, np.pi, n_bins)<br/>    kde = np.zeros(n_bins)<br/>    <br/>    for angle in series:<br/>        kde += vonmises.pdf(bins, kappa, loc=angle)<br/>    <br/>    kde = kde / len(series)<br/>    return bins, kde<br/><br/><br/>def plot_circular_distribution(<br/>    data: pd.DataFrame,<br/>    plot_type: str = 'kde',<br/>    bins: int = 30,<br/>    figsize: tuple = (4, 4),<br/>    **kwargs<br/>) -&gt; None:<br/>    """<br/>    Plot a compass rose with either KDE or histogram for circular data.<br/>    <br/>    Parameters:<br/>    -----------<br/>    data: pd.DataFrame<br/>        DataFrame containing 'degrees'and 'radians' columns with circular data<br/>    plot_type: str<br/>        Type of plot to create: 'kde' or 'histogram'<br/>    bins: int<br/>        Number of bins for histogram or smoothing parameter for KDE<br/>    figsize: tuple<br/>        Figure size as (width, height)<br/>    **kwargs: dict<br/>        Additional styling arguments for histogram (color, edgecolor, etc.)<br/>    """<br/>    plt.figure(figsize=figsize)<br/>    ax = plt.subplot(111, projection='polar')<br/><br/>    ax.set_theta_zero_location('N')<br/>    ax.set_theta_direction(-1)<br/><br/>    # add cardinal directions<br/>    directions = ['N', 'E', 'S', 'W']<br/>    angles = [0, np.pi / 2, np.pi, 3 * np.pi / 2]<br/>    for direction, angle in zip(directions, angles):<br/>        ax.text(<br/>            angle, 0.45, direction,<br/>            horizontalalignment='center',<br/>            verticalalignment='center',<br/>            fontsize=12,<br/>            weight='bold'<br/>        )<br/><br/>    if plot_type.lower() == 'kde':<br/>        x, kde = vonmises_kde(data['radians'].values, bins)<br/>        ax.plot(x, kde, color=kwargs.get('color', 'red'), lw=2)<br/>    <br/>    elif plot_type.lower() == 'histogram':<br/>        hist_kwargs = {<br/>            'color': 'teal',<br/>            'edgecolor': 'black',<br/>            'alpha': 0.7<br/>        }<br/>        hist_kwargs.update(kwargs) <br/>        <br/>        angles_rad = np.deg2rad(data['degrees'].values)<br/>        counts, bin_edges = np.histogram(<br/>            angles_rad, <br/>            bins=bins, <br/>            range=(0, 2*np.pi), <br/>            density=True<br/>        )<br/>        widths = np.diff(bin_edges)<br/>        ax.bar(<br/>            bin_edges[:-1],<br/>            counts,<br/>            width=widths,<br/>            align='edge',<br/>            **hist_kwargs<br/>        )<br/>    <br/>    else:<br/>        raise ValueError("plot_type must be either 'kde' or 'histogram'")<br/>    <br/>    ax.xaxis.grid(True, linestyle='--', alpha=0.5)<br/>    ax.yaxis.grid(True, linestyle='--', alpha=0.5)<br/>    ax.set_yticklabels([]) <br/>    plt.show()</span></pre><p id="7c91" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Now let’s load the data and plot the charts:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="3f2f" class="qe mk fq qa b bg qf qg l qh qi">data = pd.read_csv('../data/processed/compass_degrees.csv', index_col=0)<br/>data['radians'] = convert_angles(data['degrees'], low=0, high=360)<br/><br/>plot_circular_distribution(data, plot_type='histogram', figsize=(6, 6))<br/>plot_circular_distribution(data, plot_type='kde', figsize=(5, 5))</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qr"><img src="../Images/c24f301cacfa262e4d67cac6d7174a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pc_mGsws-XRszPfYj8lmnQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Circular histogram of doggy business (image by author)</figcaption></figure><p id="c2bb" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">From the histogram it’s clear that Auri has his preferences when choosing the relief direction. There’s a clear spike towards the North and a dip towards the South. Nice!</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qs"><img src="../Images/222d793339b8090e7ae2514350de4bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pif-6G3G9ynD0NJhYMPOag.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Circular KDE of doggy business (image by author)</figcaption></figure><p id="0720" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">With the KDE plot we get a smoother representation of the distribution. And the good news is that it is very far from being a uniform circle.</p><p id="d577" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">It’s time to validate it statistically!</p><h1 id="4a5f" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Statistically Significant Poops</h1><p id="460e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Just as circular data requires special treatment for visualizations and distributions, it also requires special statistical tests.</p><p id="e5a6" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">I’ll be using a few tests from the <a class="af oz" href="https://pingouin-stats.org/" rel="noopener ugc nofollow" target="_blank">pingouin</a> library I already mentioned earlier. The first test I’ll use is the <a class="af oz" href="https://en.wikipedia.org/wiki/Rayleigh_test" rel="noopener ugc nofollow" target="_blank">Rayleigh test</a> which is a test for uniformity of circular data. The null hypothesis claims that the data is uniformly distributed around the circle and the alternative is that it is not.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="098e" class="qe mk fq qa b bg qf qg l qh qi">from pingouin import circ_rayleigh<br/><br/>z, pval = circ_rayleigh(data['radians'])<br/>print(f"Z-statistics: {z:.3f}; p-value: {pval:.6f}")</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="da8b" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Z-statistics: 3.893; p-value: 0.020128</span></pre><p id="fe31" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Good news, everyone! The p-value is less than 0.05 and we reject the null hypothesis. Auri’s strategic poop positioning is not random!</p><p id="3db7" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The only downside is that the test <a class="af oz" href="https://pingouin-stats.org/build/html/generated/pingouin.circ_rayleigh.html#:~:text=The%20assumptions%20for%20the%20Rayleigh%20test%20are%20that%20(1)%20the%20distribution%20has%20only%20one%20mode%20and%20(2)%20the%20data%20is%20sampled%20from%20a%20von%20Mises%20distribution." rel="noopener ugc nofollow" target="_blank">assumes</a> that the distribution has only one mode and the data is sampled from a von Mises distribution. Oh well, let’s try something else then. Auri’s data clearly has multiple modes.</p><p id="8924" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The next on the list is the <a class="af oz" href="https://pingouin-stats.org/build/html/generated/pingouin.circ_vtest.html" rel="noopener ugc nofollow" target="_blank">V-test</a>. This test checks if the data is non-uniform with a specified mean direction. From the documentation we get that:</p><blockquote class="qo qp qq"><p id="9e5f" class="nf ng pa nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The V test has more power than the Rayleigh test and is preferred if there is reason to believe in a specific mean direction.</p></blockquote><p id="c9f4" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Perfect! Let’s try it.</p><p id="9942" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">From the distribution it’s clear that Auri prefers the South direction above all. I’ll set the mean direction to <em class="pa">π</em> radians (South) and run the test.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="f072" class="qe mk fq qa b bg qf qg l qh qi">from pingouin import circ_vtest<br/><br/>v, pval = circ_vtest(data['radians'], dir=np.pi)<br/>print(f"V-statistics: {v:.3f}; p-value: {pval:.6f}")</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="beed" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; V-statistics: 24.127; p-value: 0.002904</span></pre><p id="1396" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Now we’re getting somewhere! The p-value is close to zero and we reject the null hypothesis. Auri is a statistically significant South-facing pooper!</p><h1 id="6e0e" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Bayesian Poops: Math Part</h1><p id="31a8" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">And now for something completely different. Let’s try a Bayesian approach.</p><p id="1d97" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">To start, I decided to see how estimate of the mean direction changes as the sample size increased. The idea is simple: I’ll start with a circular uniform prior distribution and update it with every new data point.</p><p id="db7c" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">We’ll need to define a few things, so let’s get down to math. If you’re not a fan of equations, feel free to skip to the next section with cool visualizations!</p><h2 id="1224" class="pc mk fq bf ml pd pe pf mo pg ph pi mr no pj pk pl ns pm pn po nw pp pq pr ps bk">1. The von Mises Distribution</h2><p id="6663" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The von Mises distribution <em class="pa">p</em>(<em class="pa">θ</em>∣<em class="pa">μ</em>,<em class="pa">κ</em>) has a probability density function given by:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc qt"><img src="../Images/153d3e75814e41fee3e319b418c92d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByIj_Dpy87spOvlMpoXFdQ.png"/></div></div></figure><p id="1df8" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">where:</p><ul class=""><li id="4aa8" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><em class="pa">μ</em> is the mean direction</li><li id="1fb1" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk"><em class="pa">κ</em> is the concentration parameter (analogous to the inverse of the variance in a normal distribution)</li><li id="db0e" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk"><em class="pa">I</em>₀(<em class="pa">κ</em>) is the modified Bessel function of the first kind, ensuring the distribution is normalized</li></ul><h2 id="2476" class="pc mk fq bf ml pd pe pf mo pg ph pi mr no pj pk pl ns pm pn po nw pp pq pr ps bk">2. Prior and Likelihood</h2><p id="fccf" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Suppose we have:</p><ul class=""><li id="a3ee" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><strong class="nh fr">Prior</strong> distribution:</li></ul><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rc"><img src="../Images/8010e8d19defce9596996125e37e3f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MDPjAp38y80ursIBrItqPw.png"/></div></div></figure><ul class=""><li id="3950" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><strong class="nh fr">Likelihood</strong> for a new observation <em class="pa">θₙ</em>​:</li></ul><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rd"><img src="../Images/c9d1d390c5ee0568b67651f9e0c25829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ru8UvR6rldmWTdVpYiNnJA.png"/></div></div></figure><p id="db71" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">We want to update our prior using Bayes’ theorem:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rj"><img src="../Images/1d8f4c98f3ecf59bb25c702fea2008a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*YUJ6fBuRtNWDKJbowamdlg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ddaf" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">where</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rm"><img src="../Images/809ddcb2163e11ec9ed62d25c60d82ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mV93q-SBI1Vp8HoIv06SdQ.png"/></div></div></figure><h2 id="3d01" class="pc mk fq bf ml pd pe pf mo pg ph pi mr no pj pk pl ns pm pn po nw pp pq pr ps bk">3. Multiply the Prior and Likelihood in the von Mises Form</h2><p id="b661" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The product of two von Mises distributions with parameters (<em class="pa">μ</em>1​, <em class="pa">κ</em>1​) and (<em class="pa">μ</em>2​, <em class="pa">κ</em>2​) leads to another von Mises distribution with updated parameters. Let’s go through the steps:</p><p id="4a99" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Given:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rn"><img src="../Images/92d4f8bcc8d3c734ff8f511f99a8ba00.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*UfBKh8cRUndnKqcWPG5k3Q.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="43e9" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">and</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ro"><img src="../Images/d8f77c2f341da83aa3755011b4a9ce09.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*9jp0bcCY4Jbn1isIGLwe2g.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f5b8" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">the posterior is proportional to the product:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rp"><img src="../Images/a74b4917ed7e0983db2e356c1ff07ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ONlwuUynlbA0nBzDqAyQlg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4ab2" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Using the trigonometric identity for the sum of cosines:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rq"><img src="../Images/8d1a83615e3beff9bdfdd82522036c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*k7MIBfIMjMLDlFaatehP1Q.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="72bd" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">This becomes:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rr"><img src="../Images/97d5cac76fba491d58565a68ad31a8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*POohl_hpUjqNeK7RHIlSvA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4727" class="pc mk fq bf ml pd pe pf mo pg ph pi mr no pj pk pl ns pm pn po nw pp pq pr ps bk">4. Convert to Polar Form for the Posterior</h2><p id="294e" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Final stretch! The expression above is a von Mises distribution in disguise. We can rewrite it in the polar form to estimate the updated mean direction and concentration parameter.</p><p id="363b" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rs"><img src="../Images/38e1ec079dac7c897cfc0d05cc05dfb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YaTq2ryQcfqUSpTUSWCgQ.png"/></div></div></figure><p id="979c" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Now the expression for posterior simplifies to:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rt"><img src="../Images/fa65729c47adb45aeb8f1f47ec179f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3uIMCNfP2thna5FH-TBaw.png"/></div></div></figure><p id="109a" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let’s pause here and take a closer look at the simplified expression.</p><ol class=""><li id="9255" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa ru qv qw bk">Notice that <em class="pa">C cos⁡(θ)+S sin⁡(θ) </em>is the dot product of two vectors <em class="pa">(C,S)</em> and <em class="pa">(cos⁡(θ),sin⁡(θ))</em> which we can represent as:</li></ol><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rv"><img src="../Images/e55fa1f7ad146a555e8d99c3ccae9565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1RJI3e93YFL4PpCXH8jEg.png"/></div></div></figure><p id="ba15" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">where <em class="pa">ϕ</em> is the angle between the vectors <em class="pa">(C,S)</em> and <em class="pa">(cos⁡(θ),sin⁡(θ))</em>.</p><p id="3c50" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">2. The magnitudes of the vectors are:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rw"><img src="../Images/1269d39f1d6f2a13fb79e3ee462307fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bPEPXlMddgq3C22EcDmtA.png"/></div></div></figure><p id="4d95" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">3. The angle between the vector <em class="pa">(cos⁡(θ),sin⁡(θ))</em> and a positive x-axis is just <em class="pa">θ</em> and between (<em class="pa">C</em>,<em class="pa">S</em>) and a positive x-axis is, by definition:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rx"><img src="../Images/8d5262110a8eb0510b7fb1535870124c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wd7aNrjUxmctw3qBNxinfw.png"/></div></div></figure><p id="6065" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">4. So the angle between the two vectors is:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc ry"><img src="../Images/9add255caf9dec0d8e5df60d5562c82f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JZX2zC7FPxNCEUjf3oIu3w.png"/></div></div></figure><p id="fd85" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Substitute our findings back into the simplified expression for the posterior:</p></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc rz"><img src="../Images/1c148acca19a7dcbcaccc482d3492e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*aVOk7IRk8Y5c_Q6Triw0qQ.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b4fd" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">or</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sa"><img src="../Images/279236a278e5a9f1aa533be01b8966b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hSntQCyMvqPidItM54TQw.png"/></div></div></figure><p id="4b89" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">where</p><ul class=""><li id="926e" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><em class="pa">kappa_post</em>​ is the posterior concentration parameter:</li></ul><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sb"><img src="../Images/af71fa7345762ab766cbd3321ca5c5e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDstDnOjbpD_GqDqA2fpAw.png"/></div></div></figure><ul class=""><li id="0e43" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><em class="pa">mu_post </em>is the posterior mean direction</li></ul><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sc"><img src="../Images/89a801944a9a6d545204de03fc64779e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7oDzHJCU1UlftAUlB_PSQ.png"/></div></div></figure><p id="54c4" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Yay, we made it! The posterior is also a von Mises distribution with updated parameters (<em class="pa">mu_post</em>, <em class="pa">kappa_post</em>). Now we can update the prior with every new observation and see how the mean direction changes.</p><h1 id="10a3" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Bayesian Poops: Fun Part</h1><p id="f5c1" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Welcome back to those of you who skipped the math and congratulations to those who made it through! Let’s code the Bayesian update and vizualize the results.</p><p id="d19b" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">First, let’s define the helper functions for visualizing the posterior distribution. We’ll need to it to create a nice animation later on.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="6d8e" class="qe mk fq qa b bg qf qg l qh qi">import imageio<br/>from io import BytesIO<br/><br/>def get_posterior_distribution_image_array(<br/>    mu_grid: np.ndarray, <br/>    posterior_pdf: np.ndarray, <br/>    current_samples: List[float], <br/>    idx: int, <br/>    fig_size: Tuple[int, int], <br/>    dpi: int, <br/>    r_max_posterior: float<br/>) -&gt; np.ndarray:<br/>    """<br/>    Creates the posterior distribution and observed samples histogram on a polar plot, <br/>    converts it to an image array, and returns it for GIF processing.<br/><br/>    Parameters:<br/>    -----------<br/><br/>    mu_grid (np.ndarray): <br/>        Grid of mean direction values for plotting the posterior PDF.<br/>    posterior_pdf (np.ndarray): <br/>        Posterior probability density function values for the given `mu_grid`.<br/>    current_samples (List[float]): <br/>        List of observed angle samples in radians.<br/>    idx (int): <br/>        The current step index, used for labeling the plot.<br/>    fig_size (Tuple[int, int]): <br/>        Size of the plot figure (width, height).<br/>    dpi (int): <br/>        Dots per inch (resolution) for the plot.<br/>    r_max_posterior (float): <br/>        Maximum radius for the posterior PDF plot, used to set plot limits.<br/><br/>    Returns:<br/>        np.ndarray: Image array of the plot in RGB format, suitable for GIF processing.<br/>    """<br/>    fig = plt.figure(figsize=fig_size, dpi=dpi)<br/>    ax = plt.subplot(1, 1, 1, projection='polar')<br/>    ax.set_theta_zero_location('N')  <br/>    ax.set_theta_direction(-1)  <br/>    ax.plot(mu_grid, posterior_pdf, color='red', linewidth=2, label='Posterior PDF')<br/><br/>    # observed samples histogram<br/>    n_bins = 48<br/>    hist_bins = np.linspace(-np.pi, np.pi, n_bins + 1)<br/>    hist_counts, _ = np.histogram(current_samples, bins=hist_bins)<br/><br/>    # normalize the histogram counts<br/>    if np.max(hist_counts) &gt; 0:<br/>        hist_counts_normalized = hist_counts / np.max(hist_counts)<br/>    else:<br/>        hist_counts_normalized = hist_counts<br/><br/>    bin_centers = (hist_bins[:-1] + hist_bins[1:]) / 2<br/>    bin_width = hist_bins[1] - hist_bins[0]<br/><br/>    # set the maximum radius to accommodate both the posterior pdf and histogram bars<br/>    r_histogram_height = r_max_posterior * 0.9 <br/>    r_max = r_max_posterior + r_histogram_height<br/>    ax.set_ylim(0, r_max)<br/><br/>    # plot the histogram bars outside the circle<br/>    for i in range(len(hist_counts_normalized)):<br/>        theta = bin_centers[i]<br/>        width = bin_width<br/>        hist_height = hist_counts_normalized[i] * r_histogram_height<br/>        if hist_counts_normalized[i] &gt; 0:<br/>            ax.bar(<br/>                theta, hist_height, width=width, bottom=r_max_posterior, <br/>                color='teal', edgecolor='black', alpha=0.5<br/>            )<br/><br/>    ax.text(<br/>        0.5, 1.1, f'Posterior Distribution (Step {idx + 1})', <br/>        transform=ax.transAxes, ha='center', va='bottom', fontsize=18<br/>    )<br/>    ax.set_yticklabels([])<br/>    ax.grid(linestyle='--')<br/>    ax.yaxis.set_visible(False)<br/>    ax.spines['polar'].set_visible(False)<br/>    plt.subplots_adjust(top=0.85, bottom=0.05, left=0.05, right=0.95)<br/><br/>    # saving to buffer for gif processing<br/>    buf = BytesIO()<br/>    plt.savefig(buf, format='png', bbox_inches=None, pad_inches=0)<br/>    buf.seek(0)<br/>    img_array = plt.imread(buf)<br/>    img_array = (img_array * 255).astype(np.uint8)<br/>    plt.close(fig)<br/>    return img_array</span></pre><p id="8915" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Now we’re ready to write the update loop. Remember that we need to set our prior distribution. I’ll start with a circular uniform distribution which is equivalent to a von Mises distribution with a concentration parameter of 0. For the <em class="pa">kappa_likelihood</em> I set a fixed moderate concentration parameter of 2. That’ll make the posterior update more visible.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="bfc1" class="qe mk fq qa b bg qf qg l qh qi"># initial prior parameters<br/>mu_prior = 0.0  # initial mean direction (any value, since kappa_prior = 0)<br/>kappa_prior = 0.0  # uniform prior over the circle<br/><br/># fixed concentration parameter for the likelihood<br/>kappa_likelihood = 2.0<br/><br/>posterior_mus = []<br/>posterior_kappas = []<br/><br/>mu_grid = np.linspace(-np.pi, np.pi, 200)<br/><br/># vizualisation parameters<br/>fig_size = (10, 10)<br/>dpi = 100<br/><br/>current_samples = []<br/>frames = []<br/><br/>for idx, theta_n in enumerate(data['radians']):<br/><br/>    # compute posterior parameters<br/>    C = kappa_prior * np.cos(mu_prior) + kappa_likelihood * np.cos(theta_n)<br/>    S = kappa_prior * np.sin(mu_prior) + kappa_likelihood * np.sin(theta_n)<br/>    kappa_post = np.sqrt(C**2 + S**2)<br/>    mu_post = np.arctan2(S, C)<br/><br/>    # posterior distribution<br/>    posterior_pdf = np.exp(kappa_post * np.cos(mu_grid - mu_post)) / (2 * np.pi * i0(kappa_post))<br/><br/>    # store posterior parameters and observed samples<br/>    posterior_mus.append(mu_post)<br/>    posterior_kappas.append(kappa_post)<br/>    current_samples.append(theta_n)<br/><br/>    # plot posterior distribution<br/>    r_max_posterior = max(posterior_pdf) * 1.1<br/>    img_array = get_posterior_distribution_image_array(<br/>        mu_grid, <br/>        posterior_pdf, <br/>        current_samples, <br/>        idx, <br/>        fig_size, <br/>        dpi, <br/>        r_max_posterior<br/>        )<br/>    frames.append(img_array)<br/><br/>    # updating priors for next iteration<br/>    mu_prior = mu_post<br/>    kappa_prior = kappa_post<br/><br/># Create GIF<br/>fps = 10<br/>frames.extend([img_array]*fps*3) # repeat last frame a few times to make a "pause" at the end of the GIF<br/>imageio.mimsave('../images/posterior_updates.gif', frames, fps=fps)</span></pre><p id="ae61" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And that’s it! The code will generate a GIF showing the posterior distribution update with every new observation. Here’s the glorious result:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sd"><img src="../Images/d5a76b28902d12b5ed3cf7d2a22721d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SpBEm_q0vcy_QYf1AYAD9w.gif"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior Distribution Updates (image by author)</figcaption></figure><p id="eb89" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">With every new observation the posterior distribution gets more and more concentrated around the true mean direction. If only I could replace the red line with Auri’s silhouette, it would be perfect!</p><p id="fe80" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">We can further visualize the history of the posterior mean direction and concentration parameter. Let’s plot them:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="a86f" class="qe mk fq qa b bg qf qg l qh qi"># Convert posterior_mus to degrees<br/>posterior_mus_deg = np.rad2deg(posterior_mus) % 360<br/>n_samples = data.shape[0]<br/>true_mu = data['degrees'].mean()<br/># Plot evolution of posterior mean direction<br/>fig, ax1 = plt.subplots(figsize=(12, 6))<br/><br/>color = 'tab:blue'<br/>ax1.set_xlabel('Number of Observations')<br/>ax1.set_ylabel('Posterior Mean Direction (Degrees)', color=color)<br/>ax1.plot(range(1, n_samples + 1), posterior_mus_deg, marker='o', color=color)<br/>ax1.tick_params(axis='y', labelcolor=color)<br/>ax1.axhline(true_mu, color='red', linestyle='--', label='Sample Distribution Mean Direction')<br/>ax1.legend(loc='upper left')<br/>ax1.grid(True)<br/><br/>ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis<br/>color = 'tab:orange'<br/>ax2.set_ylabel('Posterior Concentration Parameter (kappa)', color=color)  # we already handled the x-label with ax1<br/>ax2.plot(range(1, n_samples + 1), posterior_kappas, marker='o', color=color)<br/>ax2.tick_params(axis='y', labelcolor=color)<br/><br/>fig.tight_layout()  # otherwise the right y-label is slightly clipped<br/>sns.despine()<br/>plt.title('Evolution of Posterior Mean Direction and Concentration Over Time')<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc se"><img src="../Images/d4c8d5d53048ea1461be6445f5414f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGWV60nE5BzsxA5Jm2kd5A.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior mu, kappa evolution (image by author)</figcaption></figure><p id="717b" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The plot shows how the posterior mean direction and concentration parameter evolve with every new observation. The mean direction eventually converges to the sample value, while the concentration parameter increases, as the estimate becomes more certain.</p><h1 id="b775" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Bayes Factor: PyMC for Doggy Compass</h1><p id="9bf5" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">The last thing I wanted to try was to use the <a class="af oz" href="https://en.wikipedia.org/wiki/Bayes_factor#:~:text=7%20External%20links-,Definition,The%20key%20data%2Ddependent%20term" rel="noopener ugc nofollow" target="_blank">Bayes Factor</a> approach for hypothesis testing. The idea behind the Bayes Factor is very simple: it’s <strong class="nh fr">the ratio of two marginal likelihoods </strong>for two competing hypotheses/models.</p><p id="528a" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">In general, the Bayes Factor is defined as:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sf"><img src="../Images/6710866b0a2e923660c1aabceea9cc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xHKzxBgpikGuGxILKe1Tg.png"/></div></div></figure><p id="1440" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">where:</p><ul class=""><li id="2bfd" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><em class="pa">p</em>(<em class="pa">D</em>∣<em class="pa">Mi</em>​) and <em class="pa">p</em>(<em class="pa">D</em>∣<em class="pa">Mj</em>​) are the marginal likelihoods of the data under the <em class="pa">i</em> and <em class="pa">j</em> hypothesis</li><li id="744e" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk"><em class="pa">p</em>(<em class="pa">Mi</em>​∣<em class="pa">D</em>) and <em class="pa">p</em>(<em class="pa">Mj</em>​∣<em class="pa">D</em>) are the posterior probabilities of the models given the data</li><li id="44de" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk"><em class="pa">p</em>(<em class="pa">Mi</em>​) and <em class="pa">p</em>(<em class="pa">Mj</em>​) are the prior probabilities of the models</li></ul><p id="d812" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The result is a number that tells us how much more likely one hypothesis is compared to the other. There are different approaches to interpret the Bayes Factor, but a common one is to use the Jeffreys’ scale by <a class="af oz" href="https://en.wikipedia.org/wiki/Harold_Jeffreys" rel="noopener ugc nofollow" target="_blank">Harold Jeffreys</a>:</p><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sg"><img src="../Images/0eaf9f40b882a7a1620073bbf551998e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiTVhcwxBvPZIptfmluCtQ.png"/></div></div></figure><p id="b8f7" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">What are the models you might ask? Simple! They are distributions with different parameters. I’ll be using PyMC to define the models and sample posterior distributions from them.</p><p id="f30f" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">First of all, let’s re-itroduce the null hypothesis. I still assume it’s a circular uniform Von Mises distribution with <em class="pa">kappa</em>=0 but this time we need to calculate the likelihood of the data under this hypothesis. To simplify further calculations, we’ll be calculating log-likelihoods.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="b6ad" class="qe mk fq qa b bg qf qg l qh qi"># Calculate log likelihood for H0<br/>log_likelihood_h0 = vonmises.logpdf(data['radians'], kappa=0, loc=0).sum()</span></pre><p id="c366" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Next, it’s time to build the alternative model. Starting with a simple scenario of a <strong class="nh fr">Unimodal South</strong> direction, where I assume that the distribution is concentrated around 180° or π in radians.</p><h1 id="fa89" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Unimodal South</h1><p id="97c0" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let’s define the model in PyMC. We’ll use Von Mises distribution with a fixed location parameter <em class="pa">μ</em>=<em class="pa">π</em> and a Half-Normal prior for the non-negative concentration parameter <em class="pa">κ</em>. This allows the model to learn the concentration parameter from the data and check if the South direction is preferred.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="a684" class="qe mk fq qa b bg qf qg l qh qi">import pymc as pm<br/>import arviz as az<br/>import arviz.data.inference_data as InferenceData<br/>from scipy.stats import halfnorm, gaussian_kde<br/><br/>with pm.Model() as model_uni:<br/>    # Prior for kappa <br/>    kappa = pm.HalfNormal('kappa', sigma=10)<br/>    # Likelihood<br/>    likelihood_h1 = pm.VonMises('angles', mu=np.pi, kappa=kappa, observed=data['radians'])<br/>    # Sample from posterior <br/>    trace_uni = pm.sample(<br/>        10000, tune=3000, chains=4, <br/>        return_inferencedata=True, <br/>        idata_kwargs={'log_likelihood': True})</span></pre><p id="a492" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">This gives us a nice simple model which we can also visualize:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="6391" class="qe mk fq qa b bg qf qg l qh qi"># Model graph<br/>pm.model_to_graphviz(model_uni)</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sh"><img src="../Images/593f54b39c0eee81f5a23a73261c574a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fu4PSvWpPCzTGcwBGKvn-Q.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">PyMC model graph (image by author)</figcaption></figure><p id="b54e" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And here’s the posterior distribution for the concentration parameter <em class="pa">κ</em>:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="cbda" class="qe mk fq qa b bg qf qg l qh qi">az.plot_posterior(trace_uni, var_names=['kappa'])<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc si"><img src="../Images/3fa40f32492d0ce6d1e920ef4f734a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UN9W1MqwzeeC29tPXGmTg.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior kappa distribution (image by author)</figcaption></figure><p id="dbde" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">All that’s left is to calculate the log-likelihood for the alternative model and the Bayes Factor.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="8d39" class="qe mk fq qa b bg qf qg l qh qi"># Posterior samples for kappa<br/>kappa_samples = trace_uni.posterior.kappa.values.flatten()<br/># Log likelihood for each sample<br/>log_likes = []<br/>for k in kappa_samples:<br/>    # Von Mises log likelihood<br/>    log_like = vonmises.logpdf(data['radians'], k, loc=np.pi).sum()<br/>    log_likes.append(log_like)<br/># Log-mean-exp trick for numerical stability<br/>log_likelihood_h1 = np.max(log_likes) +\<br/>       np.log(np.mean(np.exp(log_likes - np.max(log_likes))))<br/>BF = np.exp(log_likelihood_h1 - log_likelihood_h0)<br/>print(f"Bayes Factor: {BF:.4f}")<br/>print(f"Probability kappa &gt; 0.5: {np.mean(kappa_samples &gt; 0.5):.4f}")</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="5d05" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Bayes Factor: 32.4645<br/>&gt;&gt; Probability kappa &gt; 0.5: 0.0649</span></pre><p id="f653" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Since we’re dividing the likelihood of the alternative model by the likelihood of the null model, the Bayes Factor indicates how much more likely the data is under the alternative hypothesis. In this case, we get 32.46, a very strong evidence, suggesting that the data is <strong class="nh fr">not uniformly distributed</strong> around the circle and there is <strong class="nh fr">a preference for the South direction</strong>.</p><p id="8ba5" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">However, we additionally calculate the probability that the concentration parameter <em class="pa">kappa</em> is greater than 0.5. This is a simple way to check if the distribution is significantly different from the uniform one. With the Unimodal South model, this probabiliy is only 0.0649, meaning that the distribution is still quite spread out.</p><p id="9342" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let’s try another model: <strong class="nh fr">Bimodal North-South Mixture</strong>.</p><h1 id="dc78" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Bimodal North-South Mixture</h1><p id="e37d" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This time I’ll assume that the distribution is bimodal with peaks around 0° and 180°, just as we’ve seen in the compass rose.</p><p id="df6d" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">To achieve this, I’ll need a mixture of two Von Mises distributions with different fixed mean directions and a shared concentration parameter.</p><p id="2584" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let’s define a few helper functions first:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="4d28" class="qe mk fq qa b bg qf qg l qh qi"># Type aliases<br/>ArrayLike = Union[np.ndarray, pd.Series]<br/>ResultDict = Dict[str, Union[float, InferenceData.InferenceData]]<br/><br/>def compute_mixture_vonmises_logpdf(<br/>    series: ArrayLike,<br/>    kappa: float,<br/>    weights: npt.NDArray[np.float64],<br/>    mus: List[float]<br/>) -&gt; float:<br/>    """<br/>    Compute log PDF for a mixture of von Mises distributions<br/>    <br/>    Parameters:<br/>    -----------<br/>    series: ArrayLike <br/>        Array of observed angles in radians<br/>    kappa: float<br/>        Concentration parameter<br/>    weights: npt.NDArray[np.float64],<br/>        Array of mixture weights<br/>    mus: List[float] <br/>        Array of means for each component<br/>    <br/>    Returns:<br/>    --------<br/>    float: Sum of log probabilities for all data points<br/>    """<br/>    mixture_pdf = np.zeros_like(series)<br/>    <br/>    for w, mu in zip(weights, mus):<br/>        mixture_pdf += w * vonmises.pdf(series, kappa, loc=mu)<br/>    <br/>    return np.log(np.maximum(mixture_pdf, 1e-300)).sum()<br/><br/>def compute_log_likelihoods(<br/>    trace: az.InferenceData, <br/>    series: ArrayLike,<br/>    mus: List[float]<br/>    ) -&gt; np.ndarray:<br/>    """<br/>    Compute log likelihoods for each sample in the trace<br/><br/>    Parameters:<br/>    -----------<br/>    trace: az.InferenceData<br/>        The trace from the PyMC3 model sampling.<br/><br/>    series: ArrayLike<br/>        Array of observed angles in radians<br/>    <br/>    """<br/><br/>    kappa_samples = trace.posterior.kappa.values.flatten()<br/>    weights_samples = trace.posterior.weights.values.reshape(-1, 2)<br/>    # Calculate log likelihood for each posterior sample<br/>    log_likes = []<br/>    for k, w in zip(kappa_samples, weights_samples):<br/>        log_like = compute_mixture_vonmises_logpdf(<br/>            series, <br/>            kappa=k, <br/>            weights=w, <br/>            mus=mus<br/>        )<br/>        log_likes.append(log_like)<br/>    <br/>    # Calculate marginal likelihood using log-sum-exp trick<br/>    log_likelihood_h1 = np.max(log_likes) + np.log(np.mean(np.exp(log_likes - np.max(log_likes))))<br/>    return log_likelihood_h1<br/><br/>def posterior_report(<br/>    log_likelihood_h0: float, <br/>    log_likelihood_h1: float, <br/>    kappa_samples: ArrayLike,<br/>    kappa_threshold: float = 0.5<br/>    ) -&gt; str:<br/><br/>    """<br/>    Generate a report with Bayes Factor and probability kappa &gt; threshold<br/><br/>    Parameters:<br/>    -----------<br/>    log_likelihood_h0: float<br/>        Log likelihood for the null hypothesis<br/>    log_likelihood_h1: float<br/>        Log likelihood for the alternative hypothesis<br/>    kappa_samples: ArrayLike<br/>        Flattened posterior samples of the concentration parameter<br/>    kappa_threshold: float<br/>        Threshold for computing the probability that kappa &gt; threshold<br/><br/>    Returns:<br/>    --------<br/>    summary: str<br/>        A formatted string containing the summary statistics.<br/>    """<br/>    BF = np.exp(log_likelihood_h1 - log_likelihood_h0)<br/><br/>    summary = (<br/>        f"Bayes Factor: {BF:.4f}\n"<br/>        f"Probability kappa &gt; {kappa_threshold}: {np.mean(kappa_samples &gt; kappa_threshold):.4f}"<br/>    )<br/><br/>    return summary</span></pre><p id="5202" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And now back to the model:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="c57f" class="qe mk fq qa b bg qf qg l qh qi">mu1 = 0            # 0 degrees<br/>mu2 = np.pi        # 180 degrees<br/><br/>with pm.Model() as model_mixture_bimodal_NS:<br/>    # Priors for concentration parameters<br/>    kappa = pm.HalfNormal('kappa', sigma=10) <br/>    # Priors for component weights<br/>    weights = pm.Dirichlet('weights', a=np.ones(2))<br/>    <br/>    # Define the von Mises components<br/>    vm1 = pm.VonMises.dist(mu=mu1, kappa=kappa)<br/>    vm2 = pm.VonMises.dist(mu=mu2, kappa=kappa)<br/>    <br/>    # Mixture distribution<br/>    likelihood = pm.Mixture(<br/>        'angles',<br/>        w=weights,<br/>        comp_dists=[vm1, vm2],<br/>        observed=data['radians']<br/>    )<br/>    <br/>    # Sample from the posterior<br/>    trace_mixture_bimodal_NS = pm.sample(<br/>        10000, tune=3000, chains=4, return_inferencedata=True, idata_kwargs={'log_likelihood': True})<br/>    <br/>    # Get kappa samples<br/>    kappa_samples = trace_mixture_bimodal_NS.posterior.kappa.values.flatten()</span></pre><p id="87f9" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Once again, let’s visualize the model graph and the posterior distribution for the concentration parameter <em class="pa">κ</em>:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="ee7e" class="qe mk fq qa b bg qf qg l qh qi"># Model graph<br/>pm.model_to_graphviz(model_mixture_bimodal_NS)</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sj"><img src="../Images/222efe40ea36448f214e2a79f610f7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHWAGo-2fnQrTcBf-07LcQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">PyMC model graph (image by author)</figcaption></figure><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="2467" class="qe mk fq qa b bg qf qg l qh qi"># Posterior Analysis<br/>az.plot_posterior(trace_mixture_bimodal_NS, var_names=['kappa'])<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sk"><img src="../Images/207caa35505a70f08a1b9cc1e25fe3c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWKtJPEIVdQron13kG4Npg.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior kappa distribution (image by author)</figcaption></figure><p id="f3cd" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And finally, let’s calculate the Bayes Factor and the probability that the concentration parameter <em class="pa">κ</em> is greater than 0.5:</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="d535" class="qe mk fq qa b bg qf qg l qh qi">log_likelihood_h1 = compute_log_likelihoods(trace_mixture_bimodal_NS, data['radians'], [mu1, mu2])<br/>print(posterior_report(log_likelihood_h0, log_likelihood_h1, kappa_samples))</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="4a77" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Bayes Factor: 214.2333<br/>&gt;&gt; Probability kappa &gt; 0.5: 0.9110</span></pre><p id="8767" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk"><strong class="nh fr">Fantastic!</strong> Both our metrics indicate that this model is a much better fit for the data. The Bayes Factor suggests a <strong class="nh fr">decisive evidence</strong> and most of the posterior <em class="pa">κ</em> samples are greater than 0.5 with the mean value of 0.99 as we’ve seen on the distribution plot.</p><p id="ec8c" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Let’s try a couple more models before wrapping up.</p><h1 id="5d9c" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Bimodal West-South Mixture</h1><p id="8904" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">This model once again assumes a bimodal distribution but this time with peaks around 270° and 180°, which were common directions in the compass rose.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="bf2a" class="qe mk fq qa b bg qf qg l qh qi">mu1 = np.pi          # 180 degrees<br/>mu2 = 3 * np.pi / 2  # 270 degrees<br/><br/>with pm.Model() as model_mixture_bimodal_WS:<br/>    # Priors for concentration parameters<br/>    kappa = pm.HalfNormal('kappa', sigma=10)<br/>    <br/>    # Priors for component weights<br/>    weights = pm.Dirichlet('weights', a=np.ones(2))<br/>    <br/>    # Define the four von Mises components<br/>    vm1 = pm.VonMises.dist(mu=mu1, kappa=kappa)<br/>    vm2 = pm.VonMises.dist(mu=mu2, kappa=kappa)<br/>    <br/>    # Mixture distribution<br/>    likelihood = pm.Mixture(<br/>        'angles',<br/>        w=weights,<br/>        comp_dists=[vm1, vm2],<br/>        observed=data['radians']<br/>    )<br/>    <br/>    # Sample from the posterior<br/>    trace_mixture_bimodal_WS = pm.sample(<br/>        10000, tune=3000, chains=4, return_inferencedata=True, idata_kwargs={'log_likelihood': True})<br/>    <br/>    # Get kappa samples<br/>    kappa_samples = trace_mixture_bimodal_WS.posterior.kappa.values.flatten()<br/><br/># Posterior Analysis<br/>az.plot_posterior(trace_mixture_bimodal_WS, var_names=['kappa'])<br/>plt.show()<br/><br/>log_likelihood_h1 = compute_log_likelihoods(trace_mixture_bimodal_WS, data['radians'], [mu1, mu2])<br/>print(posterior_report(log_likelihood_h0, log_likelihood_h1, kappa_samples))</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="e8e1" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Bayes Factor: 20.2361<br/>&gt;&gt; Probability kappa &gt; 0.5: 0.1329</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sl"><img src="../Images/5219e45a7ce051409efb73ec8d109d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPqa33fXfgBG3SeOl0WLtQ.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior kappa distribution (image by author)</figcaption></figure><p id="ebbf" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Nope, definitely not as good as the previous model. Next!</p><h1 id="35d9" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Quadrimodal Mixture</h1><p id="2ad9" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Final round. Maybe my dog really likes to align himself with the cardinal directions? Let’s try a quadrimodal distribution with peaks around 0°, 90°, 180°, and 270°.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="85f6" class="qe mk fq qa b bg qf qg l qh qi">mu1 = 0            # 0 degrees<br/>mu2 = np.pi / 2    # 90 degrees<br/>mu3 = np.pi        # 180 degrees<br/>mu4 = 3 * np.pi / 2  # 270 degrees<br/><br/>with pm.Model() as model_mixture_quad:<br/>    # Priors for concentration parameters<br/>    kappa = pm.HalfNormal('kappa', sigma=10)<br/>    <br/>    # Priors for component weights<br/>    weights = pm.Dirichlet('weights', a=np.ones(4))<br/>    <br/>    # Define the four von Mises components<br/>    vm1 = pm.VonMises.dist(mu=mu1, kappa=kappa)<br/>    vm2 = pm.VonMises.dist(mu=mu2, kappa=kappa)<br/>    vm3 = pm.VonMises.dist(mu=mu3, kappa=kappa)<br/>    vm4 = pm.VonMises.dist(mu=mu4, kappa=kappa)<br/>    <br/>    # Mixture distribution<br/>    likelihood = pm.Mixture(<br/>        'angles',<br/>        w=weights,<br/>        comp_dists=[vm1, vm2, vm3, vm4],<br/>        observed=data['radians']<br/>    )<br/>    <br/>    # Sample from the posterior<br/>    trace_mixture_quad = pm.sample(<br/>        10000, tune=3000, chains=4, return_inferencedata=True, idata_kwargs={'log_likelihood': True}<br/>    )<br/>    # Get kappa samples<br/>    kappa_samples = trace_mixture_quad.posterior.kappa.values.flatten()<br/># Posterior Analysis<br/>az.plot_posterior(trace_mixture_quad, var_names=['kappa'])<br/>plt.show()<br/>log_likelihood_h1 = compute_log_likelihoods(trace_mixture_quad, data['radians'], [mu1, mu2, mu3, mu4])<br/>print(posterior_report(log_likelihood_h0, log_likelihood_h1, kappa_samples))</span></pre><pre class="qm qb qa qc bp qd bb bk"><span id="79cd" class="qe mk fq qa b bg qf qg l qh qi">&gt;&gt; Bayes Factor: 0.0000<br/>&gt;&gt; Probability kappa &gt; 0.5: 0.9644</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sm"><img src="../Images/7255e38fa56766715c59510730e66198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PIF3Gq9Km05_tsZuxjuetg.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Posterior kappa distribution (image by author)</figcaption></figure><p id="00f2" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">Well… Not really. Although the probability that the concentration parameter κ<em class="pa">κ</em> is greater than 0.5 is quite high, the Bayes Factor is 0.0.</p><p id="01c4" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">The great thing about Bayes Factor is that it penalizes overly complex models effectively preventing overfitting.</p><h1 id="35a6" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Model Comparison</h1><p id="f754" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let’s summarize the results of all models using information criteria. We’ll use the <a class="af oz" href="https://en.wikipedia.org/wiki/Watanabe%E2%80%93Akaike_information_criterion#:~:text=In%20statistics%2C%20the%20Widely%20Applicable,it%20wasn't%20trained%20on." rel="noopener ugc nofollow" target="_blank">Widely Applicable Information Criterion</a> (WAIC) and the Leave-One-Out Cross-Validation (LOO) to compare the models.</p><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="b9ac" class="qe mk fq qa b bg qf qg l qh qi"># Compute WAIC for each model<br/>wail_uni = az.waic(trace_uni)<br/>waic_quad = az.waic(trace_mixture_quad)<br/>waic_bimodal_NS = az.waic(trace_mixture_bimodal_NS)<br/>waic_bimodal_WS = az.waic(trace_mixture_bimodal_WS)<br/><br/>model_dict = {<br/>    'Quadrimodal Model': trace_mixture_quad,<br/>    'Bimodal Model (NS)': trace_mixture_bimodal_NS,<br/>    'Bimodal Model (WS)': trace_mixture_bimodal_WS,<br/>    'Unimodal Model': trace_uni <br/>}<br/># Compare models using WAIC<br/>waic_comparison = az.compare(model_dict, ic='waic')<br/>waic_comparison</span></pre></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sn"><img src="../Images/933f6bcd2e583901690ea52c834927aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*KaBLaPo3El4KZ9ogDg8v9Q.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="13d1" class="qe mk fq qa b bg qf qg l qh qi"># Compare models using LOO<br/>loo_comparison = az.compare(model_dict, ic='loo')<br/>loo_comparison</span></pre></div></div><div class="oj"><div class="ab cb"><div class="lm re ln rf lo rg cf rh cg ri ci bh"><figure class="oe of og oh oi oj rk rl paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc so"><img src="../Images/9efe5507cc67224a3d34af59f266bc41.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*BiAK0vgEex4TRpIDaYLaQA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><pre class="oe of og oh oi qb qa qc bp qd bb bk"><span id="42d5" class="qe mk fq qa b bg qf qg l qh qi"># Visualize the comparison<br/>az.plot_compare(waic_comparison)<br/>plt.show()</span></pre><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sp"><img src="../Images/043cc2eaa59b47d70b13d446fdb79308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyNR1uhd6F6ajjXhRLvCfA.png"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">WAIC comparison (image by author)</figcaption></figure><p id="7bc9" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And we have a winner! The <strong class="nh fr">Bimodal North-South model</strong> is the best fit for the data according to both WAIC and LOO.</p><h1 id="bd35" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk">Conclusion</h1><figure class="oe of og oh oi oj ob oc paragraph-image"><div role="button" tabindex="0" class="ok ol ed om bh on"><div class="ob oc sq"><img src="../Images/b56b077782d1353c637f65c70d300be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qVEXmPZembAxfRYGSy-Dfg.jpeg"/></div></div><figcaption class="op oq or ob oc os ot bf b bg z dx">Christmas Auri (image by author)</figcaption></figure><p id="2501" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">What a journey! What started a few months ago as a simple observation of my dog’s pooping habits turned into a full-blown Bayesian analysis.</p><p id="83b1" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">In this article, I’ve shown how to model circular data, estimate the mean direction and concentration parameter, and update the posterior distribution with new observations. We’ve also seen how to use the Bayes Factor for hypothesis testing and compare models using information criteria.</p><p id="4686" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">And the results were super interesting! Auri does indeed have preferences and somehow manages to align himself across the North-South axis. If I ever get lost in the woods with my dog, I know what direction to follow. Just need a big enough sample size to be sure!</p><p id="e4a8" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">I hope you enjoyed this journey as much as I did. If you have any questions or suggestions, feel free to reach out. And if you’d like to support my work, consider buying me a coffee ❤️</p><figure class="oe of og oh oi oj ob oc paragraph-image"><a href="https://www.buymeacoffee.com/datawondering"><div class="ob oc sr"><img src="../Images/961c46077716eccc51e2bd91a0299d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q0ti6F33refKgInIJfpyng.png"/></div></a></figure><p id="85b9" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">My socials:</p><ul class=""><li id="4b0e" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk"><a class="af oz" href="https://substack.com/@datawondering" rel="noopener ugc nofollow" target="_blank">Data Wondering blog [English]</a></li><li id="afbe" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk"><a class="af oz" href="https://t.me/data_wondering" rel="noopener ugc nofollow" target="_blank">Data Wondering blog [Russian]</a></li></ul><p id="c49b" class="pw-post-body-paragraph nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa fj bk">References:</p><ul class=""><li id="db3c" class="nf ng fq nh b go ou nj nk gr ov nm nn no ow nq nr ns ox nu nv nw oy ny nz oa qu qv qw bk">Dogs are sensitive to small variations of the Earth’s magnetic field, Vlastimil Hart et al., <a class="af oz" href="https://frontiersinzoology.biomedcentral.com/counter/pdf/10.1186/1742-9994-10-80.pdf" rel="noopener ugc nofollow" target="_blank">[link]</a></li><li id="3640" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk">Biostatistical Analysis, Fifth Edition, Jerrold H. Zar, <a class="af oz" href="https://bayesmath.com/wp-content/uploads/2021/05/Jerrold-H.-Zar-Biostatistical-Analysis-5th-Edition-Prentice-Hall-2009.pdf" rel="noopener ugc nofollow" target="_blank">[link]</a></li><li id="853b" class="nf ng fq nh b go qx nj nk gr qy nm nn no qz nq nr ns ra nu nv nw rb ny nz oa qu qv qw bk">PyMC, Probabilistic Programming in Python, <a class="af oz" href="https://www.pymc.io/welcome.html" rel="noopener ugc nofollow" target="_blank">[link]</a></li></ul></div></div></div></div>    
</body>
</html>