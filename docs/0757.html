<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Fine-Tune a Pretrained Vision Transformer on Satellite Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Fine-Tune a Pretrained Vision Transformer on Satellite Data</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=collection_archive---------7-----------------------#2024-03-21">https://towardsdatascience.com/how-to-fine-tune-a-pretrained-vision-transformer-on-satellite-data-d0ddd8359596?source=collection_archive---------7-----------------------#2024-03-21</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="18b3" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">A step-by-step tutorial in PyTorch Lightning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@caroline.arnold_63207?source=post_page---byline--d0ddd8359596--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Caroline Arnold" class="l ep by dd de cx" src="../Images/fb13ba36e302d8161b67c4888d0601e4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*rMa21dr4FkLAt8NY-pdWRw.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d0ddd8359596--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@caroline.arnold_63207?source=post_page---byline--d0ddd8359596--------------------------------" rel="noopener follow">Caroline Arnold</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d0ddd8359596--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">1</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/052cfa4a7466bc057f2271ab6f3aaa70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDfkp-iW-jEa1hJvu-yiig.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image created by the author using <a class="af nd" href="https://www.midjourney.com/jobs/dc3e78a3-9782-4624-82dd-5d4df21bb17d?index=1" rel="noopener ugc nofollow" target="_blank">Midjourney</a>.</figcaption></figure><p id="5878" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The <a class="af nd" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> is a powerful AI model for image classification. Released in 2020, it brought the efficient transformer architecture to computer vision.</p><p id="00b1" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">In pretraining, an AI model ingests large amounts of data and learns common patterns. The Vision Transformer was pretrained on <a class="af nd" href="https://arxiv.org/abs/2104.10972" rel="noopener ugc nofollow" target="_blank">ImageNet-21K</a>, a dataset of 14 million images and 21,000 classes.</p><p id="793b" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Satellite images are not covered in ImageNet-21K, and the Vision Transformer would perform poorly if applied out-of-the-box.</p><p id="ec93" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Here, I will show you how to fine-tune a pretrained Vision Transformer on 27,000 satellite images from the <a class="af nd" href="https://zenodo.org/records/7711810#.ZAm3k-zMKEA" rel="noopener ugc nofollow" target="_blank">EuroSat dataset</a>. We will predict land cover, such as forests, crops, and industrial areas.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml oa"><img src="../Images/5dc989f9eba1a2ab016f8d1fb6985575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUM77GVE_48dxSWNtnAF2A.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Example images from the <a class="af nd" href="https://zenodo.org/records/7711810#.ZAm3k-zMKEA" rel="noopener ugc nofollow" target="_blank">EuroSAT RGB dataset</a>. Sentinel data is free and open to the public under EU law.</figcaption></figure><p id="382d" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">We will work in <a class="af nd" href="https://lightning.ai" rel="noopener ugc nofollow" target="_blank">PyTorch Lightning</a>, a deep learning library that builds on PyTorch. Lightning reduces the amount of code one has to write, and lets us focus on modeling.</p><p id="fb0e" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">All code is available on <a class="af nd" href="https://github.com/crlna16/pretrained-vision-transformer/" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><h2 id="f48d" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Setting up the project</h2><p id="ae78" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">The pretrained Vision Transformer is available on <!-- -->Huggingface<!-- -->. The model architecture and weights can be installed from GitHub. We will also need to install PyTorch Lightning. I used version 2.2.1 for this tutorial, but any version &gt; 2.0 should work.</p><pre class="mn mo mp mq mr pb pc pd bp pe bb bk"><span id="fb0d" class="pf oc fr pc b bg pg ph l pi pj">pip install -q git+https://github.com/huggingface/transformers<br/>pip install lightning=2.2.1</span></pre><p id="8ca7" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">We can split our project in four steps, which we will cover in detail:</p><ul class=""><li id="1797" class="ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz pk pl pm bk">Pretrained Vision Transformer: <a class="af nd" href="https://lightning.ai/docs/pytorch/stable/common/lightning_module.html" rel="noopener ugc nofollow" target="_blank">Lightning Module</a></li><li id="2eb6" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">EuroSAT dataset</li><li id="d7a5" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Train the Vision Transformer on the EuroSAT dataset</li><li id="8530" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Calculate the accuracy on the test set</li></ul><h2 id="e052" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Adapting the Vision Transformer to our dataset</h2><p id="8767" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">The Vision Transformer from Huggingface is optimized for a subset of ImageNet with 1,000 classes.</p><p id="24ca" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Our dataset contains only 10 classes for different types of land cover. Therefore, we need to modify the output section of the Vision Transformer to a new classification head with the correct number of classes.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml ps"><img src="../Images/69ef618dc1fda224692e94b5a4d2833e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DoCukeBBkjo2ssmUlovchQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Vision Transformer architecture. Adapted by the author from the original paper. <a class="af nd" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">[arxiv]</a></figcaption></figure><p id="f06c" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The code for instantiating a pretrained model from Huggingface makes this straightforward. We only need to specify the new number of classes by <em class="pt">num_labels</em>, and tell the model to ignore the fact we changed the output size.</p><pre class="mn mo mp mq mr pb pc pd bp pe bb bk"><span id="b9e4" class="pf oc fr pc b bg pg ph l pi pj">from transformers import ViTForImageClassification<br/>ViTForImageClassification.from_pretrained("google/vit-base-patch16-224", <br/>                                          num_labels=10, <br/>                                          ignore_mismatched_sizes=True<br/>                                          )</span></pre><p id="4bfc" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The model reminds us that we now need to re-train:</p><blockquote class="pu pv pw"><p id="ad27" class="ne nf pt ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16–224 and are newly initialized because the shapes did not match:<br/>…<br/>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p></blockquote><p id="018f" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">We can choose different flavours of the Vision Transformer, here we stick with <em class="pt">vit-base-patch16–224</em>, the smallest model that uses <em class="pt">16 x 16</em> patches from images with a size of <em class="pt">224 x 224</em> pixels. This model has 85.8 million parameters and requires 340 MB of memory.</p><h2 id="b865" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Vision Transformer as a Lightning Module</h2><p id="aa75" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">In PyTorch Lightning, a deep learning model is defined as a <a class="af nd" href="https://lightning.ai/docs/pytorch/stable/common/lightning_module.html" rel="noopener ugc nofollow" target="_blank">Lightning Module</a>. We only need to specify</p><ul class=""><li id="a1f8" class="ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz pk pl pm bk">Setup of the model: Load the pretrained Vision Transformer</li><li id="5cb9" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Forward step: Apply the model to a batch of data</li><li id="23c3" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Training, validation, and test step</li><li id="3457" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">The optimizer to be used in training</li></ul><figure class="mn mo mp mq mr ms"><div class="px ip l ed"><div class="py pz l"/></div></figure><p id="2216" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The training step must return the loss, in this case the cross-entropy loss to quantify the mismatch between the predicted and the true classes.</p><p id="72b2" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Logging is convenient. With calls to <em class="pt">self.log</em>, we can log training and evaluation metrics directly to our preferred logger — in my case, <a class="af nd" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank">TensorBoard</a>. Here, we log the training loss and the validation accuracy.</p><p id="f34a" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Note that, in order to access the predictions made by Huggingface’s Vision Transformer, we need to retrieve them from the model output as <em class="pt">predictions.logits.</em></p><h2 id="26e1" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Lightning DataModule for the EuroSAT dataset</h2><p id="cf81" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">You can download the EuroSAT dataset from <a class="af nd" href="https://zenodo.org/records/7711810#.ZAm3k-zMKEA" rel="noopener ugc nofollow" target="_blank">Zenodo</a>. Make sure to select the RGB version, which has already been converted from the original satellite image. We will define the dataset within a <a class="af nd" href="https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/datamodules.html#Using-DataModules" rel="noopener ugc nofollow" target="_blank">LightningDataModule</a>.</p><p id="a55f" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The setup stage uses <a class="af nd" href="https://pytorch.org/vision/0.17/" rel="noopener ugc nofollow" target="_blank">torchvision</a> transform functions. In order to comply with the input that is expected by the Vision Transformer, we need to upscale the satellite images to 224 x 224 pixels, convert the images to torch datatypes and normalize them.</p><p id="5b14" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">We split the dataset so that 70% remain for training (fine-tuning), 10% for validation, and 20% for testing. By stratifying on the class labels, we ensure an equal distribution of classes across all three subsets.</p><figure class="mn mo mp mq mr ms"><div class="px ip l ed"><div class="py pz l"/></div></figure><p id="a10a" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The functions <em class="pt">train_dataloader</em> etc. are convenient for setting up the dataloaders later in the run script.</p><h2 id="c19a" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Putting it all together: the run script</h2><p id="0955" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">Now that we have the building blocks for the dataset and the model, we can write a run script that performs the fine tuning.</p><p id="9a3b" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">For clarity, I created separate modules for the dataset (<em class="pt">eurosat_module</em>) and the model (<em class="pt">vision_transformer</em>) that need to be imported.</p><figure class="mn mo mp mq mr ms"><div class="px ip l ed"><div class="py pz l"/></div></figure></div></div></div><div class="ab cb qa qb qc qd" role="separator"><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="89bd" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Train the model</h2><p id="705b" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">The <a class="af nd" href="https://lightning.ai/docs/pytorch/stable/common/trainer.html" rel="noopener ugc nofollow" target="_blank">Lightning Trainer</a> takes a model and dataloaders for training and validation data. It offers a variety of flags that can be customized to your needs — here we use only three of them:</p><ul class=""><li id="3260" class="ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz pk pl pm bk"><em class="pt">devices</em>, to use only one GPU for training</li><li id="a38d" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk"><em class="pt">early stopping callback</em>, to stop training if the validation loss does not improve for six epochs</li><li id="ba1a" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk"><em class="pt">logger</em>, to log the training process to TensorBoard</li></ul><p id="e1a9" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The beauty of PyTorch Lightning is that training is now done in one line of code:</p><pre class="mn mo mp mq mr pb pc pd bp pe bb bk"><span id="6a03" class="pf oc fr pc b bg pg ph l pi pj">trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=valid_dataloader)</span></pre><p id="da2b" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Under the hood, the trainer uses backward propagation and the Adam optimizer to update the model weights. It stops training when the validation accuracy has not improved for the specified number of epochs.</p><p id="f3a4" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">In fact, fine-tuning on EuroSAT is completed within few epochs. The panel shows the training loss, as logged by TensorBoard. After two epochs, the model has already reached 98.3% validation accuracy.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qi"><img src="../Images/1b58b4a473a4f6791d71481a7d1b4de7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPGIwAN_1c-0vo-dQ3qk7A.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Snapshot from TensorBoard created by the author.</figcaption></figure></div></div></div><div class="ab cb qa qb qc qd" role="separator"><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="bbe4" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Evaluate on the test set</h2><p id="889a" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">Our dataset is small and fine-tuning is fast, so we do not need to store the trained model separately and can apply it directly to the test set.</p><p id="67ec" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">In one line of code, we compute the accuracy on the test set:</p><pre class="mn mo mp mq mr pb pc pd bp pe bb bk"><span id="53a9" class="pf oc fr pc b bg pg ph l pi pj">trainer.test(model=model, dataloaders=test_dataloader, verbose=True)</span></pre><p id="9d3c" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">With only one epoch of fine-tuning, I achieved a test set accuracy of 98.4%, meaning that the land cover types were correctly classified for almost all satellite images.</p><p id="fc58" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Even with less samples, we can achieve great accuracy. The panel shows the test set accuracy for different numbers of training samples seen only once during fine-tuning. With only 320 satellite images, an average of 32 per class, the test set accuracy is already 80%.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml qj"><img src="../Images/fe1672f643e4eeda9b56bd587156dbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSjqc5l0m9spFNFILDHINQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Image created by the author.</figcaption></figure><h2 id="a08c" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">Key takeaways</h2><p id="134e" class="pw-post-body-paragraph ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz fk bk">Pretrained models are a great way to reduce your training time. They are already good at a general task and just need to be adapted to your specific dataset.</p><p id="36b7" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">In real-world applications, data is often scarce. The EuroSAT dataset consists of only 27,000 images, about 0.5% the magnitude of ImageNet-21K. Pretraining takes advantage of larger datasets, and we can efficiently use the application-specific dataset for fine-tuning.</p><p id="ca44" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">Lightning is great for training deep learning models without having to worry about all the technical details. The LightningModule and the Trainer API offer convenient abstractions and are performant.</p><p id="9840" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">If you want to stay within the Huggingface ecosystem to fine-tune a Vision Transformer, I recommend <a class="af nd" href="https://medium.com/@supersjgk/fine-tuning-vision-transformer-with-hugging-face-and-pytorch-df19839d5396" rel="noopener">this tutorial</a>.</p><p id="1ca7" class="pw-post-body-paragraph ne nf fr ng b gp nh ni nj gs nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz fk bk">The complete code, including configuration files that allow you to add your own datasets, is available on GitHub:</p><div class="qk ql qm qn qo qp"><a href="https://github.com/crlna16/pretrained-vision-transformer?source=post_page-----d0ddd8359596--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qq ab ih"><div class="qr ab co cb qs qt"><h2 class="bf fs hx z ip qu ir is qv iu iw fq bk">GitHub - crlna16/pretrained-vision-transformer: Pretrained Vision Transformer with PyTorch…</h2><div class="qw l"><h3 class="bf b hx z ip qu ir is qv iu iw dx">Pretrained Vision Transformer with PyTorch Lightning - crlna16/pretrained-vision-transformer</h3></div><div class="qx l"><p class="bf b dy z ip qu ir is qv iu iw dx">github.com</p></div></div><div class="qy l"><div class="qz l ra rb rc qy rd ls qp"/></div></div></a></div></div></div></div><div class="ab cb qa qb qc qd" role="separator"><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg qh"/><span class="qe by bm qf qg"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="cde1" class="ob oc fr bf od oe of og oh oi oj ok ol nn om on oo nr op oq or nv os ot ou ov bk">References</h2><ul class=""><li id="5fa1" class="ne nf fr ng b gp ow ni nj gs ox nl nm nn oy np nq nr oz nt nu nv pa nx ny nz pk pl pm bk">EuroSAT Dataset: <a class="af nd" href="https://zenodo.org/doi/10.5281/zenodo.7711096" rel="noopener ugc nofollow" target="_blank">10.5281/zenodo.7711096</a> (<a class="af nd" href="https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice" rel="noopener ugc nofollow" target="_blank">Copernicus Sentinel Data License</a>)</li><li id="8690" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Dosovitskiy et al, <em class="pt">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>, <a class="af nd" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.11929</a> (2020)</li><li id="2d73" class="ne nf fr ng b gp pn ni nj gs po nl nm nn pp np nq nr pq nt nu nv pr nx ny nz pk pl pm bk">Vision Transformer on <a class="af nd" href="https://huggingface.co/docs/transformers/model_doc/vit#vision-transformer-vit" rel="noopener ugc nofollow" target="_blank">Huggingface</a> and <a class="af nd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">GitHub</a> (Apache 2.0)</li></ul></div></div></div></div>    
</body>
</html>