<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into LlaMA 3 by Hand ✍️</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into LlaMA 3 by Hand ✍️</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03">https://towardsdatascience.com/deep-dive-into-llama-3-by-hand-%EF%B8%8F-6c6b23dc92b2?source=collection_archive---------0-----------------------#2024-05-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a878" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the nuances of the transformer architecture behind Llama 3 and its prospects for the GenAI ecosystem</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--6c6b23dc92b2--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c6b23dc92b2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">17</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/74386917c561d9966bec6d17337785ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7FiX5znXyFDlK3qgBJuPw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author (The shining LlaMA 3 rendition by my 4-year old.)</figcaption></figure><p id="b8eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">“In the rugged mountain of the Andes, lived three very beautiful creatures — Rio, Rocky and Sierra. With their lustrous coat and sparkling eyes, they stood out as a beacon of strength and resilience.</em></p><p id="1899" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">As the story goes, it was said that from a very young age their thirst for knowledge was never-ending. They would seek out the wise elders of their herd, listening intently to their stories and absorbing their wisdom like a sponge. With that grew their superpower which was working together with others and learning that teamwork was the key to acing the trials in the challenging terrain of the Andes.</em></p><p id="01eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">If they encountered travelers who had lost their way or needed help, Rio took in their perspective and led them with comfort, Rocky provided swift solutions while Sierra made sure they had the strength to carry on. And with this they earned admiration and encouraged everyone to follow their example.</em></p><p id="ca64" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">As the sun set over the Andes, Rio, Rocky, and Sierra stood together, their spirits intertwined like the mountains themselves. And so, their story lived on as a testament to the power of knowledge, wisdom and collaboration and the will to make a difference.</em></p><p id="c94d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">They were the super-Llamas and the trio was lovingly called LlaMA3!”</em></p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="efee" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">LlaMA 3 by Meta</h1><p id="c28c" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">And this story is not very far from the story of Meta’s open-source Large Language Model (LLM) — LlaMA 3 (Large Language Model Meta AI). On April 18, 2024, Meta released their LlaMa 3 family of large language models in 8B and 70B parameter sizes, claiming a major leap over LlaMA 2 and vying for the best state-of-the-art LLM models at that scale.</p><p id="e3a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af pi" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">According to Meta</a>, there were four key focus points while building LlaMA 3 — <strong class="ne fr">the model architecture, the pre-training data, scaling up pre-training, and instruction fine-tuning</strong>. This leads us to ponder what we can do to reap the most out of this very competent model — on an enterprise scale as well as at the grass-root level.</p><p id="8191" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To help explore the answers to some of these questions, I collaborated with <a class="af pi" href="https://www.linkedin.com/in/eordax/" rel="noopener ugc nofollow" target="_blank">Edurado Ordax</a>, Generative AI Lead at AWS and <a class="af pi" href="https://www.linkedin.com/in/tom-yeh/" rel="noopener ugc nofollow" target="_blank">Prof. Tom Yeh</a>, CS Professor at University of Colorado, Boulder.</p><p id="0376" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, let’s start the trek:</p><h1 id="07e2" class="oh oi fq bf oj ok pj gq om on pk gt op oq pl os ot ou pm ow ox oy pn pa pb pc bk">How can we leverage the power of LlaMA 3?</h1><h2 id="900a" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">API vs Fine-Tuning</h2><p id="f982" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">As per the recent practices, there are two main ways by which these LLMs are being accessed and worked with — <strong class="ne fr">API</strong> and <strong class="ne fr">Fine-Tuning</strong>. Even with those two very diverse approaches there are other factors in the process, as can be seen in the following images, that become crucial.</p><p id="cd12" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(All images in this section are courtesy to </em><a class="af pi" href="https://www.linkedin.com/in/eordax/" rel="noopener ugc nofollow" target="_blank"><em class="ny">Eduardo Ordax</em></a><em class="ny">.)</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/b29e0419cc9a36c5c3c9145bf4ddb1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_wgkil-XclgdD5cd"/></div></div></figure><p id="e805" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are mainly <strong class="ne fr">6 stages</strong> of how a user can interact with LlaMA 3.</p><p id="169c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 1</strong> : Cater to a broad-case usage by using the model as is.</p><p id="e255" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 2</strong> : Use the model as per a user-defined application.</p><p id="e867" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 3</strong> : Use prompt-engineering to train the model to produce the desired outputs.</p><p id="296c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 4</strong> : Use prompt-engineering on the user side along with delving a bit into data retrieval and fine-tuning which is still mostly managed by the LLM provider.</p><p id="cee3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 5 </strong>: Take most of the matters in your own hand (the user), starting from prompt-engineering to data retrieval and fine-tuning (RAG models, PEFT models and so on).</p><p id="9781" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Stage 6</strong> : Create the entire foundational model starting from scratch — pre-training to post-training.</p><p id="dd24" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To gain the most out of these models, it is suggested that the best approach would be entering <strong class="ne fr">Stage 5 </strong>because then the flexibility lies a lot with the user. Being able to customize the model as per the domain-need is crucial in order to maximize its gains. And for that, not getting involved into the systems does not yield optimal returns.</p><p id="80b7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To be able to do so, here is a <strong class="ne fr">high-level picture of the tools </strong>that could prove to be useful:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/ceab170abce46869b15be12ab3991fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vHOcMx1XoITn4BiR"/></div></div></figure><p id="27ac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The picture dictates that in order to get the highest benefit from the models, a set structure and a road map is essential. There are three components to it:</p><ol class=""><li id="c26b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qg qh qi bk"><strong class="ne fr">People</strong>: Not just end-users, but the whole range of data engineers, data scientists, MLOps Engineers, ML Engineers along with Prompt Engineers are important.</li><li id="679e" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qg qh qi bk"><strong class="ne fr">Process</strong>: Not just plugging in the LLM into an API but focusing on the entire lifecycle of model evaluation, model deployment and fine-tuning to cater to specific needs.</li><li id="baae" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qg qh qi bk"><strong class="ne fr">Tools</strong>: Not just the API access and API tools but the entire range of environments, different ML pipelines, separate accounts for access and running checks.</li></ol><p id="906f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Of course, this is true for an enterprise-level deployment such that the actual benefits of the model can be reaped. And to be able to do so, the tools and practices under <strong class="ne fr">MLOps </strong>become very important. Combined with <strong class="ne fr">FMOps</strong>, these models can prove to be very valuable and enrich the <strong class="ne fr">GenAI ecosystem</strong>.</p><blockquote class="qo qp qq"><p id="9ce1" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">FMOps ⊆ MLOps ⊆ DevOps</strong></p><p id="3383" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">MLOps</strong> also known as <strong class="ne fr">Machine Learning Operations</strong> is a part of Machine Learning Engineering that focuses on the development as well as the deployment, and maintenance of ML models ensuring that they run reliably and efficiently.</p><p id="3e9e" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">MLOps fall under <strong class="ne fr">DevOps (Development and Operations) </strong>but specifically for ML models.</p><p id="c3b8" class="nc nd ny ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">FMOps (Foundational Model Operations)</strong> on the other hand work for Generative AI scenarios by selecting, evaluating and fine-tuning the LLMs.</p></blockquote><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/36ba41369196954718fea36559770015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dsm-fG6i0V-Iha9W"/></div></div></figure><p id="86b0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With all if it being said, one thing however remains constant. And that is the fact that LlaMA 3 is after all an LLM and its implementation on the enterprise-level is possible and beneficial only after the foundational elements are set and validated with rigor. To be able to do so, let us explore the technical details behind LlaMA 3.</p><h1 id="63d4" class="oh oi fq bf oj ok pj gq om on pk gt op oq pl os ot ou pm ow ox oy pn pa pb pc bk">What is the secret sauce toward LlaMa 3’s claim to fame?</h1><p id="aeb4" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">At the fundamental level, yes, it is the transformer. If we go a little higher up in the process, the answer would be the transformer architecture but <strong class="ne fr">highly optimized</strong> to achieve superior performance on the common industry benchmarks while also enabling newer capabilities.</p><p id="e385" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Good news is that since LlaMa 3 is open (open-source at Meta’s discretion), we have access to the Model Card that gives us the details to how this powerful architecture is configured.</p><p id="2a48" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, let’s dive in and unpack the goodness:</p><h1 id="aba4" class="oh oi fq bf oj ok pj gq om on pk gt op oq pl os ot ou pm ow ox oy pn pa pb pc bk"><strong class="al">How does the transformer architecture coupled with self-attention play its role in LlaMA 3?</strong></h1><p id="1b6a" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">To start with, here is a quick review on how the transformer works:</p><ol class=""><li id="f374" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qg qh qi bk">The transformer architecture can be perceived as a combination of the attention layer and the feed-forward layer.</li><li id="dc72" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qg qh qi bk">The attention layer combines across features horizontally to produce a new feature.</li><li id="f777" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qg qh qi bk">The feed-forward layer (FFN) combines the parts or the characteristics of a feature to produce new parts/characteristics. It does it vertically across dimensions.</li></ol><p id="2feb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">(All the images in this section, unless otherwise noted, are by </em><a class="af pi" href="https://www.linkedin.com/in/tom-yeh/" rel="noopener ugc nofollow" target="_blank"><em class="ny">Prof. Tom Yeh</em></a><em class="ny">, which I have edited with his permission.)</em></p><p id="b3f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below is a basic form of how the architecture looks like and how it functions.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qr"><img src="../Images/7b96e71b0f0dd4611e9f881c39b6e4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JhNgGmKQK_Q93PvP"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The transformer architecture containing the attention and the feed-forward blocks.</figcaption></figure><p id="3a5d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the links to the deep-dive articles for <a class="af pi" href="https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813" rel="noopener">Transformers</a> and <a class="af pi" href="https://medium.com/towards-data-science/deep-dive-into-self-attention-by-hand-︎-f02876e49857" rel="noopener">Self-Attention</a> where the entire process is discussed in detail.</p><h1 id="f383" class="oh oi fq bf oj ok pj gq om on pk gt op oq pl os ot ou pm ow ox oy pn pa pb pc bk">The essentials of LlaMA 3</h1><p id="27d3" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">It’s time to get into the nitty-gritty and discover how the transformer numbers play out in the real-life LlaMa 3 model. For our discussion, we will only consider the 8B variant. Here we go:</p><h2 id="894a" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">- What are the LlaMA 3 — 8B model parameters?</h2><p id="4e57" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The primary numbers/values that we need to explore here are for the parameters that play a key role in the transformer architecture. And they are as below:</p><ul class=""><li id="d3a2" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qs qh qi bk"><strong class="ne fr">Layers</strong> : Layers here refer to the basic blocks of the transformers — the attention layer and the FFN as can be seen in the image above. The layers are stacked one above the other where the input flows into one layer and its output is passed on to the next layer, gradually transforming the input data.</li><li id="0d8c" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk"><strong class="ne fr">Attention heads</strong> : Attention heads are part of the self-attention mechanism. Each head scans the input sequence independently and performs the attention steps <em class="ny">(Remember: the QK-module, SoftMax function.)</em></li><li id="36b1" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk"><strong class="ne fr">Vocabulary words</strong> : The vocabulary refers to the number of words the model recognizes or knows. Essentially, think of it as humans’ way of building our word repertoire so that we develop knowledge and versatility in a language. Most times bigger the vocabulary, better the model performance.</li><li id="7f72" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk"><strong class="ne fr">Feature dimensions</strong> : These dimensions specify the size of the vectors representing each token in the input data. This number remains consistent throughout the model from the input embedding to the output of each layer.</li><li id="cee7" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk"><strong class="ne fr">Hidden dimensions</strong> : These dimensions are the internal size of the layers within the model, more commonly the size of hidden layers of the feed-forward layers. As is norm, the size of these layers can be larger than the feature dimension helping the model extract and process more complex representations from the data.</li><li id="658e" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk"><strong class="ne fr">Context-window size</strong> : The ‘window-size’ here refers to the number of tokens from the input sequence that the model considers at once when calculating attention.</li></ul><p id="642d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With the terms defined, let us refer to the actual numbers for these parameters in the LlaMA 3 model. (The original source code where these numbers are stated can be found <a class="af pi" href="https://github.com/meta-llama/llama3/tree/main/llama" rel="noopener ugc nofollow" target="_blank">here</a>.)</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/62f350ed1621ce15eba3af4eec174c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QZIJl12AHjrwiCbp"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The original source code where these numbers are stated can be found <a class="af pi" href="https://github.com/meta-llama/llama3/tree/main/llama" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="db84" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Keeping these values in mind, the next steps illustrate how each of them play their part in the model. They are listed in their order of appearance in the source-code.</p><h2 id="d8e8" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[1] The context-window</h2><p id="2458" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">While instantiating the <strong class="ne fr">LlaMa class</strong>, the variable <em class="ny">max_seq_len </em>defines the <strong class="ne fr">context-window</strong>. There are other parameters in the class but this one serves our purpose in relation to the transformer model. The <em class="ny">max_seq_len</em> here is 8K which implies the attention head is able to scan 8K tokens at one go.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/52fa47f3cc7a98b0058601d3c95f8fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EpIfDibKey3HX4CN"/></div></div></figure><h2 id="ee4f" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[2] Vocabulary-size and Attention Layers</h2><p id="d622" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Next up is the <strong class="ne fr">Transformer class</strong> which defines the vocabulary size and the number of layers. Once again the <strong class="ne fr">vocabulary size</strong> here refers to the set of words (and tokens) that the model can recognize and process. <strong class="ne fr">Attention layers </strong>here refer to the transformer block (the combination of the attention and feed-forward layers) used in the model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/b2cbb782a640277df59b729321fa2316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FV6-pzAo-9jUZBla"/></div></div></figure><p id="68aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on these numbers, LlaMA 3 has a vocabulary size of 128K which is quite large. Additionally, it has 32 copies of the transformer block.</p><h2 id="0df3" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[3] Feature-dimension and Attention-Heads</h2><p id="62a6" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The feature dimension and the attention-heads make their way into the <strong class="ne fr">Self-Attention</strong> <strong class="ne fr">module</strong>. <strong class="ne fr">Feature dimension</strong> refers to the vector-size of the tokens in the embedding space and the <strong class="ne fr">attention-heads</strong> consist of the QK-module that powers the self-attention mechanism in the transformers.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/a50f5d8790c3be92fb78661299d1bad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iVa416s3k3BjL90P"/></div></div></figure><h2 id="e985" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[4] Hidden Dimensions</h2><p id="53a0" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">The <strong class="ne fr">hidden dimension</strong> features in the <strong class="ne fr">Feed-Forward class</strong> specifying the number of hidden layers in the model. For LlaMa 3, the hidden layer is 1.3 times the size of the feature dimension. A larger number of hidden layers allows the network to create and manipulate richer representations internally before projecting them back to the smaller output dimension.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/0c19e00a159f5dd5675336897e548db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eeP6RWKWKmKenddP"/></div></div></figure><h2 id="fb2b" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[5] Combining the above parameters to form the Transformer</h2><ul class=""><li id="4706" class="nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx qs qh qi bk">The first matrix is the input feature matrix which goes through the <strong class="ne fr">Attention layer</strong> to create the Attention Weighted features. In this image the input feature matrix only has a size of 5 x 3 matrix, but in the real-world Llama 3 model it grows up to be 8K x 4096 which is enormous.</li><li id="5874" class="nc nd fq ne b go qj ng nh gr qk nj nk nl ql nn no np qm nr ns nt qn nv nw nx qs qh qi bk">The next one is the hidden layer in the Feed-Forward Network that grows up to 5325 and then comes back down to 4096 in the final layer.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/9f56c01797cb45c4ac6070e565d25041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VAJjmoVm50OMkakO"/></div></div></figure><h2 id="fb82" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk">[6] Multiple-layers of the Transformer block</h2><p id="4e38" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">LlaMA 3 combines 32 of these above transformer blocks with the output of one passing down into the next block until the last one is reached.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/3c9da18de90dc59b4833b593c0e84fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6VDy4m1z8G0FcRpU"/></div></div></figure><h2 id="e279" class="po oi fq bf oj pp pq pr om ps pt pu op nl pv pw px np py pz qa nt qb qc qd qe bk"><strong class="al">[7] Let’s put it all together</strong></h2><p id="d789" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Once we have set all the above pieces in motion, it is time to put it all together and see how they produce the LlaMA effect.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/e2d916ff16aba1d6fef6d19b698e0475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RZrEKKfdTkPFMkZ9A89uKQ.gif"/></div></div></figure><p id="f400" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, what is happening here?</p><p id="8a79" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Step 1</strong> : First we have our input matrix, which is the size of 8K (context-window) x 128K (vocabulary-size). This matrix undergoes the process of embedding which takes this high-dimensional matrix into a lower dimension.</p><p id="2b65" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Step 2</strong> : This lower dimension in this case turns out to be 4096 which is the specified dimension of the features in the LlaMA model as we had seen before. <em class="ny">(A reduction from 128K to 4096 is immense and noteworthy.)</em></p><p id="4fcd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Step 3:</strong> This feature goes through the Transformer block where it is processed first by the Attention layer and then the FFN layer. The attention layer processes it across features horizontally whereas the FFN layer does it vertically across dimensions.</p><p id="c2ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Step 4</strong>: Step 3 is repeated for 32 layers of the Transformer block. In the end the resultant matrix has the same dimension as the one used for the feature dimension.</p><p id="4ad5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Step 5</strong>: Finally this matrix is transformed back to the original size of the vocabulary matrix which is 128K so that the model can choose and map those words as available in the vocabulary.</p><p id="165f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And that’s how LlaMA 3 is essentially scoring high on those benchmarks and creating the LlaMA 3 effect.</p><h1 id="9980" class="oh oi fq bf oj ok pj gq om on pk gt op oq pl os ot ou pm ow ox oy pn pa pb pc bk">The LlaMA 3 Effect</h1><p id="4312" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">LlaMA 3 was released in two model versions — 8B and 70B parameters to serve a wide range of use-cases. In addition to achieving state-of-the-art performances on standard benchmarks, a new and rigorous human-evaluation set was also developed. And Meta promises to release better and stronger versions of the model with it becoming multilingual and multimodal. The news is newer and larger models are coming soon with over 400B parameters (early reports <a class="af pi" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">here</a> show that it is already crushing benchmarks by an almost 20% score increase over LlaMA 3).</p><p id="eb53" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, it is imperative to say that in spite of all the upcoming changes and all the updates, one thing is going to remain the same — the foundation of it all — the transformer architecture and the transformer block that enables this incredible technical advancement.</p><p id="862d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It could be a coincidence that LlaMA models were named so, but based on legend from the Andes mountains, the real llamas have always been revered for their strength and wisdom. Not very different from the Gen AI — ‘LlaMA’ models.</p><p id="e427" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, let’s follow along in this exciting journey of the GenAI Andes while keeping in mind the foundation that powers these large language models!</p><p id="3ae4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">P.S. If you would like to work through this exercise on your own, here is a link to a blank template for your use.</em></p><p id="09a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af pi" href="https://drive.google.com/file/d/1NfHBSQQTgH1bPXiNUHyMhT2UGXqUaTPE/view?usp=drive_link" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="0be4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now go have fun and create some LlaMA 3 effect!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/143da434496829910a76dbfe868dce8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ir1LnAHoysqqwtpvNcp4Fg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author</figcaption></figure></div></div></div></div>    
</body>
</html>