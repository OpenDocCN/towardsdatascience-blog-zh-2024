["```py\n!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n```", "```py\nimport torch\nfrom trl import SFTTrainer\nfrom datasets import load_dataset\nfrom transformers import TrainingArguments, TextStreamer\nfrom unsloth.chat_templates import get_chat_template\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\n```", "```py\nmax_seq_length = 2048\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n    max_seq_length=max_seq_length,\n    load_in_4bit=True,\n    dtype=None,\n)\n```", "```py\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"], \n    use_rslora=True,\n    use_gradient_checkpointing=\"unsloth\"\n)\n```", "```py\n<|im_start|>system\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.<|im_end|>\n<|im_start|>user\nRemove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.\n<|im_end|>\n<|im_start|>assistant\nItpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.<|im_end|>\n```", "```py\ntokenizer = get_chat_template(\n    tokenizer,\n    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n    chat_template=\"chatml\",\n)\n\ndef apply_template(examples):\n    messages = examples[\"conversations\"]\n    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n    return {\"text\": text}\n\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\ndataset = dataset.map(apply_template, batched=True)\n```", "```py\ntrainer=SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=True,\n    args=TrainingArguments(\n        learning_rate=3e-4,\n        lr_scheduler_type=\"linear\",\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        warmup_steps=10,\n        output_dir=\"output\",\n        seed=0,\n    ),\n)\n\ntrainer.train()\n```", "```py\nmodel = FastLanguageModel.for_inference(model)\n\nmessages = [\n    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\",\n).to(\"cuda\")\n\ntext_streamer = TextStreamer(tokenizer)\n_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n```", "```py\nmodel.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")\nmodel.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")\n```", "```py\nquant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\nfor quant in quant_methods:\n    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)\n```"]