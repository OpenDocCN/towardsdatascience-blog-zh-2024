["```py\ngpt2_small: HookedTransformer = HookedTransformer.from_pretrained(\"gpt2-small\")\n```", "```py\ndef generate_repeated_tokens(\n    model: HookedTransformer, seq_len: int, batch: int = 1\n) -> Int[Tensor, \"batch full_seq_len\"]:\n    '''\n    Generates a sequence of repeated random tokens\n\n    Outputs are:\n        rep_tokens: [batch, 1+2*seq_len]\n    '''\n    bos_token = (t.ones(batch, 1) * model.tokenizer.bos_token_id).long()  # generate bos token for each batch\n\n    rep_tokens_half = t.randint(0, model.cfg.d_vocab, (batch, seq_len), dtype=t.int64)\n    rep_tokens = t.cat([bos_token,rep_tokens_half,rep_tokens_half], dim=-1).to(device)\n    return rep_tokens\n```", "```py\ndef induction_score_hook(\n    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"],\n    hook: HookPoint,\n):\n    '''\n    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.\n    '''\n    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len) # src_pos, des_pos, one position right from seq_len\n    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n    induction_score_store[hook.layer(), :] = induction_score\n\nseq_len = 50\nbatch = 30\nrep_tokens_30 = generate_repeated_tokens(gpt2_small, seq_len, batch)\ninduction_score_store = t.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n\n    rep_tokens_30,\n    return_type=None, \n        pattern_hook_names_filter,\n        induction_score_hook\n    )]\n)\n```", "```py\ndef patch_residual_component(\n    residual_component,\n    hook,\n    pos,\n    cache,\n):\n    residual_component[0,pos, :] = cache[hook.name][pos-seq_len, :]\n    return residual_component\n\nablation_scores = t.zeros((gpt2_small.cfg.n_layers, seq_len), device=gpt2_small.cfg.device)\n\ngpt2_small.reset_hooks()\nlogits = gpt2_small(rep_tokens, return_type=\"logits\")\nloss_no_ablation = cross_entropy_loss(logits[:, seq_len: max_len],rep_tokens[:, seq_len: max_len])\n\nfor layer in tqdm(range(gpt2_small.cfg.n_layers)):\n  for position in range(seq_len, max_len):\n    hook_fn = functools.partial(patch_residual_component, pos=position, cache=rep_cache)\n    ablated_logits = gpt2_small.run_with_hooks(rep_tokens, fwd_hooks=[\n              (utils.get_act_name(\"mlp_out\", layer), hook_fn)\n    ])\n    loss = cross_entropy_loss(ablated_logits[:, seq_len: max_len], rep_tokens[:, seq_len: max_len])\n    ablation_scores[layer, position-seq_len] = loss - loss_no_ablation\n```", "```py\ndef K_comp_full_circuit(\n    model: HookedTransformer,\n    prev_token_layer_index: int,\n    ind_layer_index: int,\n    prev_token_head_index: int,\n    ind_head_index: int\n) -> FactoredMatrix:\n    '''\n    Returns a (vocab, vocab)-size FactoredMatrix,\n    with the first dimension being the query side\n    and the second dimension being the key side (going via the previous token head)\n\n    '''\n    W_E = gpt2_small.W_E\n    W_Q = gpt2_small.W_Q[ind_layer_index, ind_head_index]\n    W_K = model.W_K[ind_layer_index, ind_head_index]\n    W_O = model.W_O[prev_token_layer_index, prev_token_head_index]\n    W_V = model.W_V[prev_token_layer_index, prev_token_head_index]\n\n    Q = W_E @ W_Q\n    K = W_E @ W_V @ W_O @ W_K\n    return FactoredMatrix(Q, K.T)\n```"]