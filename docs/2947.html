<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Combining Large and Small LLMs to Boost Inference Time and Quality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Combining Large and Small LLMs to Boost Inference Time and Quality</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05">https://towardsdatascience.com/combining-large-and-small-llms-for-inference-time-and-quality-boosts-1779b6b5100b?source=collection_archive---------9-----------------------#2024-12-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b0d4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Implementing Speculative and Contrastive Decoding</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Richa Gadgil" class="l ep by dd de cx" src="../Images/c39ace5df0438240647ea751e8f6ba9e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/0*Uo1K1OOtUkVPpO-V.jpg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mlsys.medium.com/?source=post_page---byline--1779b6b5100b--------------------------------" rel="noopener follow">Richa Gadgil</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1779b6b5100b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="5d7c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Large Language models are comprised of billions of parameters (weights). For each word it generates, the model has to perform computationally expensive calculations across all of these parameters.</p><p id="eb1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Large Language models accept a sentence, or sequence of tokens, and generate a probability distribution of the next most likely token.</p><p id="da26" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thus, typically decoding <strong class="ml fr">n</strong> tokens (or generating <strong class="ml fr">n </strong>words from the model) requires running the model <strong class="ml fr">n </strong>number of times. At each iteration, the new token is appended to the input sentence and passed to the model again. This can be costly.</p><p id="e124" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Additionally, decoding strategy can influence the quality of the generated words. Generating tokens in a simple way, by just taking the token with the highest probability in the output distribution, can result in repetitive text. Random sampling from the distribution can result in unintended drift.</p><p id="fb2b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thus, a solid decoding strategy is required to ensure both:</p><ul class=""><li id="e628" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">High Quality Outputs</li><li id="6df6" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">Fast Inference Time</li></ul><p id="b67a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Both requirements can be addressed by using a combination of a large and small language model, as long as the amateur and expert models are similar (e.g., same architecture but different sizes).</strong></p><ul class=""><li id="d2f5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">Target/Large Model: </strong>Main LM with larger number of parameters (e.g. OPT-13B)</li><li id="6f35" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">Amateur/Small Model:</strong> Smaller version of Main LM with fewer parameters (e.g. OPT-125M)</li></ul><p id="b22b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Speculative</strong> and <strong class="ml fr">contrastive</strong> decoding leverage large and small LLMs to achieve reliable and efficient text generation.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no np"><img src="../Images/0108a15ac5b5abb54cf65b964db6f0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sYU-r355eE8LL8ug8tngmQ.png"/></div></div></figure><h1 id="a499" class="ob oc fq bf od oe of gq og oh oi gt oj ok ol om on oo op oq or os ot ou ov ow bk">Contrastive Decoding for High Quality Inference</h1><p id="197f" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk"><a class="af pc" href="https://arxiv.org/abs/2210.15097" rel="noopener ugc nofollow" target="_blank">Contrastive Decoding</a> is a strategy that exploits the fact that that failures in large LLMs (such as repetition, incoherence) are even more pronounced in small LLMs. Thus, this strategy optimizes for the tokens with the highest probability difference between the small and large model.</p><p id="4e4b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For a single prediction, contrastive decoding generates two probability distributions:</p><ul class=""><li id="16e5" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><em class="pd">q = </em>logit probabilities for amateur model</li><li id="3fae" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><em class="pd">p = </em>logit probabilities for expert model</li></ul><p id="5d2b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next token is chosen based on the following criteria:</p><ul class=""><li id="51e6" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">Discard all tokens that do not have sufficiently high probability under the expert model (discard <em class="pd">p(x) &lt; alpha * max(p)</em>)</li><li id="ac7a" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">From the remaining tokens, select the one the with the largest difference between large model and small model log probabilities, <em class="pd">max(p(x) - q(x)).</em></li></ul><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no pe"><img src="../Images/4e99947389dc73464dbe732a72555688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkZ4H_FMF0VhW5y-pUFX6g.png"/></div></div></figure><h2 id="9bb4" class="pf oc fq bf od pg ph pi og pj pk pl oj ms pm pn po mw pp pq pr na ps pt pu pv bk">Implementing Contrastive Decoding</h2><pre class="nq nr ns nt nu pw px py bp pz bb bk"><span id="7e85" class="qa oc fq px b bg qb qc l qd qe">from transformers import AutoTokenizer, AutoModelForCausalLM<br/>import torch<br/><br/># Load models and tokenizer<br/>tokenizer = AutoTokenizer.from_pretrained('gpt2')<br/>amateur_lm = AutoModelForCausalLM.from_pretrained('gpt2')<br/>expert_lm = AutoModelForCausalLM.from_pretrained('gpt2-large')<br/><br/>def contrastive_decoding(prompt, max_length=50):<br/>    input_ids = tokenizer(prompt, return_tensors="pt").input_ids<br/><br/>    while input_ids.shape[1] &lt; max_length:<br/><br/>        # Generate amateur model output<br/>        amateur_outputs = amateur_lm(input_ids, return_dict=True)<br/>        amateur_logits = torch.softmax(amateur_outputs.logits[:, -1, :], dim=-1)<br/>        log_probs_amateur = torch.log(amateur_logits)<br/><br/>        # Generate expert model output<br/>        expert_outputs = expert_lm(input_ids, return_dict=True)<br/>        expert_logits = torch.softmax(expert_outputs.logits[:, -1, :], dim=-1)<br/>        log_probs_exp = torch.log(expert_logits)<br/><br/>        log_probs_diff = log_probs_exp - log_probs_amateur<br/><br/>        # Set an alpha threshold to eliminate less confident tokens in expert<br/>        alpha = 0.1<br/>        candidate_exp_prob = torch.max(expert_logits)<br/><br/>        # Mask tokens below threshold for expert model<br/>        V_head = expert_logits &lt; alpha * candidate_exp_prob<br/><br/>        # Select the next token from the log-probabilities difference, ignoring masked values<br/>        token = torch.argmax(log_probs_diff.masked_fill(V_head, -torch.inf)).unsqueeze(0)<br/><br/>        # Append token and accumulate generated text<br/>        input_ids = torch.cat([input_ids, token.unsqueeze(1)], dim=-1)<br/><br/>    return tokenizer.batch_decode(input_ids)<br/><br/>prompt = "Large Language Models are"<br/>generated_text = contrastive_decoding(prompt, max_length=25)<br/>print(generated_text)</span></pre><h1 id="c587" class="ob oc fq bf od oe of gq og oh oi gt oj ok ol om on oo op oq or os ot ou ov ow bk">Speculative Decoding For Fast Inference</h1><p id="64ff" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk"><a class="af pc" href="https://arxiv.org/abs/2211.17192" rel="noopener ugc nofollow" target="_blank">Speculative decoding</a> is based on the principle that the smaller model must sample from the same distribution as the larger model. Thus, this strategy aims to accept as many predictions from the smaller model as possible, provided they align with the distribution of the larger model.</p><p id="c6bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The smaller model generates <strong class="ml fr">n</strong> tokens in sequence, as possible guesses. However, all <strong class="ml fr">n</strong> sequences are fed into the larger expert model as a single batch, which is faster than sequential generation.</p><p id="9ea5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This results in a cache for each model, with <strong class="ml fr">n </strong>probability distributions in each cache.</p><ul class=""><li id="7b29" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><em class="pd">q = </em>logit probabilities for amateur model</li><li id="1fee" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><em class="pd">p = </em>logit probabilities for expert model</li></ul><p id="9d36" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Next, the sampled tokens from the amateur model are accepted or rejected based on the following conditions:</p><ul class=""><li id="9c7f" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk">If probability of the token is higher in expert distribution (p) than amateur distribution (q), or <em class="pd">p(x) &gt; q(x), </em>accept token</li><li id="9ddc" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk">If probability of token is lower in expert distribution (p) than amateur distribution (q), or <em class="pd">p(x) &lt; q(x)</em>, reject token with probability <em class="pd">1 - p(x) / q(x)</em></li></ul><p id="bc01" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If a token is rejected, the next token is sampled from the expert distribution or adjusted distribution. Additionally, the amateur and expert model reset the cache and re-generate <strong class="ml fr">n </strong>guesses and probability distributions <em class="pd">p</em> and <em class="pd">q</em>.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no qf"><img src="../Images/460e2c2954e742063edb26a642582e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxlaFBJg4mEtmZfA33RBqA.png"/></div></div><figcaption class="qg qh qi nn no qj qk bf b bg z dx">Here, the blue signifies accepted tokens, and red/green signify tokens rejected and then sampled from the expert or adjusted distribution.</figcaption></figure><h2 id="f5a0" class="pf oc fq bf od pg ph pi og pj pk pl oj ms pm pn po mw pp pq pr na ps pt pu pv bk">Implementing Speculative Decoding</h2><pre class="nq nr ns nt nu pw px py bp pz bb bk"><span id="aed4" class="qa oc fq px b bg qb qc l qd qe">from transformers import AutoTokenizer, AutoModelForCausalLM<br/>import torch<br/><br/># Load models and tokenizer<br/>tokenizer = AutoTokenizer.from_pretrained('gpt2')<br/>amateur_lm = AutoModelForCausalLM.from_pretrained('gpt2')<br/>expert_lm = AutoModelForCausalLM.from_pretrained('gpt2-large')<br/><br/># Sample next token from output distribution<br/>def sample_from_distribution(logits):<br/>    sampled_index = torch.multinomial(logits, 1)<br/>    return sampled_index<br/><br/>def generate_cache(input_ids, n_tokens):<br/>    # Store logits at each step for amateur and expert models<br/>    amateur_logits_per_step = []<br/>    generated_tokens = []<br/><br/>    batch_input_ids = []<br/><br/>    with torch.no_grad():<br/>        for _ in range(n_tokens):<br/>            # Generate amateur model output<br/>            amateur_outputs = amateur_lm(input_ids, return_dict=True)<br/>            amateur_logits = torch.softmax(amateur_outputs.logits[:, -1, :], dim=-1)<br/>            amateur_logits_per_step.append(amateur_logits)<br/><br/>            # Sampling from amateur logits<br/>            next_token = sample_from_distribution(amateur_logits)<br/>            generated_tokens.append(next_token)<br/><br/>            # Append to input_ids for next generation step<br/>            input_ids = torch.cat([input_ids, next_token], dim=-1)<br/>            batch_input_ids.append(input_ids.squeeze(0))<br/><br/>    # Feed IDs to expert model as batch <br/>    batched_input_ids = torch.nn.utils.rnn.pad_sequence(batch_input_ids, batch_first=True, padding_value=0 )<br/>    expert_outputs = expert_lm(batched_input_ids, return_dict=True)<br/>    expert_logits = torch.softmax(expert_outputs.logits[:, -1, :], dim=-1)<br/><br/>    return amateur_logits_per_step, expert_logits, torch.cat(generated_tokens, dim=-1)<br/><br/>def speculative_decoding(prompt, n_tokens=5, max_length=50):<br/>    input_ids = tokenizer(prompt, return_tensors="pt").input_ids<br/><br/>    while input_ids.shape[1] &lt; max_length:<br/>        amateur_logits_per_step, expert_logits, generated_ids = generate_cache(<br/>            input_ids, n_tokens<br/>        )<br/><br/>        accepted = 0<br/>        for n in range(n_tokens):<br/>            token = generated_ids[:, n][0]<br/>            r = torch.rand(1).item()<br/><br/>            # Extract probabilities<br/>            p_x = expert_logits[n][token].item()<br/>            q_x = amateur_logits_per_step[n][0][token].item()<br/><br/>            # Speculative decoding acceptance criterion<br/>            if ((q_x &gt; p_x) and (r &gt; (1 - p_x / q_x))):<br/>                break  # Reject token and restart the loop<br/>            else:<br/>                accepted += 1<br/>                <br/>            # Check length<br/>            if (input_ids.shape[1] + accepted) &gt;= max_length:<br/>                return tokenizer.batch_decode(input_ids)<br/><br/>        input_ids = torch.cat([input_ids, generated_ids[:, :accepted]], dim=-1)<br/><br/>        if accepted &lt; n_tokens:<br/>            diff = expert_logits[accepted] - amateur_logits_per_step[accepted][0]<br/>            clipped_diff = torch.clamp(diff, min=0) <br/><br/>            # Sample a token from the adjusted expert distribution<br/>            normalized_result = clipped_diff / torch.sum(clipped_diff, dim=0, keepdim=True)<br/>            next_token = sample_from_distribution(normalized_result)<br/>            input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=-1)<br/>        else:<br/>            # Sample directly from the expert logits for the last accepted token<br/>            next_token = sample_from_distribution(expert_logits[-1])<br/>            input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=-1)<br/><br/>    return tokenizer.batch_decode(input_ids)<br/><br/># Example usage<br/>prompt = "Large Language models are"<br/>generated_text = speculative_decoding(prompt, n_tokens=3, max_length=25)<br/>print(generated_text)</span></pre><h2 id="fbc9" class="pf oc fq bf od pg ph pi og pj pk pl oj ms pm pn po mw pp pq pr na ps pt pu pv bk">Evaluation</h2><p id="7eed" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">We can evaluate both decoding approaches by comparing them to a naive decoding method, where we randomly pick the next token from the probability distribution.</p><pre class="nq nr ns nt nu pw px py bp pz bb bk"><span id="cf98" class="qa oc fq px b bg qb qc l qd qe">def sequential_sampling(prompt, max_length=50):<br/>    """<br/>    Perform sequential sampling with the given model.<br/>    """<br/>    # Tokenize the input prompt<br/>    input_ids = tokenizer(prompt, return_tensors="pt").input_ids<br/><br/>    with torch.no_grad():<br/>          while input_ids.shape[1] &lt; max_length:<br/>            # Sample from the model output logits for the last token<br/>            outputs = expert_lm(input_ids, return_dict=True)<br/>            logits = outputs.logits[:, -1, :]<br/><br/>            probabilities = torch.softmax(logits, dim=-1)<br/>            next_token = torch.multinomial(probabilities, num_samples=1)<br/>            input_ids = torch.cat([input_ids, next_token], dim=-1)<br/><br/>    return tokenizer.batch_decode(input_ids)</span></pre><p id="e2c2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To evaluate contrastive decoding, we can use the following metrics for lexical richness.</p><ul class=""><li id="ccea" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh bk"><strong class="ml fr">n-gram Entropy</strong>: Measures the unpredictability or diversity of n-grams in the generated text. High entropy indicates more diverse text, while low entropy suggests repetition or predictability.</li><li id="2347" class="mj mk fq ml b go ni mn mo gr nj mq mr ms nk mu mv mw nl my mz na nm nc nd ne nf ng nh bk"><strong class="ml fr">distinct-n</strong>: Measures the proportion of unique n-grams in the generated text. Higher distinct-n values indicate more lexical diversity.</li></ul><pre class="nq nr ns nt nu pw px py bp pz bb bk"><span id="239e" class="qa oc fq px b bg qb qc l qd qe">from collections import Counter<br/>import math<br/><br/>def ngram_entropy(text, n):<br/>    """<br/>    Compute n-gram entropy for a given text.<br/>    """<br/>    # Tokenize the text<br/>    tokens = text.split()<br/>    if len(tokens) &lt; n:<br/>        return 0.0  # Not enough tokens to form n-grams<br/>    <br/>    # Create n-grams<br/>    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]<br/>    <br/>    # Count frequencies of n-grams<br/>    ngram_counts = Counter(ngrams)<br/>    total_ngrams = sum(ngram_counts.values())<br/>    <br/>    # Compute entropy<br/>    entropy = -sum((count / total_ngrams) * math.log2(count / total_ngrams)<br/>                   for count in ngram_counts.values())<br/>    return entropy<br/><br/>def distinct_n(text, n):<br/>    """<br/>    Compute distinct-n metric for a given text.<br/>    """<br/>    # Tokenize the text<br/>    tokens = text.split()<br/>    if len(tokens) &lt; n:<br/>        return 0.0  # Not enough tokens to form n-grams<br/>    <br/>    # Create n-grams<br/>    ngrams = [tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)]<br/>    <br/>    # Count unique and total n-grams<br/>    unique_ngrams = set(ngrams)<br/>    total_ngrams = len(ngrams)<br/>    <br/>    return len(unique_ngrams) / total_ngrams if total_ngrams &gt; 0 else 0.0<br/><br/>prompts = [<br/>    "Large Language models are",<br/>    "Barack Obama was",<br/>    "Decoding strategy is important because",<br/>    "A good recipe for Halloween is",<br/>    "Stanford is known for"<br/>]<br/><br/># Initialize accumulators for metrics<br/>naive_entropy_totals = [0, 0, 0]  # For n=1, 2, 3<br/>naive_distinct_totals = [0, 0]    # For n=1, 2<br/>contrastive_entropy_totals = [0, 0, 0]<br/>contrastive_distinct_totals = [0, 0]<br/><br/>for prompt in prompts:<br/>    naive_generated_text = sequential_sampling(prompt, max_length=50)[0]<br/><br/>    for n in range(1, 4):<br/>        naive_entropy_totals[n - 1] += ngram_entropy(naive_generated_text, n)<br/><br/>    for n in range(1, 3):<br/>        naive_distinct_totals[n - 1] += distinct_n(naive_generated_text, n)<br/><br/>    contrastive_generated_text = contrastive_decoding(prompt, max_length=50)[0]<br/><br/>    for n in range(1, 4):<br/>        contrastive_entropy_totals[n - 1] += ngram_entropy(contrastive_generated_text, n)<br/><br/>    for n in range(1, 3):<br/>        contrastive_distinct_totals[n - 1] += distinct_n(contrastive_generated_text, n)<br/><br/># Compute averages<br/>naive_entropy_averages = [total / len(prompts) for total in naive_entropy_totals]<br/>naive_distinct_averages = [total / len(prompts) for total in naive_distinct_totals]<br/>contrastive_entropy_averages = [total / len(prompts) for total in contrastive_entropy_totals]<br/>contrastive_distinct_averages = [total / len(prompts) for total in contrastive_distinct_totals]<br/><br/># Display results<br/>print("Naive Sampling:")<br/>for n in range(1, 4):<br/>    print(f"Average Entropy (n={n}): {naive_entropy_averages[n - 1]}")<br/>for n in range(1, 3):<br/>    print(f"Average Distinct-{n}: {naive_distinct_averages[n - 1]}")<br/><br/>print("\nContrastive Decoding:")<br/>for n in range(1, 4):<br/>    print(f"Average Entropy (n={n}): {contrastive_entropy_averages[n - 1]}")<br/>for n in range(1, 3):<br/>    print(f"Average Distinct-{n}: {contrastive_distinct_averages[n - 1]}")</span></pre><p id="dfc0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The following results show us that contrastive decoding outperforms naive sampling for these metrics.</p><blockquote class="ql qm qn"><p id="eac5" class="mj mk pd ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Naive Sampling:</strong><br/>Average Entropy (n=1): 4.990499826537679<br/>Average Entropy (n=2): 5.174765791328267<br/>Average Entropy (n=3): 5.14373124004409<br/>Average Distinct-1: 0.8949694135740648<br/>Average Distinct-2: 0.9951219512195122</p><p id="d492" class="mj mk pd ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Contrastive Decoding:</strong><br/>Average Entropy (n=1): 5.182773920916605<br/>Average Entropy (n=2): 5.3495681172235665<br/>Average Entropy (n=3): 5.313720275712986<br/>Average Distinct-1: 0.9028425204970866<br/>Average Distinct-2: 1.0</p></blockquote><p id="eca9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To evaluate speculative decoding, we can look at the average runtime for a set of prompts for different <strong class="ml fr">n</strong> values.</p><pre class="nq nr ns nt nu pw px py bp pz bb bk"><span id="97c4" class="qa oc fq px b bg qb qc l qd qe">import time<br/>import matplotlib.pyplot as plt<br/><br/># Parameters<br/>n_tokens = range(1, 11)<br/>speculative_decoding_times = []<br/>naive_decoding_times = []<br/><br/>prompts = [<br/>    "Large Language models are",<br/>    "Barack Obama was",<br/>    "Decoding strategy is important because",<br/>    "A good recipe for Halloween is",<br/>    "Stanford is known for"<br/>]<br/><br/># Loop through n_tokens values<br/>for n in n_tokens:<br/>    avg_time_naive, avg_time_speculative = 0, 0<br/><br/>    for prompt in prompts:<br/>        start_time = time.time()<br/>        _ = sequential_sampling(prompt, max_length=25)<br/>        avg_time_naive += (time.time() - start_time)<br/><br/>        start_time = time.time()<br/>        _ = speculative_decoding(prompt, n_tokens=n, max_length=25)<br/>        avg_time_speculative += (time.time() - start_time)<br/><br/>    naive_decoding_times.append(avg_time_naive / len(prompts))<br/>    speculative_decoding_times.append(avg_time_speculative / len(prompts))<br/><br/>avg_time_naive = sum(naive_decoding_times) / len(naive_decoding_times)<br/><br/># Plotting the results<br/>plt.figure(figsize=(8, 6))<br/>plt.bar(n_tokens, speculative_decoding_times, width=0.6, label='Speculative Decoding Time', alpha=0.7)<br/>plt.axhline(y=avg_time_naive, color='red', linestyle='--', label='Naive Decoding Time')<br/><br/># Labels and title<br/>plt.xlabel('n_tokens', fontsize=12)<br/>plt.ylabel('Average Time (s)', fontsize=12)<br/>plt.title('Speculative Decoding Runtime vs n_tokens', fontsize=14)<br/>plt.legend()<br/>plt.grid(axis='y', linestyle='--', alpha=0.7)<br/><br/># Show the plot<br/>plt.show()<br/>plt.savefig("plot.png")</span></pre><p id="4cdb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can see that the average runtime for the naive decoding is much higher than for speculative decoding across <strong class="ml fr">n</strong> values.</p><figure class="nq nr ns nt nu nv nn no paragraph-image"><div role="button" tabindex="0" class="nw nx ed ny bh nz"><div class="nn no qo"><img src="../Images/475bc024a1d7d059c360c87cc8293ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*slaABX5O1pPoxtZ7rOKarA.png"/></div></div></figure><p id="b61d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Combining large and small language models for decoding strikes a balance between quality and efficiency. While these approaches introduce additional complexity in system design and resource management, their benefits apply to conversational AI, real-time translation, and content creation.</p><p id="208a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These approaches require careful consideration of deployment constraints. For instance, the additional memory and compute demands of running dual models may limit feasibility on edge devices, though this can be mitigated through techniques like model quantization.</p><p id="74b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Unless otherwise noted, all images are by the author.</strong></p></div></div></div></div>    
</body>
</html>