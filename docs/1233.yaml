- en: Exploring LLMs for ICD Coding — Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM在ICD编码中的应用——第一部分
- en: 原文：[https://towardsdatascience.com/exploring-llms-for-icd-coding-part-1-959e48b58b9e?source=collection_archive---------1-----------------------#2024-05-16](https://towardsdatascience.com/exploring-llms-for-icd-coding-part-1-959e48b58b9e?source=collection_archive---------1-----------------------#2024-05-16)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-llms-for-icd-coding-part-1-959e48b58b9e?source=collection_archive---------1-----------------------#2024-05-16](https://towardsdatascience.com/exploring-llms-for-icd-coding-part-1-959e48b58b9e?source=collection_archive---------1-----------------------#2024-05-16)
- en: Building automated clinical coding systems with LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基于LLM的自动化临床编码系统
- en: '[](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)[![Anand
    Subramanian](../Images/096dc5504d6ada2493e0ac26959e60f0.png)](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)
    [Anand Subramanian](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)[![Anand
    Subramanian](../Images/096dc5504d6ada2493e0ac26959e60f0.png)](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)
    [Anand Subramanian](https://medium.com/@anand.subu10?source=post_page---byline--959e48b58b9e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)
    ·16 min read·May 16, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--959e48b58b9e--------------------------------)·阅读时间16分钟·2024年5月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Clinical coding isn’t common parlance, but it significantly impacts everyone
    who interacts with the healthcare system in most countries. Clinical coding involves
    translating and mapping medical information from patient health records, such
    as diagnoses and procedures, into standardized numeric or alphanumeric codes.
    These codes are crucial for billing, healthcare analytics, and ensuring that patients
    receive appropriate care.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 临床编码并不是日常用语，但在大多数国家，它对所有与医疗保健系统互动的人都产生了重要影响。临床编码涉及将患者健康记录中的医疗信息（如诊断和手术）翻译并映射为标准化的数字或字母数字编码。这些编码对计费、医疗分析以及确保患者获得适当护理至关重要。
- en: '![](../Images/d56b7d4ee7bac168b0d1961b3e2c6378.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d56b7d4ee7bac168b0d1961b3e2c6378.png)'
- en: A representative workflow of automated ICD coding (Image by Author)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化ICD编码的典型工作流程（图由作者提供）
- en: Clinical coding is typically performed by human coders with medical expertise.
    These coders navigate complex and often hierarchical coding terminologies with
    specific codes for a vast range of diagnoses and procedures. As such, coders must
    have a deep familiarity with and experience in the coding terminology used. However,
    manually coding documents can be slow, error-prone, and bottlenecked by the requirement
    for significant human expertise.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 临床编码通常由具备医学专业知识的人工编码员完成。这些编码员需要熟悉复杂且通常具有层级结构的编码术语，这些术语为各种诊断和手术指定了特定的编码。因此，编码员必须对使用的编码术语有深刻的理解和经验。然而，手动编码文档可能会很慢、容易出错，并且受到对大量人力专业知识的依赖，导致瓶颈。
- en: Deep learning can play a significant role in automating clinical coding. By
    automating the extraction and translation of complex medical information into
    codes, deep learning systems can function as a valuable tool within a human-in-the-loop
    system. They can support coders by processing large volumes of data quickly, potentially
    improving speed and accuracy. This can help streamline administrative operations,
    reduce billing errors and enhance patient care outcomes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在临床编码自动化中可以发挥重要作用。通过自动提取和翻译复杂的医疗信息为编码，深度学习系统可以作为“人机协作”系统中的一项有价值的工具。它们可以通过快速处理大量数据来支持编码员，从而潜在地提高速度和准确性。这有助于简化行政操作，减少计费错误，并改善患者护理结果。
- en: In this first part, I describe what ICD coding is, characterize the various
    challenges that an automated coding system must overcome in order to be effective.
    I also analyze how Large Language Models (LLMs) can be effectively used for overcoming
    these problems, and illustrate that by implementing an algorithm from a recent
    paper that leveraged LLMs effectively for ICD coding.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我描述了ICD编码的概念，阐明了自动化编码系统必须克服的各种挑战。我还分析了大型语言模型（LLMs）如何有效地用于克服这些问题，并通过实现一篇近期论文中的算法来说明如何有效地利用LLMs进行ICD编码。
- en: 'Table of Contents:'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录：
- en: '[**What is ICD Coding?**](#bfdf)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**什么是ICD编码？**](#bfdf)'
- en: '[**What are the challenges in automated ICD coding**](#76c7)**?**'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**自动化ICD编码中的挑战是什么**](#76c7)**?**'
- en: '[**How can LLMs help in automated ICD coding?**](#46fd)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**大型语言模型（LLMs）如何帮助自动化ICD编码？**](#46fd)'
- en: '[**Exploring the paper “Automated clinical coding using off-the-shelf large
    language models”**](#2949)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**探索论文“使用现成的大型语言模型进行自动化临床编码”**](#2949)'
- en: '[**Implementing the technique described in the paper**](#ddc6)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**实施论文中描述的技术**](#ddc6)'
- en: '[**Conclusion**](#0356)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**结论**](#0356)'
- en: '[**References**](#f82e)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**参考文献**](#f82e)'
- en: What is ICD Coding?
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是ICD编码？
- en: The International Classification of Diseases (ICD) coding is a clinical terminology
    system developed and maintained by the World Health Organization [1]. It is used
    in most countries to categorize and code all diagnoses, symptoms, and procedures
    recorded for a patient.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 国际疾病分类（ICD）编码是由世界卫生组织[1]开发和维护的临床术语系统。它在大多数国家用于对记录的患者所有诊断、症状和程序进行分类和编码。
- en: Medical notes, which document the diagnoses and medical procedures for a patient,
    are crucial for ICD coding . The ICD terminology features a hierarchical, tree-like
    structure to efficiently organize extensive information, with approximately 75,000
    different assignable codes available for various medical conditions and diagnoses.
    Coding these documents precisely is vital; accurate coding ensures appropriate
    billing and influences the quality of healthcare analysis, directly impacting
    patient care outcomes, reimbursement and healthcare efficiency.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗记录中记录患者诊断和医疗程序的医疗笔记对于ICD编码至关重要。ICD术语采用层次结构的树状结构，旨在有效地组织大量信息，提供约75,000个可用于各种医疗状况和诊断的不同编码。精确编码这些文件至关重要；准确的编码确保适当的计费，并影响医疗分析的质量，直接影响患者护理结果、报销和医疗效率。
- en: '**What are the challenges in automated ICD coding?**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**自动化ICD编码中的挑战是什么？**'
- en: ICD coding poses multiple challenges that an automated system must overcome
    in order to be effective.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ICD编码面临多个挑战，自动化系统必须克服这些挑战才能有效。
- en: 'Label Diversity in ICD Coding:'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ICD编码中的标签多样性：
- en: One significant challenge is the extensive output space of labels. ICD codes
    are numerous, and each code can differ in minute details — for instance, a condition
    affecting the right hand versus the left hand will have different codes. Additionally,
    there exists a long tail of rare codes that appear infrequently in medical records,
    making it difficult for deep learning models to learn and accurately predict these
    codes due to the scarcity of examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重大挑战是标签的输出空间非常广泛。ICD编码众多，每个编码在细节上可能有所不同——例如，影响右手和左手的病症将有不同的编码。此外，还存在一些稀有编码，这些编码在医疗记录中出现频率较低，深度学习模型由于缺乏足够的示例，很难学习并准确预测这些编码。
- en: 'Adapting to New ICD Codes:'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应新的ICD编码：
- en: Traditional datasets used for training, such as MIMIC-III [2] , while comprehensive,
    often limit the scope of ICD codes to those included in the training corpus. This
    restriction means that deep-learning models treating ICD coding as a multi-label
    classification problem from medical notes to ICD codes have difficulty handling
    new codes introduced into the ICD system after the model’s training. This makes
    retraining necessary and potentially challenging.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传统用于训练的数据集，如MIMIC-III [2]，虽然非常全面，但通常将ICD编码的范围限制在训练语料库中包含的编码。这一限制意味着将ICD编码视为从医疗笔记到ICD编码的多标签分类问题的深度学习模型，难以处理在模型训练后引入ICD系统的新编码。这使得重新训练成为必要且可能具有挑战性的任务。
- en: 'Extracting and Contextualizing Information:'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取和情境化信息：
- en: Another major challenge is the accurate extraction and contextualization of
    information from medical notes. ICD coding is fundamentally an Information Retrieval
    problem that requires not only identifying the diagnoses in the medical records
    but also capturing all supplementary information necessary for correctly mapping
    these diagnoses to their respective ICD codes. Therefore, it is crucial for an
    automated system to extract the various medical diagnoses in the medical note
    and contextualize them appropriately to ensure accurate mapping to the ICD codes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个主要挑战是准确提取和上下文化医疗记录中的信息。ICD编码本质上是一个信息检索问题，不仅需要识别医疗记录中的诊断，还需要捕捉所有必要的补充信息，以便将这些诊断正确地映射到相应的ICD编码。因此，自动化系统必须提取医疗记录中的各种医疗诊断，并适当地上下文化它们，以确保准确地映射到ICD编码。
- en: '![](../Images/e8fc0ee85ce38370994cb37a6e4dc546.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8fc0ee85ce38370994cb37a6e4dc546.png)'
- en: An example of the coarse-to-fine grained nature of ICD Coding — The final code
    that is to be assigned to a diagnosis is a function of how contextualized and
    precise the final query is. (Image by Author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ICD编码的粗粒度到细粒度的示例——分配给诊断的最终代码取决于最终查询的上下文化程度和精确度。（图片来源：作者）
- en: What does contextualization mean here? When dealing with medical notes, to contextualize
    a diagnosis means to link it with all pertinent details — such as the bodypart
    affected and the symptoms of the condition — to fully characterize the diagnosis.
    Generally this task is referred to as **relation extraction**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的上下文化是什么意思？在处理医疗记录时，上下文化诊断意味着将其与所有相关细节（例如受影响的身体部位和病情的症状）关联起来，以充分表征诊断。通常，这一任务被称为**关系提取**。
- en: '![](../Images/3bcf3cb6b8074705379fc555c62cf3c2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bcf3cb6b8074705379fc555c62cf3c2.png)'
- en: A representative example of the Relation Extraction process. Relation Extraction
    can help associate all relevant information for the main diagnosis in the medical
    note. (Image by Author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关系提取过程的代表性示例。关系提取可以帮助关联医疗记录中与主要诊断相关的所有信息。（图片来源：作者）
- en: How can LLMs help in automated ICD coding?
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs如何帮助自动化ICD编码？
- en: When addressing the challenges of automated ICD coding, Large Language Models
    (LLMs) are well-positioned to address these problems, particularly due to their
    adaptability to new labels and their ability to manage complex information extraction
    tasks. However the point here is not to argue that LLMs are the best solution
    for automated ICD coding, or that these are problems that only LLMs can solve.
    Rather, by establishing some of the main challenges than an automated ICD coding
    system must overcome, I analyze how best the abilities of LLMs can be leveraged
    to solve them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决自动化ICD编码的挑战时，大型语言模型（LLMs）处于解决这些问题的有利位置，尤其是由于它们能够适应新标签并处理复杂的信息提取任务。然而，这里并不是要争论LLMs是自动化ICD编码的最佳解决方案，或者这些问题只有LLMs能解决。相反，通过建立自动化ICD编码系统必须克服的一些主要挑战，我分析了如何最好地利用LLMs的能力来解决这些问题。
- en: 'Adapting to New and Rare ICD Codes:'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应新和稀有的ICD编码：
- en: LLMs demonstrate robust zero-shot and few-shot learning capabilities, allowing
    them to adapt to new tasks with minimal examples and instructions provided in
    the prompt. Retrieval-Augmented Generation (RAG) is another paradigm that enables
    LLMs to access more contextual information to adapt to new tasks without fine-tuning.
    This is particularly useful for adapting LLMs to new and/or rare ICD codes, which
    may not be frequently represented in training datasets, from just a few descriptions
    or examples of usage.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs展示了强大的零样本和少样本学习能力，使其能够在提供最少示例和指令的情况下适应新任务。增强生成（RAG）是另一种范式，它使LLMs能够访问更多的上下文信息，从而在不进行微调的情况下适应新任务。这对于将LLMs适应新的和/或稀有的ICD编码特别有用，因为这些编码在训练数据集中可能不会频繁出现，只需要通过一些描述或使用示例即可。
- en: 'Contextualizing Information:'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文化信息：
- en: LLMs are found to be effective at zero-shot relation extraction in the clinical
    domain [3] [4]. Zero-shot relation extraction allows LLMs to identify and categorize
    relationships in text without prior specific training on those relationships.
    This allows for better contextualization of the diagnosis in the medical coding
    to fetch more precise ICD codes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在临床领域的零样本关系提取方面被发现非常有效[3] [4]。零样本关系提取允许LLMs在没有针对特定关系的先验训练情况下识别和分类文本中的关系。这使得在医疗编码中对诊断进行更好的上下文化，从而获得更精确的ICD编码。
- en: 'Exploring the paper “Automated clinical coding using off-the-shelf large language
    models”:'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索论文《使用现成的大型语言模型进行自动化临床编码》：
- en: While exploring recent works that applied LLMs towards ICD coding, I came across
    a very interesting paper that leveraged LLMs for ICD coding without any specific
    fine-tuning. The authors came up with a method which they termed **LLM-guided
    tree-search** [5].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索最近应用LLM进行ICD编码的相关工作时，我发现了一篇非常有趣的论文，作者利用LLM进行ICD编码，而无需进行任何特定的微调。作者提出了一种方法，称为**LLM引导的树搜索**[5]。
- en: How does it work?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The ICD terminology is a hierarchical, tree-like structure. Each ICD code exists
    within this hierarchical structure where parent codes cover more general conditions,
    and child codes detail specific diseases. Traversing the ICD tree leads to more
    specific and fine-grained diagnosis codes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ICD术语是一种层次化的树状结构。每个ICD代码都存在于这个层次结构中，父级代码涵盖更广泛的疾病，而子级代码则详细描述特定的疾病。遍历ICD树可以得到更具体、更细化的诊断代码。
- en: In LLM-guided tree search, the search begins at the root and uses the LLM to
    select branches for exploration, continuing iteratively until all paths are exhausted.
    Practically, this process is implemented by providing the descriptions of all
    codes at any given level of the tree, along with the medical note, as a prompt
    to the LLM and asking it to identify the relevant codes for the medical note.
    The codes selected by the LLM in each instance are then further traversed and
    explored. This method identifies the most pertinent ICD codes, which are subsequently
    assigned as predicted labels for the clinical note.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM引导的树搜索中，搜索从根节点开始，使用LLM来选择要探索的分支，并且会迭代地进行，直到所有路径都被遍历。实际上，这个过程是通过将树中每一层所有代码的描述与医疗记录一起作为提示提供给LLM，要求其识别与医疗记录相关的代码。LLM在每个实例中选择的代码将进一步被遍历和探索。此方法能够识别出最相关的ICD代码，这些代码随后被分配为临床记录的预测标签。
- en: '![](../Images/0a2b99ac2336be8186f28fa9601522cb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a2b99ac2336be8186f28fa9601522cb.png)'
- en: The Tree-Search algorithm starts at the first level of the ICD tree. The descriptions
    of all the nodes in the first level along with the Medical Note are provided to
    the LLM, which is prompted to identify all relevant codes for the provided note.
    The output of the LLM is resolved as a set of Yes/No answers for each ICD code
    description. (Image by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Tree-Search算法从ICD树的第一层开始。将第一层所有节点的描述和医疗记录一起提供给LLM，并提示LLM识别与给定记录相关的所有代码。LLM的输出将解析为每个ICD代码描述的“是/否”答案集合。（图片由作者提供）
- en: 'Let’s clarify this with an example. Imagine a tree with two root nodes: ICD
    Code 1 and ICD Code 2\. Each node has a plain-text description characterizing
    the code. In the initial stage, the LLM is given the medical note along with the
    descriptions of the codes and asked to identify the codes pertinent to the medical
    note.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来澄清这个概念。假设有一棵树，包含两个根节点：ICD代码1和ICD代码2。每个节点都有一个描述文本，用于表征该代码。在初始阶段，LLM会接收医疗记录和代码的描述，并被要求识别与医疗记录相关的代码。
- en: '![](../Images/76ca69660ab14dc3e1c47bfdc5a52415.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76ca69660ab14dc3e1c47bfdc5a52415.png)'
- en: Given that the LLM predicted both ICD Code 1 and 2 as relevant to the medical
    note, the algorithm traverses the children of each of these nodes. Each node has
    2 children codes, and the LLM is again invoked for each node’s children individually
    to identify if the child nodes are relevant to the medical note. (Image by Author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM预测ICD代码1和2与医疗记录相关，算法会遍历每个节点的子节点。每个节点有2个子节点，LLM再次被调用来分别识别每个子节点是否与医疗记录相关。（图片由作者提供）
- en: In this scenario, the LLM identifies both ICD Code 1 and ICD Code 2 as relevant
    to the medical note. The algorithm then examines the child nodes of each code.
    Each parent code has two child nodes representing more specific ICD codes. Starting
    with ICD Code 1, the LLM uses the descriptions of ICD Code 1.1 and ICD Code 1.2
    along with the medical note to determine the relevant codes. The LLM concludes
    that ICD Code 1.1 is relevant, while ICD Code 1.2 is not. Since ICD Code 1.1 has
    no further child nodes, the algorithm checks if it is an assignable code and assigns
    it to the document. Next, the algorithm evaluates the child nodes of ICD Code
    2\. Invoking the LLM again, it determines that only ICD Code 2.1 is relevant.
    This is a simplified example; in reality, the ICD tree is extensive and deeper,
    meaning the algorithm will continue to traverse the children of each relevant
    node until it reaches the end of the tree or exhausts valid traversals.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，LLM 识别出 ICD 代码 1 和 ICD 代码 2 都与医疗记录相关。然后，算法检查每个代码的子节点。每个父代码有两个子节点，代表更具体的
    ICD 代码。从 ICD 代码 1 开始，LLM 使用 ICD 代码 1.1 和 ICD 代码 1.2 的描述与医疗记录一起确定相关代码。LLM 认为 ICD
    代码 1.1 相关，而 ICD 代码 1.2 不相关。由于 ICD 代码 1.1 没有进一步的子节点，算法检查它是否是一个可分配的代码，并将其分配给文档。接下来，算法评估
    ICD 代码 2 的子节点。再次调用 LLM，确定只有 ICD 代码 2.1 相关。这是一个简化的示例；在实际情况中，ICD 树非常庞大且较深，意味着算法将继续遍历每个相关节点的子节点，直到到达树的末端或耗尽有效的遍历。
- en: '**Highlights**'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**亮点**'
- en: This method does not require any fine-tuning of the LLM; it leverages the LLM’s
    ability to contextually understand the medical note and dynamically identify the
    relevant ICD codes based on the provided descriptions.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法不需要对 LLM 进行任何微调；它利用 LLM 在语境中理解医疗记录的能力，并基于提供的描述动态地识别相关的 ICD 代码。
- en: Furthermore, this paper shows that LLMs can effectively adapt to a large output
    space when given relevant information in the prompt, outperforming PLM-ICD [6]
    on rare codes in terms of macro-average metrics.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，本文显示，LLM 在提供相关信息的提示下，可以有效地适应广泛的输出空间，在宏平均指标上超越 PLM-ICD [6] 在稀有代码方面的表现。
- en: This technique also outperforms the baseline of directly asking the LLM to predict
    the ICD codes for a medical note based on its parametric knowledge. This highlights
    the potential in integrating LLMs with tools or external knowledge for solving
    clinical coding tasks.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该技术的表现也优于基准方法，即直接要求大型语言模型（LLM）基于其参数知识预测医疗记录的 ICD 代码。这突显了将 LLM 与工具或外部知识集成来解决临床编码任务的潜力。
- en: Drawbacks
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缺点
- en: The algorithm invokes the LLM at every level in the tree. That leads to a high
    number of LLM invocations as you traverse the tree, compounded by the vastness
    of the ICD tree. This leads to high latency and costs in processing a single document.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该算法在树的每一层都会调用 LLM。这导致在遍历树时调用 LLM 的次数非常多，再加上 ICD 树的庞大，这会导致处理单个文档时的高延迟和高成本。
- en: As the authors also note in the paper, in order to correctly predict a relevant
    code, the LLM must correctly identify its parent nodes at all levels. Even if
    a mistake is made at one level, the LLM will be unable to reach the final relevant
    code.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如作者在论文中提到的那样，为了正确预测相关代码，LLM 必须在所有层级上正确识别其父节点。即使在某一层级上出现错误，LLM 也无法到达最终的相关代码。
- en: The authors were unable to evaluate their method using datasets like MIMIC-III
    due to limitations that prohibit the transfer of data to external services such
    as OpenAI’s GPT endpoints. Instead, they evaluated the method using the test set
    of the CodiEsp dataset [7,8], which includes 250 medical notes. The small size
    of this dataset suggests that the method’s effectiveness on larger clinical datasets
    is yet to be established.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于限制因素无法将数据传输到像 OpenAI GPT 终端这样的外部服务，作者无法使用像 MIMIC-III 这样的数据集来评估他们的方法。相反，他们使用
    CodiEsp 数据集 [7,8] 的测试集进行了评估，该数据集包含 250 条医疗记录。该数据集的较小规模表明，该方法在更大的临床数据集上的有效性尚待验证。
- en: '**Implementing the technique described in the paper**'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**实施论文中描述的技术**'
- en: All code and resources related to this article are made available at [this link](https://github.com/anand-subu/automated-clinical-coding-llm)
    with a mirror of the repo available in my original [blog-related repository](https://github.com/anand-subu/blog_resources).
    I wish to stress that my reimplementation is not exactly identical to the paper
    and differs in subtle ways that I’ve documented in the original repository. I’ve
    tried to replicate the prompts used for invoking GPT-3.5 and Llama-70B based on
    the details in the original paper. For translating the datasets from Spanish to
    English, I created my own prompt for doing that, as the details were not accessible
    in the paper.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 与本文相关的所有代码和资源都可以在[这个链接](https://github.com/anand-subu/automated-clinical-coding-llm)找到，且我的原始[博客相关库](https://github.com/anand-subu/blog_resources)中有该仓库的镜像。我希望强调的是，我的重实现并不完全与论文相同，并且在细节上有所不同，我已经在原始库中记录了这些差异。我尽力根据原论文中的细节复制了用于调用GPT-3.5和Llama-70B的提示语。至于将数据集从西班牙语翻译成英文，我创建了自己的提示语来执行此操作，因为论文中没有提供相关细节。
- en: Let’s implement the technique to better understand how it works. As mentioned,
    the paper uses the CodiEsp test set for its evaluation. This dataset consists
    of Spanish medical notes along with their ICD codes. Although the dataset includes
    an English translated version, the authors note that they translated the Spanish
    medical notes into English using GPT-3.5, which they claim provided a modest performance
    improvement over using the pre-translated version. We replicate this functionality
    and translate the notes into English.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现这个技术，以更好地理解它是如何工作的。如前所述，本文使用了CodiEsp测试集进行评估。该数据集包含西班牙语医学笔记及其ICD编码。尽管数据集中有英文翻译版，但作者指出，他们使用GPT-3.5将西班牙语医学笔记翻译成英文，并声称这种方式比使用预先翻译版本提供了适度的性能提升。我们复制了这个功能并将笔记翻译成英文。
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have the evaluation corpus ready, let’s implement the core logic
    for the tree-search algorithm. We define the functionality in **get_icd_codes**,
    which accepts the medical note to process, the model name, and the temperature
    setting. The model name must be either **“gpt-3.5-turbo-0613”** for GPT-3.5 or
    **“meta-llama/Llama-2–70b-chat-hf”** for Llama-2 70B Chat. This specification
    determines the LLM that the tree-search algorithm will invoke during its processing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 既然评估语料库已经准备好，让我们实现树搜索算法的核心逻辑。我们在**get_icd_codes**中定义该功能，接受需要处理的医学笔记、模型名称和温度设置。模型名称必须是**“gpt-3.5-turbo-0613”**用于GPT-3.5，或**“meta-llama/Llama-2–70b-chat-hf”**用于Llama-2
    70B Chat。这个规格确定了树搜索算法在处理过程中将调用的LLM。
- en: Evaluating GPT-4 is possible using the same code-base by providing the appropriate
    model name, but we choose to skip it as it is quite time-consuming.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的代码库评估GPT-4是可能的，只需提供适当的模型名称，但由于非常耗时，我们选择跳过这一步。
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to the paper, we use the [**simple_icd_10_cm**](https://github.com/StefanoTrv/simple_icd_10_CM)library,
    which provides access to the ICD-10 tree. This allows us to traverse the tree,
    access the descriptions for each code, and identify valid codes. First, we get
    the nodes at the first level of the tree.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于论文中使用的方法，我们使用了[**simple_icd_10_cm**](https://github.com/StefanoTrv/simple_icd_10_CM)库，该库提供了访问ICD-10树的功能。这使我们能够遍历树，访问每个编码的描述，并识别有效的编码。首先，我们获取树的第一层节点。
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Inside the loop, we obtain the descriptions corresponding to each of the nodes.
    Now, we need to construct the prompt for the LLM based on the medical note and
    the code descriptions. We create the prompts for GPT-3.5 and Llama-2 based on
    the details provided in the paper.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环内部，我们获取与每个节点对应的描述。现在，我们需要根据医学笔记和编码描述为LLM构建提示语。我们根据论文中提供的细节，为GPT-3.5和Llama-2创建提示语。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We now construct the prompt based on the medical note and code descriptions.
    An advantage for us, in terms of prompting and coding, is that we can use the
    same **openai** library to interact with both GPT-3.5 and Llama 2, provided that
    Llama-2 is deployed using [deepinfra](https://deepinfra.com/), which also supports
    the **openai** format for sending requests to the LLM.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们基于医学笔记和编码描述构建提示语。在提示和编码方面，对我们有利的是，我们可以使用相同的**openai**库与GPT-3.5和Llama 2进行交互，前提是Llama-2通过[deepinfra](https://deepinfra.com/)部署，并且deepinfra也支持**openai**格式来向LLM发送请求。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Having constructed the prompts, we now invoke the LLM to obtain the response:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建好提示语后，我们现在调用LLM以获取响应：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Great, we’ve obtained the output! From the response, we now parse each code
    description to identify the nodes that the LLM has deemed relevant for further
    traversal, as well as those nodes the LLM has rejected. We break the output response
    into new lines and split each response to identify the prediction of the LLM for
    each code description.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们已经得到了输出！从响应中，我们现在解析每个代码描述，识别出LLM认为需要进一步遍历的节点，以及那些被LLM拒绝的节点。我们将输出响应分成新行，并拆分每个响应，以识别LLM对每个代码描述的预测。
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let’s look at the remainder of the loop now. So far, we have constructed the
    prompt, obtained the response from the LLM, and parsed the output to identify
    the codes deemed relevant by the LLM.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下循环的其余部分。到目前为止，我们已经构建了提示，获取了LLM的响应，并解析了输出以识别LLM认为相关的代码。
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we iterate through the predicted codes and check if each code is a **“leaf”**
    code, which essentially ensures that the code is a valid and assignable ICD code.
    If the predicted code is valid, we consider it as a prediction by the LLM for
    that medical note. If not, we add it to our parent codes and obtain the children
    nodes to further traverse the ICD tree. We break out of the loop if there are
    no more parent codes to further traverse.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们遍历预测的代码，并检查每个代码是否为**“叶子”**代码，这本质上确保了该代码是一个有效且可分配的ICD代码。如果预测的代码有效，我们将其视为LLM对该医疗记录的预测。如果无效，我们将其添加到父代码中，并获取子节点以进一步遍历ICD树。如果没有更多的父代码可供进一步遍历，我们将跳出循环。
- en: In theory, the number of LLM invocations per medical note can be arbitrarily
    high, leading to increased latency if the algorithm traverses many nodes. The
    authors enforce a maximum of 50 prompts/LLM invocations per medical note to terminate
    the processing, a limit we also adopt in our implementation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，每个医疗记录的LLM调用次数可以非常高，如果算法遍历许多节点，可能会导致延迟增加。作者强制规定每个医疗记录最多50次提示/LLM调用来终止处理，这是我们在实现中也采用的限制。
- en: Results
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: We can now evaluate the results of the tree-search algorithm using GPT-3.5 and
    Llama-2 as the LLMs. We assess the performance of the algorithm in terms of micro-average
    and macro-average precision, recall, and F1-score.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用GPT-3.5和Llama-2作为LLM来评估树搜索算法的结果。我们根据微平均（micro-average）和宏平均（macro-average）精度、召回率以及F1分数来评估算法的表现。
- en: '![](../Images/a5e30b996754e8cab45988d7e2ce23e8.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5e30b996754e8cab45988d7e2ce23e8.png)'
- en: Results of our implementation for GPT-3.5 and Llama-2 70B Chat
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPT-3.5和Llama-2 70B Chat实现结果
- en: While the implementation’s results are roughly in the ball-park of the reported
    scores in the paper, there are some note-worthy differences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管实现结果大致与论文中报告的分数相符，但仍存在一些值得注意的差异。
- en: In this implementation, GPT-3.5’s micro-average metrics slightly exceed the
    reported figures, while the macro-average metrics fall a bit short of the reported
    values.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个实现中，GPT-3.5的微平均指标略高于报告的数值，而宏平均指标稍微低于报告的值。
- en: Similarly, Llama-70B’s micro-average metrics either match or slightly exceed
    the reported figures, but the macro-average metrics are lower than the reported
    values.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，Llama-70B的微平均指标要么与报告的数值相符，要么略有超出，但宏平均指标低于报告的数值。
- en: As mentioned earlier, this implementation differs from the paper in a few minor
    ways, all of which impact the final performance. Please refer to the [linked repository](https://github.com/anand-subu/automated-clinical-coding-llm)
    for a more detailed discussion of how this implementation differs from the original
    paper.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这个实现与论文中的方法在一些小地方有所不同，这些差异影响了最终的表现。有关本实现与原始论文的差异，敬请参考[相关仓库](https://github.com/anand-subu/automated-clinical-coding-llm)以获得更详细的讨论。
- en: Conclusion
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Understanding and implementing this method was quite insightful for me in many
    ways. It allowed me to develop a more nuanced understanding of the strengths and
    weaknesses of Large Language Models (LLMs) in the clinical coding case. Specifically,
    it became evident that providing LLMs dynamic access to pertinent information
    about codes can help improve their performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 理解并实现这种方法在许多方面对我来说都是一次很有启发的经历。它让我对大语言模型（LLMs）在临床编码案例中的优缺点有了更细致的理解。具体来说，显而易见的是，向LLMs提供关于代码的动态相关信息可以帮助提高它们的表现。
- en: It would be interesting to explore whether utilizing LLMs as agents for clinical
    coding could further improve performance. Given the abundance of external knowledge
    sources for biomedical and clinical texts in the form of papers or knowledge graphs,
    LLM agents could potentially be used in workflows that analyze medical documents
    at a finer granularity. They could also invoke tools that allow them to refer
    to external knowledge on the fly if required, to arrive at the final code.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 探索将LLMs作为临床编码代理是否能进一步提高性能将是很有趣的。考虑到生物医学和临床文本中外部知识源（如论文或知识图谱）的丰富性，LLM代理可以潜在地用于分析医疗文档的工作流程，进行更精细的粒度分析。如果需要，它们还可以调用工具，在必要时即时参考外部知识，以最终得出编码结果。
- en: Limitations
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'In this article, while I tried to analyze how LLMs can help with respect to
    ICD coding, there are also some practical limitations to consider in general:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，尽管我尝试分析LLMs如何帮助进行ICD编码，但也有一些实际的局限性需要考虑：
- en: LLMs require significant computational resources for deployment. This leads
    to considerations such as the need for powerful GPUs without which applications
    may suffer from high latency, which can constrain their adoption.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在部署时需要大量计算资源。这导致了诸如需要强大GPU的考虑，缺乏GPU可能会导致应用程序出现高延迟，从而限制其应用。
- en: Furthermore, healthcare data processing may often require strict data security
    and privacy safeguards. Online LLM services may not necessarily comply with the
    security and privacy standards required for healthcare data processing.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，医疗数据处理通常可能需要严格的数据安全和隐私保护措施。在线LLM服务可能并不一定符合医疗数据处理所需的安全和隐私标准。
- en: Acknowledgement
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Huge thanks to Joseph, the lead author of this paper, for clarifying my doubts
    regarding the evaluation of this method!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢本文的主作者Joseph，他澄清了我在评估该方法时的疑问！
- en: 'References:'
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] [https://www.who.int/standards/classifications/classification-of-diseases](https://www.who.int/standards/classifications/classification-of-diseases)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.who.int/standards/classifications/classification-of-diseases](https://www.who.int/standards/classifications/classification-of-diseases)'
- en: '[2] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi,
    M., … & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database
    Sci. *Data*, *3*(1), 1.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L. W. H., Feng, M., Ghassemi,
    M., … & Mark, R. G. (2016). MIMIC-III，一个可自由访问的重症监护数据库。Sci. *Data*，*3*(1)，1。'
- en: '[3] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., & Sontag, D. (2022). Large
    language models are few-shot clinical information extractors. *arXiv preprint
    arXiv:2205.12689*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., & Sontag, D. (2022). 大型语言模型是少量样本的临床信息提取器。*arXiv预印本arXiv:2205.12689*。'
- en: '[4] Zhou, H., Li, M., Xiao, Y., Yang, H., & Zhang, R. (2023). LLM Instruction-Example
    Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction. *medRxiv
    : the preprint server for health sciences*, 2023.12.15.23300059\. [https://doi.org/10.1101/2023.12.15.23300059](https://doi.org/10.1101/2023.12.15.23300059)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Zhou, H., Li, M., Xiao, Y., Yang, H., & Zhang, R. (2023). LLM 指令-示例自适应提示（LEAP）框架用于临床关系提取。*medRxiv：健康科学预印本服务器*，2023.12.15.23300059。
    [https://doi.org/10.1101/2023.12.15.23300059](https://doi.org/10.1101/2023.12.15.23300059)'
- en: '[5] Boyle, J. S., Kascenas, A., Lok, P., Liakata, M., & O’Neil, A. Q. (2023,
    October). Automated clinical coding using off-the-shelf large language models.
    In *Deep Generative Models for Health Workshop NeurIPS 2023*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Boyle, J. S., Kascenas, A., Lok, P., Liakata, M., & O’Neil, A. Q. (2023年10月).
    使用现成的大型语言模型进行自动化临床编码。发表于*深度生成模型在健康领域工作坊 NeurIPS 2023*。'
- en: '[6] Huang, C. W., Tsai, S. C., & Chen, Y. N. (2022). PLM-ICD: automatic ICD
    coding with pretrained language models. *arXiv preprint arXiv:2207.05289*.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Huang, C. W., Tsai, S. C., & Chen, Y. N. (2022). PLM-ICD：使用预训练语言模型自动化ICD编码。*arXiv预印本arXiv:2207.05289*。'
- en: '[7] Miranda-Escalada, A., Gonzalez-Agirre, A., Armengol-Estapé, J., & Krallinger,
    M. (2020). Overview of Automatic Clinical Coding: Annotations, Guidelines, and
    Solutions for non-English Clinical Cases at CodiEsp Track of CLEF eHealth 2020\.
    *CLEF (Working Notes)*, *2020*.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Miranda-Escalada, A., Gonzalez-Agirre, A., Armengol-Estapé, J., & Krallinger,
    M. (2020). 自动化临床编码概述：注释、指南以及CodiEsp赛道上针对非英语临床案例的解决方案，发表于CLEF eHealth 2020。*CLEF（工作笔记）*，*2020*。'
- en: '[8] Miranda-Escalada, A., Gonzalez-Agirre, A., & Krallinger, M. (2020). CodiEsp
    corpus: gold standard Spanish clinical cases coded in ICD10 (CIE10) — eHealth
    CLEF2020 (1.4) [Data set]. Zenodo. [https://doi.org/10.5281/zenodo.3837305](https://doi.org/10.5281/zenodo.3837305)
    (CC BY 4.0)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Miranda-Escalada, A., Gonzalez-Agirre, A., & Krallinger, M. (2020). CodiEsp
    语料库：使用 ICD10（CIE10）编码的西班牙临床病例金标准 — eHealth CLEF2020 (1.4) [数据集]。Zenodo。[https://doi.org/10.5281/zenodo.3837305](https://doi.org/10.5281/zenodo.3837305)
    (CC BY 4.0)'
