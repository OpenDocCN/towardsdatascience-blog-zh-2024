- en: Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build your own deep learning framework based on C/C++, CUDA, and Python, with
    GPU support and automatic differentiation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    ¬∑24 min read¬∑May 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fb280914af3d0239f5a27ae8d414ce0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For many years I have been using PyTorch to construct and train deep learning
    models. Even though I have learned its syntax and rules, something has always
    aroused my curiosity: what is happening internally during these operations? How
    does all of this work?'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have gotten here, you probably have the same questions. If I ask you
    how to create and train a model in PyTorch, you will probably come up with something
    similar to the code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: But what if I ask you how does this backward step works? Or, for instance, what
    happens when you reshape a tensor? Is the data rearranged internally? How does
    that happens? Why is PyTorch so fast? How does PyTorch handle GPU operations?
    These are the types of questions that have always intrigued me, and I imagine
    they also intrigue you. Thus, in order to better understand these concepts, what
    is better than building your own tensor library ***from scratch***? And that is
    what you will learn in this article!
  prefs: []
  type: TYPE_NORMAL
- en: '#1 ‚Äî Tensor'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to construct a *tensor library*, the first concept you need to learn
    obviously is: what is a tensor?'
  prefs: []
  type: TYPE_NORMAL
- en: You may have an intuitive idea that a tensor is a mathematical concept of a
    n-dimensional data structure that contains some numbers. But here we need to understand
    how to model this data structure from a computational perspective. We can think
    of a tensor as consisting of the data itself and also some metadata describing
    aspects of the tensor such as its shape or the device it lives in (i.e. CPU memory,
    GPU memory‚Ä¶).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/126751c339f220fec6bc0a40690fb069.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: There is also a less popular metadata that you may have never heard of, called
    *stride*. This concept is very important to understand the internals of tensor
    data rearrangement, so we need to discuss it a little more.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a 2-D tensor with shape [4, 8], illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/169e1865490d4603d66dc792052c1458.png)'
  prefs: []
  type: TYPE_IMG
- en: 4x8 Tensor (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The data (i.e. float numbers) of a tensor is actually stored as a 1-dimensional
    array on memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edc491c716df7cab51cd1ca4a61ff296.png)'
  prefs: []
  type: TYPE_IMG
- en: 1-D dimensional data array of the tensor (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in order to represent this 1-dimensional array as a N-dimensional tensor,
    we use strides. Basically the idea is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a matrix with 4 rows and 8 columns. Considering that all of its elements
    are organized by rows on the 1-dimensional array, if we want to access the value
    at position [2, 3], we need to traverse 2 rows (of 8 elements each) plus 3 positions.
    In mathematical terms, we need to traverse 3 + 2 * 8 elements on the 1-dimensional
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/644a13550e4601385357edc5bb88071b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: So this ‚Äò8‚Äô is the *stride* of the *second* dimension. In this case, it is the
    information of how many elements I need to traverse on the array to ‚Äújump‚Äù to
    other positions on the *second* *dimension.*
  prefs: []
  type: TYPE_NORMAL
- en: Thus, for accessing the element [i, j] of a 2-dimensional tensor with shape
    [shape_0, shape_1], we basically need to access the element at position j + i
    * shape_1
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us imagine a 3-dimensional tensor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc1c13d004a29b88a4f73e8b11a51483.png)'
  prefs: []
  type: TYPE_IMG
- en: 5x4x8 Tensor (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: You can think of this 3-dimensional tensor as a sequence of matrices. For example,
    you can think of this [5, 4, 8] tensor as 5 matrices of shape [4, 8].
  prefs: []
  type: TYPE_NORMAL
- en: Now, in order to access the element at position [1, 2, 7], you need to traverse
    1 complete matrix of shape [4,8], 2 rows of shape [8] and 7 columns of shape [1].
    So, you need to traverse (1 * 4 * 8) + (2 * 8) + (7 * 1) positions on the 1-dimensional
    array.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/292c1ea36eb8d9c10528822680f5c698.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to access the element [i][j][k] of a 3-D tensor with [shape_0, shape_1,
    shape_2] on the 1-D data array, you do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81de413fc9cf4c48a4a1b0c764dea74f.png)'
  prefs: []
  type: TYPE_IMG
- en: This shape_1 * shape_2 is the *stride* of the first dimension, the shape_2 is
    the *stride* of the second dimension and 1 is the stride of the third dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in order to generalize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the *strides* of each dimension can be calculated using the product of
    the next dimension tensor shapes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b9a7f16cd0d11e4767e681da1993aef.png)'
  prefs: []
  type: TYPE_IMG
- en: Then we set stride[n-1] = 1.
  prefs: []
  type: TYPE_NORMAL
- en: On our tensor example of shape [5, 4, 8] we would have strides = [4*8, 8, 1]
    = [32, 8, 1]
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Ok, but why do we need shapes and strides? Beyond accessing elements of N-dimensional
    tensors stored as 1-dimensional arrays, this concept can be used to manipulate
    tensor arrangements very easily.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to reshape a tensor, you only need to set the new shape and calculate
    the new strides based on it! (since the new shape guarantees the same number of
    elements)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Internally, the tensor is still stored as the same 1-dimensional array. The
    reshape method did not change the order of the elements within the array! That‚Äôs
    amazing, isn‚Äôt? üòÅ
  prefs: []
  type: TYPE_NORMAL
- en: 'You can verify on your own using the following function that accesses the internal
    1-dimensional array on PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Or for instance, you want to transpose two axes. Internally, you just need to
    swap the respective strides!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you print the internal array, both have the same values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: However, the stride of `new_t` now does not match with the equation I showed
    above. This is due to the fact that the tensor is now not contiguous. That means
    that although the internal array remains the same, the order of its values in
    memory does not match with the actual order of the tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the accessing the non-contiguous elements in sequence is less efficient
    (as the real tensor elements is not ordered in sequence on memory). In order to
    fix that, we can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we analyze the internal array, its order now matches with the actual tensor
    order now, which can provide better memory access efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now that we comprehend how a tensor is modeled, let us start creating our library!
  prefs: []
  type: TYPE_NORMAL
- en: I will call it *Norch*, which stands for NOT PyTorch, and also makes an allusion
    to my last name, Nogueira üòÅ
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to know is that although PyTorch is used through Python, internally
    it runs C/C++. So we will first create our internals C/C++ functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can first define a tensor as a struct to store its data and metadata, and
    create a function to instantiate it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to access some element, we can take advantage of strides, as we learned
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can create the tensor operations. I will show some examples and you
    can find the complete version in the repository linked at the end of this article.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After that we are able to create our other tensor functions that will call
    these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned before, the tensor reshaping does not modify the internal data
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Although we can now do some tensor operations, nobody deserves to run it using
    C/C++, right? Let us start building our Python wrapper!
  prefs: []
  type: TYPE_NORMAL
- en: There are a lot of options to run C/C++ code using Python, such as *Pybind11*
    and *Cython.* For our example I will use *ctypes.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The basically structure of *ctypes* is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, it is very intuitive. After you compile the C/C++ code, you
    can use it on Python with *ctypes* very easily. You only need to define the arguments
    & return c_types of the function, convert the variable to its respective c_types
    and call the function. For more complex types such as arrays (float lists) you
    can use pointers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And for struct types we can create our own c_type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: After this brief explanation, let us construct the Python wrapper for our tensor
    C/C++ library!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now we include the Python tensor operations to call the C/C++ operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you got here, you are now capable to run the code and start doing some tensor
    operations!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#2 ‚Äî GPU Support'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After creating the basic structure of our library, now we will take it to a
    new level. It is well-known that you can call `.to("cuda")` to send data to GPU
    and run math operations faster. I will assume that you have basic knowledge on
    how CUDA works, but if you do not, you can read my other article: [CUDA tutorial.](/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    I will wait here. üòä'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'For those in a hurry, a simple introduction here:'
  prefs: []
  type: TYPE_NORMAL
- en: Basically, all of our code until here is running on CPU memory. Altough for
    single operations CPUs are faster, the advantage of GPUs relies on its parallelization
    capabilities. While CPU design aims to execute a sequence of operations (threads)
    fast (but can only execute dozens of them), the GPU design aims to execute millions
    of operations in parallel (by sacrificing individual threads performance).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can take advantage of this capability to perform operations in parallel.
    For example, in a million-sized tensor addition, instead of adding the elements
    of each index sequentially inside a loop, using a GPU we can add all of them in
    parallel at once. In order to do that, we can use CUDA, which is a platform developed
    by NVIDIA to enable developers to integrate GPU support to their software applications.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do that, you can use CUDA C/C++, which is a simple C/C++ based interface
    designed to run specific GPU operations (such as copy data from CPU memory to
    GPU memory).
  prefs: []
  type: TYPE_NORMAL
- en: The code below basically uses some CUDA C/C++ functions to copy data from CPU
    to GPU, and run the AddTwoArrays function (also called kernel) on a total of N
    GPU threads in parallel, each responsible to add a different element of the array.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can notice, instead of adding each element pair per operation, we run
    all of the adding operations in parallel, getting rid of the loop instruction.
  prefs: []
  type: TYPE_NORMAL
- en: After this brief introduction, we can go back to our tensor library.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a function to send tensor data from CPU to GPU and
    vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create GPU versions for all of our tensor operations. I will write
    examples for addition and subtraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsequently, we include a new tensor attribute `char* device` on the `tensor.cpp`
    and we can use it to select where the operations will be run (CPU or GPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now our library has GPU support!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '#3 ‚Äî Automatic Differentation (Autograd)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the main reasons why PyTorch got so popular is due to its Autograd module.
    It is a core component that allows automatic differentiation in order to compute
    gradients (crucial for training models with optimization algorithms such as gradient
    descent). By calling a single method `.backward()`, it computes all of the gradients
    from previous tensor operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to understand what is happening, let‚Äôs try to replicate the same procedure
    manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/255a2823dc15242c9d7414c9175d073d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let‚Äôs first calculate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d5353e2d05c964798dfe75e81572db5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that *x* is a matrix, thus we need to calculate the derivative of L with
    respect to each element individually. Additionally, L is a summation over all
    elements, but it is important to remember that for each elements, the other elements
    does not interfere on its derivative. Therefore, we obtain the following term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/612ffe8a8a672f3c2ebac55743b1a82c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By applying the chain rule for each term, we differentiate the outer function
    and multiply by the derivative of the inner function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0834be1c1e2772168593070107e386de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4dec4240d0004de073e923b6d937831d.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have the following final equation to calculate the derivative of L
    with respect to *x:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f6843db3a0d35b765a74b1241ac61ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting the values into the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95897f1570971bf15b239d7182f2b8cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Calculating the result, we get the same values we obtained with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let‚Äôs analyze what we just did:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basically, we observed all the operations involved in reserved order: a summation,
    a power of 3 and a subtraction. Then, we applied chain of rule, calculating the
    derivative of each operation and recursively calculated the derivative for the
    next operation. So, first we need an implementation of the derivative for different
    math operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For addition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0bea287a54af0930d57584959d48ecd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For sin:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'For cosine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4359a9a560ad9072af54cf872019b3bd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'For element-wise multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3564658c2002c0917544fbbef510374c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'For summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You can access the GitHub repository link at the end of the article to explore
    other operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have derivative expressions for each operation, we can proceed
    with implementing the recursive backward chain rule. We can set a `requires_grad`
    argument for our tensor to indicate that we want to store the gradients of this
    tensor. If true, we will store the gradients for each tensor operation. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, implement the `.backward()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, just implement `.zero_grad()` to zero the gradient of a tensor, and
    `.detach()` to remove the tensor autograd history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You just created a complete tensor library with GPU support
    and automatic differentiation! Now we can create the `nn` and `optim` modules
    to train some deep learning models more easily.
  prefs: []
  type: TYPE_NORMAL
- en: '#4 ‚Äî nn and optim modules'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nn` is a module for building neural networks and deep learning models and
    `optim` is related to optimization algorithms to train these models. In order
    to recreate them, the first thing to do is implementing a Parameter, which simply
    is a trainable tensor, with the same operations, but with `requires_grad` set
    always as `True` and with some random initialization technique.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'By using parameters, we can start contructing modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: For example, we can construct our custom modules by inheriting from `nn.Module`,
    or we can use some previously created modules, such as the `linear`, which implements
    the ***y*** *= W****x*** *+ b* operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can implement some loss and activation functions. For instance, a mean
    squared error loss and a sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, create the optimizers. On our example I will implement the Stochastic
    Gradient Descent algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: And, that‚Äôs it! We just created our own deep learning framework! ü•≥
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs do some training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e7662c657102cedd3717cdac495019f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The model was successfully created and trained using our custom deep learning
    framework!
  prefs: []
  type: TYPE_NORMAL
- en: You can check the complete code [here](https://github.com/lucasdelimanogueira/PyNorch).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post we covered the basic concepts of what is a tensor, how it is modeled
    and more advanced topics such as CUDA and Autograd. We successfully created a
    deep learning framework with GPU support and automatic differentiation. I hope
    this post helped you to briefly understand how PyTorch works under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: In future posts, I will try to cover more advanced topics such as distributed
    training (multinode / multi GPU) and memory management. Please let me know what
    you think or what you would like me to write about next in the comments! Thanks
    so much for reading! üòä
  prefs: []
  type: TYPE_NORMAL
- en: Also, follow me here and on my [LinkedIn profile](https://www.linkedin.com/in/lucas-de-lima-nogueira/)
    to stay updated on my latest articles!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) ‚Äî GitHub repository
    of this project.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Tutorial CUDA](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    ‚Äî A brief introduction on how CUDA works.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch](https://pytorch.org/docs/stable/index.html) ‚Äî PyTorch documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[MartinLwx‚Äôs blog](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)
    ‚Äî Tutorial on strides.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Strides tutorial](https://ajcr.net/stride-guide-part-1/) ‚Äî Another tutorial
    about strides.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/) ‚Äî A
    guide on how PyTorch is structured.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Nets](https://github.com/arthurdjn/nets) ‚Äî A PyTorch recreation using NumPy.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) ‚Äî A live coding
    of an Autograd library.'
  prefs: []
  type: TYPE_NORMAL
