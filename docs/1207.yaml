- en: Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹é‡å»ºPyTorchï¼ˆæ”¯æŒGPUå’Œè‡ªåŠ¨æ±‚å¯¼ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14](https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14)
- en: Build your own deep learning framework based on C/C++, CUDA, and Python, with
    GPU support and automatic differentiation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºäºC/C++ã€CUDAå’ŒPythonæ„å»ºä½ è‡ªå·±çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒGPUå¹¶å…·æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½
- en: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[![Lucas
    de Lima Nogueira](../Images/76edd8ee4005d4c0b8bd476261eb06ae.png)](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    [Lucas de Lima Nogueira](https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    Â·24 min readÂ·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------)
    Â·é˜…è¯»æ—¶é•¿24åˆ†é’ŸÂ·2024å¹´5æœˆ14æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7fb280914af3d0239f5a27ae8d414ce0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fb280914af3d0239f5a27ae8d414ce0.png)'
- en: Image by Author with the assistance of AI ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…ä¸AIåä½œåˆ¶ä½œ ([https://copilot.microsoft.com/images/create](https://copilot.microsoft.com/images/create))
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: 'For many years I have been using PyTorch to construct and train deep learning
    models. Even though I have learned its syntax and rules, something has always
    aroused my curiosity: what is happening internally during these operations? How
    does all of this work?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¹´æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨ä½¿ç”¨PyTorchæ„å»ºå’Œè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å°½ç®¡æˆ‘å·²ç»å­¦ä¼šäº†å®ƒçš„è¯­æ³•å’Œè§„åˆ™ï¼Œä½†ä¸€ç›´æœ‰ä¸€ä¸ªé—®é¢˜è®©æˆ‘æ„Ÿåˆ°å¥½å¥‡ï¼šåœ¨è¿™äº›æ“ä½œè¿‡ç¨‹ä¸­ï¼Œå†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿè¿™ä¸€åˆ‡æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: 'If you have gotten here, you probably have the same questions. If I ask you
    how to create and train a model in PyTorch, you will probably come up with something
    similar to the code below:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»è¯»åˆ°è¿™é‡Œï¼Œä½ å¯èƒ½æœ‰ç›¸åŒçš„é—®é¢˜ã€‚å¦‚æœæˆ‘é—®ä½ å¦‚ä½•åœ¨PyTorchä¸­åˆ›å»ºå’Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½ å¯èƒ½ä¼šå†™å‡ºç±»ä¼¼äºä¸‹é¢çš„ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: But what if I ask you how does this backward step works? Or, for instance, what
    happens when you reshape a tensor? Is the data rearranged internally? How does
    that happens? Why is PyTorch so fast? How does PyTorch handle GPU operations?
    These are the types of questions that have always intrigued me, and I imagine
    they also intrigue you. Thus, in order to better understand these concepts, what
    is better than building your own tensor library ***from scratch***? And that is
    what you will learn in this article!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœæˆ‘é—®ä½ ï¼Œåå‘ä¼ æ’­æ˜¯å¦‚ä½•å·¥ä½œçš„å‘¢ï¼Ÿæˆ–è€…ï¼Œä¾‹å¦‚ï¼Œå½“ä½ é‡æ–°å¡‘å½¢ä¸€ä¸ªå¼ é‡æ—¶ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæ•°æ®æ˜¯å¦åœ¨å†…éƒ¨é‡æ–°æ’åˆ—ï¼Ÿè¿™ä¸€è¿‡ç¨‹æ˜¯å¦‚ä½•å‘ç”Ÿçš„ï¼Ÿä¸ºä»€ä¹ˆPyTorchå¦‚æ­¤é«˜æ•ˆï¼ŸPyTorchæ˜¯å¦‚ä½•å¤„ç†GPUæ“ä½œçš„ï¼Ÿè¿™äº›é—®é¢˜ä¸€ç›´ä»¤æˆ‘æ„Ÿåˆ°å¥½å¥‡ï¼Œæˆ‘æƒ³å®ƒä»¬ä¹Ÿä¼šè®©ä½ æ„Ÿå…´è¶£ã€‚å› æ­¤ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™äº›æ¦‚å¿µï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿé‚£å°±æ˜¯ä»***é›¶å¼€å§‹***æ„å»ºä½ è‡ªå·±çš„å¼ é‡åº“ï¼è€Œè¿™æ­£æ˜¯ä½ å°†åœ¨æœ¬æ–‡ä¸­å­¦åˆ°çš„å†…å®¹ï¼
- en: '#1 â€” Tensor'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#1 â€” å¼ é‡'
- en: 'In order to construct a *tensor library*, the first concept you need to learn
    obviously is: what is a tensor?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ„å»ºä¸€ä¸ª*å¼ é‡åº“*ï¼Œä½ é¦–å…ˆéœ€è¦äº†è§£çš„æ¦‚å¿µæ˜¾ç„¶æ˜¯ï¼šä»€ä¹ˆæ˜¯å¼ é‡ï¼Ÿ
- en: You may have an intuitive idea that a tensor is a mathematical concept of a
    n-dimensional data structure that contains some numbers. But here we need to understand
    how to model this data structure from a computational perspective. We can think
    of a tensor as consisting of the data itself and also some metadata describing
    aspects of the tensor such as its shape or the device it lives in (i.e. CPU memory,
    GPU memoryâ€¦).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šç›´è§‚åœ°è®¤ä¸ºå¼ é‡æ˜¯ä¸€ä¸ªæ•°å­¦æ¦‚å¿µï¼Œè¡¨ç¤ºä¸€ä¸ªåŒ…å«ä¸€äº›æ•°å­—çš„ n ç»´æ•°æ®ç»“æ„ã€‚ä½†åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦ä»è®¡ç®—çš„è§’åº¦ç†è§£å¦‚ä½•å»ºæ¨¡è¿™ä¸ªæ•°æ®ç»“æ„ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå¼ é‡çœ‹ä½œæ˜¯ç”±æ•°æ®æœ¬èº«å’Œä¸€äº›å…ƒæ•°æ®ç»„æˆï¼Œè¿™äº›å…ƒæ•°æ®æè¿°äº†å¼ é‡çš„æŸäº›æ–¹é¢ï¼Œå¦‚å®ƒçš„å½¢çŠ¶æˆ–å®ƒæ‰€å­˜å‚¨çš„è®¾å¤‡ï¼ˆå³
    CPU å†…å­˜ã€GPU å†…å­˜ç­‰ï¼‰ã€‚
- en: '![](../Images/126751c339f220fec6bc0a40690fb069.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/126751c339f220fec6bc0a40690fb069.png)'
- en: Image by Author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: There is also a less popular metadata that you may have never heard of, called
    *stride*. This concept is very important to understand the internals of tensor
    data rearrangement, so we need to discuss it a little more.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä¸ªä¸å¤ªå¸¸è§çš„å…ƒæ•°æ®ï¼Œå¯èƒ½ä½ ä»æœªå¬è¯´è¿‡ï¼Œå«åš *æ­¥å¹…*ã€‚è¿™ä¸ªæ¦‚å¿µå¯¹äºç†è§£å¼ é‡æ•°æ®é‡æ’çš„å†…éƒ¨åŸç†éå¸¸é‡è¦ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ›´è¯¦ç»†åœ°è®¨è®ºä¸€ä¸‹ã€‚
- en: Imagine a 2-D tensor with shape [4, 8], illustrated below.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªå½¢çŠ¶ä¸º [4, 8] çš„äºŒç»´å¼ é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/169e1865490d4603d66dc792052c1458.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/169e1865490d4603d66dc792052c1458.png)'
- en: 4x8 Tensor (Image by Author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 4x8 å¼ é‡ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: 'The data (i.e. float numbers) of a tensor is actually stored as a 1-dimensional
    array on memory:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ é‡çš„æ•°æ®ï¼ˆå³æµ®ç‚¹æ•°ï¼‰å®é™…ä¸Šæ˜¯ä»¥ä¸€ç»´æ•°ç»„çš„å½¢å¼å­˜å‚¨åœ¨å†…å­˜ä¸­çš„ï¼š
- en: '![](../Images/edc491c716df7cab51cd1ca4a61ff296.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edc491c716df7cab51cd1ca4a61ff296.png)'
- en: 1-D dimensional data array of the tensor (Image by Author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ é‡çš„ä¸€ç»´æ•°æ®æ•°ç»„ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: 'So, in order to represent this 1-dimensional array as a N-dimensional tensor,
    we use strides. Basically the idea is the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸ºäº†å°†è¿™ä¸ªä¸€ç»´æ•°ç»„è¡¨ç¤ºä¸ºä¸€ä¸ª N ç»´å¼ é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨æ­¥å¹…ã€‚åŸºæœ¬çš„æ€æƒ³å¦‚ä¸‹ï¼š
- en: 'We have a matrix with 4 rows and 8 columns. Considering that all of its elements
    are organized by rows on the 1-dimensional array, if we want to access the value
    at position [2, 3], we need to traverse 2 rows (of 8 elements each) plus 3 positions.
    In mathematical terms, we need to traverse 3 + 2 * 8 elements on the 1-dimensional
    array:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ä¸€ä¸ª 4 è¡Œ 8 åˆ—çš„çŸ©é˜µã€‚è€ƒè™‘åˆ°å®ƒçš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æŒ‰è¡Œåœ¨ä¸€ç»´æ•°ç»„ä¸­ç»„ç»‡çš„ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è®¿é—®ä½ç½® [2, 3] çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦éå† 2 è¡Œï¼ˆæ¯è¡Œ 8
    ä¸ªå…ƒç´ ï¼‰ï¼ŒåŠ ä¸Š 3 ä¸ªä½ç½®ã€‚ç”¨æ•°å­¦æœ¯è¯­æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ç»´æ•°ç»„ä¸Šéå† 3 + 2 * 8 ä¸ªå…ƒç´ ï¼š
- en: '![](../Images/644a13550e4601385357edc5bb88071b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/644a13550e4601385357edc5bb88071b.png)'
- en: Image by Author
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: So this â€˜8â€™ is the *stride* of the *second* dimension. In this case, it is the
    information of how many elements I need to traverse on the array to â€œjumpâ€ to
    other positions on the *second* *dimension.*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªâ€˜8â€™æ˜¯ç¬¬äºŒç»´åº¦çš„ *æ­¥å¹…*ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå®ƒè¡¨ç¤ºäº†æˆ‘éœ€è¦åœ¨æ•°ç»„ä¸­éå†å¤šå°‘ä¸ªå…ƒç´ æ‰èƒ½â€œè·³è½¬â€åˆ°ç¬¬äºŒç»´åº¦çš„å…¶ä»–ä½ç½®ã€‚
- en: Thus, for accessing the element [i, j] of a 2-dimensional tensor with shape
    [shape_0, shape_1], we basically need to access the element at position j + i
    * shape_1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¯¹äºè®¿é—®å½¢çŠ¶ä¸º [shape_0, shape_1] çš„äºŒç»´å¼ é‡ä¸­çš„å…ƒç´  [i, j]ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šéœ€è¦è®¿é—®ä½ç½® j + i * shape_1 çš„å…ƒç´ ã€‚
- en: 'Now, let us imagine a 3-dimensional tensor:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æƒ³è±¡ä¸€ä¸ªä¸‰ç»´å¼ é‡ï¼š
- en: '![](../Images/bc1c13d004a29b88a4f73e8b11a51483.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc1c13d004a29b88a4f73e8b11a51483.png)'
- en: 5x4x8 Tensor (Image by Author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 5x4x8 å¼ é‡ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: You can think of this 3-dimensional tensor as a sequence of matrices. For example,
    you can think of this [5, 4, 8] tensor as 5 matrices of shape [4, 8].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æŠŠè¿™ä¸ªä¸‰ç»´å¼ é‡çœ‹ä½œæ˜¯çŸ©é˜µçš„åºåˆ—ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥æŠŠè¿™ä¸ªå½¢çŠ¶ä¸º [5, 4, 8] çš„å¼ é‡çœ‹ä½œæ˜¯ 5 ä¸ªå½¢çŠ¶ä¸º [4, 8] çš„çŸ©é˜µã€‚
- en: Now, in order to access the element at position [1, 2, 7], you need to traverse
    1 complete matrix of shape [4,8], 2 rows of shape [8] and 7 columns of shape [1].
    So, you need to traverse (1 * 4 * 8) + (2 * 8) + (7 * 1) positions on the 1-dimensional
    array.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸ºäº†è®¿é—®ä½ç½® [1, 2, 7] çš„å…ƒç´ ï¼Œä½ éœ€è¦éå†ä¸€ä¸ªå®Œæ•´çš„å½¢çŠ¶ä¸º [4, 8] çš„çŸ©é˜µï¼Œ2 è¡Œå½¢çŠ¶ä¸º [8] å’Œ 7 åˆ—å½¢çŠ¶ä¸º [1]ã€‚æ‰€ä»¥ï¼Œä½ éœ€è¦åœ¨ä¸€ç»´æ•°ç»„ä¸Šéå†
    (1 * 4 * 8) + (2 * 8) + (7 * 1) ä¸ªä½ç½®ã€‚
- en: '![](../Images/292c1ea36eb8d9c10528822680f5c698.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/292c1ea36eb8d9c10528822680f5c698.png)'
- en: Image by Author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'Thus, to access the element [i][j][k] of a 3-D tensor with [shape_0, shape_1,
    shape_2] on the 1-D data array, you do:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¦åœ¨ä¸€ç»´æ•°æ®æ•°ç»„ä¸Šè®¿é—®ä¸€ä¸ªå½¢çŠ¶ä¸º [shape_0, shape_1, shape_2] çš„ä¸‰ç»´å¼ é‡ä¸­çš„å…ƒç´  [i][j][k]ï¼Œä½ éœ€è¦ï¼š
- en: '![](../Images/81de413fc9cf4c48a4a1b0c764dea74f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81de413fc9cf4c48a4a1b0c764dea74f.png)'
- en: This shape_1 * shape_2 is the *stride* of the first dimension, the shape_2 is
    the *stride* of the second dimension and 1 is the stride of the third dimension.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª shape_1 * shape_2 æ˜¯ç¬¬ä¸€ç»´çš„ *æ­¥å¹…*ï¼Œshape_2 æ˜¯ç¬¬äºŒç»´çš„ *æ­¥å¹…*ï¼Œ1 æ˜¯ç¬¬ä¸‰ç»´çš„æ­¥å¹…ã€‚
- en: 'Then, in order to generalize:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œä¸ºäº†è¿›è¡Œæ¦‚æ‹¬ï¼š
- en: '![](../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png)'
- en: 'Where the *strides* of each dimension can be calculated using the product of
    the next dimension tensor shapes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªç»´åº¦çš„*æ­¥å¹…*å¯ä»¥é€šè¿‡ä¸‹ä¸€ä¸ªç»´åº¦å¼ é‡å½¢çŠ¶çš„ä¹˜ç§¯æ¥è®¡ç®—ï¼š
- en: '![](../Images/7b9a7f16cd0d11e4767e681da1993aef.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b9a7f16cd0d11e4767e681da1993aef.png)'
- en: Then we set stride[n-1] = 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬è®¾ç½®æ­¥å¹…[n-1] = 1ã€‚
- en: On our tensor example of shape [5, 4, 8] we would have strides = [4*8, 8, 1]
    = [32, 8, 1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„å½¢çŠ¶ä¸º[5, 4, 8]çš„å¼ é‡ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­¥å¹… = [4*8, 8, 1] = [32, 8, 1]
- en: 'You can test on your own:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è‡ªå·±æµ‹è¯•ï¼š
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Ok, but why do we need shapes and strides? Beyond accessing elements of N-dimensional
    tensors stored as 1-dimensional arrays, this concept can be used to manipulate
    tensor arrangements very easily.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å½¢çŠ¶å’Œæ­¥å¹…ï¼Ÿé™¤äº†è®¿é—®å­˜å‚¨ä¸º1ç»´æ•°ç»„çš„Nç»´å¼ é‡çš„å…ƒç´ ä¹‹å¤–ï¼Œè¿™ä¸ªæ¦‚å¿µå¯ä»¥éå¸¸å®¹æ˜“åœ°ç”¨æ¥æ“ä½œå¼ é‡æ’åˆ—ã€‚
- en: For example, to reshape a tensor, you only need to set the new shape and calculate
    the new strides based on it! (since the new shape guarantees the same number of
    elements)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¦é‡å¡‘ä¸€ä¸ªå¼ é‡ï¼Œä½ åªéœ€è¦è®¾ç½®æ–°å½¢çŠ¶å¹¶æ ¹æ®å®ƒè®¡ç®—æ–°æ­¥å¹…ï¼ï¼ˆå› ä¸ºæ–°å½¢çŠ¶ä¿è¯äº†ç›¸åŒæ•°é‡çš„å…ƒç´ ï¼‰
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Internally, the tensor is still stored as the same 1-dimensional array. The
    reshape method did not change the order of the elements within the array! Thatâ€™s
    amazing, isnâ€™t? ğŸ˜
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å†…éƒ¨ï¼Œå¼ é‡ä»ç„¶å­˜å‚¨ä¸ºç›¸åŒçš„1ç»´æ•°ç»„ã€‚é‡å¡‘æ–¹æ³•æ²¡æœ‰æ”¹å˜æ•°ç»„å†…å…ƒç´ çš„é¡ºåºï¼è¿™å¾ˆç¥å¥‡ï¼Œä¸æ˜¯å—ï¼ŸğŸ˜
- en: 'You can verify on your own using the following function that accesses the internal
    1-dimensional array on PyTorch:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è‡ªå·±éªŒè¯ï¼Œä½¿ç”¨ä»¥ä¸‹å‡½æ•°è®¿é—®PyTorchçš„å†…éƒ¨1ç»´æ•°ç»„ï¼š
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Or for instance, you want to transpose two axes. Internally, you just need to
    swap the respective strides!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä¾‹å¦‚ï¼Œä½ æƒ³è¦è½¬ç½®ä¸¤ä¸ªè½´ã€‚åœ¨å†…éƒ¨ï¼Œä½ åªéœ€è¦äº¤æ¢ç›¸åº”çš„æ­¥å¹…ï¼
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you print the internal array, both have the same values:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ‰“å°å†…éƒ¨æ•°ç»„ï¼Œä¸¤è€…éƒ½æœ‰ç›¸åŒçš„å€¼ï¼š
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: However, the stride of `new_t` now does not match with the equation I showed
    above. This is due to the fact that the tensor is now not contiguous. That means
    that although the internal array remains the same, the order of its values in
    memory does not match with the actual order of the tensor.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œ`new_t`çš„æ­¥å¹…ç°åœ¨ä¸æˆ‘ä¸Šé¢å±•ç¤ºçš„æ–¹ç¨‹ä¸åŒ¹é…ã€‚è¿™æ˜¯å› ä¸ºå¼ é‡ç°åœ¨ä¸å†æ˜¯è¿ç»­çš„ã€‚è¿™æ„å‘³ç€è™½ç„¶å†…éƒ¨æ•°ç»„ä¿æŒä¸å˜ï¼Œä½†å…¶åœ¨å†…å­˜ä¸­çš„å€¼çš„é¡ºåºä¸å¼ é‡çš„å®é™…é¡ºåºä¸åŒ¹é…ã€‚
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This means the accessing the non-contiguous elements in sequence is less efficient
    (as the real tensor elements is not ordered in sequence on memory). In order to
    fix that, we can do:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æŒ‰é¡ºåºè®¿é—®éè¿ç»­å…ƒç´ çš„æ•ˆç‡è¾ƒä½ï¼ˆå› ä¸ºçœŸå®çš„å¼ é‡å…ƒç´ åœ¨å†…å­˜ä¸­ä¸æ˜¯æŒ‰é¡ºåºæ’åˆ—çš„ï¼‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we analyze the internal array, its order now matches with the actual tensor
    order now, which can provide better memory access efficiency:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬åˆ†æå†…éƒ¨æ•°ç»„ï¼Œç°åœ¨å…¶é¡ºåºä¸å®é™…å¼ é‡é¡ºåºåŒ¹é…ï¼Œè¿™å¯ä»¥æä¾›æ›´å¥½çš„å†…å­˜è®¿é—®æ•ˆç‡ï¼š
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now that we comprehend how a tensor is modeled, let us start creating our library!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç†è§£äº†å¼ é‡æ˜¯å¦‚ä½•å»ºæ¨¡çš„ï¼Œè®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºæˆ‘ä»¬çš„åº“å§ï¼
- en: I will call it *Norch*, which stands for NOT PyTorch, and also makes an allusion
    to my last name, Nogueira ğŸ˜
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†å…¶ç§°ä¸º*Norch*ï¼Œä»£è¡¨ç€NOT PyTorchï¼Œå¹¶ä¸”ä¹Ÿæš—æŒ‡äº†æˆ‘çš„å§“æ°ï¼ŒNogueira ğŸ˜
- en: The first thing to know is that although PyTorch is used through Python, internally
    it runs C/C++. So we will first create our internals C/C++ functions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¦çŸ¥é“çš„æ˜¯ï¼Œå°½ç®¡PyTorchæ˜¯é€šè¿‡Pythonä½¿ç”¨çš„ï¼Œä½†åœ¨å†…éƒ¨å®ƒè¿è¡Œçš„æ˜¯C/C++ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºæˆ‘ä»¬çš„å†…éƒ¨C/C++å‡½æ•°ã€‚
- en: 'We can first define a tensor as a struct to store its data and metadata, and
    create a function to instantiate it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é¦–å…ˆå®šä¹‰å¼ é‡ä½œä¸ºä¸€ä¸ªç»“æ„ä½“æ¥å­˜å‚¨å…¶æ•°æ®å’Œå…ƒæ•°æ®ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å®ä¾‹åŒ–å®ƒï¼š
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In order to access some element, we can take advantage of strides, as we learned
    before:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¿é—®æŸä¸ªå…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨ä¹‹å‰å­¦åˆ°çš„æ­¥å¹…ï¼š
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, we can create the tensor operations. I will show some examples and you
    can find the complete version in the repository linked at the end of this article.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºå¼ é‡æ“ä½œã€‚æˆ‘å°†å±•ç¤ºä¸€äº›ç¤ºä¾‹ï¼Œä½ å¯ä»¥åœ¨æœ¬æ–‡æœ«å°¾é“¾æ¥çš„å­˜å‚¨åº“ä¸­æ‰¾åˆ°å®Œæ•´ç‰ˆæœ¬ã€‚
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After that we are able to create our other tensor functions that will call
    these operations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åæˆ‘ä»¬å°±èƒ½åˆ›å»ºæˆ‘ä»¬çš„å…¶ä»–è°ƒç”¨è¿™äº›æ“ä½œçš„å¼ é‡å‡½æ•°ï¼š
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As mentioned before, the tensor reshaping does not modify the internal data
    array:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œå¼ é‡é‡å¡‘ä¸ä¼šä¿®æ”¹å†…éƒ¨æ•°æ®æ•°ç»„ï¼š
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Although we can now do some tensor operations, nobody deserves to run it using
    C/C++, right? Let us start building our Python wrapper!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬ç°åœ¨å¯ä»¥è¿›è¡Œä¸€äº›å¼ é‡æ“ä½œï¼Œä½†æ˜¯æ²¡æœ‰äººæ„¿æ„ä½¿ç”¨C/C++æ¥è¿è¡Œå®ƒï¼Œå¯¹å§ï¼Ÿè®©æˆ‘ä»¬å¼€å§‹æ„å»ºæˆ‘ä»¬çš„PythonåŒ…è£…å™¨ï¼
- en: There are a lot of options to run C/C++ code using Python, such as *Pybind11*
    and *Cython.* For our example I will use *ctypes.*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šé€‰é¡¹å¯ä»¥ä½¿ç”¨Pythonè¿è¡ŒC/C++ä»£ç ï¼Œæ¯”å¦‚*Pybind11*å’Œ*Cython*ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨*ctypes*ã€‚
- en: 'The basically structure of *ctypes* is shown below:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*ctypes*çš„åŸºæœ¬ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, it is very intuitive. After you compile the C/C++ code, you
    can use it on Python with *ctypes* very easily. You only need to define the arguments
    & return c_types of the function, convert the variable to its respective c_types
    and call the function. For more complex types such as arrays (float lists) you
    can use pointers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¿™éå¸¸ç›´è§‚ã€‚åœ¨ä½ ç¼–è¯‘C/C++ä»£ç åï¼Œå¯ä»¥éå¸¸è½»æ¾åœ°åœ¨Pythonä¸­ä½¿ç”¨*ctypes*ã€‚ä½ åªéœ€è¦å®šä¹‰å‡½æ•°çš„å‚æ•°å’Œè¿”å›å€¼ç±»å‹ï¼Œå¹¶å°†å˜é‡è½¬æ¢ä¸ºç›¸åº”çš„Cç±»å‹ï¼Œå†è°ƒç”¨å‡½æ•°ã€‚å¯¹äºæ›´å¤æ‚çš„ç±»å‹ï¼Œå¦‚æ•°ç»„ï¼ˆæµ®åŠ¨åˆ—è¡¨ï¼‰ï¼Œä½ å¯ä»¥ä½¿ç”¨æŒ‡é’ˆã€‚
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And for struct types we can create our own c_type:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç»“æ„ç±»å‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„Cç±»å‹ï¼š
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After this brief explanation, let us construct the Python wrapper for our tensor
    C/C++ library!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µç®€çŸ­çš„è§£é‡Šä¹‹åï¼Œè®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„å¼ é‡C/C++åº“æ„å»ºPythonåŒ…è£…å™¨ï¼
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now we include the Python tensor operations to call the C/C++ operations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åŒ…æ‹¬äº†Pythonå¼ é‡æ“ä½œï¼Œä»¥ä¾¿è°ƒç”¨C/C++æ“ä½œã€‚
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you got here, you are now capable to run the code and start doing some tensor
    operations!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»çœ‹åˆ°è¿™é‡Œï¼Œä½ ç°åœ¨å¯ä»¥è¿è¡Œä»£ç å¹¶å¼€å§‹è¿›è¡Œä¸€äº›å¼ é‡æ“ä½œäº†ï¼
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#2 â€” GPU Support'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#2 â€” GPUæ”¯æŒ'
- en: 'After creating the basic structure of our library, now we will take it to a
    new level. It is well-known that you can call `.to("cuda")` to send data to GPU
    and run math operations faster. I will assume that you have basic knowledge on
    how CUDA works, but if you do not, you can read my other article: [CUDA tutorial.](/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    I will wait here. ğŸ˜Š'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ›å»ºäº†æˆ‘ä»¬åº“çš„åŸºæœ¬ç»“æ„ä¹‹åï¼Œç°åœ¨æˆ‘ä»¬å°†æŠŠå®ƒæå‡åˆ°ä¸€ä¸ªæ–°å±‚æ¬¡ã€‚ä¼—æ‰€å‘¨çŸ¥ï¼Œä½ å¯ä»¥è°ƒç”¨`.to("cuda")`å°†æ•°æ®å‘é€åˆ°GPUï¼Œå¹¶åŠ å¿«æ•°å­¦è¿ç®—é€Ÿåº¦ã€‚æˆ‘å‡è®¾ä½ å¯¹CUDAçš„åŸºæœ¬å·¥ä½œåŸç†æœ‰æ‰€äº†è§£ï¼Œä½†å¦‚æœä¸äº†è§£ï¼Œä½ å¯ä»¥é˜…è¯»æˆ‘å¦å¤–ä¸€ç¯‡æ–‡ç« ï¼š[CUDAæ•™ç¨‹.](/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    æˆ‘åœ¨è¿™é‡Œç­‰ä½ ã€‚ğŸ˜Š
- en: â€¦
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦
- en: 'For those in a hurry, a simple introduction here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ€¥äºäº†è§£çš„äººï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç®€çŸ­çš„ä»‹ç»ï¼š
- en: Basically, all of our code until here is running on CPU memory. Altough for
    single operations CPUs are faster, the advantage of GPUs relies on its parallelization
    capabilities. While CPU design aims to execute a sequence of operations (threads)
    fast (but can only execute dozens of them), the GPU design aims to execute millions
    of operations in parallel (by sacrificing individual threads performance).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„æ‰€æœ‰ä»£ç éƒ½åœ¨CPUå†…å­˜ä¸Šè¿è¡Œã€‚è™½ç„¶å¯¹äºå•ä¸€æ“ä½œCPUé€Ÿåº¦æ›´å¿«ï¼Œä½†GPUçš„ä¼˜åŠ¿åœ¨äºå®ƒçš„å¹¶è¡ŒåŒ–èƒ½åŠ›ã€‚CPUè®¾è®¡æ—¨åœ¨å¿«é€Ÿæ‰§è¡Œä¸€ç³»åˆ—æ“ä½œï¼ˆçº¿ç¨‹ï¼‰ï¼ˆä½†å®ƒåªèƒ½æ‰§è¡Œå‡ åä¸ªï¼‰ï¼Œè€ŒGPUè®¾è®¡åˆ™æ—¨åœ¨å¹¶è¡Œæ‰§è¡Œæ•°ç™¾ä¸‡ä¸ªæ“ä½œï¼ˆé€šè¿‡ç‰ºç‰²å•ä¸ªçº¿ç¨‹çš„æ€§èƒ½ï¼‰ã€‚
- en: Thus, we can take advantage of this capability to perform operations in parallel.
    For example, in a million-sized tensor addition, instead of adding the elements
    of each index sequentially inside a loop, using a GPU we can add all of them in
    parallel at once. In order to do that, we can use CUDA, which is a platform developed
    by NVIDIA to enable developers to integrate GPU support to their software applications.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è¿™ä¸€èƒ½åŠ›å¹¶è¡Œæ‰§è¡Œæ“ä½œã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªç™¾ä¸‡å¤§å°çš„å¼ é‡åŠ æ³•ä¸­ï¼Œä¸å¿…åœ¨å¾ªç¯å†…éƒ¨ä¾æ¬¡æ·»åŠ æ¯ä¸ªç´¢å¼•çš„å…ƒç´ ï¼Œä½¿ç”¨GPUæˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æ€§å¹¶è¡Œåœ°åŠ å’Œæ‰€æœ‰å…ƒç´ ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨CUDAï¼Œè¿™æ˜¯NVIDIAå¼€å‘çš„ä¸€ä¸ªå¹³å°ï¼Œæ—¨åœ¨è®©å¼€å‘è€…å°†GPUæ”¯æŒé›†æˆåˆ°ä»–ä»¬çš„è½¯ä»¶åº”ç”¨ä¸­ã€‚
- en: In order to do that, you can use CUDA C/C++, which is a simple C/C++ based interface
    designed to run specific GPU operations (such as copy data from CPU memory to
    GPU memory).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å¯ä»¥ä½¿ç”¨CUDA C/C++ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºC/C++çš„ç®€å•æ¥å£ï¼Œæ—¨åœ¨è¿è¡Œç‰¹å®šçš„GPUæ“ä½œï¼ˆä¾‹å¦‚å°†æ•°æ®ä»CPUå†…å­˜å¤åˆ¶åˆ°GPUå†…å­˜ï¼‰ã€‚
- en: The code below basically uses some CUDA C/C++ functions to copy data from CPU
    to GPU, and run the AddTwoArrays function (also called kernel) on a total of N
    GPU threads in parallel, each responsible to add a different element of the array.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç åŸºæœ¬ä¸Šä½¿ç”¨äº†ä¸€äº›CUDA C/C++å‡½æ•°ï¼Œå°†æ•°æ®ä»CPUå¤åˆ¶åˆ°GPUï¼Œå¹¶åœ¨æ€»å…±Nä¸ªGPUçº¿ç¨‹ä¸Šå¹¶è¡Œè¿è¡ŒAddTwoArrayså‡½æ•°ï¼ˆä¹Ÿå«åškernelï¼‰ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£åŠ å’Œæ•°ç»„ä¸­çš„ä¸åŒå…ƒç´ ã€‚
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can notice, instead of adding each element pair per operation, we run
    all of the adding operations in parallel, getting rid of the loop instruction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬ä¸æ˜¯æ¯æ¬¡æ“ä½œæ—¶åŠ å’Œæ¯å¯¹å…ƒç´ ï¼Œè€Œæ˜¯å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åŠ æ³•æ“ä½œï¼Œä»è€Œçœå»äº†å¾ªç¯æŒ‡ä»¤ã€‚
- en: After this brief introduction, we can go back to our tensor library.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µç®€çŸ­çš„ä»‹ç»ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥å›åˆ°æˆ‘ä»¬çš„å¼ é‡åº“ã€‚
- en: The first step is to create a function to send tensor data from CPU to GPU and
    vice versa.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†å¼ é‡æ•°æ®ä»CPUå‘é€åˆ°GPUï¼Œåä¹‹äº¦ç„¶ã€‚
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The Python wrapper:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: PythonåŒ…è£…å™¨ï¼š
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, we create GPU versions for all of our tensor operations. I will write
    examples for addition and subtraction:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰çš„å¼ é‡æ“ä½œåˆ›å»ºGPUç‰ˆæœ¬ã€‚æˆ‘å°†ä¸ºåŠ æ³•å’Œå‡æ³•å†™å‡ºç¤ºä¾‹ï¼š
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Subsequently, we include a new tensor attribute `char* device` on the `tensor.cpp`
    and we can use it to select where the operations will be run (CPU or GPU):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: éšåï¼Œæˆ‘ä»¬åœ¨`tensor.cpp`ä¸ŠåŒ…å«ä¸€ä¸ªæ–°çš„å¼ é‡å±æ€§`char* device`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥é€‰æ‹©æ“ä½œå°†åœ¨å“ªé‡Œè¿è¡Œï¼ˆCPUæˆ–GPUï¼‰ï¼š
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now our library has GPU support!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„åº“æ”¯æŒGPUï¼
- en: '[PRE28]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '#3 â€” Automatic Differentation (Autograd)'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#3 â€” è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰'
- en: 'One of the main reasons why PyTorch got so popular is due to its Autograd module.
    It is a core component that allows automatic differentiation in order to compute
    gradients (crucial for training models with optimization algorithms such as gradient
    descent). By calling a single method `.backward()`, it computes all of the gradients
    from previous tensor operations:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorchå˜å¾—å¦‚æ­¤å—æ¬¢è¿çš„ä¸»è¦åŸå› ä¹‹ä¸€æ˜¯å…¶Autogradæ¨¡å—ã€‚è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œå…è®¸è‡ªåŠ¨å¾®åˆ†ä»¥è®¡ç®—æ¢¯åº¦ï¼ˆå¯¹äºä½¿ç”¨æ¢¯åº¦ä¸‹é™ç­‰ä¼˜åŒ–ç®—æ³•è®­ç»ƒæ¨¡å‹è‡³å…³é‡è¦ï¼‰ã€‚é€šè¿‡è°ƒç”¨å•ä¸ªæ–¹æ³•`.backward()`ï¼Œå®ƒè®¡ç®—å‡ºå…ˆå‰å¼ é‡æ“ä½œçš„æ‰€æœ‰æ¢¯åº¦ï¼š
- en: '[PRE29]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In order to understand what is happening, letâ€™s try to replicate the same procedure
    manually:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆï¼Œè®©æˆ‘ä»¬å°è¯•æ‰‹åŠ¨å¤åˆ¶ç›¸åŒçš„è¿‡ç¨‹ï¼š
- en: '![](../Images/255a2823dc15242c9d7414c9175d073d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/255a2823dc15242c9d7414c9175d073d.png)'
- en: 'Letâ€™s first calculate:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¦–å…ˆè®¡ç®—ï¼š
- en: '![](../Images/3d5353e2d05c964798dfe75e81572db5.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d5353e2d05c964798dfe75e81572db5.png)'
- en: 'Note that *x* is a matrix, thus we need to calculate the derivative of L with
    respect to each element individually. Additionally, L is a summation over all
    elements, but it is important to remember that for each elements, the other elements
    does not interfere on its derivative. Therefore, we obtain the following term:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„*x*æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦åˆ†åˆ«è®¡ç®—*L*å¯¹æ¯ä¸ªå…ƒç´ çš„å¯¼æ•°ã€‚æ­¤å¤–ï¼Œ*L*æ˜¯æ‰€æœ‰å…ƒç´ çš„æ€»å’Œï¼Œä½†é‡è¦çš„æ˜¯è¦è®°ä½ï¼Œå¯¹äºæ¯ä¸ªå…ƒç´ ï¼Œå…¶ä»–å…ƒç´ ä¸ä¼šå½±å“å…¶å¯¼æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹é¡¹ï¼š
- en: '![](../Images/612ffe8a8a672f3c2ebac55743b1a82c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/612ffe8a8a672f3c2ebac55743b1a82c.png)'
- en: 'By applying the chain rule for each term, we differentiate the outer function
    and multiply by the derivative of the inner function:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ºæ¯ä¸ªé¡¹åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬åŒºåˆ†å¤–éƒ¨å‡½æ•°å¹¶ä¹˜ä»¥å†…éƒ¨å‡½æ•°çš„å¯¼æ•°ï¼š
- en: '![](../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png)'
- en: 'Where:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '![](../Images/0834be1c1e2772168593070107e386de.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0834be1c1e2772168593070107e386de.png)'
- en: 'Finally:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼š
- en: '![](../Images/4dec4240d0004de073e923b6d937831d.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4dec4240d0004de073e923b6d937831d.png)'
- en: Thus, we have the following final equation to calculate the derivative of L
    with respect to *x:*
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹æœ€ç»ˆæ–¹ç¨‹æ¥è®¡ç®—*L*ç›¸å¯¹äº*x*çš„å¯¼æ•°ï¼š
- en: '![](../Images/1f6843db3a0d35b765a74b1241ac61ae.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f6843db3a0d35b765a74b1241ac61ae.png)'
- en: 'Substituting the values into the equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å€¼ä»£å…¥æ–¹ç¨‹å¼ï¼š
- en: '![](../Images/95897f1570971bf15b239d7182f2b8cb.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95897f1570971bf15b239d7182f2b8cb.png)'
- en: 'Calculating the result, we get the same values we obtained with PyTorch:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬å¾—åˆ°ä¸PyTorchè·å¾—çš„ç›¸åŒå€¼ï¼š
- en: '![](../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png)'
- en: 'Now, letâ€™s analyze what we just did:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬åˆ†æåˆšæ‰åšçš„äº‹æƒ…ï¼š
- en: 'Basically, we observed all the operations involved in reserved order: a summation,
    a power of 3 and a subtraction. Then, we applied chain of rule, calculating the
    derivative of each operation and recursively calculated the derivative for the
    next operation. So, first we need an implementation of the derivative for different
    math operations:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬æŒ‰ç…§ä¿ç•™é¡ºåºè§‚å¯Ÿäº†æ‰€æœ‰æ¶‰åŠçš„æ“ä½œï¼šæ±‚å’Œã€3çš„å¹‚å’Œå‡æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œè®¡ç®—æ¯ä¸ªæ“ä½œçš„å¯¼æ•°ï¼Œå¹¶é€’å½’åœ°è®¡ç®—ä¸‹ä¸€ä¸ªæ“ä½œçš„å¯¼æ•°ã€‚å› æ­¤ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºä¸åŒçš„æ•°å­¦æ“ä½œå®ç°å¯¼æ•°ï¼š
- en: 'For addition:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåŠ æ³•ï¼š
- en: '![](../Images/f0bea287a54af0930d57584959d48ecd.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0bea287a54af0930d57584959d48ecd.png)'
- en: '[PRE30]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For sin:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ­£å¼¦ï¼š
- en: '![](../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png)'
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'For cosine:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä½™å¼¦ï¼š
- en: '![](../Images/4359a9a560ad9072af54cf872019b3bd.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4359a9a560ad9072af54cf872019b3bd.png)'
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For element-wise multiplication:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºé€å…ƒç´ ä¹˜æ³•ï¼š
- en: '![](../Images/3564658c2002c0917544fbbef510374c.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3564658c2002c0917544fbbef510374c.png)'
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'For summation:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ±‚å’Œï¼š
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can access the GitHub repository link at the end of the article to explore
    other operations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨æ–‡ç« æœ«å°¾è®¿é—®GitHubå­˜å‚¨åº“é“¾æ¥ä»¥æ¢ç´¢å…¶ä»–æ“ä½œã€‚
- en: 'Now that we have derivative expressions for each operation, we can proceed
    with implementing the recursive backward chain rule. We can set a `requires_grad`
    argument for our tensor to indicate that we want to store the gradients of this
    tensor. If true, we will store the gradients for each tensor operation. For instance:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹æ¯ä¸ªæ“ä½œæœ‰äº†å¯¼æ•°è¡¨è¾¾å¼ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å®ç°é€’å½’åå‘é“¾è§„åˆ™ã€‚æˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„å¼ é‡è®¾ç½®ä¸€ä¸ª`requires_grad`å‚æ•°ï¼Œä»¥æŒ‡ç¤ºæˆ‘ä»¬è¦å­˜å‚¨æ­¤å¼ é‡çš„æ¢¯åº¦ã€‚å¦‚æœä¸ºçœŸï¼Œæˆ‘ä»¬å°†ä¸ºæ¯ä¸ªå¼ é‡æ“ä½œå­˜å‚¨æ¢¯åº¦ã€‚ä¾‹å¦‚ï¼š
- en: '[PRE35]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, implement the `.backward()` method:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå®ç°`.backward()`æ–¹æ³•ï¼š
- en: '[PRE36]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Finally, just implement `.zero_grad()` to zero the gradient of a tensor, and
    `.detach()` to remove the tensor autograd history:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåªéœ€å®ç°`.zero_grad()`å°†å¼ é‡çš„æ¢¯åº¦å½’é›¶ï¼Œ`.detach()`å°†ç§»é™¤å¼ é‡çš„è‡ªåŠ¨æ¢¯åº¦å†å²ï¼š
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Congratulations! You just created a complete tensor library with GPU support
    and automatic differentiation! Now we can create the `nn` and `optim` modules
    to train some deep learning models more easily.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼æ‚¨åˆšåˆšåˆ›å»ºäº†ä¸€ä¸ªå®Œæ•´çš„å¼ é‡åº“ï¼Œæ”¯æŒGPUå’Œè‡ªåŠ¨å¾®åˆ†ï¼ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»º`nn`å’Œ`optim`æ¨¡å—ï¼Œæ›´è½»æ¾åœ°è®­ç»ƒä¸€äº›æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
- en: '#4 â€” nn and optim modules'
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '#4 â€” nnå’Œoptimæ¨¡å—'
- en: The `nn` is a module for building neural networks and deep learning models and
    `optim` is related to optimization algorithms to train these models. In order
    to recreate them, the first thing to do is implementing a Parameter, which simply
    is a trainable tensor, with the same operations, but with `requires_grad` set
    always as `True` and with some random initialization technique.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn`æ˜¯ç”¨äºæ„å»ºç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ¨¡å—ï¼Œè€Œ`optim`ä¸ä¼˜åŒ–ç®—æ³•ç›¸å…³ï¼Œç”¨äºè®­ç»ƒè¿™äº›æ¨¡å‹ã€‚ä¸ºäº†é‡æ–°åˆ›å»ºå®ƒä»¬ï¼Œé¦–å…ˆè¦å®ç°ä¸€ä¸ªParameterï¼Œå®ƒåªæ˜¯ä¸€ä¸ªå¯è®­ç»ƒçš„å¼ é‡ï¼Œå…·æœ‰ç›¸åŒçš„æ“ä½œï¼Œä½†`requires_grad`å§‹ç»ˆè®¾ç½®ä¸º`True`ï¼Œå¹¶é‡‡ç”¨ä¸€äº›éšæœºåˆå§‹åŒ–æŠ€æœ¯ã€‚'
- en: '[PRE38]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'By using parameters, we can start contructing modules:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹æ„å»ºæ¨¡å—ï¼š
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For example, we can construct our custom modules by inheriting from `nn.Module`,
    or we can use some previously created modules, such as the `linear`, which implements
    the ***y*** *= W****x*** *+ b* operation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿`nn.Module`æ¥æ„å»ºè‡ªå®šä¹‰æ¨¡å—ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›å…ˆå‰åˆ›å»ºçš„æ¨¡å—ï¼Œæ¯”å¦‚`linear`ï¼Œå®ƒå®ç°äº†***y*** *= W****x***
    *+ b*çš„æ“ä½œã€‚
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can implement some loss and activation functions. For instance, a mean
    squared error loss and a sigmoid function:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å®ç°ä¸€äº›æŸå¤±å’Œæ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå‡æ–¹è¯¯å·®æŸå¤±å’Œsigmoidå‡½æ•°ï¼š
- en: '[PRE42]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, create the optimizers. On our example I will implement the Stochastic
    Gradient Descent algorithm:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ›å»ºä¼˜åŒ–å™¨ã€‚åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘å°†å®ç°éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼š
- en: '[PRE44]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: And, thatâ€™s it! We just created our own deep learning framework! ğŸ¥³
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬åˆšåˆšåˆ›å»ºäº†è‡ªå·±çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ğŸ¥³
- en: 'Letâ€™s do some training:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è¿›è¡Œä¸€äº›è®­ç»ƒï¼š
- en: '[PRE45]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](../Images/e7662c657102cedd3717cdac495019f6.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7662c657102cedd3717cdac495019f6.png)'
- en: Image by Author
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: The model was successfully created and trained using our custom deep learning
    framework!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹å·²æˆåŠŸä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶åˆ›å»ºå’Œè®­ç»ƒï¼
- en: You can check the complete code [here](https://github.com/lucasdelimanogueira/PyNorch).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/lucasdelimanogueira/PyNorch)æŸ¥çœ‹å®Œæ•´çš„ä»£ç ã€‚
- en: Conclusion
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this post we covered the basic concepts of what is a tensor, how it is modeled
    and more advanced topics such as CUDA and Autograd. We successfully created a
    deep learning framework with GPU support and automatic differentiation. I hope
    this post helped you to briefly understand how PyTorch works under the hood.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¼ é‡çš„åŸºæœ¬æ¦‚å¿µï¼Œå®ƒæ˜¯å¦‚ä½•å»ºæ¨¡çš„ï¼Œä»¥åŠæ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚CUDAå’ŒAutogradã€‚æˆ‘ä»¬æˆåŠŸåœ°åˆ›å»ºäº†ä¸€ä¸ªæ”¯æŒGPUå’Œè‡ªåŠ¨å¾®åˆ†çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å¸Œæœ›æœ¬æ–‡èƒ½å¸®åŠ©æ‚¨ç®€è¦äº†è§£PyTorchåœ¨å¹•åæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: In future posts, I will try to cover more advanced topics such as distributed
    training (multinode / multi GPU) and memory management. Please let me know what
    you think or what you would like me to write about next in the comments! Thanks
    so much for reading! ğŸ˜Š
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœªæ¥çš„å¸–å­ä¸­ï¼Œæˆ‘å°†å°è¯•æ¶µç›–æ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤šèŠ‚ç‚¹/å¤šGPUï¼‰å’Œå†…å­˜ç®¡ç†ã€‚è¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘æ‚¨çš„æƒ³æ³•æˆ–æ‚¨å¸Œæœ›æˆ‘å†™ä¸‹çš„å†…å®¹ï¼éå¸¸æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼ğŸ˜Š
- en: Also, follow me here and on my [LinkedIn profile](https://www.linkedin.com/in/lucas-de-lima-nogueira/)
    to stay updated on my latest articles!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¯·å…³æ³¨æˆ‘çš„[LinkedInä¸ªäººèµ„æ–™](https://www.linkedin.com/in/lucas-de-lima-nogueira/)ï¼Œä»¥è·å–æœ€æ–°æ–‡ç« æ›´æ–°ï¼
- en: References
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) â€” GitHub repository
    of this project.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyNorch](https://github.com/lucasdelimanogueira/PyNorch) â€” æœ¬é¡¹ç›®çš„GitHubå­˜å‚¨åº“ã€‚'
- en: '[Tutorial CUDA](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    â€” A brief introduction on how CUDA works.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[CUDAæ•™ç¨‹](https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66)
    â€” CUDAå·¥ä½œåŸç†ç®€ä»‹ã€‚'
- en: '[PyTorch](https://pytorch.org/docs/stable/index.html) â€” PyTorch documentation.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorch](https://pytorch.org/docs/stable/index.html) â€” PyTorchæ–‡æ¡£ã€‚'
- en: '[MartinLwxâ€™s blog](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)
    â€” Tutorial on strides.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[MartinLwxçš„åšå®¢](https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/)
    â€” å…³äºæ­¥å¹…çš„æ•™ç¨‹ã€‚'
- en: '[Strides tutorial](https://ajcr.net/stride-guide-part-1/) â€” Another tutorial
    about strides.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ­¥å¹…æ•™ç¨‹](https://ajcr.net/stride-guide-part-1/) â€” å…³äºæ­¥å¹…çš„å¦ä¸€ä¸ªæ•™ç¨‹ã€‚'
- en: '[PyTorch internals](http://blog.ezyang.com/2019/05/pytorch-internals/) â€” A
    guide on how PyTorch is structured.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[PyTorchå†…éƒ¨](http://blog.ezyang.com/2019/05/pytorch-internals/) â€” PyTorchçš„ç»“æ„æŒ‡å—ã€‚'
- en: '[Nets](https://github.com/arthurdjn/nets) â€” A PyTorch recreation using NumPy.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nets](https://github.com/arthurdjn/nets) â€” ä½¿ç”¨ NumPy é‡å»ºçš„ PyTorchã€‚'
- en: '[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) â€” A live coding
    of an Autograd library.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[Autograd](https://www.youtube.com/watch?v=RxmBukb-Om4&t=935s) â€” Autograd åº“çš„å®æ—¶ç¼–ç ã€‚'
