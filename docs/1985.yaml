- en: Heckman Selection Bias Modeling in Causal Studies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/heckman-selection-bias-modeling-in-causal-studies-30e207987025?source=collection_archive---------9-----------------------#2024-08-14](https://towardsdatascience.com/heckman-selection-bias-modeling-in-causal-studies-30e207987025?source=collection_archive---------9-----------------------#2024-08-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How selection bias is related to the identification assumptions of OLS, and
    what steps should be taken to address it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pollak.daniel?source=post_page---byline--30e207987025--------------------------------)[![Daniel
    Pollak](../Images/a48f0aa944aeb4189e75cfc99949b4a7.png)](https://medium.com/@pollak.daniel?source=post_page---byline--30e207987025--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--30e207987025--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--30e207987025--------------------------------)
    [Daniel Pollak](https://medium.com/@pollak.daniel?source=post_page---byline--30e207987025--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--30e207987025--------------------------------)
    ·9 min read·Aug 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ca5a683283586a11d43d54ab03986aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dimitry B](https://unsplash.com/@dimitry_b?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Throughout my applied studies, I struggled to grasp the complexities of selection
    and sample bias problems. These issues manifest in various forms, can arise from
    different factors, and can affect both external and internal validity in causal
    models. Additionally, they are often the source of semantic confusion.
  prefs: []
  type: TYPE_NORMAL
- en: One of the foundational concepts to understand when addressing bias and inconsistencies
    in a linear causal model is the **omitted variable problem**. This occurs when
    a typically unobserved random variable is correlated with both the independent
    variable and the model error. Failing to account for this variable when estimating
    a linear model leads to biased estimators. Consequently, this problem hinders
    the isolation of variance in the dependent variable in response to changes in
    the independent variable, thus obscuring the true causal relationship between
    the two.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2ffe873e11bc0eac0f1fb74cc3ca058.png)'
  prefs: []
  type: TYPE_IMG
- en: Confounder variable in causal DAG
  prefs: []
  type: TYPE_NORMAL
- en: Are these concepts connected? Can selection bias be considered a form of the
    omitted variable problem? Let’s dive in and explore!
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’d like to lay out the foundational elements needed to fully grasp how selection
    bias affects our linear model estimation process. We have a dependent random variable,
    Y, which we assume has a linear relationship (subject to some error terms) with
    another variable, X, known as the independent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Identification Assumptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a subsample Y’, X’ of the population variables Y, X -
  prefs: []
  type: TYPE_NORMAL
- en: The error terms (**of the original model !!!**) and X’ are not correlated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The mean of the error terms is zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Y and X are really related in a linear way —
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9a4c58ead8f2e2f9a2e5d8b01f5e4722.png)'
  prefs: []
  type: TYPE_IMG
- en: It’s important to note that in empirical research, we observe X and Y (or a
    subsample of them), but **we don’t observe the error terms**, making assumption
    (1) impossible to test or validate directly. At this point, we usually rely on
    a theoretical explanation to justify this assumption. A common justification is
    through randomized controlled trials (RCTs), where the subsample, X, is collected
    entirely at random, ensuring that it is uncorrelated with any other variable,
    particularly with the error terms.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional Expectation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the assumptions mentioned earlier, we can precisely determine the form
    of the conditional expectation of Y given X —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/675b7d031b44f15b10d8266152485914.png)'
  prefs: []
  type: TYPE_IMG
- en: Conditional expectation in linear models
  prefs: []
  type: TYPE_NORMAL
- en: The last transition follows from the identification assumptions. It’s important
    to note that this is a function of x, meaning it represents the average of all
    observed values of y given that x is equal to a specific value (Or the local average
    of y’s given a small range of values of x’s — more information can be found [here](https://theeffectbook.net/ch-DescribingRelationships.html#conditional-means))
  prefs: []
  type: TYPE_NORMAL
- en: OLS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a sample of X that meets the identification assumptions, it’s well-established
    that the ordinary least squares (OLS) method provides a closed-form solution for
    consistent and unbiased estimators of the linear model parameters, alpha and beta,
    and thus for the conditional expectation function of Y given X.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, OLS is a technique for **fitting a linear line** (or linear hyperplane
    in the case of a multivariate sample) to a set of (y_i, x_i) pairs. What’s particularly
    interesting about OLS is that —
  prefs: []
  type: TYPE_NORMAL
- en: If Y and X have a linear relationship (accounting for classic error terms),
    we’ve seen that the conditional expectation of Y given X is perfectly linear.
    In this scenario, OLS effectively uncovers this function with strong statistical
    accuracy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OLS achieves this even with **any subsample** of X that meets the identification
    assumptions previously discussed — with large enough sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin with a straightforward example using simulated data. We’ll simulate
    the linear model from above.
  prefs: []
  type: TYPE_NORMAL
- en: A significant advantage of working with simulated data is that it allows us
    to better understand relationships between variables that are not observable in
    real-world scenarios, such as the error terms in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and running OLS for the full sample -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d8e5548a623c71f3021dd30046846b0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s generate a random subsample of our population, X, and apply OLS to
    this subsample. I’ll randomly select 100 x’s from the 500 samples I previously
    generated, and then run OLS on this subset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: and plot -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ea78ac3e7ba238e593e06adf5b3ebc5.png)'
  prefs: []
  type: TYPE_IMG
- en: It appears we obtain consistent estimators for the random subsample, as both
    OLS results produce quite similar conditional expectation lines. Additionally,
    you can observe the correlation between X and the error terms —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This suggests that the identification assumptions are being met. In practice,
    however, we cannot directly calculate these since the errors are not observable.
    Now, let’s create a new subsample — I’ll select all (y, x) pairs where y ≤ 10:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: and we get -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92fb9d05ee88537e6a2028ba330184bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, OLS has provided us with a completely different line. Let’s check the correlation
    between the subsample X’s and the errors.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Seems like the identification assumptions are violated. Let’s also plot the
    sub-sample error terms, as a function of X -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83fcaf6e5c9f1ff6b6ac3f022ee00d26.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, as X increases, there are fewer large errors, indicating a clear
    correlation that results in biased and inconsistent OLS estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore this further.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what’s going on here?
  prefs: []
  type: TYPE_NORMAL
- en: I’ll reference the model introduced by James Heckman, who, along with Daniel
    McFadden, received the Nobel Memorial Prize in Economic Sciences in 2000\. Heckman
    is renowned for his pioneering work in econometrics and microeconomics, particularly
    for his contributions to addressing selection bias and self-selection in quantitative
    analysis. His well-known Heckman correction will be discussed later in this context.
  prefs: []
  type: TYPE_NORMAL
- en: In his paper from 1979, “Sample Selection Bias as a Specification Error,” Heckman
    illustrates how selection bias arises from censoring the dependent variable —
    a specific case of selection that can be extended to more non-random sample selection
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: Censoring the dependent variable is exactly what we did when creating the last
    subsample in the previous section. Let’s examine Heckman’s framework.
  prefs: []
  type: TYPE_NORMAL
- en: We start with a full sample (or population) of (y_i, x_i) pairs. In this scenario,
    given x_i, ε_i can vary — it can be positive, negative, small, or large, depending
    solely on the error distribution. We refer to this complete sample of the dependent
    variable as y*. We then define y as the censored dependent variable, which includes
    only the values we actually observe.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d2158cfcf3d7fc86a1652cc5430bee3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let’s calculate the conditional expectation of the censored variable,
    y:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1b77e1aa939cf4b1452c3d4aeb796fc.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, this function resembles the one we saw earlier, but it includes
    an additional term that differs from before. This last term cannot be ignored,
    which means the conditional expectation function is **not purely linear** in terms
    of x (with some noise). Consequently, running OLS on the uncensored values will
    produce biased estimators for alpha and beta.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, this equation illustrates how the selection bias problem can be viewed
    as an **omitted variable** problem. Since the last term depends on X, it shares
    a significant amount of variance with the dependent variable.
  prefs: []
  type: TYPE_NORMAL
- en: Heckman’s Correction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Inverse Mills ratio**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Heckman’s correction method is based on the following principle: Given a random
    variable Z that follows a normal distribution with mean μ and standard deviation
    σ, the following equations apply:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bee5544cf6b47d5414ca07ac6945e0b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Given any constant α, Φ (capital phi) represents the standard normal distribution’s
    CDF, and ɸ denotes the standard normal distribution’s PDF. These values are known
    as the **inverse Mills ratio**.
  prefs: []
  type: TYPE_NORMAL
- en: So, how does this help us? Let’s revisit the last term of the previous conditional
    expectation equation —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a0c28909cde6017e96ff61941315479.png)'
  prefs: []
  type: TYPE_IMG
- en: Combined with the fact that our error terms follow a normal distribution, we
    can use the inverse Mills ratio to characterize their behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Back to our model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advantage of the inverse Mills ratio is that it transforms the previous
    conditional expectation function into the following form —
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34182c74fe884cfa79f029dbb26060db.png)'
  prefs: []
  type: TYPE_IMG
- en: This results in a linear function with an additional covariate — the inverse
    Mills ratio. Therefore, to estimate the model parameters, we can apply OLS to
    this revised formula.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first calculate the inverse Mills ratio -
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3080cc31de48d31065b1b2686b2c64b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: and run OLS —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: And the output —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In Reality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: α and β are the unobserved parameters of the model that we aim to estimate,
    so in practice, we cannot directly calculate the inverse Mills ratio as we did
    previously. Heckman introduces a preliminary step in his correction method to
    assist in estimating the inverse Mills ratio. This is why the Heckman’s correction
    is known as a **two stage estimator**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, our issue is that we don’t observe all the values of the dependent
    variable. For instance, if we’re examining how education (Z) influences wage (Y),
    but only observe wages above a certain threshold, we need to develop a theoretical
    explanation for the education levels of individuals with wages below this threshold.
    Once we have that, we can estimate a [probit](https://en.wikipedia.org/wiki/Probit)
    model of the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d7d7775e31a6d62647800c4a63ec600.png)'
  prefs: []
  type: TYPE_IMG
- en: and use the estimated parameters of this probit model to calculate an estimator
    for the inverse Mills ratio. In our case (notice I don’t use α and β) —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: and again, OLS for the second stage gives us consistent estimators —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Wrap up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used simulated data to demonstrate a sample selection bias problem resulting
    from censoring dependent variable values. We explored how this issue relates to
    OLS causal identification assumptions by examining the simulated errors of our
    model and the biased subsample. Finally, we introduced Heckman’s method for correcting
    the bias, allowing us to obtain consistent and unbiased estimators even when working
    with a biased sample.
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoyed this story, I’d greatly appreciate your support — [buying me
    a coffee would mean a lot!](https://ko-fi.com/dapollak)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] James J. Heckman, [Sample Selection Bias as a Specification Error](https://www.jstor.org/stable/1912352)
    (1979), Econometrica'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Nick Huntington-Klein, [The Effect](https://theeffectbook.net/index.html)
    Book (2022)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Christopher Winship, [Models for sample bias](https://typeset.io/pdf/models-for-sample-selection-bias-4qnrg4cc1f.pdf)
    (1992)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author*'
  prefs: []
  type: TYPE_NORMAL
