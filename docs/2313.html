<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Using Generative AI to Automatically Create a Video Talk from an Article</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Using Generative AI to Automatically Create a Video Talk from an Article</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=collection_archive---------2-----------------------#2024-09-22">https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=collection_archive---------2-----------------------#2024-09-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c947" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Using Gemini + Text to Speech + MoviePy to create a video, and what this says about what GenAI is becoming rapidly useful for</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://lakshmanok.medium.com/?source=post_page---byline--6381c44c5fe0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lak Lakshmanan" class="l ep by dd de cx" src="../Images/9faaaf72d600f592cbaf3e9089cbb913.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*TveVoapl-TEk-jBTrbis8w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6381c44c5fe0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://lakshmanok.medium.com/?source=post_page---byline--6381c44c5fe0--------------------------------" rel="noopener follow">Lak Lakshmanan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6381c44c5fe0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="2026" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Like most everyone, I was flabbergasted by <a class="af nf" href="https://blog.google/technology/ai/notebooklm-audio-overviews/" rel="noopener ugc nofollow" target="_blank">NotebookLM and its ability to generate a podcast</a> from a set of documents. And then, I got to thinking: “how do they do that, and where can I get some of that magic?” How easy would it be to replicate?</p><h2 id="255b" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">Goal: Create a video talk from an article</h2><p id="0250" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">I don’t want to create a podcast, but I’ve often wished I could generate slides and a video talk from my blog posts —some people prefer paging through slides, and others prefer to watch videos, and this would be a good way to meet them where they are. In this article, I’ll show you how to do this.</p><p id="9742" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <a class="af nf" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb" rel="noopener ugc nofollow" target="_blank">full code for this article</a> is on GitHub — in case you want to follow along with me. And the goal is to create this video from <a class="af nf" href="https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc" rel="noopener">this article</a>:</p><figure class="og oh oi oj ok ol"><div class="om io l ed"><div class="on oo l"/></div><figcaption class="op oq or os ot ou ov bf b bg z dx">Video created automatically using the code described in this article. Video generated by author.</figcaption></figure><h2 id="48cd" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">1. Initialize the LLM</h2><p id="b99d" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">I am going to use Google Gemini Flash because (a) it is the least expensive frontier LLM today, (b) it’s multimodal in that it can read and understand images also, and (c) it supports controlled generation, meaning that we can make sure the output of the LLM matches a desired structure.</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="d0e9" class="pa nh fq ox b bg pb pc l pd pe">import pdfkit<br/>import os<br/>import google.generativeai as genai<br/>from dotenv import load_dotenv<br/><br/>load_dotenv("../genai_agents/keys.env")<br/>genai.configure(api_key=os.environ["GOOGLE_API_KEY"])</span></pre><p id="cdd5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that I’m using Google Generative AI and not Google Cloud Vertex AI. The two packages are different. The Google one supports Pydantic objects for controlled generation; the Vertex AI one only supports JSON for now.</p><h2 id="9235" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">2. Get a PDF of the article</h2><p id="81e4" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">I used Python to download the article as a PDF, and upload it to a temporary storage location that Gemini can read:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="d953" class="pa nh fq ox b bg pb pc l pd pe">ARTICLE_URL = "https://lakshmanok.medium...."<br/>pdfkit.from_url(ARTICLE_URL, "article.pdf")<br/>pdf_file = genai.upload_file("article.pdf")</span></pre><p id="7263" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unfortunately, something about medium prevents pdfkit from getting the images in the article (perhaps because they are webm and not png …). So, my slides are going to be based on just the text of the article and not the images.</p><h2 id="9e3d" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">3. Create lecture notes in JSON</h2><p id="7beb" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Here, the data format I want is a set of slides each of which has a title, key points, and a set of lecture notes. The lecture as a whole has a title and an attribution also.</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="a2f7" class="pa nh fq ox b bg pb pc l pd pe">class Slide(BaseModel):<br/>    title: str<br/>    key_points: List[str]<br/>    lecture_notes: str<br/><br/>class Lecture(BaseModel):<br/>    slides: List[Slide]<br/>    lecture_title: str<br/>    based_on_article_by: str</span></pre><p id="3609" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s tell Gemini what we want it to do:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="83b8" class="pa nh fq ox b bg pb pc l pd pe">lecture_prompt = """<br/>You are a university professor who needs to create a lecture to<br/>a class of undergraduate students.<br/><br/>* Create a 10-slide lecture based on the following article.<br/>* Each slide should contain the following information:<br/>  - title: a single sentence that summarizes the main point<br/>  - key_points: a list of between 2 and 5 bullet points. Use phrases, not full sentences.<br/>  - lecture_notes: 3-10 sentences explaining the key points in easy-to-understand language. Expand on the points using other information from the article.<br/>* Also, create a title for the lecture and attribute the original article's author.<br/>"""</span></pre><p id="572a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The prompt is pretty straightforward — ask Gemini to read the article, extract key points and create lecture notes.</p><p id="cf00" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, invoke the model, passing in the PDF file and asking it to populate the desired structure:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="5bf5" class="pa nh fq ox b bg pb pc l pd pe"><br/>model = genai.GenerativeModel(<br/>    "gemini-1.5-flash-001",<br/>    system_instruction=[lecture_prompt]<br/>)<br/>generation_config={<br/>    "temperature": 0.7,<br/>    "response_mime_type": "application/json",<br/>    "response_schema": Lecture<br/>}<br/>response = model.generate_content(<br/>    [pdf_file],<br/>    generation_config=generation_config,<br/>    stream=False<br/>)</span></pre><p id="8469" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A few things to note about the code above:</p><ul class=""><li id="6d07" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">We pass in the prompt as the system prompt, so that we don’t need to keep sending in the prompt with new inputs.</li><li id="fd97" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We specify the desired response type as JSON, and the schema to be a Pydantic object</li><li id="f903" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We send the PDF file to the model and tell it generate a response. We’ll wait for it to complete (no need to stream)</li></ul><p id="48ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is JSON, so extract it into a Python object:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="1dac" class="pa nh fq ox b bg pb pc l pd pe">lecture = json.loads(response.text)</span></pre><p id="9a90" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, this is what the 3rd slide looks like:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="b0d8" class="pa nh fq ox b bg pb pc l pd pe">{'key_points': [<br/>    'Silver layer cleans, structures, and prepares data for self-service analytics.',<br/>    'Data is denormalized and organized for easier use.',<br/>    'Type 2 slowly changing dimensions are handled in this layer.',<br/>    'Governance responsibility lies with the source team.'<br/>  ],<br/> 'lecture_notes': 'The silver layer takes data from the bronze layer and transforms it into a usable format for self-service analytics. This involves cleaning, structuring, and organizing the data. Type 2 slowly changing dimensions, which track changes over time, are also handled in this layer. The governance of the silver layer rests with the source team, which is typically the data engineering team responsible for the source system.',<br/> 'title': 'The Silver Layer: Data Transformation and Preparation'<br/>}</span></pre><h2 id="9592" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">4. Convert to PowerPoint</h2><p id="f36a" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We can use the Python package pptx to create a Presentation with notes and bullet points. The code to create a slide looks like this:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="c273" class="pa nh fq ox b bg pb pc l pd pe">for slidejson in lecture['slides']:<br/>    slide = presentation.slides.add_slide(presentation.slide_layouts[1])<br/>    title = slide.shapes.title<br/>    title.text = slidejson['title']<br/>    # bullets<br/>    textframe = slide.placeholders[1].text_frame<br/>    for key_point in slidejson['key_points']:<br/>        p = textframe.add_paragraph()<br/>        p.text = key_point<br/>        p.level = 1<br/>    # notes<br/>    notes_frame = slide.notes_slide.notes_text_frame<br/>    notes_frame.text = slidejson['lecture_notes']</span></pre><p id="4735" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is a PowerPoint presentation that looks like this:</p><figure class="og oh oi oj ok ol os ot paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="os ot pn"><img src="../Images/532193d938ec39c67ae8e4b4c5ca81e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NDLoGhJb-THBvAOgRFQ5tQ.jpeg"/></div></div><figcaption class="op oq or os ot ou ov bf b bg z dx">The PowerPoint file that was generated from the keypoints and lecture notes. Screenshot by author.</figcaption></figure><p id="31ff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Not very fancy, but definitely a great starting point for editing if you are going to give a talk.</p><h2 id="e1d1" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">5. Read the notes aloud and save audio</h2><p id="3a89" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Well, we were inspired by a podcast, so let’s see how to create just an audio of someone summarizing the article.</p><p id="3765" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We already have the lecture notes, so let’s create audio files of each of the slides.</p><p id="bc06" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s the code to take some text, and have an AI voice read it out. We save the resulting audio into an mp3 file:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="e764" class="pa nh fq ox b bg pb pc l pd pe">from google.cloud import texttospeech<br/><br/>def convert_text_audio(text, audio_mp3file):<br/>    """Synthesizes speech from the input string of text."""<br/>    tts_client = texttospeech.TextToSpeechClient()    <br/>    input_text = texttospeech.SynthesisInput(text=text)<br/>    <br/>    voice = texttospeech.VoiceSelectionParams(<br/>        language_code="en-US",<br/>        name="en-US-Standard-C",<br/>        ssml_gender=texttospeech.SsmlVoiceGender.FEMALE,<br/>    )<br/>    audio_config = texttospeech.AudioConfig(<br/>        audio_encoding=texttospeech.AudioEncoding.MP3<br/>    )<br/><br/>    response = tts_client.synthesize_speech(<br/>        request={"input": input_text, "voice": voice, "audio_config": audio_config}<br/>    )<br/><br/>    # The response's audio_content is binary.<br/>    with open(audio_mp3file, "wb") as out:<br/>        out.write(response.audio_content)<br/>        print(f"{audio_mp3file} written.")</span></pre><p id="ddd2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What’s happening in the code above?</p><ul class=""><li id="9023" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">We are using Google Cloud’s text to speech API</li><li id="3c29" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">Asking it to use a standard US accent female voice. If you were doing a podcast, you’d pass in a “speaker map” here, one voice for each speaker.</li><li id="4b88" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">We then give it in the input text, ask it generate audio</li><li id="2127" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">Save the audio as an mp3 file. Note that this has to match the audio encoding.</li></ul><p id="28d7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, create audio by iterating through the slides, and passing in the lecture notes:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="43d7" class="pa nh fq ox b bg pb pc l pd pe">for slideno, slide in enumerate(lecture['slides']):<br/>        text = f"On to {slide['title']} \n"<br/>        text += slide['lecture_notes'] + "\n\n"<br/>        filename = os.path.join(outdir, f"audio_{slideno+1:02}.mp3")<br/>        convert_text_audio(text, filename)<br/>        filenames.append(filename)</span></pre><p id="7317" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is a bunch of audio files. You can concatenate them if you wish using pydub:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="3853" class="pa nh fq ox b bg pb pc l pd pe">combined = pydub.AudioSegment.empty()<br/>for audio_file in audio_files:<br/>    audio = pydub.AudioSegment.from_file(audio_file)<br/>    combined += audio<br/>    # pause for 4 seconds<br/>    silence = pydub.AudioSegment.silent(duration=4000)<br/>    combined += silence<br/>combined.export("lecture.wav", format="wav")</span></pre><p id="e049" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But it turned out that I didn’t need to. The individual audio files, one for each slide, were what I needed to create a video. For a podcast, of course, you’d want a single mp3 or wav file.</p><h2 id="7285" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">6. Create images of the slides</h2><p id="ee07" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Rather annoyingly, there’s no easy way to render PowerPoint slides as images using Python. You need a machine with Office software installed to do that — not the kind of thing that’s easily automatable. Maybe I should have used Google Slides … Anyway, a simple way to render images is to use the Python Image Library (PIL):</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="0d17" class="pa nh fq ox b bg pb pc l pd pe">def text_to_image(output_path, title, keypoints):<br/>    image = Image.new("RGB", (1000, 750), "black")<br/>    draw = ImageDraw.Draw(image)<br/>    title_font = ImageFont.truetype("Coval-Black.ttf", size=42)<br/>    draw.multiline_text((10, 25), wrap(title, 50), font=title_font)<br/>    text_font = ImageFont.truetype("Coval-Light.ttf", size=36)<br/>    for ptno, keypoint in enumerate(keypoints):<br/>        draw.multiline_text((10, (ptno+2)*100), wrap(keypoint, 60), font=text_font) <br/>    image.save(output_path)</span></pre><p id="c476" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The resulting image is not great, but it is serviceable (you can tell no one pays me to write production code anymore):</p><figure class="og oh oi oj ok ol os ot paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="os ot pt"><img src="../Images/417e2c0e11b84caacc8069f9a4e922d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHY-JxSGXkzP7Ko4Xr_d5w.jpeg"/></div></div><figcaption class="op oq or os ot ou ov bf b bg z dx">The images used along with the audio clips look like this. Image generated by author.</figcaption></figure><h2 id="0706" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">7. Create a Video</h2><p id="35b0" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Now that we have a set of audio files and a set of image files, we can use a Python package moviepy to create a video clip:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="c94b" class="pa nh fq ox b bg pb pc l pd pe">clips = []<br/>for slide, audio in zip(slide_files, audio_files):<br/>    audio_clip = AudioFileClip(f"article_audio/{audio}")<br/>    slide_clip = ImageClip(f"article_slides/{slide}").set_duration(audio_clip.duration)<br/>    slide_clip = slide_clip.set_audio(audio_clip)<br/>    clips.append(slide_clip)<br/>full_video = concatenate_videoclips(clips)</span></pre><p id="b4dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And we can now write it out:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="c600" class="pa nh fq ox b bg pb pc l pd pe">full_video.write_videofile("lecture.mp4", fps=24, codec="mpeg4", <br/>                           temp_audiofile='temp-audio.mp4', remove_temp=True)</span></pre><p id="6adb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">End result? We have four artifacts, all created automatically from the article.pdf:</p><pre class="og oh oi oj ok ow ox oy bp oz bb bk"><span id="b1b8" class="pa nh fq ox b bg pb pc l pd pe">lecture.json  lecture.mp4  lecture.pptx  lecture.wav</span></pre><p id="b067" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There’s:</p><ul class=""><li id="e719" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pf pg ph bk">a JSON file with keypoints, lecture notes, etc.</li><li id="68ba" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">A PowerPoint file that you can modify. The slides have the key points, and the notes section of the slides has the “lecture notes”</li><li id="a005" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">An audio file consisting of an AI voice reading out the lecture notes</li><li id="1cd8" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pf pg ph bk">A mp4 movie (that I uploaded to YouTube) of the audio + images. This is the video talk that I set out to create.</li></ul><p id="1822" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Pretty cool, eh?</p><h2 id="da7f" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">8. What this says about the future of software</h2><p id="e376" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">We are all, as a community, probing around to find what this really cool technology (generative AI) can be used for. Obviously, you can use it to create content, but the content that it creates is good for brainstorming, but not to use as-is. Three years of improvements in the tech have not solved the problem that GenAI generates blah content, and not-ready-to-use code.</p><p id="2427" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That brings us to some of the ancillary capabilities that GenAI has opened up. And these turn out to be extremely useful. There are four capabilities of GenAI that this post illustrates.</p><p id="5336" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">(1) Translating unstructured data to structured data</strong></p><p id="f430" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Attention paper was written to solve the translation problem, and it turns out transformer-based models are really good at translation. We keep discovering use cases of this. But not just <a class="af nf" href="https://mse238blog.stanford.edu/2017/08/jchoi8/machine-learning-transforms-google-translate-overnight/" rel="noopener ugc nofollow" target="_blank">Japanese to English</a>, but also <a class="af nf" href="https://digiday.com/media/how-amazons-genai-tool-for-developers-is-saving-4500-years-of-work-260-million-annually/" rel="noopener ugc nofollow" target="_blank">Java 11 to Java 17</a>, of <a class="af nf" href="https://paperswithcode.com/task/text-to-sql" rel="noopener ugc nofollow" target="_blank">text to SQL</a>, of text to speech, between database dialects, …, and now of articles to audio-scripts. This, it turns out is the stepping point of using GenAI to create podcasts, lectures, videos, etc.</p><p id="e556" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All I had to do was to prompt the LLM to construct a series of slide contents (keypoints, title, etc.) from the article, and it did. It even returned the data to me in structured format, conducive to using it from a computer program. Specifically, <em class="pu">GenAI is really good at translating unstructured data to structured data</em>.</p><p id="0da0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">(2) Code search and coding assistance are now dramatically better</strong></p><p id="ca0f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The other thing that GenAI turns out to be really good at is at adapting code samples dynamically. I don’t write code to create presentations or text-to-speech or moviepy everyday. Two years ago, I’d have been using Google search and getting Stack Overflow pages and adapting the code by hand. Now, Google search is giving me ready-to-incorporate code:</p><figure class="og oh oi oj ok ol os ot paragraph-image"><div class="os ot pv"><img src="../Images/318fc9e541190e93ace8486d8dcd05f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*7Tumb0Rc8co0cYZAlpgFug.png"/></div><figcaption class="op oq or os ot ou ov bf b bg z dx">Google Search returning code samples, adapated to my specific query. Screenshot by author.</figcaption></figure><p id="ce1c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Of course, had I been using a Python IDE (rather than a Jupyter notebook), I could have avoided the search step completely — I could have written a comment and gotten the code generated for me. This is hugely helpful, and speeds up development using general purpose APIs.</p><p id="a33b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">(3) GenAI web services are robust and easy-to-consume</strong></p><p id="3d8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s not lose track of the fact that I used the Google Cloud Text-to-Speech service to turn my audio script into actual audio files. Text-to-speech is itself a generative AI model (and another example of the translation superpower). The Google TTS service which was <a class="af nf" href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-text-to-speech-powered-by-deepmind-wavenet-technology" rel="noopener ugc nofollow" target="_blank">introduced in 2018</a> (and presumably improved since then) was one of the first generative AI services in production and made available through an API.</p><p id="91cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I used two generative AI models — TTS and Gemini — that are made available as web services. All I had to do was to call their APIs.</p><p id="d3b8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">(4) It’s easier than ever to provide end-user customizability</strong></p><p id="8bfa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I didn’t do this, but you can squint a little and see where things are headed. If I’d wrapped up the presentation creation, audio creation, and movie creation code in services, I could have had a prompt create the function call to invoke these services as well. And put a request-handling agent that would allow you to use text to change the look-and-feel of the slides or the voice of the person reading the video.</p><p id="cb01" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It becomes extremely easy to add open-ended customizability to the software you build.</p><h2 id="86cd" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">Summary</h2><p id="7da7" class="pw-post-body-paragraph mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne fj bk">Inspired by the NotebookLM podcast feature, I set out to build an application that would convert my articles to video talks. The key step is to prompt an LLM to produce slide contents from the article, another GenAI model to convert the audio script into audio files, and use existing Python APIs to put them together into a video.</p><p id="d7a1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This article illustrates four capabilities that GenAI is unlocking: translation of all kinds, coding assistance, robust web services, and end-user customizability.</p><p id="8902" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I loved being able to easily and quickly create video lectures from my articles. But I’m even more excited about the potential that we keep discovering in this new tool we have in our hands.</p><h2 id="bedd" class="ng nh fq bf ni nj nk nl nm nn no np nq ms nr ns nt mw nu nv nw na nx ny nz oa bk">Further Reading</h2><ol class=""><li id="bde2" class="mj mk fq ml b go ob mn mo gr oc mq mr ms od mu mv mw oe my mz na of nc nd ne pw pg ph bk">Full code for this article: <a class="af nf" href="https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb" rel="noopener ugc nofollow" target="_blank">https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb</a></li><li id="d129" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pw pg ph bk">The source article that I converted to a video: <a class="af nf" href="https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc" rel="noopener">https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc</a></li><li id="af96" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pw pg ph bk">The resulting video: <a class="af nf" href="https://youtu.be/jKzmj8-1Y9Q" rel="noopener ugc nofollow" target="_blank">https://youtu.be/jKzmj8-1Y9Q</a></li><li id="2e58" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pw pg ph bk">Turns out <a class="af nf" href="https://medium.com/google-cloud/building-a-dynamic-podcast-generator-inspired-by-googles-notebooklm-and-illuminate-e585cfcd0af1" rel="noopener">Sascha Heyer wrote up how to use GenAI to generate a podcast</a>, which is the exact Notebook LM usecase. His approach is somewhat similar to mine, except that there is no video, just audio. In a cool twist, he uses his own voice as one of the podcast speakers!</li><li id="2b8f" class="mj mk fq ml b go pi mn mo gr pj mq mr ms pk mu mv mw pl my mz na pm nc nd ne pw pg ph bk">Of course, here’s the video talk of this article created using the technique shown in this video. Ideally, we are pulling out code snippets and images from the article, but this is a start …</li></ol><figure class="og oh oi oj ok ol"><div class="om io l ed"><div class="on oo l"/></div></figure></div></div></div></div>    
</body>
</html>