- en: 'Mastering Sensor Fusion: LiDAR Obstacle Detection with KITTI Data — Part 1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通传感器融合：基于 KITTI 数据的 LiDAR 障碍物检测 — 第一部分
- en: 原文：[https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25](https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25](https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25)
- en: How to use Lidar data for obstacle detection with unsupervised learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 LiDAR 数据进行无监督学习的障碍物检测
- en: '[](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)[![Erol
    Çıtak](../Images/621dc247f6fdbf5e25b74fdc07d25f7b.png)](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)
    [Erol Çıtak](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)[![Erol
    Çıtak](../Images/621dc247f6fdbf5e25b74fdc07d25f7b.png)](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)
    [Erol Çıtak](https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)
    ·20 min read·Dec 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------)
    ·阅读时间 20 分钟 ·2024年12月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Sensor fusion, multi-modal perception, autonomous vehicles** — if these keywords
    pique your interest, this Medium blog is for you. Join me as I explore the fascinating
    world of LiDAR and color image-based environment understanding, showcasing how
    these technologies are combined to enhance obstacle detection and decision-making
    for autonomous vehicles. This blog and the following series dive into practical
    implementations and theoretical insights, offering an engaging read for all curious
    eyes.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**传感器融合，多模态感知，自动驾驶车辆** — 如果这些关键词引起了你的兴趣，那么这篇 Medium 博客就是为你准备的。跟随我一起探索 LiDAR
    和基于彩色图像的环境理解的迷人世界，展示这些技术如何结合以增强障碍物检测和自动驾驶车辆的决策能力。本博客及随后的系列将深入探讨实际应用和理论见解，提供一个引人入胜的阅读体验，适合所有好奇的读者。'
- en: In this Medium blog series, we will examine the KITTI 3D Object Detection dataset
    [1][3] in three distinct parts. In the first article, which is this one, we will
    be talking about the KITTI Velodyne Lidar sensor and single-mode obstacle detection
    with this sensor only. In the second article of the series, we will be working
    on detection studies on color images with a uni-modal approach. In the last article
    of the series, we will work on multi-modal object detection, which can be called
    sensor fusion. During that process, both the Lidar and the Color Image sensors
    come into play to work together.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 Medium 博客系列中，我们将分三部分来探讨 KITTI 3D 物体检测数据集 [1][3]。在第一篇文章中，也就是这篇文章，我们将讨论 KITTI
    Velodyne Lidar 传感器以及仅使用该传感器的单模态障碍物检测。在系列的第二篇文章中，我们将基于单一模态方法进行彩色图像的检测研究。在系列的最后一篇文章中，我们将探讨多模态物体检测，也就是传感器融合。在这个过程中，Lidar
    和彩色图像传感器将共同工作。
- en: One last note before we get into the topic! I promise that I will provide all
    the theoretical information about each subtopic at a basic level throughout this
    series :) However, I will also be leaving very high-quality references for each
    subtopic without forgetting those who want more in-depth information.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入主题之前，有最后一点小提示！我保证在整个系列中，我将以基础的层次提供每个子主题的所有理论信息 :) 但是，我也会为每个子主题提供非常高质量的参考资料，并且不会忘记那些想要深入了解的人。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: KITTI or KITTI Vision Benchmark Suite is a project created in collaboration
    with Karlsruhe Institute of Technology and Toyota Research Institute. We can say
    that it is a platform that includes many different test scenarios, including 2D/3D
    object detection, multi-object tracking, semantic segmentation, and so forth.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI或KITTI视觉基准测试套件是一个由卡尔斯鲁厄理工大学和丰田研究院合作创建的项目。我们可以说它是一个平台，包含了许多不同的测试场景，包括2D/3D目标检测、多目标跟踪、语义分割等。
- en: For 3D object detection, which is the subject of this article series, there
    are 7481 training and 7518 test data from different sensors, which are Velodyne
    Lidar Sensor and Stereo Color Image Sensors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于3D目标检测，这是本文系列的主题，数据集包含7481条训练数据和7518条测试数据，数据来源于不同的传感器，包括Velodyne激光雷达传感器和立体视觉图像传感器。
- en: '![](../Images/093310bd95e8a099953e0ee4c0850ee6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/093310bd95e8a099953e0ee4c0850ee6.png)'
- en: A sample image for 3D Object Detection [3] (Image Taken from [https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 3D目标检测的示例图像 [3]（图片来自 [https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)）
- en: In this blog post, we will perform obstacle detection using Velodyne Lidar point
    clouds. In this context, reading point clouds, visualization, and segmentation
    with the help of unsupervised machine learning algorithms will be the main topics.
    In addition to these, we will talk a lot about camera calibration and its internal
    and external parameters, the RANSAC algorithm for vehicle path detection, and
    basic evaluation metrics to measure the performance of the outputs that we will
    need while performing these steps.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博文中，我们将使用Velodyne激光雷达点云进行障碍物检测。在这个背景下，利用无监督机器学习算法进行点云的读取、可视化和分割将是主要内容。除了这些，我们还会讨论很多关于相机标定及其内外部参数、用于车辆路径检测的RANSAC算法，以及在执行这些步骤时我们需要的评估指标。
- en: Also, I will be using Python language throughout this series, but don’t worry,
    I will share with you the information about the virtual environment I use. This
    way, you can quickly get your own environment up and running. Please check the
    Github repo to get the **requirements.txt** file.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 同时，在这一系列文章中，我将使用Python语言，但不用担心，我会与大家分享我使用的虚拟环境信息。这样你就可以快速搭建自己的环境。请查看Github仓库以获取**requirements.txt**文件。
- en: '**Problem Definition**'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**问题定义**'
- en: The main goal of this blog post is to detect obstacles in the environment detected
    by the sensor using the unsupervised learning method on point clouds obtained
    with the Veloydne Lidar in the KITTI dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇博文的主要目标是使用无监督学习方法，通过对KITTI数据集中由Velodyne激光雷达获取的点云，检测环境中的障碍物。
- en: Within this scope, I am sharing an example Lidar point cloud image below to
    visualize the problem. If we analyze the following sample point cloud, we can
    easily recognize some cars at the left bottom or some other objects on the road.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个范围内，我将分享一个示例的激光雷达点云图像，以便可视化问题。如果我们分析下面的示例点云，我们可以很容易地识别出左下角的一些汽车或路上的其他物体。
- en: '![](../Images/31b4f75729eac8a14edf8da70fe4eb61.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31b4f75729eac8a14edf8da70fe4eb61.png)'
- en: A sample Lidar point cloud [3] ( from KITTI dataset)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的激光雷达点云 [3]（来自KITTI数据集）
- en: To make it more visible, let me draw some arrows and boxes to show them. In
    the following image, red arrows indicate cars, orange arrows stand for pedestrians,
    and red boxes are drawn for street lambs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更加清晰，我将在图像中画一些箭头和框来表示它们。在下图中，红色箭头表示汽车，橙色箭头表示行人，红色框表示路灯。
- en: '![](../Images/a5609b489f7b3e2806d3169d93368ddf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5609b489f7b3e2806d3169d93368ddf.png)'
- en: A sample Lidar point cloud [3] ( from KITTI dataset)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的激光雷达点云 [3]（来自KITTI数据集）
- en: Then, you may wonder and ask this question ***“Wouldn’t we also say that there
    are other objects around, perhaps walls or trees?”*** The answer is YES! The proof
    of my answer can be obtained from the color image corresponding to this point
    cloud. As can be seen from the image below, there are people, a car, street lights,
    and trees on the scene.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可能会好奇并问这个问题 ***“我们是不是也可以说周围还有其他物体，可能是墙壁或者树木？”*** 答案是肯定的！我的答案的证明可以从与这个点云对应的彩色图像中得到。从下面的图像中可以看到，现场有行人、汽车、路灯和树木。
- en: '![](../Images/71fa96542279146571ee9bc73e0227d4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71fa96542279146571ee9bc73e0227d4.png)'
- en: A sample color image [3] ( from KITTI dataset)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例的彩色图像 [3]（来自KITTI数据集）
- en: After this visual analysis, we come to a subject that careful readers will immediately
    notice. While the Lidar point cloud provides a 360-degree view of the scene, color
    image only provides a limited wide perception of the scene. The following blog
    will be taking only this colored image into consideration for object detection
    and the last one will try to fuse Lidar point cloud and color image sensors to
    handle the problem ***(I hope they will be available soon!)***
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一视觉分析之后，我们来到了一个细心读者会立刻注意到的问题。虽然激光雷达点云提供了场景的360度视角，彩色图像却只能提供有限的场景宽度感知。接下来的博客将只考虑使用这一彩色图像进行目标检测，最后一篇将尝试融合激光雷达点云和彩色图像传感器来解决这个问题***(我希望它们很快就能使用！)***
- en: Sensor Setup
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传感器设置
- en: Then let’s talk about the sensors and their installations and so on. The KITTI
    3D object detection dataset was collected using a specially modified Volkswagen
    Passat B6\. Data recording was handled by an eight-core i7 computer with a RAID
    system, running Ubuntu Linux alongside a real-time database for efficient data
    management.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来讨论传感器及其安装等问题。KITTI 3D目标检测数据集是通过专门改装的大众帕萨特B6收集的。数据记录由一台八核i7计算机处理，该计算机配备RAID系统，运行Ubuntu
    Linux，并配有实时数据库以便高效的数据管理。
- en: 'The following sensors were used for data collection:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下传感器用于数据采集：
- en: '**Inertial Navigation System (GPS/IMU):** OXTS RT 3003'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**惯性导航系统（GPS/IMU）：** OXTS RT 3003'
- en: '**Lidar Sensor:** Velodyne HDL-64E'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激光雷达传感器：** Velodyne HDL-64E'
- en: '**Grayscale Cameras:** Two Point Grey Flea 2 (FL2–14S3M-C), each with 1.4 Megapixels'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灰度相机：** 两台 Point Grey Flea 2（FL2–14S3M-C），每台1.4百万像素'
- en: '**Color Cameras:** Two Point Grey Flea 2 (FL2–14S3C-C), each with 1.4 Megapixels'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**彩色相机：** 两台 Point Grey Flea 2（FL2–14S3C-C），每台1.4百万像素'
- en: '**Varifocal Lenses:** Four Edmund Optics NT59–917 (4–8 mm)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可变焦镜头：** 四个 Edmund Optics NT59–917（4–8 毫米）'
- en: The visualization of the aforementioned setup is presented in the following
    figure.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 上述设置的可视化展示在下图中。
- en: '![](../Images/9a76a762e1083e1a660dde606ddf0009.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a76a762e1083e1a660dde606ddf0009.png)'
- en: KITTI dataset setup visualization [3] ( Image Taken from KITTI)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI 数据集设置可视化 [3]（图片来源：KITTI）
- en: The Velodyne Lidar sensor and the Color cameras are installed on top of the
    car but their height from the ground and their coordinates are different than
    each other. No worries! As promised, we will go step by step. It means that, before
    getting the core of the algorithm of this blog post, we need to revisit the camera
    calibration topic first!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Velodyne激光雷达传感器和彩色相机安装在汽车的顶部，但它们距离地面的高度和坐标不同。别担心！如承诺所示，我们将一步一步来。这意味着，在进入本博客文章的核心算法之前，我们需要先回顾一下相机标定的主题！
- en: Camera Calibration
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相机标定
- en: Cameras, or sensors in a broader sense, provide perceptual outputs of the surrounding
    environment in different ways. In this concept, let’s take an RGB camera, it could
    be your webcam or maybe a professional digital compact camera. It projects 3D
    points in the world onto a 2D image plane using two sets of parameters; the intrinsic
    and extrinsic parameters.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 相机或更广义上的传感器，以不同的方式提供了周围环境的感知输出。在这个概念中，我们以RGB相机为例，它可能是你的网络摄像头，也可能是专业的数码紧凑型相机。它使用两组参数：内参和外参，将世界中的3D点投影到二维图像平面上。
- en: '![](../Images/609246ec696c69c18964161d7ba54c02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/609246ec696c69c18964161d7ba54c02.png)'
- en: 'Projection of 3D points in the world to the 2D image plane ( Image taken from:
    [https://de.mathworks.com/help/vision/ug/camera-calibration.html](https://de.mathworks.com/help/vision/ug/camera-calibration.html))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将世界中的3D点投影到2D图像平面上（图片来源：[https://de.mathworks.com/help/vision/ug/camera-calibration.html](https://de.mathworks.com/help/vision/ug/camera-calibration.html)）
- en: While the extrinsic parameters are about the location and the orientation of
    the camera in the world frame domain, the intrinsic parameters map the camera
    coordinates to the pixel coordinates in the image frame.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 外参参数描述了相机在世界坐标系中的位置和方向，而内参参数则将相机坐标映射到图像坐标系中的像素坐标。
- en: In this concept, the camera extrinsic parameters can be represented as a matrix
    like T = [R | t ] where R stands for the rotation matrix, which is 3x3 and t stands
    for the translation vector, which is 3x1\. As a result, the T matrix is a 3x4
    matrix that takes a point in the world and maps it to the ‘camera coordinate’
    domain.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个概念中，相机的外参可以表示为一个矩阵，形式为 T = [R | t]，其中 R 是旋转矩阵（3x3），t 是平移向量（3x1）。因此，T 矩阵是一个3x4矩阵，它将世界中的一个点映射到“相机坐标”域。
- en: On the other hand, the camera's intrinsic parameters can be represented as a
    3x3 matrix. The corresponding matrix, K, can be given as follows. While fx and
    fy represent the focal length of the camera, cx and cy stand for principal points,
    and s indicates the skewness of the pixel.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，相机的内参可以表示为一个3x3矩阵。对应的矩阵K可以表示如下。这里，fx和fy表示相机的焦距，cx和cy表示主点，s表示像素的偏斜度。
- en: '![](../Images/4a9d8c128ffe6502e63edba0f1a8f298.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a9d8c128ffe6502e63edba0f1a8f298.png)'
- en: The camera’s intrinsic parameters
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相机的内参
- en: As a result, any 3D point can be projectable to the 2D image plane via following
    complete camera matrix.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何3D点都可以通过以下完整相机矩阵投影到2D图像平面。
- en: '![](../Images/dc45b8f23bdd6444edd26494941da738.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc45b8f23bdd6444edd26494941da738.png)'
- en: The complete camera matrix to project a 3D world point into the image plane
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将3D世界点投影到图像平面所需的完整相机矩阵
- en: I know that camera calibration seems a little bit complicated especially if
    you encounter it for the first time. But I have searched for some really good
    references for you. Also, I will be talking about the applied camera calibration
    operations for our problem in the following sections.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道相机标定看起来有点复杂，特别是当你第一次接触时。但是我已经为你找到了一些非常好的参考资料。另外，我将在接下来的章节中讲解我们问题中应用的相机标定操作。
- en: 'References for the camera calibration topic:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相机标定主题的参考文献：
- en: '*— Carnegie Mellon University,* [*https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf*](https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*— 卡内基梅隆大学，* [*https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf*](https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf)'
- en: '*— Columbia University,* [*https://www.youtube.com/watch?v=GUbWsXU1mac*](https://www.youtube.com/watch?v=GUbWsXU1mac)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*— 哥伦比亚大学，* [*https://www.youtube.com/watch?v=GUbWsXU1mac*](https://www.youtube.com/watch?v=GUbWsXU1mac)'
- en: '*— Camera Calibration Medium Post,* [*https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789*](https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*— 相机标定中篇文章，* [*https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789*](https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789)'
- en: Dataset Understanding
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集理解
- en: After a couple of terminologies and the required basic theory, now we are able
    to get into the problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些术语和所需的基础理论后，现在我们可以进入问题的核心了。
- en: First of all, I highly suggest you download the dataset from here [2] for the
    following ones;
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我强烈建议你从这里下载数据集[2]，用于接下来的操作；
- en: Left Color Images (size is 12GB)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 左侧彩色图像（大小为12GB）
- en: Velodyne Point Cloud (size is 29GB)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Velodyne 点云（大小为29GB）
- en: Camera Calibration Matrices of the Object Dataset (size is negligible)
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象数据集的相机标定矩阵（大小可以忽略不计）
- en: Training Labels (size is negligible)
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练标签（大小可以忽略不计）
- en: The data that we are going to analyze is the ground truth (G.T.)label files.
    G.T. files are presented in ‘.txt’ format and each object is labeled with 15 different
    fields. No worries, I prepared a detailed G.T. file read function in my Github
    repo as follows.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要分析的数据是地面实况（G.T.）标签文件。G.T. 文件以 '.txt' 格式呈现，每个物体都用15个不同的字段进行标注。别担心，我已经为你准备了一个详细的
    G.T. 文件读取函数，存放在我的Github仓库中，具体如下。
- en: '[PRE0]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The color images are presented as files in the folder and they can be read
    easily, which means without any further operations. As a result of this operation,
    it can be that ***# of training and testing images: 7481 / 7518***'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像以文件形式呈现于文件夹中，且可以轻松读取，无需进一步操作。通过此操作，得到的结果是***训练和测试图像的数量：7481 / 7518***
- en: The next data that we will be taking into consideration is the calibration files
    for each scene. As I did before, I prepared another function to parse calibration
    files as follows.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们要考虑的数据是每个场景的标定文件。像之前一样，我为你准备了另一个函数，用于解析标定文件，具体如下。
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The final data is the Velodyne point cloud and they are presented in ‘.bin’
    format. In this format, each point cloud line consists of the location of x, y,
    and z plus the reflectivity score. As before, the corresponding parse function
    is as follows.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最终数据是Velodyne点云，格式为 '.bin'。在此格式中，每一行点云包含x、y、z的位置以及反射率分数。与之前一样，相应的解析函数如下所示。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At the end of this section, all the required files will be loaded and ready
    to be used.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节结束时，所有必需的文件将被加载并准备好使用。
- en: For the sample scene, which was presented at the top of this post in the ‘Problem
    Definition’ section, there are 122794 points in the point cloud.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在本篇文章“问题定义”部分顶部展示的示例场景，点云中有122794个点。
- en: But since that amount of information could be hard to analyze for some systems
    in terms of CPU or GPU power, we may want to reduce the number of points in the
    cloud. To make it possible we can use the “Voxel Downsampling” operation, which
    is similar to the “Pooling” operation in deep neural networks. Roughly it divides
    the complete point cloud into a grid of equally sized voxels and chooses a single
    point from each voxel.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，由于这些信息对某些系统（特别是CPU或GPU性能较低的系统）来说可能较难处理，我们可能希望减少点云中的点数。为了实现这一点，我们可以使用“体素下采样”操作，这与深度神经网络中的“池化”操作类似。大致来说，它将完整的点云划分为一个等大小的体素网格，并从每个体素中选择一个点。
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output of this downsampling looks like this;
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此下采样的输出结果如下：
- en: 'Points before downsampling: 122794'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样前的点数：122794
- en: 'Points after downsampling: 33122'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下采样后的点数：33122
- en: But it shouldn’t be forgotten that reducing the number of points may cause to
    loss of some information as might be expected. Also, the voxel grid size is a
    hyper-parameter that we can choose is another crucial thing. Smaller sizes return
    a high number of points or vice versa.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，不应忽视的是，减少点数可能会导致一些信息的丢失，这是可以预见的。此外，体素网格的大小是一个超参数，选择它也非常关键。较小的体素大小会返回更多的点，反之亦然。
- en: But, before getting into the road segmentation by RANSAC, let's quickly re-visit
    the Voxel Downsampling operation together.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，在进行RANSAC道路分割之前，让我们快速回顾一下体素下采样操作。
- en: Voxel Downsampling
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 体素下采样
- en: Voxel Downsampling is a technique to create a downsampled point cloud. It highly
    helps to reduce some noise and not-required points. It also reduces the required
    computational power in light of the selected voxel grid size hyperparameter. The
    visualization of this operation can be given as follows.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 体素下采样是一种创建下采样点云的技术。它有助于减少噪声和不必要的点。同时，它通过选择的体素网格大小超参数减少了所需的计算能力。此操作的可视化可以如下展示。
- en: '![](../Images/ecc3d606cb1364f65fb83effc4485c82.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecc3d606cb1364f65fb83effc4485c82.png)'
- en: The illustration of Voxel Downsampling (Image taken from [https://www.mdpi.com/2076-3417/14/8/3160](https://www.mdpi.com/2076-3417/14/8/3160))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 体素下采样的示意图（图片来源：[https://www.mdpi.com/2076-3417/14/8/3160](https://www.mdpi.com/2076-3417/14/8/3160)）
- en: Besides that, the steps of this algorithm can be presented as follows.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，该算法的步骤可以如下展示。
- en: '![](../Images/f6eebbf332cd7806759832551cba58c7.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6eebbf332cd7806759832551cba58c7.png)'
- en: To apply this function, we will be using the “open3d” library with a single
    line;
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用此功能，我们将使用“open3d”库，代码只需一行；
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the above single-line code, it can be observed that the voxel size is chosen
    as 0.2
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述单行代码中，可以观察到体素大小被设置为0.2
- en: RANSAC
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RANSAC
- en: The next step will be segmenting the largest plane, which is the road for our
    problem. RANSAC, Random Sample Consensus, is an iterative algorithm and works
    by randomly sampling a subset of the data points to hypothesize a model and then
    evaluating its fit to the entire dataset. It aims to find the model that best
    explains the inliers while ignoring the outliers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步将是分割最大的平面，即我们的道路问题。RANSAC（随机采样一致性）是一种迭代算法，它通过随机采样数据点的子集来假设一个模型，并评估其与整个数据集的拟合度。其目标是找到能够最好地解释内点的模型，同时忽略离群值。
- en: While the algorithm is highly robust to the extreme outliers, it requires to
    sample of *n* points at the beginning (n=2 for a 2D line or 3 for a 3D plane).
    Then evaluates the performance of the mathematical equation with respect to it.
    Then it means;
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该算法对极端离群值具有很强的鲁棒性，但在开始时需要采样*n*个点（二维线条时n=2，三维平面时n=3）。然后评估该数学方程在此点集下的表现。也就是说；
- en: — the chosen points at the beginning are so crucial
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: — 起始时选择的点非常关键
- en: — the number of iterations to find the best values is so crucial
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: — 查找最佳值的迭代次数至关重要
- en: — it may require some computation power, especially for large datasets
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: — 它可能需要一定的计算能力，特别是对于大型数据集
- en: But it’s a kind of de-facto operation for many different cases. So first let's
    visualize the RANSAC to find a 2D line then let me present the key steps of this
    algorithm.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 但它已成为多种不同情况的事实标准操作。所以，首先让我们可视化RANSAC来寻找一个二维线条，然后再介绍该算法的关键步骤。
- en: '![](../Images/bdb851b12132c171ecc93373df50273f.png)![](../Images/e9ccf091fbab0098c74cd8a79051d028.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdb851b12132c171ecc93373df50273f.png)![](../Images/e9ccf091fbab0098c74cd8a79051d028.png)'
- en: The key steps and working flow of the RANSAC algorithm
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: RANSAC算法的关键步骤和工作流程
- en: After reviewing the concept of RANSAC, it is time to apply the algorithm on
    the point cloud to determine the largest plane, which is a road, for our problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了RANSAC的概念之后，现在是时候将算法应用于点云，以确定最大平面，这就是我们问题中的道路。
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output of this process will show the outside of the road in red and the
    road will be colored in a mixture of Green and Blue.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的输出将显示道路外部为红色，且道路将被涂成绿色和蓝色的混合色。
- en: '![](../Images/ed299ba9ce94bff26bece9e1afb7b4df.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed299ba9ce94bff26bece9e1afb7b4df.png)'
- en: The output of the RANSAC algorithm (Image taken from KITTI dataset [3])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: RANSAC算法的输出（图片来自KITTI数据集[3]）
- en: DBSCAN — a density-based clustering non-parametric algorithm
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN — 一种基于密度的无参数聚类算法
- en: At this stage, the detection of objects outside the road will be performed using
    the segmented version of the road with RANSAC.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，将使用RANSAC对分割后的道路版本进行物体检测，从而检测道路外的物体。
- en: 'In this context, we will be using unsupervised learning algorithms. However,
    the question that may come to mind here is **“Can’t a detection be made using
    supervised learning algorithms?”** The answer is very short and clear: Yes! However,
    since we want to introduce the problem and get a quick result with this blog post,
    we will continue with DBSCAN, which is a segmentation algorithm in the unsupervised
    learning domain. If you would like to see the results with a supervised learning-based
    object detection algorithm on point clouds, please indicate this in the comments.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将使用无监督学习算法。然而，可能会有一个问题浮现在脑海中：**“难道不能使用监督学习算法进行检测吗？”** 答案既简短又明确：可以！然而，由于我们希望通过这篇博客快速介绍问题并得到结果，因此我们将继续使用DBSCAN，这是一种无监督学习领域的分割算法。如果你希望看到基于监督学习的点云物体检测算法的结果，请在评论中注明。
- en: 'Anyway, let’s try to answer these three questions: What is DBSCAN and how does
    it work? What are the hyper-parameters to consider? How do we apply it to this
    problem?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不管怎样，让我们尝试回答这三个问题：什么是DBSCAN，它是如何工作的？需要考虑哪些超参数？我们如何将其应用于这个问题？
- en: 'DBSCAN also known as a density-based clustering non-parametric algorithm, is
    an unsupervised clustering algorithm. Even if there are some other unsupervised
    clustering algorithms, maybe one of the most popular ones is K-Means, DBSCAN is
    capable of clustering the objects in arbitrary shape while K-Means asumes the
    shape of the object is spherical. Moreover, probably the most important feature
    of DBSCAN is that it does not require the number of clusters to be defined/estimated
    in advance, as in the K-Means algorithm. If you would like to analyze some really
    good visualizations for some specific problems like “2Moons”, you can visit here:
    [*https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference*](https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN，也被称为基于密度的无参数聚类算法，是一种无监督的聚类算法。尽管还有其他一些无监督聚类算法，例如K-Means，但DBSCAN能够对形状任意的物体进行聚类，而K-Means假设物体的形状是球形的。此外，DBSCAN最重要的特性之一是，它不需要像K-Means算法那样预先定义或估算聚类的数量。如果你想查看一些针对特定问题（如“2Moons”）的优秀可视化结果，可以访问这里：[*https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference*](https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference)
- en: 'DBSCAN works like our eyes. It means it takes the densities of different groups
    in the data and then makes a decision for clustering. It has two different hyper-parameters:
    “Epsilon” and “MinimumPoints”. Initially, DBSCAN identifies *core points*, which
    are points with at least a minimum number of neighbors (*minPts*) within a specified
    radius (*epsilon*). Clusters are then formed by expanding from these core points,
    connecting all reachable points within the density criteria. Points that cannot
    be connected to any cluster are classified as noise. To get in-depth information
    about this algorithm like ‘Core Point’, ‘Border Point’ and ‘Noise Point’ please
    visit there: *Josh Starmer,* [*https://www.youtube.com/watch?v=RDZUdRSDOok&t=61s*](https://www.youtube.com/watch?v=RDZUdRSDOok&t=61s)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN的工作原理类似于我们的眼睛。这意味着它根据数据中不同组的密度做出聚类决策。它有两个不同的超参数：“Epsilon”和“MinimumPoints”。最初，DBSCAN会识别*核心点*，即在指定半径内至少有最小数量邻居（*minPts*）的点。然后，从这些核心点扩展，形成聚类，连接所有满足密度标准的可达点。无法与任何聚类连接的点被分类为噪声。要深入了解这个算法中的“核心点”、“边界点”和“噪声点”，请访问：*Josh
    Starmer,* [*https://www.youtube.com/watch?v=RDZUdRSDOok&t=61s*](https://www.youtube.com/watch?v=RDZUdRSDOok&t=61s)
- en: '![](../Images/06993b7230ebdbcb4ca4979cf6964103.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06993b7230ebdbcb4ca4979cf6964103.png)'
- en: A sample clustering result of the DBSCAN algorithm
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 算法的一个示例聚类结果
- en: For our problem, while we can use DBSCAN from the SKLearn library, let's use
    the open3d as follows.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的任务，虽然我们可以使用 SKLearn 库中的 DBSCAN，但让我们使用 open3d，代码如下。
- en: '[PRE6]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, ‘epsilon’ was chosen as 0.45, and ‘MinPts’ was chosen as 10\.
    A quick comment about these. Since they are hyper-parameters, there are no best
    “numbers” out there. Unfortunately, it’s a matter of trying and measuring success.
    But no worries! After you read the last chapter of this blog post, “Evaluation
    Metrics”, you will be able to measure your algorithm’s performance in total. Then
    it means you can apply GridSearch *( ref:* [*https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/*](https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/)*)*
    to find the best hyper-param pairs!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，‘epsilon’ 被选择为 0.45，‘MinPts’ 被选择为 10。简单评论一下这些。由于它们是超参数，实际上没有最佳的“数值”。不幸的是，这需要通过尝试和测量成功来进行调整。但别担心！在阅读了本博客文章的最后一章《评估指标》后，你将能够全面衡量你的算法表现。那时，你就可以应用
    GridSearch *(参考：* [*https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/*](https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/)*)*
    来找到最佳的超参数组合！
- en: Yep, then let me visualize the output of DBCAN for our point cloud then let's
    move to the next step!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，然后让我可视化 DBCAN 对我们点云的输出，然后我们继续下一步！
- en: '![](../Images/49c147bb85328c988cd4578517547d34.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49c147bb85328c988cd4578517547d34.png)'
- en: The output of the DBSCAN clustering algorithm (Image taken from KITTI dataset
    [3])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 聚类算法的输出（图像来自 KITTI 数据集 [3]）
- en: To recall, we can see that some of the objects that I first showed and marked
    by hand are separate and in different colors here! This shows that these objects
    belong to different clusters (as it should be).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，我们可以看到一些我最初展示并手动标记的物体在这里是分开的，并且显示为不同的颜色！这表明这些物体属于不同的聚类（这正是应该的）。
- en: G.T. Labels and Their Calibration Process
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: G.T. 标签及其标定过程
- en: Now it’s time to analyze G.T. labels and Calibration files of the KITTI 3D Object
    Detection benchmark. In the previous section, I shared some tips about them like
    how to read, how to parse, etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是分析 KITTI 3D 目标检测基准的 G.T. 标签和标定文件的时候了。在前一部分中，我分享了一些关于它们的小贴士，比如如何读取、如何解析等。
- en: But now I want to mention the relation between the G.T. object and the Calibration
    matrices. First of all, let me share a figure of the G.T. file and the Calibration
    file side by side.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我想提一下 G.T. 物体与标定矩阵之间的关系。首先，让我并排展示 G.T. 文件和标定文件的图示。
- en: '![](../Images/04e7940cc9b9d1ad687b7f4b4bb56eaf.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04e7940cc9b9d1ad687b7f4b4bb56eaf.png)'
- en: A sample training label file in .txt format
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 .txt 格式的示例训练标签文件
- en: As we discussed before, the last element of the training label refers to the
    rotation of the object around the y-axis. The three numbers before the rotation
    element (1.84, 1.47, and 8.41) stand for the 3D location of the object’s centroid
    in the camera coordinate system.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，训练标签的最后一个元素表示物体围绕 y 轴的旋转。旋转元素之前的三个数字（1.84、1.47 和 8.41）代表物体质心在相机坐标系中的三维位置。
- en: '![](../Images/4ca6a2d76848a8253ad7dbc32e718d6d.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ca6a2d76848a8253ad7dbc32e718d6d.png)'
- en: A sample calibration file in .txt format
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 .txt 格式的示例标定文件
- en: On the calibration file side; *P0, P1, P2*, and *P3* are the camera projection
    matrices for their corresponding cameras. In this blog post, as we indicated before,
    we are using the ‘Left Color Images’ which is equal to *P2*. Also, *R0_rect* is
    a rectification matrix for aligning stereo images. As can be understood from their
    names, *Tr_velo_to_cam* and *Tr_imu_to_velo* are transformation matrices that
    will be used to provide the transition between different coordinate systems. For
    example, *Tr_velo_to_cam* is a transformation matrix converting Velodyne coordinates
    to the unrectified camera coordinate system.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在标定文件方面；*P0, P1, P2* 和 *P3* 是对应相机的投影矩阵。在本文中，正如我们之前所指出的，我们使用的是‘左侧彩色图像’，即 *P2*。此外，*R0_rect*
    是用于对齐立体图像的矩阵。从它们的名称可以理解，*Tr_velo_to_cam* 和 *Tr_imu_to_velo* 是转换矩阵，用于在不同坐标系之间进行转换。例如，*Tr_velo_to_cam*
    是一个转换矩阵，用于将 Velodyne 坐标转换为未校正的相机坐标系。
- en: After this explanation, I really paid attention to which matrix or which label
    in the which coordinate system, now we can mention the transformation of G.T.
    object coordinates to the Velodyne coordinate system easily. It’s a good point
    to both understand the use of matrices between coordinate systems and evaluate
    our predicted bounding boxes and G.T. object bounding boxes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个解释之后，我真的很注意哪个矩阵或哪个标签在哪个坐标系中，现在我们可以轻松地提到从G.T.物体坐标到Velodyne坐标系的变换。这是一个很好的点，可以帮助我们理解坐标系之间矩阵的使用，并评估我们预测的边界框和G.T.物体边界框。
- en: The first thing that we will be doing is computing the G.T. object bounding
    box in 3D. To do so, you can reach out to the following function in the repo.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是计算G.T.物体的3D边界框。为此，您可以访问以下代码库中的函数。
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given an object’s dimensions (`height`, `width`, `length`) and position (`x,
    y, z`) in the camera coordinate system, this function first rotates the bounding
    box based on its orientation (`rotation_y`) and then computes the corners of the
    box in 3D space.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个物体在相机坐标系中的尺寸（`height`、`width`、`length`）和位置（`x, y, z`），该函数首先根据物体的朝向（`rotation_y`）旋转边界框，然后计算出3D空间中边界框的角点。
- en: This computation is based on the transformation that uses a matrix that is capable
    of transferring any point from the camera coordinate system to the Velodyne coordinate
    system. But, wait? We don’t have the camera to Velodyne matrix, do we? Yes, we
    need to calculate it first by taking the inverse of the *Tr_velo_to_cam* matrix,
    which is presented in the calibration files.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算是基于一种变换，它使用一个能够将任何点从相机坐标系统转换到Velodyne坐标系统的矩阵。但是，等一下？我们没有相机到Velodyne的矩阵，对吧？是的，我们需要首先通过取*Tr_velo_to_cam*矩阵的逆矩阵来计算它，这个矩阵在标定文件中给出。
- en: No worries, all this workflow is presented by these functions.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 别担心，所有这些工作流都由这些函数呈现。
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the end, we can easily see the G.T. objects and project them into the Velodyne
    point cloud coordinate system. Now let's visualize the output and then jump into
    the evaluation section!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以轻松地看到G.T.物体并将它们投影到Velodyne点云坐标系中。现在让我们可视化输出结果，然后进入评估部分！
- en: '![](../Images/fbaed431d6042557ca876c4ad2910dfd.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbaed431d6042557ca876c4ad2910dfd.png)'
- en: The projected G.T. object bounding boxes (Image taken from KITTI dataset [3])
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 投影后的G.T.物体边界框（图片来自KITTI数据集[3]）
- en: (I know the green bounding boxes can be a little hard to see, so I added arrows
    next to them in black.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （我知道绿色边框可能有点难以看清，所以我在它们旁边加上了黑色的箭头。）
- en: Evaluation Metrics
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标
- en: Now we have the predicted bounding boxes by our pipeline and G.T. object boxes!
    Then let's calculate some metrics to evaluate our pipeline. In order to perform
    the hyperparameter optimization that we talked about earlier, we must be able
    to continuously monitor our performance for each parameter group.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了通过我们的管道预测的边界框和G.T.物体框！接下来，我们来计算一些指标，评估我们的管道。为了执行我们之前讨论的超参数优化，我们必须能够持续监控每个参数组的性能。
- en: But before getting into the evaluation metric I need to mention two things.
    First of all, KITTI has different evaluation criteria for different objects. For
    example, while a 50% match between the labels produced for pedestrians and G.T.
    is sufficient, it is 70% for vehicles. Another issue is that while the pipeline
    we created performs object detection in a 360-degree environment, the KITTI G.T.
    labels only include the label values ​​of the objects in the viewing angle of
    the color cameras. Consequently, we can detect more bounding boxes than presented
    in G.T. label files. So what to do? Based on the concepts I will talk about here,
    you can reach the final result by carefully analyzing KITTI’s evaluation criteria.
    But for now, I will not do a more detailed analysis in this section for the continuation
    posts of this Medium blog post series.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 但在进入评估指标之前，我需要提到两件事。首先，KITTI对不同物体有不同的评估标准。例如，对于行人，标签与G.T.之间50%的匹配度就足够了，而对于车辆则是70%。另一个问题是，尽管我们创建的管道在360度环境中执行物体检测，但KITTI的G.T.标签仅包括在彩色相机视角内的物体标签值。因此，我们可以检测到比G.T.标签文件中呈现的更多边界框。那么该怎么办呢？基于我在这里要讲的概念，您可以通过仔细分析KITTI的评估标准来得出最终结果。但现在，我不会在这一部分做更详细的分析，而是会在这篇Medium博客系列的后续文章中进行。
- en: To evaluate the predicted bounding boxes and G.T. bounding boxes, we will be
    using the TP, FP, and FN metrics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估预测的边界框和G.T.边界框，我们将使用TP、FP和FN指标。
- en: TP represents the predicted boxes that match with G.T. boxes, FP stands for
    the predicted boxes that do NOT match with any G.T. boxes, and FN is the condition
    that there are no corresponding predicted bounding boxes for G.T. bounding boxes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: TP代表与真实边界框匹配的预测框，FP代表与任何真实边界框都不匹配的预测框，FN则表示没有与真实边界框对应的预测边界框的情况。
- en: In this context, of course, we need to find a tool to measure how a predicted
    bounding box and a G.T. bounding box match. The name of our tool is IOU, intersected
    over union.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们当然需要找到一个工具来衡量预测的边界框与真实边界框的匹配度。我们工具的名称是IOU，即交并比。
- en: You can easily reach out to the IOU and evaluation functions as follows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式轻松访问IOU和评估功能。
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let me finalize this section by giving predicted bounding boxes (RED) and G.T.
    bounding boxes (GREEN) over the point cloud.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我通过在点云上给出预测的边界框（红色）和真实边界框（绿色）来总结这一部分内容。
- en: '![](../Images/35cd707ba8068c92b4b7499567027efb.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35cd707ba8068c92b4b7499567027efb.png)'
- en: Predicted bounding boxes and G.T. bounding boxes are presented together on the
    point cloud (Image taken from KITTI dataset [3])
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的边界框和真实边界框一起显示在点云上（图像来源于KITTI数据集[3]）
- en: Conclusion
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Yeah, it’s a little bit long, but we are about to finish it. First, we have
    learned a couple of things about the KITTI 3D Object Detection Benchmark and some
    terminology about different topics, like camera coordinate systems and unsupervised
    learning, etc.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，虽然有点长，但我们快要完成了。首先，我们已经学到了关于KITTI 3D目标检测基准的一些知识以及一些与不同主题相关的术语，如相机坐标系统和无监督学习等。
- en: 'Now interested readers can extend this study by adding a grid search to find
    the best hyper-param elements. For example, the number of minimum points in segmentation,
    or maybe the # of iteration RANSAC or the voxel grid size in Voxel Downsampling
    operation, all could be possible improvement points.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有兴趣的读者可以通过添加网格搜索来扩展本研究，以找到最佳的超参数元素。例如，分割中的最小点数，或者可能是RANSAC的迭代次数，或者体素下采样操作中的体素网格大小，所有这些都是潜在的改进点。
- en: What’s next?
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 接下来是什么？
- en: 'The next part will be investigating object detection on ONLY Left Color Camera
    frames. This is another fundamental step of this series cause we will be fusing
    the Lidar Point Cloud and Color Camera frames in the last part of this blog series.
    Then we will be able to make a conclusion and answer this question: *“Does Sensor
    Fusion reduce the uncertainty and improve the performance in KITTI 3D Object Detection
    Benchmark?”*'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将研究仅在左侧彩色相机帧上的目标检测。这是本系列的另一个基础步骤，因为我们将在本博客系列的最后部分融合激光雷达点云和彩色相机帧。然后我们将能够得出结论并回答这个问题：*“传感器融合是否减少了不确定性并提高了KITTI
    3D目标检测基准的性能？”*
- en: '**Any comments, error fixes, or improvements are welcome!**'
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**欢迎任何评论、错误修复或改进！**'
- en: ''
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Thank you all and I wish you healthy days.***'
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***感谢大家，祝你们健康！***'
- en: '********************************************************************************************************************************************************'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '********************************************************************************************************************************************************'
- en: '***Github link***: [https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main/lidar_based_obstacle_detection](https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '***Github链接***: [https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main/lidar_based_obstacle_detection](https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main)'
- en: '**References**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] — [https://www.cvlibs.net/datasets/kitti/](https://www.cvlibs.net/datasets/kitti/)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [https://www.cvlibs.net/datasets/kitti/](https://www.cvlibs.net/datasets/kitti/)'
- en: '[2] — [https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d)'
- en: '[3] — Geiger, Andreas, et al. “Vision meets robotics: The kitti dataset.” *The
    International Journal of Robotics Research* 32.11 (2013): 1231–1237.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — Geiger, Andreas, 等. “Vision meets robotics: The kitti dataset.” *国际机器人研究杂志*
    32.11 (2013): 1231–1237.'
- en: Disclaimer
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: The images used in this blog series are taken from the KITTI dataset for education
    and research purposes. If you want to use it for similar purposes, you must go
    to the relevant website, approve the intended use there, and use the citations
    defined by the benchmark creators as follows.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列博客中使用的图像来自KITTI数据集，旨在用于教育和研究。如果你希望将其用于类似的目的，必须访问相关网站，批准在那里规定的用途，并按照基准创建者定义的引用格式使用。
- en: 'For the **stereo 2012**, **flow 2012**, **odometry**, **object detection,**
    or **tracking benchmarks**, please cite:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**立体视觉2012**，**光流2012**，**里程计**，**物体检测**，或**跟踪基准**，请引用：
- en: '@inproceedings{[Geiger2012CVPR](https://www.cvlibs.net/publications/Geiger2012CVPR.pdf),'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '@inproceedings{[Geiger2012CVPR](https://www.cvlibs.net/publications/Geiger2012CVPR.pdf),'
- en: author = {[Andreas Geiger](https://www.cvlibs.net/) and [Philip Lenz](http://www.mrt.kit.edu/mitarbeiter_lenz.php)
    and [Raquel Urtasun](http://ttic.uchicago.edu/~rurtasun)},
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: author = {[Andreas Geiger](https://www.cvlibs.net/) 和 [Philip Lenz](http://www.mrt.kit.edu/mitarbeiter_lenz.php)
    和 [Raquel Urtasun](http://ttic.uchicago.edu/~rurtasun)},
- en: title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: title = {我们准备好迎接自动驾驶了吗？KITTI视觉基准套件},
- en: booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: booktitle = {计算机视觉与模式识别会议（CVPR）},
- en: year = {2012}
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: year = {2012}
- en: '}'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'For the **raw dataset**, please cite:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**原始数据集**，请引用：
- en: '@article{[Geiger2013IJRR](https://www.cvlibs.net/publications/Geiger2013IJRR.pdf),'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '@article{[Geiger2013IJRR](https://www.cvlibs.net/publications/Geiger2013IJRR.pdf),'
- en: author = {[Andreas Geiger](https://www.cvlibs.net/) and [Philip Lenz](http://www.mrt.kit.edu/mitarbeiter_lenz.php)
    and [Christoph Stiller](http://www.mrt.kit.edu/mitarbeiter_stiller.php) and [Raquel
    Urtasun](http://ttic.uchicago.edu/~rurtasun)},
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: author = {[Andreas Geiger](https://www.cvlibs.net/) 和 [Philip Lenz](http://www.mrt.kit.edu/mitarbeiter_lenz.php)
    和 [Christoph Stiller](http://www.mrt.kit.edu/mitarbeiter_stiller.php) 和 [Raquel
    Urtasun](http://ttic.uchicago.edu/~rurtasun)},
- en: 'title = {Vision meets Robotics: The KITTI Dataset},'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: title = {视觉与机器人学相遇：KITTI数据集},
- en: journal = {International Journal of Robotics Research (IJRR)},
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: journal = {国际机器人研究杂志（IJRR）},
- en: year = {2013}
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: year = {2013}
- en: '}'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'For the **road benchmark**, please cite:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**道路基准**，请引用：
- en: '@inproceedings{[Fritsch2013ITSC](https://www.cvlibs.net/publications/Fritsch2013ITSC.pdf),'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '@inproceedings{[Fritsch2013ITSC](https://www.cvlibs.net/publications/Fritsch2013ITSC.pdf),'
- en: author = {Jannik Fritsch and Tobias Kuehnl and [Andreas Geiger](https://www.cvlibs.net/)},
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: author = {Jannik Fritsch 和 Tobias Kuehnl 和 [Andreas Geiger](https://www.cvlibs.net/)},
- en: title = {A New Performance Measure and Evaluation Benchmark for Road Detection
    Algorithms},
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: title = {一种新的性能度量和道路检测算法评估基准},
- en: booktitle = {International Conference on Intelligent Transportation Systems
    (ITSC)},
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: booktitle = {国际智能交通系统会议（ITSC）},
- en: year = {2013}
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: year = {2013}
- en: '}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'For the **stereo 2015**, **flow 2015,** and **scene flow 2015 benchmarks**,
    please cite:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**立体视觉2015**，**光流2015**，和**场景流2015基准**，请引用：
- en: '@inproceedings{[Menze2015CVPR](https://www.cvlibs.net/publications/Menze2015CVPR.pdf),'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '@inproceedings{[Menze2015CVPR](https://www.cvlibs.net/publications/Menze2015CVPR.pdf),'
- en: author = {[Moritz Menze](http://www.ipi.uni-hannover.de/tmm.html) and [Andreas
    Geiger](https://www.cvlibs.net/)},
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: author = {[Moritz Menze](http://www.ipi.uni-hannover.de/tmm.html) 和 [Andreas
    Geiger](https://www.cvlibs.net/)},
- en: title = {Object Scene Flow for Autonomous Vehicles},
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: title = {自动驾驶车辆的物体场景流},
- en: booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: booktitle = {计算机视觉与模式识别会议（CVPR）},
- en: year = {2015}
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: year = {2015}
- en: '}'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
