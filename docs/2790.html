<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Increasing Transformer Model Efficiency Through Attention Layer Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Increasing Transformer Model Efficiency Through Attention Layer Optimization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18">https://towardsdatascience.com/increasing-transformer-model-efficiency-through-attention-layer-optimization-fefa6f87b1d6?source=collection_archive---------2-----------------------#2024-11-18</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1d58" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How paying “better” attention can drive ML cost savings</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--fefa6f87b1d6--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fefa6f87b1d6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 18, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/392d04054c201f7813635598c7dd5502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lrOei3pkOtgNhjqF"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@amseaman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrew Seaman</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2e6d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Introduced in the landmark 2017 paper <a class="af nb" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"><em class="ny">“Attention Is All You Need”</em></a> (Vaswani et al., 2017), the Transformer architecture is widely regarded as one of the most influential scientific breakthroughs of the past decade. At the core of the Transformer is the attention mechanism, a novel approach that enables AI models to comprehend complex structures by focusing on different parts of input sequences based on the task at hand. Originally demonstrated in the world of natural language processing, the success of the Transformer architecture has quickly spread to many other domains, including speech recognition, scene understanding, reinforcement learning, protein structure prediction, and more. However, attention layers are highly resource-intensive, and as these layers become the standard across increasingly large models, the costs associated with their training and deployment have surged. This has created an urgent need for strategies that reduce the computational cost of this core layer so as to increase the efficiency and scalability of Transformer-based AI models.</p><p id="3ab5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post, we will explore several tools for optimizing attention in <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a>. Our focus will be on methods that maintain the accuracy of the attention layer. These will include <a class="af nb" href="https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch SDPA</a>, <a class="af nb" href="https://pytorch.org/blog/flashattention-3/" rel="noopener ugc nofollow" target="_blank">FlashAttention</a>, <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine" rel="noopener ugc nofollow" target="_blank">TransformerEngine</a> Attention, <a class="af nb" href="https://pytorch.org/blog/flexattention/" rel="noopener ugc nofollow" target="_blank">FlexAttention</a>, and <a class="af nb" href="https://github.com/facebookresearch/xformers" rel="noopener ugc nofollow" target="_blank">xFormer</a> attention. Other methods that reduce the computational cost via approximation of the attention calculation (e.g., <a class="af nb" href="https://www.deepspeed.ai/tutorials/sparse-attention/" rel="noopener ugc nofollow" target="_blank">DeepSpeed’s Sparse Attention</a>, <a class="af nb" href="https://github.com/allenai/longformer" rel="noopener ugc nofollow" target="_blank">Longformer</a>, <a class="af nb" href="https://arxiv.org/abs/2006.04768" rel="noopener ugc nofollow" target="_blank">Linformer</a>, and more) will not be considered. Additionally, we will not discuss general optimization techniques that, while beneficial to attention performance, are not specific to the attention computation itself (e.g., <a class="af nb" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">FP8 training</a>, <a class="af nb" href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/" rel="noopener ugc nofollow" target="_blank">model sharding</a>, and <a class="af nb" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">more</a>).</p><p id="d7d9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Importantly, attention optimization is an active area of research with new methods coming out on a pretty regular basis. Our goal is to increase your awareness of some of the existing solutions and provide you with a foundation for further exploration and experimentation. The code we will share below is intended for demonstrative purposes only — we make no claims regarding its accuracy, optimality, or robustness. Please do not interpret our mention of any platforms, libraries, or optimization techniques as an endorsement for their use. The best options for you will depend greatly on the specifics of your own use-case.</p><p id="3445" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Many thanks to <a class="af nb" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a> for his contributions to this post.</p><h1 id="a42e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Toy Model</h1><p id="e825" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">To facilitate our discussion, we build a <a class="af nb" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT)-backed classification model using the popular <a class="af nb" href="https://pypi.org/project/timm/" rel="noopener ugc nofollow" target="_blank">timm</a> Python package (version 0.9.7). We will use this model to illustrate the performance impact of various attention kernels.</p><p id="59f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We start by defining a simplified Transformer block that allows for programming the attention function by passing it into its constructor. Since attention implementations assume specific input tensor formats, we also include an option for controlling the format, ensuring compatibility with the attention kernel of our choosing.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="0925" class="pe oa fq pb b bg pf pg l ph pi"># general imports<br/>import os, time, functools<br/><br/># torch imports<br/>import torch<br/>from torch.utils.data import Dataset, DataLoader<br/>import torch.nn as nn<br/><br/># timm imports<br/>from timm.models.vision_transformer import VisionTransformer<br/>from timm.layers import Mlp<br/><br/>IMG_SIZE = 224<br/>BATCH_SIZE = 128<br/><br/># Define ViT settings<br/>NUM_HEADS = 16<br/>HEAD_DIM = 64<br/>DEPTH = 24<br/>PATCH_SIZE = 16<br/>SEQ_LEN = (IMG_SIZE // PATCH_SIZE)**2 # 196<br/><br/>class MyAttentionBlock(nn.Module):<br/>    def __init__(<br/>            self,<br/>            attn_fn,<br/>            format = None,<br/>            dim: int = 768,<br/>            num_heads: int = 12,<br/>            **kwargs<br/>    ) -&gt; None:<br/>        super().__init__()<br/>        self.attn_fn = attn_fn<br/>        self.num_heads = num_heads<br/>        self.head_dim = dim // num_heads<br/>        self.norm1 = nn.LayerNorm(dim)<br/>        self.norm2 = nn.LayerNorm(dim)<br/>        self.qkv = nn.Linear(dim, dim * 3, bias=False)<br/>        self.proj = nn.Linear(dim, dim)<br/>        self.mlp = Mlp(<br/>            in_features=dim,<br/>            hidden_features=dim * 4,<br/>        )<br/>        permute = (2, 0, 3, 1, 4)<br/>        self.permute_attn = functools.partial(torch.transpose,dim0=1,dim1=2)<br/><br/>        if format == 'bshd':<br/>            permute = (2, 0, 1, 3, 4)<br/>            self.permute_attn = nn.Identity()<br/>        self.permute_qkv = functools.partial(torch.permute,dims=permute)<br/><br/>    def forward(self, x_in: torch.Tensor) -&gt; torch.Tensor:<br/>        x = self.norm1(x_in)<br/>        B, N, C = x.shape<br/>        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)<br/>        # permute tensor based on the specified format<br/>        qkv = self.permute_qkv(qkv)<br/>        q, k, v = qkv.unbind(0)<br/>        # use the attention function specified by the user<br/>        x = self.attn_fn(q, k, v)<br/>        # permute output according to the specified format<br/>        x = self.permute_attn(x).reshape(B, N, C)<br/>        x = self.proj(x)<br/>        x = x + x_in<br/>        x = x + self.mlp(self.norm2(x))<br/>        return x</span></pre><p id="9a03" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We define a randomly generated dataset which we will use to feed to our model during training.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="ab5b" class="pe oa fq pb b bg pf pg l ph pi"># Use random data<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, IMG_SIZE, IMG_SIZE],<br/>                                 dtype=torch.float32)<br/>        label = torch.tensor(data=index % 1000, dtype=torch.int64)<br/>        return rand_image, label </span></pre><p id="3f06" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we define our ViT training function. While our example focuses on demonstrating a <em class="ny">training </em>workload, it is crucial to emphasize that optimizing the attention layer is equally, if not more, important during model <em class="ny">inference</em>.</p><p id="af9f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The training function we define accepts the customized Transformer block and a flag that controls the use of <a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a>.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="0e77" class="pe oa fq pb b bg pf pg l ph pi">def train_fn(block_fn, compile):<br/>    torch.random.manual_seed(0)<br/>    device = torch.device("cuda:0")<br/>    torch.set_float32_matmul_precision("high")<br/><br/>    # Create dataset and dataloader<br/>    train_set = FakeDataset()<br/>    train_loader = DataLoader(<br/>        train_set, batch_size=BATCH_SIZE,<br/>        num_workers=12, pin_memory=True, drop_last=True)<br/><br/>    model = VisionTransformer(<br/>       img_size=IMG_SIZE,<br/>       patch_size=PATCH_SIZE,<br/>       embed_dim=NUM_HEADS*HEAD_DIM,<br/>       depth=DEPTH,<br/>       num_heads=NUM_HEADS,<br/>       class_token=False,<br/>       global_pool="avg",<br/>       block_fn=block_fn<br/>    ).to(device)<br/><br/>    if compile:<br/>        model = torch.compile(model)<br/><br/>    # Define loss and optimizer<br/>    criterion = torch.nn.CrossEntropyLoss()<br/>    optimizer = torch.optim.SGD(model.parameters())<br/><br/>    model.train()<br/><br/>    t0 = time.perf_counter()<br/>    summ = 0<br/>    count = 0<br/>    for step, data in enumerate(train_loader):<br/>        # Copy data to GPU<br/>        inputs = data[0].to(device=device, non_blocking=True)<br/>        label = data[1].to(device=device, non_blocking=True)<br/>        with torch.amp.autocast('cuda', enabled=True, dtype=torch.bfloat16):<br/>            outputs = model(inputs)<br/>            loss = criterion(outputs, label)<br/>        optimizer.zero_grad(set_to_none=True)<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        # Capture step time<br/>        batch_time = time.perf_counter() - t0<br/>        if step &gt; 20:  # Skip first steps<br/>            summ += batch_time<br/>            count += 1<br/>        t0 = time.perf_counter()<br/>        if step &gt; 100:<br/>            break<br/>    print(f'average step time: {summ / count}')<br/><br/># define compiled and uncompiled variants of our train function<br/>train = functools.partial(train_fn, compile=False)<br/>train_compile = functools.partial(train_fn, compile=True)</span></pre><p id="7509" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the code block below we define a PyTorch-native attention function and use it to train our ViT model:</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="2c9e" class="pe oa fq pb b bg pf pg l ph pi">def attn_fn(q, k, v):<br/>    scale = HEAD_DIM ** -0.5<br/>    q = q * scale<br/>    attn = q @ k.transpose(-2, -1)<br/>    attn = attn.softmax(dim=-1)<br/>    x = attn @ v<br/>    return x<br/><br/>block_fn = functools.partial(MyAttentionBlock, attn_fn=attn_fn)<br/><br/>print('Default Attention')<br/>train(block_fn)<br/>print('Compiled Default Attention')<br/>train_compile(block_fn)</span></pre><p id="c233" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We ran this on an <a class="af nb" href="https://www.nvidia.com/en-eu/data-center/h100/" rel="noopener ugc nofollow" target="_blank">NVIDIA H100</a> with <a class="af nb" href="https://developer.nvidia.com/cuda-toolkit" rel="noopener ugc nofollow" target="_blank">CUDA 12.4</a> and <a class="af nb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">PyTorch</a> 2.5.1. The uncompiled variant resulted in an average step time of 370 milliseconds (ms), while the compiled variant improved to 242 ms. We will use these results as a baseline for comparison as we consider alternative solutions for performing the attention computation.</p><h1 id="99e3" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">PyTorch SDPA</h1><p id="a3f6" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">One of the easiest ways to boost the performance of our attention layers in PyTorch is to use the <a class="af nb" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" rel="noopener ugc nofollow" target="_blank">scaled_dot_product_attention</a> (SDPA) function. Currently in beta, PyTorch SDPA consolidates multiple kernel-level optimizations and dynamically selects the most efficient one based on the input's properties. Supported backends (as of now) include: <a class="af nb" href="https://arxiv.org/abs/2307.08691" rel="noopener ugc nofollow" target="_blank">FlashAttention-2</a>, <a class="af nb" href="https://github.com/facebookresearch/xformers" rel="noopener ugc nofollow" target="_blank">Memory-Efficient Attention</a>, a C++-based Math Attention, and <a class="af nb" href="https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa" rel="noopener ugc nofollow" target="_blank">CuDNN</a>. These backends fuse together high-level operations while employing GPU-level optimizations for increasing compute efficiency and memory utilization.</p><p id="deb1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SDPA is continuously evolving, with new and improved backend implementations being introduced regularly. Staying up to date with the latest PyTorch releases is key to leveraging the most recent performance improvements. For example, <a class="af nb" href="https://pytorch.org/blog/pytorch2-5/" rel="noopener ugc nofollow" target="_blank">PyTorch 2.5</a> introduced an updated <a class="af nb" href="https://pytorch.org/blog/pytorch2-5/#beta-cudnn-backend-for-sdpa" rel="noopener ugc nofollow" target="_blank">CuDNN backend</a> featuring a specialized <a class="af nb" href="https://developer.nvidia.com/blog/accelerating-transformers-with-nvidia-cudnn-9/" rel="noopener ugc nofollow" target="_blank">SDPA primitive</a> specifically tailored for training on <a class="af nb" href="https://www.nvidia.com/en-us/data-center/technologies/hopper-architecture/" rel="noopener ugc nofollow" target="_blank">NVIDIA Hopper architecture</a> GPUs.</p><p id="1f4b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the code block below, we iterate through the list of supported backends and assess the runtime performance of training with each one. We use a helper function, <em class="ny">set_sdpa_backend</em>, for programming the SDPA backend:</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="4483" class="pe oa fq pb b bg pf pg l ph pi">from torch.nn.functional import scaled_dot_product_attention as sdpa<br/><br/>def set_sdpa_backend(backend):<br/>    torch.backends.cuda.enable_flash_sdp(False)<br/>    torch.backends.cuda.enable_mem_efficient_sdp(False)<br/>    torch.backends.cuda.enable_math_sdp(False)<br/>    torch.backends.cuda.enable_cudnn_sdp(False)<br/><br/>    if backend in ['flash_sdp','all']:<br/>        torch.backends.cuda.enable_flash_sdp(True)<br/>    if backend in ['mem_efficient_sdp','all']:<br/>        torch.backends.cuda.enable_mem_efficient_sdp(True)<br/>    if backend in ['math_sdp','all']:<br/>        torch.backends.cuda.enable_math_sdp(True)<br/>    if backend in ['cudnn_sdp','all']:<br/>        torch.backends.cuda.enable_cudnn_sdp(True)<br/><br/>for backend in ['flash_sdp', 'mem_efficient_sdp',<br/>                'math_sdp', 'cudnn_sdp']:<br/>    set_sdpa_backend(backend)<br/>    block_fn = functools.partial(MyAttentionBlock,<br/>                                 attn_fn=sdpa)<br/><br/>    print(f'PyTorch SDPA - {backend}')<br/>    train(block_fn)<br/>    print(f'Compiled PyTorch SDPA - {backend}')<br/>    train_compile(block_fn)</span></pre><p id="fcb6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We summarize our interim results in the table below</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/7f98ef38f1ec30988ed4b79706748b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*Bkx78UkBFFkyA0zBjl-Qwg.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Step times for various attention functions (lower is better) — by Author</figcaption></figure><p id="04d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the choice of SDPA backend has a noticeable impact on performance when running in eager mode, the optimizations performed by <a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">model compilation</a> appear to overshadow the differences between the attention kernels. Once again, we caution against deriving any conclusions from these results as the performance impact of different attention functions can vary significantly depending on the specific model and use case.</p><h1 id="c3fc" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Third-Party Attention Kernels</h1><p id="ce66" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">While PyTorch SDPA is a great place to start, using third-party attention kernels can help accelerate your ML workloads further. These alternatives often come with added flexibility, offering a wider range of configuration options for attention. Some may also include optimizations tailored for specific hardware accelerators or newer GPU architectures.</p><p id="04c4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this section, we will explore some of the third-party attention kernels available and evaluate their potential impact on runtime performance.</p><h2 id="5a87" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">FlashAttention-3</h2><p id="7850" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">While Pytorch SDPA supports a <a class="af nb" href="https://arxiv.org/abs/2307.08691" rel="noopener ugc nofollow" target="_blank">FlashAttention</a> backend, more advanced FlashAttention implementations can be found in the <a class="af nb" href="https://pypi.org/project/flash-attn/" rel="noopener ugc nofollow" target="_blank">flash-attn</a> library. Here we will explore the <a class="af nb" href="https://pytorch.org/blog/flashattention-3/" rel="noopener ugc nofollow" target="_blank">FlashAttention-3</a> beta release which boasts a speed of up to 2x compared to FlashAttention-2. Given the early stage in its development, FlashAttention-3 can only be installed directly from the <a class="af nb" href="https://github.com/HazyResearch/flash-attention" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> and its use is limited to certain head dimensions. Additionally, it does not yet support model compilation. In the following code block, we configure our transformer block to use flash-attn-3 while setting the attention input format to “bshd” (batch, sequence, head, depth) to meet the expectations of the library.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="d48d" class="pe oa fq pb b bg pf pg l ph pi"># flash attention 3<br/>from flash_attn_interface import flash_attn_func as fa3<br/>attn_fn = lambda q,k,v: fa3(q,k,v)[0]<br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=attn_fn,<br/>                             format='bshd')<br/><br/>print(f'Flash Attention 3')<br/>train(block_fn)</span></pre><p id="d4c3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The resultant step time was 240 ms, making it 5% faster than the SDPA flash-attn.</p><h2 id="6385" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Transformer Engine</h2><p id="d428" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><a class="af nb" href="https://github.com/NVIDIA/TransformerEngine" rel="noopener ugc nofollow" target="_blank">Transformer Engine</a> (TE) is a specialized library designed to accelerate Transformer models on NVIDIA GPUs. TE is updated regularly with optimizations that leverage the capabilities of the latest NVIDIA hardware and software offerings, giving users access to specialized kernels long before they are integrated into general-purpose frameworks such as PyTorch.</p><p id="82cf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the code block below we use <a class="af nb" href="https://github.com/NVIDIA/TransformerEngine/blob/main/transformer_engine/pytorch/attention.py#L7271" rel="noopener ugc nofollow" target="_blank">DotProductAttention</a> from <a class="af nb" href="https://pypi.org/project/transformer-engine/" rel="noopener ugc nofollow" target="_blank">TE version 1.11.0</a>. Similar to PyTorch SDPA, TE supports a number of backends which are controlled via environment variables. Here we demonstrate the use of the <em class="ny">NVTE_FUSED_ATTN</em> backend.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="e722" class="pe oa fq pb b bg pf pg l ph pi">def set_te_backend(backend):<br/>    # must be applied before first use of<br/>    # transformer_engine.pytorch.attention<br/>    os.environ["NVTE_FLASH_ATTN"] = '0'<br/>    os.environ["NVTE_FUSED_ATTN"] = '0'<br/>    os.environ["NVTE_UNFUSED_ATTN"] = '0'<br/>    if backend == 'flash':<br/>        os.environ["NVTE_FLASH_ATTN"] = '1'<br/>    if backend == 'fused':<br/>        os.environ["NVTE_FUSED_ATTN"] = '1'<br/>    if backend == 'unfused':<br/>        os.environ["NVTE_UNFUSED_ATTN"] = '1'<br/><br/>from transformer_engine.pytorch.attention import DotProductAttention<br/>set_te_backend('fused')<br/>attn_fn = DotProductAttention(NUM_HEADS, HEAD_DIM, NUM_HEADS,<br/>                              qkv_format='bshd',<br/>                              # disable masking (default is causal mask)<br/>                              attn_mask_type='no_mask')<br/><br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=attn_fn,<br/>                             format='bshd')<br/><br/>print(f'Transformer Engine Attention')<br/>train(block_fn)<br/>print(f'Compiled Transformer Engine Attention')<br/>train_compile(block_fn)</span></pre><p id="a281" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">TE attention resulted in average step times of 243 ms and 204 ms for the eager and compiled model variants, correspondingly.</p><h2 id="87d1" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">XFormer Attention</h2><p id="ae23" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Underlying the memory-efficient backend of PyTorch SDPA is an attention kernel provided by the <a class="af nb" href="https://github.com/facebookresearch/xformers/tree/main" rel="noopener ugc nofollow" target="_blank">xFormers</a> library. Once again, we can go to the source to benefit from the latest kernel optimizations and from the full set of API capabilities. In the following code block we use the <a class="af nb" href="https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.memory_efficient_attention" rel="noopener ugc nofollow" target="_blank">memory_efficient_attention</a> operator from <a class="af nb" href="https://pypi.org/project/xformers/" rel="noopener ugc nofollow" target="_blank">xFormers version 0.0.28</a>.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="821a" class="pe oa fq pb b bg pf pg l ph pi"># xformer memory efficient attention<br/>from xformers.ops import memory_efficient_attention as mea<br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=mea,<br/>                             format='bshd')<br/><br/>print(f'xFormer Attention ')<br/>train(block_fn)<br/>print(f'Compiled xFormer Attention ')<br/>train_compile(block_fn)</span></pre><p id="aa3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This eager model variant resulted in an average step time of 246 ms, making it 10.5% faster than the SDPA memory efficient kernel. The compiled variant resulted in a step time of 203 ms.</p><h2 id="90d5" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Results</h2><p id="6322" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The table below summarizes our experiments:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pj"><img src="../Images/8e3051e670033c5e99bcdf855549632c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*u4_JT8VmYA4DoR61pzxhhQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Step times for various attention functions (lower is better) — by Author</figcaption></figure><p id="cbc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The winner for the eager model was flash-attn-3 with an average step time that is 54% faster than our baseline model. This translates to a similar 54% reduction in training costs. In compiled mode, the performance across the optimized kernels was more or less equal, with the fastest implementations achieving 202 ms, representing a 20% improvement compared to the baseline experiment.</p><p id="d8b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned above, the precise impact savings is greatly dependent on the model definition. To assess this variability, we reran the experiments using modified settings that increased the attention sequence length to 3136 tokens.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="8298" class="pe oa fq pb b bg pf pg l ph pi">IMG_SIZE = 224<br/>BATCH_SIZE = 8<br/><br/># Define ViT settings<br/>NUM_HEADS = 12<br/>HEAD_DIM = 64<br/>DEPTH = 6<br/>PATCH_SIZE = 4<br/>SEQ_LEN = (IMG_SIZE // PATCH_SIZE)**2 # 3136</span></pre><p id="2a13" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results are summarized in the table below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qb"><img src="../Images/54ce4ff619bd4657bb51114e22be1772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*-oDY4_r2DLMDiuJP64RL2w.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Results for large seqlen (lower is better) — by Author</figcaption></figure><p id="7836" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our immediate observation is that when the sequence length is greater the performance impact of the attention kernels is far more pronounced. Once again, flash-attn-3 came out in front for the eager execution mode — this time with a ~5x increase in performance compared to the PyTorch-native function. For the compiled model we see that the TE kernel broke away from the pack with an overall best step-time of 53 ms.</p><h1 id="5b18" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Customizing Attention with FlexAttention</h1><p id="fe28" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Thus far, we’ve focused on the standard attention function. However, sometimes we may want to use a variant of the typical attention computation in which we either mask out some of the values of intermediate tensors or apply some operation on them. These types of changes may interfere with our ability to use the optimized attention blocks we covered above. In this section we discuss some of the ways to address this:</p><p id="c31b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Leverage Advanced Kernel APIs</strong><br/>Many optimized attention kernels provide extensive APIs with controls for customizing the attention computation. Before implementing a new solution, explore these APIs to determine if they already support your required functionality.</p><p id="4f03" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Implement a custom kernel:<br/></strong>If the existing APIs do not meet your needs, you could consider creating your own custom attention implementation. In previous posts (e.g.,<a class="af nb" rel="noopener" target="_blank" href="/unleashing-the-power-of-triton-mastering-gpu-kernel-optimization-in-python-160a3f52701e"> here</a>) we discussed some of the pros and cons of custom kernel development. Achieving optimal performance can be extremely difficult. If you do go down this path, one approach might be to start with an existing (optimal) kernel and apply minimal changes to integrate the desired change.</p><p id="28a0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Use FlexAttention:<br/></strong>A recent addition to PyTorch, <a class="af nb" href="https://pytorch.org/blog/flexattention/" rel="noopener ugc nofollow" target="_blank">FlexAttention</a> empowers users to implement a wide variety of attention variants without needing to compromise on performance. Denoting the result of the dot product of the query and key tokens by <em class="ny">score</em>, <a class="af nb" href="https://github.com/pytorch/pytorch/blob/v2.5.1/torch/nn/attention/flex_attention.py#L927" rel="noopener ugc nofollow" target="_blank">flex_attention</a> allows for programming either a <a class="af nb" href="https://pytorch.org/blog/flexattention/#score-mod-examples" rel="noopener ugc nofollow" target="_blank"><em class="ny">score_mod</em> </a>function or a <a class="af nb" href="https://pytorch.org/blog/flexattention/#mask-mods" rel="noopener ugc nofollow" target="_blank"><em class="ny">block_mask</em></a> mask that is automatically applied to the <em class="ny">score </em>tensor. See the <a class="af nb" href="https://pytorch.org/blog/flexattention/" rel="noopener ugc nofollow" target="_blank">documentation</a> as well as the accompanying <a class="af nb" href="https://github.com/pytorch-labs/attention-gym" rel="noopener ugc nofollow" target="_blank">attention-gym</a> repository for examples of the types of operations that the API enables.</p><p id="0216" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://pytorch.org/blog/flexattention/" rel="noopener ugc nofollow" target="_blank">FlexAttention</a> works by <a class="af nb" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">compiling</a> the <em class="ny">score_mod</em> operator into the attention operator, thereby creating a single fused kernel. It also leverages the sparsity of <em class="ny">block_masks</em> to avoid unnecessary computations. The <a class="af nb" href="https://pytorch.org/blog/flexattention/#performance" rel="noopener ugc nofollow" target="_blank">benchmarks</a> reported in the <a class="af nb" href="https://pytorch.org/blog/flexattention/" rel="noopener ugc nofollow" target="_blank">FlexAttention</a> documentation show considerable performance gains for a variety of use cases.</p><p id="3880" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s see both the <em class="ny">score_mod </em>and <em class="ny">block_mask </em>in action.</p><h2 id="24ab" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Score Mod Example — Soft-Capping with Tanh</h2><p id="f910" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Soft-capping is a common technique used to control the logit sizes (e.g., see <a class="af nb" href="https://arxiv.org/pdf/1611.09940" rel="noopener ugc nofollow" target="_blank">here</a>). The following code block extends our PyTorch-native attention kernel with soft-capping:</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="697d" class="pe oa fq pb b bg pf pg l ph pi">def softcap_attn(q, k, v):<br/>    scale = HEAD_DIM ** -0.5<br/>    q = q * scale<br/>    attn = q @ k.transpose(-2, -1)<br/>    # apply soft-capping<br/>    attn = 30 * torch.tanh(attn/30)<br/>    attn = attn.softmax(dim=-1)<br/>    x = attn @ v<br/>    return x</span></pre><p id="4ac6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the code block below we train our model, first with our PyTorch-native kernel, and then with the optimized Flex Attention API. These experiments were run with the 3136-length sequence settings.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="d3cd" class="pe oa fq pb b bg pf pg l ph pi"># flex attention imports<br/>from torch.nn.attention.flex_attention import (<br/>    create_block_mask,<br/>    create_mask,<br/>    flex_attention<br/>)<br/>compiled_flex = torch.compile(flex_attention)<br/><br/># score_mod definition<br/>def tanh_softcap(score, b, h, q_idx, kv_idx):<br/>    return 30 * torch.tanh(score/30)<br/><br/><br/>block_fn = functools.partial(MyAttentionBlock, attn_fn=softcap_attn)<br/><br/>print(f'Attention with Softcap')<br/>train(block_fn)<br/>print(f'Compiled Attention with Softcap')<br/>train_compile(block_fn)<br/><br/>flex_fn = functools.partial(flex_attention, score_mod=tanh_softcap)<br/>compiled_flex_fn = functools.partial(compiled_flex, score_mod=tanh_softcap)<br/><br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=flex_fn)<br/>compiled_block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=compiled_flex_fn)<br/><br/>print(f'Flex Attention with Softcap')<br/>train(compiled_block_fn)<br/>print(f'Compiled Flex Attention with Softcap')<br/>train_compile(block_fn)</span></pre><p id="f9e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results of the experiments are captured in the table below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qb"><img src="../Images/de916d94926beddd350137bca752fa5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*bjgirovhLFp96g7wQDopdw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Soft-cap step time results (lower is better) — by Author</figcaption></figure><p id="7216" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The impact of the Flash Attention kernel is clearly evident, delivering performance boosts of approximately 3.5x in eager mode and 1.5x in compiled mode.</p><h2 id="d852" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Mask Mod Example — Neighborhood Masking</h2><p id="730e" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">We assess the <em class="ny">mask_mod</em> functionality by applying a sparse mask to our attention <em class="ny">score</em>. Recall that each token in our sequence represents a patch in our 2D input image. We modify our kernel so that each token attends only to other tokens that are within a 5x5 window in the corresponding 2-D token array.</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="a168" class="pe oa fq pb b bg pf pg l ph pi"># convert the token id to a 2d index<br/>def seq_indx_to_2d(idx):<br/>    n_row_patches = IMG_SIZE // PATCH_SIZE<br/>    r_ind = idx // n_row_patches<br/>    c_ind = idx % n_row_patches<br/>    return r_ind, c_ind<br/><br/># only attend to tokens in a 5x5 surrounding window in our 2D token array<br/>def mask_mod(b, h, q_idx, kv_idx):<br/>    q_r, q_c = seq_indx_to_2d(q_idx)<br/>    kv_r, kv_c = seq_indx_to_2d(kv_idx)<br/>    return torch.logical_and(torch.abs(q_r-kv_r)&lt;5, torch.abs(q_c-kv_c)&lt;5)</span></pre><p id="7dd3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As a baseline for our experiment, we use PyTorch SDPA which includes support for passing in an attention mask. The following block includes the masked SDPA experiment followed by the Flex Attention implementation:</p><pre class="ml mm mn mo mp pa pb pc bp pd bb bk"><span id="54f9" class="pe oa fq pb b bg pf pg l ph pi"># materialize the mask to use in SDPA<br/>mask = create_mask(mask_mod, 1, 1, SEQ_LEN, SEQ_LEN, device='cuda')<br/><br/>set_sdpa_backend('all')<br/>masked_sdpa = functools.partial(sdpa, attn_mask=mask)<br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=masked_sdpa)<br/>print(f'Masked SDPA Attention')<br/>train(block_fn)<br/>print(f'Compiled Masked SDPA Attention')<br/>train_compile(block_fn)<br/><br/>block_mask = create_block_mask(mask_mod, None, None, SEQ_LEN, SEQ_LEN)<br/>flex_fn = functools.partial(flex_attention, block_mask=block_mask)<br/>compiled_flex_fn = functools.partial(compiled_flex, block_mask=block_mask)<br/><br/>block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=flex_fn)<br/>compiled_block_fn = functools.partial(MyAttentionBlock,<br/>                             attn_fn=compiled_flex_fn)<br/><br/>print(f'Masked Flex Attention')<br/>train(compiled_block_fn)<br/>print(f'Compiled Masked Flex Attention')<br/>train_compile(block_fn)</span></pre><p id="cbf0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results of the experiments are captured below:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj qb"><img src="../Images/3e73e0cff7eb528d567982bc709f8bb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*Xk-LqCxWIa4YhDGMH-y7hw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Masked attention step time results (lower is better) — by Author</figcaption></figure><p id="c9dd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once again, Flex Attention offers a considerable performance boost, amounting to 2.19x in eager mode and 2.59x in compiled mode.</p><h2 id="1d10" class="pk oa fq bf ob pl pm pn oe po pp pq oh nl pr ps pt np pu pv pw nt px py pz qa bk">Flex Attention Limitations</h2><p id="4e5a" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Although we have succeeded in demonstrating the power and potential of Flex Attention, there are a few limitations that should be noted:</p><ol class=""><li id="dac6" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qc qd qe bk"><strong class="ne fr">Limited Scope of Modifications</strong>: With Flex Attention you can (as of the time of this writing) only modify the attention score (the result of the dot product between the query and key tokens). It does not support changes at other stages of the attention computation.</li><li id="8017" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk"><strong class="ne fr">Dependency on torch.compile:</strong> Given the reliance on torch.compile, care must be taken to avoid excessive recompilations which could greatly degrade runtime performance. For instance, while the support for <a class="af nb" href="https://pytorch.org/blog/flexattention/#document-maskingjagged-sequences" rel="noopener ugc nofollow" target="_blank">Document Masking</a> is very compelling, it will perform as expected only if the sum of the lengths of all of the documents remains fixed.</li><li id="1992" class="nc nd fq ne b go qf ng nh gr qg nj nk nl qh nn no np qi nr ns nt qj nv nw nx qc qd qe bk"><strong class="ne fr">No Support for Trainable Parameters in <em class="ny">score_mod</em>: </strong>At the time of this writing, Flex Attention does not support a <em class="ny">score_mod</em> implementation that includes <em class="ny">trainable</em> parameters. For example, while the documentation highlights support for <a class="af nb" href="https://pytorch.org/blog/flexattention/#relative-position-encodings" rel="noopener ugc nofollow" target="_blank">relative position encodings</a>, these are commonly implemented with <em class="ny">trainable</em> parameters (rather than fixed values) which cannot currently be accommodated.</li></ol><p id="c10b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the face of these limitations, we can return to one of the other optimization opportunities discussed above.</p><h1 id="0b1e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Summary</h1><p id="baa8" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">As the reliance on transformer architectures and attention layers in ML models increases, so does the need for tools and techniques for optimizing these components. In this post, we have explored a number of attention kernel variants, each with its own unique properties, capabilities, and limitations. Importantly, one size does not fit all — different models and use cases will warrant the use of different kernels and different optimization strategies. This underscores the importance of having a wide variety of tools and techniques for optimizing attention layers.</p><p id="b52b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a <a class="af nb" href="https://chaimrand.medium.com/optimizing-transformer-models-for-variable-length-input-sequences-19fb88fddf71" rel="noopener">sequel to this post</a>, we will further explore attention layer optimization by focusing on applying some of the tools we discussed to tackle the challenge of handling variable-sized input sequences. Stay tuned…</p></div></div></div></div>    
</body>
</html>