["```py\nimport numpy as np\n\nclass PositionalEncoder():\n    \"\"\" An implementation of positional encoding.\n\n    Attributes:\n        d_model (int): The number of embedding dimensions in the learned\n            embeddings. This is used to determine the length of the positional\n            encoding vectors, which make up the rows of the positional encoding\n            matrix.\n        max_length (int): The maximum sequence length in the transformer. This\n            is used to determine the size of the positional encoding matrix.\n        rounding (int): The number of decimal places to round each of the\n            values to in the output positional encoding matrix.\n    \"\"\"\n\n    def __init__(self, d_model, max_length, rounding):\n        self.d_model = d_model\n        self.max_length = max_length\n        self.rounding = rounding\n\n    def generate_positional_encoding(self):\n        \"\"\" Generate positional information to add to inputs for encoding.\n\n        The positional information is generated using the number of embedding\n        dimensions (d_model), the maximum length of the sequence (max_length),\n        and the number of decimal places to round to (rounding). The output\n        matrix generated is of size (max_length X embedding_dim), where each\n        row is the positional information to be added to the learned\n        embeddings, and each column is an embedding dimension.\n        \"\"\"\n\n        position = np.arange(0, self.max_length).reshape(self.max_length, 1)\n        even_i = np.arange(0, self.d_model, 2)\n        denominator = 10_000**(even_i / self.d_model)\n\n        even_encoded = np.round(np.sin(position / denominator), self.rounding)\n        odd_encoded = np.round(np.cos(position / denominator), self.rounding)\n\n        # Interleave the even and odd encodings\n        positional_encoding = np.stack((even_encoded, odd_encoded),2)\\\n        .reshape(even_encoded.shape[0],-1)\n\n        # If self.d_model is odd remove the extra column generated\n        if self.d_model % 2 == 1:\n            positional_encoding = np.delete(positional_encoding, -1, axis=1)\n\n        return positional_encoding\n\n    def encode(self, input):\n        \"\"\" Encode the input by adding positional information.\n\n        Args:\n            input (np.array): A two-dimensional array of embeddings. The array\n                should be of size (self.max_length x self.d_model).\n\n        Returns:\n            output (np.array): A two-dimensional array of embeddings plus the\n                positional information. The array has size (self.max_length x\n                self.d_model).\n        \"\"\"\n        positional_encoding = self.generate_positional_encoding()\n        output = input + positional_encoding\n\n        return output\n\nMAX_LENGTH = 5\nEMBEDDING_DIM = 3\nROUNDING = 2\n\n# Instantiate the encoder\nPE = PositionalEncoder(d_model=EMBEDDING_DIM,\n                       max_length=MAX_LENGTH,\n                       rounding=ROUNDING)\n\n# Create an input matrix of word embeddings without positional encoding\ninput = np.round(np.random.rand(MAX_LENGTH, EMBEDDING_DIM), ROUNDING)\n\n# Create an output matrix of word embeddings by adding positional encoding\noutput = PE.encode(input)\n\n# Print the results\nprint(f'Embeddings without positional encoding:\\n\\n{input}\\n')\nprint(f'Positional encoding:\\n\\n{output-input}\\n')\nprint(f'Embeddings with positional encoding:\\n\\n{output}')\n```", "```py\nEmbeddings without positional encoding:\n\n[[0.12 0.94 0.9 ]\n [0.14 0.65 0.22]\n [0.29 0.58 0.31]\n [0.69 0.37 0.62]\n [0.25 0.61 0.65]]\n\nPositional encoding:\n\n[[ 0\\.    1\\.    0\\.  ]\n [ 0.84  0.54  0\\.  ]\n [ 0.91 -0.42  0\\.  ]\n [ 0.14 -0.99  0.01]\n [-0.76 -0.65  0.01]]\n\nEmbeddings with positional encoding:\n\n[[ 0.12  1.94  0.9 ]\n [ 0.98  1.19  0.22]\n [ 1.2   0.16  0.31]\n [ 0.83 -0.62  0.63]\n [-0.51 -0.04  0.66]]\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Instantiate a PositionalEncoder class\nd_model = 400\nmax_length = 100\nrounding = 4\n\nPE = PositionalEncoder(d_model=d_model,\n                       max_length=max_length,\n                       rounding=rounding)\n\n# Generate positional encodings\ninput = np.round(np.random.rand(max_length, d_model), 4)\npositional_encoding = PE.generate_positional_encoding()\n\n# Plot positional encodings\ncax = plt.matshow(positional_encoding, cmap='coolwarm')\nplt.title(f'Positional Encoding Matrix ({d_model=}, {max_length=})')\nplt.ylabel('Position of the Embedding\\nin the Sequence, pos')\nplt.xlabel('Embedding Dimension, i')\nplt.gcf().colorbar(cax)\nplt.gca().xaxis.set_ticks_position('bottom')\n```", "```py\nimport matplotlib.pyplot as plt\n\n# Create word embeddings\nxs = [0.5, 1.5, 2.5, 6.0, 7.5, 8.0]\nys = [3.0, 1.2, 0.5, 8.0, 7.5, 5.5]\nwords = ['money', 'deposit', 'withdraw', 'nature', 'river', 'water']\nbank = [[4.5, 4.5], [6.7, 6.5]]\n\n# Create figure\nfig, ax = plt.subplots(ncols=2, figsize=(8,4))\n\n# Add titles\nax[0].set_title('Learned Embedding for \"bank\"\\nwithout context')\nax[1].set_title('Contextual Embedding for\\n\"bank\" after self-attention')\n\n# Add trace on plot 2 to show the movement of \"bank\"\nax[1].scatter(bank[0][0], bank[0][1], c='blue', s=50, alpha=0.3)\nax[1].plot([bank[0][0]+0.1, bank[1][0]],\n           [bank[0][1]+0.1, bank[1][1]],\n           linestyle='dashed',\n           zorder=-1)\n\nfor i in range(2):\n    ax[i].set_xlim(0,10)\n    ax[i].set_ylim(0,10)\n\n    # Plot word embeddings\n    for (x, y, word) in list(zip(xs, ys, words)):\n        ax[i].scatter(x, y, c='red', s=50)\n        ax[i].text(x+0.5, y, word)\n\n    # Plot \"bank\" vector\n    x = bank[i][0]\n    y = bank[i][1]\n\n    color = 'blue' if i == 0 else 'purple'\n\n    ax[i].text(x+0.5, y, 'bank')\n    ax[i].scatter(x, y, c=color, s=50)\n```", "```py\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\ndef extract_le(sequence, tokenizer, model):\n    \"\"\" Extract the learned embedding for each token in an input sequence.\n\n    Tokenize an input sequence (string) to produce a tensor of token IDs.\n    Return a tensor containing the learned embedding for each token in the\n    input sequence.\n\n    Args:\n        sequence (str): The input sentence(s) to tokenize and extract\n            embeddings from.\n        tokenizer: The tokenizer used to produce tokens.\n        model: The model to extract learned embeddings from.\n\n    Returns:\n        learned_embeddings (torch.tensor): A tensor containing tensors of\n            learned embeddings for each token in the input sequence.\n    \"\"\"\n    token_dict = tokenizer(sequence, return_tensors='pt')\n    token_ids = token_dict['input_ids']\n    learned_embeddings = model.embeddings.word_embeddings(token_ids)[0]\n\n    # Additional processing for display purposes\n    learned_embeddings = learned_embeddings.tolist()\n    learned_embeddings = [[round(i,2) for i in le] \\\n                          for le in learned_embeddings]\n\n    return learned_embeddings\n\ndef extract_te(sequence, tokenizer, model):\n    \"\"\" Extract the tranformer embedding for each token in an input sequence.\n\n    Tokenize an input sequence (string) to produce a tensor of token IDs.\n    Return a tensor containing the transformer embedding for each token in the\n    input sequence.\n\n    Args:\n        sequence (str): The input sentence(s) to tokenize and extract\n            embeddings from.\n        tokenizer: The tokenizer used to produce tokens.\n        model: The model to extract learned embeddings from.\n\n    Returns:\n        transformer_embeddings (torch.tensor): A tensor containing tensors of\n            transformer embeddings for each token in the input sequence.\n    \"\"\"\n    token_dict = tokenizer(sequence, return_tensors='pt')\n\n    with torch.no_grad():\n        base_model_output = model(**token_dict)\n\n    transformer_embeddings = base_model_output.last_hidden_state[0]\n\n    # Additional processing for display purposes\n    transformer_embeddings = transformer_embeddings.tolist()\n    transformer_embeddings = [[round(i,2) for i in te] \\\n                              for te in transformer_embeddings]\n\n    return transformer_embeddings\n\n# Instantiate DistilBERT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = AutoModel.from_pretrained('distilbert-base-uncased')\n\n# Extract the learned embedding for bank from DistilBERT\nle_bank = extract_le('bank', tokenizer, model)[1]\n\n# Write sentences containing \"bank\" in two different contexts\ns1 = 'Write a poem about a man fishing on a river bank.'\ns2 = 'Write a poem about a man withdrawing money from a bank.'\n\n# Extract the transformer embedding for bank from DistilBERT in each sentence\ns1_te_bank = extract_te(s1, tokenizer, model)[11]\ns2_te_bank = extract_te(s2, tokenizer, model)[11]\n\n# Print the results\nprint('------------------- Embedding vectors for \"bank\" -------------------\\n')\nprint(f'Learned embedding:                  {le_bank[:5]}')\nprint(f'Transformer embedding (sentence 1): {s1_te_bank[:5]}')\nprint(f'Transformer embedding (sentence 2): {s2_te_bank[:5]}')\n```", "```py\n------------------- Embedding vectors for \"bank\" -------------------\n\nLearned embedding:                  [-0.03, -0.06, -0.09, -0.07, -0.03]\nTransformer embedding (sentence 1): [0.15, -0.16, -0.17, -0.08, 0.44]\nTransformer embedding (sentence 2): [0.27, -0.23, -0.23, -0.21, 0.79]\n```", "```py\nimport torch\nfrom transformers import DistilBertTokenizerFast, DistilBertModel\n\n# Instantiate DistilBERT tokenizer and model\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\n```", "```py\nimport torch\nfrom transformers import AutoModel, AutoTokenizer\n\n# Instantiate DistilBERT tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = AutoModel.from_pretrained('distilbert-base-uncased')\n```", "```py\n# Create example sentences to produce embeddings for\ns1 = 'Write a poem about a man fishing on a river bank.'\ns2 = 'Write a poem about a man withdrawing money from a bank.'\n```", "```py\ntoken_dict = tokenizer(s1, return_tensors='pt')\ntoken_ids = token_dict['input_ids'][0]\n```", "```py\nlearned_embeddings = model.embeddings.word_embeddings(token_ids)\nlearned_embeddings\n```", "```py\ntensor([[ 0.0390, -0.0123, -0.0208,  ...,  0.0607,  0.0230,  0.0238],\n        [-0.0300, -0.0070, -0.0247,  ...,  0.0203, -0.0566, -0.0264],\n        [ 0.0062,  0.0100,  0.0071,  ..., -0.0043, -0.0132,  0.0166],\n        ...,\n        [-0.0261, -0.0571, -0.0934,  ..., -0.0351, -0.0396, -0.0389],\n        [-0.0244, -0.0138, -0.0078,  ...,  0.0069,  0.0057, -0.0016],\n        [-0.0199, -0.0095, -0.0099,  ..., -0.0235,  0.0071, -0.0071]],\n       grad_fn=<EmbeddingBackward0>)\n```", "```py\nwith torch.no_grad():\n    base_model_output = model(**token_dict)\n\ntransformer_embeddings = base_model_output.last_hidden_state\ntransformer_embeddings\n```", "```py\ntensor([[[-0.0957, -0.2030, -0.5024,  ...,  0.0490,  0.3114,  0.1348],\n         [ 0.4535,  0.5324, -0.2670,  ...,  0.0583,  0.2880, -0.4577],\n         [-0.1893,  0.1717, -0.4159,  ..., -0.2230, -0.2225,  0.0207],\n         ...,\n         [ 0.1536, -0.1616, -0.1735,  ..., -0.3608, -0.3879, -0.1812],\n         [-0.0182, -0.4264, -0.6702,  ...,  0.3213,  0.5881, -0.5163],\n         [ 0.7911,  0.2633, -0.4892,  ..., -0.2303, -0.6364, -0.3311]]])\n```", "```py\ntokens = tokenizer.convert_ids_to_tokens(token_ids)\ntokens\n```", "```py\n['[CLS]', 'write', 'a', 'poem', 'about', 'a', 'man', 'fishing', 'on', 'a',\n 'river', 'bank', '.', '[SEP]']\n```"]