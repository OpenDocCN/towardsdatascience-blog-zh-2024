<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Rainbow: The Colorful Evolution of Deep Q-Networks 🌈</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Rainbow: The Colorful Evolution of Deep Q-Networks 🌈</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12">https://towardsdatascience.com/rainbow-the-colorful-evolution-of-deep-q-networks-37e662ab99b2?source=collection_archive---------8-----------------------#2024-07-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="509e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Everything you need to assemble the DQN Megazord in JAX.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryan Pégoud" class="l ep by dd de cx" src="../Images/9314b76c2be56bda8b73b4badf9e3e4d.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ep3ebfAfE1csq42qzviNgg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ryanpegoud?source=post_page---byline--37e662ab99b2--------------------------------" rel="noopener follow">Ryan Pégoud</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--37e662ab99b2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">17 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/9c3ff5fae978b121c7db85b75b3e992f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6V_SKO4X1mk7rKH8WaRQw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">“The Rainbow Megazord”, Dall-E 3</figcaption></figure><p id="c407" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In 2013, the introduction of Deep Q-Networks (DQN) by <em class="nx">Mnih et al.</em>[1]<em class="nx"> </em>marked the first breakthrough in Deep Reinforcement Learning, surpassing expert human players in three Atari games. Over the years, several variants of DQN were published, each improving on specific weaknesses of the original algorithm.</p><p id="db4c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In 2017, <em class="nx">Hessel et al.</em>[2]<em class="nx"> </em>made the best out of the DQN palette by combining 6 of its powerful variants, crafting what could be called the DQN Megazord: Rainbow.</p><p id="9deb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this article, we’ll break down the individual components that make up Rainbow, while reviewing their JAX implementations in the <a class="af ny" href="https://github.com/EdanToledo/Stoix" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Stoix library.</strong></a></p><h1 id="7dd1" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">DQN</h1><p id="a83b" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The fundamental building block of Rainbow is DQN, an extension of Q-learning using a neural network with parameters <strong class="nd fr">θ</strong> to approximate the Q-function (i.e. action-value function). In particular, DQN uses convolutional layers to extract features from images and a linear layer to produce a scalar estimate of the Q-value.</p><p id="69a7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">During training, the network parameterized by <strong class="nd fr">θ</strong>, referred to as the <em class="nx">“online network”</em> is used to select actions while the <em class="nx">“target network”</em> parameterized by <strong class="nd fr">θ-</strong> is a delayed copy of the online network used to provide stable targets. This way, the targets are not dependent on the parameters being updated.<br/>Additionally, DQN uses a replay buffer <strong class="nd fr"><em class="nx">D</em></strong> to sample past transitions (observations, reward, and done flag tuples) to train on at fixed intervals.</p><p id="0394" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">At each iteration <strong class="nd fr"><em class="nx">i</em></strong>, DQN samples a transition <strong class="nd fr"><em class="nx">j </em></strong>and takes a gradient step on the following loss:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/b62e326d7b69d832ec425e3d753b56e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zgDgFjtgvGsGUb1kYyaSzA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">DQN loss function, all images are made by the author, unless specified otherwise</figcaption></figure><p id="3410" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This loss aims at minimizing the expectation of the squared temporal-difference (TD) error.</p><p id="5c9a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Note that DQN is an <strong class="nd fr">off-policy</strong> algorithm because it learns the optimal policy defined by the <strong class="nd fr">maximum Q-value</strong> term while following a different behavior policy, such as an epsilon-greedy policy.</p><p id="d8f6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s the DQN algorithm in detail:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pb"><img src="../Images/baf5816cef9399a587418e16b013d99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8uRBmpYM16wcZEvqgZCQFg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">DQN algorithm</figcaption></figure><h2 id="dfed" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">DQN in practice</h2><p id="3c49" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">As mentioned above, we’ll reference code snippets from the Stoix library to illustrate the core parts of DQN and Rainbow <em class="nx">(some of the code was slightly edited or commented for pedagogical purposes)</em>.</p><p id="1ec8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s start with the neural network: Stoix lets us break down our model architecture into a pre-processor and a post-processor, referred to as <strong class="nd fr">torso</strong> and <strong class="nd fr">head</strong> respectively. In the case of DQN, the torso would be a multi-layer perceptron (MLP) or convolutional neural network (CNN) and the head an epsilon greedy policy, both implemented as <a class="af ny" href="https://flax.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Flax </strong></a>modules:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A Q-Network, defined as a CNN Torso and an Epsilon-Greedy policy in Stoix</figcaption></figure><p id="5aaf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Additionally, DQN uses the following loss (<em class="nx">note that Stoix follows the </em><a class="af ny" href="https://github.com/google-deepmind/rlax" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Rlax</em></strong></a><strong class="nd fr"><em class="nx"> </em></strong><em class="nx">naming conventions, therefore tm1 is equivalent to timestep t in the above equations, while t refers to timestep t+1</em>):</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The Q-learning loss used in the context of DQN</figcaption></figure><h2 id="f4e6" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">The Rainbow blueprint</h2><p id="ebaf" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Now that we have laid the foundations for DQN, we’ll review each part of the algorithm in more detail, while identifying potential weaknesses and how they are addressed by Rainbow.<br/>In particular, we’ll cover:</p><ul class=""><li id="f4c2" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">Double DQN and the overestimation bias</li><li id="6e45" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Dueling DQN and the state-value / advantage prediction</li><li id="cfda" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Distributional DQN and the return distribution</li><li id="acc1" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Multi-step learning</li><li id="1f38" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Noisy DQN and flexible exploration strategies</li><li id="a77f" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Prioritized Experience Replay and learning potential</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qe"><img src="../Images/20275c2f4f6e79e1dc782a0d2beb1f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nP0nGY7dtgM0zKbr5HT-HA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The Rainbow Blueprint, Dall-E 3</figcaption></figure><h1 id="a8ae" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Double DQN</h1><ul class=""><li id="f047" class="nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pw px py bk"><strong class="nd fr">Source:</strong> <a class="af ny" href="http://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"><em class="nx">Deep Reinforcement Learning with Double Q-learning</em></a><em class="nx"> </em>[3]</li><li id="fbba" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Improvement:</strong> Reduced overestimation bias</li></ul><h2 id="c376" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">The overestimation bias</h2><p id="5361" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">One issue with the loss function used in vanilla DQN arises from the Q-target. Remember that we define the target as:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/3bd664e7d9b3dd60aff3bb4a501b9bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s1dH__rS9nRQaypJ4061kg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Objective in the DQN loss</figcaption></figure><p id="30ca" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This objective may lead to an <strong class="nd fr">overestimation bias</strong>. Indeed, as DQN uses bootstrapping (learning estimates from estimates), the max term may select overestimated values to update the Q-function, leading to overestimated Q-values.</p><p id="d1db" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As an example, consider the following figure:</p><ul class=""><li id="ad0a" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">The Q-values predicted by the network are represented in blue.</li><li id="24e0" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The true Q-values are represented in purple.</li><li id="9c91" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The gap between the predictions and true values is represented by red arrows.</li></ul><p id="804a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this case, action 0 has the highest predicted Q-value because of a large prediction error. This value will therefore be used to construct the target. <br/>However, the action with the highest true value is action 2. This illustration shows how the max term in the target favors <strong class="nd fr">large positive estimation errors</strong>, inducing an overestimation bias.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qf"><img src="../Images/1bf3d33b853a2c71019024eae646b567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifcuiTXwna1NYAY6owLx8A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustration of the overestimation bias.</figcaption></figure><h2 id="5c1f" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Decoupling action selection and evaluation</h2><p id="6e89" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">To solve this problem, <em class="nx">Hasselt et al.</em> (2015)[3] propose a new target where the action is selected by the online network, while its value is estimated by the target network:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/1c377edbece9356607e4a8a0b5723f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bbFmTqWiHEB6GUJYl-cccA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The Double DQN target</figcaption></figure><p id="6cb2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">By decoupling action selection and evaluation, the estimation bias is significantly reduced, leading to better value estimates and improved performance.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qg"><img src="../Images/be824709136eed9f80431b6882ad92c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bXIdPN_xA4oz6unCyVsS0Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Double DQN provides stable and accurate value estimates, leading to improved performance. Source: Hasselt et al. (2015), Figure 3</figcaption></figure><h2 id="a7e2" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Double DQN in practice</h2><p id="3f2e" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">As expected, implementing Double DQN only requires us to modify the loss function:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h1 id="6e4e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Dueling DQN</h1><ul class=""><li id="d4c8" class="nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pw px py bk"><strong class="nd fr">Source:</strong> <a class="af ny" href="http://arxiv.org/abs/1511.06581" rel="noopener ugc nofollow" target="_blank"><em class="nx">Dueling Network Architectures for Deep Reinforcement Learning</em></a></li><li id="25c7" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Improvement:</strong> Separation of the value and advantage computation</li></ul><h2 id="e281" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">State value, Q-value, and advantage</h2><p id="513f" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In RL, we use several functions to estimate the value of a given state, action, or sequence of actions from a given state:</p><ul class=""><li id="4cc4" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk"><strong class="nd fr">State-value V(s): </strong>The state value corresponds to the expected return when starting in a given state <strong class="nd fr">s </strong>and following a policy <strong class="nd fr">π </strong>thereafter.</li><li id="2b94" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Q-value Q(s, a): </strong>Similarly, the Q-value corresponds to the expected return when starting in a given state <strong class="nd fr">s</strong>, taking action<strong class="nd fr"> a, </strong>and following a policy <strong class="nd fr">π </strong>thereafter.</li><li id="0eb1" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Advantage A(s, a): </strong>The advantage is defined as the difference between the Q-value and the state-value in a given state <strong class="nd fr">s </strong>for an action <strong class="nd fr">a</strong>. It represents the inherent value of action <strong class="nd fr">a </strong>in the current state.</li></ul><p id="ed00" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The following figure attempts to represent the differences between these value functions on a backup diagram (<em class="nx">note that the state value is weighted by the probability of taking each action under policy </em><strong class="nd fr">π</strong>).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qh"><img src="../Images/cba2c32f4c583fd964850468b441292f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eOAbQBykllUwckuo2FLa2g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Visualization of the state value (in purple), state-action value (Q-function, in blue), and the advantage (in pink) on a backup diagram.</figcaption></figure><p id="40c5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Usually, DQN estimates the Q-value directly, using a feed-forward neural network. This implies that DQN has to learn the Q-values for each action in each state independently.</p><h2 id="c45c" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">The dueling architecture</h2><p id="44a9" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Introduced by <em class="nx">Wang et al</em>.[4] in 2016, Dueling DQN uses a neural network with two separate streams of computation:</p><ul class=""><li id="bbf5" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">The <strong class="nd fr">state value stream </strong>predicts the scalar value of a given state.</li><li id="5657" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The <strong class="nd fr">advantage stream </strong>predicts to predict the advantage of each action for a given state.</li></ul><p id="4852" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This decoupling enables the <strong class="nd fr">independent estimation</strong> of the state value and advantages, which has several benefits. For instance, the network can learn state values without having to update the action values regularly. Additionally, it can better generalize to unseen actions in familiar states.<br/>These improvements lead to stabler and faster convergence, especially in environments with many similar-valued actions.</p><p id="6c4b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In practice, a dueling network uses a <strong class="nd fr">common representation </strong>(i.e. a shared linear or convolutional layer) parameterized by parameters <strong class="nd fr">θ</strong> before splitting into two streams, consisting of linear layers with parameters <strong class="nd fr">α</strong> and <strong class="nd fr">β</strong> respectively. The state value stream outputs a scalar value while the advantage stream returns a scalar value for each available action. <br/>Adding the outputs of the two streams allows us to reconstruct the Q-value for each action as <strong class="nd fr">Q(s, a) = V(s) + A(s, a)</strong>.</p><p id="9692" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">An important detail is that the mean is usually subtracted from the advantages. Indeed, the advantages need to have<strong class="nd fr"> zero mean</strong>, otherwise, it would be impossible to decompose Q into V and A, making the problem ill-defined. With this constraint, <strong class="nd fr">V</strong> represents the value of the state while <strong class="nd fr">A</strong> represents how much better or worse each action is compared to the average action in that state.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qi"><img src="../Images/af7d67549d33c0babd85d701ffa5900e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wqmeuL471NfDOQs3-BvaqQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustration of a dueling network</figcaption></figure><h2 id="52de" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Dueling Network in practice</h2><p id="8aef" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Here’s the Stoix implementation of a Q-network:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h1 id="97cb" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Distributional DQN</h1><ul class=""><li id="5781" class="nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pw px py bk"><strong class="nd fr">Source:</strong> <a class="af ny" href="http://arxiv.org/abs/1707.06887" rel="noopener ugc nofollow" target="_blank">A distributional perspective on Reinforcement Learning</a>[5]</li><li id="d85d" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Improvement:</strong> Richer value estimates</li></ul><h2 id="7bc3" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">The return distribution</h2><p id="7cc8" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Most RL systems model the expectation of the return, however, a promising body of literature approaches RL from a distributional perspective. In this setting, the goal becomes to model the <strong class="nd fr">return distribution</strong>, which allows us to consider other statistics than the mean.<br/>In 2017, <em class="nx">Bellemare et al.</em>[5] published a distributional version of DQN called C51 predicting the return distribution for each action, reaching new state-of-the-art performances on the Atari benchmark.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qj"><img src="../Images/194c16ae2151c70103d4d1834a0a919b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hequN6-JLnMm_mujmTQeBg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustrated comparison between DQN and C51. Source [5']</figcaption></figure><p id="6923" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s take a step back and review the theory behind C51.<br/>In traditional RL, we evaluate a policy using the <strong class="nd fr">Bellman Equation</strong>, which allows us to define the Q-function in a recursive form. Alternatively, we can use a distributional version of the Bellman equation, which accounts for randomness in the returns:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qk"><img src="../Images/ef036b8c95c7be0d0c6ab34ccfdafdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8jPhPrsaWjGC6JO_V68E6A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Standard and Distributional versions of the Bellman Equation</figcaption></figure><p id="ece6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here, <strong class="nd fr">ρ</strong> is the transition function.<br/>The main difference between those functions is that <strong class="nd fr">Q</strong> <strong class="nd fr">is a numerical value</strong>, summing expectations over random variables. In contrast, <strong class="nd fr">Z is a random variable</strong>, summing the reward distribution and the discounted distribution of future returns.</p><p id="8751" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The following illustration helps visualize how to derive <strong class="nd fr">Z </strong>from the distributional Bellman equation:</p><ul class=""><li id="34e1" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">Consider the distribution of returns <strong class="nd fr">Z</strong> at a given timestep and the transition operator <strong class="nd fr">Pπ.</strong> <strong class="nd fr">PπZ</strong> is the distribution of future returns <strong class="nd fr">Z(s’, a’)</strong>.</li><li id="614a" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Multiplying this by the discount factor <strong class="nd fr">γ</strong> contracts the distribution towards 0 (as <strong class="nd fr">γ</strong> is less than 1).</li><li id="5790" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Adding the reward distribution shifts the previous distribution by a set amount <em class="nx">(Note that the figure assumes a constant reward for simplicity. In practice, adding the reward distribution would shift but also modify the discounted return</em>).</li><li id="dc43" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Finally, the distribution is projected on a discrete support using an L2 projection operator <strong class="nd fr">Φ</strong>.</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj ql"><img src="../Images/97a3aac37d854dcd502775572e175e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*wmgLoYfR6x28kLbFM6tc7g.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Illustration of the distributional Bellman equation. Source: [5]</figcaption></figure><p id="033a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This fixed support is a vector of <strong class="nd fr"><em class="nx">N</em></strong> atoms separated by a constant gap within a set interval:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qm"><img src="../Images/1aa87d6a69d5414ec7ebb7e17c72596a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-sJ8bVmI-rl4fEEew8vQPg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Definition of the discrete support <strong class="bf ob">z</strong></figcaption></figure><p id="2098" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">At inference time, the Q-network returns an approximating distribution <strong class="nd fr">dt</strong> defined on this support with the probability mass <strong class="nd fr">pθ(st, at) </strong>on each atom <strong class="nd fr"><em class="nx">i</em></strong> such that:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/da3c5c9967c24aea64b82f9e9891c613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTM-SEKtX2Qzc5TN2y1FkQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Predicted return distribution</figcaption></figure><p id="9ff2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The goal is to update <strong class="nd fr">θ</strong> such that the distribution closely matches the true distribution of returns. To learn the probability masses, the target distribution is built using a <strong class="nd fr">distributional variant of Bellman’s optimality equation</strong>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/81ad365cd4b43af4ea6fd2e4388e975e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gLS1YpzdZpYieCR8r24DA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Target return distribution</figcaption></figure><p id="efa6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To be able to compare the distribution predicted by our neural network and the target distribution, we need to discretize the target distribution and project it on the same support <strong class="nd fr">z</strong>.</p><p id="6484" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To this end, we use an L2 projection (<em class="nx">a projection onto </em><strong class="nd fr"><em class="nx">z</em></strong><em class="nx"> such that the difference between the original and projected distribution is minimized in terms of the L2 norm</em>):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/c58835a895b9ebeba742c78533a5f1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIMIRPJTZXgSsIlvbMUDqg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">L2 projection of the target distribution</figcaption></figure><p id="1cad" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, we need to define a loss function that minimizes the difference between the two distributions. As we’re dealing with distributions, we can’t simply subtract the prediction from the target, as we did previously.</p><p id="50e7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Instead, we minimize the Kullback-Leibler divergence between <strong class="nd fr">dt </strong>and <strong class="nd fr">d’t </strong>(in practice, this is implemented as a cross-entropy loss):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/4e6e7171841467790558b6c6ce556321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I9wouicVn0QBSf2l0o4Qlg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">KL divergence between the projected target and the predicted return distribution</figcaption></figure><p id="50cf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">For a more exhaustive description of Distributional DQN, you can refer to Massimiliano Tomassoli’s article[8] as well as Pascal Poupart’s video on the topic[11].</em></p><h2 id="75a5" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">C51 in practice</h2><p id="41f5" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The key components of C51 in Stoix are the Distributional head and the categorical loss, which uses double Q-learning by default as introduced previously. The choice of defining the C51 network as a head lets us use an MLP or a CNN torso interchangeably depending on the use case.</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h1 id="da72" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Noisy DQN</h1><ul class=""><li id="61e8" class="nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pw px py bk"><strong class="nd fr">Source:</strong> <a class="af ny" href="http://arxiv.org/abs/1706.10295" rel="noopener ugc nofollow" target="_blank">Noisy Networks for Exploration</a>[6]</li><li id="9b36" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Improvement:</strong> Learnable and state-dependent exploration mechanism</li></ul><h2 id="e46b" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Noisy parameterization of Neural Networks</h2><p id="bffc" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">As many off-policy algorithms, DQN relies on an epsilon-greedy policy as its main exploration mechanism. Therefore, the algorithm will behave greedily with respect to the Q-values most of the time and select random actions with a predefined probability.</p><p id="e71e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Fortunato et al.</em>[6] introduce NoisyNets as a more flexible alternative. NoisyNets are neural networks whose weights and biases are <strong class="nd fr">perturbed</strong> by a <strong class="nd fr">parametric function of Gaussian noise</strong>. Similarly to an epsilon-greedy policy, such noise injects randomness in the agent’s action selection, thus encouraging exploration.</p><p id="1438" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, this noise is scaled and offset by <strong class="nd fr">learned parameters</strong>, allowing the level of noise to be adapted state-by-state. This way, the balance between exploration and exploitation is optimized <em class="nx">dynamically</em> during training. Eventually, the network may learn to ignore the noise, but will do so at <strong class="nd fr">different rates</strong> in <strong class="nd fr">different parts of the state space</strong>, leading to more flexible exploration.</p><p id="d331" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A network parameterized by a vector of noisy parameters is defined as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/e34ae4ce245756ebcea722260869dc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0NjLAh8LynCZEFBJsVp_A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Neural Network parameterized by Noisy parameters</figcaption></figure><p id="c7a4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Therefore, a linear layer <strong class="nd fr">y = wx + b </strong>becomes:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/0c00383ff99930448d0296b65faca5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv4upTlUK9X2Dke7UShW6w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Noisy linear layer</figcaption></figure><p id="fe26" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For performance, the noise is generated at inference time using <strong class="nd fr">Factorized Gaussian Noise</strong>. For a linear layer with <strong class="nd fr">M </strong>inputs and <strong class="nd fr">N </strong>outputs, a noise matrix of shape (<strong class="nd fr">M x N</strong>) is generated as a combination of two noise vectors with size <strong class="nd fr">M</strong> and <strong class="nd fr">N</strong>. This methods reduces the number of required random variables from <strong class="nd fr">M x N </strong>to <strong class="nd fr">M + N</strong>.<br/>The noise matrix is defined as the outer product of the noise vectors, each scaled by a function <strong class="nd fr">f</strong>:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qn"><img src="../Images/c2930b27127318d9c7088beb98339ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zpNEw8WATncHMb9Iu_0CpA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Noise generation using Factorised Gaussian Noise</figcaption></figure><h2 id="f0d3" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Improved exploration</h2><p id="097f" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The improved exploration induced by noisy networks allow a wide range of algorithms, such as DQN, Dueling DQN and A3C to benefit from improved performances with a reasonably low amount of extra parameters.</p></div></div><div class="mq"><div class="ab cb"><div class="ll qo lm qp ln qq cf qr cg qs ci bh"><figure class="ml mm mn mo mp mq qu qv paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qt"><img src="../Images/0546839f96938a710ebdd383e8aee747.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*fd5h7tODJ4G6FUsJ6wQoGw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">NoisyNets improve the performance of several algorithms on the Atari benchmark. Source: [6]</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1e17" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Noisy DQN in practice</h2><p id="2552" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In Stoix, we implement a noisy layer as follows:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><p id="4abb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="nx">Note: All the linear layers in Rainbow are replaced with their noisy equivalent (see the </em><strong class="nd fr"><em class="nx">“Assembling Rainbow”</em></strong><em class="nx"> section for more details).</em></p><h1 id="4c79" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Prioritized Experience Replay</h1><p id="2206" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk"><strong class="nd fr">Source:</strong> Prioritized Experience Replay[7]<br/><strong class="nd fr">Improvement:</strong> Prioritization of experiences with higher learning potential</p><h2 id="e213" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Estimating the Learning Potential</h2><p id="caee" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">After taking an environment step, vanilla DQN uniformly samples a batch of experiences (also called <em class="nx">transitions</em>) from a replay buffer and performs a gradient descent step on this batch. Although this approach produces satisfying results, some specific experiences might be more valuable from a learning perspective than others. Therefore, we could potentially speed up the training process by sampling such experiences more often.</p><p id="df16" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is precisely the idea explored in the Prioritized Experience Replay (PER) paper published by <em class="nx">Schaul et al.</em>[7] in 2016. However, the main question remains: how to approximate the <strong class="nd fr">expected learning potential</strong> of a transition?</p><blockquote class="qw qx qy"><p id="de54" class="nb nc nx nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">One idealized criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress). While this measure is not directly accessible, a reasonable proxy is the magnitude of a transition’s TD error δ, which indicates how ‘surprising’ or unexpected the transition is: specifically, how far the value is from its next-step bootstrap estimate (Andre et al., 1998).<br/>Prioritized Experience Replay, Schaul et al. (2016)</p></blockquote><p id="ed47" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As a reminder, the TD error is defined as follows:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/47087938e81974e308d2a37b8fcb400a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w8IbPsTFpLgYfueR65Hnlw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The temporal-difference error</figcaption></figure><p id="e87e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This metric is a decent estimate of the learning potential of a specific transition, as a high TD error indicates a large difference between the predicted and actual outcomes, meaning that the agent would benefit from updating its beliefs.</p><p id="eb8a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, it is worth noting that alternative prioritization metrics are still being studied. For instance, <em class="nx">Lahire et al.</em>[9] (2022) argue that the optimal sampling scheme is distributed according to the per-sample gradient norms:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/5ae86716a279e2c41dbbd7f9dc33b80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cymThHdJ312PnNgCBW8B4A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Per-sample gradient norms</figcaption></figure><p id="6f98" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, let’s continue with the TD error, as Rainbow uses this metric.</p><h2 id="bce2" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Deriving Sampling Probabilities</h2><p id="7a1f" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Once we have selected the prioritization criterion, we can derive the probabilities of sampling each transition from it. In Prioritized Experience Replay, two alternatives are showcased:</p><ul class=""><li id="ba42" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk"><strong class="nd fr">Proportional</strong>: Here the probability of replaying a transition is equal to the absolute value of the associated TD error. A small positive constant is added to prevent transitions not being revisited once their error is zero.</li><li id="c874" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Rank-based</strong>: In this mode, transitions are ranked in descending order according to their absolute TD error, and their probability is defined based on their rank. This option is supposed to be more robust as it is insensible to outliers.</li></ul><p id="918f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The sampling probabilities are then normalized and raised to the power <strong class="nd fr">α</strong>, a hyperparameter determining the degree of prioritization (<strong class="nd fr">α=0</strong> is the uniform case).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qz"><img src="../Images/dd93373aeafc0258d8097ddcc8b98e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfjpCvW_6DCwY9U8OK1Xhg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Prioritization modes and probability normalization</figcaption></figure><h2 id="2140" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Importance sampling and bias annealing</h2><p id="8ae4" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">In RL, the estimation of the expected value of the return relies on the assumption that the updates correspond to the same distribution as the expectation (i.e., the uniform distribution). However, PER introduces bias as we now sample experiences according to their TD error.</p><p id="f29b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To rectify this bias, we use <strong class="nd fr">importance sampling</strong>, a statistical method used to <em class="nx">estimate the properties of a distribution while sampling from a different distribution</em>. Importance sampling re-weights samples so that the estimates remain unbiased and accurate.<br/>Typically, the correcting weights are defined as the ratio of the two probabilities:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/df361e74e24201fc048a33e4eaf8cc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTDmKcMX0F6SfZ5ubQHQ3A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Importance sampling ratio</figcaption></figure><p id="cb16" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this case, the target distribution is the uniform distribution, where every transition has a probability of being sampled equal to 1/<strong class="nd fr">N</strong>, with <strong class="nd fr">N </strong>being the size of the replay buffer. <br/>Therefore, the importance sampling coefficient in the context of PER is defined by:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/8c435485dea322925507c5bc7a38e668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLuzHB3-WVErREoxCtXTaw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Importance sampling weight used in PER</figcaption></figure><p id="fa27" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">With <strong class="nd fr">β</strong> a coefficient adjusting the amount of bias correction (the bias is fully corrected for <strong class="nd fr">β=1</strong>). Finally, the weights are normalized for stability:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/868b56f8913a40c593f78675c09d784e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oLNMy56p3ZuLlpXBzxwk0Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Normalization of the importance sampling weights</figcaption></figure><p id="a4f8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To summarize, here’s the full algorithm for Prioritized Experience Replay (the update and training steps are identical to DQN):</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rb"><img src="../Images/e4e0192fa820ff77f9ef36d510c7fc91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JK7-l89NqTEamkpVp_IeYA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">The Prioritized Experience Replay algorithm</figcaption></figure><h2 id="1268" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Increased convergence speed with PER</h2><p id="ee86" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">The following plots highlight the performance benefits of PER. Indeed, the proportional and rank-based prioritization mechanisms enable DQN to reach the same baseline performances roughly twice as fast on the Atari benchmark.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rc"><img src="../Images/d4585c6368905c2144be046608fdc86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NEcTZgrj6Z91q86c7InGCw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Normalized maximum and average scores (in terms of Double DQN performance) on 57 Atari games. Source:[7]</figcaption></figure><h2 id="b71e" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Prioritized Experience Replay in practice</h2><p id="34ca" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Stoix seamlessly integrates the <a class="af ny" href="https://github.com/instadeepai/flashbax" rel="noopener ugc nofollow" target="_blank">Flashbax</a> library which provides a variety of replay buffers. Here are the relevant code snippets used to instantiate the replay buffer, compute the sampling probabilities from the TD error, and update the buffer’s priorities:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h1 id="63f6" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Multi-step Learning</h1><ul class=""><li id="db10" class="nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw pw px py bk"><strong class="nd fr">Source:</strong> <a class="af ny" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">Reinforcement Learning: an Introduction, chapter 7</a></li><li id="a690" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Improvement:</strong> Enhanced reward signal and sample efficiency, reduced variance</li></ul><p id="e6d7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Multi-step learning is an improvement on traditional one-step temporal difference learning which allows us to consider the return over <strong class="nd fr">n</strong> steps when building our targets. For instance, instead of considering the reward at the next timestep, we’ll consider the n-step truncated rewards (see the below equation). This process has several advantages, among which:</p><ul class=""><li id="d427" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk"><strong class="nd fr">Immediate feedback:</strong> considering a larger time horizon allows the agent to learn the value of state-action pairs much faster, especially in environments where rewards are delayed and specific actions might not pay out immediately.</li><li id="081a" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Sample efficiency:</strong> Each update in multi-step learning incorporates information from multiple time steps, making each sample more informative. This improves sample efficiency, meaning the agent can learn more from fewer experiences.</li><li id="c833" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Balancing Bias and Variance: </strong>Multi-step methods offer a trade-off between bias and variance. One-step methods have low bias but high variance, while multi-step methods have higher bias but lower variance. By tuning the number of steps, one can find a balance that works best for the given environment.</li></ul><p id="606a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The multi-step distributional loss used in Rainbow is defined as:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ra"><img src="../Images/4f2b412a351afc3dfa642272ce6a1fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ko2ZaPBNHqQnbyyE0glidQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Multi-step target return distribution</figcaption></figure><p id="967a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In practice, using n-step returns implies a few adjustments to our code:</p><ul class=""><li id="3b40" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">We now sample trajectories of <strong class="nd fr">n</strong> experiences, instead of individual experiences</li><li id="0b3f" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The reward is replaced with the n-step discounted returns</li><li id="1085" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The done flag is set to True if any of the <strong class="nd fr">n </strong>done flag is True</li><li id="b390" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">The next state <strong class="nd fr">s(t+1)</strong> is replaced by the last observation of the trajectory <strong class="nd fr">s(t+n)</strong></li></ul><h2 id="94de" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Multi-Step learning in practice</h2><p id="5493" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Finally, we can reuse the categorical loss function used in C51 with these updated inputs:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h1 id="802f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Assembling Rainbow</h1><p id="42b7" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">Congratulations on making it this far! We now have a better understanding of all the moving pieces that constitute Rainbow. Here’s a summary of the Rainbow agent:</p><ul class=""><li id="34e4" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk"><strong class="nd fr">Neural Network Architecture:</strong><br/> —<strong class="nd fr"> Torso:</strong> A convolutional neural network (CNN) or multi-layer perceptron (MLP) base that creates embeddings for the head network.<br/> — <strong class="nd fr">Head:</strong> Combines Dueling DQN and C51. The value stream outputs the state value distribution over atoms, while the advantage stream outputs the advantage distribution over actions and atoms. These streams are aggregated, and Q-values are computed as the weighted sum of atom values and their respective probabilities. An action is selected using an epsilon-greedy policy.<br/> —<strong class="nd fr"> Noisy Layers: </strong>All linear layers are replaced with their noisy equivalents to aid in exploration.</li><li id="5e8c" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Loss Function:</strong> Uses a distributional loss modeling the n-step returns, where targets are computed using Double Q-learning.</li><li id="944d" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Replay Buffer: </strong>Employs a prioritization mechanism based on the TD error to improve learning efficiency.</li></ul><p id="17d4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s the network used for the Rainbow head:</p><figure class="ml mm mn mo mp mq"><div class="pt io l ed"><div class="pu pv l"/></div></figure><h2 id="f135" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Performances and ablations</h2><p id="2725" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">To conclude this article, let’s take a closer look at Rainbow’s performances on the Atari benchmark, as well as the ablation study.<br/>The following figure compares Rainbow with the other DQN baselines we studied. The measured metric is the median human-normalized score. In other words, the median human performance on Atari games is set to 100%, which enables us to quickly spot algorithms achieving a human level.</p><p id="d6fc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Three of the DQN baselines reach this level after 200 million frames:</p><ul class=""><li id="8c2f" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk"><strong class="nd fr">Distributional DQN</strong></li><li id="cc1c" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Dueling DQN</strong></li><li id="888b" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk"><strong class="nd fr">Prioritized Double DQN</strong></li></ul><p id="aa4a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interestingly, Rainbow reaches the same level after only 44 million frames, making it <strong class="nd fr">roughly 5 times more sample efficient</strong> than the best baselines. At the end of training, it exceeds <strong class="nd fr">200%</strong> of the median human-normalized score.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj rd"><img src="../Images/ea23c893a795be0c7bd8eced1e0d9c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Whxt4H20QUrV_RBDRt7Qcw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Median human-normalized performance across 57 Atari games. Each line represents a DQN baseline. Source: [2]</figcaption></figure><p id="c607" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This second figure represents the ablation study, which represents the performances of Rainbow without one of its components. These results allow us to make several observations:</p><ul class=""><li id="442f" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pw px py bk">The three most crucial components of Rainbow are the distributional head, the use of multi-step learning, and the prioritization of the replay buffer.</li><li id="98f5" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Noisy layers contribute significantly to the overall performance. Using standard layers with an epsilon-greedy policy doesn’t allow the agent to reach the 200% score in 200 million frames.</li><li id="d7ab" class="nb nc fq nd b go pz nf ng gr qa ni nj nk qb nm nn no qc nq nr ns qd nu nv nw pw px py bk">Despite achieving strong performances on their own, the dueling structure and double Q-learning only provide marginal improvements in the context of Rainbow.</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj re"><img src="../Images/848c4c480a1e32c78e5393a9567b2c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhtiBMXnBj8T9Fibuntmuw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Median human-normalized performance across 57 Atari games. Each line represents an ablation of Rainbow. Source: [2]</figcaption></figure><p id="7eee" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thank you very much for reading this article, I hope it provided you with a comprehensive introduction to Rainbow and its components. I highly advise reading through the <a class="af ny" href="https://github.com/EdanToledo/Stoix/blob/main/stoix/systems/q_learning/ff_rainbow.py" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr">Stoix implementation of Rainbow</strong></a> for a more detailed description of the training process and the Rainbow architecture.</p><p id="7679" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Until next time 👋</p><h1 id="09c1" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Bibliography</h1><p id="3246" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). <a class="af ny" href="http://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Playing Atari with Deep Reinforcement Learning</em></strong></a>, arXiv<br/>[2] Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., &amp; Silver, D. (2017). <a class="af ny" href="http://arxiv.org/abs/1710.02298" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Rainbow: Combining Improvements in Deep Reinforcement Learning</em></strong></a>, arXiv.<br/>[3] van Hasselt, H., Guez, A., &amp; Silver, D. (2015). <a class="af ny" href="http://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Deep Reinforcement Learning with Double Q-learning</em></strong></a>, arXiv. <br/>[4] Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., &amp; de Freitas, N. (2016). <a class="af ny" href="http://arxiv.org/abs/1511.06581" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Dueling Network Architectures for Deep Reinforcement Learning</em></strong></a> (No. arXiv:1511.06581), arXiv<br/>[5] Bellemare, M. G., Dabney, W., &amp; Munos, R. (2017). <a class="af ny" href="http://arxiv.org/abs/1707.06887" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">A Distributional Perspective on Reinforcement Learning</em></strong></a>, arXiv<br/>[5'] Dabney, W., Ostrovski, G., Silver, D., &amp; Munos, R. (2018). <a class="af ny" href="http://arxiv.org/abs/1806.06923" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Implicit Quantile Networks for Distributional Reinforcement Learning</em></strong></a>, arXiv<br/><a class="af ny" href="http://arxiv.org/abs/1806.06923](http://arxiv.org/abs/1806.06923)[6]" rel="noopener ugc nofollow" target="_blank">[6]</a> Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., &amp; Legg, S. (2019). <a class="af ny" href="http://arxiv.org/abs/1706.10295" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Noisy Networks for Exploration</em></strong></a>, arXiv. <br/>[7] Schaul, T., Quan, J., Antonoglou, I., &amp; Silver, D. (2016). <a class="af ny" href="http://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Prioritized Experience Replay</em></strong></a><strong class="nd fr"><em class="nx">,</em></strong> arXiv</p><h2 id="3b31" class="pc oa fq bf ob pd pe pf oe pg ph pi oh nk pj pk pl no pm pn po ns pp pq pr ps bk">Additional resources</h2><p id="243e" class="pw-post-body-paragraph nb nc fq nd b go ov nf ng gr ow ni nj nk ox nm nn no oy nq nr ns oz nu nv nw fj bk">[8] Massimiliano Tomassoli, <a class="af ny" href="https://mtomassoli.github.io/2017/12/08/distributional_rl/" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Distributional RL: An intuitive explanation of Distributional RL</em></strong></a><br/>[9] Lahire, T., Geist, M., &amp; Rachelson, E. (2022). <a class="af ny" href="http://arxiv.org/abs/2110.01528" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">Large Batch Experience Replay</em></strong></a>, arXiv. <br/>[10] Sutton, R. S., &amp; Barto, A. G. (1998). <strong class="nd fr"><em class="nx">Reinforcement Learning: An Introduction</em></strong>.<br/>[11] Pascal Poupart, <a class="af ny" href="https://youtu.be/r-Yk6-jagDU?si=9lqQHHNaQz8Uiclw" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="nx">CS885 Module 5: Distributional RL</em></strong></a><strong class="nd fr"><em class="nx">, </em></strong>YouTube</p></div></div></div></div>    
</body>
</html>