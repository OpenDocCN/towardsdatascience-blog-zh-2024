- en: 'Neural Speed: Fast Inference on CPU for 4-bit Large Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18](https://towardsdatascience.com/neural-speed-fast-inference-on-cpu-for-4-bit-large-language-models-0d611978f399?source=collection_archive---------2-----------------------#2024-04-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Up to 40x faster than llama.cpp?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--0d611978f399--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0d611978f399--------------------------------)
    ·5 min read·Apr 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1517e7fb1e45ea750af91efd6cc45765.png)'
  prefs: []
  type: TYPE_IMG
- en: Generate with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Running large language models (LLMs) on consumer hardware can be challenging.
    If the LLM doesn’t fit on the GPU memory, quantization is usually applied to reduce
    its size. However, even after quantization, the model might still be too large
    to fit on the GPU. An alternative is to run it on the CPU RAM using a framework
    optimized for [CPU inference such as llama.cpp](https://medium.com/@bnjmn_marie/gguf-quantization-for-fast-and-memory-efficient-inference-on-your-cpu-d10fbe58fbca).
  prefs: []
  type: TYPE_NORMAL
- en: Intel is also working on accelerating inference on the CPU. They propose a framework,
    [Intel’s extension for Transformers](https://github.com/intel/intel-extension-for-transformers),
    built on top of Hugging Face Transformers and easy to use to exploit the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: With [Neural Speed](https://github.com/intel/neural-speed) (Apache 2.0 license),
    which relies on Intel’s extension for Transformers, Intel further accelerates
    inference for 4-bit LLMs on CPUs. According to Intel, using this framework can
    [make inference up to 40x faster than llama.cpp](https://github.com/intel/neural-speed?tab=readme-ov-file#key-features).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I review the main optimizations Neural Speed brings. I show
    how to use it and benchmark the inference throughput. I also compare it with llama.cpp.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Speed’s Inference Optimizations for 4-bit LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At NeurIPS 2023, Intel presented the main optimizations for inference on CPUs:'
  prefs: []
  type: TYPE_NORMAL
