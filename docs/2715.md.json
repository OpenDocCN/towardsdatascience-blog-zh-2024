["```py\nimport csv\nimport datetime\nimport numpy as np\nimport random\n\n# Remove existing ‘retail_transactions.csv’ file, if any\n! rm -f /p/a/t/h retail_transactions.csv\n\n# Set the no of transactions and othet configs\nno_of_iterations = 1000000\ndata = []\ncsvFile = 'retail_transactions.csv'\n\n# Open a file in write mode\nwith open(csvFile, 'w', newline='') as f:\n\n   fieldnames = ['orderID', 'customerID', 'productID', 'state', 'paymentMthd', 'totalAmt', 'invoiceTime']\n   writer = csv.DictWriter(f, fieldnames=fieldnames)\n   writer.writeheader()\n\n   for num in range(no_of_iterations):\n     # Create a transaction record with random values\n     new_txn = {\n     'orderID': num,\n     'customerID': random.choice([100, random.randint(1, 100000)]),\n     'productID': np.random.randint(10000, size=random.randint(1, 5)).tolist(),\n     'state': random.choice(['CA', 'TX', 'FL', 'NY', 'PA', 'OTHERS']),\n     'paymentMthd': random.choice(['Credit card', 'Debit card', 'Digital wallet', 'Cash on delivery', 'Cryptocurrency']),\n     'totalAmt': round(random.random() * 5000, 2),\n     'invoiceTime': datetime.datetime.now().isoformat()\n     }\n\n     data.append(new_txn)\n\n  writer.writerows(data)\n```", "```py\n# Set file location and type\nfile_location = \"/FileStore/tables/retail_transactions.csv\"\nfile_type = \"csv\"\n\n# Define CSV options\nschema = \"orderID INTEGER, customerID INTEGER, productID INTEGER, state STRING, paymentMthd STRING, totalAmt DOUBLE, invoiceTime TIMESTAMP\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# Read CSV files into DataFrame\ndf = spark.read.format(file_type) \\\n  .schema(schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"delimiter\", delimiter) \\\n  .load(file_location) \n```", "```py\nimport time\n\n# Measure the excution time of a given function\ndef time_decorator(func):\n  def wrapper(*args, **kwargs):\n    begin_time = time.time()\n    output = func(*args, **kwargs)\n    end_time = time.time()\n    print(f\"Execution time of function {func.__name__}: {round(end_time - begin_time, 2)} seconds.\")\n  return output\nreturn wrapper\n```", "```py\nfrom pyspark.sql.functions import col\n\n@time_decorator\ndef without_cache(data):\n  # 1st filtering\n  df2 = data.where(col(\"paymentMthd\") == \"Digital wallet\")\n  count = df2.count()\n\n  # 2nd filtering\n  df3 = df2.where(col(\"totalAmt\") > 2000)\n  count = df3.count()\n\n  return count\n\ndisplay(without_cache(df))\n```", "```py\nfrom pyspark.sql.functions import col\n\n@time_decorator\ndef after_cache(data):\n  # 1st filtering with cache\n  df2 = data.where(col(\"paymentMthd\") == \"Digital wallet\").cache()\n  count = df2.count()\n\n  # 2nd filtering\n  df3 = df2.where(col(\"totalAmt\") > 2000)\n  count = df3.count()\n\n  return count\n\ndisplay(after_cache(df))\n```", "```py\nfrom pyspark.sql.functions import col\n\n@time_decorator\ndef no_broadcast_var(data):\n  # Create small dataframe\n  small_data = [(\"CA\", \"California\"), (\"TX\", \"Texas\"), (\"FL\", \"Florida\")]\n  small_df = spark.createDataFrame(small_data, [\"state\", \"stateLF\"])\n\n  # Perform joining\n  result_no_broadcast = data.join(small_df, \"state\")\n\n  return result_no_broadcast.count()\n\ndisplay(no_broadcast_var(df))\n```", "```py\nfrom pyspark.sql.functions import col, broadcast\n\n@time_decorator\ndef have_broadcast_var(data):\n  small_data = [(\"CA\", \"California\"), (\"TX\", \"Texas\"), (\"FL\", \"Florida\")]\n  small_df = spark.createDataFrame(small_data, [\"state\", \"stateFullName\"])\n\n  # Create broadcast variable and perform joining\n  result_have_broadcast = data.join(broadcast(small_df), \"state\")\n\n  return result_have_broadcast.count()\n\ndisplay(have_broadcast_var(df))\n```", "```py\nfrom pyspark.sql.functions import col, desc\n\n@time_decorator\ndef no_salting(data):\n  # Perform aggregation\n  agg_data = data.groupBy(\"customerID\").agg({\"totalAmt\": \"sum\"}).sort(desc(\"sum(totalAmt)\"))\n  return agg_data\n\ndisplay(no_salting(df))\n```", "```py\nfrom pyspark.sql.functions import col, lit, concat, rand, split, desc\n\n@time_decorator\ndef have_salting(data):\n  # Salt the customerID by adding the suffix\n  salted_data = data.withColumn(\"salt\", (rand() * 8).cast(\"int\")) \\\n                .withColumn(\"saltedCustomerID\", concat(col(\"customerID\"), lit(\"_\"), col(\"salt\")))\n\n  # Perform aggregation\n agg_data = salted_data.groupBy(\"saltedCustomerID\").agg({\"totalAmt\": \"sum\"})\n\n # Remove salt for further aggregation\n final_result = agg_data.withColumn(\"customerID\", split(col(\"saltedCustomerID\"), \"_\")[0]).groupBy(\"customerID\").agg({\"sum(totalAmt)\": \"sum\"}).sort(desc(\"sum(sum(totalAmt))\"))\n\n return final_result\n\ndisplay(have_salting(df))\n```", "```py\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\nfrom pyspark.sql import functions as F\nimport numpy as np\n\n# UDF to calculate discounted amount\ndef calculate_discount(state, amount):\n  if state == \"CA\":\n    return amount * 0.90  # 10% off\n  else:\n    return amount * 0.85  # 15% off\n\ndiscount_udf = udf(calculate_discount, DoubleType())\n\n@time_decorator\ndef have_udf(data):\n  # Use the UDF\n  discounted_data = data.withColumn(\"discountedTotalAmt\", discount_udf(\"state\", \"totalAmt\"))\n\n  # Show the results\n  return discounted_data.select(\"customerID\", \"totalAmt\", \"state\", \"discountedTotalAmt\").show()\n\ndisplay(have_udf(df))\n```", "```py\nfrom pyspark.sql.functions import when\n\n@time_decorator\ndef no_udf(data):\n  # Use when and otherwise to discount the amount based on conditions \n  discounted_data = data.withColumn(\n  \"discountedTotalAmt\",\n  when(data.state == \"CA\", data.totalAmt * 0.90)  # 10% off\n  .otherwise(data.totalAmt * 0.85))  # 15% off\n\n  # Show the results\n  return discounted_data.select(\"customerID\", \"totalAmt\", \"state\", \"discountedTotalAmt\").show()\n\ndisplay(no_udf(df))\n```"]