<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10">https://towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0b7f" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="bcc7" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Finding the perfect weights to fit the data in</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">2</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/08286a411c63fdbe68e056f07d6540a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JcF4erc6Ny_zk-rUe6KKXw.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">⛳️ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained: <br/> · <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a> <br/> ▶ <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a> <br/> · <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="92af" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While some probabilistic-based machine learning models (like <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Naive Bayes</a>) make bold assumptions about feature independence, logistic regression takes a more measured approach. Think of it as drawing a line (or plane) that separates two outcomes, allowing us to predict probabilities with a bit more flexibility.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/add544fe281d9cac3d807605d793740a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8yZndCHlD95FZU1YVktgA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="fd58" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="1757" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression is a statistical method used for predicting binary outcomes. Despite its name, it’s used for classification rather than regression. It estimates the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, the model predicts that the instance belongs to that class (or vice versa).</p><h1 id="efbc" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">📊 Dataset Used</h1><p id="ab00" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, we’ll use this artificial golf dataset (inspired by [1]) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><p id="9379" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">Just like in KNN</a>, logistic regression requires the data to be scaled first. <a class="af oc" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">Convert categorical columns</a> into 0 &amp; 1 and also <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">scale the numerical features</a> so that no single feature dominates the distance metric.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/be47d8fc7fb892e46cc3ac4f26cdf8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0yzCcfULaC8BMZeMNDj-Wg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: ‘Outlook’, ‘Temperature’, ‘Humidity’, ‘Wind’ and ‘Play’ (target feature). The categorical columns (Outlook &amp; Windy) are encoded using one-hot encoding while the numerical columns are scaled using standard scaling (z-normalization).</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="a121" class="pm oj fq ob b bg pn po l pp pq"># Import required libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.preprocessing import StandardScaler<br/>import pandas as pd<br/>import numpy as np<br/><br/># Create dataset from dictionary<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data: encode categorical variables<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Split data into features and target<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])<br/>X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])<br/><br/># Print results<br/>print("Training set:")<br/>print(pd.concat([X_train, y_train], axis=1), '\n')<br/>print("Test set:")<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="a805" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="2a2f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression works by applying the logistic function to a linear combination of the input features. Here’s how it operates:</p><ol class=""><li id="20cd" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Calculate a weighted sum of the input features (similar to linear regression).</li><li id="13e3" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Apply the logistic function (also called sigmoid function) to this sum, which maps any real number to a value between 0 and 1.</li><li id="278d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Interpret this value as the probability of belonging to the positive class.</li><li id="a3c8" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Use a threshold (typically 0.5) to make the final classification decision.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a7075b4e966c4487e4ddf0397a705947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H3HruUapK3iNjlEoQUZHNQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For our golf dataset, logistic regression might combine the weather factors into a single score, then transform this score into a probability of playing golf.</figcaption></figure><h1 id="8c09" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="677c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The training process for logistic regression involves finding the best weights for the input features. Here’s the general outline:</p><ol class=""><li id="e2ea" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Initialize the weights (often to small random values).</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3b3d36e76e221058bfb5bdef6dbcb540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5OXW3nXWknMrtVATjThRAg.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="131b" class="pm oj fq ob b bg pn po l pp pq"># Initialize weights (including bias) to 0.1<br/>initial_weights = np.full(X_train_np.shape[1], 0.1)<br/><br/># Create and display DataFrame for initial weights<br/>print(f"Initial Weights: {initial_weights}")</span></pre><p id="ce5c" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. For each training example: <br/>a. Calculate the predicted probability using the current weights.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/7ad914e2c81f2ef22d3423c61b065128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAG76J5rGFZG6QqG2c7Qnw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="a6d9" class="pm oj fq ob b bg pn po l pp pq">def sigmoid(z):<br/>    return 1 / (1 + np.exp(-z))<br/><br/>def calculate_probabilities(X, weights):<br/>    z = np.dot(X, weights)<br/>    return sigmoid(z)<br/><br/>def calculate_log_loss(probabilities, y):<br/>    return -y * np.log(probabilities) - (1 - y) * np.log(1 - probabilities)<br/><br/>def create_output_dataframe(X, y, weights):<br/>    probabilities = calculate_probabilities(X, weights)<br/>    log_losses = calculate_log_loss(probabilities, y)<br/>    <br/>    df = pd.DataFrame({<br/>        'Probability': probabilities,<br/>        'Label': y,<br/>        'Log Loss': log_losses<br/>    })<br/>    <br/>    return df<br/><br/>def calculate_average_log_loss(X, y, weights):<br/>    probabilities = calculate_probabilities(X, weights)<br/>    log_losses = calculate_log_loss(probabilities, y)<br/>    return np.mean(log_losses)<br/><br/># Convert X_train and y_train to numpy arrays for easier computation<br/>X_train_np = X_train.to_numpy()<br/>y_train_np = y_train.to_numpy()<br/><br/># Add a column of 1s to X_train_np for the bias term<br/>X_train_np = np.column_stack((np.ones(X_train_np.shape[0]), X_train_np))<br/><br/># Create and display DataFrame for initial weights<br/>initial_df = create_output_dataframe(X_train_np, y_train_np, initial_weights)<br/>print(initial_df.to_string(index=False, float_format=lambda x: f"{x:.6f}"))<br/>print(f"\nAverage Log Loss: {calculate_average_log_loss(X_train_np, y_train_np, initial_weights):.6f}")</span></pre><p id="cbb0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">b. Compare this probability to the actual class label by calculating its log loss.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pz"><img src="../Images/d03bec8cdef352fdd52c9e1cf0a8985b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjhtvzwT3AH8FOMGFzVIWQ.png"/></div></div></figure><p id="6fe9" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. Update the weights to minimize the loss (usually using some optimization algorithm, like gradient descent. This include repeatedly do Step 2 until log loss cannot get smaller).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/7970c48934b6717e83f2f94d7f770846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miCJmd5CRYZ6roPlIan5nA.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="dd26" class="pm oj fq ob b bg pn po l pp pq">def gradient_descent_step(X, y, weights, learning_rate):<br/>    m = len(y)<br/>    probabilities = calculate_probabilities(X, weights)<br/>    gradient = np.dot(X.T, (probabilities - y)) / m<br/>    new_weights = weights - learning_rate * gradient  # Create new array for updated weights<br/>    return new_weights<br/><br/># Perform one step of gradient descent (one of the simplest optimization algorithm)<br/>learning_rate = 0.1<br/>updated_weights = gradient_descent_step(X_train_np, y_train_np, initial_weights, learning_rate)<br/><br/># Print initial and updated weights<br/>print("\nInitial weights:")<br/>for feature, weight in zip(['Bias'] + list(X_train.columns), initial_weights):<br/>    print(f"{feature:11}: {weight:.2f}")<br/><br/>print("\nUpdated weights after one iteration:")<br/>for feature, weight in zip(['Bias'] + list(X_train.columns), updated_weights):<br/>    print(f"{feature:11}: {weight:.2f}")</span></pre><pre class="qa pj ob pk bp pl bb bk"><span id="2a05" class="pm oj fq ob b bg pn po l pp pq"># With sklearn, you can get the final weights (coefficients)<br/># and final bias (intercepts) easily.<br/># The result is almost the same as doing it manually above.<br/><br/>from sklearn.linear_model import LogisticRegression<br/><br/>lr_clf = LogisticRegression(penalty=None, solver='saga')<br/>lr_clf.fit(X_train, y_train)<br/><br/>coefficients = lr_clf.coef_<br/>intercept = lr_clf.intercept_<br/><br/>y_train_prob = lr_clf.predict_proba(X_train)[:, 1]<br/>loss = -np.mean(y_train * np.log(y_train_prob) + (1 - y_train) * np.log(1 - y_train_prob))<br/><br/>print(f"Weights &amp; Bias Final: {coefficients[0].round(2)}, {round(intercept[0],2)}")<br/>print("Loss Final:", loss.round(3))</span></pre><h1 id="7492" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Classification Steps</h1><p id="3a3c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Once the model is trained:<br/>1. For a new instance, calculate the probability with the final weights (also called coefficients), just like during the training step.</p><p id="5794" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. Interpret the output by seeing the probability: if <em class="qb">p </em>≥ 0.5, predict class 1; otherwise, predict class 0</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/99a363cd8b98d3754e27643122b7c9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4yRW2Q3AF4MHtiZ6OoVvQ.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="5419" class="pm oj fq ob b bg pn po l pp pq"># Calculate prediction probability<br/>predicted_probs = lr_clf.predict_proba(X_test)[:, 1]<br/><br/>z_values = np.log(predicted_probs / (1 - predicted_probs))<br/><br/>result_df = pd.DataFrame({<br/>    'ID': X_test.index,<br/>    'Z-Values': z_values.round(3),<br/>    'Probabilities': predicted_probs.round(3)<br/>}).set_index('ID')<br/><br/>print(result_df)<br/><br/># Make predictions<br/>y_pred = lr_clf.predict(X_test)<br/>print(y_pred)</span></pre><h2 id="5d9e" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Evaluation Step</h2><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c7574523a49fa326eb623dcf5cf0d23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-93pG3rGugpoL_8Clh6Aw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="9a4e" class="pm oj fq ob b bg pn po l pp pq">result_df = pd.DataFrame({<br/>    'ID': X_test.index,<br/>    'Label': y_test,<br/>    'Probabilities': predicted_probs.round(2),<br/>    'Prediction': y_pred,<br/>}).set_index('ID')<br/><br/>print(result_df)</span></pre><h1 id="d8b4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="fa18" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression has several important parameters that control its behavior:</p><p id="93b5" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">1.Penalty</strong>: The type of regularization to use (‘l1’, ‘l2’, ‘elasticnet’, or ‘none’). Regularization in logistic regression prevents overfitting by adding a penalty term to the model’s loss function, that encourages simpler models.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/d6388db7b9a3516fa793253062783429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMyxwwJszKRW1k07zIRDuw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="6e72" class="pm oj fq ob b bg pn po l pp pq">from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score<br/><br/>regs = [None, 'l1', 'l2']<br/>coeff_dict = {}<br/><br/>for reg in regs:<br/>    lr_clf = LogisticRegression(penalty=reg, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[reg] = {<br/>        'Coefficients': coefficients,<br/>        'Intercept': intercept,<br/>        'Loss': loss,<br/>        'Accuracy': accuracy<br/>    }<br/><br/>for reg, vals in coeff_dict.items():<br/>    print(f"{reg}: Coeff: {vals['Coefficients'][0].round(2)}, Intercept: {vals['Intercept'].round(2)}, Loss: {vals['Loss'].round(3)}, Accuracy: {vals['Accuracy'].round(3)}")</span></pre><p id="53e2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">2. Regularization Strength (C)</strong>: Controls the trade-off between fitting the training data and keeping the model simple. A smaller C means stronger regularization.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/b456b69f56e095c9ced88959e77bab79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAZbXPmDECO3ezQElFL2sg.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="bc49" class="pm oj fq ob b bg pn po l pp pq"># List of regularization strengths to try for L1<br/>strengths = [0.001, 0.01, 0.1, 1, 10, 100]<br/><br/>coeff_dict = {}<br/><br/>for strength in strengths:<br/>    lr_clf = LogisticRegression(penalty='l1', C=strength, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/><br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/><br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/><br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[f'L1_{strength}'] = {<br/>        'Coefficients': coefficients[0].round(2),<br/>        'Intercept': round(intercept[0],2),<br/>        'Loss': round(loss,3),<br/>        'Accuracy': round(accuracy*100,2)<br/>    }<br/><br/>print(pd.DataFrame(coeff_dict).T)</span></pre><pre class="qa pj ob pk bp pl bb bk"><span id="3df3" class="pm oj fq ob b bg pn po l pp pq"># List of regularization strengths to try for L2<br/>strengths = [0.001, 0.01, 0.1, 1, 10, 100]<br/><br/>coeff_dict = {}<br/><br/>for strength in strengths:<br/>    lr_clf = LogisticRegression(penalty='l2', C=strength, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/><br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/><br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[f'L2_{strength}'] = {<br/>        'Coefficients': coefficients[0].round(2),<br/>        'Intercept': round(intercept[0],2),<br/>        'Loss': round(loss,3),<br/>        'Accuracy': round(accuracy*100,2)<br/>    }<br/><br/>print(pd.DataFrame(coeff_dict).T)</span></pre><p id="0c37" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">3. Solver:</strong> The algorithm to use for optimization (‘liblinear’, ‘newton-cg’, ‘lbfgs’, ‘sag’, ‘saga’). Some regularization might require a particular algorithm.</p><p id="43b0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">4. Max Iterations</strong>: The maximum number of iterations for the solver to converge.</p><p id="db37" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For our golf dataset, we might start with ‘l2’ penalty, ‘liblinear’ solver, and C=1.0 as a baseline.</p><h1 id="a060" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><p id="6a27" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like any algorithm in machine learning, logistic regression has its strengths and limitations.</p><h2 id="eeaa" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Pros:</h2><ol class=""><li id="436b" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Simplicity</strong>: Easy to implement and understand.</li><li id="9639" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Interpretability</strong>: The weights directly show the importance of each feature.</li><li id="f1ee" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Efficiency</strong>: Doesn’t require too much computational power.</li><li id="5528" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Probabilistic Output</strong>: Provides probabilities rather than just classifications.</li></ol><h2 id="18ad" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Cons:</h2><ol class=""><li id="db1b" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Linearity Assumption</strong>: Assumes a linear relationship between features and log-odds of the outcome.</li><li id="227d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Feature Independence</strong>: Assumes features are not highly correlated.</li><li id="8057" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Limited Complexity</strong>: May underfit in cases where the decision boundary is highly non-linear.</li><li id="1cc6" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Requires More Data</strong>: Needs a relatively large sample size for stable results.</li></ol><p id="82a1" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our golf example, logistic regression might provide a clear, interpretable model of how each weather factor influences the decision to play golf. However, it might struggle if the decision involves complex interactions between weather conditions that can’t be captured by a linear model.</p><h1 id="7fa4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remark</h1><p id="18cd" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression shines as a powerful yet straightforward classification tool. It stands out for its ability to handle complex data while remaining easy to interpret. Unlike <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">some other basic models</a>, it provides smooth probability estimates and works well with many features. In the real world, from predicting customer behavior to medical diagnoses, logistic regression often performs surprisingly well. It’s not just a stepping stone — it’s a reliable model that can match more complex models in many situations.</p><h1 id="ec51" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">🌟 Logistic Regression Code Summarized</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="f564" class="pm oj fq ob b bg pn po l pp pq"># Import required libraries<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data: encode categorical variables<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split data into training and testing sets<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>float_cols = X_train.select_dtypes(include=['float64']).columns<br/>X_train[float_cols] = scaler.fit_transform(X_train[float_cols])<br/>X_test[float_cols] = scaler.transform(X_test[float_cols])<br/><br/># Train the model<br/>lr_clf = LogisticRegression(penalty='l2', C=1, solver='saga')<br/>lr_clf.fit(X_train, y_train)<br/><br/># Make predictions<br/>y_pred = lr_clf.predict(X_test)<br/><br/># Evaluate the model<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre></div></div></div><div class="ab cb qs qt qu qv" role="separator"><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="95e7" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Further Reading</h2><p id="e992" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">Logistic Regression</a> and its implementation in scikit-learn, readers can refer to the official documentation [2], which provides comprehensive information on its usage and parameters.</p><h2 id="940d" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Technical Environment</h2><p id="e985" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="23a5" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">About the Illustrations</h2><p id="6f23" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp ra"><img src="../Images/3954bb576147e89326d88edd83f57b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIWePBeEwMwg4XxSvzfyoA.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary, check out <a class="af oc" href="https://www.instagram.com/p/C_viu8bSXJ8/" rel="noopener ugc nofollow" target="_blank">the companion Instagram post</a>.</figcaption></figure><h2 id="487c" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Reference</h2><p id="a572" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="e1a3" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝘾𝙡𝙖𝙨𝙨𝙞𝙛𝙞𝙘𝙖𝙩𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>