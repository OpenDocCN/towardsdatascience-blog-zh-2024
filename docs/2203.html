<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10">https://towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0b7f" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">CLASSIFICATION ALGORITHM</h2><div/><div><h2 id="bcc7" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Finding the perfect weights to fit the data in</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Sep 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">2</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/08286a411c63fdbe68e056f07d6540a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JcF4erc6Ny_zk-rUe6KKXw.png"/></div></div></figure><p id="c690" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">â›³ï¸ More <a class="af oc" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c" rel="noopener">CLASSIFICATION ALGORITHM</a>, explained: <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e">Dummy Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">K Nearest Neighbor Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Bernoulli Naive Bayes</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c">Gaussian Naive Bayes</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">Decision Tree Classifier</a> <br/> â–¶ <a class="af oc" rel="noopener" target="_blank" href="/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505">Logistic Regression</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9">Support Vector Classifier</a> <br/> Â· <a class="af oc" rel="noopener" target="_blank" href="/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c">Multilayer Perceptron</a></code></p><p id="92af" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While some probabilistic-based machine learning models (like <a class="af oc" rel="noopener" target="_blank" href="/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6">Naive Bayes</a>) make bold assumptions about feature independence, logistic regression takes a more measured approach. Think of it as drawing a line (or plane) that separates two outcomes, allowing us to predict probabilities with a bit more flexibility.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/add544fe281d9cac3d807605d793740a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8yZndCHlD95FZU1YVktgA.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="fd58" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Definition</h1><p id="1757" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression is a statistical method used for predicting binary outcomes. Despite its name, itâ€™s used for classification rather than regression. It estimates the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, the model predicts that the instance belongs to that class (or vice versa).</p><h1 id="efbc" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸ“Š Dataset Used</h1><p id="ab00" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Throughout this article, weâ€™ll use this artificial golf dataset (inspired by [1]) as an example. This dataset predicts whether a person will play golf based on weather conditions.</p><p id="9379" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af oc" rel="noopener" target="_blank" href="/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1">Just like in KNN</a>, logistic regression requires the data to be scaled first. <a class="af oc" rel="noopener" target="_blank" href="/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae">Convert categorical columns</a> into 0 &amp; 1 and also <a class="af oc" rel="noopener" target="_blank" href="/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb">scale the numerical features</a> so that no single feature dominates the distance metric.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/be47d8fc7fb892e46cc3ac4f26cdf8f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0yzCcfULaC8BMZeMNDj-Wg.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">Columns: â€˜Outlookâ€™, â€˜Temperatureâ€™, â€˜Humidityâ€™, â€˜Windâ€™ and â€˜Playâ€™ (target feature). The categorical columns (Outlook &amp; Windy) are encoded using one-hot encoding while the numerical columns are scaled using standard scaling (z-normalization).</figcaption></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="a121" class="pm oj fq ob b bg pn po l pp pq"># Import required libraries<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.preprocessing import StandardScaler<br/>import pandas as pd<br/>import numpy as np<br/><br/># Create dataset from dictionary<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data: encode categorical variables<br/>df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Rearrange columns<br/>column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']<br/>df = df[column_order]<br/><br/># Split data into features and target<br/>X, y = df.drop(columns='Play'), df['Play']<br/><br/># Split data into training and testing sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])<br/>X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])<br/><br/># Print results<br/>print("Training set:")<br/>print(pd.concat([X_train, y_train], axis=1), '\n')<br/>print("Test set:")<br/>print(pd.concat([X_test, y_test], axis=1))</span></pre><h1 id="a805" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Main Mechanism</h1><p id="2a2f" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression works by applying the logistic function to a linear combination of the input features. Hereâ€™s how it operates:</p><ol class=""><li id="20cd" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Calculate a weighted sum of the input features (similar to linear regression).</li><li id="13e3" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Apply the logistic function (also called sigmoid function) to this sum, which maps any real number to a value between 0 and 1.</li><li id="278d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Interpret this value as the probability of belonging to the positive class.</li><li id="a3c8" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Use a threshold (typically 0.5) to make the final classification decision.</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/a7075b4e966c4487e4ddf0397a705947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H3HruUapK3iNjlEoQUZHNQ.png"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For our golf dataset, logistic regression might combine the weather factors into a single score, then transform this score into a probability of playing golf.</figcaption></figure><h1 id="8c09" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Training Steps</h1><p id="677c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">The training process for logistic regression involves finding the best weights for the input features. Hereâ€™s the general outline:</p><ol class=""><li id="e2ea" class="nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">Initialize the weights (often to small random values).</li></ol><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/3b3d36e76e221058bfb5bdef6dbcb540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5OXW3nXWknMrtVATjThRAg.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="131b" class="pm oj fq ob b bg pn po l pp pq"># Initialize weights (including bias) to 0.1<br/>initial_weights = np.full(X_train_np.shape[1], 0.1)<br/><br/># Create and display DataFrame for initial weights<br/>print(f"Initial Weights: {initial_weights}")</span></pre><p id="ce5c" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. For each training example: <br/>a. Calculate the predicted probability using the current weights.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/7ad914e2c81f2ef22d3423c61b065128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAG76J5rGFZG6QqG2c7Qnw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="a6d9" class="pm oj fq ob b bg pn po l pp pq">def sigmoid(z):<br/>    return 1 / (1 + np.exp(-z))<br/><br/>def calculate_probabilities(X, weights):<br/>    z = np.dot(X, weights)<br/>    return sigmoid(z)<br/><br/>def calculate_log_loss(probabilities, y):<br/>    return -y * np.log(probabilities) - (1 - y) * np.log(1 - probabilities)<br/><br/>def create_output_dataframe(X, y, weights):<br/>    probabilities = calculate_probabilities(X, weights)<br/>    log_losses = calculate_log_loss(probabilities, y)<br/>    <br/>    df = pd.DataFrame({<br/>        'Probability': probabilities,<br/>        'Label': y,<br/>        'Log Loss': log_losses<br/>    })<br/>    <br/>    return df<br/><br/>def calculate_average_log_loss(X, y, weights):<br/>    probabilities = calculate_probabilities(X, weights)<br/>    log_losses = calculate_log_loss(probabilities, y)<br/>    return np.mean(log_losses)<br/><br/># Convert X_train and y_train to numpy arrays for easier computation<br/>X_train_np = X_train.to_numpy()<br/>y_train_np = y_train.to_numpy()<br/><br/># Add a column of 1s to X_train_np for the bias term<br/>X_train_np = np.column_stack((np.ones(X_train_np.shape[0]), X_train_np))<br/><br/># Create and display DataFrame for initial weights<br/>initial_df = create_output_dataframe(X_train_np, y_train_np, initial_weights)<br/>print(initial_df.to_string(index=False, float_format=lambda x: f"{x:.6f}"))<br/>print(f"\nAverage Log Loss: {calculate_average_log_loss(X_train_np, y_train_np, initial_weights):.6f}")</span></pre><p id="cbb0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">b. Compare this probability to the actual class label by calculating its log loss.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pz"><img src="../Images/d03bec8cdef352fdd52c9e1cf0a8985b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjhtvzwT3AH8FOMGFzVIWQ.png"/></div></div></figure><p id="6fe9" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3. Update the weights to minimize the loss (usually using some optimization algorithm, like gradient descent. This include repeatedly do Step 2 until log loss cannot get smaller).</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/7970c48934b6717e83f2f94d7f770846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miCJmd5CRYZ6roPlIan5nA.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="dd26" class="pm oj fq ob b bg pn po l pp pq">def gradient_descent_step(X, y, weights, learning_rate):<br/>    m = len(y)<br/>    probabilities = calculate_probabilities(X, weights)<br/>    gradient = np.dot(X.T, (probabilities - y)) / m<br/>    new_weights = weights - learning_rate * gradient  # Create new array for updated weights<br/>    return new_weights<br/><br/># Perform one step of gradient descent (one of the simplest optimization algorithm)<br/>learning_rate = 0.1<br/>updated_weights = gradient_descent_step(X_train_np, y_train_np, initial_weights, learning_rate)<br/><br/># Print initial and updated weights<br/>print("\nInitial weights:")<br/>for feature, weight in zip(['Bias'] + list(X_train.columns), initial_weights):<br/>    print(f"{feature:11}: {weight:.2f}")<br/><br/>print("\nUpdated weights after one iteration:")<br/>for feature, weight in zip(['Bias'] + list(X_train.columns), updated_weights):<br/>    print(f"{feature:11}: {weight:.2f}")</span></pre><pre class="qa pj ob pk bp pl bb bk"><span id="2a05" class="pm oj fq ob b bg pn po l pp pq"># With sklearn, you can get the final weights (coefficients)<br/># and final bias (intercepts) easily.<br/># The result is almost the same as doing it manually above.<br/><br/>from sklearn.linear_model import LogisticRegression<br/><br/>lr_clf = LogisticRegression(penalty=None, solver='saga')<br/>lr_clf.fit(X_train, y_train)<br/><br/>coefficients = lr_clf.coef_<br/>intercept = lr_clf.intercept_<br/><br/>y_train_prob = lr_clf.predict_proba(X_train)[:, 1]<br/>loss = -np.mean(y_train * np.log(y_train_prob) + (1 - y_train) * np.log(1 - y_train_prob))<br/><br/>print(f"Weights &amp; Bias Final: {coefficients[0].round(2)}, {round(intercept[0],2)}")<br/>print("Loss Final:", loss.round(3))</span></pre><h1 id="7492" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Classification Steps</h1><p id="3a3c" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Once the model is trained:<br/>1. For a new instance, calculate the probability with the final weights (also called coefficients), just like during the training step.</p><p id="5794" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2. Interpret the output by seeing the probability: if <em class="qb">p </em>â‰¥ 0.5, predict class 1; otherwise, predict class 0</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/99a363cd8b98d3754e27643122b7c9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4yRW2Q3AF4MHtiZ6OoVvQ.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="5419" class="pm oj fq ob b bg pn po l pp pq"># Calculate prediction probability<br/>predicted_probs = lr_clf.predict_proba(X_test)[:, 1]<br/><br/>z_values = np.log(predicted_probs / (1 - predicted_probs))<br/><br/>result_df = pd.DataFrame({<br/>    'ID': X_test.index,<br/>    'Z-Values': z_values.round(3),<br/>    'Probabilities': predicted_probs.round(3)<br/>}).set_index('ID')<br/><br/>print(result_df)<br/><br/># Make predictions<br/>y_pred = lr_clf.predict(X_test)<br/>print(y_pred)</span></pre><h2 id="5d9e" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Evaluation Step</h2><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/c7574523a49fa326eb623dcf5cf0d23c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-93pG3rGugpoL_8Clh6Aw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="9a4e" class="pm oj fq ob b bg pn po l pp pq">result_df = pd.DataFrame({<br/>    'ID': X_test.index,<br/>    'Label': y_test,<br/>    'Probabilities': predicted_probs.round(2),<br/>    'Prediction': y_pred,<br/>}).set_index('ID')<br/><br/>print(result_df)</span></pre><h1 id="d8b4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Key Parameters</h1><p id="fa18" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression has several important parameters that control its behavior:</p><p id="93b5" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">1.Penalty</strong>: The type of regularization to use (â€˜l1â€™, â€˜l2â€™, â€˜elasticnetâ€™, or â€˜noneâ€™). Regularization in logistic regression prevents overfitting by adding a penalty term to the modelâ€™s loss function, that encourages simpler models.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/d6388db7b9a3516fa793253062783429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMyxwwJszKRW1k07zIRDuw.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="6e72" class="pm oj fq ob b bg pn po l pp pq">from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score<br/><br/>regs = [None, 'l1', 'l2']<br/>coeff_dict = {}<br/><br/>for reg in regs:<br/>    lr_clf = LogisticRegression(penalty=reg, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[reg] = {<br/>        'Coefficients': coefficients,<br/>        'Intercept': intercept,<br/>        'Loss': loss,<br/>        'Accuracy': accuracy<br/>    }<br/><br/>for reg, vals in coeff_dict.items():<br/>    print(f"{reg}: Coeff: {vals['Coefficients'][0].round(2)}, Intercept: {vals['Intercept'].round(2)}, Loss: {vals['Loss'].round(3)}, Accuracy: {vals['Accuracy'].round(3)}")</span></pre><p id="53e2" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">2. Regularization Strength (C)</strong>: Controls the trade-off between fitting the training data and keeping the model simple. A smaller C means stronger regularization.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/b456b69f56e095c9ced88959e77bab79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAZbXPmDECO3ezQElFL2sg.png"/></div></div></figure><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="bc49" class="pm oj fq ob b bg pn po l pp pq"># List of regularization strengths to try for L1<br/>strengths = [0.001, 0.01, 0.1, 1, 10, 100]<br/><br/>coeff_dict = {}<br/><br/>for strength in strengths:<br/>    lr_clf = LogisticRegression(penalty='l1', C=strength, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/><br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/><br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/><br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[f'L1_{strength}'] = {<br/>        'Coefficients': coefficients[0].round(2),<br/>        'Intercept': round(intercept[0],2),<br/>        'Loss': round(loss,3),<br/>        'Accuracy': round(accuracy*100,2)<br/>    }<br/><br/>print(pd.DataFrame(coeff_dict).T)</span></pre><pre class="qa pj ob pk bp pl bb bk"><span id="3df3" class="pm oj fq ob b bg pn po l pp pq"># List of regularization strengths to try for L2<br/>strengths = [0.001, 0.01, 0.1, 1, 10, 100]<br/><br/>coeff_dict = {}<br/><br/>for strength in strengths:<br/>    lr_clf = LogisticRegression(penalty='l2', C=strength, solver='saga')<br/>    lr_clf.fit(X_train, y_train)<br/><br/>    coefficients = lr_clf.coef_<br/>    intercept = lr_clf.intercept_<br/><br/>    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]<br/>    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))<br/>    predictions = lr_clf.predict(X_test)<br/>    accuracy = accuracy_score(y_test, predictions)<br/><br/>    coeff_dict[f'L2_{strength}'] = {<br/>        'Coefficients': coefficients[0].round(2),<br/>        'Intercept': round(intercept[0],2),<br/>        'Loss': round(loss,3),<br/>        'Accuracy': round(accuracy*100,2)<br/>    }<br/><br/>print(pd.DataFrame(coeff_dict).T)</span></pre><p id="0c37" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">3. Solver:</strong> The algorithm to use for optimization (â€˜liblinearâ€™, â€˜newton-cgâ€™, â€˜lbfgsâ€™, â€˜sagâ€™, â€˜sagaâ€™). Some regularization might require a particular algorithm.</p><p id="43b0" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne ga">4. Max Iterations</strong>: The maximum number of iterations for the solver to converge.</p><p id="db37" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For our golf dataset, we might start with â€˜l2â€™ penalty, â€˜liblinearâ€™ solver, and C=1.0 as a baseline.</p><h1 id="a060" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Pros &amp; Cons</h1><p id="6a27" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Like any algorithm in machine learning, logistic regression has its strengths and limitations.</p><h2 id="eeaa" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Pros:</h2><ol class=""><li id="436b" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Simplicity</strong>: Easy to implement and understand.</li><li id="9639" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Interpretability</strong>: The weights directly show the importance of each feature.</li><li id="f1ee" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Efficiency</strong>: Doesnâ€™t require too much computational power.</li><li id="5528" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Probabilistic Output</strong>: Provides probabilities rather than just classifications.</li></ol><h2 id="18ad" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Cons:</h2><ol class=""><li id="db1b" class="nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pr ps pt bk"><strong class="ne ga">Linearity Assumption</strong>: Assumes a linear relationship between features and log-odds of the outcome.</li><li id="227d" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Feature Independence</strong>: Assumes features are not highly correlated.</li><li id="8057" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Limited Complexity</strong>: May underfit in cases where the decision boundary is highly non-linear.</li><li id="1cc6" class="nc nd fq ne b gt pu ng nh gw pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk"><strong class="ne ga">Requires More Data</strong>: Needs a relatively large sample size for stable results.</li></ol><p id="82a1" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our golf example, logistic regression might provide a clear, interpretable model of how each weather factor influences the decision to play golf. However, it might struggle if the decision involves complex interactions between weather conditions that canâ€™t be captured by a linear model.</p><h1 id="7fa4" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Final Remark</h1><p id="18cd" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Logistic regression shines as a powerful yet straightforward classification tool. It stands out for its ability to handle complex data while remaining easy to interpret. Unlike <a class="af oc" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e">some other basic models</a>, it provides smooth probability estimates and works well with many features. In the real world, from predicting customer behavior to medical diagnoses, logistic regression often performs surprisingly well. Itâ€™s not just a stepping stone â€” itâ€™s a reliable model that can match more complex models in many situations.</p><h1 id="ec51" class="oi oj fq bf ok ol om gv on oo op gy oq or os ot ou ov ow ox oy oz pa pb pc pd bk">ğŸŒŸ Logistic Regression Code Summarized</h1><pre class="mr ms mt mu mv pj ob pk bp pl bb bk"><span id="f564" class="pm oj fq ob b bg pn po l pp pq"># Import required libraries<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.metrics import accuracy_score<br/><br/># Load the dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],<br/>    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']<br/>}<br/>df = pd.DataFrame(dataset_dict)<br/><br/># Prepare data: encode categorical variables<br/>df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df['Play'] = (df['Play'] == 'Yes').astype(int)<br/><br/># Split data into training and testing sets<br/>X, y = df.drop(columns='Play'), df['Play']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>scaler = StandardScaler()<br/>float_cols = X_train.select_dtypes(include=['float64']).columns<br/>X_train[float_cols] = scaler.fit_transform(X_train[float_cols])<br/>X_test[float_cols] = scaler.transform(X_test[float_cols])<br/><br/># Train the model<br/>lr_clf = LogisticRegression(penalty='l2', C=1, solver='saga')<br/>lr_clf.fit(X_train, y_train)<br/><br/># Make predictions<br/>y_pred = lr_clf.predict(X_test)<br/><br/># Evaluate the model<br/>print(f"Accuracy: {accuracy_score(y_test, y_pred)}")</span></pre></div></div></div><div class="ab cb qs qt qu qv" role="separator"><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy qz"/><span class="qw by bm qx qy"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="95e7" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Further Reading</h2><p id="e992" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">For a detailed explanation of the <a class="af oc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">Logistic Regression</a> and its implementation in scikit-learn, readers can refer to the official documentation [2], which provides comprehensive information on its usage and parameters.</p><h2 id="940d" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Technical Environment</h2><p id="e985" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions.</p><h2 id="23a5" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">About the Illustrations</h2><p id="6f23" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp ra"><img src="../Images/3954bb576147e89326d88edd83f57b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cIWePBeEwMwg4XxSvzfyoA.jpeg"/></div></div><figcaption class="od oe of mo mp og oh bf b bg z dx">For a concise visual summary, check out <a class="af oc" href="https://www.instagram.com/p/C_viu8bSXJ8/" rel="noopener ugc nofollow" target="_blank">the companion Instagram post</a>.</figcaption></figure><h2 id="487c" class="qc oj fq bf ok qd qe qf on qg qh qi oq nl qj qk ql np qm qn qo nt qp qq qr fw bk">Reference</h2><p id="a572" class="pw-post-body-paragraph nc nd fq ne b gt pe ng nh gw pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx fj bk">[1] T. M. Mitchell, <a class="af oc" href="https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html" rel="noopener ugc nofollow" target="_blank">Machine Learning</a> (1997), McGraw-Hill Science/Engineering/Math, pp. 59</p><p id="e1a3" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><p id="595f" class="pw-post-body-paragraph nc nd fq ne b gt nf ng nh gw ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><div class="rb rc rd re rf"><div role="button" tabindex="0" class="ab bx cp kj it rg rh bp ri lw ao"><div class="rj l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rk rl cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rk rl em n ay tw"/></div><div class="rm l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq rp hp l"><h2 class="bf ga wt ic it wu iv iw wv iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk ww vv vw vx vy lj vz wa uh ii wb wc wd ul um un ep bm uo oe" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----81baf5871505--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wx l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="ry dz rz it ab sa il ed"><div class="ed rs bx rt ru"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed rs bx kk rv rw"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx rx rw"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>