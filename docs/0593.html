<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Training CausalLM Models Part 1: What Actually Is CausalLM?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Training CausalLM Models Part 1: What Actually Is CausalLM?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-causallm-models-part-1-what-actually-is-causallm-6c3efb2490ec?source=collection_archive---------4-----------------------#2024-03-04">https://towardsdatascience.com/training-causallm-models-part-1-what-actually-is-causallm-6c3efb2490ec?source=collection_archive---------4-----------------------#2024-03-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="b28c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The first part of a practical guide to using HuggingFace’s CausalLM class</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://tlebryk.medium.com/?source=post_page---byline--6c3efb2490ec--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Theo Lebryk" class="l ep by dd de cx" src="../Images/c2e0d606f4a99831fad5575f59848544.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*FQIW0ZdFzdQBfDWgWzsh1Q.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6c3efb2490ec--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://tlebryk.medium.com/?source=post_page---byline--6c3efb2490ec--------------------------------" rel="noopener follow">Theo Lebryk</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6c3efb2490ec--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/2a68f7d550af4571fd5c25099893c910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NoYeJ4fe_I0JsEjvyRMYWw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Causal langauge models model each new word as a function of all previous words. Source: <a class="af nb" href="https://www.pexels.com/photo/dog-eating-a-carrot-5255204/" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="c3dd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’ve played around with recent models on <a class="af nb" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>, chances are you encountered a causal language model. When you pull up the documentation for a <a class="af nb" href="https://huggingface.co/docs/transformers/main/en/model_doc/llama" rel="noopener ugc nofollow" target="_blank">model</a> family, you’ll get a page with “tasks” like LlamaForCausalLM or LlamaForSequenceClassification.</p><p id="08c0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’re like me, going from that documentation to actually finetuning a model can be a bit confusing. We’re going to focus on CausalLM, starting by explaining what CausalLM is in this post followed by a practical example of how to finetune a CausalLM model in a subsequent post.</p><h2 id="5299" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Background: Encoders and Decoders</h2><p id="a17d" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Many of the best models today such as LLAMA-2, GPT-2, or Falcon are “decoder-only” models. A decoder-only model:</p><ol class=""><li id="55ef" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oy oz pa bk">takes a sequence of <em class="pb">previous </em>tokens (AKA a prompt)</li><li id="d703" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oy oz pa bk">runs those tokens through the model (often creating embeddings from tokens and running them through transformer blocks)</li><li id="3490" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oy oz pa bk">outputs a single output (usually the probability of the next token).</li></ol><p id="4650" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is contrasted with models with “encoder-only” or hybrid “encoder-decoder” architectures which will input the <em class="pb">entire </em>sequence, not just <em class="pb">previous </em>tokens. This difference disposes the two architectures towards different tasks. Decoder models are designed for the generative task of writing new text. Encoder models are designed for tasks which require looking at a full sequence such as translation or sequence classification. Things get murky because you can repurpose a decoder-only model to do translation or use an encoder-only model to generate new text. Sebastian Raschka has a nice <a class="af nb" href="https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder" rel="noopener ugc nofollow" target="_blank">guide</a> if you want to dig more into encoders vs decoders. There’s a also a <a class="af nb" rel="noopener" target="_blank" href="/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5">medium article</a> which goes more in-depth into the differeneces between masked langauge modeling and causal langauge modeling.</p><p id="5129" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For our purposes, all you need to know is that:</p><ol class=""><li id="5bb8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oy oz pa bk">CausalLM models generally are decoder-only models</li><li id="846a" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oy oz pa bk">Decoder-only models look at past tokens to predict the next token</li></ol><p id="3510" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With decoder-only language models, we can think of the next token prediction process as “causal language modeling” because the previous tokens “cause” each additional token.</p><h2 id="295f" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">HuggingFace CausalLM</h2><p id="ad5b" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">In HuggingFace world, CausalLM (LM stands for language modeling) is a class of models which take a prompt and predict new tokens. In reality, we’re predicting one token at a time, but the class abstracts away the tediousness of having to loop through sequences one token at a time. During inference, CausalLMs will iteratively predict individual tokens until some stopping condition at which point the model returns the final concatenated tokens.</p><p id="96d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During training, something similar happens where we give the model a sequence of tokens we want to learn. We start by predicting the second token given the first one, then the third token given the first two tokens and so on.</p><p id="736e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Thus, if you want to learn how to predict the sentence “the dog likes food,” assuming each word is a token, you’re making 3 predictions:</p><ol class=""><li id="f2cd" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oy oz pa bk">“the” → dog,</li><li id="a3d9" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oy oz pa bk">“the dog” → likes</li><li id="8b16" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx oy oz pa bk">“the dog likes” → food</li></ol><p id="ef34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">During training, you can think about each of the three snapshots of the sentence as three observations in your training dataset. Manually splitting long sequences into individual rows for each token in a sequence would be tedious, so HuggingFace handles it for you.</p><p id="1461" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As long as you give it a sequence of tokens, it will break out that sequence into individual single token predictions behind the scenes.</p><p id="e5e9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can create this ‘sequence of tokens’ by running a regular string through the model’s tokenizer. The tokenizer will output a dictionary-like object with <em class="pb">input_ids </em>and an <em class="pb">attention_mask </em>as keys, like with any ordinary HuggingFace model.</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="8f76" class="pl nz fq pi b bg pm pn l po pp">from transformers import AutoTokenizer<br/>tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-560m")<br/>tokenizer("the dog likes food")<br/>&gt;&gt;&gt; {'input_ids': [5984, 35433, 114022, 17304], 'attention_mask': [1, 1, 1, 1]}</span></pre><p id="e4b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With CausalLM models, there’s one additional step where the model expects a <em class="pb">labels </em>key. During training, we use the “previous” <em class="pb">input_ids </em>to predict the “current” <em class="pb">labels </em>token. However, you do <strong class="ne fr">not </strong>want to think about labels like a question answering model where the first index of <em class="pb">labels </em>corresponds with the answer to the <em class="pb">input_ids </em>(i.e. that the <em class="pb">labels </em>should be concatenated to the end of the <em class="pb">input_ids</em>). Rather, you want <em class="pb">labels </em>and <em class="pb">input_ids </em>to mirror each other with identical shapes. In algebraic notation, to predict <em class="pb">labels </em>token at index k, we use all the <em class="pb">input_ids </em>through the k-1 index.</p><p id="d26d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If this is confusing, practically, you can usually just make <em class="pb">labels </em>an identical copy of <em class="pb">input_ids </em>and call it a day. If you do want to understand what’s going on, we’ll walk through an example.</p><h2 id="ab85" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">A quick worked example</h2><p id="9343" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Let’s go back to “the dog likes food.” For simplicity, let’s leave the words as words rather than assigning them to token numbers, but in practice these would be numbers which you can map back to their true string representation using the tokenizer.</p><p id="73d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our input for a single element batch would look like this:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="592f" class="pl nz fq pi b bg pm pn l po pp">{<br/>    "input_ids": [["the", "dog", "likes", "food"]],<br/>    "attention_mask": [[1, 1, 1, 1]],<br/>    "labels": [["the", "dog", "likes", "food"]],<br/>}</span></pre><p id="97b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The double brackets denote that technically the shape for the arrays for each key is batch_size x sequence_size. To keep things simple, we can ignore batching and just treat them like one dimensional vectors.</p><p id="e468" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Under the hood, if the model is predicting the kth token in a sequence, it will do so kind of like so:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="102a" class="pl nz fq pi b bg pm pn l po pp">pred_token_k = model(input_ids[:k]*attention_mask[:k]^T)</span></pre><blockquote class="pq pr ps"><p id="5695" class="nc nd pb ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note this is pseudocode.</p></blockquote><p id="c930" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can ignore the attention mask for our purposes. For CausalLM models, we usually want the attention mask to be all 1s because we want to attend to all previous tokens. Also note that [:k] really means we use the 0th index through the k-1 index because the ending index in slicing is <em class="pb">exclusive</em>.</p><p id="fbd8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With that in mind, we have:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="7c65" class="pl nz fq pi b bg pm pn l po pp">pred_token_k = model(input_ids[:k])</span></pre><p id="51c4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The loss would be taken by comparing the true value of <em class="pb">labels[k]</em> with <em class="pb">pred_token_k</em>.</p><p id="0cb1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In reality, both get represented as 1xv vectors where v is the size of the vocabulary. Each element represents the probability of that token. For the predictions (<em class="pb">pred_token_k</em>), these are real probabilities the model predicts. For the true label (<em class="pb">labels[k]</em>), we can artificially make it the correct shape by making a vector with 1 for the actual true token and 0 for all other tokens in the vocabulary.</p><p id="fa4e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s say we’re predicting the second word of our sample sentence, meaning k=1 (we’re zero indexing k). The first bullet item is the context we use to generate a prediction and the second bullet item is the true label token we’re aiming to predict.</p><p id="ea56" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">k=1:</p><ul class=""><li id="0ae4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pt oz pa bk">Input_ids[:1] == [the]</li><li id="59a1" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx pt oz pa bk">Labels[1] == dog</li></ul><p id="058a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">k=2:</p><ul class=""><li id="123b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pt oz pa bk">Input_ids[:2] == [the, dog]</li><li id="c730" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx pt oz pa bk">Labels[2] == likes</li></ul><p id="4623" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">k =3:</p><ul class=""><li id="1181" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pt oz pa bk">Input_ids[:3] == [the, dog, likes]</li><li id="9338" class="nc nd fq ne b go pc ng nh gr pd nj nk nl pe nn no np pf nr ns nt pg nv nw nx pt oz pa bk">Labels[3] == food</li></ul><p id="3f33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s say k=3 and we feed the model “[the, dog, likes]”. The model outputs:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="c871" class="pl nz fq pi b bg pm pn l po pp">[P(dog)=10%, P(food)=60%,P(likes)=0%, P(the)=30%]</span></pre><p id="52a1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In other words, the model thinks there’s a 10% chance the next token is “dog,” 60% chance the next token is “food” and 30% chance the next token is “the.”</p><p id="c63a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The true label could be represented as:</p><pre class="ml mm mn mo mp ph pi pj bp pk bb bk"><span id="cc58" class="pl nz fq pi b bg pm pn l po pp">[P(dog)=0%, P(food)=100%, P(likes)=0%, P(the)=0%]</span></pre><figure class="ml mm mn mo mp mq"><div class="pu io l ed"><div class="pv pw l"/></div></figure><p id="f2d4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In real training, we’d use a loss function like cross-entropy. To keep it as intuitive as possible, let’s just use absolute difference to get an approximate feel for loss. By absolute difference, I mean the absolute value of the difference between the predicted probability and our “true” probability: e.g. <em class="pb">absolute_diff_dog = |0.10–0.00| = 0.10</em>.</p><p id="4426" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Even with this crude loss function, you can see that to minimize the loss we want to predict a high probability for the actual label (e.g. food) and low probabilities for all other tokens in the vocabulary.</p><p id="3005" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For instance, let’s say after training, when we ask our model to predict the next token given [the, dog, likes], our outputs look like the following:</p><figure class="ml mm mn mo mp mq"><div class="pu io l ed"><div class="pv pw l"/></div></figure><p id="98ca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now our loss is smaller now that we’ve learned to predict “food” with high probability given those inputs.</p><p id="6e28" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Training would just be repeating this process of trying to align the predicted probabilities with the true next token for all the tokens in your training sequences.</p><h2 id="5306" class="ny nz fq bf oa ob oc od oe of og oh oi nl oj ok ol np om on oo nt op oq or os bk">Conclusion</h2><p id="b798" class="pw-post-body-paragraph nc nd fq ne b go ot ng nh gr ou nj nk nl ov nn no np ow nr ns nt ox nv nw nx fj bk">Hopefully you’re getting an intuition about what’s happening under the hood to train a CausalLM model using HuggingFace. You might have some questions like “why do we need <em class="pb">labels </em>as a separate array when we could just use the kth index of <em class="pb">input_ids </em>directly at each step? Is there any case when <em class="pb">labels </em>would be different than <em class="pb">input_ids</em>?”</p><p id="bf31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I’m going to leave you to think about those questions and stop there for now. We’ll pick back up with answers and real code in the next post!</p></div></div></div></div>    
</body>
</html>