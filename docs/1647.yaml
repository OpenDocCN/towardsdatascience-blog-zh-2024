- en: 'PySpark Explained: Four Ways to Create and Populate DataFrames'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04](https://towardsdatascience.com/pyspark-explained-four-ways-to-create-and-populate-dataframes-31f3e4322ad9?source=collection_archive---------3-----------------------#2024-07-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/49e4e3686f9183a0297651c636080f94.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by AI (Dalle-3)
  prefs: []
  type: TYPE_NORMAL
- en: 'From CSVs to databases: loading data into PySpark DataFrames'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[![Thomas
    Reid](../Images/c1b4e5f577272633ba07e5dbfd21c02d.png)](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    [Thomas Reid](https://medium.com/@thomas_reid?source=post_page---byline--31f3e4322ad9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31f3e4322ad9--------------------------------)
    ·11 min read·Jul 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: When using PySpark, especially if you have a background in SQL, one of the first
    things you’ll want to do is get the data you want to process into a DataFrame.
    Once the data is in a DataFrame, it’s easy to create a temporary view (or permanent
    table) from the DataFrame. At that stage, all of PySpark SQL's rich set of operations
    becomes available for you to use to further explore and process the data.
  prefs: []
  type: TYPE_NORMAL
- en: Since many standard SQL skills are easily transferable to PySpark SQL, it’s
    crucial to prepare your data for direct use with PySpark SQL as early as possible
    in your processing pipeline. Doing this should be a top priority for efficient
    data handling and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t ***have*** to do this of course, as anything you can do with PySpark
    SQL on views or tables can be done directly on DataFrames too using the API. But
    as someone who is far more comfortable using SQL than the DataFrame API, my goto
    process when using Spark has always been,
  prefs: []
  type: TYPE_NORMAL
- en: '**input data -> DataFrame-> temporary view-> SQL processing**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To help you with this process, this article will discuss the first part of this
    pipeline, i.e. getting your data into DataFrames, by showcasing four of…
  prefs: []
  type: TYPE_NORMAL
