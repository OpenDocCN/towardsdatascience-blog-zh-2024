<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Constrained Sentence Generation Using Gibbs Sampling and BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Constrained Sentence Generation Using Gibbs Sampling and BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19">https://towardsdatascience.com/constrained-sentence-generation-using-gibbs-sampling-and-bert-8d326b9027b1?source=collection_archive---------6-----------------------#2024-07-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1dbc" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A fast and effective approach to generating fluent sentences from given keywords using public pre-trained models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Sergey Vilov" class="l ep by dd de cx" src="../Images/42efe223e2aa575250e050cf3660cf20.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*rdiaYHfIcYB6R4M9"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@sergeyvilov?source=post_page---byline--8d326b9027b1--------------------------------" rel="noopener follow">Sergey Vilov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8d326b9027b1--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/9c688d373215ed1941eb26ed02e66cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zIJzWLq1Hl7Eh-FZ"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Brett Jordan</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="95e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://en.wikipedia.org/wiki/Large_language_model" rel="noopener ugc nofollow" target="_blank">Large language models</a>, like <a class="af nb" href="https://en.wikipedia.org/wiki/Generative_pre-trained_transformer" rel="noopener ugc nofollow" target="_blank">GPT</a>, have achieved unprecedented results in <em class="ny">free-form</em> text generation. They’re widely used for writing e-mails, copyrighting, or storytelling. However, their success in <em class="ny">constrained</em> text generation remains limited [1].</p><p id="0efe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Constrained text generation involves producing sentences with specific attributes like sentiment, tense, template, or style. We will consider one specific kind of constrained text generation, namely keyword-based generation. In this task, it is required that the model generate sentences that include given keywords. Depending on the application, these sentences should (a) contain all the keywords (i.e. assure high coverage) (b) be grammatically correct (c) respect common sense (d) exhibit lexical and grammatical diversity.</p><p id="5a38" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For auto-regressive forward generation models, like GPT, constrained generation is particularly challenging. These models yield tokens sequentially from left to right, one at a time. By design, they lack precise control over the generated sequence and struggle to support constraints at arbitrary positions in the output or constraints involving multiple keywords. As a result, these models usually exhibit poor coverage (a) and diversity (d), while providing fluent sentences (b,c). Although some sampling strategies, like dynamic beam allocation [2], were specifically designed to improve constrained text generation with forward models, they demonstrated inferior results in independent testing [3].</p><p id="a042" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An alternative approach [4], known as CGMH, consists in constructing the sentence iteratively by executing elementary operations on the existing sequence, such as word deletion, insertion, or replacement. The initial sequence is usually an ordered sequence of given keywords. Because of the vast search space, such methods often struggle to produce a meaningful sentence within a reasonable time frame. Therefore, although these models may ensure good coverage (a) and diversity (d), they might fail to satisfy fluency requirements (b,c). To overcome these problems, it was suggested to restrict the search space by including a differentiable loss function [5] or a pre-trained neural network [6] to guide the sampler. However, these adjustments did not lead to any practically significant improvement compared to CGMH.</p><p id="278e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the following, we will propose a new approach to generating sentences with given keywords. The idea is to limit the search space by starting from a correct sentence and reducing the set of possible operations. It turns out that when only the replacement operation is considered, the BERT model provides a convenient way to generate desired sentences via Gibbs sampling.</p><h1 id="e0c5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Gibbs sampling from BERT</h1><p id="c73c" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Sampling sentences via <a class="af nb" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank">Gibbs sampling</a> from <a class="af nb" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank">BERT</a> was first proposed in [7]. Here, we adapt this idea for constrained sentence generation.</p><p id="bc49" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To simplify theoretical introduction, we will start by explaining the grounds of the CGMH approach [4], which uses the <a class="af nb" href="https://en.wikipedia.org/wiki/Metropolis–Hastings_algorithm" rel="noopener ugc nofollow" target="_blank">Metropolis-Hastings algorithm</a> to sample from a sentence distribution satisfying the given constraints.</p><p id="57e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The sampler starts from a given sequence of keywords. At each step, a random position in the current sentence is selected and one of the three possible actions (chosen with probability <em class="ny">p</em>=1/3) is executed: insertion, deletion, or word replacement. After that, a candidate sentence is sampled from the corresponding proposal distribution. In particular, the proposal distribution for replacement takes up the form:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pa"><img src="../Images/463f34fa318c22f5a9d63318be308e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9yahhqxzNOqi5Y1BQR1BQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(image by the author)</figcaption></figure><p id="8dac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where <em class="ny">x</em> is the current sentence, <em class="ny">x’</em> is a candidate sentence, <em class="ny">w_1</em>…<em class="ny">w_n</em> are the words in the sentence, <em class="ny">w^c</em> is the proposed word, <em class="ny">V</em> is the dictionary size, and π is the sampled distribution. The candidate sentence can then be either accepted or rejected using the acceptance rate:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pb"><img src="../Images/7c3776efb5b375b1a1f94b2ee4cab722.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*1_6d0roa9XEBpv3XkVsRJw.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(image by the author)</figcaption></figure><p id="8b36" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To get a sentence probability, the authors propose to use a simple seq2seq LSTM-based network:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pc"><img src="../Images/d2180dc7f1fd9c9400ad22d542596b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*6srAa3TR1gUiq1gI8Ecq_w.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(Image by the author)</figcaption></figure><p id="4672" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">where <em class="ny">p_LM(x) </em>is the sentence probability given by a language model and <em class="ny">χ(x)</em> is an indicator function, which is 1 when all of the keyword words are included in the sentence and 0 otherwise.</p><p id="8cff" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When keyword constraints are imposed, the generation starts from a given sequence of keywords. These words are then excluded from deletion and replacement operations. After a certain time (<a class="af nb" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank">the burn-in period</a>), generation converges to a stationary distribution.</p><p id="eca7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As noted above, a weak point of such methods is the large search space that prevents them from generating meaningful sentences within a reasonable time. We will now reduce the search space by completely eliminating insertions and deletions from sentence generation.</p><p id="1c9d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Ok, but what does this have to do with Gibbs sampling and BERT?</p><p id="4f73" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af nb" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank">Citing Wikipedia</a>, Gibbs sampling is used when the joint distribution is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable is known and is easy (or at least, easier) to sample from.</p><p id="554c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">BERT is a transformer-based model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context, enabling it to understand the context of a word based on its surroundings. For us, it is particularly important that BERT is trained in a masked language model fashion, i.e. it predicts masked words (tokens) given all other words (tokens) in the sentence. If only a single word is masked, then the model directly provides the conditional probability <em class="ny">p(w_c|w_1,…,w_{m-1},w_{m+1},…,w_n)</em>. Note that it is only possible due to the bidirectional nature of BERT, since it provides access to tokens on the left as well as on the right from the masked word. On the other hand, the joint probability <em class="ny">p(w_1,…w_n)</em> is not readily available from the BERT output. Looks like a Gibbs sampling use case, doesn’t it? Rewriting <em class="ny">g(x’|x)</em>, one obtains:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pd"><img src="../Images/0854eb2db1eb0f84989372d7a7ab27b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NcUrm67efNHvCSYf1E68Sg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(image by the author)</figcaption></figure><p id="d28b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that as far as only the replacement action is considered, the acceptance rate is always 1:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pe"><img src="../Images/1392aaaf6018fb4b4216bc21cfda36dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gW3MelHww4s9PqRH0q4ioA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(image by the author)</figcaption></figure><p id="9054" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, replacement is, in fact, a Gibbs sampling step, with the proposal distribution directly provided by the BERT model!</p><h1 id="6487" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Experiment</h1><p id="839c" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">To illustrate the method, we will use a pre-trained BERT model from <a class="af nb" href="https://huggingface.co" rel="noopener ugc nofollow" target="_blank">Hugging Face</a>. To have an independent assessment of sentence fluency, we will also compute sentence <a class="af nb" href="https://en.wikipedia.org/wiki/Perplexity" rel="noopener ugc nofollow" target="_blank">perplexity</a> using the <a class="af nb" href="https://huggingface.co/openai-community/gpt2" rel="noopener ugc nofollow" target="_blank">GPT2</a> model.</p><p id="f129" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let us start by loading all the required modules and models into memory:</p><pre class="ml mm mn mo mp pf pg ph bp pi bb bk"><span id="dbab" class="pj oa fq pg b bg pk pl l pm pn">from transformers import BertForMaskedLM, AutoModelForCausalLM, AutoTokenizer<br/><br/>import torch<br/>import torch.nn.functional as F<br/>import numpy as np<br/>import pandas as pd<br/><br/>device = torch.device('cpu') #works just fine<br/><br/>#Load BERT<br/>tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")<br/>model = BertForMaskedLM.from_pretrained("bert-base-uncased")<br/>model.to(device)<br/><br/>#Load GPT2<br/>gpt2_model = AutoModelForCausalLM.from_pretrained("gpt2") #dbmdz/german-gpt2<br/>gpt2_tokenizer = AutoTokenizer.from_pretrained("gpt2")<br/><br/>gpt2_tokenizer.padding_side = "left" <br/>gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token</span></pre><p id="bff3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We then need to define some important constants:</p><pre class="ml mm mn mo mp pf pg ph bp pi bb bk"><span id="712c" class="pj oa fq pg b bg pk pl l pm pn">N_GIBBS_RUNS = 4 #number of runs<br/>N_ITR_PER_RUN = 500 #number of iterations per each run<br/>N_MASKED_WORDS_PER_ITR = 1 #number of masked tokens per iteration<br/>MIN_TOKENS_PROB = 1e-3 #don't use tokens with lower probability for replacement</span></pre><p id="e263" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we will use only the replacement action, we need to select an initial sentences containing the desired keywords. Let it be</p><p id="e27f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">I often dream about a spacious villa by the sea</em>.</p><p id="8791" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Everybody must have dreamt about this at some time… As keywords we will fix, quite arbitrary, <em class="ny">dream</em> and <em class="ny">sea</em>.</p><pre class="ml mm mn mo mp pf pg ph bp pi bb bk"><span id="666e" class="pj oa fq pg b bg pk pl l pm pn">initial_sentence = 'I often dream about a spacious villa by the sea .'<br/><br/>words = initial_sentence.split(' ')<br/><br/>keyword_idx = [2,9]<br/>keyword_idx.append(len(words)-1) # always keep the punctuation mark at the end of the sentence</span></pre><p id="9c7a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we are ready to sample:</p><pre class="ml mm mn mo mp pf pg ph bp pi bb bk"><span id="8d0d" class="pj oa fq pg b bg pk pl l pm pn">def get_bert_tokens(words, indices):<br/>    sentence = " ".join(words)<br/>    masked_sentence = [word if not word_idx in indices else "[MASK]" for word_idx,word in enumerate(words) ]<br/>    masked_sentence = ' '.join(masked_sentence)<br/>    bert_sentence = f'[CLS] {masked_sentence} [SEP] '<br/>    bert_tokens = tokenizer.tokenize(bert_sentence)<br/>    return bert_tokens<br/><br/>n_words = len(words)<br/>n_fixed = len(keyword_idx)<br/><br/>generated_sent = []<br/><br/>for j in range(N_GIBBS_RUNS):<br/>    <br/>    words = initial_sentence.split(' ')<br/><br/>    for i in range(N_ITR_PER_RUN):<br/>        <br/>        if i%10==0:<br/>            print(i)<br/><br/>        #choose N_MASKED_WORDS_PER_ITR random words to mask (excluding keywords)<br/>        masked_words_idx = np.random.choice([x for x in range(n_words) if not x in keyword_idx], replace=False, size=N_MASKED_WORDS_PER_ITR).tolist() <br/>        <br/>        masked_words_idx.sort()<br/>        <br/>        while len(masked_words_idx)&gt;0:<br/><br/>            #reconstruct successively each of the masked word<br/>            bert_tokens = get_bert_tokens(words, masked_words_idx) #get tokens from tokenizer<br/>            <br/>            masked_index = [i for i, x in enumerate(bert_tokens) if x == '[MASK]']<br/>            indexed_tokens = tokenizer.convert_tokens_to_ids(bert_tokens)<br/>            segments_ids = [0] * len(bert_tokens)<br/>            <br/>            tokens_tensor = torch.tensor([indexed_tokens]).to(device)<br/>            segments_tensors = torch.tensor([segments_ids]).to(device)<br/>            <br/>            with torch.no_grad():<br/>                outputs = model(tokens_tensor, token_type_ids=segments_tensors)<br/>                predictions = outputs[0][0]<br/>                reconstruct_pos = 0 #reconstruct leftmost masked token<br/>                probs = F.softmax(predictions[masked_index[reconstruct_pos]],dim=0).cpu().numpy()<br/>            <br/>            probs[probs&lt;MIN_TOKENS_PROB] = 0 #ignore low probabily tokens<br/>            <br/>            if len(probs)&gt;0:<br/><br/>                #sample a token using the conditional probability from BERT<br/>                token = np.random.choice(range(len(probs)), size=1, p=probs/probs.sum(), replace=False)<br/>            <br/>                predicted_token = tokenizer.convert_ids_to_tokens(token)[0]<br/>    <br/>                words[masked_words_idx[reconstruct_pos]] = predicted_token #replace the word in the sequence with the chosen token<br/>            <br/>            del masked_words_idx[reconstruct_pos]<br/><br/>        sentence = ' '.join(words)<br/>        <br/>        with torch.no_grad():<br/>            inputs = gpt2_tokenizer(sentence, return_tensors = "pt")<br/>            loss = gpt2_model(input_ids = inputs["input_ids"], labels = inputs["input_ids"]).loss<br/>            gpt2_perplexity = torch.exp(loss).item()<br/><br/>        #sentence = sentence.capitalize().replace(' .','.')<br/>        gpt2_perplexity = int(gpt2_perplexity)<br/>        <br/>        generated_sent.append((sentence,gpt2_perplexity))<br/><br/>df = pd.DataFrame(generated_sent, columns=['sentence','perplexity'])</span></pre><p id="3f9a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s now have a look at the perplexity plot:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj po"><img src="../Images/e35e69a4a91a0732947a87e3e45c1d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_IycuQtlXQpNrMCumapseA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">GPT2 perplexity for the sampled sentences (image by the author).</figcaption></figure><p id="04ac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are two things to note here. First, the perplexity starts from a relatively small value (<em class="ny">perplexity</em>=147). This is just because we initialized the sampler with a valid sentence that doesn’t look awkward to GPT2. Basically, the sentences whose perplexity does not exceed the starting value (dashed red line) can be considered passing the external check. Second, subsequent samples are correlated. This is a <a class="af nb" href="https://en.wikipedia.org/wiki/Gibbs_sampling" rel="noopener ugc nofollow" target="_blank">known property</a> of the Gibbs sampler and the reason why it is often recommended to take every <em class="ny">k</em>th sample.</p><p id="8b8e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In fact, out of 2000 generated sentences we got 822 unique. Their perplexity ranges from 60 to 1261 with 341 samples having perplexity below that of the initial sentence:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pp"><img src="../Images/dd5c75df96c63cba4241ac6768dfb5cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAdBsmGkCPZLRrSP6jHEEg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">GPT2 perplexity distribution across unique sentences (image by the author).</figcaption></figure><p id="6a4d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">How do these sentences look like? Let’s take a random subset:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pq"><img src="../Images/013a1614f64dceeb9eafd2650bf0a2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrJoQUOS8eoWJ9prbGDvlw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A random subset of generated sentences with perplexity below the starting value (image by the author).</figcaption></figure><p id="4e0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">These sentences look indeed quite fluent. Note that the chosen keywords (<em class="ny">dream</em> and <em class="ny">sea</em>) appear in each sentence.</p><p id="a285" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is also tempting to see what happens if we don’t set any keywords. Let’s take a random subset of sentences generated with an empty keywords set:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/acf141780fcff6e0c5b418ea748a3808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lk1J_p00RU0yz0BkhDr3tw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A random subset of sentences generated without fixing keywords (image by the author).</figcaption></figure><p id="a7c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, these sentence also look quite fluent and diverse! In fact, using an empty keyword set simply turns BERT into a random sentence generator. Note, however, that all these sentences have 10 words, as the initial sentence. The reason is that the BERT model can’t change the sentence length arbitrary.</p><p id="1c70" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, why do we need to run the sampler N_GIBBS_RUNS=4 times, wouldn’t just a single run be enough? In fact, running several times is necessary since a Gibbs sampler can get stuck in a local minimum [7]. To illustrate this case, we computed the accumulated vocabulary size (number of distinct words used so far in the generated sentences) when running the sampler once for 2000 iterations and when re-initializing the sampler with the initial sentence every 500 iterations:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/f14bcdd41d6bdc70405c95a3a23ca9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpOa9OpYnri9D56TydVL6g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Accumulated vocabulary size when running Gibbs samplig for 2000 iterations in a single run and in 4 runs, 500 iterations each (image by the author)</figcaption></figure><p id="46b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It can be clearly seen that a single run gets stuck at about 1500 iterations and the sampler is not able to generate sentences with new words after this point. In contrast, re-initializing the sampler every 500 iterations helps to get out of this local minimum and improves lexically diversity of the generated sentences.</p><h1 id="198e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion</h1><p id="4ff4" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">In sum, the proposed method generates realistic sentences starting from a sentence containing given keywords. The resulting sentences ensure 100% coverage (a), sound grammatically correct (b), respect common sense (c), and provide lexical diversity (d). Additionally, the method is incredibly simple and can be used with publicly available pre-trained models. The main weaknesses of the method, is, of course, its dependence of a starting sentence satisfying the given constraints. First, the starting sentence should be somehow provided from an expert or any other external source. Second, while ensuring grammatically correct sentence generation, it also limits the grammatical diversity of the output. A possible solution would be to provide several input sentences by mining a reliable sentence database.</p><h1 id="99eb" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><p id="33e8" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">[1] Garbacea, Cristina, and Qiaozhu Mei. “Why is constrained neural language generation particularly challenging?.” <em class="ny">arXiv preprint arXiv:2206.05395</em> (2022).</p><p id="c309" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] Post, Matt, and David Vilar. “Fast lexically constrained decoding with dynamic beam allocation for neural machine translation.” <em class="ny">arXiv preprint arXiv:1804.06609</em> (2018).</p><p id="8b95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] Lin, Bill Yuchen, et al. “CommonGen: A constrained text generation challenge for generative commonsense reasoning.” <em class="ny">arXiv preprint arXiv:1911.03705</em> (2019).</p><p id="d3ab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] Miao, Ning, et al. “Cgmh: Constrained sentence generation by metropolis-hastings sampling.” <em class="ny">Proceedings of the AAAI Conference on Artificial Intelligence</em>. Vol. 33. №01. 2019.</p><p id="953f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5] Sha, Lei. “Gradient-guided unsupervised lexically constrained text generation.” <em class="ny">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 2020.</p><p id="5d00" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[6] He, Xingwei, and Victor OK Li. “Show me how to revise: Improving lexically constrained sentence generation with xlnet.” <em class="ny">Proceedings of the AAAI Conference on Artificial Intelligence</em>. Vol. 35. №14. 2021.</p><p id="bab6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[7] Wang, Alex, and Kyunghyun Cho. “BERT has a mouth, and it must speak: BERT as a Markov random field language model.” <em class="ny">arXiv preprint arXiv:1902.04094</em> (2019).</p></div></div></div></div>    
</body>
</html>