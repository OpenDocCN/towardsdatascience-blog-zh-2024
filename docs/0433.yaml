- en: 'The Rise of Sparse Mixtures of Experts: Switch Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏专家混合的崛起：Switch Transformers
- en: 原文：[https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15](https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15](https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15)
- en: A deep-dive into the technology that paved the way for the most capable LLMs
    in the industry today
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨为今天行业中最强大的大规模语言模型（LLM）铺平道路的技术
- en: '[](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    ·8 min read·Feb 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: · 发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    ·8 分钟阅读·2024年2月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f8e64759f9946f38accbec2ec3e09a23.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8e64759f9946f38accbec2ec3e09a23.png)'
- en: Image generated using Dall-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dall-E 生成的图像
- en: Sparse Mixtures of Experts (MoE) has become a key technology in the latest generation
    of LLMs such as OpenAI’s GPT-4, Mistral AI’s Mixtral-8x7, and more. In a nutshell,
    sparse MoE is an extremely powerful technology because — in theory — it allows
    us to scale up capacity of *any* model with a computational complexity of O(1)!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏专家混合（MoE）已成为最新一代大规模语言模型（LLM）中的关键技术，如 OpenAI 的 GPT-4、Mistral AI 的 Mixtral-8x7
    等。简而言之，稀疏 MoE 是一种极其强大的技术，因为——从理论上讲——它使我们能够以 O(1) 的计算复杂度扩展*任何*模型的容量！
- en: However, as is often the case, the devil lies in the details, and getting sparse
    MoE to work correctly requires to get these details exactly right.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如通常所说，问题往往隐藏在细节中，正确实现稀疏 MoE 需要精确掌握这些细节。
- en: 'In this post, we’ll dive into one of the pivotal contributions in the domain
    of sparse MoE, the Switch Transformer ([Fedus et al 2022](https://arxiv.org/abs/2101.03961)),
    which demonstrated for the first time the impressive scaling properties one can
    achieve with this technology, achieving 7X speed-up in training of a Transformer
    model. We’ll cover:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨稀疏专家混合（MoE）领域的一个重要贡献——Switch Transformer（[Fedus et al 2022](https://arxiv.org/abs/2101.03961)），这是首次展示了使用这一技术可以实现的惊人扩展性，成功地在训练
    Transformer 模型时达到了 7 倍的加速。我们将涵盖：
- en: 'hard routing: the favorable scaling properties that come from executing just
    a single expert per token,'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬路由：通过为每个 token 执行单一专家来获得有利的扩展性特性，
- en: 'the Switch Transformer architecture: how MoE fits into the broader context
    of the Transformer architecture,'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Switch Transformer 架构：MoE 如何融入 Transformer 架构的更广泛背景，
- en: 'token routing dynamics: how the capacity factor is used to trade off…'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: token 路由动态：如何利用容量因子进行权衡……
