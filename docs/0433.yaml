- en: 'The Rise of Sparse Mixtures of Experts: Switch Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15](https://towardsdatascience.com/the-rise-of-sparse-mixtures-of-experts-switch-transformers-39cf3671f8a0?source=collection_archive---------6-----------------------#2024-02-15)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep-dive into the technology that paved the way for the most capable LLMs
    in the industry today
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page---byline--39cf3671f8a0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--39cf3671f8a0--------------------------------)
    ·8 min read·Feb 15, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8e64759f9946f38accbec2ec3e09a23.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated using Dall-E
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Mixtures of Experts (MoE) has become a key technology in the latest generation
    of LLMs such as OpenAI’s GPT-4, Mistral AI’s Mixtral-8x7, and more. In a nutshell,
    sparse MoE is an extremely powerful technology because — in theory — it allows
    us to scale up capacity of *any* model with a computational complexity of O(1)!
  prefs: []
  type: TYPE_NORMAL
- en: However, as is often the case, the devil lies in the details, and getting sparse
    MoE to work correctly requires to get these details exactly right.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’ll dive into one of the pivotal contributions in the domain
    of sparse MoE, the Switch Transformer ([Fedus et al 2022](https://arxiv.org/abs/2101.03961)),
    which demonstrated for the first time the impressive scaling properties one can
    achieve with this technology, achieving 7X speed-up in training of a Transformer
    model. We’ll cover:'
  prefs: []
  type: TYPE_NORMAL
- en: 'hard routing: the favorable scaling properties that come from executing just
    a single expert per token,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'the Switch Transformer architecture: how MoE fits into the broader context
    of the Transformer architecture,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'token routing dynamics: how the capacity factor is used to trade off…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
