<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Leverage KeyBERT, HDBSCAN and Zephyr-7B-Beta to Build a Knowledge Graph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Leverage KeyBERT, HDBSCAN and Zephyr-7B-Beta to Build a Knowledge Graph</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07">https://towardsdatascience.com/leverage-keybert-hdbscan-and-zephyr-7b-beta-to-build-a-knowledge-graph-33d7534ee01b?source=collection_archive---------0-----------------------#2024-01-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6974" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">LLM-enhanced natural language processing and traditional machine learning techniques are used to extract structure and to build a knowledge graph from unstructured corpus.</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Silvia Onofrei" class="l ep by dd de cx" src="../Images/198b04b2063b4269eaff52402dc5f8d5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*sMBSQ7O4JfKYt-L7TGmlIQ.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@silviaonofrei?source=post_page---byline--33d7534ee01b--------------------------------" rel="noopener follow">Silvia Onofrei</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--33d7534ee01b--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">8</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/f81dc37979040a1df7f80c88c9bfba77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2dOzHsmoT4Neuz2wbX91g.jpeg"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx"><a class="af nd" href="https://www.freepik.com/" rel="noopener ugc nofollow" target="_blank">Designed by Freepik</a></figcaption></figure><h1 id="e72e" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Introduction</h1><p id="2813" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">While the Large Language Models (LLMs) are useful and skilled tools, relying entirely on their output is not always advisable as they often require verification and grounding. However, merging traditional NLP methods with the capabilities of generative AI typically yields satisfactory results. An excellent example of this synergy is the enhancement of KeyBERT with KeyLLM for keyword extraction.</p><p id="400b" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">In this blog, I intend to explore the efficacy of combining traditional NLP and machine learning techniques with the versatility of LLMs. This exploration includes integrating simple keyword extraction using KeyBERT, sentence embeddings with BERT, and employing UMAP for dimensionality reduction coupled with HDBSCAN for clustering. All these are used in conjunction with Zephyr-7B-Beta, a highly performant LLM. The findings are uploaded into a knowledge graph for enhanced analysis and discovery.</p><p id="e82a" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">My goal is to develop structure on a corpus of unstructured arXiv article titles in computer science. I selected these articles based on abstract length, not expecting inherent topics clusters. Indeed, a preliminary community analysis revealed nearly as many clusters as articles. Consequently, I’m exploring a different approach to linking these titles. Despite lacking clear communities, the titles often share common words. By extracting and clustering these keywords, I aim to uncover underlying connections between the titles, offering a versatile strategy for structuring the dataset.</p><p id="2c58" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">To simplify and enhance data exploration, I upload my results in a Neo4j knowledge graph. Here’s a snapshot of the output:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div class="mk ml pb"><img src="../Images/96e2d0b9ac4a38c4edcf1392d6108b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*khTWST6p-1T_kagmeh84aw.png"/></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">The two purple nodes represent the titles: “Cores of Countably Categorical Structures” (left) and “Transforming Structures by Set Interpretations” (right). They are linked by the common theme of “Mathematical Logic” (tan node) via the keyword “structures”. — Image by author —</figcaption></figure></div></div></div><div class="ab cb pc pd pe pf" role="separator"><span class="pg by bm ph pi pj"/><span class="pg by bm ph pi pj"/><span class="pg by bm ph pi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="43c2" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Outlined below are the project’s steps:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml pk"><img src="../Images/c16d1bc3d9daa4b48d465d17360fc24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kkQAbv7sa4cG6s4-MMWcwg.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">— Diagram by author —</figcaption></figure><ul class=""><li id="1376" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Collect and parse the dataset, focusing on titles while retaining the abstracts for context.</li><li id="b6ca" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">Employ KeyBERT to extract candidate keywords, which are then refined using KeyLLM, based on Zephyr-7B-Beta, to generate a list of enhanced keywords and keyphrases.</li><li id="10e2" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">Gather all extracted keywords and keyphrases and cluster them using HDBSCAN.</li><li id="97ce" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">Use Zephyr-7B-Beta again, to derive labels and descriptions for each cluster.</li><li id="d1b1" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">Combine these elements in a knowledge graph whith nodes representing Articles, Keywords and (cluster) Topics.</li></ul><p id="7feb" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">It’s important to note that each step in this process offers the flexibility to experiment with alternative methods, algorithms, or models.</p></div></div></div><div class="ab cb pc pd pe pf" role="separator"><span class="pg by bm ph pi pj"/><span class="pg by bm ph pi pj"/><span class="pg by bm ph pi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="bdfe" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The work is done in a Google Colab Pro with a V100 GPU and High RAM setting for the steps involving LLM. The notebook is divided into self-contained sections, most of which can be executed independently, minimizing dependency on previous steps. Data is saved after each section, allowing continuation in a new session if needed. Additionally, the parsed dataset and the Python modules, are readily available in this <a class="af nd" href="https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j" rel="noopener ugc nofollow" target="_blank">Github repository.</a></p><h1 id="b662" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Data Preparation</h1><p id="aa96" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">I use a subset of the <a class="af nd" href="https://www.kaggle.com/datasets/Cornell-University/arxiv" rel="noopener ugc nofollow" target="_blank">arXiv Dataset</a> that is openly available on the Kaggle platform and primarly maintained by Cornell University. In a machine readable format, it contains a repository of 1.7 million scholarly papers across STEM, with relevant features such as article titles, authors, categories, abstracts, full text PDFs, and more. It is updated regularly.</p><p id="053e" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The dataset is clean and in an easy to use format, so we can focus on our task, without spending too much time on data preprocessing. To further simplify the data preparation process, I built a Python module that performs the relevant steps. It can be found at <code class="cx pt pu pv pw b"><a class="af nd" href="https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/arxiv_parser.py" rel="noopener ugc nofollow" target="_blank">utils/arxiv_parser.py</a></code> if you want to take a peek at the code, otherwise follow along the Google Colab:</p><ul class=""><li id="c5c9" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">download the zipped arXiv file (1.2 GB) in the directory of your choice which is labelled <code class="cx pt pu pv pw b">data_path</code>,</li><li id="58b2" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">download the <code class="cx pt pu pv pw b">arxiv_parser.py</code> in the directory <code class="cx pt pu pv pw b">utils</code>,</li><li id="e4a0" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">import and initialize the module in your Google Colab notebook,</li><li id="9907" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">unzip the file, this will extract a 3.7 GB file: <code class="cx pt pu pv pw b">archive-metadata-oai-snapshot.json</code>,</li><li id="3d0b" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">specify a general topic (I work with <code class="cx pt pu pv pw b">cs</code> which stands for computer science), so you’ll have a more maneagable size data,</li><li id="3f9f" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">choose the features to keep (there are 14 features in the downloaded dataset),</li><li id="0195" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">the abstracts can vary in length quite a bit, so I added the option of selecting entries for which the number of tokens in the abstract is in a given interval and used this feature to downsize the dataset,</li><li id="730e" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">although I choose to work with the <code class="cx pt pu pv pw b">title</code> feature, there is an option to take the more common approach of concatenating the title and the abstact in a single feature denoted <code class="cx pt pu pv pw b">corpus</code> .</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="a290" class="qa nf fq pw b bg qb qc l qd qe"># Import the data parser module<br/>from utils.arxiv_parser import *<br/><br/># Initialize the data parser<br/>parser = ArXivDataProcessor(data_path)<br/><br/># Unzip the downloaded file to extract a json file in data_path<br/>parser.unzip_file()<br/><br/># Select a topic and extract the articles on that topic<br/>topic='cs'<br/>entries = parser.select_topic('cs')<br/><br/># Build a pandas dataframe with specified selections<br/>df = parser.select_articles(entries, # extracted articles<br/>                            cols=['id', 'title', 'abstract'], # features to keep<br/>                            min_length = 100, # min tokens an abstract should have<br/>                            max_length = 120, # max tokens an abstract should have<br/>                            keep_abs_length = False, # do not keep the abs_length column<br/>                            build_corpus=False) # do not build a corpus column<br/><br/># Save the selected data to a csv file 'selected_{topic}.csv', uses data_path<br/>parser.save_selected_data(df,topic)</span></pre><p id="9a6c" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">With the options above I extract a dataset of 983 computer science articles. We are ready to move to the next step.</p><blockquote class="qf qg qh"><p id="1c97" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">If you want to skip the data processing steps, you may use the <code class="cx pt pu pv pw b">cs</code> dataset, available in the Github repository.</p></blockquote><h1 id="5fec" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Keyword Extraction with KeyBERT and KeyLLM</h1><h2 id="7964" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">The Method</h2><p id="3dc5" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk"><a class="af nd" href="https://maartengr.github.io/KeyBERT/guides/quickstart.html" rel="noopener ugc nofollow" target="_blank">KeyBERT</a> is a method that extracts keywords or keyphrases from text. It uses document and word embeddings to find the sub-phrases that are most similar to the document, via cosine similarity. KeyLLM is another minimal method for keyword extraction but it is based on LLMs. Both methods are developed and maintained by Maarten Grootendorst.</p><p id="db6d" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The two methods can be combined for enhanced results. Keywords extracted with KeyBERT are fine-tuned through KeyLLM. Conversely, candidate keywords identified through traditional NLP techniques help grounding the LLM, minimizing the generation of undesired outputs.</p><p id="8993" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">For details on different ways of using KeyLLM see <a class="af nd" rel="noopener" target="_blank" href="/introducing-keyllm-keyword-extraction-with-llms-39924b504813">Maarten Grootendorst, Introducing KeyLLM — Keyword Extraction with LLMs</a>.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div class="mk ml ra"><img src="../Images/f59f1b8d7f5ad0fadf9cd40a15394682.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Gp2rbIGMipQjMnyPO8G4mg.png"/></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">— Diagram by author —</figcaption></figure><p id="721c" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Use KeyBERT [<a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py" rel="noopener ugc nofollow" target="_blank">source</a>] to extract keywords from each document — these are the candidate keywords provided to LLM to fine-tune:</p><ul class=""><li id="be3f" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">documents are embedded using Sentence Transformers to build a document level representation,</li><li id="e9c9" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">word embeddings are extracted for N-grams words/phrases,</li><li id="2f0d" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">cosine similarity is used to find the words or phrases that are most similar to each document.</li></ul><p id="0844" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Use KeyLLM [<a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_llm.py" rel="noopener ugc nofollow" target="_blank">source</a>] to finetune the kewords extracted by KeyBERT via text generation with transformers [<a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py" rel="noopener ugc nofollow" target="_blank">source</a>]:</p><ul class=""><li id="483f" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">the community detection method in Sentence Transformers [<a class="af nd" href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/util.py" rel="noopener ugc nofollow" target="_blank">source</a>] groups the similar documents, so we will extract keywords only from one document in each group,</li><li id="380d" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">the candidate keywords are provided the LLM which fine-tunes the keywords for each cluster.</li></ul><blockquote class="qf qg qh"><p id="6ea0" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Besides Sentence Transformers, KeyBERT supports other embedding models, see [<a class="af nd" href="https://maartengr.github.io/KeyBERT/guides/embeddings.html" rel="noopener ugc nofollow" target="_blank">here</a>].</p><p id="4894" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Sentence Transformers facilitate community detection by using a specified threshold. When documents lack inherent clusters, clear groupings may not emerge. In my case, out of 983 titles, approximately 800 distinct communities were identified. More naturally clustered data tends to yield better-defined communities.</p></blockquote><h2 id="877d" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">The Large Language Model</h2><p id="f8f1" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">After experimting with various smaller LLMs, I choose <a class="af nd" href="https://arxiv.org/pdf/2310.16944.pdf" rel="noopener ugc nofollow" target="_blank">Zephyr-7B-Beta</a> for this project. This model is based on <a class="af nd" href="https://mistral.ai/news/announcing-mistral-7b/" rel="noopener ugc nofollow" target="_blank">Mistral-7B</a>, and it is one of the first models fine-tuned with Direct Preference Optimization (DPO). It not only outperforms other models in its class but also surpasses Llama2–70B on some benchmarks. For more insights on this LLM take a look at <a class="af nd" rel="noopener" target="_blank" href="/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7">Benjamin Marie, Zephyr 7B Beta: A Good Teacher is All You Need</a>. Although it’s feasible to use the model directly on a Google Colab Pro, I opted to work with a GPTQ quantized version prepared by <a class="af nd" href="https://huggingface.co/TheBloke" rel="noopener ugc nofollow" target="_blank">TheBloke</a>.</p><p id="9e1e" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Start by downloading the model and its tokenizer following the instructions provided in the <a class="af nd" href="https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ" rel="noopener ugc nofollow" target="_blank">model card:</a></p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="2832" class="qa nf fq pw b bg qb qc l qd qe"># Required installs<br/>!pip install transformers optimum accelerate<br/>!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/<br/><br/># Required imports<br/>from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline<br/><br/># Load the model and the tokenizer<br/>model_name_or_path = "TheBloke/zephyr-7B-beta-GPTQ"<br/><br/>llm = AutoModelForCausalLM.from_pretrained(model_name_or_path,<br/>                                             device_map="auto",<br/>                                             trust_remote_code=False,<br/>                                             revision="main") # change revision for a different branch<br/>tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, <br/>                     use_fast=True)</span></pre><p id="3aa8" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Additionally, build the text generation pipeline:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="fd21" class="qa nf fq pw b bg qb qc l qd qe">generator = pipeline(<br/>    model=llm,<br/>    tokenizer=tokenizer,<br/>    task='text-generation',<br/>    max_new_tokens=50,<br/>    repetition_penalty=1.1,<br/>)</span></pre><h2 id="101a" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">The Keyword Extraction Prompt</h2><p id="b198" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">Experimentation is key in this step. Finding the optimal prompt requires some trial and error, and the performance depends on the chosen model. Let’s not forget that LLMs are probabilistic, so it is not guaranteed that they will return the same output every time. To develop the prompt below, I relied on both experimentation and the following considerations:</p><ul class=""><li id="9083" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">the prompt template provided in the <a class="af nd" href="https://huggingface.co/TheBloke/zephyr-7B-beta-GPTQ" rel="noopener ugc nofollow" target="_blank">model card</a>:</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="4ee9" class="qa nf fq pw b bg qb qc l qd qe">prompt = "Tell me about AI"<br/>prompt_template=f'''&lt;|system|&gt;<br/>&lt;/s&gt;<br/>&lt;|user|&gt;<br/>{prompt}&lt;/s&gt;<br/>&lt;|assistant|&gt;<br/>'''</span></pre><ul class=""><li id="b18e" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">the suggestions from the <a class="af nd" href="https://medium.com/towards-data-science/introducing-keyllm-keyword-extraction-with-llms-39924b504813" rel="noopener">KeyLLM blogpost</a> and from the <a class="af nd" href="https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm" rel="noopener ugc nofollow" target="_blank">documentation</a>,</li><li id="2914" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">some experimentation with ChatGPT and KeyBERT to build an example,</li><li id="688c" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">the <a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py" rel="noopener ugc nofollow" target="_blank">code for text_generation wrapper for KeyLLM</a>.</li></ul><p id="c793" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">And here is the prompt I use to fine-tune the keywords extracted with KeyBERT:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="4d3e" class="qa nf fq pw b bg qb qc l qd qe">prompt_keywords= """<br/>&lt;|system|&gt;<br/>I have the following document:<br/>Semantics and Termination of Simply-Moded Logic Programs with Dynamic Scheduling<br/>and five candidate keywords:<br/>scheduling, logic, semantics, termination, moded<br/><br/>Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.<br/>Follow the requirements below:<br/>1. Make sure to extract only the keywords or keyphrases that appear in the text.<br/>2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!<br/>3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!<br/><br/>semantics, termination, simply-moded, logic programs, dynamic scheduling&lt;/s&gt;<br/><br/>&lt;|user|&gt;<br/>I have the following document:<br/>[DOCUMENT]<br/>and five candidate keywords:<br/>[CANDIDATES]<br/><br/>Based on the information above, extract the keywords or the keyphrases that best describe the topic of the text.<br/>Follow the requirements below:<br/>1. Make sure to extract only the keywords or keyphrases that appear in the text.<br/>2. Provide five keywords or keyphrases! Do not number or label the keywords or the keyphrases!<br/>3. Do not include anything else besides the keywords or the keyphrases! I repeat do not include any comments!&lt;/s&gt;<br/><br/>&lt;|assistant|&gt;<br/>"""</span></pre><h2 id="fe3e" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">Keyword Extraction and Parsing</h2><p id="5704" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">We now have everything needed to proceed with the keyword extraction. Let me remind you, that I work with the titles, so the input documents are short, staying well within the token limits for the BERT embeddings.</p><p id="b343" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Start with creating a <a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/llm/_textgeneration.py" rel="noopener ugc nofollow" target="_blank">TextGeneration pipeline wrapper</a> for the LLM and instantiate <a class="af nd" href="https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py" rel="noopener ugc nofollow" target="_blank">KeyBERT</a>. Choose the embedding model. If no embedding model is specified, the default model is <code class="cx pt pu pv pw b">all-MiniLM-L6-v2</code>. In this case, I select the highest-performant pretrained model for sentence embeddings, see <a class="af nd" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">here</a> for a complete list.</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="2c41" class="qa nf fq pw b bg qb qc l qd qe"># Install the required packages<br/>!pip install keybert<br/>!pip install sentence-transformers<br/><br/># The required imports<br/>from keybert.llm import TextGeneration<br/>from keybert import KeyLLM, KeyBERT<br/>from sentence_transformers import SentenceTransformer<br/><br/># KeyBert TextGeneration pipeline wrapper<br/>llm_tg = TextGeneration(generator, prompt=prompt_keywords)<br/><br/># Instantiate KeyBERT and specify an embedding model<br/>kw_model= KeyBERT(llm=llm_tg, model = "all-mpnet-base-v2")</span></pre><p id="dcbe" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Recall that the dataset was prepared and saved as a pandas dataframe <code class="cx pt pu pv pw b">df</code>. To process the titles, just call the <code class="cx pt pu pv pw b">extract_keywords</code> method:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="060f" class="qa nf fq pw b bg qb qc l qd qe"># Retain the articles titles only for analysis<br/>titles_list = df.title.tolist()<br/><br/># Process the documents and collect the results<br/>titles_keys = kw_model.extract_keywords(titles_list, thresold=0.5)<br/><br/># Add the results to df<br/>df["titles_keys"] = titles_keys</span></pre><blockquote class="qf qg qh"><p id="264b" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The <code class="cx pt pu pv pw b">threshold</code> parameter determines the minimum similarity required for documents to be grouped into the same community. A higher value will group nearly identical documents, while a lower value will cluster documents covering similar topics.</p><p id="044a" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The choice of embeddings significantly influences the appropriate threshold, so it’s advisable to consult the model card for guidance. I’m grateful to Maarten Grootendorst for highlighting this aspect, as can be seen <a class="af nd" href="https://github.com/MaartenGr/KeyBERT/issues/190" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="01bf" class="oa ob qi oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">It’s important to note that my observations apply exclusively to sentence transformers, as I haven’t experimented with other types of embeddings.</p></blockquote><p id="0851" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Let’s take a look at some outputs:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml rb"><img src="../Images/c770ad00ee29463c4e22dd7621eb95fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofgC43m31Io9HD4CGflnCw.png"/></div></div></figure><p id="fa51" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk"><strong class="oc fr">Comments</strong>:</p><ul class=""><li id="a18a" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">In the second example provided here, we observe keywords or keyphrases not present in the original text. If this poses a problem in your case, consider enabling <code class="cx pt pu pv pw b">check_vocab=True</code> as done [<a class="af nd" href="https://maartengr.github.io/KeyBERT/guides/keyllm.html#2-extract-keywords-with-keyllm" rel="noopener ugc nofollow" target="_blank">here</a>]. However, it's important to remember that these results are highly influenced by the LLM choice, with quantization having a minor effect, as well as the construction of the prompt.</li><li id="ea45" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">With longer input documents, I noticed more deviations from the required output.</li><li id="00b1" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">One consistent observation is that the number of keywords extracted often deviates from five. It’s common to encounter titles with fewer extracted keywords, especially when the input is brief. Conversely, some titles yield as many as 10 extracted keywords. Let’s examine the distribution of keyword counts for this run:</li></ul><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div class="mk ml rc"><img src="../Images/77b2a23d02c46e836763d01129a71041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*qbLiwxHCrnYXDnPhFfD7mA.png"/></div></figure><p id="e009" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">These variations complicate the subsequent parsing steps. There are a few options for addressing this: we could investigate these cases in detail, request the model to revise and either trim or reiterate the keywords, or simply overlook these instances and focus solely on titles with exactly five keywords, as I’ve decided to do for this project.</p><h1 id="a14e" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Clustering Keywords with HDBSCAN</h1><p id="1e16" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">The following step is to cluster the keywords and keyphrases to reveal common topics across articles. To accomplish this I use two algorithms: UMAP for dimensionality reduction and HDBSCAN for clustering.</p><h2 id="01ff" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">The Algorithms: HDBSCAN and UMAP</h2><p id="cb78" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk"><a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#" rel="noopener ugc nofollow" target="_blank">Hierarchical Density-Based Spatial Clustering of Applications with Noise</a> or <strong class="oc fr">HDBSCAN</strong>, is a highly performant unsupervised algorithm designed to find patterns in the data. It finds the optimal clusters based on their density and proximity. This is especially useful in cases where the number and shape of the clusters may be unknown or difficult to determine.</p><p id="74d2" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The results of HDBSCAN clustering algorithm can vary if you run the algorithm multiple times with the same hyperparameters. This is because HDBSCAN is a stochastic algorithm, which means that it involves some degree of randomness in the clustering process. Specifically, HDBSCAN uses a random initialization of the cluster hierarchy, which can result in different cluster assignments each time the algorithm is run.</p><p id="e4d1" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">However, the degree of variation between different runs of the algorithm can depend on several factors, such as the dataset, the hyperparameters, and the seed value used for the random number generator. In some cases, the variation may be minimal, while in other cases it can be significant.</p><p id="12ec" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">There are two clustering options with HDBSCAN.</p><ul class=""><li id="a151" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">The primary clustering algorithm, denoted <code class="cx pt pu pv pw b">hard_clustering</code> assigns each data point to a cluster or labels it as noise. This is a hard assignment; there are no mixed memberships. This approach might result in one large cluster categorized as noise (cluster labelled -1) and numerous smaller clusters. Fine-tuning the hyperparameters is crucial [<a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/faq.html" rel="noopener ugc nofollow" target="_blank">see here</a>], as it is selecting an embedding model specifically tailored for the domain. Take a look at the associated <a class="af nd" href="https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab</a> for the results of hard clustering on the project’s dataset.</li><li id="0507" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk"><code class="cx pt pu pv pw b">Soft clustering</code> on the other side is a newer feature of the HDBSCAN library. In this approach points are not assigned cluster labels, but instead they are assigned a vector of probabilities. The length of the vector is equal to the number of clusters found. The probability value at the entry of the vector is the probability the point is a member of the the cluster. This allows points to potentially be a mix of clusters. If you want to better understand how soft clustering works please refer to <a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/soft_clustering_explanation.html" rel="noopener ugc nofollow" target="_blank">How Soft Clustering for HDBSCAN Works</a>. This approach is better suited for the present project, as it generates a larger set of rather similar sizes clusters.</li></ul><p id="24c8" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">While HDBSCAN can perform well on low to medium dimensional data, the performance tends to decrease significantly as dimension increases. In general HDBSCAN performs best on up to around 50 dimensional data, [<a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/faq.html" rel="noopener ugc nofollow" target="_blank">see here</a>].</p><p id="4607" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Documents for clustering are typically embedded using an efficient transformer from the BERT family, resulting in a several hundred dimensions data set.</p><p id="bbd6" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">To reduce the dimension of the embeddings vectors we use <strong class="oc fr">UMAP</strong> (<a class="af nd" href="https://umap-learn.readthedocs.io/en/latest/basic_usage.html" rel="noopener ugc nofollow" target="_blank">Uniform Manifold Approximation and Projection)</a>, a non-linear dimension reduction algorithm and the best performing in its class. It seeks to learn the manifold structure of the data and to find a low dimensional embedding that preserves the essential topological structure of that manifold.</p><p id="7fb2" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">UMAP has been shown to be highly effective at preserving the overall structure of high-dimensional data in lower dimensions, while also providing superior performance to other popular algorithms like t-SNE and PCA.</p><h2 id="d17a" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">Keyword Clustering</h2><ul class=""><li id="ce2c" class="oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov pl pm pn bk">Install and import the required packages and libraries.</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="fbc0" class="qa nf fq pw b bg qb qc l qd qe"># Required installs<br/>!pip install umap-learn<br/>!pip install hdbscan<br/>!pip install -U sentence-transformers<br/><br/># General imports<br/>import pandas as pd<br/>import numpy as np<br/>import re<br/>import pickle<br/><br/># Imports needed to generate the BERT embeddings<br/>from sentence_transformers import SentenceTransformer<br/><br/># Libraries for dimensionality reduction<br/>import umap.umap_ as umap<br/><br/># Import the clustering algorithm<br/>import hdbscan</span></pre><ul class=""><li id="c8d5" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Prepare the dataset by aggregating all keywords and keyphrases from each title’s individual quintet into a single list of unique keywords and save it as a pandas dataframe.</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="407c" class="qa nf fq pw b bg qb qc l qd qe"># Load the data if needed - titles with 5 extracted keywords<br/>df5 = pd.read_csv(data_path+parsed_keys_file) <br/><br/># Create a list of all sublists of keywords and keyphrases<br/>df5_keys = df5.titles_keys.tolist()<br/><br/># Flatten the list of sublists<br/>flat_keys = [item for sublist in df5_keys for item in sublist]<br/><br/># Create a list of unique keywords<br/>flat_keys = list(set(flat_keys))<br/><br/># Create a dataframe with the distinct keywords<br/>keys_df = pd.DataFrame(flat_keys, columns = ['key'])</span></pre><p id="7efc" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">I obtain almost 3000 unique keywords and keyphrases from the 884 processed titles. Here is a sample: n-colorable graphs, experiments, constraints, tree structure, complexity, etc.</p><ul class=""><li id="d2cf" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Generate 768-dimensional embeddings with Sentence Transformers.</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="3c08" class="qa nf fq pw b bg qb qc l qd qe"># Instantiate the embedding model<br/>model = SentenceTransformer('all-mpnet-base-v2')<br/><br/># Embed the keywords and keyphrases into 768-dim real vector space<br/>keys_df['key_bert'] = keys_df['key'].apply(lambda x: model.encode(x))</span></pre><ul class=""><li id="231d" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Perform dimensionality reduction with UMAP.</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="1aca" class="qa nf fq pw b bg qb qc l qd qe"># Reduce to 10-dimensional vectors and keep the local neighborhood at 15<br/>embeddings = umap.UMAP(n_neighbors=15, # Balances local vs. global structure.<br/>                       n_components=10, # Dimension of reduced vectors<br/>                       metric='cosine').fit_transform(list(keys_df.key_bert))<br/><br/># Add the reduced embedding vectors to the dataframe<br/>keys_df['key_umap'] = embeddings.tolist()</span></pre><ul class=""><li id="0c27" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Cluster the 10-dimensional vectors with HDBSCAN. To keep this blog succinct, I will omit descriptions of the parameters that pertain more to hard clustering. For detailed information on each parameter, please refer to [<a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/parameter_selection.html" rel="noopener ugc nofollow" target="_blank">Parameter Selection for HDBSCAN*</a>].</li></ul><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="5bc6" class="qa nf fq pw b bg qb qc l qd qe"># Initialize the clustering model<br/>clusterer = hdbscan.HDBSCAN(algorithm='best',<br/>                            prediction_data=True,<br/>                            approx_min_span_tree=True,<br/>                            gen_min_span_tree=True,<br/>                            min_cluster_size=20,<br/>                            cluster_selection_epsilon = .1,<br/>                            min_samples=1,<br/>                            p=None,<br/>                            metric='euclidean',<br/>                            cluster_selection_method='leaf')<br/><br/># Fit the data<br/>clusterer.fit(embeddings)<br/><br/># Create soft clusters<br/>soft_clusters = hdbscan.all_points_membership_vectors(clusterer)<br/><br/># Add the soft cluster information to the data<br/>closest_clusters = [np.argmax(x) for x in soft_clusters]<br/>keys_df['cluster'] = closest_clusters</span></pre><p id="758d" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Below is the distribution of keywords across clusters. Examination of the spread of keywords and keyphrases into soft clusters reveals a total of 60 clusters, with a fairly even distribution of elements per cluster, varying from about 20 to nearly 100.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div class="mk ml ch"><img src="../Images/07b5e9852a75b59e177d37efc1149ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*PAdTTUGz_NdzvnX8rfSf9A.png"/></div></figure><h1 id="010f" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Extract Cluster Descriptions and Labels</h1><p id="6b21" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">Having clustered the keywords, we are now ready to employ GenAI once more to enhance and refine our findings. At this step, we will use a LLM to analyze each cluster, summarize the keywords and keyphrases while assigning a brief label to the cluster.</p><p id="b5b9" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">While it’s not necessary, I choose to continue with the same LLM, Zephyr-7B-Beta. Should you require downloading the model, please consult the relevant section. Notably, I will adjust the prompt to suit the distinct nature of this task.</p><p id="9b1d" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The following function is designed to extract a label and a description for a cluster, parse the output and integrate it into a pandas dataframe.</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="4c86" class="qa nf fq pw b bg qb qc l qd qe">def extract_description(df: pd.DataFrame,<br/>                        n: int     <br/>                        )-&gt; pd.DataFrame:<br/>    """<br/>    Use a custom prompt to send to a LLM<br/>    to extract labels and descriptions for a list of keywords.<br/>    """<br/><br/>    one_cluster = df[df['cluster']==n]<br/>    one_cluster_copy = one_cluster.copy()<br/>    sample = one_cluster_copy.key.tolist()<br/><br/>    prompt_clusters= f"""<br/>    &lt;|system|&gt;<br/>    I have the following list of keywords and keyphrases:<br/>    ['encryption','attribute','firewall','security properties',<br/>    'network security','reliability','surveillance','distributed risk factors',<br/>    'still vulnerable','cryptographic','protocol','signaling','safe',<br/>    'adversary','message passing','input-determined guards','secure communication',<br/>    'vulnerabilities','value-at-risk','anti-spam','intellectual property rights',<br/>    'countermeasures','security implications','privacy','protection',<br/>    'mitigation strategies','vulnerability','secure networks','guards']<br/><br/>    Based on the information above, first name the domain these keywords or keyphrases <br/>  belong to, secondly give a brief description of the domain.<br/>    Do not use more than 30 words for the description!<br/>    Do not provide details!<br/>    Do not give examples of the contexts, do not say 'such as' and do not list the keywords <br/>  or the keyphrases!<br/>    Do not start with a statement of the form 'These keywords belong to the domain of' or <br/>  with 'The domain'.<br/><br/>    Cybersecurity: Cybersecurity, emphasizing methods and strategies for safeguarding digital information<br/>    and networks against unauthorized access and threats.<br/>    &lt;/s&gt;<br/><br/>    &lt;|user|&gt;<br/>    I have the following list of keywords and keyphrases:<br/>    {sample}<br/>    Based on the information above, first name the domain these keywords or keyphrases belong to, secondly<br/>    give a brief description of the domain.<br/>    Do not use more than 30 words for the description!<br/>    Do not provide details!<br/>    Do not give examples of the contexts, do not say 'such as' and do not list the keywords or the keyphrases!<br/>    Do not start with a statement of the form 'These keywords belong to the domain of' or with 'The domain'.<br/>    &lt;|assistant|&gt;<br/>    """<br/><br/>    # Generate the outputs<br/>    outputs = generator(prompt_clusters,<br/>                    max_new_tokens=120,<br/>                    do_sample=True,<br/>                    temperature=0.1,<br/>                    top_k=10,<br/>                    top_p=0.95)<br/><br/>    text = outputs[0]["generated_text"]<br/><br/>    # Example string<br/>    pattern = "&lt;|assistant|&gt;\n"<br/><br/>    # Extract the output<br/>    response = text.split(pattern, 1)[1].strip(" ")<br/>    # Check if the output has the desired format<br/>    if len(response.split(":", 1)) == 2:<br/>        label  = response.split(":", 1)[0].strip(" ")<br/>        description = response.split(":", 1)[1].strip(" ")<br/>    else:<br/>        label = description = response<br/><br/>    # Add the description and the labels to the dataframe<br/>    one_cluster_copy.loc[:, 'description'] = description<br/>    one_cluster_copy.loc[:, 'label'] = label<br/><br/>    return one_cluster_copy</span></pre><p id="44aa" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Now we can apply the above function to each cluster and collect the results:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="59c8" class="qa nf fq pw b bg qb qc l qd qe">import re<br/>import pandas as pd<br/><br/># Initialize an empty list to store the cluster dataframes<br/>dataframes = []<br/>clusters = len(set(keys_df.cluster))<br/><br/># Iterate over the range of n values<br/>for n in range(clusters-1):<br/>    df_result = extract_description(keys_df,n)<br/>    dataframes.append(df_result)<br/><br/># Concatenate the individual dataframes<br/>final_df = pd.concat(dataframes, ignore_index=True)</span></pre><p id="cb3b" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Let’s take a look at a sample of outputs. For complete list of outputs please refer to the <a class="af nd" href="https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab</a>.</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml rd"><img src="../Images/bd5b0a43c213efe4df24d1d6b122cb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nSNTwOisHIJKNvk7vQMbQ.png"/></div></div></figure><p id="aa5b" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">We must remember that LLMs, with their inherent probabilistic nature, can be unpredictable. While they generally adhere to instructions, their compliance is not absolute. Even slight alterations in the prompt or the input text can lead to substantial differences in the output. In the <code class="cx pt pu pv pw b">extract_description()</code> function, I've incorporated a feature to log the <em class="qi">response</em> in both <em class="qi">label</em> and <em class="qi">description</em> columns in those cases where the <em class="qi">Label: Description</em> format is not followed, as illustrated by the irregular output for cluster 7 above. The outputs for the entire set of 60 clusters are available in the accompanying <a class="af nd" href="https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/KeyBERT_LLM_Neo4j.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab</a> notebook.</p><p id="bd52" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">A second observation, is that each cluster is parsed independently by the LLM and it is possible to get repeated labels. Additionally, there may be instances of recurring keywords extracted from the input list.</p><p id="4dc7" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The effectiveness of the process is highly reliant on the choice of the LLM and issues are minimal with a highly performant LLM. The output also depends on the quality of the keyword clustering and the presence of an inherent topic within the cluster.</p><p id="4622" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Strategies to mitigate these challenges depend on the cluster count, dataset characteristics and the required accuracy for the project. Here are two options:</p><ul class=""><li id="604c" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Manually rectify each issue, as I did in this project. With only 60 clusters and merely three erroneous outputs, manual adjustments were made to correct the faulty outputs and to ensure unique labels for each cluster.</li><li id="355f" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">Employ an LLM to make the corrections, although this method does not guarantee flawless results.</li></ul><h1 id="120c" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Build the Knowledge Graph</h1><h2 id="0dd1" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">Data to Upload into the Graph</h2><p id="9a90" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">There are two csv files (or pandas dataframes if working in a single session) to extract the data from.</p><ul class=""><li id="4cfa" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk"><code class="cx pt pu pv pw b">articles</code> - it contains unique <code class="cx pt pu pv pw b">id</code> for each article, <code class="cx pt pu pv pw b">title</code> , <code class="cx pt pu pv pw b">abstract</code> and <code class="cx pt pu pv pw b">titles_keys</code> which is the list of five extracted keywords or keyphrases;</li><li id="f95d" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk"><code class="cx pt pu pv pw b">keywords</code> - with columns <code class="cx pt pu pv pw b">key</code> , <code class="cx pt pu pv pw b">cluster</code> , <code class="cx pt pu pv pw b">description</code> and <code class="cx pt pu pv pw b">label</code> , where <code class="cx pt pu pv pw b">key</code> contains a complete list of unique keywords or keyphrases, and the remaining features describe the cluster the keyword belongs to.</li></ul><h2 id="adeb" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">Neo4j Connection</h2><p id="cf2e" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">To build a knowledge graph, we start with setting up a Neo4j instance, choosing from options like Sandbox, AuraDB, or Neo4j Desktop. For this project, I’m using AuraDB’s free version. It is straightforward to launch a blank instance and download its credentials.</p><p id="2e6a" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Next, establish a connection to Neo4j. For convenience, I use a custom Python module, which can be found at <code class="cx pt pu pv pw b">[utils/neo4j_conn.py](&lt;https://github.com/SolanaO/Blogs_Content/blob/master/keyllm_neo4j/utils/neo4j_conn.py&gt;)</code> . This module contains methods for connecting and interacting with the graph database.</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="6918" class="qa nf fq pw b bg qb qc l qd qe"># Install neo4j<br/>!pip install neo4j<br/><br/># Import the connector<br/>from utils.neo4j_conn import *<br/><br/># Graph DB instance credentials<br/>URI = 'neo4j+ssc://xxxxxx.databases.neo4j.io'<br/>USER = 'neo4j'<br/>PWD = 'your_password_here'<br/><br/># Establish the connection to the Neo4j instance<br/>graph = Neo4jGraph(url=URI, username=USER, password=PWD)</span></pre><p id="8f7d" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">The graph we are about to build has a simple schema consisting of three nodes and two relationships:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml re"><img src="../Images/71d425143be8fb32291f099340ec6c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMFiXsWw0Oov1uVs0_yzIw.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">— Image by author —</figcaption></figure><p id="a348" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Building the graph now is straightforward with just two Cypher queries:</p><pre class="mn mo mp mq mr px pw py bp pz bb bk"><span id="b9ae" class="qa nf fq pw b bg qb qc l qd qe"># Load Keyword and Topic nodes, and the relationships HAS_TOPIC<br/>query_keywords_topics = """<br/>    UNWIND $rows AS row<br/>    MERGE (k:Keyword {name: row.key})<br/>    MERGE (t:Topic {cluster: row.cluster, description: row.description, label: row.label})<br/>    MERGE (k)-[:HAS_TOPIC]-&gt;(t)<br/>    """<br/>graph.load_data(query_keywords_topics, keywords)<br/><br/># Load Article nodes and the relationships HAS_KEY<br/>query_articles = """<br/>    UNWIND $rows as row<br/>    MERGE (a:Article {id: row.id, title: row.title, abstract: row.abstract})<br/>    WITH a, row<br/>    UNWIND row.titles_keys as key<br/>    MATCH (k:Keyword {name: key})<br/>    MERGE (a)-[:HAS_KEY]-&gt;(k)<br/>    """<br/>graph.load_data(query_articles, articles)</span></pre><h2 id="f948" class="qj nf fq bf ng qk ql qm nj qn qo qp nm oj qq qr qs on qt qu qv or qw qx qy qz bk">Query the Graph</h2><p id="f225" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">Let’s check the distribution of the nodes and relationships on types:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml rf"><img src="../Images/01877831cc27975b9ded0373be022c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-H0r7zpXvmUvGgZuipXyA.png"/></div></div></figure><p id="026a" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">We can find what individual topics (or clusters) are the most popular among our collection of articles, by counting the cumulative number of articles associated to the keywords they are connected to:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml rg"><img src="../Images/cb582327dfffc75475762e64f9ecad96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RBJ_sJP-2PNpTGToHwV1A.png"/></div></div></figure><p id="c46d" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Here is a snapshot of the node <code class="cx pt pu pv pw b">Semantics</code> that corresponds to cluster 58 and its connected keywords:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml rh"><img src="../Images/42523efd9df95b35eb5d9ef5c4743273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hL9YnFj010RGtzEKTUkkxQ.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">— Image by author —</figcaption></figure><p id="91e8" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">We can also identify commonly occurring works in titles, using the query below:</p><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml ri"><img src="../Images/0fe0b27744cbcc18c6df860cf2221184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4U2rzvHOrwpGUuKHEiynPw.png"/></div></div></figure><h1 id="7bb3" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Conclusion</h1><p id="c56a" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk">We saw how we can structure and enrich a collection of semingly unrelated short text entries. Using traditional NLP and machine learning, we first extract keywords and then we cluster them. These results guide and ground the refinement process performed by Zephyr-7B-Beta. While some oversight of the LLM is still neccessary, the initial output is significantly enriched. A knowledge graph is used to reveal the newly discovered connections in the corpus.</p><p id="574c" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk">Our key takeaway is that no single method is perfect. However, by strategically combining different techniques, acknowledging their strenghts and weaknesses, we can achieve superior results.</p><h1 id="75ee" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">References</h1><p id="03b7" class="pw-post-body-paragraph oa ob fq oc b go od oe of gr og oh oi oj ok ol om on oo op oq or os ot ou ov fj bk"><strong class="oc fr">Google Colab Notebook and Code</strong></p><ul class=""><li id="ee6d" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk"><a class="af nd" href="https://github.com/SolanaO/Blogs_Content/tree/master/keyllm_neo4j" rel="noopener ugc nofollow" target="_blank">Link to associated Github Repo.</a></li></ul><p id="5e1f" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk"><strong class="oc fr">Data</strong></p><ul class=""><li id="f5e1" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk">Repository of scholary articles: <a class="af nd" href="https://www.kaggle.com/datasets/Cornell-University/arxiv" rel="noopener ugc nofollow" target="_blank">arXiv Dataset</a> that has <a class="af nd" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">CC0: Public Domain</a> license.</li></ul><p id="7bf8" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk"><strong class="oc fr">Technical Documentation</strong></p><ul class=""><li id="766a" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk"><a class="af nd" href="https://maartengr.github.io/KeyBERT/guides/quickstart.html" rel="noopener ugc nofollow" target="_blank">KeyBERT and KeyLLM</a> — repository pages.</li><li id="2a3c" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk"><a class="af nd" href="https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html#" rel="noopener ugc nofollow" target="_blank">HDBSCAN</a> — documentation.</li><li id="1e47" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk"><a class="af nd" href="https://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">UMAP</a> — documentation.</li></ul><p id="c4c0" class="pw-post-body-paragraph oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov fj bk"><strong class="oc fr">Blogs and Articles</strong></p><ul class=""><li id="b9f8" class="oa ob fq oc b go ow oe of gr ox oh oi oj oy ol om on oz op oq or pa ot ou ov pl pm pn bk"><a class="af nd" rel="noopener" target="_blank" href="/introducing-keyllm-keyword-extraction-with-llms-39924b504813">Maarten Grootendorst, Introducing KeyLLM — Keyword Extraction with LLMs</a>, Towards Data Science, Oct 5, 2023.</li><li id="8cf9" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk"><a class="af nd" rel="noopener" target="_blank" href="/zephyr-7b-beta-a-good-teacher-is-all-you-need-c931fcd0bfe7">Benjamin Marie, Zephyr 7B Beta: A Good Teacher Is All You Need</a>, Towards Data Science, Nov 10, 2023.</li><li id="6578" class="oa ob fq oc b go po oe of gr pp oh oi oj pq ol om on pr op oq or ps ot ou ov pl pm pn bk">The H4 Team, Zephyr: Direct Distillation of LM Alignment, Technical Report, <a class="af nd" href="https://arxiv.org/pdf/2310.16944.pdf" rel="noopener ugc nofollow" target="_blank">arXiv: 2310.16944</a>, Oct 25, 2023.</li></ul></div></div></div></div>    
</body>
</html>