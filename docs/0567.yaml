- en: Leveraging Large Language Models for Business Efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/leveraging-large-language-models-for-business-efficiency-b06cb943a286?source=collection_archive---------2-----------------------#2024-03-01](https://towardsdatascience.com/leveraging-large-language-models-for-business-efficiency-b06cb943a286?source=collection_archive---------2-----------------------#2024-03-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Implementing Large Language Models for Business Improvement: A Step-by-Step
    Guide'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benoit_courty?source=post_page---byline--b06cb943a286--------------------------------)[![Benoît
    Courty](../Images/3c68e22acdb98c15c7fdd5a33be6d260.png)](https://medium.com/@benoit_courty?source=post_page---byline--b06cb943a286--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b06cb943a286--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b06cb943a286--------------------------------)
    [Benoît Courty](https://medium.com/@benoit_courty?source=post_page---byline--b06cb943a286--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b06cb943a286--------------------------------)
    ·15 min read·Mar 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**TL;DR:** This article talks about how Large Language Models can improve your
    company process. Its target audience is people with technical backgrounds like
    software architects or CTO. The article shows the options to use LLM efficiently,
    you will learn how to use modern techniques like Retrieval Augmented Generation
    (RAG), function calling and fine-tuning with examples on a use case.'
  prefs: []
  type: TYPE_NORMAL
- en: Table of content
  prefs: []
  type: TYPE_NORMAL
- en: · [Identifying a Business Need](#46f5)
  prefs: []
  type: TYPE_NORMAL
- en: · [Explore an idea by yourself](#e419)
  prefs: []
  type: TYPE_NORMAL
- en: · [Creating an Evaluation Dataset](#4336)
  prefs: []
  type: TYPE_NORMAL
- en: · [Considering Internal Industrialization](#2e17)
  prefs: []
  type: TYPE_NORMAL
- en: · [Customizing Responses with Company Data](#5f9c)
  prefs: []
  type: TYPE_NORMAL
- en: · [Function Calling to use APIs](#ef52)
  prefs: []
  type: TYPE_NORMAL
- en: · [Breaking Down Tasks into Multiple Prompts](#4391)
  prefs: []
  type: TYPE_NORMAL
- en: · [Fine-tuning to improve performance](#a809)
  prefs: []
  type: TYPE_NORMAL
- en: · [Combining Model](#0397)
  prefs: []
  type: TYPE_NORMAL
- en: · [Conclusion](#5549)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ff7d5145d86e9094d2342638cf3ce6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In the rapidly evolving landscape of technology, Artificial Intelligence (AI)
    and Machine Learning (ML) have emerged as pivotal forces driving innovation, efficiency,
    and competitive advantage across industries. For Chief Technology Officers, IT
    Directors, Tech Project Managers, and Tech Product Managers, understanding and
    integrating these technologies into business strategies is no longer optional;
    it’s imperative.
  prefs: []
  type: TYPE_NORMAL
- en: It’s not a surprise, Large language Models (LLMs) like ChatGPT could do more
    than chat.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore step by step strategies to prevent data distortion, enhance
    operational efficiency, and better use your company’s resources.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying a Business Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You already know that Large Language Models (LLMs) such as ChatGPT, Gemini,
    Mistral, etc… have emerged as powerful tools that can automate tasks and enhance
    customer service. As a business decision-maker, understanding the capabilities
    and limitations of LLMs can help you make informed decisions about their implementation.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in leveraging LLMs is to identify a task that can be automated
    to improve customer service or relieve employees of repetitive tasks. For instance,
    LLMs can be used to automate information retrieval in documents, write reports,
    or process customer requests.
  prefs: []
  type: TYPE_NORMAL
- en: Explore an idea by yourself
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you have identified a business case, the next step is to manually evaluate
    this with ChatGPT (or Gemini) to estimate if the current reasoning capabilities
    of generative AI are sufficient to meet the need.
  prefs: []
  type: TYPE_NORMAL
- en: You can create a list of sample inputs and evaluate the accuracy of the responses
    generated by ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to automate the dispatching of emails your company receives.
  prefs: []
  type: TYPE_NORMAL
- en: You have to get some emails and test if an online LLM is able to sort them and
    prepare an answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/532ebdc0bb344935bdbd2ff2c95786c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Serhat Beyazkaya](https://unsplash.com/@serhatbeyazkaya?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a sample prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Hi,
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope this email finds you well. I am writing to request time off from work
    for the upcoming holiday season. Specifically, I would like to take the following
    days off:'
  prefs: []
  type: TYPE_NORMAL
- en: Monday, December 20th
  prefs: []
  type: TYPE_NORMAL
- en: Tuesday, December 21st
  prefs: []
  type: TYPE_NORMAL
- en: Wednesday, December 22nd
  prefs: []
  type: TYPE_NORMAL
- en: Thursday, December 23rd
  prefs: []
  type: TYPE_NORMAL
- en: Friday, December 24th
  prefs: []
  type: TYPE_NORMAL
- en: I understand that this is a busy time of year for the company, and I apologize
    for any inconvenience my absence may cause. However, I have accrued enough vacation
    time to cover these days, and I believe that taking this time off will allow me
    to come back refreshed and ready to tackle the new year.
  prefs: []
  type: TYPE_NORMAL
- en: Please let me know if there are any issues with my request, or if you need any
    additional information from me. Thank you for your time and consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Best regards, [Employee Name]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The model answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ⚠️ Don’t hesitate to iterate with your prompting until you get the intended
    form of response you want to reproduce. This could take some time to get used
    to it. Don’t stop at first failure, nor first success. Don’t hesitate to restart
    from scratch. You could even challenge the model to write a prompt for you.
  prefs: []
  type: TYPE_NORMAL
- en: ⚠️ Don’t forget to test edge cases, be sure to evaluate at least for 80% of
    your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an Evaluation Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to expand your test. Previously you proved that it worked with few
    examples, now try again with more data to measure the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset is essential to evaluate a process. You can use this dataset
    to test the model, ensuring that it meets your business needs. The dataset should
    be representative of the real-world scenarios that your business will encounter.
    Ideally, one should use code to create a reproducible evaluation chain. For example,
    by calling OpenAI’s API from a list of questions and automatically comparing expected
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: With a ChatGPT subscription if you look at Explore GPTs you can also try Data
    Analyst to upload an Excel file and interact with the AI on it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compile an Email Dataset**: Start by assembling an Excel file containing
    100 sample emails that your company might receive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Draft a Detailed Prompt**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case you can structure your prompt in three segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 1**: Detail the various departments within your company, outlining their
    specific functions and areas of responsibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part 2**: Introduce the dataset to the model, instructing it to analyze the
    content of each email to determine the most suitable department.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Part 3**: Direct the model to append its recommendations in a new column
    within your Excel file, effectively categorizing each email.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Execute and Evaluate**: Utilize the prompt to task the model with identifying
    the correct recipient department for each email. Following the model’s processing,
    review its suggestions to assess accuracy and relevance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f617610727359982d5d6ecf15c73d01d.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of a sample dataset (AI generated by the autor with Mistral-medium)
  prefs: []
  type: TYPE_NORMAL
- en: 'Before considering going further you can manually rate each answer and compute
    the average to evaluate if the result is good enough for this use case. In our
    example, remember that the use case is a <human>(email) to <machine> (routing
    & proposed answer) to <human> (department) workflow, so an error can be tolerated
    : the human could modify the answer, or a department can reroute an email to another…
    If it happens on ten emails in a hundred it can be good enough.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering Internal Industrialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can fastrack a production ready solution by using an API provided by an
    external provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use OpenAI API or others for your MVP, but there are several factors
    that you should consider, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '**All the Data you provide to an external API or chat is recorded somewhere**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should anonymize your data even if the service provider claims that it is
    not using your data…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Risk of industrial **secret leakage**: If you are outside of the US, be aware
    that OpenAI is subject to the [Cloud Act](https://en.wikipedia.org/wiki/CLOUD_Act).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed limitations**: It often takes several seconds to obtain a complete
    response from OpenAI, which may not be fast enough for certain use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Call limitations**: The number of calls per second [are limited](https://platform.openai.com/docs/guides/rate-limits?context=tier-five),
    as well as maximum monthly expenses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental impact**: Large generalist models have a [significant environmental
    impact](/chatgpts-energy-use-per-query-9383b8654487), and this should be taken
    into account when considering their use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost variation**: ie OpenAI APIs are subject to cost variation, which can
    impact your budget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Difficulty in asserting a **competitive advantage**: It can be challenging
    to assert a competitive advantage when using OpenAI APIs, as they are available
    to all businesses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stability**: LLM private models like Gemini, Mistral, Claude2, GPT4 … are
    not always stable and you should consider monitoring the quality and stability
    of the answers provided. You also have to add rail guards to protect your service
    quality and you & your customers from hazardous behaviors coming from in and out.
    Problems can occur from the input or the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To avoid some of these pitfalls, you can turn to open-source models such as
    LLAMA or Mistral. These open-source alternatives offer several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy and Security**: Self hosted models, reduce the risk of industrial
    secret leakage.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Customization**: You can fine-tune open-source models to better suit your
    specific business needs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lower Costs**: Open-source models are often less expensive than proprietary
    solutions, especially when considering the limitations on the number of calls
    and monthly expenses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Environmental Impact**: Open-source models are smaller and can be optimized
    for specific use cases, potentially reducing their environmental footprint. You
    could measure it with [CodeCarbon](https://codecarbon.io/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Competitive Advantage**: By customizing an open-source model, you can create
    a unique solution that sets your business apart from competitors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now you have automated the routing of the email, let’s improve the quality of
    the answer. A way to do it is to add company documents to the capability of the
    model. This will allow the model to find answers in your document instead of his
    “memory”.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing Responses with Company Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customizing responses from a LLM with company data will create a more accurate
    and tailored experience for users.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3443cc7f2e6867538d16ab5847fde206.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Yasamine June](https://unsplash.com/@yasamine?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: You can’t send all company data within the prompt. That’s why [**Retrieval Augmented
    Generation**](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html)(RAG)
    is useful, it’s a technique that combines information retrieval from a database
    and generation capabilities of a LLM. By using RAG, you can improve the accuracy
    of responses. And you could tell to the user which documents have been used for
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG technique can be simply presented by this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: <LLM trained with billion of data> + <**Your prompt**> + <**Your company dataset**>
    = **Responses aligned with your context**
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG is often done with a vector database as it works in most cases, here is
    how to create the database:'
  prefs: []
  type: TYPE_NORMAL
- en: Split your documents by shorts chapters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert chapters to vectors using an [embedding model](/neural-network-embeddings-explained-4d028e6f0526).
    The vector on the same subjects will be near in the n-dimensional spaces. Typical
    vector is an array of 1,024 floats values. Think of it like if each value represents
    a characteristic, like color, size, gender… It’s not hard coded, the model finds
    the value by himself in training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Store them in a vector database
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/9e9f8462bd722e026c4a767adfe505b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'When you receive an email, you will use RAG like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the email of your customer to a vector
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query the database with this vector to retrieve the 10 nearest vectors of paragraphs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take the text of these paragraphs and add them to the prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask the LLM for an answer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The answer will be based on the data provided in the prompt
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/25cdc87b81e0f1bef58cf2c207b834a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more, read [**Retrieval Augmented Generation**](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)(RAG)
  prefs: []
  type: TYPE_NORMAL
- en: Now your answer will be using your data, so it helps prevent what is called
    *hallucination*.
  prefs: []
  type: TYPE_NORMAL
- en: ℹ️ Model Hallucination is not an easy problem to manage. Because the “memory”
    of a LLM is more like a human memory (compressed representation of the world)
    than a computer’s exact memory. And models are trained to help you so they will
    try to, even when they don’t know the answer, misleading information will be presented
    as fact. RAG helps cope with this problem by providing relevant data to the model.
  prefs: []
  type: TYPE_NORMAL
- en: RAG is really good for unstructured data, but sometimes you have a better way
    to answer the question like tabular data with pricing for each product, or you
    may even want to compute taxes, or looking for a slot in an agenda to arrange
    a meeting. Let’s see how to do that with *function calling*.
  prefs: []
  type: TYPE_NORMAL
- en: Function Calling to use APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Function calling is a way to allow interaction between a LLM and your enterprise
    API, like:'
  prefs: []
  type: TYPE_NORMAL
- en: Salesforce, SAP for your ERP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service Now or other ticketing services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agendas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invoice, pricing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Custom API to do anything in your company
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third party API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Function calling](https://platform.openai.com/docs/guides/function-calling)
    is an essential feature that allows you to use APIs without exposing them to the
    outside world. This feature opens up many possibilities beyond simple chat applications.
    For instance, you can integrate specialized internal services or tools into the
    LLM, making it more versatile and valuable for your business. You can take a mail
    from a customer requesting a price, send it to the LLM to turn it into a parameter
    to call your pricing API, then use the API answer to ask the LLM back to write
    the answer to the customer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the request:'
  prefs: []
  type: TYPE_NORMAL
- en: “
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Hello,'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I really like your company. I would like to order you a solar panel mounting
    rail, what would be the price ?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Best regards* “
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You send the request to the LLM, with the definitions of the API that exist
    in your company:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'So the LLM extract the product name from the mail and give you the JSON to
    make the API call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It’s up to you to call the API, so it is totally secured : the LLM never knows
    where your API is, just what it can do.'
  prefs: []
  type: TYPE_NORMAL
- en: The answer of the API could be sent back to the LLM to build a natural language
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Can you answer this email given that the price for a “solar panel mounting
    rail” is $10 without a VAT of 5% ? “Hello, I really like your company. I would
    like to order you a solar panel mounting rail, what would be the price ? Best
    regards Your customer “*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The answer will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Hello,*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Thank you for your interest in our company and for considering our solar panel
    mounting rail. The price for the mounting rail is 10 before taxes, with a VAT
    of 5%, so $10.50 taxes included.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Please let me know if you have any other questions or if you would like to
    proceed with the order.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Best regards,*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So you now have a system that can use your internal services to better prepare
    answers for your customers. That’s a game changer if you have already [invested
    in APIs](https://blog.dreamfactory.com/api-first-the-advantages-of-an-api-first-approach-to-app-development/).
  prefs: []
  type: TYPE_NORMAL
- en: We just saw that we may call a LLM more than once for a single task, let see
    that in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Down Tasks into Multiple Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s important to note that a single prompt is often not enough for complex
    tasks. Your project will likely require breaking down the task into multiple prompts
    that will chain together and combine several techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For exemple [https://360learning.com/](https://360learning.com/) build a platform
    to help building online courses with AI from a single text document as input.
    Their pipelines make use of 9 prompts, used for 30 OpenAI calls, and RAG to achieve
    their goal. A first prompt asks for a resume of the document, a second asks for
    a plan for an online course from the resume, then RAG is used to retrieve each
    part of the document from the title, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is some slides of their presentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd6966165afb053a352611e5d59bade0.png)'
  prefs: []
  type: TYPE_IMG
- en: Caption from [360learning](https://360learning.com/)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2afd654348d284f9094fc757ec7440ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Caption from [360learning](https://360learning.com/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Video source : [https://www.youtube.com/watch?v=1Eyc2GypnF4](https://www.youtube.com/watch?v=1Eyc2GypnF4)
    (in French)'
  prefs: []
  type: TYPE_NORMAL
- en: They are using LangChain, a framework that helps to create these types of LLM
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'ℹ️ You probably heard of “AI Agents”: they are *just* a way to combine prompts,
    but without writing them in advance. An agent is a call to a LLM to get a list
    of tasks. Then, make a call to LLM for each task, and so on. It works best with
    giving the ability to the LLM to call external tools like browsing the web using
    functions like we saw before.'
  prefs: []
  type: TYPE_NORMAL
- en: Now you have a powerful pipeline, but how to improve the model itself to have
    faster and better answers ? You can fine tune a model.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning to improve performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fine-tuning can often improve the model’s performance and reduce its size while
    maintaining equal performance, because you could use smaller models, like [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/),
    or even [Phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/).
  prefs: []
  type: TYPE_NORMAL
- en: Very few companies could afford to train a LLM from scratch because it requires
    a huge dataset and hundreds of GPUs, almost 2 millions GPU hours for Llama2–70B
    for example. But you can take an already pre-trained model and fine-tune it, only
    an afternoon of fine-tuning is needed in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: The drawback is that you have to build a training dataset with hundreds of questions
    and answers.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s a new technique to combine multiple models in one. The result is a big
    model, called [Mixture of Experts](https://medium.aiplanet.com/create-your-own-mixture-of-experts-model-with-mergekit-and-runpod-8b3e91fb027a)
    (MoE), with better capabilities than a single of the same size. The easiest way
    to do that is with [MergeKit](https://medium.com/towards-data-science/merge-large-language-models-with-mergekit-2118fb392b54).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29ea2b44bbdd41147283fb765f16bdd0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Generated with AI** — Bing Copilot — “An image of a mathematician, a physicist
    and a mechanical engineer working on the same problem around a desk featuring
    a dismantled uav”'
  prefs: []
  type: TYPE_NORMAL
- en: 'This could help you if it’s difficult to decide which model to use : with MoE,
    it’s the model who decides which one to use.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Customizing responses from LLMs with company data and API create a more accurate
    and tailored experience for users. Fine-tuning can improve the performance, and
    breaking down tasks into multiple prompts can help tackle complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: While all of this may seem complex and reserved for specialists, abundant documentation
    and numerous libraries are available to facilitate implementation. Popular libraries
    include [HuggingFace](https://github.com/huggingface/transformers), [Langchain](https://python.langchain.com/docs/get_started/introduction),
    [HayStack](https://haystack.deepset.ai/), [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)
    and so on…
  prefs: []
  type: TYPE_NORMAL
- en: However, don’t forget the cost of integration. As with any project, there is
    a significant cost associated with moving from a functional prototype to a fully
    industrialized solution within an existing IT system. You will often discover
    that the process of your company is more complex than expected. Or that the data
    needs a bit of cleaning to be processed.
  prefs: []
  type: TYPE_NORMAL
- en: While large language models offer many advantages, don’t neglect the benefits
    of “older” machine learning techniques like [random forest](https://www.ibm.com/topics/random-forest)
    or [DistiliBert](https://huggingface.co/docs/transformers/model_doc/distilbert).
    These techniques can still provide values, including faster processing, easier
    integration into existing tools, no need for GPUs, better explainability, and
    lower costs.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that this article provides a view on how to include LLM in your software
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Article written in february 2024 by Benoît Courty, data scientist, with the
    help of Stéphane Van-Bosterhaudt, CEO of [UpScale](https://upscaleparis.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: 'More readings on the subject:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud Act: [https://en.wikipedia.org/wiki/CLOUD_Act](https://en.wikipedia.org/wiki/CLOUD_Act)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAG: [https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Embeddings: [https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526](/neural-network-embeddings-explained-4d028e6f0526)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Function calling: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning: [https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mixture of Experts: [https://medium.aiplanet.com/create-your-own-mixture-of-experts-model-with-mergekit-and-runpod-8b3e91fb027a](https://medium.aiplanet.com/create-your-own-mixture-of-experts-model-with-mergekit-and-runpod-8b3e91fb027a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
