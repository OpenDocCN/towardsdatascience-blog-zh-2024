- en: A Beginner’s Guide to Building Knowledge Graphs from Videos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5?source=collection_archive---------5-----------------------#2024-01-17](https://towardsdatascience.com/a-beginners-guide-to-building-knowledge-graphs-from-videos-6cafcba5f3e5?source=collection_archive---------5-----------------------#2024-01-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Build a pipeline to analyze and store the data within videos.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mohammed249.medium.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)[![Mohammed
    Mohammed](../Images/33e1776db18c6f71c5b4138fd4536043.png)](https://mohammed249.medium.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)
    [Mohammed Mohammed](https://mohammed249.medium.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6cafcba5f3e5--------------------------------)
    ·10 min read·Jan 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the technical aspect of the article let’s set the context
    and answer the question that you might have, What is a knowledge graph ?
  prefs: []
  type: TYPE_NORMAL
- en: And to answer this, imagine instead of storing the knowledge in cabinets we
    store them in a fabric net. Each fact, concept, piece of information about people,
    places, events, or even abstract ideas are knots, and the line connecting them
    together is the relationship they have with each other. This intricate web, my
    friends, is the essence of a knowledge graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ecf3cc7e1b15408e2b01383c3a18df7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Shubham Dhage](https://unsplash.com/@theshubhamdhage?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Think of it like a bustling city map, not just showing streets but revealing
    the connections between landmarks, parks, and shops. Similarly, a knowledge graph
    doesn’t just store cold facts; it captures the rich tapestry of how things are
    linked. For example, you might learn that Marie Curie discovered radium, then
    follow a thread to see that radium is used in medical treatments, which in turn
    connect to hospitals and cancer research. See how one fact effortlessly leads
    to another, painting a bigger picture?
  prefs: []
  type: TYPE_NORMAL
- en: So why is this map-like way of storing knowledge so popular? Well, imagine searching
    for information online. Traditional methods often leave you with isolated bits
    and pieces, like finding only buildings on a map without knowing the streets that
    connect them. A knowledge graph, however, takes you on a journey, guiding you
    from one fact to another, like having a friendly guide whisper fascinating stories
    behind every corner of the information world. Interesting right? I know.
  prefs: []
  type: TYPE_NORMAL
- en: Since I discovered this magic, it captured my attention and I explored and played
    around with many potential applications. In this article, I will show you how
    to build a pipeline that extracts audio from video, then transcribes that audio,
    and from the transcription, build a knowledge graph allowing for a more nuanced
    and interconnected representation of information within the video.
  prefs: []
  type: TYPE_NORMAL
- en: 'I will be using Google Drive to upload the video sample. I will also use Google
    Colab to write the code, and finally, you need access to the GPT Plus API for
    this project. I will break this down into steps to make it clear and easy for
    beginners:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up everything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting audio from video.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transcribing audio to text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the knowledge graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this article, you will construct a graph with the following schema.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06861e8c6ec369ade11ea179528932f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive right into it!
  prefs: []
  type: TYPE_NORMAL
- en: '**1- Setting up everything**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, we will be using Google Drive and Colab. In the first cell, let’s
    connect Google Drive to Colab and create our directory folders (video_files, audio_files,
    text_files). The following code can get this done. (*If you want to follow along
    with the code, I have uploaded all the code for this project on GitHub; you can
    access it from* [***here***](https://github.com/mohammed-249/Data_Science_Projects/tree/main/NLP%20%7C%20Building%20Knowledge%20Graph%20from%20videos)*.*)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Or you can create the folders manually and upload your video sample to the
    “video_files” folder, whichever is easier for you.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now we have our three folders with a video sample in the “video_files” folder
    to test the code.
  prefs: []
  type: TYPE_NORMAL
- en: 2- Extracting audio from video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next thing we want to do is to import our video and extract the audio from
    it. We can use the *Pydub* library, which is a high-level audio processing library
    that can help us to do that. Let’s see the code and then explain it underneath.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After installing our package *pydub*, we imported the *AudioSegment* class from
    the *Pydub* library. Then, we created a loop that iterates through all the video
    files in the “video_files” folder we created earlier and passes each file through
    *AudioSegment.from_file* to load the audio from the video file. The loaded audio
    is then exported as a WAV file using *audio.export* and saved in the specified
    “audio_files” folder with the same name as the video file but with the extension
    .wav.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you can go to the “audio_files” folder in Google Drive where
    you will see the extracted audio.
  prefs: []
  type: TYPE_NORMAL
- en: 3- Transcribing audio to text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the third step, we will transcribe the audio file we have to a text file
    and save it as a .txt file in the “text_files” folder. Here I used the Whisper
    ASR (Automatic Speech Recognition) system from OpenAI to do this. I used it because
    it’s easy and fairly accurate, beside it has different models for different accuracy.
    But the more accurate the model is the larger the model the slower to load, hence
    I will be using the medium one just for demonstration. To make the code cleaner,
    let’s create a function that transcribes the audio and then use a loop to use
    the function on all the audio files in our directory
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Libraries Used:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*os*: Provides a way of interacting with the operating system, used for handling
    file paths and names.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*re*: Regular expression module for pattern matching and substitution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*subprocess*: Allows the creation of additional processes, used here to execute
    the Whisper ASR system from the command line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We created a Whisper command and saved it as a variable to facilitate the process.
    After that, we used *subprocess.check_output* to run the Whisper command and save
    the resulting transcription in the transcription variable. But the transcription
    at this point is not clean (*you can check it by printing the transcription variable
    out of the function; it has timestamps and a couple of lines that are not relevant
    to the transcription*), so we added a cleaning code that removes the timestamp
    using *re.sub* and joins the sentences together. After that, we created a text
    file within the “text_files” folder with the same name as the audio and saved
    the cleaned transcription in it.
  prefs: []
  type: TYPE_NORMAL
- en: Now if you go to the “text_files” folder, you can see the text file that contains
    the transcription. Woah, step 3 done successfully! Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: 4- Building the knowledge graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the crucial part — and maybe the longest. I will follow a modular approach
    with 5 functions to handle this task, but before that, let’s begin with the libraries
    and modules necessary for making HTTP requests *requests*, handling JSON *json*,
    working with data frames *pandas*, and creating and visualizing graphs *networkx*
    and *matplotlib*. And setting the global constants which are variables used throughout
    the code. *API_ENDPOINT* is the endpoint for OpenAI’s API, *API_KEY* is where
    the OpenAI API key will be stored, and *prompt_text* will store the text used
    as input for the OpenAI prompt. All of this is done in this code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Then let’s continue with breaking down the structure of our functions:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**The first function**, *create_graph()*, the task of this function is to create
    a graph visualization using the *networkx* library. It takes a DataFrame *df*
    and a dictionary of edge labels *rel_labels* — which will be created on the following
    function — as input. Then, it uses the DataFrame to create a directed graph and
    visualizes it using *matplotlib* with some customization and outputs the beautiful
    graph we need'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The DataFrame *df* and the edge labels *rel_labels* are the output of the next
    function, which is: *preparing_data_for_graph()*. This function takes the OpenAI
    *api_response* — which will be created from the following function — as input
    and extracts the entity-relation triples (*source, target, edge*) from it. Here
    we used the *json* module to parse the response and obtain the relevant data,
    then filter out elements that have missing data. After that, build a knowledge
    base dataframe *kg_df* from the triples, and finally, create a dictionary (*relation_labels*)
    mapping pairs of nodes to their corresponding edge labels, and of course, return
    the DataFrame and the dictionary.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The third function is *call_gpt_api()*, which is responsible for making a POST
    request to the OpenAI API and output the *api_response*. Here we construct the
    data payload with model information, prompt, and other parameters like the model
    (in this case: *gpt-3.5-turbo-instruct*), max_tokens, stop, and temperature. Then
    send the request using *requests.post* and return the response. I have also included
    simple error handling to print an error message in case an exception occurs. The
    try block contains the code that might raise an exception from the request during
    execution, so if an exception occurs during this process (for example, due to
    network issues, API errors, etc.), the code within the except block will be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then the function before the last is the *main()* function, which orchestrates
    the main flow of the script. First, it reads the text file contents from the “text_files”
    folder we had earlier and saves it in the variable *kb_text*. Bring the global
    variable *prompt_text*, which stores our prompt, then replace a placeholder in
    the prompt template (*$prompt*) with the text file content *kb_text*. Then call
    the *call_gpt_api()* function, give it the *api_key* and *prompt_text* to get
    the OpenAI API response. The response is then passed to *preparing_data_for_graph()*
    to prepare the data and get the DataFrame and the edge labels dictionary, finally
    pass these two values to the *create_graph()* function to build the knowledge
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we have the *start()* function, which iterates through all the text
    files in our “text_files” folder — if we have more than one, gets the name and
    the path of the file, and passes it along with the *api_key* to the main function
    to do its job.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you have correctly followed the steps, after running the *start()* function,
    you should see a similar visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06861e8c6ec369ade11ea179528932f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: You can of course save this knowledge graph in the Neo4j database and take it
    further.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE: This workflow ONLY applies to videos you own or whose terms allow this
    kind of download/processing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge graphs use semantic relationships to represent data, enabling a more
    nuanced and context-aware understanding. This semantic richness allows for more
    sophisticated querying and analysis, as the relationships between entities are
    explicitly defined.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I outline detailed steps on how to build a pipeline that involves
    extracting audio from videos, transcribing with OpenAI’s Whisper ASR, and crafting
    a knowledge graph. As someone interested in this field, I hope that this article
    makes it easier to understand for beginners, demonstrating the potential and versatility
    of knowledge graph applications.
  prefs: []
  type: TYPE_NORMAL
- en: And as always the whole code is available in [GitHub](https://github.com/mohammed-249/Data_Science_Projects/tree/main/NLP%20%7C%20Building%20Knowledge%20Graph%20from%20videos).
  prefs: []
  type: TYPE_NORMAL
