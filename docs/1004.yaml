- en: The Business Guide to Tailoring Language AI Part 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19](https://towardsdatascience.com/the-business-guide-to-tailoring-language-ai-part-2-989a5e987f0c?source=collection_archive---------12-----------------------#2024-04-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prompting ChatGPT and other chat-based language AI — and why you should (not)
    care about it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[![Georg
    Ruile, Ph.D.](../Images/83b04db23852ba2df2818fe62250ca22.png)](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    [Georg Ruile, Ph.D.](https://medium.com/@georg.ruile?source=post_page---byline--989a5e987f0c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--989a5e987f0c--------------------------------)
    ·12 min read·Apr 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d38c05848cc0a25ec1ad62765d9fe3e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Foreword
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article sheds some light on the question of how to “talk” to Large Language
    Models (LLM) that are designed to interact in conversational ways, like ChatGPT,
    Claude and others, so that the answers you get from them are as useful as possible
    for the task at hand. This particular communication from human to the language
    chatbot is what’s typically referred to as prompting. With this article, I mean
    to give people with no computer science background a compact overview of the topic
    so that everyone can understand. It can also help businesses to contextualize
    of what (not) to expect from their LLM adaption endeavors.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting is the first of four steps you can take when climbing the ladder of
    adapting language models for your businesses’ custom use. I have introduced the
    overall **4 Step Framework** of unlocking custom LLM in [my previous post](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838).
    If you haven’t already it might be helpful to read it first so that you can fit
    the ideas presented here into a larger context.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
    [## The Business Guide to Tailoring Language AI'
  prefs: []
  type: TYPE_NORMAL
- en: A Framework for Unlocking Custom LLM Solutions You’ll Understand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-business-guide-to-tailoring-language-ai-5f0fa806e838?source=post_page-----989a5e987f0c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shortly after the mass market introduction of ChatGPT, a new, hot profession
    has entered the AI scene: **Prompt engineering**. These AI “whisperers”, i.e.
    people that have certain skills to “prompt”, that is, talk to language AI so that
    it responds in useful ways, have become highly sought-after ([and generously paid](https://www.forbes.com/sites/jackkelly/2024/03/06/the-hot-new-high-paying-career-is-an-ai-prompt-engineer/))
    new roles. Considering that a main building block of proper prompting is simply
    (or not so simply) giving precise instructions (see below), I must confess that
    I was surprised by this development (regardless of the fact that prompt *engineering*
    certainly involves more than just “whispering”): Isn’t communicating in a precise
    and concise manner a basic professional skill that we all should possess? But
    then again, I was reflecting on how important it is to have well-crafted requirements
    in software development, and “requirement engineering” roles have been an important
    ingredient of successful software development projects for a while now.'
  prefs: []
  type: TYPE_NORMAL
- en: I observe a level of uncertainty and “best guessing” and even contradictions
    in the topic of LLM and prompting that I have not yet experienced in any IT-related
    subject. This has to do with the type and size of AI models and their **stochastic
    characteristics**, which is beyond the scope of this article. Considering the
    [1.76 trillion parameters](https://en.wikipedia.org/wiki/GPT-4) of models like
    GPT-4, the number of possible combinations and paths from input (your “prompt”)
    to output (the model response) is virtually indefinite and non-deterministic.
    Hence, applications treat these models mainly as black boxes, and related research
    focuses on empirical approaches such as benchmarking their performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sad news is that I cannot present you a perfect one-size-fits-all prompting
    solution that will forever solve your LLM requirements. Add to this that **different
    models behave differently**, and you may understand my dilemma. There’s some good
    news, though: On the one hand, you can, and should, always consider some **basic
    principles and concepts** that will help you optimize your interactions with the
    machines. Well-crafted prompts gets you farther than poor ones, and this is why
    it is well worthwhile to dig a bit deeper into the topic. On the other hand, **it
    may not even be necessary to worry too much about prompting at all**, which saves
    you valuable computing time (literally, CPU/GPU and figuratively, in your own
    brain).'
  prefs: []
  type: TYPE_NORMAL
- en: Start with Why
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here I am not referring to Simon Sinek’s classic TEDx business advice. Instead,
    I encourage you to curiously wonder **why technology does what it does**. I strongly
    believe in the notion that if you understand at least a bit of the inner workings
    of software, it will tremendously help you in its application.
  prefs: []
  type: TYPE_NORMAL
- en: So how, in principle, is the input (the prompt) related to the output (the response),
    and why is it that proper prompts result in better suited responses? To figure
    this out, we need to have at least a superficial look at the model architecture
    and its training and fine-tuning, without needing to understand the details of
    the impressive underlying concepts like the infamous Transformer Architecture
    and Attention Mechanisms which ultimately caused the breakthrough of ChatGPT-like
    Generative AI as we know it today.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we can look at it from two angles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*How does the model* ***retrieve knowledge and generate its response?***and
    closely related'
  prefs: []
  type: TYPE_NORMAL
- en: '*How has the model been* ***trained and fine-tuned?***'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that an LLM is in essence a **Deep Neural Network**
    and as such, it works based on **statistics and probabilities**. Put very simplistically,
    the model generates output which reflects the closest match to the context, based
    on its knowledge it has learned from vast amounts of training data. One of the
    building blocks here are so-called **Embeddings**, where similar word meanings
    are (mathematically) close to each other, even though the model does not actually
    “understand” those meanings. If this sounds fancy, it kinda is, but at same time,
    it is “only” mathematics, so don’t be afraid.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdb1b66d57538d9d96891eb1aea06b47.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple illustration of word vector embeddings — similar word “meanings” are
    close to each other
  prefs: []
  type: TYPE_NORMAL
- en: When looking at training, it helps considering the training data and process
    a language model has gone through. Not only has the model seen vast amounts of
    text data. It has also learned what makes out a high rated response to a specific
    question, for instance on sites like StackOverflow, and on high-quality Q&A assistant
    documents written for model training and tuning. In addition, in its fine-tuning
    stage, it learned and iteratively adapted its optimal responses **based on human
    feedback**. Without all this intense training and tuning efforts, the model might
    answer a question like “what is your first name” simply with “what is your last
    name”, because it has seen this frequently on internet forms [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'Where I am trying to get at is this: When interacting with natural language
    AI, always keep in mind what and how the model has learned and how it gets to
    its output, given your input. Even though no one really knows this exactly, it
    is useful to consider probable correlations: Where and in what context could the
    model have seen input similar to yours before? What data has been available during
    the pre-training stage, and in which quality and quantity? For instance: Ever
    wondered why LLMs can solve mathematical equations (not reliably, however, sometimes
    still surprisingly), without inherent calculation capabilities? LLMs don’t calculate,
    they match patterns!'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a plethora of Prompting techniques, and plenty of scientific literature
    that benchmarks their effectiveness. Here, I just want to introduce a few well-known
    concepts. I believe that once you get the general idea, you will be able to expand
    your prompting repertoire and even develop and test new techniques yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Ask and it will be given to you
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before going into specific prompting concepts, I would like to stress a general
    idea that, in my opinion, cannot be stressed enough:'
  prefs: []
  type: TYPE_NORMAL
- en: '***The quality of your prompt highly determines the response of the model.***'
  prefs: []
  type: TYPE_NORMAL
- en: And by quality I don’t necessarily mean a sophisticated prompt construction.
    I mean the basic idea of asking a precise question or giving well-structured instructions
    and providing necessary context. I have touched on this already when we met Sam,
    the piano player, in [my previous article](https://medium.com/towards-data-science/the-business-guide-to-tailoring-language-ai-5f0fa806e838).
    If you ask a bar piano player to play some random Jazz tune, chances are that
    he might not play what you had in mind. Instead, if you ask exactly what it is
    you want to hear, your satisfaction with the result is likely to increase.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you ever had the chance of, say, hire someone to do something
    around your house and your contract specification only says, say, “bathroom renovation”,
    you might be surprised that in the end your bathroom does not look like what you
    had in mind. The contractor, just like the model, will only refer to what he has
    learned about renovations and bathroom tastes and will take the learned route
    to deliver.
  prefs: []
  type: TYPE_NORMAL
- en: 'So here are some **general guidelines for prompting**:'
  prefs: []
  type: TYPE_NORMAL
- en: · Be clear and specific.
  prefs: []
  type: TYPE_NORMAL
- en: · Be complete.
  prefs: []
  type: TYPE_NORMAL
- en: · Provide context.
  prefs: []
  type: TYPE_NORMAL
- en: · Specify the desired output style, length, etc.
  prefs: []
  type: TYPE_NORMAL
- en: This way, the model has sufficient and matching reference data in your prompt
    that it can relate to when generating its response.
  prefs: []
  type: TYPE_NORMAL
- en: Roleplay prompting — simple, but overrated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the early days of ChatGPT, the idea of roleplay prompting was all around:
    Instead of asking the assistant to give you an immediate answer (i.e. a simple
    query), you first assign it a specific role, such as “teacher” or “consultant”
    etc. Such a prompt could look like [2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '***From now on, you are an excellent math teacher and always teach your students
    math problems correctly. And I am one of your students.***'
  prefs: []
  type: TYPE_NORMAL
- en: It has been shown that this concept yields superior results. One [paper](https://arxiv.org/abs/2308.07702)
    reports that by this role play, the model implicitly triggers a step by step reasoning
    process, which is what you want it to do when applying the CoT- technique, see
    below. However, this approach has also been shown to **sometimes perform sub-optimal**
    and needs to be well designed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In my experience, simply assigning a role doesn’t do the trick. I have experimented
    with the example task from the paper referred to above. Unlike in this research,
    GPT3.5 (which is as of today the free version of OpenAI’s ChatGPT, so you can
    try it yourself) has given the correct result, using a simple query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89a418b3d6837d5d59787565537487e2.png)'
  prefs: []
  type: TYPE_IMG
- en: An example using a simple query instead of the roleplay prompt suggested by
    [2], still yielding the correct response
  prefs: []
  type: TYPE_NORMAL
- en: 'I have also experimented with different logical challenges with both simple
    queries and roleplay, using a similar prompt like the one above. In my experiments
    two things happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '*either* ***simple queries provides the correct answer on the first attempt****,
    or*'
  prefs: []
  type: TYPE_NORMAL
- en: '*both* ***simple queries and roleplay come up with false, however different
    answers***'
  prefs: []
  type: TYPE_NORMAL
- en: '**Roleplay did not outperform** the queries in any of my simple (not scientifically
    sound) experiments. Hence, I conclude that the models must have improved recently
    and the **impact of roleplay prompting is diminishing**.'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at different research, and without extensive further own experimenting,
    I believe that in order to outperform simple queries, roleplay prompts need to
    be embedded into a **sound and thoughtful design** to outperform the most basic
    approaches — or are not valuable at all.
  prefs: []
  type: TYPE_NORMAL
- en: I am happy to read your experiences on this in the comments below.
  prefs: []
  type: TYPE_NORMAL
- en: Few-Shot aka in-context learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another intuitive and relatively simple concept is what’s referred to as Few-Shot
    prompting, also referred to as in-context learning. Unlike in a Zero-Shot Prompt,
    we not only ask the model to perform a task and expect it to deliver, we additionally
    **provide (“few”) examples of the solutions**. Even though you may find this obvious
    that providing examples leads to better performance, this is quite a remarkable
    ability: These LLMs are able to in-context learn, i.e. perform new tasks via inference
    alone by conditioning on a few input-label pairs and making predictions for new
    inputs [3].'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a few-shot prompt involves
  prefs: []
  type: TYPE_NORMAL
- en: '*(1)* ***collecting examples of the desired responses****, and'
  prefs: []
  type: TYPE_NORMAL
- en: (2) writing your prompt with* ***instructions on what to do with these examples****.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a typical classification example. Here the model is given several
    examples of statements that are either positive, neutral or negative judgements.
    The model’s task is to rate the final statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/964b159019f7b64cc3c4b8f5ea1fa594.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical classification example of a Few-Shot prompt. The model is required
    to classify statements into the given categories (positive / negative)
  prefs: []
  type: TYPE_NORMAL
- en: Again, even though this is a simple and intuitive approach, I am sceptical about
    its value in state-of-the-art language models. In my (again, not scientifically
    sound) experiments, **Few-Shot prompts have not outperformed Zero-Shot in any
    case**. (The model knew already that a drummer who doesn’t keep the time, is a
    negative experience, without me teaching it…). My finding seems to be consistent
    with recent research, where even the opposite effect (**Zero-Shot outperforming
    Few-Shot**) has been shown [4].
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion and on this empirical background it is worth considering if the
    cost of design as well as computational, API and latency cost of this approach
    are a worthwhile investment.
  prefs: []
  type: TYPE_NORMAL
- en: CoT-Prompting or “Let’s think step-by-step’’
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chain of Thought (CoT) Prompting aims to make our models better at solving complex,
    multi-step reasoning problems. It can be as simple as adding the CoT instruction
    “Let’s think step-by-step’’ to the input query, to improve accuracy significantly
    [5][6].
  prefs: []
  type: TYPE_NORMAL
- en: Instead of just providing the final query or add one or few examples within
    your prompt like in the Few-Shot approach, you prompt the model to **break down
    its reasoning process into a series of intermediate steps**. This is akin to how
    a human would (ideally) approach a challenging problem.
  prefs: []
  type: TYPE_NORMAL
- en: Remember your math exams in school? Often, at more advanced classes, you were
    asked to not only solve a mathematical equation, but also write down the logical
    steps how you arrived at the final solution. And even if it was incorrect, you
    might have gotten some credits for mathematically sound solution steps. Just like
    your teacher in school, you expect the model to break the task down into sub-tasks,
    perform intermediate reasoning, and arrive at the final answer.
  prefs: []
  type: TYPE_NORMAL
- en: Again, I have experimented with CoT myself quite a bit. And again, most of the
    time, simply adding **“Let’s think step-by-step” didn’t improve the quality of
    the response**. In fact, it seems that the **CoT approach has become an implicit
    standard** of the recent fine-tuned chat-based LLM like ChatGPT, and the response
    is frequently broken down into chunks of reasoning without the explicit command
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, I came across one instance where the explicit CoT **command did in
    fact improve the answer significantly**. I used a CoT example from [this article](https://medium.com/@thomasczerny/chain-of-thought-cot-prompting-9ee4967e927c),
    however, altered it into a trick question. Here you can see how ChatGPT fell into
    my trap, when not explicitly asked for a CoT approach (even though the response
    shows a step wise reasoning):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d66d445d5c947e3d7919d1c8553ca5da.png)'
  prefs: []
  type: TYPE_IMG
- en: A trick question with a simple query instead of a CoT prompt. Even though the
    response is broken down “step by step”, it is not quite correct.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I added “Let’s think step-by-step” to the same prompt, it solved the trick
    question correctly (well, it is unsolvable, which ChatGPT rightfully pointed out):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/286f905eaa9ca7c70dc194b2da1c2fd3.png)'
  prefs: []
  type: TYPE_IMG
- en: The same trick question with an explicit CoT prompt, delivering a correct response
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, Chain of Thought prompting aims at building up reasoning skills
    that are otherwise difficult for language models to acquire implicitly. It encourages
    models to articulate and refine their reasoning process rather than attempting
    to jump directly from question to answer.
  prefs: []
  type: TYPE_NORMAL
- en: Again, my experiments have revealed **only limited benefits of the simple CoT
    approach** (adding “Let’s think step-by-step“). **CoT did outperform a simple
    query on one occasion**, and at the same time the extra effort of adding the CoT
    command is minimal. This **cost-benefit ratio** is one of the reasons why this
    approach is one of my favorites. Another reason why I personally like this approach
    is, it not only helps the model, but also can **help us humans to reflect** and
    maybe even iteratively consider necessary reasoning steps while crafting the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: As before, we will likely see diminishing benefits of this simple CoT approach
    when models become more and more fine-tuned and accustomed to this reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have taken a journey into the world of prompting chat-based
    Large Language Models. Rather than just giving you the most popular prompting
    techniques, I have encouraged you to begin the journey with the question of Why
    prompting matters at all. During this journey we have discovered that the importance
    of prompting is diminishing thanks to the evolution of the models. Instead of
    requiring users to invest in continuously improving their prompting skills, currently
    evolving model architectures will likely further reduce their relevance. An **agent-based
    framework**, where different “routes” are taken while processing specific queries
    and tasks, is one of those.
  prefs: []
  type: TYPE_NORMAL
- en: This does not mean, however, that **being clear and specific and providing the
    necessary context within your prompts** isn’t worth the effort. On the contrary,
    I am a strong advocate of this, since it not only helps the model but also yourself
    to figure out what exactly it is you’re trying to achieve.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in human communication, multiple factors determine the appropriate
    approach for reaching a desired outcome. Often, it is a mix and iteration of different
    approaches that yield optimal results for the given context. Try, test, iterate!
  prefs: []
  type: TYPE_NORMAL
- en: And finally, unlike in human interactions, you can test virtually limitlessly
    into your personal trial-and-error prompting journey. Enjoy the ride!
  prefs: []
  type: TYPE_NORMAL
- en: '*Notes and references'
  prefs: []
  type: TYPE_NORMAL
- en: All drawings are hand crafted with pride by the author :)*
  prefs: []
  type: TYPE_NORMAL
- en: '[1]: How Large Language Models work: From zero to ChatGPT'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f](https://medium.com/data-science-at-microsoft/how-large-language-models-work-91c362f5b78f)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]: Better Zero-Shot Reasoning with Role-Play Prompting'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/2308.07702](https://arxiv.org/abs/2308.07702)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]: Rethinking the Role of Demonstrations: What Makes In-Context Learning
    Work?'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/2202.12837](https://arxiv.org/abs/2202.12837)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]: Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://dl.acm.org/doi/abs/10.1145/3411763.3451760](https://dl.acm.org/doi/abs/10.1145/3411763.3451760).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5]: When do you need Chain-of-Thought Prompting for ChatGPT?'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/2304.03262](https://arxiv.org/abs/2304.03262)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6]: Large Language Models are Zero-Shot Reasoners'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/2205.11916](https://arxiv.org/abs/2205.11916)'
  prefs: []
  type: TYPE_NORMAL
