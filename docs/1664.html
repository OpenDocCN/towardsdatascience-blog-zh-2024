<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Neural Network (MLP) for Time Series Forecasting in Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Neural Network (MLP) for Time Series Forecasting in Practice</h1>
<blockquote>åŽŸæ–‡ï¼š<a href="https://towardsdatascience.com/neural-network-mlp-for-time-series-forecasting-in-practice-04c47c1e3711?source=collection_archive---------0-----------------------#2024-07-08">https://towardsdatascience.com/neural-network-mlp-for-time-series-forecasting-in-practice-04c47c1e3711?source=collection_archive---------0-----------------------#2024-07-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="128c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Practical Example for Feature Engineering and Constructing an MLP Model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://tothjd.medium.com/?source=post_page---byline--04c47c1e3711--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel J. TOTH" class="l ep by dd de cx" src="../Images/a7fd7d723abdba92c493c3dd9aeb2273.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*1jpk926mvFC9nn0wPYCDxg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--04c47c1e3711--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://tothjd.medium.com/?source=post_page---byline--04c47c1e3711--------------------------------" rel="noopener follow">Daniel J. TOTH</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--04c47c1e3711--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Jul 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="75c9" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al">Introduction</strong></h1><p id="c456" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Time series and more specifically time series forecasting is a very well known data science problem among professionals and business users alike.</p><p id="1681" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Several forecasting methods exist, which may be grouped as statistical or machine learning methods for comprehension and a better overview, but as a matter of fact, the demand for forecasting is so high that the available options are abundant.</p><p id="3781" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Machine learning methods are considered state-of-the-art approach in time series forecasting and are increasing in popularity, due to the fact that they are able to capture complex non-linear relationships within the data and generally yield higher accuracy in forecasting [1]. One popular machine learning field is the landscape of neural networks. Specifically for time series analysis, recurrent neural networks have been developed and applied to solve forecasting problems [2].</p><p id="ea12" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Data science enthusiasts might find the complexity behind such models intimidating and being one of you I can tell that I share that feeling. However, this article aims to show that</p><blockquote class="oo op oq"><p id="c187" class="nn no or np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">despite the latest developments in machine learning methods, it is not necessarily worth pursuing the most complex application when looking for a solution for a particular problem. Well-established methods enhanced with powerful feature engineering techniques could still provide satisfactory results.</p></blockquote><p id="2e90" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">More specifically, I apply a Multi-Layer Perceptron model and share the code and results, so you can get a hands-on experience on engineering time series features and forecasting effectively.</p><h1 id="1e93" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk"><strong class="al">Goal of the Article</strong></h1><p id="1857" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">More precisely what I aim at to provide for fellow self-taught professionals, could be summarized in the following points:</p><ol class=""><li id="4aee" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">forecasting based on real-world problem / data</li><li id="9378" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">how to engineer time series features for capturing temporal patterns</li><li id="64c6" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">build an MLP model capable of utilizing mixed variables: floats and integers (treated as categoricals via embedding)</li><li id="a0b9" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">use MLP for point forecasting</li><li id="cd5c" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">use MLP for multi-step forecasting</li><li id="1e22" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">assess feature importance using permutation feature importance method</li><li id="bdd6" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">retrain model for a subset of grouped features (multiple groups, trained for individual groups) to refine the feature importance of grouped features</li><li id="10f9" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">evaluate the model by comparing to an <code class="cx pf pg ph pi b">UnobservedComponents</code> model</li></ol><h1 id="5647" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk"><strong class="al">Key Technical Terms</strong></h1><p id="823b" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Please note, that this article assumes the prior knowledge of some key technical terms and do not intend to explain them in details. Find those key terms below, with references provided, which could be checked for clarity:</p><ol class=""><li id="4d43" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk"><strong class="np fr">Time Series </strong>[3]</li><li id="52f8" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Prediction</strong> [4] â€” in this context it will be used to distinguish model outputs in the training period</li><li id="91ae" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Forecast</strong> [4] â€” in this context it will be used to distinguish model outputs in the test period</li><li id="4ad5" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Feature Engineering</strong> [5]</li><li id="d967" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Autocorrelation</strong> [6]</li><li id="1dc4" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Partial Autocorrelation</strong> [6]</li><li id="6b79" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">MLP (Multi-Layer Perceptron)</strong> [7]</li><li id="30d7" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Input Layer </strong>[7]</li><li id="c60a" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Hidden Layer</strong> [7]</li><li id="cf34" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Output Layer</strong> [7]</li><li id="42f4" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Embedding</strong> [8]</li><li id="4a05" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">State Space Models</strong> [9]</li><li id="ec8c" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Unobserved Components Model</strong> [9]</li><li id="17d7" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">RMSE (Root Mean Squared Error)</strong> [10]</li><li id="c65a" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Feature Importance</strong> [11]</li><li id="8462" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk"><strong class="np fr">Permutation Feature Importance</strong> [11]</li></ol><h1 id="7388" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk"><strong class="al">Data Exploration</strong></h1><p id="f42c" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The essential packages used during the analysis are <code class="cx pf pg ph pi b">numpy</code> and <code class="cx pf pg ph pi b">pandas</code> for data manipulation, <code class="cx pf pg ph pi b">plotly</code> for interactive charts, <code class="cx pf pg ph pi b">statsmodels</code> for statistics and state space modeling and finally, <code class="cx pf pg ph pi b">tensorflow</code> for MLP architcture.</p><p id="d8f5" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk"><em class="or">Note: due to technical limitations, I will provide the code snippets for interactive plotting, but the figures will be static presented here.</em></p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="e79a" class="pr ms fq pi b bg ps pt l pu pv">import opendatasets as od<br/>import numpy as np<br/>import pandas as pd<br/>import plotly.graph_objects as go<br/>from plotly.subplots import make_subplots<br/>import tensorflow as tf<br/><br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.inspection import permutation_importance<br/>import statsmodels.api as sm<br/>from statsmodels.tsa.stattools import acf, pacf<br/>import datetime<br/><br/>import warnings<br/>warnings.filterwarnings('ignore')</span></pre><p id="768a" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The data is loaded automatically using <code class="cx pf pg ph pi b">opendatasets</code>.</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="ec14" class="pr ms fq pi b bg ps pt l pu pv">dataset_url = "https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/"<br/>od.download(dataset_url)<br/>df = pd.read_csv(".\hourly-energy-consumption" + "\AEP_hourly.csv", index_col=0)<br/>df.sort_index(inplace = True)</span></pre><p id="8523" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Keep in my mind, that data cleaning was an essential first step in order to progress with the analysis. If you are interested in the details and also in state space modeling, please refer to my previous article <a class="af pw" href="https://medium.com/analytics-vidhya/multi-seasonal-time-series-analysis-decomposition-and-forecasting-with-python-609409570007" rel="noopener">here</a>. â˜šðŸ“° In a nutshell, the following steps were conducted:</p><ol class=""><li id="1427" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">Identifying gaps, when specific timestamps are missing (only single steps were identified)</li><li id="e316" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Perform imputation (using mean of previous and next records)</li><li id="fb81" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Identifying and dropping duplicates</li><li id="17ff" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Set timestamp column as index for dataframe</li><li id="586c" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Set dataframe index frequency to hourly, because it is a requirement for further processing</li></ol><p id="8c01" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">After preparing the data, letâ€™s explore it by drawing 5 random timestamp samples and compare the time series at different scales.</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="0308" class="pr ms fq pi b bg ps pt l pu pv">fig = make_subplots(rows=5, cols=4, shared_yaxes=True, horizontal_spacing=0.01, vertical_spacing=0.04)<br/><br/>#  drawing a random sample of 5 indices without repetition<br/>sample = sorted([x for x in np.random.choice(range(0, len(df), 1), 5, replace=False)])<br/><br/># zoom x scales for plotting<br/>periods = [9000, 3000, 720, 240]<br/><br/>colors = ["#E56399", "#F0B67F", "#DE6E4B", "#7FD1B9", "#7A6563"]<br/><br/># s for sample datetime start<br/>for si, s in enumerate(sample):<br/>    <br/>    # p for period length<br/>    for pi, p in enumerate(periods):<br/>        cdf = df.iloc[s:(s+p+1),:].copy()<br/>        fig.add_trace(go.Scatter(x=cdf.index,<br/>                                 y=cdf.AEP_MW.values,<br/>                                 marker=dict(color=colors[si])),<br/>                        row=si+1, col=pi+1)<br/><br/>fig.update_layout(<br/>    font=dict(family="Arial"),<br/>    margin=dict(b=8, l=8, r=8, t=8),<br/>    showlegend=False,<br/>    height=1000,<br/>    paper_bgcolor="#FFFFFF",<br/>    plot_bgcolor="#FFFFFF")<br/>fig.update_xaxes(griddash="dot", gridcolor="#808080")<br/>fig.update_yaxes(griddash="dot", gridcolor="#808080")</span></pre></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/0a11e7108055273a11e5804af3946d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*3Wv14WFiF8FmTQzcLVrf5w.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Random sampling of dataset and visuals at different time scales. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="88c3" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">State Space Modeling</h1><p id="98ad" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">By closely examining this simple, yet effective plot, for me it is clearly visible, that the analysis should address several seasonal effects:</p><ol class=""><li id="ecf6" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">energy consumption â€” in general â€” peak in mid summer and mid winter, regardless of the year selected</li><li id="ea86" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">a weekly minimum pattern seems to emerge on Mondays</li><li id="7762" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">there is a daily minimum during the nights, maximum during the days</li></ol><p id="bf5d" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Further analysis would reveal, that the yearly pattern of the dataset has 2 harmonics, as the winter and summer peaks have different levels. As a result, the following state space model has been considered, where the periods are measured in hours (see model summary as well below):</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="e779" class="pr ms fq pi b bg ps pt l pu pv"># splitting time series to train and test subsets<br/>y_train = df.iloc[:-8766, :].copy()<br/>y_test = df.iloc[-8766:, :].copy()<br/><br/># Unobserved Components model definition<br/>model = sm.tsa.UnobservedComponents(y_train,<br/>                                    level='dtrend',<br/>                                    irregular=True,<br/>                                    stochastic_level = False,<br/>                                    stochastic_trend = False,<br/>                                    stochastic_freq_seasonal = [False, False, False],<br/>                                    freq_seasonal=[{'period': 24, 'harmonics': 1},<br/>                                                    {'period': 168, 'harmonics': 1},<br/>                                                    {'period': 8766, 'harmonics': 2}])<br/># fitting model to train data<br/>model_results = model.fit()<br/><br/># printing statsmodels summary for model<br/>print(model_results.summary())</span></pre><pre class="qs po pi pp bp pq bb bk"><span id="447d" class="pr ms fq pi b bg ps pt l pu pv">Value of `irregular` may be overridden when the trend component is specified using a model string.<br/><br/>                           Unobserved Components Results                            <br/>====================================================================================<br/>Dep. Variable:                       AEP_MW   No. Observations:               112530<br/>Model:                  deterministic trend   Log Likelihood            -1002257.017<br/>                     + freq_seasonal(24(1))   AIC                        2004516.033<br/>                    + freq_seasonal(168(1))   BIC                        2004525.664<br/>                   + freq_seasonal(8766(2))   HQIC                       2004518.941<br/>Date:                      Tue, 25 Jun 2024                                         <br/>Time:                              08:13:35                                         <br/>Sample:                          10-01-2004                                         <br/>                               - 08-02-2017                                         <br/>Covariance Type:                        opg                                         <br/>====================================================================================<br/>                       coef    std err          z      P&gt;|z|      [0.025      0.975]<br/>------------------------------------------------------------------------------------<br/>sigma2.irregular  3.168e+06    1.3e+04    244.095      0.000    3.14e+06    3.19e+06<br/>===================================================================================<br/>Ljung-Box (L1) (Q):              104573.71   Jarque-Bera (JB):              2731.37<br/>Prob(Q):                              0.00   Prob(JB):                         0.00<br/>Heteroskedasticity (H):               1.04   Skew:                             0.35<br/>Prob(H) (two-sided):                  0.00   Kurtosis:                         3.30<br/>===================================================================================<br/><br/>Warnings:<br/>[1] Covariance matrix calculated using the outer product of gradients (complex-step).</span></pre><p id="4dfb" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Without getting too much ahead of myself, let me note, that this model approximates the total energy consumption for the last 365 days with an error of ~2%, which is fairly accurate from a business perspective I believe. The MLP model constructed below will be evaluated by comparing it to the abovementioned state space model.</p><h1 id="21bb" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">Feature Engineering</h1><p id="fbc8" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Before constructing the MLP model, we should make the unique trend and seasonal effects available for the model to learn it. That is achieved by adding new features to the dataset, derived from the original 1D time series data. Derived features for capturing already identified or unidentified patterns include:</p><ol class=""><li id="e4a4" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">Lags</li><li id="2316" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Differences</li><li id="3bd0" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Rolling means</li><li id="7e77" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Rolling standard deviations</li><li id="2beb" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Hour of the day</li><li id="aefc" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Day of week</li><li id="0123" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Labeling weekends</li></ol><p id="bec4" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Such derived â€” and numerical â€” features could be considered in multiple intervals. In order to determine which intervals a model would benefit, it is highly recommended to check the autocorrelation properties of the dataset.</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="abbd" class="pr ms fq pi b bg ps pt l pu pv">dff = df.copy()<br/>acorr = acf(dff.AEP_MW.values, nlags=2*366)     # autocorrelation<br/>pacorr = pacf(dff.AEP_MW.values, nlags=2*366)   # partial autocorrelation<br/><br/>fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0)<br/>fig.add_trace(go.Scatter(<br/>    x=np.linspace(0, len(acorr), len(acorr)+1),<br/>    y=acorr,<br/>    name="Autocorrelation",<br/>    marker=dict(color="rgb(180, 120, 80)")<br/>), row=1, col=1)<br/>fig.add_trace(go.Scatter(<br/>    x=np.linspace(0, len(pacorr), len(pacorr)+1),<br/>    y=pacorr,<br/>    name="Partial Autocorrelation",<br/>    marker=dict(color="rgb(80, 180, 120)")<br/>), row=2, col=1)<br/>fig.update_layout(<br/>    font=dict(family="Arial"),<br/>    margin=dict(b=4, l=4, r=4, t=4),<br/>    showlegend=False,<br/>    height=500,<br/>    paper_bgcolor="#FFFFFF",<br/>    plot_bgcolor="#FFFFFF")<br/>fig.update_xaxes(griddash="dot", gridcolor="#808080", row=1, col=1)<br/>fig.update_xaxes(griddash="dot", gridcolor="#808080", title_text="No. of lags", row=2, col=1)<br/>fig.update_yaxes(griddash="dot", gridcolor="#808080", title_text="Autocorrelation", row=1, col=1)<br/>fig.update_yaxes(griddash="dot", gridcolor="#808080", title_text="Partial Autocorrelation", row=2, col=1)</span></pre></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/2db5e7d8a24a253430c45eadd4fb3031.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*leMCk8vTNPaZeFOVtmUPfQ.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Autocorrelation and partial autocorrelation plots of time series. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f369" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The dataset is highly autocorrelated, which makes sense as the values vary mostly between 10K MW and 20K MW with a smooth transition from hour to hour. However, focusing on partial autocorrelations as shown on the plot below, a significant correlation seems to be present in the multiples of 24 hours and in the last couple of hours. As a result, the derived features can be mainly classified as:</p><ol class=""><li id="16ed" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">Daily (multiples of 24 hours),</li><li id="d1e9" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Hourly (focusing on the last couple of hours) and</li><li id="a0b3" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Categorical features</li></ol><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="ed5c" class="pr ms fq pi b bg ps pt l pu pv">dff = df.reset_index(drop=False)<br/>dff["Datetime"] = pd.to_datetime(dff.Datetime.values)<br/><br/># lags and difference of multiple days for capturing seasonal effects<br/>for i in np.linspace(24, 15*24, 15, dtype=int):<br/>    dff[f"lag_{i}"] = dff.AEP_MW.shift(i)<br/>    dff[f"difference_{i}"] = dff.AEP_MW.diff(periods=i)<br/><br/># rolling mean and standard deviation up to 3 days for capturing seasonal effects better<br/>for i in np.linspace(24, 72, 3, dtype=int):<br/>    dff[f"rolling_mean_{i}"] = dff.AEP_MW.rolling(window=i).mean()<br/>    dff[f"rolling_std_{i}"] = dff.AEP_MW.rolling(window=i).std()<br/><br/># lag, rolling mean, rolling standard deviation and difference up to 4 hours for capturing immediate effects<br/>for i in range(2, 5, 1):<br/>    dff[f"lag_{i}"] = dff.AEP_MW.shift(i)<br/>    dff[f"rolling_mean_{i}"] = dff.AEP_MW.rolling(window=i).mean()<br/>    dff[f"rolling_std_{i}"] = dff.AEP_MW.rolling(window=i).std()<br/>    dff[f"difference_{i}"] = dff.AEP_MW.diff(periods=i)<br/><br/># categorical features<br/>dff["hour_of_day"] = dff.Datetime.dt.hour<br/>dff["day_of_week"] = dff.Datetime.dt.day_of_week<br/>dff["is_weekend"] = dff["day_of_week"].isin([5, 6]).astype(int)<br/><br/># grouping derived features for later use in feature importance analysis<br/>daily_lags = [col for col in dff.columns if all(["lag_" in col, len(col)&gt;5])]<br/>hourly_lags = [col for col in dff.columns if all(["lag_" in col, len(col)&lt;=5])]<br/>daily_differences = [col for col in dff.columns if all(["difference_" in col, len(col)&gt;12])]<br/>hourly_differences = [col for col in dff.columns if all(["difference_" in col, len(col)&lt;=12])]<br/>daily_rolling_means = [col for col in dff.columns if all(["rolling_mean_" in col, len(col)&gt;14])]<br/>hourly_rolling_means = [col for col in dff.columns if all(["rolling_mean_" in col, len(col)&lt;=14])]<br/>daily_rolling_stds = [col for col in dff.columns if all(["rolling_std_" in col, len(col)&gt;13])]<br/>hourly_rolling_stds = [col for col in dff.columns if all(["rolling_std_" in col, len(col)&lt;=13])]<br/>categoricals = ["hour_of_day", "day_of_week", "is_weekend"]</span></pre><h1 id="f8c9" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk"><strong class="al">Constructing the MLP Model</strong></h1><p id="439c" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Generating the above detailed features, the input shapes are known and the MLP model can be constructed. It is important to notice, that we are dealing with mixed datatypes: floats and integers. Please also note, that while all features are of numerical type, the integer inputs are fundamentally categorical features and should be treated as such.</p><p id="18be" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">There is an option to encode the categories with e.g. one hot encoding technique, but that would significantly increase the number of features as each categorical column should be expanded to as many columns as many categories exist (minus one) [12]. I deliberately chose embedding instead to limit the number of features on the expense, that the model input layer will be more complex as the categoricals are converted to vectors via embedding first and then combined with the float inputs.</p><p id="6f98" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Please see the graph after the code section for clarity. The architecture has been built using rule of thumbs, as the hyperparameter tuning is out of scope for this article. However, if you are interested in a general framework how it can be done, please check ðŸ“°â˜› <a class="af pw" href="https://medium.com/towards-data-science/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d" rel="noopener">my previous article</a> (I tuned an XGBoost model with Optuna as a tool for Bayesian search of optimal hyperparameter values).</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="03c7" class="pr ms fq pi b bg ps pt l pu pv"># segmenting last year as test data<br/>inputs = dff.dropna().iloc[:, 2:].columns<br/>xs_train = dff.dropna().iloc[:-8766, 2:]<br/>xs_test = dff.dropna().iloc[-8766:, 2:]<br/>ys_train = dff.dropna().iloc[:-8766, 1]<br/>ys_test = dff.dropna().iloc[-8766:, 1]<br/>embedding_dim = 4       # potential hyperparameter<br/><br/># defining baseline NN model<br/>float_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name="float_inputs")           # floats can be directly used in model fitting<br/>integer_inputs = tf.keras.layers.Input(shape=(3,), dtype="int32", name="integer_inputs")    # integers should be treated as categoricals ang get them embedded<br/>embedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)          # embedding will be performed during model fitting<br/>embedded_integer_inputs = embedding_layer(integer_inputs)<br/>flattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)                   <br/>preprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])    # float and embedded inputs are combined<br/>hidden_layers = tf.keras.layers.Dense(units=64, activation="relu")(preprocessing_layers)    # No. of hidden layers, No. of units, activation function are potential hyperparameters<br/>hidden_layers = tf.keras.layers.Dense(units=32, activation="relu")(hidden_layers)<br/>output = tf.keras.layers.Dense(units=1, activation="linear")(hidden_layers)                 # single unit for one step ahead, multiple units for multiple step prediction<br/>model_NN_baseline = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)<br/><br/># compiling baseline NN model<br/>model_NN_baseline.compile(<br/>    optimizer=tf.keras.optimizers.Adam(),<br/>    loss=tf.keras.losses.MeanSquaredError(),<br/>    jit_compile=True)<br/><br/># fitting baseline NN model<br/>model_NN_baseline.fit(<br/>    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],<br/>    y=ys_train,<br/>    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],<br/>    epochs=128,<br/>    batch_size=64,<br/>    verbose=1<br/>)</span></pre></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qt"><img src="../Images/d1e93289f78adc2ad27fe54af42aa5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*_Ro6Lnd2G7Mn0Ap9Jsy_1Q.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">MLP architecture created by using Tensorflow/Keras. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3e87" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">As far as point forecasts goes, the results are ridiculously accurate. This is a good sign, that the feature engineering principles applied are correctly capturing the underlying patterns in the data and the model was able to generalize it.</p></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/181dd0c122d63276258e6f6f5576e070.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*bkbh97OV4zY5VlJVw-Quqg.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Baseline MLP model point-forecasts vs. test data. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f5f6" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The point forecasts overlap with the test set and the two figure traces are indistinguishable from each other. More precisely, the RMSE of the predictions (training set) and forecasts (test set) are approx. 19.3 and 18.9 respectively (in the ballpark of 0.1% in relative terms).</p><h1 id="9cef" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk"><strong class="al">Feature Importance</strong></h1><p id="5ee1" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">What led the model to be accurate? Are all derived features equally important or is there a subset which has a greater weight in determining the outcome? These are valid questions for two distinct reasons:</p><ol class=""><li id="0f35" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">In real-world scenarios and in the case of big data, the resources for training the model is limited and the amount of data used could make a significant difference if the model could be trained at all</li><li id="44f5" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">Without any explanation, the model works as a black box, which creates uncertainty regarding its perfomance. Neural Networks are especially prone to be black box models and interpreting them is a field of its own [11]</li></ol><p id="9f0e" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">There is an abundance of techniques to interpret models, each has its pros and cons. I selected the permutation feature importance method to give some insights on model interpretation however, a key takeway from my analysis is that such</p><blockquote class="oo op oq"><p id="b263" class="nn no or np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">model interpretation techniques are only interpreting the model in scope and not necessarily the underlying process itself. Reality could be very different from feature importance analysis, hence it should not be taken as ground truth of causal relationship between independent variables and the target variable.</p></blockquote><p id="6fa1" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Let me explain that with my analysis results. Permuting features one at a time, recalculating the RMSE score and recording the relative change in RMSE compared to forecasts using the original data will give the relative importance of features [13].</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="4481" class="pr ms fq pi b bg ps pt l pu pv"># permutation feature importance<br/>features = xs_test.columns<br/>permutation_importance_results = {}<br/>rmse = tf.keras.metrics.RootMeanSquaredError()<br/>rmse_permuted = tf.keras.metrics.RootMeanSquaredError()<br/>rmse.update_state(ys_test.values, model_NN_baseline.predict([xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], verbose=0).flatten())<br/><br/>for feature in features:<br/><br/>    xs_test_permuted = xs_test.copy()<br/>    xs_test_permuted.loc[:, feature] = xs_test.loc[:, feature].sample(frac=1, axis=0, replace=False, random_state=42).values<br/><br/>    rmse_permuted.reset_state()<br/>    rmse_permuted.update_state(ys_test.values, model_NN_baseline.predict([xs_test_permuted.iloc[:, :-3], xs_test_permuted.iloc[:, -3:]], verbose=0).flatten())<br/><br/>    permutation_importance_results[feature] = rmse_permuted.result().numpy() / rmse.result().numpy()<br/><br/>pi_results_sorted_keys = sorted(permutation_importance_results, key=permutation_importance_results.get, reverse=True)<br/><br/>fig3 = make_subplots()<br/>fig3.add_trace(go.Bar(<br/>    x=pi_results_sorted_keys,<br/>    y=[permutation_importance_results[key] for key in pi_results_sorted_keys]))<br/>fig3.update_layout(<br/>    title="&lt;b&gt;Permutation Feature Importance&lt;/b&gt;",<br/>    font=dict(family="Arial"),<br/>    margin=dict(b=4, l=4, r=4, t=36),<br/>    showlegend=False,<br/>    height=500,<br/>    paper_bgcolor="#FFFFFF",<br/>    plot_bgcolor="#FFFFFF"<br/>)<br/>fig3.update_xaxes(griddash="dot", gridcolor="#808080", row=1, col=1)<br/>fig3.update_yaxes(griddash="dot", gridcolor="#808080", row=1, col=1)</span></pre></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/2564e8b61f4fb48c7d19a4b671007da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*yQJhN915pe3XJa2iXJeBbg.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Permutation feature importance histogram. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8f54" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Hourly-, daily lags and differences seem important and maybe the hourly rolling means as well. However, the daily and hourly rolling standards just as well as the categorical features seem negligible, relative to the aforementioned features. One caveat of permutation feature importance is that it does not take into account multicollinearity and consequently may give inaccurate results. Remember, the features have been derived from a dataset with high autocorrelation.</p><p id="0cef" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">One possible way to handle the situation is following <code class="cx pf pg ph pi b">scikit learn</code> â€˜s guidance:</p><blockquote class="oo op oq"><p id="b141" class="nn no or np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">perform hierarchical clustering on the Spearman rank-order correlations, pick a threshold, and keep a single feature from each cluster. [13]</p></blockquote><p id="9ddc" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">However, I would like to focus on highlighting the inaccuracy and adding more insights to the dataset by training alternative models with the grouped features one group at a time. The same MLP architecture was used for this purpose with adjustments only applied on the input layer to accomodate a subset of data. The following groups were created in the feature engineering section and tested here (train/test dataset RMSE results also reported respectively):</p><ol class=""><li id="9a2b" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">daily lags (942 and 994)</li><li id="1cd0" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">daily differences (1792 and 1952)</li><li id="542f" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">hourly lags (686 and 611)</li><li id="06ef" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">daily rolling means and standard deviations (1710 and 1663)</li><li id="339d" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">hourly rolling means and standard deviations (84.4 and 75.5)</li></ol><p id="d5d3" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">It is clear that the alternative models show results not anticipated from simple permutation feature importance analysis, without handling multicollinearity: e.g. daily rolling features yielded better scores than daily differences and the model trained on hourly rolling features has the best performance out of the alternative models, close to the baseline model (RMSE reported in percentage ~0.5% vs. ~0.1% respectively).</p><h1 id="21d4" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">A Note on a Specific Anomaly in the Data</h1><p id="8459" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">I would like to highlight a very specific case of anomaly observed at 14:00 on 20th October 2008. This is the highest ever recorded value with no apparent cause, no similar datapoints before or after in the dataset.</p><blockquote class="oo op oq"><p id="3c97" class="nn no or np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Yet, the baseline model powered by feature engineering was able to predict that datapoint and is not considered an outlier!</p></blockquote></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qu"><img src="../Images/fe5663c1e874d9c4bcbf0d8dd541ab3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*X_IC-oySjx2zC8rD54FoTQ.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Baseline MLP model point-predictions and the observed potential anomaly. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="477d" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">From which features the model was able to predict that point? Letâ€™s use our alternative models for inference. The best alternative (hourly rolling features) seems extremely accurate in the vicinity, but could only explain the phenomenon partially:</p></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/1d891089cfb4b6494f8ac7e5afc5959e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*8eVhrItNri0ZFBhkCOSDmw.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Alternative MLP model (utilizing hourly rolling features) point-predictions and the observed potential anomaly. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="28b6" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The second best alternative is the one utilizing hourly lags, but it has absolutely no answer why that happened:</p></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/4d699078a9afb2b07b740af85194b30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zK9KT9XP-lC4EPXI17WOVg.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Alternative MLP model (utilizing hourly lag features) point-predictions and the observed potential anomaly. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2a98" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Making a long story short, the daily differences might contain important information regarding the underlying patterns. Although utilizing the daily differences group solely gives higher predictions, the baseline model seemingly found a good balance of weights for the features.</p></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qv"><img src="../Images/c0f627ebf8eb29789a55dda1e01f9faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*cVJVGuGD882WzBIXnde1yA.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Alternative MLP model (utilizing daily difference features) point-predictions and the observed potential anomaly. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="dfb3" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">Multi-step Prediction Model</h1><p id="a247" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Finally, the model architecture has been modified to yield multi-step predictions. The forecasting period is one year, as suggested by the dataset publisher [14]. Given all uncertainties in such a process with special regard to weather conditions, it might not make sense to consider such a long forecasting period. However, it is an intersting exercise to evaluate the multi-step modelâ€™s performance to the state space model, which explicitly models the trend and seasonal effects observed across the year (see next section).</p><p id="73cb" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The key points for implementing a multi-step model are as follows:</p><ol class=""><li id="ecfb" class="nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi ox oy oz bk">the target was a series of vectors (next 8766 hours defined for ech step)</li><li id="0bf8" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">as a result, the prediction or forecast is the next 8766 hours (approx. one year) for the last row of inputs</li><li id="9459" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">due to resource limitations I had to limit the training data for the last year of the former training dataset</li><li id="accf" class="nn no fq np b go pa nr ns gr pb nu nv nw pc ny nz oa pd oc od oe pe og oh oi ox oy oz bk">the output layer was modified accordingly, to give the desired vector output</li></ol><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="3009" class="pr ms fq pi b bg ps pt l pu pv">first_index = -8766*5<br/>last_index = -8766*2<br/>final_index = -8766<br/>inputs = dff.dropna().iloc[:, 2:].columns<br/>xs_train = dff.dropna().iloc[first_index:last_index, 2:]<br/>xs_train.iloc[:, :-3] = xs_train.iloc[:, :-3].astype(np.float32)<br/>xs_test = dff.dropna().iloc[last_index:final_index, 2:]<br/>xs_test.iloc[:, :-3] = xs_test.iloc[:, :-3].astype(np.float32)<br/>ys_train = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(first_index, last_index, 1)])<br/>ys_test = np.vstack([dff.dropna().iloc[i:i+8765, 1].astype(int).values for i in range(last_index, final_index, 1)])<br/>embedding_dim = 4<br/><br/># defining, compiling and training NN model for MULTIPLE STEP PREDICTIONS. Model architecture is the same, except output layer<br/>float_inputs = tf.keras.layers.Input(shape=(len(inputs)-3,), name="float_inputs")<br/>integer_inputs = tf.keras.layers.Input(shape=(3,), dtype="int32", name="integer_inputs")<br/>embedding_layer = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim)<br/>embedded_integer_inputs = embedding_layer(integer_inputs)<br/>flattened_embeddings = tf.keras.layers.Flatten()(embedded_integer_inputs)<br/>preprocessing_layers = tf.keras.layers.concatenate([float_inputs, flattened_embeddings])<br/>hidden_layers = tf.keras.layers.Dense(units=64, activation="relu")(preprocessing_layers)<br/>hidden_layers = tf.keras.layers.Dense(units=32, activation="relu")(hidden_layers)<br/>output = tf.keras.layers.Dense(units=np.abs(final_index)-1, activation="linear")(hidden_layers)<br/><br/>model_NN_multistep = tf.keras.Model(inputs=[float_inputs, integer_inputs], outputs=output)<br/>model_NN_multistep.compile(<br/>    optimizer=tf.keras.optimizers.Adam(),<br/>    loss=tf.keras.losses.MeanSquaredError(),<br/>    jit_compile=True)<br/>model_NN_multistep.fit(<br/>    x=[xs_train.iloc[:, :-3], xs_train.iloc[:, -3:]],<br/>    y=ys_train,<br/>    validation_data=[[xs_test.iloc[:, :-3], xs_test.iloc[:, -3:]], ys_test],<br/>    epochs=128,<br/>    batch_size=64,<br/>    verbose=1<br/>)</span></pre><p id="0b35" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">For a visual evaluation, one could see the model was trying to generalize the patterns:</p></div></div><div class="px"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="pj pk pl pm pn px qg qh paragraph-image"><div role="button" tabindex="0" class="qi qj ed qk bh ql"><div class="qd qe qf"><img src="../Images/2c6c7c1d334527dfc742b07d89445916.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*MR5u0QIOebyHDHieF9u8Ew.png"/></div></div><figcaption class="qn qo qp qd qe qq qr bf b bg z dx">Multistep MLP model predictions and forecasts vs. original data. Source: author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5a6a" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">MLP vs. State Space Model</h1><p id="441e" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Due to generalization of the data, the RMSE score increased significantly: 1982 and 2017 for the train and test dataset respectively. However, in order the properly evaluate the multi-step MLP, we should use another model for comparison. As I mentioned in the previous section, state space models gives fairly understandable approximations of the trend and seasonal effects observed across the year. This feature make them relatively easily interpretable, unlike neural networks. The primary reason is that hidden layers have many connections and understanding how they are activated is not a straightforward process. [11].</p><p id="4ae7" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">In <a class="af pw" href="https://medium.com/analytics-vidhya/multi-seasonal-time-series-analysis-decomposition-and-forecasting-with-python-609409570007" rel="noopener">my previous article</a>, â˜šðŸ“° I used a simplified, yet meaningful evaluation method: comparing the total energy consumption within the last year. Practically, that is the area under the curve of the energy consumption time series. The values for the original data and model forecasts can be compared directly. For the <code class="cx pf pg ph pi b">UnobservedComponents</code> model:</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="3418" class="pr ms fq pi b bg ps pt l pu pv">y_train = df.iloc[:-8766, 0].values<br/>y_test = df.iloc[-8766:, 0].values<br/>observed_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]<br/>forecast = model_results.forecast(steps=8766)<br/>UC_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]<br/><br/># calculating absolute and percentage error of forecast integral compared to observed integral<br/>fcast_integral_abserror = UC_integral - observed_integral<br/>fcast_integral_perror4 = (UC_integral - observed_integral) * 100 / observed_integral<br/><br/>print(f"Observed yearly energy demand: {'%.3e' % observed_integral} MWh")<br/>print(f"Forecast yearly energy demand: {'%.3e' % UC_integral} MWh")<br/>print(f"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %")</span></pre><pre class="qs po pi pp bp pq bb bk"><span id="33e9" class="pr ms fq pi b bg ps pt l pu pv">Observed yearly energy demand: 1.312e+08 MWh<br/>Forecast yearly energy demand: 1.283e+08 MWh<br/>Forecast error of yearly energy demand: -2.832e+06 MWh or -2.159 %</span></pre><p id="b382" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">For the MLP model:</p><pre class="pj pk pl pm pn po pi pp bp pq bb bk"><span id="722d" class="pr ms fq pi b bg ps pt l pu pv">y_test = dff.dropna().iloc[-8766:-1, 1].values<br/>observed_integral = np.cumsum([y_test[x] + (y_test[x+1] - y_test[x]) / 2 for x in range(len(y_test)-1)])[-1]<br/>forecast = model_NN_multistep.predict([xs_test.iloc[-1:, :-3], xs_test.iloc[-1:, -3:]], verbose=0).flatten()<br/>model_NN_multistep_integral = np.cumsum([forecast[x] + (forecast[x+1] - forecast[x]) / 2 for x in range(len(forecast)-1)])[-1]<br/><br/># calculating absolute and percentage error of forecast integral compared to observed integral<br/>fcast_integral_abserror = model_NN_multistep_integral - observed_integral<br/>fcast_integral_perror4 = (model_NN_multistep_integral - observed_integral) * 100 / observed_integral<br/><br/>print(f"Observed yearly energy demand: {'%.3e' % observed_integral} MWh")<br/>print(f"Forecast yearly energy demand: {'%.3e' % model_NN_multistep_integral} MWh")<br/>print(f"Forecast error of yearly energy demand: {'%.3e' % fcast_integral_abserror} MWh or {'%.3f' % fcast_integral_perror4} %")</span></pre><pre class="qs po pi pp bp pq bb bk"><span id="8469" class="pr ms fq pi b bg ps pt l pu pv">Observed yearly energy demand: 1.312e+08 MWh<br/>Forecast yearly energy demand: 1.286e+08 MWh<br/>Forecast error of yearly energy demand: -2.508e+06 MWh or -1.912 %</span></pre><p id="92ef" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">In short: it is -1.912% vs. -2.159% in favor of the MLP model. Please note, that this has been achieved by utilizing an MLP architecture using some simple rule of thumbs, not even considering hyperparameter tuning or some effective model training features, e.g. reducing the learning rate when the evaluation metric reaches a plateau or early stopping.</p><p id="0366" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">The results should be fairly convincing that indeed, utilizing relatively simple neural network architectures combined with powerful feature engineering techniques, accurate forecasting tools are within reach for a data scientist early in his or her seniority level.</p><h1 id="c3f3" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">Resources</h1><p id="29a2" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">Data source:<br/><a class="af pw" href="https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/</a> (CC0)</p><p id="269d" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">Notebook (only code, without outputs): <a class="af pw" href="https://gist.github.com/danielandthelions/2e6f0edd30902113ad10fd9f20bda215" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/danielandthelions/2e6f0edd30902113ad10fd9f20bda215</a></p><h1 id="48d2" class="mr ms fq bf mt mu os gq mw mx ot gt mz na ou nc nd ne ov ng nh ni ow nk nl nm bk">References</h1><p id="9ac6" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">[1] <a class="af pw" href="https://preset.io/blog/time-series-forecasting-a-complete-guide/" rel="noopener ugc nofollow" target="_blank">https://preset.io/blog/time-series-forecasting-a-complete-guide/</a></p><p id="9c69" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[2] <a class="af pw" href="https://www.ibm.com/topics/recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www.ibm.com/topics/recurrent-neural-networks</a></p><p id="bde2" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[3] <a class="af pw" href="https://www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it/" rel="noopener ugc nofollow" target="_blank">https://www.timescale.com/blog/time-series-analysis-what-is-it-how-to-use-it/</a></p><p id="5e0e" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[4] <a class="af pw" href="https://plat.ai/blog/difference-between-prediction-and-forecast/" rel="noopener ugc nofollow" target="_blank">https://plat.ai/blog/difference-between-prediction-and-forecast/</a></p><p id="b26f" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[5] <a class="af pw" href="https://dotdata.com/blog/practical-guide-for-feature-engineering-of-time-series-data/" rel="noopener ugc nofollow" target="_blank">https://dotdata.com/blog/practical-guide-for-feature-engineering-of-time-series-data/</a></p><p id="44e3" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[6] <a class="af pw" href="https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/" rel="noopener ugc nofollow" target="_blank">https://statisticsbyjim.com/time-series/autocorrelation-partial-autocorrelation/</a></p><p id="4d0a" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[7] <a class="af pw" href="https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron" rel="noopener ugc nofollow" target="_blank">https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron</a></p><p id="6513" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[8] <a class="af pw" href="https://jina.ai/news/embeddings-in-depth/" rel="noopener ugc nofollow" target="_blank">https://jina.ai/news/embeddings-in-depth/</a></p><p id="dc3a" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[9] Hyndman, R.J., &amp; Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 7th July 2024</p><p id="44a9" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[10] <a class="af pw" href="https://statisticsbyjim.com/regression/root-mean-square-error-rmse/" rel="noopener ugc nofollow" target="_blank">https://statisticsbyjim.com/regression/root-mean-square-error-rmse/</a></p><p id="cdea" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[11] <a class="af pw" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></p><p id="cda0" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[12] <a class="af pw" href="https://scikit-learn.org/stable/modules/preprocessing.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/preprocessing.html</a></p><p id="ac57" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[13] <a class="af pw" href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance</a></p><p id="6452" class="pw-post-body-paragraph nn no fq np b go oj nr ns gr ok nu nv nw ol ny nz oa om oc od oe on og oh oi fj bk">[14] <a class="af pw" href="https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/robikscube/hourly-energy-consumption/</a></p></div></div></div></div>    
</body>
</html>