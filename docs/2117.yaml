- en: A Simple Framework for RAG Enhanced Visual Question Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30](https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Empowering Phi-3.5-vision with Wikipedia knowledge for augmented Visual Question
    Answering.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    ·17 min read·Aug 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/186c503f14d17c4e369edaaa3013e655.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Christian Lue](https://unsplash.com/@christianlue?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval Augmented Generation (RAG) is a powerful technique that can improve
    the accuracy and reliability of the answer generated by Large Language Models
    (LLMs). It also offers the possibility of checking the sources used by the model
    during a particular generation, allowing easier fact-checking by human users.
    Furthermore, RAG makes it possible to keep the model knowledge up-to-date and
    incorporate topic-specific information without the need for fine-tuning. Overall,
    RAG provides many benefits and few drawbacks, and its workflow is straightforward
    to implement. Because of this, it has become the go-to solution for many LLM use
    cases that require up-to-date and/or specialized knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the latest developments in the Generative AI field have focused on extending
    the popular transformer architecture to tackle multiple input and/or output modalities,
    trying to replicate the huge success of LLMs. There are already several models,
    both open and closed source, that have demonstrated a remarkable ability to handle
    multiple modalities. A popular multimodal setting, and one of the first to be
    tackled, is that of Vision Language Models (VLMs), which has seen interesting
    open-source contributions with the release of small yet powerful models like LLaVA,
    Idefics, and Phi-vision. If you want to get started with VLMs and learn more about
    building a Vision Language Chat Assistant using LLaVA, you can look at my previous
    post [Create your Vision Chat Assistant with LLaVA](/create-your-vision-chat-assistant-with-llava-610b02c3283e).
  prefs: []
  type: TYPE_NORMAL
- en: Designing RAG systems for multimodal models is more challenging than in the
    text-only case. In fact, the design of RAG systems for LLM is well-established
    and there is some consensus about the general workflow, as many of the recent
    developments focus on improving accuracy, reliability, and scalability rather
    than fundamentally changing the RAG architecture. On the other hand, multimodality
    opens up multiple ways of retrieving relevant information and, consequentially,
    there are several different architectural choices that can be made, each with
    its own advantages and drawbacks. For example, it is possible to use a multimodal
    embedding model to create a shared vector space for the different modalities or,
    instead, choose to ground the information in one modality only.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, I will discuss a simple framework to extend RAG to Vision
    Language Models (VLMs), focusing on the Visual Question Answering task. The core
    idea of the method is to exploit the capabilities of the VLM to understand both
    text and images to generate a suitable search query that will be used to retrieve
    external information before answering the user’s prompt.
  prefs: []
  type: TYPE_NORMAL
- en: I will also provide a practical tutorial on how to implement the framework to
    empower Phi-3.5-vision with access to Wikipedia information, discussing the main
    points of the implementation and showing some examples. I will leave the details
    to the full code I shared in the following [Git Hub repo](https://github.com/GabrieleSgroi/vision_rag).
  prefs: []
  type: TYPE_NORMAL
- en: RAG for Visual Question Answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I will describe the general workflow of the framework mentioned
    in the introduction. For the sake of exposition, I will discuss the case where
    there is only one user’s prompt about one image. This is the case, for example,
    for simple Visual Question Answering (VQA) tasks. The method can be generalized
    straightforwardly to multiple prompts and images, but the pipeline will become
    more complex and introduce further complications. Furthermore, I will only consider
    the case in which the external data consists solely of textual documents. Using
    a multimodal embedding model for retrieval, or more generally a multimodal search
    engine, it is possible to include images in the external data as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the usual RAG workflow, the framework workflow can be divided into two
    parts: retrieval of the relevant external information and generation conditioned
    on the provided external data.'
  prefs: []
  type: TYPE_NORMAL
- en: During the retrieval phase, the goal is to retrieve some passages from the external
    text documents that can provide useful information to answer the user’s prompt.
    In order to do so effectively, we must ensure that the retrieved passages are
    relevant to the provided image, the prompt, and, more importantly, the relationship
    between the two. In fact, even if the retrieved documents contain information
    about the image, they may not include the specific information needed to provide
    an answer to the user’s prompt. On the other hand, the prompt may only be correctly
    understood when paired with the image it refers to. To address these challenges,
    the framework discussed in this post exploits the multimodal model to generate
    an appropriate search query, tailored to capture the information needed to answer
    the user’s prompt in the context of the provided image. A search engine will use
    the produced query to retrieve the relevant information from the external data.
  prefs: []
  type: TYPE_NORMAL
- en: In more detail, the multimodal model receives as input both the user’s prompt
    and the image and it is tasked with creating a search query that is relevant to
    both of them as a whole. This process can be seen as a special case of a query
    transformation, designed to consider the multimodal nature of the problem. In
    fact, the model translates the user’s prompt into a search query while also considering
    the image it refers to.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this approach over other methods that treat each input modality
    separately, such as using a multimodal embedding model for retrieval or using
    a generated image caption/description for semantic similarity, is that it can
    capture the relationships between the prompt and the image more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: The flowchart for the retrieval phase is sketched below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd8841a050d566ca5fe7c3a326d3d9d7.png)'
  prefs: []
  type: TYPE_IMG
- en: During retrieval, the model is tasked to create a query that will be used by
    a search engine to retrieve the relevant passages. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The generation phase is very similar to the standard text-only RAG workflow,
    the only difference being that the model receives the image in its context in
    addition to the prompt and the retrieved passages. This process is illustrated
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79f324bd23a63cc7873b6633ece07966.png)'
  prefs: []
  type: TYPE_IMG
- en: During generation, the model receives the retrieved passages, the prompt, and
    the image as input. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Empowering Phi-3.5-vision with Wikipedia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I will provide a practical guide on how to apply the discussed
    framework to enhance a multimodal model by giving it access to Wikipedia. I chose
    the model [Phi-3.5-vision](https://huggingface.co/microsoft/Phi-3.5-vision-instruct),
    as it is a very powerful yet lightweight open-source Vision Language Model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I will discuss only the general aspects of the implementation,
    leaving the details to the code provided in the [GitHub repo](https://github.com/GabrieleSgroi/vision_rag).
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the retrieval phase is to gather some passages from Wikipedia that
    can provide useful information to answer a user’s question about an image. In
    the code implementation, I used the Python package [wikipedia](https://pypi.org/project/wikipedia/)
    to search and retrieve content from Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps implemented to retrieve the relevant passages:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the multimodal model to generate keywords capturing the meaning of the question
    about the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the generated keywords to search relevant pages on Wikipedia.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the content of each retrieved page into chunks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the top chunks by semantic textual similarity to the question and the
    keywords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first step exploits Phi-3.5-vision to generate an appropriate search query
    that will be used to retrieve relevant Wikipedia pages. In order to do so, I tasked
    Phi-3.5-vision to produce keywords relevant to the user’s question and the image.
    I then used the built-in search function of the wikipedia package to retrieve
    some pages relevant to the generated keywords.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general single-turn single-image chat template for Phi-vision-3.5 has the
    following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To generate the keywords I used the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The tag {question} is substituted with the user question before inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the keywords have been generated, the built-in search function of the
    wikipedia package is used to retrieve some pages relevant to the generated keywords.
    Finally, the selected pages are split into passages, and then the most relevant
    passages are selected using an embedding model and the LangChain implementation
    of the FAISS vector store. I used the embedding model [*snowflake-arctic-embed-l*](https://huggingface.co/Snowflake/snowflake-arctic-embed-l)
    to embed the concatenation of the question and the keywords, and the chunks of
    the retrieved pages. In practice, the retrieval phase is effectively a form of
    “hybrid search” consisting of two sequential steps: keyword search using the built-in
    search function of the wikipedia package, and embedding similarity retrieval using
    an embedding model. In this way, the retrieval operates on the smaller space of
    the passages of the most relevant pages selected using keyword search, avoiding
    the need to build an enormous vector store with the embeddings of all the content
    of Wikipedia. In different settings, the retrieval phase could be remodeled to
    use similarity retrieval on the whole external corpus or using different combinations
    of retrieval methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving passages from multiple pages can help reduce the chance of selecting
    the wrong page and it can also be useful when information from multiple pages
    is needed to produce an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the generation phase, the user’s question, the retrieved passages, and the
    original images are used as inputs for Phi-3.5-vision to generate an answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used the following prompt in the general chat template for Phi-3.5-vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: At generation time, the tag {question} is substituted by the user question as
    before, while the tag {passages} is substituted by the retrieved passages and
    the names of the corresponding pages with the following format
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Providing the name of the page from which the passage is extracted can help
    resolve ambiguities when the content of the latter is not enough to uniquely determine
    the subject or topic it refers to.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I will show some examples of answers obtained with the implementation
    discussed in the previous section, comparing the outputs of the Vision Language
    Model empowered with RAG with the base version.
  prefs: []
  type: TYPE_NORMAL
- en: For each example below, I will show the image provided to the model, a block
    with the question and the answers of both the RAG augmented and base VLM, a block
    with the search query created by the model, and a block with the passages retrieved
    from Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/fc32fffd5a80c737c08d79cf43adfc2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [engin akyurt](https://unsplash.com/@enginakyurt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The VLM generated the following search keywords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And retrieved the following passages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The model augmented with RAG correctly reported the height range stated on the
    Wikipedia page ‘Tomato’, while the base model answered with a lower range.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2820d1d705765e00d8da60086af89f1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hans-Jurgen Mager](https://unsplash.com/@hansjurgen007?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Again, the RAG model answered correctly with the weights reported in the Wikipedia
    page ‘Polar bear’, while the base model answered with a slightly different range
    of weights.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my previous blog post [Create your Vision Chat Assistant with LLaVA](/create-your-vision-chat-assistant-with-llava-610b02c3283e),
    I showed how the LLaVA-1.5 model can be induced to hallucinate when the user’s
    prompt asks a misleading question about the image. Let’s see what happens in that
    example using the Phi-3.5-vision model with and without the RAG enhancement. In
    particular, the model could now be more prone to hallucination if any misleading
    passages were retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f408a52ca0ef21463de523769cb64d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Neither the base nor the RAG model were induced to hallucinate by the misleading
    prompt, indicating a possible better robustness of Phi-3.5-vision compared to
    LLaVA-1.5\. In particular, the RAG VLM did not retrieve any page from Wikipedia.
    In fact, when asked to generate keywords, the model answered
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This search query did not produce any results with the wikipedia package search
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: in the latter example, in earlier experiments conducted with Idefics
    2, both the base VLM and RAG VLM models were affected by hallucinations, answering
    with the names of fish species not present in the image. In particular, the RAG
    model retrieved misleading passages related to various fishes. The phenomenon
    of hallucinations is model-dependent and the RAG framework may not be enough to
    eliminate it in all instances.*'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main limitation of the framework is that it requires the model to have at
    least a general knowledge of the image and the question provided (while the exact
    details will be retrieved with RAG) in order to produce a useful search query.
    In some domain-specific settings, it may be necessary to fine-tune the model for
    search query generation.
  prefs: []
  type: TYPE_NORMAL
- en: The framework discussed in this post also suffers from the limitations common
    to traditional text-only RAG systems. A common source of errors in the generation
    is the incorrect retrieval of external information. In some cases, the model is
    not able to correctly identify the failure of the retrieval phase and it produces
    an answer based on incorrect, irrelevant, or misleading information. In particular,
    the presence of irrelevant passages can sometimes induce hallucinations that the
    model wouldn’t have suffered otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Another common source of errors is ignoring the retrieved passages or quoting
    them incorrectly. This can sometimes lead to subtle differences between the retrieved
    passages and the generated answer that may be difficult to spot at first glance.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, I will show an instance in which the model incorrectly quoted
    the retrieved passages.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e7e67c1da39dd4d66fda8912926ecf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Search keywords: lion, weight, average weightPhoto by [Luke Tanis](https://unsplash.com/@saluken?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: While the answer stating the weight in kilograms is correct, the model gave
    a wrong conversion to lbs for the average weight of male lions in Southern Africa,
    even though the respective passage extracted from Wikipedia reported the correct
    amount.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, I illustrated a simple framework that can be used to enhance Visual
    Question Answering with Retrieval Augmented Generation capabilities. The core
    idea of the method is to exploit the Vision Language Model to generate queries
    that will be then used by a standard RAG pipeline to retrieve information from
    an external corpus. I also presented an implementation of the framework that grants
    Phi-3.5-vision access to Wikipedia. The full code for this implementation is available
    in the [GitHub repo](https://github.com/GabrieleSgroi/vision_rag).
  prefs: []
  type: TYPE_NORMAL
- en: While the discussed method is simple and effective, it is not immune to the
    limitations common to all RAG systems, and to new challenges posed by the complexity
    of the multimodal setting. On one hand, retrieving the relevant information for
    some specific questions can be difficult. Since the search queries are created
    with the Vision Language Model, the retrieval accuracy is further limited by the
    ability of the VLM to recognize the image and to understand the details the question
    refers to. On the other hand, even after the correct information has been retrieved,
    there is no guarantee that the model won’t hallucinate while producing the answer.
    In the multimodal setting, this could be exacerbated by the fact that the model
    has to associate the correct meaning to both the text and the image and also understand
    the interactions between them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework I discussed in this post is a straightforward extension of the
    vanilla RAG pipeline, adapted to the Visual Question Answering task. Standard
    advanced RAG techniques, such as query transformation, re-ranking the retrieved
    passages, and [Hypothetical Document Embeddings (HyDE)](https://arxiv.org/abs/2212.10496)
    can be easily included to increase the performance. Furthermore, using a multimodal
    embedding model (like CLIP) new opportunities appear: the image embeddings can
    be used when searching by similarity for relevant text documents, and it is also
    possible to retrieve similar and/or relevant images to the original image and
    the question. The latter could be useful, for example, when a different point
    of view of the image is needed to answer the prompt. Another direction for improvement
    is to perform fine-tuning to get more specialized and effective models. Given
    the role of the multimodal model in the retrieval and generation process, two
    different fine-tuning processes can be performed: one to get a model specialized
    in writing search queries, and one to increase the model''s performance on the
    grounded generation task. Finally, the framework could be incorporated into a
    specialized agentic system to further boost its performance and robustness. An
    agentic system could, for example, iteratively refine the generated query by giving
    feedback on the retrieved passages and asking follow-up questions or focusing
    on searching for information about particular details of the image only when needed.
    It could also handle multi-hop question-answering tasks for more complicated questions,
    and decide when the retrieval of further external information is needed to answer
    the user’s query.'
  prefs: []
  type: TYPE_NORMAL
- en: I’d be happy to discuss further improvements and/or different approaches to
    multimodal RAG in the comment section!
  prefs: []
  type: TYPE_NORMAL
