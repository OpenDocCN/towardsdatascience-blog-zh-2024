- en: A Simple Framework for RAG Enhanced Visual Question Answering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的RAG增强视觉问答框架
- en: 原文：[https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30](https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30](https://towardsdatascience.com/a-simple-framework-for-rag-enhanced-visual-question-answering-06768094762e?source=collection_archive---------7-----------------------#2024-08-30)
- en: Empowering Phi-3.5-vision with Wikipedia knowledge for augmented Visual Question
    Answering.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为Phi-3.5-vision注入维基百科知识以增强视觉问答能力。
- en: '[](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[![Gabriele
    Sgroi, PhD](../Images/b81978d35e6238d160457de2affc2b0e.png)](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    [Gabriele Sgroi, PhD](https://medium.com/@gabrielesgroi94?source=post_page---byline--06768094762e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    ·17 min read·Aug 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--06768094762e--------------------------------)
    ·17分钟阅读·2024年8月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/186c503f14d17c4e369edaaa3013e655.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/186c503f14d17c4e369edaaa3013e655.png)'
- en: Photo by [Christian Lue](https://unsplash.com/@christianlue?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Christian Lue](https://unsplash.com/@christianlue?utm_source=medium&utm_medium=referral)
    于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: Retrieval Augmented Generation (RAG) is a powerful technique that can improve
    the accuracy and reliability of the answer generated by Large Language Models
    (LLMs). It also offers the possibility of checking the sources used by the model
    during a particular generation, allowing easier fact-checking by human users.
    Furthermore, RAG makes it possible to keep the model knowledge up-to-date and
    incorporate topic-specific information without the need for fine-tuning. Overall,
    RAG provides many benefits and few drawbacks, and its workflow is straightforward
    to implement. Because of this, it has become the go-to solution for many LLM use
    cases that require up-to-date and/or specialized knowledge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种强大的技术，可以提高大型语言模型（LLMs）生成答案的准确性和可靠性。它还提供了检查模型在特定生成过程中使用的来源的可能性，从而使人工用户更容易进行事实核查。此外，RAG使得模型的知识保持最新，并能够在不进行微调的情况下融入特定领域的信息。总的来说，RAG提供了许多好处且几乎没有缺点，并且其工作流程容易实现。因此，它已成为许多需要最新和/或专业知识的LLM使用案例的首选解决方案。
- en: Some of the latest developments in the Generative AI field have focused on extending
    the popular transformer architecture to tackle multiple input and/or output modalities,
    trying to replicate the huge success of LLMs. There are already several models,
    both open and closed source, that have demonstrated a remarkable ability to handle
    multiple modalities. A popular multimodal setting, and one of the first to be
    tackled, is that of Vision Language Models (VLMs), which has seen interesting
    open-source contributions with the release of small yet powerful models like LLaVA,
    Idefics, and Phi-vision. If you want to get started with VLMs and learn more about
    building a Vision Language Chat Assistant using LLaVA, you can look at my previous
    post [Create your Vision Chat Assistant with LLaVA](/create-your-vision-chat-assistant-with-llava-610b02c3283e).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式人工智能领域，一些最新的进展集中在扩展流行的Transformer架构，以应对多种输入和/或输出模态，尝试复制大语言模型（LLMs）的巨大成功。目前，已经有几个模型，无论是开源还是闭源，展示了处理多种模态的卓越能力。一个流行的多模态场景，且是首批被解决的场景之一，是视觉语言模型（VLMs），这一领域通过发布一些小而强大的模型，如LLaVA、Idefics和Phi-vision，取得了有趣的开源贡献。如果你想入门VLMs并了解如何使用LLaVA构建一个视觉语言聊天助手，可以查看我之前的文章[使用LLaVA创建你的视觉聊天助手](/create-your-vision-chat-assistant-with-llava-610b02c3283e)。
- en: Designing RAG systems for multimodal models is more challenging than in the
    text-only case. In fact, the design of RAG systems for LLM is well-established
    and there is some consensus about the general workflow, as many of the recent
    developments focus on improving accuracy, reliability, and scalability rather
    than fundamentally changing the RAG architecture. On the other hand, multimodality
    opens up multiple ways of retrieving relevant information and, consequentially,
    there are several different architectural choices that can be made, each with
    its own advantages and drawbacks. For example, it is possible to use a multimodal
    embedding model to create a shared vector space for the different modalities or,
    instead, choose to ground the information in one modality only.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为多模态模型设计RAG系统比纯文本模型更具挑战性。事实上，RAG系统在LLM中的设计已经成熟，并且在一般工作流程上已有共识，因为最近的许多发展都集中在提高准确性、可靠性和可扩展性，而不是从根本上改变RAG架构。另一方面，多模态性开辟了多种检索相关信息的方式，因此可以做出几种不同的架构选择，每种选择都有其优缺点。例如，可以使用多模态嵌入模型为不同的模态创建一个共享的向量空间，或者选择仅在一种模态中扎根信息。
- en: In this blog post, I will discuss a simple framework to extend RAG to Vision
    Language Models (VLMs), focusing on the Visual Question Answering task. The core
    idea of the method is to exploit the capabilities of the VLM to understand both
    text and images to generate a suitable search query that will be used to retrieve
    external information before answering the user’s prompt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将讨论一个简单的框架，如何将RAG扩展到视觉语言模型（VLMs），并重点介绍视觉问答任务。该方法的核心思想是利用VLM的能力，理解文本和图像，以生成适当的搜索查询，用于在回答用户提示之前检索外部信息。
- en: I will also provide a practical tutorial on how to implement the framework to
    empower Phi-3.5-vision with access to Wikipedia information, discussing the main
    points of the implementation and showing some examples. I will leave the details
    to the full code I shared in the following [Git Hub repo](https://github.com/GabrieleSgroi/vision_rag).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将提供一个实践教程，介绍如何实现该框架，使Phi-3.5-vision能够访问维基百科信息，讨论实现的关键点并展示一些示例。我会把详细内容留给我在以下[Git
    Hub仓库](https://github.com/GabrieleSgroi/vision_rag)中分享的完整代码。
- en: RAG for Visual Question Answering
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉问答的RAG
- en: In this section, I will describe the general workflow of the framework mentioned
    in the introduction. For the sake of exposition, I will discuss the case where
    there is only one user’s prompt about one image. This is the case, for example,
    for simple Visual Question Answering (VQA) tasks. The method can be generalized
    straightforwardly to multiple prompts and images, but the pipeline will become
    more complex and introduce further complications. Furthermore, I will only consider
    the case in which the external data consists solely of textual documents. Using
    a multimodal embedding model for retrieval, or more generally a multimodal search
    engine, it is possible to include images in the external data as well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将描述介绍中提到的框架的一般工作流程。为了便于说明，我将讨论仅有一个用户提示和一张图像的情况。这种情况，例如，适用于简单的视觉问答（VQA）任务。该方法可以直接推广到多个提示和图像，但流程将变得更加复杂，并引入更多的挑战。此外，我将仅考虑外部数据仅由文本文档组成的情况。使用多模态嵌入模型进行检索，或者更一般地，使用多模态搜索引擎，也可以将图像包含在外部数据中。
- en: 'As for the usual RAG workflow, the framework workflow can be divided into two
    parts: retrieval of the relevant external information and generation conditioned
    on the provided external data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 至于常见的RAG工作流程，该框架的工作流程可以分为两部分：检索相关的外部信息和基于提供的外部数据进行生成。
- en: During the retrieval phase, the goal is to retrieve some passages from the external
    text documents that can provide useful information to answer the user’s prompt.
    In order to do so effectively, we must ensure that the retrieved passages are
    relevant to the provided image, the prompt, and, more importantly, the relationship
    between the two. In fact, even if the retrieved documents contain information
    about the image, they may not include the specific information needed to provide
    an answer to the user’s prompt. On the other hand, the prompt may only be correctly
    understood when paired with the image it refers to. To address these challenges,
    the framework discussed in this post exploits the multimodal model to generate
    an appropriate search query, tailored to capture the information needed to answer
    the user’s prompt in the context of the provided image. A search engine will use
    the produced query to retrieve the relevant information from the external data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索阶段，目标是从外部文本文档中检索一些段落，这些段落可以提供有用的信息来回答用户的提示。为了有效地做到这一点，我们必须确保检索到的段落与提供的图像、提示以及更重要的两者之间的关系相关。实际上，即使检索到的文档包含有关图像的信息，它们也可能没有包含提供答案所需的特定信息。另一方面，提示只有在与它所指的图像配对时才可能被正确理解。为了解决这些挑战，本文讨论的框架利用多模态模型生成一个适当的搜索查询，旨在捕捉在提供的图像背景下回答用户提示所需的信息。搜索引擎将使用生成的查询从外部数据中检索相关信息。
- en: In more detail, the multimodal model receives as input both the user’s prompt
    and the image and it is tasked with creating a search query that is relevant to
    both of them as a whole. This process can be seen as a special case of a query
    transformation, designed to consider the multimodal nature of the problem. In
    fact, the model translates the user’s prompt into a search query while also considering
    the image it refers to.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地说，多模态模型将用户的提示和图像作为输入，并负责创建一个与两者整体相关的搜索查询。这个过程可以看作是查询转换的一种特例，旨在考虑问题的多模态性质。事实上，模型将用户的提示转化为一个搜索查询，同时考虑到它所指的图像。
- en: The advantage of this approach over other methods that treat each input modality
    separately, such as using a multimodal embedding model for retrieval or using
    a generated image caption/description for semantic similarity, is that it can
    capture the relationships between the prompt and the image more effectively.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于将每种输入模态单独处理的其他方法，例如使用多模态嵌入模型进行检索或使用生成的图像标题/描述来进行语义相似性分析，这种方法的优势在于，它能够更有效地捕捉提示与图像之间的关系。
- en: The flowchart for the retrieval phase is sketched below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 检索阶段的流程图如下所示。
- en: '![](../Images/bd8841a050d566ca5fe7c3a326d3d9d7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd8841a050d566ca5fe7c3a326d3d9d7.png)'
- en: During retrieval, the model is tasked to create a query that will be used by
    a search engine to retrieve the relevant passages. Image by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索过程中，模型的任务是创建一个查询，该查询将被搜索引擎用来检索相关的段落。图片由作者提供。
- en: The generation phase is very similar to the standard text-only RAG workflow,
    the only difference being that the model receives the image in its context in
    addition to the prompt and the retrieved passages. This process is illustrated
    below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成阶段与标准的文本-only RAG工作流程非常相似，唯一的区别是模型在推理时除了接收提示和检索到的段落外，还接收到图像。这一过程如下所示。
- en: '![](../Images/79f324bd23a63cc7873b6633ece07966.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79f324bd23a63cc7873b6633ece07966.png)'
- en: During generation, the model receives the retrieved passages, the prompt, and
    the image as input. Image by the author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程中，模型将检索到的段落、提示和图像作为输入。图像由作者提供。
- en: Empowering Phi-3.5-vision with Wikipedia
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Phi-3.5-vision赋能，加入维基百科
- en: In this section, I will provide a practical guide on how to apply the discussed
    framework to enhance a multimodal model by giving it access to Wikipedia. I chose
    the model [Phi-3.5-vision](https://huggingface.co/microsoft/Phi-3.5-vision-instruct),
    as it is a very powerful yet lightweight open-source Vision Language Model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将提供一份实用指南，讲解如何应用讨论过的框架，通过让多模态模型访问维基百科来增强其能力。我选择了模型[Phi-3.5-vision](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)，因为它是一个非常强大且轻量的开源视觉语言模型。
- en: In this section, I will discuss only the general aspects of the implementation,
    leaving the details to the code provided in the [GitHub repo](https://github.com/GabrieleSgroi/vision_rag).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将仅讨论实现的总体方面，具体细节请参考提供的[GitHub仓库](https://github.com/GabrieleSgroi/vision_rag)中的代码。
- en: Retrieval
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: The goal of the retrieval phase is to gather some passages from Wikipedia that
    can provide useful information to answer a user’s question about an image. In
    the code implementation, I used the Python package [wikipedia](https://pypi.org/project/wikipedia/)
    to search and retrieve content from Wikipedia.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 检索阶段的目标是从维基百科收集一些段落，这些段落可以为回答用户关于图像的问题提供有用的信息。在代码实现中，我使用了Python包[wikipedia](https://pypi.org/project/wikipedia/)来搜索和检索维基百科的内容。
- en: 'Here are the steps implemented to retrieve the relevant passages:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现检索相关段落的步骤：
- en: Use the multimodal model to generate keywords capturing the meaning of the question
    about the image.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多模态模型生成捕捉问题与图像含义的关键词。
- en: Use the generated keywords to search relevant pages on Wikipedia.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成的关键词在维基百科上搜索相关页面。
- en: Split the content of each retrieved page into chunks.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个检索到的页面的内容分割成若干块。
- en: Select the top chunks by semantic textual similarity to the question and the
    keywords.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与问题和关键词在语义文本相似度上最相关的内容块。
- en: The first step exploits Phi-3.5-vision to generate an appropriate search query
    that will be used to retrieve relevant Wikipedia pages. In order to do so, I tasked
    Phi-3.5-vision to produce keywords relevant to the user’s question and the image.
    I then used the built-in search function of the wikipedia package to retrieve
    some pages relevant to the generated keywords.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步利用Phi-3.5-vision生成一个适当的搜索查询，用于检索相关的维基百科页面。为此，我要求Phi-3.5-vision生成与用户问题和图像相关的关键词。然后，我使用维基百科包的内置搜索功能来检索与生成的关键词相关的页面。
- en: 'The general single-turn single-image chat template for Phi-vision-3.5 has the
    following structure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Phi-vision-3.5的通用单轮单图聊天模板具有以下结构：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To generate the keywords I used the following prompt:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成关键词，我使用了以下提示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The tag {question} is substituted with the user question before inference.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 标签{question}在推理前会被用户的问题替代。
- en: 'After the keywords have been generated, the built-in search function of the
    wikipedia package is used to retrieve some pages relevant to the generated keywords.
    Finally, the selected pages are split into passages, and then the most relevant
    passages are selected using an embedding model and the LangChain implementation
    of the FAISS vector store. I used the embedding model [*snowflake-arctic-embed-l*](https://huggingface.co/Snowflake/snowflake-arctic-embed-l)
    to embed the concatenation of the question and the keywords, and the chunks of
    the retrieved pages. In practice, the retrieval phase is effectively a form of
    “hybrid search” consisting of two sequential steps: keyword search using the built-in
    search function of the wikipedia package, and embedding similarity retrieval using
    an embedding model. In this way, the retrieval operates on the smaller space of
    the passages of the most relevant pages selected using keyword search, avoiding
    the need to build an enormous vector store with the embeddings of all the content
    of Wikipedia. In different settings, the retrieval phase could be remodeled to
    use similarity retrieval on the whole external corpus or using different combinations
    of retrieval methods.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成关键词后，使用 Wikipedia 包的内置搜索功能来检索与生成的关键词相关的页面。最后，将选定的页面拆分为段落，然后使用嵌入模型和 LangChain
    实现的 FAISS 向量存储来选择最相关的段落。我使用了嵌入模型[*snowflake-arctic-embed-l*](https://huggingface.co/Snowflake/snowflake-arctic-embed-l)来嵌入问题和关键词的拼接，以及检索到的页面的片段。实际上，检索阶段本质上是一种“混合搜索”形式，包含两个顺序步骤：使用
    Wikipedia 包的内置搜索功能进行关键词搜索，以及使用嵌入模型进行相似度检索。通过这种方式，检索在通过关键词搜索选定的最相关页面的较小段落空间上进行，避免了需要建立一个包含整个
    Wikipedia 内容的庞大向量存储。在不同的设置中，检索阶段可以重新设计为在整个外部语料库上进行相似度检索，或使用不同的检索方法组合。
- en: Retrieving passages from multiple pages can help reduce the chance of selecting
    the wrong page and it can also be useful when information from multiple pages
    is needed to produce an answer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个页面检索段落有助于减少选择错误页面的概率，并且当需要从多个页面获取信息来生成答案时也很有用。
- en: Generation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成
- en: In the generation phase, the user’s question, the retrieved passages, and the
    original images are used as inputs for Phi-3.5-vision to generate an answer.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成阶段，用户的问题、检索到的段落和原始图像作为输入传递给 Phi-3.5-vision 来生成答案。
- en: 'I used the following prompt in the general chat template for Phi-3.5-vision:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Phi-3.5-vision 的通用聊天模板中使用了以下提示：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At generation time, the tag {question} is substituted by the user question as
    before, while the tag {passages} is substituted by the retrieved passages and
    the names of the corresponding pages with the following format
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成时，标签 {question} 会像之前一样被用户问题替代，而标签 {passages} 会被检索到的段落和相应页面名称替代，格式如下：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Providing the name of the page from which the passage is extracted can help
    resolve ambiguities when the content of the latter is not enough to uniquely determine
    the subject or topic it refers to.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 提供提取段落的页面名称有助于解决歧义，特别是当段落内容不足以唯一确定其所指的主题或话题时。
- en: Examples
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: In this section, I will show some examples of answers obtained with the implementation
    discussed in the previous section, comparing the outputs of the Vision Language
    Model empowered with RAG with the base version.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将展示上一节中讨论的实现获得的一些答案示例，比较带有 RAG 功能的视觉语言模型和基础版本的输出。
- en: For each example below, I will show the image provided to the model, a block
    with the question and the answers of both the RAG augmented and base VLM, a block
    with the search query created by the model, and a block with the passages retrieved
    from Wikipedia.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下面的每个示例，我将展示提供给模型的图像、包含问题及 RAG 增强和基础 VLM 答案的块、模型生成的搜索查询块，以及从 Wikipedia 检索到的段落块。
- en: Example 1
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1
- en: '![](../Images/fc32fffd5a80c737c08d79cf43adfc2f.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc32fffd5a80c737c08d79cf43adfc2f.png)'
- en: Photo by [engin akyurt](https://unsplash.com/@enginakyurt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[engin akyurt](https://unsplash.com/@enginakyurt?utm_source=medium&utm_medium=referral)于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The VLM generated the following search keywords:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: VLM 生成了以下搜索关键词：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And retrieved the following passages:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 并检索到以下段落：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The model augmented with RAG correctly reported the height range stated on the
    Wikipedia page ‘Tomato’, while the base model answered with a lower range.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 增强了RAG的模型正确报告了Wikipedia页面‘番茄’上列出的高度范围，而基础模型的回答则是一个较低的范围。
- en: Example 2
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 2
- en: '![](../Images/2820d1d705765e00d8da60086af89f1b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2820d1d705765e00d8da60086af89f1b.png)'
- en: Photo by [Hans-Jurgen Mager](https://unsplash.com/@hansjurgen007?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Hans-Jurgen Mager](https://unsplash.com/@hansjurgen007?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Again, the RAG model answered correctly with the weights reported in the Wikipedia
    page ‘Polar bear’, while the base model answered with a slightly different range
    of weights.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，RAG模型使用Wikipedia页面‘北极熊’中报告的权重正确回答，而基础模型则使用稍有不同范围的权重作答。
- en: Example 3
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 3
- en: In my previous blog post [Create your Vision Chat Assistant with LLaVA](/create-your-vision-chat-assistant-with-llava-610b02c3283e),
    I showed how the LLaVA-1.5 model can be induced to hallucinate when the user’s
    prompt asks a misleading question about the image. Let’s see what happens in that
    example using the Phi-3.5-vision model with and without the RAG enhancement. In
    particular, the model could now be more prone to hallucination if any misleading
    passages were retrieved.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的博客文章 [使用LLaVA创建视觉聊天助手](/create-your-vision-chat-assistant-with-llava-610b02c3283e)，我展示了当用户的提示询问图像相关的误导性问题时，LLaVA-1.5模型如何被诱导产生幻觉。让我们看看使用Phi-3.5-vision模型在有无RAG增强情况下会发生什么。在这个例子中，特别是当检索到任何误导性段落时，模型现在可能更容易产生幻觉。
- en: '![](../Images/1f408a52ca0ef21463de523769cb64d1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f408a52ca0ef21463de523769cb64d1.png)'
- en: Photo by [Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Wolfgang Hasselmann](https://unsplash.com/@wolfgang_hasselmann?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Neither the base nor the RAG model were induced to hallucinate by the misleading
    prompt, indicating a possible better robustness of Phi-3.5-vision compared to
    LLaVA-1.5\. In particular, the RAG VLM did not retrieve any page from Wikipedia.
    In fact, when asked to generate keywords, the model answered
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型和RAG模型都没有被误导性提示诱导产生幻觉，表明Phi-3.5-vision相比LLaVA-1.5可能具有更好的鲁棒性。特别是，RAG VLM没有从Wikipedia检索到任何页面。事实上，当被要求生成关键词时，模型回答了
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This search query did not produce any results with the wikipedia package search
    function.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 该搜索查询未通过Wikipedia包的搜索功能生成任何结果。
- en: '*Note: in the latter example, in earlier experiments conducted with Idefics
    2, both the base VLM and RAG VLM models were affected by hallucinations, answering
    with the names of fish species not present in the image. In particular, the RAG
    model retrieved misleading passages related to various fishes. The phenomenon
    of hallucinations is model-dependent and the RAG framework may not be enough to
    eliminate it in all instances.*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：在后面的例子中，在与Idefics 2进行的早期实验中，基础VLM模型和RAG VLM模型都受到了幻觉的影响，回答了图像中不存在的鱼类种类名称。特别是，RAG模型检索到了与各种鱼类相关的误导性段落。幻觉现象是模型依赖的，RAG框架可能无法在所有情况下完全消除它。*'
- en: Limitations
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 限制
- en: The main limitation of the framework is that it requires the model to have at
    least a general knowledge of the image and the question provided (while the exact
    details will be retrieved with RAG) in order to produce a useful search query.
    In some domain-specific settings, it may be necessary to fine-tune the model for
    search query generation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架的主要限制是它要求模型至少具备图像和提供问题的基本知识（具体细节将通过RAG检索），以生成有效的搜索查询。在某些特定领域的设置中，可能需要对模型进行微调，以生成搜索查询。
- en: The framework discussed in this post also suffers from the limitations common
    to traditional text-only RAG systems. A common source of errors in the generation
    is the incorrect retrieval of external information. In some cases, the model is
    not able to correctly identify the failure of the retrieval phase and it produces
    an answer based on incorrect, irrelevant, or misleading information. In particular,
    the presence of irrelevant passages can sometimes induce hallucinations that the
    model wouldn’t have suffered otherwise.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中讨论的框架也存在传统文本-only RAG系统常见的局限性。生成中的一个常见错误来源是外部信息检索不正确。在某些情况下，模型无法正确识别检索阶段的失败，从而生成基于错误、不相关或误导性信息的答案。特别是，不相关段落的存在有时会引发模型本不会出现的幻觉。
- en: Another common source of errors is ignoring the retrieved passages or quoting
    them incorrectly. This can sometimes lead to subtle differences between the retrieved
    passages and the generated answer that may be difficult to spot at first glance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的错误来源是忽视检索到的段落或错误引用它们。这有时会导致检索到的段落和生成的答案之间存在微妙的差异，可能一开始不容易察觉。
- en: Example
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: In this example, I will show an instance in which the model incorrectly quoted
    the retrieved passages.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我将展示一个模型错误引用检索到的段落的实例。
- en: '![](../Images/7e7e67c1da39dd4d66fda8912926ecf4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7e67c1da39dd4d66fda8912926ecf4.png)'
- en: 'Search keywords: lion, weight, average weightPhoto by [Luke Tanis](https://unsplash.com/@saluken?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索关键词：雄狮，体重，平均体重  图片由[Luke Tanis](https://unsplash.com/@saluken?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: While the answer stating the weight in kilograms is correct, the model gave
    a wrong conversion to lbs for the average weight of male lions in Southern Africa,
    even though the respective passage extracted from Wikipedia reported the correct
    amount.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管答案中以千克为单位的体重是正确的，但模型在转换为南非雄狮的平均体重时给出了错误的磅数，尽管从维基百科提取的相关段落报告了正确的数值。
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, I illustrated a simple framework that can be used to enhance Visual
    Question Answering with Retrieval Augmented Generation capabilities. The core
    idea of the method is to exploit the Vision Language Model to generate queries
    that will be then used by a standard RAG pipeline to retrieve information from
    an external corpus. I also presented an implementation of the framework that grants
    Phi-3.5-vision access to Wikipedia. The full code for this implementation is available
    in the [GitHub repo](https://github.com/GabrieleSgroi/vision_rag).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了一个可以用来增强视觉问答系统（Visual Question Answering，VQA）并具备检索增强生成（RAG）能力的简单框架。该方法的核心思想是利用视觉语言模型生成查询，然后通过标准的RAG管道来检索外部语料库中的信息。我还展示了该框架的实现，它使Phi-3.5-vision能够访问维基百科。该实现的完整代码可以在[GitHub仓库](https://github.com/GabrieleSgroi/vision_rag)中找到。
- en: While the discussed method is simple and effective, it is not immune to the
    limitations common to all RAG systems, and to new challenges posed by the complexity
    of the multimodal setting. On one hand, retrieving the relevant information for
    some specific questions can be difficult. Since the search queries are created
    with the Vision Language Model, the retrieval accuracy is further limited by the
    ability of the VLM to recognize the image and to understand the details the question
    refers to. On the other hand, even after the correct information has been retrieved,
    there is no guarantee that the model won’t hallucinate while producing the answer.
    In the multimodal setting, this could be exacerbated by the fact that the model
    has to associate the correct meaning to both the text and the image and also understand
    the interactions between them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所讨论的方法简单且有效，但它仍然无法避免所有RAG系统常见的局限性，也无法应对多模态设置所带来的新挑战。一方面，检索某些特定问题的相关信息可能会很困难。由于搜索查询是通过视觉语言模型（Vision
    Language Model，VLM）创建的，因此检索准确性进一步受限于VLM识别图像和理解问题所指的细节的能力。另一方面，即使正确的信息已被检索出来，也不能保证模型在生成答案时不会出现幻觉。在多模态设置中，这个问题可能因模型需要同时关联文本和图像的正确含义，并且还需要理解它们之间的互动而变得更加严重。
- en: 'The framework I discussed in this post is a straightforward extension of the
    vanilla RAG pipeline, adapted to the Visual Question Answering task. Standard
    advanced RAG techniques, such as query transformation, re-ranking the retrieved
    passages, and [Hypothetical Document Embeddings (HyDE)](https://arxiv.org/abs/2212.10496)
    can be easily included to increase the performance. Furthermore, using a multimodal
    embedding model (like CLIP) new opportunities appear: the image embeddings can
    be used when searching by similarity for relevant text documents, and it is also
    possible to retrieve similar and/or relevant images to the original image and
    the question. The latter could be useful, for example, when a different point
    of view of the image is needed to answer the prompt. Another direction for improvement
    is to perform fine-tuning to get more specialized and effective models. Given
    the role of the multimodal model in the retrieval and generation process, two
    different fine-tuning processes can be performed: one to get a model specialized
    in writing search queries, and one to increase the model''s performance on the
    grounded generation task. Finally, the framework could be incorporated into a
    specialized agentic system to further boost its performance and robustness. An
    agentic system could, for example, iteratively refine the generated query by giving
    feedback on the retrieved passages and asking follow-up questions or focusing
    on searching for information about particular details of the image only when needed.
    It could also handle multi-hop question-answering tasks for more complicated questions,
    and decide when the retrieval of further external information is needed to answer
    the user’s query.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这篇文章中讨论的框架是对基础 RAG 流水线的直接扩展，适用于视觉问答任务。可以轻松加入标准的高级 RAG 技术，如查询转换、重新排序检索到的段落，以及[假设文档嵌入（HyDE）](https://arxiv.org/abs/2212.10496)来提高性能。此外，使用多模态嵌入模型（如
    CLIP）可以带来新的机会：在进行相似性搜索时，可以利用图像嵌入来查找相关的文本文档，同时也可以检索与原始图像和问题相似和/或相关的图像。后者在某些情况下可能会很有用，比如当需要从不同角度查看图像以回答提示时。另一个改进方向是进行微调，以获得更专业和更有效的模型。考虑到多模态模型在检索和生成过程中的作用，可以执行两种不同的微调过程：一种是专门针对生成搜索查询的模型，另一种是提高模型在基于事实生成任务中的表现。最后，框架可以集成到一个专门的代理系统中，进一步提升其性能和鲁棒性。比如，一个代理系统可以通过对检索到的段落提供反馈、提出后续问题，或仅在必要时专注于搜索图像中特定细节的信息，来迭代地完善生成的查询。它还可以处理更复杂问题的多跳问答任务，并决定何时需要检索更多外部信息来回答用户的查询。
- en: I’d be happy to discuss further improvements and/or different approaches to
    multimodal RAG in the comment section!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我很乐意在评论区讨论更多改进和/或多模态 RAG 的不同方法！
