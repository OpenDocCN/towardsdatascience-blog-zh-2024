- en: 'Understanding Positional Embeddings in Transformers: From Absolute to Rotary'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20](https://towardsdatascience.com/understanding-positional-embeddings-in-transformers-from-absolute-to-rotary-31c082e16b26?source=collection_archive---------2-----------------------#2024-07-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into absolute, relative, and rotary positional embeddings with code
    examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page---byline--31c082e16b26--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--31c082e16b26--------------------------------)
    ·17 min read·Jul 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4cf43e5eac52b5568bc147832f80905.png)'
  prefs: []
  type: TYPE_IMG
- en: Rotary position embedding — Image from [[6](https://arxiv.org/pdf/2104.09864)]
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the key components of transformers are **positional embeddings**. You
    may ask: why? Because the self-attention mechanism in transformers is permutation-invariant;
    that means it computes the amount of `attention` each token in the input receives
    from other tokens in the sequence, however it does not take the order of the tokens
    into account. In fact, *attention mechanism treats the sequence as a bag of tokens*.
    For this reason, we need to have another component called positional embedding
    which accounts for the order of tokens and it influences token embeddings. But
    what are the different types of positional embeddings and how are they implemented?'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we take a look at three major types of positional embeddings and
    dive deep into their implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Here is the table of content for this post:**'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Context and Background
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Absolute Positional Embedding
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Learned Approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.2 Fixed Approach (Sinusoidal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2.3 Code Example: RoBERTa Implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
