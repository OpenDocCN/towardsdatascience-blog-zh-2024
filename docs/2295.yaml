- en: 'AdEMAMix: A Deep Dive into a New Optimizer for Your Deep Neural Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19](https://towardsdatascience.com/ademamix-a-deep-dive-into-a-new-optimizer-for-your-deep-neural-network-6168e2c1da35?source=collection_archive---------10-----------------------#2024-09-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A better and faster option than the ADAM optimizer, from Apple Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page---byline--6168e2c1da35--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6168e2c1da35--------------------------------)
    ·12 min read·Sep 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48723d07a415766eeb3f87f164e02730.png)'
  prefs: []
  type: TYPE_IMG
- en: Which way to go? Let’s Optimize (Generated with DALL·E 3 with Author’s Prompt)
  prefs: []
  type: TYPE_NORMAL
- en: Deep Neural Networks (DNNs) are regarded as one of the most effective tools
    for finding patterns in large datasets through training. At the core of the training
    problems, we have complex loss landscapes and the training of a DNN boils down
    to optimizing the loss as the number of iterations increases. A few of the most
    commonly used optimizers are Stochastic Gradient Descent, RMSProp (Root Mean Square
    Propagation), Adam (Adaptive Moment Estimation) etc.
  prefs: []
  type: TYPE_NORMAL
- en: Recently (September 2024), researchers from Apple (and EPFL) proposed a new
    optimizer, AdEMAMix¹, which they show to work better and faster than AdamW optimizer
    for language modeling and image classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, I will go into detail about the mathematical concepts behind
    this optimizer and discuss some very interesting results presented in this paper.
    Topics that will be covered in this post are:'
  prefs: []
  type: TYPE_NORMAL
- en: Review of Adam Optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exponential Moving Average (EMA) in Adam.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Main Idea Behind AdEMAMix: Mixture of two EMAs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Exponential Decay Rate Scheduler in AdEMAMix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
