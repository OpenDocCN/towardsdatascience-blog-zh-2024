- en: 'Long-form video representation learning (Part 1: Video as graphs)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长格式视频表示学习（第1部分：视频作为图）
- en: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100?source=collection_archive---------7-----------------------#2024-05-14)
- en: We explore novel video representations methods that are equipped with long-form
    reasoning capability. This is part 1 focusing on video representation as graphs
    and how to learn light-weights graph neural networks for several downstream applications.
    [Part II](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)
    focuses on sparse video-text transformers. And [Part III](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e)
    provides a sneak peek into our latest and greatest explorations.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们探索了具备长格式推理能力的新型视频表示方法。第1部分着重讨论视频作为图的表示，以及如何为多个下游应用学习轻量级的图神经网络。[第2部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)聚焦于稀疏视频-文本变换器。而[第3部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e)则展示了我们最新最前沿的探索。
- en: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[![Subarna
    Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)
    [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--c55b609d9100--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)
    ·10 min read·May 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c55b609d9100--------------------------------)
    ·10分钟阅读·2024年5月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Existing video architectures tend to hit computation or memory bottlenecks after
    processing only a few seconds of the video content. So, how do we enable accurate
    and efficient long-form visual understanding? An important first step is to have
    a model that practically runs on long videos. To that end, we explore novel video
    representations methods that are equipped with long-form reasoning capability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的视频架构在处理视频内容的几秒钟后往往会遇到计算或内存瓶颈。那么，我们如何实现准确高效的长格式视觉理解呢？一个重要的第一步是拥有一个能在长视频上实际运行的模型。为此，我们探索了具备长格式推理能力的新型视频表示方法。
- en: '**What is long-form reasoning and why ?**'
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**什么是长格式推理，为什么要进行长格式推理？**'
- en: As we saw the huge leap of success of image-based understanding tasks with deep
    learning models such as convolutions or transformers, the next step naturally
    became going beyond still images and exploring video understanding. Developing
    video understanding models require two equally important focus areas. First is
    a large scale video dataset and the second is the learnable backbone for extracting
    video features efficiently. Creating finer-grained and consistent annotations
    for a dynamic signal such as a video is not trivial even with the best intention
    from both the system designer as well as the annotators. Naturally, the large
    video datasets that were created, took the relatively easier approach of annotating
    at the whole video level. About the second focus area, again it was natural to
    extend image-based models (such as CNN or transformers) for video understanding
    since videos are perceived as a collection of video frames each of which is identical
    in size and shape of an image. Researchers made their models that use sampled
    frames as inputs as opposed to all the video frames for obvious memory budget.
    To put things into perspective, when analyzing a 5-minute video clip at 30 frames/second,
    we need to process a bundle of 9,000 video frames. Neither CNN nor Transformers
    can operate on a sequence of 9,000 frames as a whole if it involves dense computations
    at the level of 16x16 rectangular patches extracted from each video frame. Thus
    most models operate in the following way. They take a short video clip as an input,
    do prediction, followed by temporal smoothing as opposed to the ideal scenario
    where we want the model to look at the video in its entirety.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们看到的，基于图像的理解任务随着深度学习模型（如卷积神经网络或变换器）取得了巨大成功，下一步自然就是超越静态图像，探索视频理解。开发视频理解模型需要两个同样重要的关注领域。首先是大规模的视频数据集，其次是用于高效提取视频特征的可学习骨干网络。即使设计者和标注者都有最好的意图，为动态信号（如视频）创建更细粒度且一致的注释也并非易事。自然，创建的大型视频数据集采用了相对更简单的方法，即在整个视频层面进行标注。至于第二个关注点，再次自然地将基于图像的模型（如CNN或变换器）扩展到视频理解中，因为视频被视为一系列视频帧的集合，每一帧的大小和形状都与图像相同。研究人员制作了使用采样帧作为输入的模型，而不是使用所有视频帧，这显然是为了节省内存。在具体应用中，当分析一个5分钟的视频片段，且视频帧率为30帧/秒时，我们需要处理9,000帧视频。若涉及到对每个视频帧提取16x16矩形块的密集计算，CNN或变换器都无法处理9,000帧的序列。因此，大多数模型的操作方式是：它们将一个短视频片段作为输入，进行预测，然后进行时间平滑，而不是理想情况下我们希望模型能够完整地观看视频。
- en: Now comes this question. If we need to know whether a video is of type ‘swimming’
    vs ‘tennis’, do we really need to analyze a minute-worth content? The answer is
    most certainly NO. In other words, the models optimized for video recognition,
    most likely learned to look at background and other spatial context information
    instead of learning to reason over what is actually happening in a ‘long’ video.
    We can term this phenomenon as learning the spatial shortcut. These models were
    good for video recognition tasks in general. Can you guess how do these models
    generalize for other tasks that require actual temporal reasoning such as action
    forecasting, video question-answering, and recently proposed episodic memory tasks?
    Since they weren’t trained for doing temporal reasoning, they turned out not quite
    good for those applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在提出这个问题。如果我们需要知道一个视频是“游泳”类型还是“网球”类型，是否真的需要分析一分钟的内容？答案显然是否定的。换句话说，优化用于视频识别的模型，很可能学会了查看背景和其他空间上下文信息，而不是学习推理一个“长”视频中实际发生的事情。我们可以将这种现象称为学习空间捷径。这些模型在视频识别任务中表现良好。那么你能猜到这些模型在需要实际时间推理的其他任务（如动作预测、视频问答和最近提出的情节记忆任务）中如何表现吗？由于它们没有经过时间推理训练，因此它们在这些应用中的表现并不好。
- en: So we understand that datasets / annotations prevented most video models from
    learning to reason over time and sequence of actions. Gradually, researchers realized
    this problem and started coming up with different benchmarks addressing long-form
    reasoning. However, one problem still persisted which is mostly memory-bound i.e.
    how do we even make the first practical stride where a model can take a long-video
    as input as opposed to a sequence of short-clips processed one after another.
    To address that, we propose a novel video representation method based on Spatio-Temporal
    Graphs Learning (SPELL) to equip the model with long-form reasoning capability.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们理解到，数据集/注释使得大多数视频模型无法学习如何在时间和动作序列上进行推理。随着时间的推移，研究人员意识到了这个问题，并开始提出不同的基准来解决长时间推理问题。然而，仍然存在一个问题，主要是由内存限制引起的，即我们如何迈出第一步，让模型能够接受一段长视频作为输入，而不是将多个短片段依次处理。为了解决这个问题，我们提出了一种基于时空图学习（Spatio-Temporal
    Graphs Learning，简称SPELL）的视频表示方法，以赋予模型长时间推理的能力。
- en: Video as a temporal graph
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频作为时间图
- en: Let G = (V, E) be a graph with the node set V and edge set E. For domains such
    as social networks, citation networks, and molecular structure, the V and E are
    available to the system, and we say the graph is given as an input to the learnable
    models. Now, let’s consider the simplest possible case in a video where each of
    the video frame is considered a node leading to the formation of V. However, it
    is not clear whether and how node t1 (frame at time=t1) and node t2 (frame at
    time=t2) are connected. Thus, the set of edges, E, is not provided. Without E,
    the topology of the graph is not complete, resulting into unavailability of the
    “ground truth” graphs. One of the most important challenges remains how to convert
    a video to a graph. This graph can be considered as a latent graph since there
    is no such labeled (or “ground truth”) graph available in the dataset.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 设G = (V, E)为一个图，其中V为节点集，E为边集。对于社交网络、引用网络和分子结构等领域，V和E是系统所提供的，我们称图作为输入提供给可学习的模型。现在，考虑视频中的最简单情况，其中每一帧视频被视为一个节点，形成V。然而，目前尚不清楚节点t1（时间=t1时的帧）和节点t2（时间=t2时的帧）是如何连接的。因此，边集E没有提供。没有E，图的拓扑结构就不完整，导致“真实值”图的不可用。一个重要的挑战依然是如何将视频转换为图。由于数据集中没有这样的标注（或“真实值”）图，所以该图可以被视为潜在图。
- en: When a video is modeled as a temporal graph, many video understanding problems
    can be formulated as either node classification or graph classification problems.
    We utilize a SPELL framework for tasks such as Action Boundary Detection, Temporal
    Action Segmentation, Video summarization / highlight reels detection.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当视频被建模为时间图时，许多视频理解问题可以被表述为节点分类或图分类问题。我们利用SPELL框架来处理诸如动作边界检测、时间动作分割、视频摘要/高亮回放检测等任务。
- en: 'Video Summarization : Formulated as a node classification problem'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视频摘要：表述为节点分类问题
- en: Here we present such a framework, namely VideoSAGE which stands for Video Summarization
    with Graph Representation Learning. We leverage the video as a temporal graph
    approach for video highlights reel creation using this framework. First, we convert
    an input video to a graph where nodes correspond to each of the video frames.
    Then, we impose sparsity on the graph by connecting only those pairs of nodes
    that are within a specified temporal distance. We then formulate the video summarization
    task as a binary node classification problem, precisely classifying video frames
    whether they should belong to the output summary video. A graph constructed this
    way (as shown in Figure 1) aims to capture long-range interactions among video
    frames, and the sparsity ensures the model trains without hitting the memory and
    compute bottleneck. Experiments on two datasets(SumMe and TVSum) demonstrate the
    effectiveness of the proposed nimble model compared to existing state-of-the-art
    summarization approaches while being one order of magnitude more efficient in
    compute time and memory.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提出了这样一个框架，称为VideoSAGE，代表视频摘要与图表示学习。我们利用视频作为时间图的方法，通过该框架生成视频亮点集锦。首先，我们将输入的视频转换为图，其中每个节点对应视频的每一帧。然后，我们通过仅连接那些在指定时间距离内的节点对来对图施加稀疏性。接着，我们将视频摘要任务表述为一个二元节点分类问题，精确地对视频帧进行分类，判断它们是否应属于输出的摘要视频。以这种方式构建的图（如图1所示）旨在捕捉视频帧之间的长程交互，而稀疏性确保了模型在训练时不会遇到内存和计算瓶颈。对两个数据集（SumMe和TVSum）进行的实验表明，与现有的最先进的摘要方法相比，所提出的高效模型在计算时间和内存使用上提高了一个数量级的效率。
- en: '![](../Images/46bd7806ce3bad4dd5aef3c2feab206c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46bd7806ce3bad4dd5aef3c2feab206c.png)'
- en: '(image by author) Figure 1: *VideoSAGE constructs a graph from the input video
    with each node encoding a frame. We formulate the video summarization problem
    as a binary node classification problem*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (图片来源：作者) 图1：*VideoSAGE从输入视频构建图，每个节点编码一帧。我们将视频摘要问题表述为一个二元节点分类问题。*
- en: The tables below show the comparative results of our method, namely VideoSAGE,
    on performances and objective scores. This has recently been accepted in a workshop
    at CVPR 2024\. The paper details and more results are available [here.](https://arxiv.org/pdf/2404.10539)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了我们方法（即VideoSAGE）在性能和客观评分上的对比结果。这篇论文最近已被CVPR 2024工作坊接受。论文详细信息和更多结果请见[此处](https://arxiv.org/pdf/2404.10539)。
- en: '![](../Images/7eb6d1cadf078899548713e0d781918a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eb6d1cadf078899548713e0d781918a.png)'
- en: '*(image by author) Table 1: (left) Comparison with SOTA methods on the SumMe
    and TVSum datasets and (right) profiling inference using A2Summ, PGL-SUM and VideoSAGE.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：作者) 表1：（左）与SOTA方法在SumMe和TVSum数据集上的比较，（右）使用A2Summ、PGL-SUM和VideoSAGE进行推理分析。*'
- en: 'Action Segmentation : Formulated as a node classification problem'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作分割：作为一个节点分类问题进行表述
- en: Similarly, we also pose the action segmentation problem as a node classification
    in such a sparse graph constructed from the input video. The GNN structure is
    similar to the above, except the last GNN layer is Graph Attention Network (GAT)
    instead of SageConv as used in the video summarization. We perform experiments
    on 50-Salads dataset. We leverage MSTCN or ASFormer as the stage 1 initial feature
    extractors. Next, we utilize our sparse, Bi-Directional GNN model that utilizes
    concurrent temporal “forward” and “backward” local message-passing operations.
    The GNN model further refine the final, fine-grain per-frame action prediction
    of our system. Refer to table 2 for the results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们也将动作分割问题作为在这种从输入视频构建的稀疏图中的节点分类问题。GNN结构与上面类似，唯一的区别是最后一层GNN使用的是图注意力网络（GAT），而不是视频摘要中使用的SageConv。我们在50-Salads数据集上进行了实验。我们利用MSTCN或ASFormer作为第一阶段的初始特征提取器。接下来，我们利用我们的稀疏双向GNN模型，该模型利用并行的时间“前向”和“后向”局部消息传递操作。GNN模型进一步优化了我们系统的最终细粒度每帧动作预测。有关结果，请参阅表2。
- en: '![](../Images/d311245330649d21ebfc5e28487a4029.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d311245330649d21ebfc5e28487a4029.png)'
- en: '*(image by author) Table 2: Action Segmentation results on 50-Salads dataset
    as measured by F1@.1 and Accuracy.*'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图片来源：作者) 表2：在50-Salads数据集上进行的动作分割结果，评估指标为F1@0.1和准确率。*'
- en: Video as “object-centric” spatio-temporal graph
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频作为“面向对象”的时空图
- en: In this section, we will describe how we can take the similar graph based approach
    where nodes denote “objects” instead of one whole video frame. We will start with
    a specific example to describe the spatio-temporal graph approach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将描述如何采用类似的基于图的方式，其中节点表示“对象”而不是整个视频帧。我们将从一个具体的示例开始，来描述时空图方法。
- en: '![](../Images/e875282b21bc287d0b5ec792a2f090fd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e875282b21bc287d0b5ec792a2f090fd.png)'
- en: '*(image by author) Figure 2: We convert a video into a canonical graph from
    the audio-visual input data, where each node corresponds to a person in a frame,
    and an edge represents a spatial or temporal interaction between the nodes. The
    constructed graph is dense enough for modeling long-term dependencies through
    message passing across the temporally-distant but relevant nodes, yet sparse enough
    to be processed within low memory and computation budget. The ASD task is posed
    as a binary node classification in this long-range spatial-temporal graph.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*(图由作者提供) 图 2：我们将视频从视听输入数据转换为标准图，其中每个节点对应于帧中的一个人，边表示节点之间的空间或时间交互。构建的图足够稠密，可以通过跨越时间上远离但相关的节点进行信息传递，来建模长期依赖关系，但又足够稀疏，可以在低内存和计算预算下处理。ASD
    任务被设定为这个长范围时空图中的二元节点分类任务。*'
- en: 'Active Speaker Detection : Task formulated as node classification'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动说话人检测：任务被表述为节点分类
- en: Figure 2 illustrates an overview of our framework designed for Active Speaker
    Detection (ASD) task. With the audio-visual data as input, we construct a multimodal
    graph and cast the ASD as a graph node classification task. Figure 3 demonstrates
    the graph construction process. First, we create a graph where the nodes correspond
    to each person within each frame, and the edges represent spatial or temporal
    relationships among them. The initial node features are constructed using simple
    and lightweight 2D convolutional neural networks (CNNs) instead of a complex 3D
    CNN or a transformer. Next, we perform binary node classification i.e. active
    or inactive speaker — on each node of this graph by learning a light-weight three-layer
    graph neural network (GNN). Graphs are constructed specifically for encoding the
    spatial and temporal dependencies among the different facial identities. Therefore,
    the GNN can leverage this graph structure and model the temporal continuity in
    speech as well as the long-term spatial-temporal context, while requiring low
    memory and computation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2 展示了我们为主动说话人检测（ASD）任务设计的框架概览。通过将视听数据作为输入，我们构建了一个多模态图，并将 ASD 转化为一个图节点分类任务。图
    3 展示了图的构建过程。首先，我们创建一个图，其中每个节点对应于每帧中的一个人，边表示它们之间的空间或时间关系。初始节点特征是通过简单且轻量级的二维卷积神经网络（CNN）构建的，而不是复杂的
    3D CNN 或 Transformer。接下来，我们在这个图的每个节点上执行二分类节点分类，即活跃或非活跃说话人——通过学习一个轻量级的三层图神经网络（GNN）。图是专门为编码不同面部身份之间的空间和时间依赖关系而构建的。因此，GNN
    可以利用这一图结构来建模语音中的时间连续性，以及长期的时空上下文，同时需要较低的内存和计算。
- en: You can ask why the graph construction is this way? Here comes the influence
    of the domain knowledge. The reason the nodes within a time distance that share
    the same face-id are connected with each other is to model the real-world scenario
    that if a person is taking at t=1 and the same person is talking at t=5, the chances
    are that person is talking at t=2,3,4\. Why we connect different face-ids if they
    share the same time-stamp? That’s because, in general, if a person is talking
    others are most likely listening. If we had connected all nodes with each other
    and made the graph dense, the model not only would have required huge memory and
    compute, they would also have become noisy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么图的构建方式是这样的？这就是领域知识的影响所在。之所以在同一时间距离内，具有相同面部 ID 的节点会相互连接，是为了模拟现实世界中的场景：如果一个人在
    t=1 时在拍摄，而同一个人在 t=5 时在讲话，那么很有可能他在 t=2、t=3、t=4 时也在讲话。为什么我们要将不同的面部 ID 连接在一起，如果它们共享相同的时间戳？这是因为一般来说，如果一个人在讲话，其他人更可能在听。如果我们将所有节点彼此连接，使得图变得稠密，模型不仅需要巨大的内存和计算资源，而且还会变得嘈杂。
- en: 'We perform extensive experiments on the AVA-ActiveSpeaker dataset. Our results
    show that SPELL outperforms all previous state-of-the-art (SOTA) approaches. Thanks
    to sparsity (~95%) of the constructed graphs, SPELL requires significantly less
    hardware resources for the visual feature encoding (11.2M #Params) compared to
    ASDNet (48.6M #Params), one of the leading state-of-the-art methods of that time.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在AVA-ActiveSpeaker数据集上进行了广泛的实验。结果表明，SPELL优于所有先前的最先进（SOTA）方法。得益于所构建图谱的稀疏性（约95%），与当时领先的SOTA方法之一ASDNet（48.6M
    #Params）相比，SPELL在视觉特征编码时需要显著更少的硬件资源（11.2M #Params）。'
- en: '![](../Images/5cdd5baf9097f49bfd4a92b2a7144280.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cdd5baf9097f49bfd4a92b2a7144280.png)'
- en: '*(image by author) Figure 3: (a): An illustration of our graph construction
    process. The frames above are temporally ordered from left to right. The three
    colors of blue, red, and yellow denote three identities that are present in the
    frames. Each node in the graph corresponds to each face in the frames. SPELL connects
    all the inter-identity faces from the same frame with the undirected edges. SPELL
    also connects the same identities by the forward/backward/undirected edges across
    the frames (controlled by a hyperparameter, τ) . In this example, the same identities
    are connected across the frames by the forward edges, which are directed and only
    go in the temporally forward direction. (b): The process for creating the backward
    and undirected graph is identical, except in the former case the edges for the
    same identities go in the opposite direction and the latter has no directed edge.
    Each node also contains the audio information which is not shown here.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*(作者提供的图片) 图 3：（a）：我们图谱构建过程的示意图。上面的框架按时间顺序从左到右排列。蓝色、红色和黄色三种颜色表示框架中存在的三种身份。图中的每个节点对应框架中的每个面孔。SPELL通过无向边连接同一框架中不同身份的面孔。SPELL还通过前向/后向/无向边跨框架连接相同身份的面孔（由超参数τ控制）。在这个示例中，相同的身份通过前向边跨框架连接，前向边是有向的，仅朝时间前进的方向延伸。（b）：创建后向图和无向图的过程是相同的，唯一的不同是在前者的情况下，相同身份的边缘朝相反方向延伸，而在后者的情况下则没有有向边。每个节点还包含音频信息，但此处未显示。*'
- en: How long the temporal context is?
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间上下文的长度是多少？
- en: Refer to figure 4 below that shows the temporal context achieved by our methods
    on two different applications.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅下方的图 4，展示了我们的方法在两个不同应用中的时间上下文。
- en: The hyper-parameter τ (= 0.9 second in our experiments) in SPELL imposes additional
    constraints on direct connectivity across temporally distant nodes. The face identities
    across consecutive timestamps are always connected. Below is the estimate of the
    effective temporal context size of SPELL. The AVA-ActiveSpeaker dataset contains
    3.65 million frames and 5.3 million annotated faces, resulting in 1.45 faces per
    frame. Averaging 1.45 faces per frame, a graph with 500 to 2000 faces in sorted
    temporal order can span 345 to 1379 frames, corresponding to anywhere between
    13 and 55 seconds for a 25 frame/second video. In other words, the nodes in the
    graph might have a time difference of about 1 minute, and SPELL is able to effectively
    reason over that long-term temporal window within a limited memory and compute
    budget. It is noteworthy that the temporal window size in [MAAS](https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf)
    is 1.9 seconds and TalkNet uses up to 4 seconds as long-term sequence-level temporal
    context.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: SPELL中的超参数τ（在我们的实验中为0.9秒）对跨时间距离较远的节点之间的直接连接施加了额外的约束。连续时间戳之间的面孔身份始终是相互连接的。以下是SPELL的有效时间上下文大小估算。AVA-ActiveSpeaker数据集包含365万帧和530万标注面孔，导致每帧1.45个面孔。平均每帧1.45个面孔，一个按时间顺序排序的包含500到2000个面孔的图谱可以跨越345到1379帧，相当于25帧/秒视频中的13到55秒。换句话说，图中的节点可能存在大约1分钟的时间差，而SPELL能够在有限的内存和计算预算下有效地推理这一长期时间窗口。值得注意的是，[MAAS](https://openaccess.thecvf.com/content/ICCV2021/papers/Alcazar_MAAS_Multi-Modal_Assignation_for_Active_Speaker_Detection_ICCV_2021_paper.pdf)中的时间窗口大小为1.9秒，TalkNet使用最多4秒的长期序列级时间上下文。
- en: The work on spatio-temporal graphs for active speaker detection has been published
    at ECCV 2022\. The manuscript can be found [here](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22)
    . In an earlier [blog](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Spatio-Temporal-Graphs-for-Long-Term-Video-Understanding/post/1425258#.Y1oG7jhUOBs.linkedin)
    we provided more details.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于用于活跃说话者检测的时空图的工作已在 ECCV 2022 上发表。手稿可以在[这里](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22)找到。我们在之前的[博客](https://community.intel.com/t5/Blogs/Tech-Innovation/Artificial-Intelligence-AI/Spatio-Temporal-Graphs-for-Long-Term-Video-Understanding/post/1425258#.Y1oG7jhUOBs.linkedin)中提供了更多细节。
- en: '![](../Images/951c6765f91d02fe1eb515f2fc3b7dc2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/951c6765f91d02fe1eb515f2fc3b7dc2.png)'
- en: '*(image by author) Figure 4: Left and right figure demonstrate the compartive
    time-support of our method compared to others for Active Speaker Detection and
    Action Detection applications, respectively.*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*(作者提供的图像) 图 4：左图和右图分别展示了我们方法在活跃说话者检测和动作检测应用中的时间支持对比。*'
- en: 'Action Detection : Task formulated as node classification'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作检测：任务形式为节点分类
- en: The ASD problem setup in Ava active speaker dataset has access to the labeled
    faces and labeled face tracks as input to the problem setup. That largely simplifies
    the construction of the graph in terms of identifying the nodes and edges. For
    other problems, such as Action Detection, where the ground truth object (person)
    locations and tracks are not provided, we use pre-processing to detect objects
    and object tracks, then utilize SPELL for the node classification problem. Similar
    to the previous case, we utilize domain knowledge and construct a sparse graph.
    The “object-centric” graphs are first created keeping the underlying application
    in mind.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Ava 活跃说话者数据集中的 ASD 问题设置可以访问标注的面孔和标注的面部轨迹，作为问题设置的输入。这在很大程度上简化了图的构建，特别是在节点和边的识别方面。对于其他问题，例如动作检测（Action
    Detection），由于没有提供地面真相的物体（人）位置和轨迹，我们使用预处理来检测物体和物体轨迹，然后利用 SPELL 来进行节点分类问题。类似于之前的情况，我们利用领域知识构建稀疏图。首先构建“面向物体”的图，始终保持底层应用的目标。
- en: On an average, we achieve ~90% sparse graphs; a key difference compared to visual
    transformer-based methods which rely on dense General Matrix Multiply (GEMM) operations.
    Our sparse GNNs allow us to (1) achieve better performance than transformer-based
    models; (2) aggregate temporal context over 10x longer windows compared to transformer-based
    models (100 seconds vs 10 seconds); and (3) Achieve 2–5X compute savings compared
    to transformers-based methods.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，我们实现了约 90% 的稀疏图；这一点与依赖于密集通用矩阵乘法（GEMM）操作的视觉 transformer 方法有显著区别。我们的稀疏 GNN
    使我们能够 (1) 实现比 transformer 方法更好的性能；(2) 在与 transformer 方法相比的 10 倍更长的时间窗口（100 秒 vs
    10 秒）中聚合时间上下文；以及 (3) 相比于 transformer 方法节省 2 到 5 倍的计算资源。
- en: 'GraVi-T: Open Source Software Library'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraVi-T：开源软件库
- en: We have open-sourced our software library, GraVi-T. At present, GraVi-T supports
    multiple video understanding applications, including Active Speaker Detection,
    Action Detection, Temporal Segmentation, Video Summarization. See our opensource
    software library [GraVi-T](https://github.com/IntelLabs/GraVi-T) to more on the
    applications.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开源了我们的软件库 GraVi-T。目前，GraVi-T 支持多种视频理解应用，包括活跃说话者检测、动作检测、时间分割、视频摘要等。请查看我们的开源软件库[GraVi-T](https://github.com/IntelLabs/GraVi-T)了解更多应用。
- en: Highlights
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亮点
- en: Compared to transformers, our graph approach can aggregate context over 10x
    longer video, consumes ~10x lower memory and 5x lower FLOPs. Our first and major
    work in this topic (Active Speaker Detection) was published at [ECCV’22](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22).
    Watch out for our latest publication at upcoming CVPR 2024 on video summarization
    aka video highlights reels creation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与 transformers 相比，我们的图方法可以在 10 倍更长的视频上聚合上下文，消耗约 10 倍更少的内存和 5 倍更低的 FLOPs。我们在这个主题（活跃说话者检测）上的首个主要工作已发表于
    [ECCV’22](https://link.springer.com/chapter/10.1007/978-3-031-19833-5_22)。请关注我们即将在即将召开的
    CVPR 2024 上发布的关于视频摘要（即视频高亮集锦创建）的最新成果。
- en: Our approach of modeling video as a sparse graph outperformed complex SOTA methods
    on several applications. It secured top places in multiple leaderboards. The list
    includes ActivityNet 2022, Ego4D audio-video diarization challenges at ECCV 2022,
    CVPR 2023\. [Source code](https://intelailabpage.github.io/2023/03/29/gravi-t.html)
    for the training the past challenge winning models are also included in our open-sourced
    software library, [GraVi-T](https://intelailabpage.github.io/2023/03/29/gravi-t.html).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将视频建模为稀疏图的方法在多个应用中超越了复杂的SOTA方法，并在多个排行榜中名列前茅。包括2022年ActivityNet，2022年ECCV的Ego4D音频-视频分割挑战赛，2023年CVPR等赛事。用于训练过去挑战赛获胜模型的[源代码](https://intelailabpage.github.io/2023/03/29/gravi-t.html)也包含在我们的开源软件库[GraVi-T](https://intelailabpage.github.io/2023/03/29/gravi-t.html)中。
- en: We are excited about this generic, lightweight and efficient framework and are
    working towards other new applications. More exciting news coming soon !!!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这个通用、轻量级且高效的框架充满期待，并正在朝着其他新应用方向努力。更多令人兴奋的消息即将发布！！！
