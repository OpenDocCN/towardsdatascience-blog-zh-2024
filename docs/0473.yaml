- en: 'How OpenAIâ€™s Sora is Changing the Game: An Insight into Its Core Technologies'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenAIçš„Soraå¦‚ä½•æ”¹å˜æ¸¸æˆè§„åˆ™ï¼šæ·±å…¥äº†è§£å…¶æ ¸å¿ƒæŠ€æœ¯
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19](https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19](https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19)
- en: A masterpiece of state of the art technologies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸€é¡¹ä»£è¡¨å‰æ²¿æŠ€æœ¯çš„æ°ä½œ
- en: '[](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Ryota
    Kiuchi, Ph.D.](../Images/5459c434848898345d932320c4a01312.png)](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    [Ryota Kiuchi, Ph.D.](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Ryota
    Kiuchi, Ph.D.](../Images/5459c434848898345d932320c4a01312.png)](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    [Ryota Kiuchi, Ph.D.](https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    Â·12 min readÂ·Feb 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2024å¹´2æœˆ19æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4045008b97b49145c5838da473557a91.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4045008b97b49145c5838da473557a91.png)'
- en: Photo by [Kaushik Panchal](https://unsplash.com/@kaushikpanchal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Kaushik Panchal](https://unsplash.com/@kaushikpanchal?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: On February 15, 2024, OpenAI, which had astonished the world by announcing ChatGPT
    in late 2022, once again stunned the world with the unveiling of Sora. This technology,
    capable of creating videos up to a minute long from a text prompt, is undeniably
    set to be a breakthrough.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2024å¹´2æœˆ15æ—¥ï¼ŒOpenAIåœ¨2022å¹´åº•éœ‡æƒŠä¸–ç•Œå‘å¸ƒChatGPTä¹‹åï¼Œå†æ¬¡é€šè¿‡æ­ç¤ºSoraéœ‡æ’¼å…¨çƒã€‚è¿™é¡¹æŠ€æœ¯èƒ½å¤Ÿæ ¹æ®æ–‡æœ¬æç¤ºç”Ÿæˆæœ€é•¿è¾¾ä¸€åˆ†é’Ÿçš„è§†é¢‘ï¼Œæ¯«æ— ç–‘é—®ï¼Œå®ƒå°†æˆä¸ºä¸€æ¬¡çªç ´ã€‚
- en: In this blog post, I will introduce the underlying methodologies and research
    behind this astonishing technology, based on the technical report released by
    OpenAI.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘å°†åŸºäºOpenAIå‘å¸ƒçš„æŠ€æœ¯æŠ¥å‘Šï¼Œä»‹ç»è¿™é¡¹ä»¤äººæƒŠå¹çš„æŠ€æœ¯èƒŒåçš„åŸºç¡€æ–¹æ³•å’Œç ”ç©¶ã€‚
- en: Incidentally, â€œSoraâ€ means â€œskyâ€ in Japanese. Although it has not been officially
    announced whether this naming was intentional, it is speculated to be so, given
    that their official release tweet featured a video themed around Tokyo.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿æä¸€ä¸‹ï¼Œâ€œSoraâ€åœ¨æ—¥è¯­ä¸­æ„å‘³ç€â€œå¤©ç©ºâ€ã€‚å°½ç®¡å®˜æ–¹å°šæœªå®£å¸ƒè¿™ä¸€å‘½åæ˜¯å¦æœ‰æ„ä¸ºä¹‹ï¼Œä½†æ®æ¨æµ‹å¯èƒ½æ˜¯æœ‰æ„çš„ï¼Œå› ä¸ºä»–ä»¬çš„å®˜æ–¹å‘å¸ƒæ¨æ–‡ä¸­æœ‰ä¸€æ®µä»¥ä¸œäº¬ä¸ºä¸»é¢˜çš„è§†é¢‘ã€‚
- en: OpenAI unveils the Sora to the world via X
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIé€šè¿‡Xå¹³å°å‘å…¨çƒå‘å¸ƒSora
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®å½•
- en: '[About Sora](#bfb1)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å…³äºSora](#bfb1)'
- en: '[What kind of technology and research is behind it?](#7e7b)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å®ƒèƒŒåæ˜¯ä»€ä¹ˆæ ·çš„æŠ€æœ¯å’Œç ”ç©¶ï¼Ÿ](#7e7b)'
- en: '[The capabilities enabled by these research efforts for Sora](#7b12)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¿™äº›ç ”ç©¶åŠªåŠ›ä¸ºSoraå¸¦æ¥çš„èƒ½åŠ›](#7b12)'
- en: '[The future of Sora](#642d)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Soraçš„æœªæ¥](#642d)'
- en: About Sora
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºSora
- en: Sora is an advanced text-to-video conversion model developed by OpenAI, and
    its capabilities and application range illustrate a new horizon in modern AI technology.
    This model is not limited to generating mere seconds of video; it can create videos
    up to one minute long, maintaining high visual quality while faithfully reproducing
    user instructions. Itâ€™s as if itâ€™s bringing dreams to life.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Soraæ˜¯OpenAIå¼€å‘çš„å…ˆè¿›æ–‡æœ¬åˆ°è§†é¢‘è½¬æ¢æ¨¡å‹ï¼Œå…¶èƒ½åŠ›å’Œåº”ç”¨èŒƒå›´å±•ç¤ºäº†ç°ä»£AIæŠ€æœ¯çš„æ–°è§†é‡ã€‚è¿™ä¸ªæ¨¡å‹ä¸ä»…é™äºç”Ÿæˆå‡ ç§’é’Ÿçš„è§†é¢‘ï¼›å®ƒå¯ä»¥åˆ›å»ºæœ€é•¿ä¸€åˆ†é’Ÿçš„è§†é¢‘ï¼Œåœ¨ä¿æŒé«˜è§†è§‰è´¨é‡çš„åŒæ—¶ï¼Œå¿ å®åœ°å†ç°ç”¨æˆ·æŒ‡ä»¤ã€‚å°±åƒæ˜¯å°†æ¢¦æƒ³å˜ä¸ºç°å®ã€‚
- en: OpenAI Soraâ€™s demo via X
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Soraé€šè¿‡Xå¹³å°è¿›è¡Œæ¼”ç¤º
- en: '**Generating Complex Scenes Based on the Real World**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**åŸºäºç°å®ä¸–ç•Œç”Ÿæˆå¤æ‚åœºæ™¯**'
- en: Sora understands how elements described in prompts exist and operate within
    the physical world. This allows the model to accurately represent user-intended
    movements and actions within videos. For example, it can realistically recreate
    the sight of a person running or the movement of natural phenomena. Furthermore,
    it reproduces precise details of multiple characters, types of movement, and the
    specifics of subjects and backgrounds.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Sora ç†è§£æç¤ºä¸­æè¿°çš„å…ƒç´ å¦‚ä½•åœ¨ç‰©ç†ä¸–ç•Œä¸­å­˜åœ¨å’Œè¿ä½œã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°å‘ˆç°ç”¨æˆ·æ„å›¾çš„åŠ¨ä½œå’Œè§†é¢‘ä¸­çš„è¿åŠ¨ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥é€¼çœŸåœ°é‡ç°ä¸€ä¸ªäººå¥”è·‘çš„åœºæ™¯æˆ–è‡ªç„¶ç°è±¡çš„è¿åŠ¨ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜èƒ½å¤Ÿå†ç°å¤šä¸ªè§’è‰²ã€å„ç§è¿åŠ¨ç±»å‹ä»¥åŠä¸»é¢˜å’ŒèƒŒæ™¯çš„ç²¾ç¡®ç»†èŠ‚ã€‚
- en: Previously, video creation with Generative AI has faced the difficult challenge
    of maintaining consistency and reproducibility across different scenes. This is
    because understanding previous contexts and details completely when generating
    each scene or frame individually and appropriately inheriting them to the next
    scene is challenging. However, this model maintains narrative consistency by combining
    a deep understanding of language with visual context and interpreting prompts
    accurately. It can also capture the emotions and personalities of characters from
    the given prompts and portray them as expressive characters within the video.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥å‰ï¼Œä½¿ç”¨ç”Ÿæˆæ€§ AI åˆ›å»ºè§†é¢‘é¢ä¸´ç€åœ¨ä¸åŒåœºæ™¯ä¹‹é—´ä¿æŒä¸€è‡´æ€§å’Œå¯é‡ç°æ€§çš„å·¨å¤§æŒ‘æˆ˜ã€‚è¿™æ˜¯å› ä¸ºåœ¨ç”Ÿæˆæ¯ä¸ªåœºæ™¯æˆ–å¸§æ—¶ï¼Œå®Œå…¨ç†è§£å…ˆå‰çš„ä¸Šä¸‹æ–‡å’Œç»†èŠ‚ï¼Œå¹¶å°†å…¶é€‚å½“ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªåœºæ™¯æ˜¯å›°éš¾çš„ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ¨¡å‹é€šè¿‡å°†å¯¹è¯­è¨€çš„æ·±åˆ»ç†è§£ä¸è§†è§‰ä¸Šä¸‹æ–‡ç»“åˆï¼Œå‡†ç¡®è§£é‡Šæç¤ºï¼Œä»è€Œä¿æŒå™äº‹ä¸€è‡´æ€§ã€‚å®ƒè¿˜èƒ½å¤Ÿæ•æ‰è§’è‰²çš„æƒ…æ„Ÿå’Œä¸ªæ€§ï¼Œå¹¶åœ¨è§†é¢‘ä¸­å‘ˆç°ä¸ºå¯Œæœ‰è¡¨ç°åŠ›çš„è§’è‰²ã€‚
- en: The post by Bill Peebles (OpenAI) via X
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Bill Peeblesï¼ˆOpenAIï¼‰é€šè¿‡ X å‘å¸ƒçš„å¸–å­
- en: What kind of technology and research is behind it?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ƒèƒŒåæ˜¯ä»€ä¹ˆæ ·çš„æŠ€æœ¯å’Œç ”ç©¶ï¼Ÿ
- en: '![](../Images/5855f8b4ebf6f0eabb6f06a22673761c.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5855f8b4ebf6f0eabb6f06a22673761c.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±[Markus Spiske](https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œæ¥è‡ª[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Sora is built upon a foundation of prior studies in image data generation modeling.
    Previous research has employed various methods such as recurrent networks, Generative
    Adversarial Networks (GANs), autoregressive transformers, and diffusion models,
    but has often focused on a narrow category of visual data, shorter videos, or
    videos of a fixed size. Sora surpasses these limitations and has been significantly
    improved to generate videos across diverse durations, aspect ratios, and resolutions.
    In this section, I will introduce the core technologies that support these advancements.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Sora æ˜¯å»ºç«‹åœ¨å…ˆå‰å›¾åƒæ•°æ®ç”Ÿæˆå»ºæ¨¡ç ”ç©¶çš„åŸºç¡€ä¹‹ä¸Šçš„ã€‚å…ˆå‰çš„ç ”ç©¶ä½¿ç”¨äº†å„ç§æ–¹æ³•ï¼Œå¦‚é€’å½’ç½‘ç»œã€ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ã€è‡ªå›å½’ Transformer
    å’Œæ‰©æ•£æ¨¡å‹ï¼Œä½†é€šå¸¸é›†ä¸­äºç‹­çª„ç±»åˆ«çš„è§†è§‰æ•°æ®ã€è¾ƒçŸ­çš„è§†é¢‘æˆ–å›ºå®šå°ºå¯¸çš„è§†é¢‘ã€‚Sora è¶…è¶Šäº†è¿™äº›å±€é™ï¼Œå¹¶åœ¨ç”Ÿæˆå„ç§æ—¶é•¿ã€çºµæ¨ªæ¯”å’Œåˆ†è¾¨ç‡çš„è§†é¢‘æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›å±•ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘å°†ä»‹ç»æ”¯æŒè¿™äº›è¿›å±•çš„æ ¸å¿ƒæŠ€æœ¯ã€‚
- en: 1\. Transformer
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. Transformer
- en: Vaswani et al. (2017), â€œAttention is all you need.â€
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Vaswani ç­‰äººï¼ˆ2017ï¼‰ï¼Œã€ŠAttention is all you needã€‹
- en: The Transformer model is a neural network architecture that revolutionized the
    field of natural language processing (NLP). It was first proposed by Vaswani et
    al. in 2017\. This model significantly overcame the challenges that traditional
    Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) faced,
    supporting various breakthrough technologies as an innovative method today.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer æ¨¡å‹æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œå½»åº•æ”¹å˜äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸã€‚å®ƒç”± Vaswani ç­‰äººäº 2017 å¹´é¦–æ¬¡æå‡ºã€‚è¯¥æ¨¡å‹æ˜¾è‘—å…‹æœäº†ä¼ ç»Ÿé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNsï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä½œä¸ºä¸€ç§åˆ›æ–°æ–¹æ³•ï¼Œä»Šå¤©æ”¯æŒäº†å¤šç§çªç ´æ€§çš„æŠ€æœ¯ã€‚
- en: '![](../Images/2295b7bce37a22b9414d8777b53f8951.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2295b7bce37a22b9414d8777b53f8951.png)'
- en: 'Figure 1: The Transformer â€” model architecture. ï½œVaswani et al. (2017)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šTransformerâ€”â€”æ¨¡å‹æ¶æ„ã€‚ï½œVaswani ç­‰äººï¼ˆ2017ï¼‰
- en: 'Issues with RNNs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: RNNsçš„é—®é¢˜ï¼š
- en: 'The problem of long-term dependencies: Although RNNs theoretically can transmit
    information through time, they struggle to capture dependencies over long durations
    in practice.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é•¿æœŸä¾èµ–é—®é¢˜ï¼šå°½ç®¡ RNNs ç†è®ºä¸Šå¯ä»¥é€šè¿‡æ—¶é—´ä¼ é€’ä¿¡æ¯ï¼Œä½†åœ¨å®è·µä¸­å®ƒä»¬éš¾ä»¥æ•æ‰é•¿æ—¶é—´è·¨åº¦çš„ä¾èµ–å…³ç³»ã€‚
- en: 'Limitations on parallelization: Since the computation at each step in an RNN
    depends on the output of the previous step, sequential processing (e.g., processing
    words or sentences in a text one by one, in order) is mandatory, preventing the
    utilization of parallel processing advantages offered by modern computer architectures.
    This made training on large datasets inefficient.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–çš„å±€é™æ€§ï¼šç”±äºRNNä¸­æ¯ä¸€æ­¥çš„è®¡ç®—ä¾èµ–äºå‰ä¸€æ­¥çš„è¾“å‡ºï¼Œå› æ­¤å¿…é¡»è¿›è¡Œé¡ºåºå¤„ç†ï¼ˆä¾‹å¦‚ï¼Œé€ä¸€å¤„ç†æ–‡æœ¬ä¸­çš„å•è¯æˆ–å¥å­ï¼‰ï¼Œæ— æ³•åˆ©ç”¨ç°ä»£è®¡ç®—æœºæ¶æ„æ‰€æä¾›çš„å¹¶è¡Œå¤„ç†ä¼˜åŠ¿ã€‚è¿™ä½¿å¾—åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è®­ç»ƒæ•ˆç‡ä½ä¸‹ã€‚
- en: 'Issues with CNNs:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: CNNçš„é—®é¢˜ï¼š
- en: 'Fixed receptive field size: While CNNs excel at extracting local features,
    their fixed receptive field size limits their ability to capture long-distance
    dependencies throughout the context.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›ºå®šæ„Ÿå—é‡å¤§å°ï¼šè™½ç„¶CNNåœ¨æå–å±€éƒ¨ç‰¹å¾æ–¹é¢è¡¨ç°ä¼˜ç§€ï¼Œä½†å…¶å›ºå®šçš„æ„Ÿå—é‡å¤§å°é™åˆ¶äº†å…¶æ•æ‰å…¨å±€ä¸Šä¸‹æ–‡ä¸­é•¿è·ç¦»ä¾èµ–çš„èƒ½åŠ›ã€‚
- en: 'Difficulty in modeling the hierarchical structure of natural language: Itâ€™s
    challenging to directly model the hierarchical structure of language, which can
    be insufficient for deep contextual understanding.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è‡ªç„¶è¯­è¨€å±‚çº§ç»“æ„å»ºæ¨¡çš„å›°éš¾ï¼šç›´æ¥å»ºæ¨¡è¯­è¨€çš„å±‚çº§ç»“æ„å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè€Œè¿™å¯¹äºæ·±å±‚æ¬¡çš„ä¸Šä¸‹æ–‡ç†è§£å¯èƒ½ä¸å¤Ÿå……åˆ†ã€‚
- en: 'New features of the Transformer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Transformerçš„æ–°ç‰¹æ€§ï¼š
- en: 'Attention Mechanism: Enables direct modeling of dependencies between any positions
    in the sequence, allowing for the direct capture of long dependencies and extensive
    context.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æœºåˆ¶ï¼šä½¿å¾—èƒ½å¤Ÿç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œä»è€Œç›´æ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å’Œå¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚
- en: 'Realization of parallelization: Since the input data is processed as a whole
    at once, a high degree of parallelization in computation is achieved, significantly
    accelerating training on large datasets.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹¶è¡ŒåŒ–çš„å®ç°ï¼šç”±äºè¾“å…¥æ•°æ®ä¸€æ¬¡æ€§æ•´ä½“å¤„ç†ï¼Œå®ç°äº†é«˜åº¦çš„å¹¶è¡Œè®¡ç®—ï¼Œå¤§å¤§åŠ é€Ÿäº†åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šçš„è®­ç»ƒã€‚
- en: 'Variable receptive field: The attention mechanism allows the model to dynamically
    adjust the â€œreceptive fieldâ€ size as needed. This means the model can naturally
    focus on local information for certain tasks or data, and consider broader context
    in other cases.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯å˜æ„Ÿå—é‡ï¼šæ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ ¹æ®éœ€è¦åŠ¨æ€è°ƒæ•´â€œæ„Ÿå—é‡â€çš„å¤§å°ã€‚è¿™æ„å‘³ç€æ¨¡å‹å¯ä»¥åœ¨æŸäº›ä»»åŠ¡æˆ–æ•°æ®ä¸­è‡ªç„¶åœ°èšç„¦äºå±€éƒ¨ä¿¡æ¯ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹è€ƒè™‘æ›´å¹¿æ³›çš„ä¸Šä¸‹æ–‡ã€‚
- en: '*For more detailed technical explanations about Transformer:*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ›´å¤šå…³äºTransformerçš„è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: '[](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
    [## How Transformers Work'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
    [## Transformerçš„å·¥ä½œåŸç†'
- en: Transformers are a type of neural network architecture that have been gaining
    popularity. Transformers were recentlyâ€¦
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Transformeræ˜¯ä¸€ç§ç¥ç»ç½‘ç»œæ¶æ„ï¼Œè¿‘å¹´æ¥è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ã€‚Transformeræœ€è¿‘â€¦
- en: towardsdatascience.com](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------)
- en: 2\. Vision Transformer (ViT)
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. è§†è§‰Transformerï¼ˆViTï¼‰
- en: 'Dosovitskiy, et al. (2020), â€œAn image is worth 16x16 words: Transformers for
    image recognition at scale.â€'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Dosovitskiyç­‰äººï¼ˆ2020å¹´ï¼‰ï¼Œâ€œä¸€å¼ å›¾ç­‰äº16x16ä¸ªè¯ï¼šç”¨äºå¤§è§„æ¨¡å›¾åƒè¯†åˆ«çš„Transformerã€‚â€
- en: In this study, the principles of the Transformer, which revolutionized natural
    language processing (NLP), are applied to image recognition, opening up new horizons.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶å°†é©å‘½æ€§åœ°æ”¹å˜è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„TransformeråŸç†åº”ç”¨äºå›¾åƒè¯†åˆ«ï¼Œå¼€è¾Ÿäº†æ–°çš„è§†é‡ã€‚
- en: '**Token and Patch**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tokenå’ŒPatch**'
- en: In the original Transformer paper, tokens primarily represent parts of words
    or sentences, and analyzing the relationships between these tokens allows for
    a deep understanding of the sentenceâ€™s meaning. In this study, to apply this concept
    of tokens to visual data, images are divided into small sections (patches) of
    16x16, and each patch is treated as a â€œtokenâ€ within the Transformer. This approach
    enables the model to learn how each patch is related within the entire image,
    allowing for the recognition and understanding of the entire image based on this.
    It surpasses the limitations of the fixed receptive field size of traditional
    CNN models used in image recognition, enabling flexible capture of any positional
    relationships within an image.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹çš„Transformerè®ºæ–‡ä¸­ï¼Œtokensä¸»è¦ä»£è¡¨å•è¯æˆ–å¥å­çš„éƒ¨åˆ†ï¼Œåˆ†æè¿™äº›tokensä¹‹é—´çš„å…³ç³»å¯ä»¥æ·±å…¥ç†è§£å¥å­çš„å«ä¹‰ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œä¸ºäº†å°†è¿™ç§tokençš„æ¦‚å¿µåº”ç”¨äºè§†è§‰æ•°æ®ï¼Œå›¾åƒè¢«åˆ’åˆ†ä¸º16x16çš„å°å—ï¼ˆpatchesï¼‰ï¼Œæ¯ä¸ªå°å—è¢«è§†ä¸ºTransformerä¸­çš„ä¸€ä¸ªâ€œtokenâ€ã€‚è¿™ç§æ–¹æ³•ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ¯ä¸ªå°å—åœ¨æ•´ä¸ªå›¾åƒä¸­çš„å…³ç³»ï¼Œä»è€ŒåŸºäºæ­¤è¿›è¡Œæ•´ä¸ªå›¾åƒçš„è¯†åˆ«å’Œç†è§£ã€‚å®ƒçªç ´äº†ä¼ ç»ŸCNNæ¨¡å‹åœ¨å›¾åƒè¯†åˆ«ä¸­å›ºå®šæ„Ÿå—é‡å¤§å°çš„å±€é™æ€§ï¼Œå®ç°äº†åœ¨å›¾åƒå†…ä»»æ„ä½ç½®å…³ç³»çš„çµæ´»æ•æ‰ã€‚
- en: '![](../Images/4b9ab7bd066f9cd9c49e6815c9b9752f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b9ab7bd066f9cd9c49e6815c9b9752f.png)'
- en: 'Figure 1: Model overview. ï½œDosovitskiy, et al. (2020)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šæ¨¡å‹æ¦‚è¿°ã€‚ï½œDosovitskiy ç­‰äººï¼ˆ2020ï¼‰
- en: '*For more detailed technical explanations about ViT:*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºViTçš„æ›´è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: '[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://machinelearningmastery.com/the-vision-transformer-model/](https://machinelearningmastery.com/the-vision-transformer-model/)'
- en: 3\. Video Vision Transformer (ViViT)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. è§†é¢‘è§†è§‰å˜æ¢å™¨ï¼ˆViViTï¼‰
- en: 'Arnab, et al. (2021), â€œVivit: A video vision transformer.â€'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Arnab ç­‰äººï¼ˆ2021ï¼‰ï¼Œâ€œVivit: A video vision transformer.â€'
- en: ViViT further extends the concept of the Vision Transformer, applying it to
    the multidimensional data of videos. Video data is more complex as it contains
    both static image information (spatial elements) and dynamic information that
    changes over time (temporal elements). ViViT decomposes videos into spatiotemporal
    patches, treating these as tokens within the Transformer model. With the introduction
    of spatiotemporal patches, ViViT is able to simultaneously capture both static
    and dynamic elements within a video and model the complex relationships between
    them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ViViTè¿›ä¸€æ­¥æ‰©å±•äº†è§†è§‰å˜æ¢å™¨çš„æ¦‚å¿µï¼Œå°†å…¶åº”ç”¨äºè§†é¢‘çš„å¤šç»´æ•°æ®ã€‚è§†é¢‘æ•°æ®æ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºå®ƒæ—¢åŒ…å«é™æ€å›¾åƒä¿¡æ¯ï¼ˆç©ºé—´å…ƒç´ ï¼‰ï¼ŒåˆåŒ…å«éšæ—¶é—´å˜åŒ–çš„åŠ¨æ€ä¿¡æ¯ï¼ˆæ—¶é—´å…ƒç´ ï¼‰ã€‚ViViTå°†è§†é¢‘åˆ†è§£ä¸ºæ—¶ç©ºè¡¥ä¸ï¼Œå¹¶å°†å…¶è§†ä¸ºå˜æ¢å™¨æ¨¡å‹ä¸­çš„ä»¤ç‰Œã€‚é€šè¿‡å¼•å…¥æ—¶ç©ºè¡¥ä¸ï¼ŒViViTèƒ½å¤ŸåŒæ—¶æ•æ‰è§†é¢‘ä¸­çš„é™æ€å’ŒåŠ¨æ€å…ƒç´ ï¼Œå¹¶å»ºæ¨¡å®ƒä»¬ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚
- en: '![](../Images/24cf64ec81ec07779af159c87eae68d6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24cf64ec81ec07779af159c87eae68d6.png)'
- en: 'Figure 3: Tubelet (the spatio-temporal input volume) embedding image. ï½œArnab,
    et al. (2021)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šTubeletï¼ˆæ—¶ç©ºè¾“å…¥ä½“ç§¯ï¼‰åµŒå…¥å›¾åƒã€‚ï½œArnab ç­‰äººï¼ˆ2021ï¼‰
- en: '*For more detailed technical explanations about ViViT:*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºViViTçš„æ›´è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: '[](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
    [## ViViT ğŸ“¹ Video Vision Transformer'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
    [## ViViT ğŸ“¹ è§†é¢‘è§†è§‰å˜æ¢å™¨'
- en: ICCV 2021 âœ¨, Google Research
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ICCV 2021 âœ¨ï¼Œè°·æ­Œç ”ç©¶
- en: medium.com](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------)
- en: 4\. Masked Autoencoders (MAE)
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. Masked Autoencoders (MAE)
- en: He, et al. (2022), â€œMasked autoencoders are scalable vision learners.â€
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: He ç­‰äººï¼ˆ2022ï¼‰ï¼Œâ€œMasked autoencoders are scalable vision learners.â€
- en: This study dramatically improved the traditionally high computational costs
    and inefficiencies in training on large datasets associated with high dimensionality
    and vast amounts of information, using a self-supervised pre-training method called
    the Masked Autoencoder.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶é€šè¿‡ä½¿ç”¨ä¸€ç§è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•ï¼Œç§°ä¸ºMasked Autoencoderï¼Œæ˜¾è‘—æé«˜äº†ä¼ ç»Ÿä¸Šåœ¨é«˜ç»´åº¦å’Œå¤§é‡ä¿¡æ¯æ•°æ®é›†ä¸Šè®­ç»ƒæ—¶çš„é«˜è®¡ç®—æˆæœ¬å’Œä½æ•ˆç‡ã€‚
- en: Specifically, by masking parts of the input image, the network is trained to
    predict the information of the hidden parts, resulting in more efficient learning
    of important features and structures within the image, and acquiring rich representations
    of visual data. This process has made the compression and representation learning
    of data more efficient, reduced computational costs, and enhanced the versatility
    of different types of visual data and tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œé€šè¿‡å¯¹è¾“å…¥å›¾åƒçš„éƒ¨åˆ†åŒºåŸŸè¿›è¡Œé®æŒ¡ï¼Œç½‘ç»œè¢«è®­ç»ƒå»é¢„æµ‹éšè—éƒ¨åˆ†çš„ä¿¡æ¯ï¼Œä»è€Œæ›´æœ‰æ•ˆåœ°å­¦ä¹ å›¾åƒä¸­çš„é‡è¦ç‰¹å¾å’Œç»“æ„ï¼Œå¹¶è·å¾—ä¸°å¯Œçš„è§†è§‰æ•°æ®è¡¨ç¤ºã€‚è¿™ä¸€è¿‡ç¨‹ä½¿å¾—æ•°æ®çš„å‹ç¼©ä¸è¡¨ç¤ºå­¦ä¹ å˜å¾—æ›´åŠ é«˜æ•ˆï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶å¢å¼ºäº†ä¸åŒç±»å‹è§†è§‰æ•°æ®å’Œä»»åŠ¡çš„é€šç”¨æ€§ã€‚
- en: The approach of this study is also closely related to the evolution of language
    models by BERT (Bidirectional Encoder Representations from Transformers). While
    BERT enabled a deep contextual understanding of text data through Masked Language
    Modeling (MLM), He et al. have applied a similar masking technique to visual data,
    achieving a deeper understanding and representation of images.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶çš„æ–¹æ³•ä¸BERTï¼ˆåŒå‘ç¼–ç å™¨è¡¨ç¤ºçš„å˜æ¢å™¨ï¼‰åœ¨è¯­è¨€æ¨¡å‹å‘å±•ä¸­çš„åº”ç”¨ç´§å¯†ç›¸å…³ã€‚BERTé€šè¿‡Masked Language Modelingï¼ˆMLMï¼‰å®ç°äº†å¯¹æ–‡æœ¬æ•°æ®çš„æ·±å±‚ä¸Šä¸‹æ–‡ç†è§£ï¼Œè€ŒHeç­‰äººå°†ç±»ä¼¼çš„é®æŒ¡æŠ€æœ¯åº”ç”¨äºè§†è§‰æ•°æ®ï¼Œä»è€Œå®ç°äº†å¯¹å›¾åƒçš„æ›´æ·±ç†è§£ä¸è¡¨ç¤ºã€‚
- en: '![](../Images/3933fb20cd83083872d34a3afdc4c2c3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3933fb20cd83083872d34a3afdc4c2c3.png)'
- en: 'Figure 1: Masked Autoencoders Image. ï½œHe, et al. (2022)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šMasked Autoencoders å›¾åƒã€‚ï½œHe ç­‰äººï¼ˆ2022ï¼‰
- en: '*For more detailed technical explanations about MAE:*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºMAEçš„æ›´è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: '[](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
    [## Paper explained: Masked Autoencoders Are Scalable Vision Learners'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
    [## è®ºæ–‡è§£æï¼šMasked Autoencoders æ˜¯å¯æ‰©å±•çš„è§†è§‰å­¦ä¹ è€…'
- en: How reconstructing masked parts of an image can be beneficial
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•é‡å»ºå›¾åƒä¸­çš„é®æŒ¡éƒ¨åˆ†èƒ½å¸¦æ¥å¥½å¤„
- en: towardsdatascience.com](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------)'
- en: 5\. Native Resolution Vision Transformer (NaViT)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. æœ¬åœ°åˆ†è¾¨ç‡è§†è§‰ Transformerï¼ˆNaViTï¼‰
- en: 'Dehghani, et al. (2023), â€œPatch nâ€™Pack: NaViT, a Vision Transformer for any
    Aspect Ratio and Resolution.â€'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Dehghani ç­‰äºº (2023)ï¼Œâ€œPatch nâ€™Pack: NaViTï¼Œä¸€ç§é€‚ç”¨äºä»»ä½•çºµæ¨ªæ¯”å’Œåˆ†è¾¨ç‡çš„è§†è§‰ Transformerã€‚â€'
- en: This study proposed the Native Resolution ViTransformer (NaViT), a model designed
    to further expand the applicability of the Vision Transformer (ViT) to images
    of any aspect ratio or resolution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç ”ç©¶æå‡ºäº†æœ¬åœ°åˆ†è¾¨ç‡ ViTransformerï¼ˆNaViTï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¿›ä¸€æ­¥æ‰©å±•è§†è§‰ Transformerï¼ˆViTï¼‰åº”ç”¨äºä»»ä½•çºµæ¨ªæ¯”æˆ–åˆ†è¾¨ç‡å›¾åƒçš„æ¨¡å‹ã€‚
- en: '**Challenges of Traditional ViT**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¼ ç»Ÿ ViT çš„æŒ‘æˆ˜**'
- en: The Vision Transformer introduced a groundbreaking approach by dividing images
    into fixed-size patches and treating these patches as tokens, applying the transformer
    model to image recognition tasks. However, this approach assumed models optimized
    for specific resolutions or aspect ratios, requiring model readjustment for images
    of different sizes or shapes. This was a significant constraint, as real-world
    applications often need to handle images of diverse sizes and aspect ratios.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰ Transformer é€šè¿‡å°†å›¾åƒåˆ†å‰²ä¸ºå›ºå®šå¤§å°çš„è¡¥ä¸å¹¶å°†è¿™äº›è¡¥ä¸ä½œä¸ºä»¤ç‰Œæ¥å¤„ç†ï¼Œä»è€Œæå‡ºäº†ä¸€ç§å¼€åˆ›æ€§çš„æ–¹æ³•ï¼Œå°† Transformer æ¨¡å‹åº”ç”¨äºå›¾åƒè¯†åˆ«ä»»åŠ¡ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å‡è®¾æ¨¡å‹æ˜¯é’ˆå¯¹ç‰¹å®šåˆ†è¾¨ç‡æˆ–çºµæ¨ªæ¯”è¿›è¡Œä¼˜åŒ–çš„ï¼Œå› æ­¤éœ€è¦å¯¹ä¸åŒå¤§å°æˆ–å½¢çŠ¶çš„å›¾åƒè¿›è¡Œæ¨¡å‹é‡æ–°è°ƒæ•´ã€‚è¿™æ˜¯ä¸€ä¸ªé‡å¤§é™åˆ¶ï¼Œå› ä¸ºå®é™…åº”ç”¨ä¸­é€šå¸¸éœ€è¦å¤„ç†ä¸åŒå¤§å°å’Œçºµæ¨ªæ¯”çš„å›¾åƒã€‚
- en: '**Innovations of NaViT**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**NaViT çš„åˆ›æ–°**'
- en: NaViT is designed to efficiently process images of any aspect ratio or resolution,
    allowing them to be directly inputted into the model without prior adjustment.
    Sora applies this flexibility to videos as well, significantly enhancing flexibility
    and adaptability by seamlessly handling videos and images of various sizes and
    shapes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: NaViT è®¾è®¡æ—¨åœ¨é«˜æ•ˆå¤„ç†ä»»ä½•çºµæ¨ªæ¯”æˆ–åˆ†è¾¨ç‡çš„å›¾åƒï¼Œä½¿å…¶èƒ½å¤Ÿç›´æ¥è¾“å…¥æ¨¡å‹è€Œæ— éœ€é¢„å…ˆè°ƒæ•´ã€‚Sora ä¹Ÿå°†è¿™ç§çµæ´»æ€§åº”ç”¨äºè§†é¢‘ï¼Œæ˜¾è‘—æå‡äº†çµæ´»æ€§å’Œé€‚åº”æ€§ï¼Œèƒ½å¤Ÿæ— ç¼å¤„ç†å„ç§å¤§å°å’Œå½¢çŠ¶çš„å›¾åƒä¸è§†é¢‘ã€‚
- en: '![](../Images/47627d8a0fd5e4cd491dd460ad0389f2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47627d8a0fd5e4cd491dd460ad0389f2.png)'
- en: Figure 2:ï½œDehghani, et al. (2023)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šï½œDehghani ç­‰äºº (2023)
- en: 6\. Diffusion Models
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6. æ‰©æ•£æ¨¡å‹
- en: Sohl-Dickstein, et al. (2015), â€œDeep unsupervised learning using nonequilibrium
    thermodynamics.â€
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sohl-Dickstein ç­‰äºº (2015)ï¼Œâ€œä½¿ç”¨éå¹³è¡¡çƒ­åŠ›å­¦çš„æ·±åº¦æ— ç›‘ç£å­¦ä¹ ã€‚â€
- en: Alongside the Transformer, Diffusion Models form the backbone technology supporting
    Sora. This research laid the theoretical foundation for diffusion models, a deep
    learning model using non-equilibrium thermodynamics. Diffusion models introduced
    the concept of a diffusion process that starts with random noise (data without
    any pattern) and gradually removes this noise to create data resembling actual
    images or videos.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº† Transformerï¼Œæ‰©æ•£æ¨¡å‹ä¹Ÿæ˜¯æ”¯æ’‘ Sora çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚æ­¤ç ”ç©¶ä¸ºæ‰©æ•£æ¨¡å‹å¥ å®šäº†ç†è®ºåŸºç¡€ï¼Œæ‰©æ•£æ¨¡å‹æ˜¯ä¸€ç§ä½¿ç”¨éå¹³è¡¡çƒ­åŠ›å­¦çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æ‰©æ•£æ¨¡å‹å¼•å…¥äº†ä¸€ä¸ªæ‰©æ•£è¿‡ç¨‹çš„æ¦‚å¿µï¼Œè¯¥è¿‡ç¨‹ä»éšæœºå™ªå£°ï¼ˆæ²¡æœ‰ä»»ä½•æ¨¡å¼çš„æ•°æ®ï¼‰å¼€å§‹ï¼Œé€æ­¥å»é™¤å™ªå£°ï¼Œç”Ÿæˆç±»ä¼¼äºå®é™…å›¾åƒæˆ–è§†é¢‘çš„æ•°æ®ã€‚
- en: For instance, imagine starting with mere random dots, which gradually transform
    into videos of beautiful landscapes or people. This approach was later applied
    to the generation of complex data such as images and sounds, contributing to the
    development of high-quality generative models.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¯ä»¥æƒ³è±¡ä»ä»…ä»…æ˜¯éšæœºçš„ç‚¹å¼€å§‹ï¼Œè¿™äº›ç‚¹é€æ¸è½¬å˜ä¸ºç¾ä¸½æ™¯è§‚æˆ–äººç‰©çš„è§†é¢‘ã€‚è¿™ç§æ–¹æ³•åæ¥è¢«åº”ç”¨äºç”Ÿæˆå¤æ‚çš„æ•°æ®ï¼Œå¦‚å›¾åƒå’Œå£°éŸ³ï¼Œä¿ƒè¿›äº†é«˜è´¨é‡ç”Ÿæˆæ¨¡å‹çš„å‘å±•ã€‚
- en: '![](../Images/5b45899f9277252955107f0d698a691c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b45899f9277252955107f0d698a691c.png)'
- en: Image of denoising processï½œImage Credit (OpenAI)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å»å™ªè¿‡ç¨‹çš„å›¾åƒï½œå›¾åƒæ¥æº (OpenAI)
- en: Ho et al. (2020), â€œDenoising diffusion probabilistic models.â€
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Ho ç­‰äºº (2020)ï¼Œâ€œå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚â€
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nichol and Dhariwal (2021), â€œImproved denoising diffusion probabilistic models.â€
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Nichol å’Œ Dhariwal (2021)ï¼Œâ€œæ”¹è¿›çš„å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚â€
- en: Building on the theoretical framework by Sohl-Dickstein et al. (2015), practical
    data generation models known as Denoising Diffusion Probabilistic Models (DDPM)
    were developed. This model has shown particularly notable results in high-quality
    image generation, demonstrating the effectiveness of diffusion models.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºSohl-Dickstein ç­‰äººï¼ˆ2015å¹´ï¼‰æå‡ºçš„ç†è®ºæ¡†æ¶ï¼Œå¼€å‘äº†è¢«ç§°ä¸ºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆDDPMï¼‰çš„å®ç”¨æ•°æ®ç”Ÿæˆæ¨¡å‹ã€‚è¯¥æ¨¡å‹åœ¨é«˜è´¨é‡å›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºäº†ç‰¹åˆ«æ˜¾è‘—çš„æ•ˆæœï¼Œè¯æ˜äº†æ‰©æ•£æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚
- en: '**Impact of Diffusion Models on Sora**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰©æ•£æ¨¡å‹å¯¹Soraçš„å½±å“**'
- en: Typically, to train machine learning models, a lot of labeled data is needed
    (for example, being told â€œThis is an image of a catâ€). However, diffusion models
    can learn from unlabeled data as well, allowing them to utilize the vast amount
    of visual content available on the internet to generate various types of videos.
    In other words, Sora can observe different videos and images and learn â€œthis is
    what a normal video looks like.â€
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦å¤§é‡çš„æ ‡æ³¨æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œè¢«å‘ŠçŸ¥â€œè¿™æ˜¯ä¸€å¼ çŒ«çš„å›¾ç‰‡â€ï¼‰ã€‚ç„¶è€Œï¼Œæ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥ä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­å­¦ä¹ ï¼Œä½¿å…¶èƒ½å¤Ÿåˆ©ç”¨äº’è”ç½‘ä¸Šæµ·é‡çš„è§†è§‰å†…å®¹ç”Ÿæˆå„ç§ç±»å‹çš„è§†é¢‘ã€‚æ¢å¥è¯è¯´ï¼ŒSoraå¯ä»¥è§‚å¯Ÿä¸åŒçš„è§†é¢‘å’Œå›¾åƒï¼Œå¹¶å­¦ä¹ â€œè¿™å°±æ˜¯æ­£å¸¸è§†é¢‘çš„æ ·å­â€ã€‚
- en: '*For more detailed technical explanations about Diffusion Models:*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ‰å…³æ‰©æ•£æ¨¡å‹çš„æ›´è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: '[](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [## Diffusion Models Made Easy'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [## æ‰©æ•£æ¨¡å‹è½»æ¾å®ç°'
- en: Understanding the Basics of Denoising Diffusion Probabilistic Models
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç†è§£å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„åŸºç¡€
- en: towardsdatascience.com](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
    [## Understanding the Denoising Diffusion Probabilistic Model, the Socratic Way
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------)
    [](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
    [## é€šè¿‡è‹æ ¼æ‹‰åº•å¼æ–¹æ³•ç†è§£å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹
- en: A deep dive into the motivation behind the denoising diffusion model and detailed
    derivations for the loss function
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢è®¨å»å™ªæ‰©æ•£æ¨¡å‹èƒŒåçš„åŠ¨æœºä»¥åŠæŸå¤±å‡½æ•°çš„è¯¦ç»†æ¨å¯¼
- en: towardsdatascience.com](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------)
- en: 7\. Latent Diffusion Models
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. æ½œåœ¨æ‰©æ•£æ¨¡å‹
- en: Rombach, et al. (2022), â€œHigh-resolution image synthesis with latent diffusion
    models.â€
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Rombach ç­‰äººï¼ˆ2022å¹´ï¼‰ï¼Œã€Šä½¿ç”¨æ½œåœ¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆã€‹
- en: This research has made a significant contribution to the field of high-resolution
    image synthesis using diffusion models. It proposes a method that significantly
    reduces computational costs compared to direct high-resolution image generation
    by utilizing diffusion models in the latent space while maintaining quality. In
    other words, instead of directly manipulating images, it demonstrates that by
    encoding and introducing the diffusion process to data represented in the latent
    space (a lower-dimensional space holding compressed representations of images),
    it is possible to achieve with fewer computational resources.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç ”ç©¶åœ¨ä½¿ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆé¢†åŸŸä½œå‡ºäº†é‡è¦è´¡çŒ®ã€‚å®ƒæå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åœ¨æ½œåœ¨ç©ºé—´ä¸­åˆ©ç”¨æ‰©æ•£æ¨¡å‹ï¼Œåœ¨ä¿æŒè´¨é‡çš„åŒæ—¶æ˜¾è‘—é™ä½äº†ä¸ç›´æ¥é«˜åˆ†è¾¨ç‡å›¾åƒç”Ÿæˆç›¸æ¯”çš„è®¡ç®—æˆæœ¬ã€‚æ¢å¥è¯è¯´ï¼Œå®ƒå±•ç¤ºäº†é€šè¿‡å¯¹æ½œåœ¨ç©ºé—´ï¼ˆä¸€ä¸ªè¡¨ç¤ºå›¾åƒå‹ç¼©è¡¨ç¤ºçš„ä½ç»´ç©ºé—´ï¼‰ä¸­çš„æ•°æ®è¿›è¡Œç¼–ç å¹¶å¼•å…¥æ‰©æ•£è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ç›´æ¥æ“ä½œå›¾åƒï¼Œå¯ä»¥ç”¨æ›´å°‘çš„è®¡ç®—èµ„æºå®ç°ç›®æ ‡ã€‚
- en: Sora applies this technology to video data, compressing the temporal+spatial
    data of videos into a lower-dimensional latent space, and then undergoing a process
    of decomposing it into spatiotemporal patches. This efficient data processing
    and generation capability in the latent space plays a crucial role in enabling
    Sora to generate higher-quality visual content more rapidly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Soraå°†è¿™ä¸€æŠ€æœ¯åº”ç”¨äºè§†é¢‘æ•°æ®ï¼Œå°†è§†é¢‘çš„æ—¶ç©ºæ•°æ®å‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´ï¼Œç„¶åç»è¿‡ä¸€ç³»åˆ—çš„è¿‡ç¨‹å°†å…¶åˆ†è§£ä¸ºæ—¶ç©ºå—ã€‚è¿™ç§åœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œé«˜æ•ˆæ•°æ®å¤„ç†å’Œç”Ÿæˆçš„èƒ½åŠ›å¯¹äºSoraèƒ½å¤Ÿæ›´å¿«é€Ÿåœ°ç”Ÿæˆæ›´é«˜è´¨é‡çš„è§†è§‰å†…å®¹èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚
- en: '![](../Images/3f152224408eb5cdaf418cf2567a2045.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f152224408eb5cdaf418cf2567a2045.png)'
- en: Image of visual encodingï½œImage Credit (OpenAI)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è§†è§‰ç¼–ç å›¾ï½œå›¾ç‰‡æ¥æºï¼ˆOpenAIï¼‰
- en: '*For more detailed technical explanations about Latent Diffusion Models:*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„æ›´å¤šæŠ€æœ¯ç»†èŠ‚ï¼š*'
- en: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
    [## Paper Explained â€” High-Resolution Image Synthesis with Latent Diffusion Models'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
    [## è®ºæ–‡è§£è¯» â€” åŸºäºæ½œåœ¨æ‰©æ•£æ¨¡å‹çš„é«˜åˆ†è¾¨ç‡å›¾åƒåˆæˆ'
- en: While OpenAI has dominated the field of natural language processing with their
    generative text models, their imageâ€¦
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å°½ç®¡ OpenAI å·²å‡­å€Ÿå…¶ç”Ÿæˆæ–‡æœ¬æ¨¡å‹ä¸»å¯¼äº†è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œä½†ä»–ä»¬çš„å›¾åƒâ€¦â€¦
- en: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------)
- en: 8\. Diffusion Transformer (DiT)
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰
- en: Peebles and Xie. (2023), â€œScalable diffusion models with transformers.â€
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Peebles å’Œ Xie. (2023)ï¼Œã€Šå…·æœ‰å˜å‹å™¨çš„å¯æ‰©å±•æ‰©æ•£æ¨¡å‹ã€‹ã€‚
- en: This research might be the most crucial in realizing Sora. As mentioned in the
    technical report published by OpenAI, Sora employs not a vanilla (normal) transformer
    but a diffusion transformer (DiT).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é¡¹ç ”ç©¶å¯èƒ½æ˜¯å®ç° Sora çš„æœ€å…³é”®ç ”ç©¶ã€‚å¦‚ OpenAI å‘å¸ƒçš„æŠ€æœ¯æŠ¥å‘Šä¸­æ‰€æåˆ°ï¼ŒSora å¹¶ä¸æ˜¯é‡‡ç”¨æ™®é€šçš„ï¼ˆæ ‡å‡†ï¼‰å˜å‹å™¨ï¼Œè€Œæ˜¯é‡‡ç”¨äº†æ‰©æ•£å˜å‹å™¨ï¼ˆDiTï¼‰ã€‚
- en: Importantly, Sora is a diffusion *transformer*. (via OpenAI Sora technical report)
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯ï¼ŒSora æ˜¯ä¸€ç§æ‰©æ•£ *å˜å‹å™¨*ã€‚ï¼ˆæ¥æºï¼šOpenAI Sora æŠ€æœ¯æŠ¥å‘Šï¼‰
- en: The study introduced a new model that replaces the U-net component, commonly
    used in diffusion models, with a Transformer structure. This structure enables
    the Latent Diffusion Model through operations on latent patches by the Transformer.
    This approach allows for more efficient handling of image patches, enabling the
    generation of high-quality images while effectively utilizing computational resources.
    The incorporation of this Transformer, which differs from the Stable Diffusion
    announced by Stability AI in 2022, is considered to contribute to more natural
    video generation.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç”¨å˜å‹å™¨ç»“æ„æ›¿ä»£äº†æ‰©æ•£æ¨¡å‹ä¸­å¸¸ç”¨çš„ U-net ç»„ä»¶ã€‚é€šè¿‡å¯¹æ½œåœ¨å›¾åƒå—è¿›è¡Œå˜å‹å™¨æ“ä½œï¼Œè¿™ç§ç»“æ„ä½¿æ½œåœ¨æ‰©æ•£æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚è¯¥æ–¹æ³•ä½¿å¾—å›¾åƒå—çš„å¤„ç†æ›´åŠ é«˜æ•ˆï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„å›¾åƒï¼ŒåŒæ—¶æœ‰æ•ˆåˆ©ç”¨è®¡ç®—èµ„æºã€‚è¿™ä¸€å˜å‹å™¨çš„å¼•å…¥ï¼Œä¸
    Stability AI åœ¨ 2022 å¹´å‘å¸ƒçš„ Stable Diffusion ä¸åŒï¼Œè¢«è®¤ä¸ºæœ‰åŠ©äºå®ç°æ›´åŠ è‡ªç„¶çš„è§†é¢‘ç”Ÿæˆã€‚
- en: '![](../Images/54dc92e3b369086a4a28bb343998d748.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54dc92e3b369086a4a28bb343998d748.png)'
- en: 'Figure 1: Generated images by the Diffusion Transformersï½œPeebles and Xie. (2023)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šæ‰©æ•£å˜å‹å™¨ç”Ÿæˆçš„å›¾åƒï½œPeebles å’Œ Xie. (2023)
- en: Furthermore, itâ€™s important to note that their validation results demonstrate
    the scalability of DiT, significantly contributing to the realization of Sora.
    Being scalable means that the modelâ€™s performance improves with an increase in
    the transformerâ€™s depth/width (making the model more complex) or the number of
    input tokens.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»–ä»¬çš„éªŒè¯ç»“æœè¯æ˜äº† DiT çš„å¯æ‰©å±•æ€§ï¼Œæ˜¾è‘—ä¿ƒè¿›äº† Sora çš„å®ç°ã€‚å¯æ‰©å±•æ€§æ„å‘³ç€æ¨¡å‹çš„æ€§èƒ½ä¼šéšç€å˜å‹å™¨æ·±åº¦/å®½åº¦çš„å¢åŠ ï¼ˆä½¿æ¨¡å‹æ›´å¤æ‚ï¼‰æˆ–è¾“å…¥æ ‡è®°æ•°é‡çš„å¢åŠ è€Œæå‡ã€‚
- en: '![](../Images/38de39d61eef4b8bfb6e62a3d800e8df.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38de39d61eef4b8bfb6e62a3d800e8df.png)'
- en: 'Figure 8 & 9: Scalability of the Diffusion Transformersï½œPeebles and Xie. (2023)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8 å’Œ 9ï¼šæ‰©æ•£å˜å‹å™¨çš„å¯æ‰©å±•æ€§ï½œPeebles å’Œ Xie. (2023)
- en: 'Gflops (Computational performance): A unit of measure for a computerâ€™s calculating
    speed equal to one billion floating-point operations per second. In this paper,
    network complexity is measured by Gflops.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gflopsï¼ˆè®¡ç®—æ€§èƒ½ï¼‰ï¼šè¡¡é‡è®¡ç®—æœºè¿ç®—é€Ÿåº¦çš„å•ä½ï¼Œç­‰äºæ¯ç§’åäº¿æ¬¡æµ®ç‚¹è¿ç®—ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œç½‘ç»œå¤æ‚åº¦é€šè¿‡ Gflops æ¥è¡¡é‡ã€‚
- en: 'FID (FrÃ©chet Inception Distance): One of the evaluation metrics for image generation,
    where a lower value indicates higher accuracy. It quantitatively assesses the
    quality of generated images by measuring the distance between the feature vectors
    of generated images and real images.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FIDï¼ˆFrÃ©chet ç”Ÿæˆè·ç¦»ï¼‰ï¼šå›¾åƒç”Ÿæˆçš„è¯„ä»·æŒ‡æ ‡ä¹‹ä¸€ï¼Œå€¼è¶Šä½è¡¨ç¤ºå‡†ç¡®åº¦è¶Šé«˜ã€‚å®ƒé€šè¿‡æµ‹é‡ç”Ÿæˆå›¾åƒå’ŒçœŸå®å›¾åƒä¹‹é—´ç‰¹å¾å‘é‡çš„è·ç¦»ï¼Œå®šé‡è¯„ä¼°ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚
- en: This has already been observed in the field of natural language processing,
    as confirmed by Kaplan et al. (2020) and Brown et al. (2020), supporting the crucial
    characteristics behind the innovative success of ChatGPT.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç‚¹åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå·²ç»è¢«è§‚å¯Ÿåˆ°ï¼Œå¦‚ Kaplan ç­‰äººï¼ˆ2020ï¼‰å’Œ Brown ç­‰äººï¼ˆ2020ï¼‰æ‰€ç¡®è®¤ï¼Œæ”¯æŒäº† ChatGPT åˆ›æ–°æˆåŠŸèƒŒåçš„å…³é”®ç‰¹å¾ã€‚
- en: Kaplan et al. (2020), â€œScaling Laws for Neural Language Models.â€
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kaplan ç­‰äººï¼ˆ2020ï¼‰ï¼Œã€Šç¥ç»è¯­è¨€æ¨¡å‹çš„æ‰©å±•æ³•åˆ™ã€‹ã€‚
- en: ''
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Brown, et al. (2020), â€œLanguage models are few-shot learners.â€
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Brownç­‰äººï¼ˆ2020å¹´ï¼‰ï¼Œã€Šè¯­è¨€æ¨¡å‹æ˜¯å°‘é‡å­¦ä¹ è€…ã€‹
- en: This significant feature, in addition to generating high-quality images at a
    lower computational cost than traditional diffusion models due to the benefits
    of the Transformer, indicates that even higher-quality images can be produced
    with larger computational resources. Sora applies this technology to video generation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ˜¾è‘—ç‰¹ç‚¹ï¼Œé™¤äº†ç”±äºTransformerçš„ä¼˜åŠ¿åœ¨è¾ƒä½è®¡ç®—æˆæœ¬ä¸‹ç”Ÿæˆé«˜è´¨é‡å›¾åƒä¹‹å¤–ï¼Œè¿˜è¡¨æ˜ï¼Œé€šè¿‡æ›´å¤§çš„è®¡ç®—èµ„æºï¼Œç”šè‡³å¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„å›¾åƒã€‚Soraå°†è¿™ä¸€æŠ€æœ¯åº”ç”¨äºè§†é¢‘ç”Ÿæˆã€‚
- en: '![](../Images/2d63e2c57fef03126f80016284ea38cb.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d63e2c57fef03126f80016284ea38cb.png)'
- en: Scalability of video generationï½œImage Credit (OpenAI)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è§†é¢‘ç”Ÿæˆçš„å¯æ‰©å±•æ€§ï½œå›¾åƒæ¥æºï¼ˆOpenAIï¼‰
- en: '*For more detailed technical explanations about DiT:*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*å…³äºDiTçš„æ›´è¯¦ç»†æŠ€æœ¯è§£é‡Šï¼š*'
- en: Review the paper of DiT by hu-po via YouTube
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡YouTubeè§‚çœ‹DiTçš„è®ºæ–‡è¯„å®¡
- en: The capabilities enabled by these research efforts for Sora
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿™äº›ç ”ç©¶æˆæœä¸ºSoraæä¾›çš„èƒ½åŠ›
- en: '**Variable durations, resolutions, aspect ratios**'
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å¯å˜çš„æŒç»­æ—¶é—´ã€åˆ†è¾¨ç‡ã€çºµæ¨ªæ¯”**'
- en: Primarily, thanks to NaViT, Sora can sample widescreen 1920x1080p videos, vertical
    1080x1920 videos, and everything in between. This means it can create visuals
    for various device types at any resolution.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦å¾—ç›ŠäºNaViTï¼ŒSoraå¯ä»¥é‡‡æ ·å®½å±1920x1080pè§†é¢‘ã€å‚ç›´1080x1920è§†é¢‘ä»¥åŠä»‹äºä¸¤è€…ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ã€‚è¿™æ„å‘³ç€å®ƒèƒ½å¤Ÿä¸ºå„ç§è®¾å¤‡ç±»å‹åœ¨ä»»ä½•åˆ†è¾¨ç‡ä¸‹åˆ›å»ºè§†è§‰æ•ˆæœã€‚
- en: '**Prompting with images and videos**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨å›¾åƒå’Œè§†é¢‘è¿›è¡Œæç¤º**'
- en: Currently, the videos generated by Sora, as demonstrated, are created in a text-to-video
    format, where instructions are given through text prompts. However, as can be
    easily anticipated from the previous research, itâ€™s also possible to use existing
    images or videos as inputs, not just text. This allows for the animation of images
    or for Sora to imagine and output the past or future of an existing video as visuals.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒSoraç”Ÿæˆçš„è§†é¢‘ï¼Œå¦‚æ¼”ç¤ºæ‰€ç¤ºï¼Œæ˜¯ä»¥æ–‡æœ¬åˆ°è§†é¢‘çš„æ ¼å¼åˆ›å»ºçš„ï¼Œå…¶ä¸­æŒ‡ä»¤é€šè¿‡æ–‡æœ¬æç¤ºç»™å‡ºã€‚ç„¶è€Œï¼Œæ­£å¦‚ä»ä¹‹å‰çš„ç ”ç©¶ä¸­å¯ä»¥è½»æ˜“é¢„è§çš„é‚£æ ·ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç°æœ‰çš„å›¾åƒæˆ–è§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œè€Œä¸ä»…ä»…æ˜¯æ–‡æœ¬ã€‚è¿™ä½¿å¾—å›¾åƒçš„åŠ¨ç”»åŒ–æˆä¸ºå¯èƒ½ï¼Œæˆ–è€…Soraå¯ä»¥æƒ³è±¡å¹¶è¾“å‡ºç°æœ‰è§†é¢‘çš„è¿‡å»æˆ–æœªæ¥ä½œä¸ºè§†è§‰æ•ˆæœã€‚
- en: '**3D consistency**'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸‰ç»´ä¸€è‡´æ€§**'
- en: While itâ€™s not clear how the aforementioned research is directly involved, Sora
    can generate videos with dynamic camera motion. As the camera shifts and rotates,
    people and scene elements move consistently through three-dimensional space.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å°šä¸æ¸…æ¥šä¸Šè¿°ç ”ç©¶å¦‚ä½•ç›´æ¥å‚ä¸å…¶ä¸­ï¼Œä½†Soraèƒ½å¤Ÿç”Ÿæˆå¸¦æœ‰åŠ¨æ€æ‘„åƒæœºè¿åŠ¨çš„è§†é¢‘ã€‚å½“æ‘„åƒæœºç§»åŠ¨å’Œæ—‹è½¬æ—¶ï¼Œäººç‰©å’Œåœºæ™¯å…ƒç´ ä¼šåœ¨ä¸‰ç»´ç©ºé—´ä¸­ä¸€è‡´åœ°ç§»åŠ¨ã€‚
- en: The future of Sora
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Soraçš„æœªæ¥
- en: In this blog post, I have explained the technologies behind OpenAIâ€™s AI for
    generating videos, Sora, which has already shocked the world. Once it becomes
    publicly available and accessible to a wider audience, it is bound to make an
    even more significant impact worldwide.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšæ–‡ä¸­ï¼Œæˆ‘å·²ç»è§£é‡Šäº†OpenAIç”¨äºç”Ÿæˆè§†é¢‘çš„AIæŠ€æœ¯â€”â€”Soraï¼Œè¿™é¡¹æŠ€æœ¯å·²ç»éœ‡æƒŠäº†å…¨ä¸–ç•Œã€‚ä¸€æ—¦å®ƒå…¬å¼€å‘å¸ƒå¹¶èƒ½è¢«æ›´å¹¿æ³›çš„ç”¨æˆ·è®¿é—®ï¼Œå®ƒåŠ¿å¿…å°†åœ¨å…¨çƒèŒƒå›´å†…äº§ç”Ÿæ›´ä¸ºæ·±è¿œçš„å½±å“ã€‚
- en: The impact of this breakthrough is expected to span across various aspects of
    video creation, but it is predicted that it may likely evolve from video to further
    advancements in 3D modeling. If that becomes the case, not only video creators
    but also the production of visuals in virtual spaces like the metaverse could
    soon be easily generated by AI.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€çªç ´çš„å½±å“é¢„è®¡å°†è·¨è¶Šè§†é¢‘åˆ›ä½œçš„å„ä¸ªæ–¹é¢ï¼Œä½†é¢„æµ‹å®ƒå¯èƒ½ä»è§†é¢‘æ¼”å˜ä¸ºä¸‰ç»´å»ºæ¨¡çš„è¿›ä¸€æ­¥å‘å±•ã€‚å¦‚æœçœŸæ˜¯å¦‚æ­¤ï¼Œä¸ä»…æ˜¯è§†é¢‘åˆ›ä½œè€…ï¼Œè¿è™šæ‹Ÿç©ºé—´ï¼ˆå¦‚å…ƒå®‡å®™ï¼‰ä¸­çš„è§†è§‰å†…å®¹ç”Ÿäº§ä¹Ÿå¯èƒ½å¾ˆå¿«ç”±AIè½»æ¾ç”Ÿæˆã€‚
- en: 'The arrival of such a future has already been implied as below:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„æœªæ¥çš„åˆ°æ¥å·²ç»åœ¨ä»¥ä¸‹å†…å®¹ä¸­æœ‰æ‰€æš—ç¤ºï¼š
- en: Martin Nebelongâ€™s post about Micael Rublofâ€™s product via X
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Martin Nebelongé€šè¿‡Xå‘å¸ƒçš„å…³äºMicael Rublofäº§å“çš„å¸–å­
- en: Currently, Sora is perceived as â€œmerelyâ€ a video generation model, but Jim Fan
    from Nvidia has implied it might be a data-driven physics engine. This suggests
    the possibility that AI, from a vast amount of real-world videos and (though not
    explicitly mentioned) videos considering physical behaviors like those from Unreal
    Engine, might understand physical laws and phenomena. If so, the emergence of
    text-to-3D in the near future is also highly probable.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼ŒSoraè¢«è§†ä¸ºâ€œä»…ä»…â€ä¸€ä¸ªè§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œä½†æ¥è‡ªNvidiaçš„Jim Fanæš—ç¤ºï¼Œå®ƒå¯èƒ½æ˜¯ä¸€ä¸ªåŸºäºæ•°æ®çš„ç‰©ç†å¼•æ“ã€‚è¿™è¡¨æ˜ï¼ŒAIé€šè¿‡å¤§é‡çš„ç°å®ä¸–ç•Œè§†é¢‘ä»¥åŠï¼ˆå°½ç®¡æœªæ˜ç¡®æåŠï¼‰åƒè™šå¹»å¼•æ“é‚£æ ·è€ƒè™‘ç‰©ç†è¡Œä¸ºçš„è§†é¢‘ï¼Œå¯èƒ½ç†è§£ç‰©ç†å®šå¾‹å’Œç°è±¡ã€‚å¦‚æœæ˜¯è¿™æ ·ï¼Œæœªæ¥ä¸ä¹…ï¼Œæ–‡æœ¬åˆ°ä¸‰ç»´çš„å‡ºç°ä¹Ÿæ˜¯é«˜åº¦å¯èƒ½çš„ã€‚
- en: Jim Fanâ€™s intriguing post via X
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Jim Fané€šè¿‡Xå‘å¸ƒçš„å¼•äººæ³¨ç›®çš„å¸–å­
- en: '***Thank you so much for reading this article.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '***éå¸¸æ„Ÿè°¢æ‚¨é˜…è¯»è¿™ç¯‡æ–‡ç« ã€‚'
- en: Your clap to this article and subscription to*** [***my newsletter***](https://rkiuchir.medium.com/subscribe)
    ***would motivate me a lot!***
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯¹è¿™ç¯‡æ–‡ç« çš„ç‚¹èµå’Œå¯¹***[***æˆ‘çš„æ–°é—»é€šè®¯***](https://rkiuchir.medium.com/subscribe) ***çš„è®¢é˜…ä¼šç»™æˆ‘å¸¦æ¥å¾ˆå¤§çš„åŠ¨åŠ›ï¼***
