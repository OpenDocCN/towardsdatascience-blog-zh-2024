- en: Data Dirtiness Score
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据脏乱度评分
- en: 原文：[https://towardsdatascience.com/data-dirtiness-score-fe2ca5678d40?source=collection_archive---------1-----------------------#2024-03-02](https://towardsdatascience.com/data-dirtiness-score-fe2ca5678d40?source=collection_archive---------1-----------------------#2024-03-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-dirtiness-score-fe2ca5678d40?source=collection_archive---------1-----------------------#2024-03-02](https://towardsdatascience.com/data-dirtiness-score-fe2ca5678d40?source=collection_archive---------1-----------------------#2024-03-02)
- en: New method to measure tabular dataset quality
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测量表格数据集质量的新方法
- en: '[](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)[![Simon
    Grah](../Images/f8fd00600db79bc910ff51e9f64503d0.png)](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)
    [Simon Grah](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)[![Simon
    Grah](../Images/f8fd00600db79bc910ff51e9f64503d0.png)](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)
    [Simon Grah](https://medium.com/@simon.grah?source=post_page---byline--fe2ca5678d40--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)
    ·11 min read·Mar 2, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fe2ca5678d40--------------------------------)
    ·阅读时间：11分钟·2024年3月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This article, the first in a series on data cleaning practices involving Large
    Language Models (LLMs), focuses on quantifying the cleanliness or dirtiness of
    a dataset
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本文是关于涉及大型语言模型（LLM）数据清洗实践系列文章的第一篇，重点讨论如何量化数据集的清洁度或脏乱度
- en: '![](../Images/4c8f68fd06ac2bc78782f13b1a251904.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c8f68fd06ac2bc78782f13b1a251904.png)'
- en: Photo by [Fabrizio Conti](https://unsplash.com/@conti_photos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Fabizio Conti](https://unsplash.com/@conti_photos?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Starting with the Why
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从为什么开始
- en: This article introduces a concept for evaluating the dirtiness of a dataset,
    a topic that presents challenges due to the lack of a tangible score or loss function
    related to data cleaning. The primary objective here is to establish a metric
    that can effectively measure the cleanliness level of a dataset, translating this
    concept into a concrete optimisation problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种用于评估数据集脏乱度的概念，这个话题由于缺乏与数据清洗相关的具体评分或损失函数而面临挑战。这里的主要目标是建立一个可以有效衡量数据集清洁度的指标，将这一概念转化为具体的优化问题。
- en: 'Data cleaning is defined as a two-phase process:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗定义为一个两阶段过程：
- en: First, **detecting data errors** such as formatting issues, duplicate records,
    and outliers;
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，**检测数据错误**，如格式问题、重复记录和离群值；
- en: Second, **fixing** these **errors**.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，**修复**这些**错误**。
- en: 'The evaluation of each phase typically relies on comparing a dirty dataset
    against a clean (ground truth) version, using classification metrics like recall,
    precision, and F1-score for error detection (see for example [Can Foundation Models
    Wrangle Your Data?](https://arxiv.org/abs/2205.09911), [Detecting Data Errors:
    Where are we and what needs to be done?](https://cs.uwaterloo.ca/~ilyas/papers/AbedjanVLDB2016.pdf))
    and accuracy or overlap-based metrics for data repair tasks (see [Automatic Data
    Repair: Are We Ready to Deploy?](https://www.semanticscholar.org/reader/5191f08424a3ec0cdaf2b3285860216caca57463)
    or [HoloClean: Holistic Data Repairs with Probabilistic Inference](https://arxiv.org/abs/1702.00820)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段的评估通常依赖于将脏乱的数据集与清洁（真实）版本进行比较，使用分类指标如召回率、精确度和F1得分进行错误检测（例如，参见[大型基础模型能处理你的数据吗？](https://arxiv.org/abs/2205.09911)，[检测数据错误：我们在哪儿，需要做什么？](https://cs.uwaterloo.ca/~ilyas/papers/AbedjanVLDB2016.pdf)）以及用于数据修复任务的准确率或重叠度度量（例如，参见[自动数据修复：我们准备好部署了吗？](https://www.semanticscholar.org/reader/5191f08424a3ec0cdaf2b3285860216caca57463)或[HoloClean：通过概率推理进行整体数据修复](https://arxiv.org/abs/1702.00820)）。
- en: However, these metrics are task-specific and do not offer a unified measure
    for the overall cleanliness of a dataset that includes various types of errors.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些指标是任务特定的，并未提供一个统一的衡量标准来评估包含各种错误类型的数据集的整体清洁度。
- en: This discussion is focused on **structured and tidy tabular datasets** (see
    [Tidy Data | Journal of Statistical Software](https://www.jstatsoft.org/article/view/v059i10)),
    **distinguishing data cleaning from broader data quality concerns** that include
    data governance, lineage, cataloguing, drift, and more.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本讨论聚焦于**结构化且整洁的表格数据集**（见[《整洁数据 | 统计软件期刊》](https://www.jstatsoft.org/article/view/v059i10)），**将数据清洗与更广泛的数据质量问题区分开来**，后者包括数据治理、数据溯源、目录管理、数据漂移等。
- en: The score blueprint
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分蓝图
- en: All the assumptions hereafter are the foundations the *Data Dirtiness Score*
    relies on. There are largely inspired by the article [How to quantify Data Quality?](/how-to-quantify-data-quality-743721bdba03).
    Of course, all of them could be debated and criticised but it is crucial to clearly
    state them to enhance discussions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有假设都是*数据脏污评分*所依赖的基础。这些假设大多来自文章[《如何量化数据质量？》](/how-to-quantify-data-quality-743721bdba03)。当然，所有这些假设都可以被辩论和批评，但明确陈述这些假设对促进讨论至关重要。
- en: '**Data errors are tied to violated constraints**, which arise from **expectations**
    about the data. For example, if the expectation is that the ID column should have
    no missing values, the presence of missing IDs would constitute a constraint violation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据错误与违反约束有关**，这些约束来源于对数据的**期望**。例如，如果期望ID列不应有缺失值，那么ID列中存在缺失值就构成了约束违反。'
- en: No Expectation No Cry. **The absence of expectations means no impact on the
    score**. In other words, no data issues can be identified without predefined expectations,
    and thus, can’t violate constraints that don’t exist.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无期望则无忧。**缺乏期望意味着不会对评分产生影响**。换句话说，若没有预定义的期望，就无法识别数据问题，因此也无法违反不存在的约束。
- en: '**Data issues should be locateable to specific cells**. The score relies on
    the ability to pinpoint errors to particular cells in the dataset.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据问题应能定位到具体单元格**。评分依赖于能够将错误精确定位到数据集中特定单元格的能力。'
- en: '**Confidence scores for data errors**. Not all data errors can be identified
    with the same level of certainty. Each detected issue should be tagged with a
    confidence level, indicating how likely it is that the identified issue is indeed
    an error. This approach acknowledges that some errors might be open to interpretation.
    Instead of using a continuous scale from 0 to 1, which might be too detailed,
    we suggest categorising this confidence level into four ordinal categories: `low`,
    `medium`, `high`, and `certain`. These categories correspond to probability values
    of 0.25, 0.5, 0.75, and 1, respectively.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据错误的置信度分数**。并非所有数据错误都能以相同的确定性被识别。每个检测到的问题应标记一个置信度级别，表示该问题确实是一个错误的可能性有多大。这种方法承认一些错误可能存在解释空间。我们建议将置信度级别分为四个顺序类别：`低`、`中`、`高`和`确定`。这些类别分别对应0.25、0.5、0.75和1的概率值。'
- en: '**Uniform impact of cells on the overall score**. Each cell in a dataset has
    an equal potential impact on the dirtiness score. Addressing an issue related
    to a given cell may resolve issues in others, suggesting a uniform distribution
    of cell weights in the score calculation.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**单元格对整体评分的均匀影响**。数据集中的每个单元格对脏数据评分都有相同的潜在影响。解决与某个单元格相关的问题可能会解决其他单元格的问题，这表明在评分计算中单元格的权重是均匀分布的。'
- en: A toy example for illustration
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简化示例以便说明
- en: 'When examining a dataset, it’s not uncommon to spot potential data quality
    issues at a glance. Consider the following simple dataset for analysis:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查数据集时，凭一眼就能发现潜在的数据质量问题并不罕见。请考虑以下用于分析的简单数据集：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This example from the book [Cleaning Data for Effective Data Science](https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science)
    illustrates data quality issues within a dataset representing a 6th-grade class.
    This dataset includes multiple variables for each student, organised such that
    there are 6 students and 5 variables per student.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例来自书籍[《有效数据科学的数据清洗》](https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science)，它展示了一个代表六年级班级的数据集中存在的数据质量问题。该数据集包含每个学生的多个变量，组织方式是每个学生有6个学生和5个变量。
- en: 'Upon inspection, certain entries might raise concerns due to apparent inconsistencies
    or errors:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查时，某些条目可能因明显的不一致性或错误而引起关注：
- en: The entry for the student with `Student#` 2 (Lopez, Liam) appears to have an
    extra value in the `Favorite Color` column, which looks like two values ('blue,green')
    have been merged. Typically, this column should only contain a single value. Given
    the uncertainty, this issue is flagged with a `high`confidence level for further
    inspection.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学号为`Student#` 2（Liam Lopez）的学生条目似乎在`Favorite Color`列中有一个额外的值，看起来像是两个值（'blue,green'）合并在了一起。通常情况下，这一列应只包含一个值。鉴于不确定性，该问题被标记为`high`置信度级别，需进一步检查。
- en: The next student, Isabella Lee, lacks a `Favorite Color` value. Given that this
    column should not have any missing entries, this issue is identified with `certain`confidence
    for correction.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一位学生，Isabella Lee，缺少`Favorite Color`值。鉴于该列不应有任何缺失项，因此该问题已被以`certain`置信度识别为需要修正。
- en: The record for student number 4, Mason Fisher, lists an age of `-1`, an implausible
    value. This might represent a sentinel value indicating missing data, as it is
    common practice to use such placeholders. However, ages should be positive integers,
    necessitating a review of this entry.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学号为4的学生Mason Fisher的记录列出了`-1`的年龄，这是一个不可信的值。这可能代表一个表示缺失数据的哨兵值，因为使用这种占位符是常见做法。然而，年龄应该是正整数，因此需要审查这一条目。
- en: The row for student number 5, Olivia Gupta, while free from structural errors,
    presents an unusual case as several explanations are plausible. The `Favorite
    Color` and `First Name` fields might be swapped, considering `Olivia` can be both
    a name and a colour. Alternatively, the number `9` could represent a colour code,
    but this hypothesis lacks corroborating evidence. Moreover, an age of `102` for
    a 6th-grade student is highly improbable, suggesting potential typographical errors
    (e.g. `102` instead of `12`).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学号为5的学生Olivia Gupta所在的行虽然没有结构性错误，但却呈现出一个不寻常的情况，因为有多个解释是合理的。`Favorite Color`和`First
    Name`字段可能被交换，因为`Olivia`既可以是名字，也可以是颜色。此外，数字`9`可能表示颜色代码，但这一假设缺乏支持证据。而且，一名六年级学生的年龄为`102`是不太可能的，这表明可能存在拼写错误（例如将`102`写成了`12`）。
- en: The last row contains superfluous commas, indicating a possible data ingestion
    issue. However, aside from this formatting concern, the entry itself seems valid,
    leading to a `high`confidence level in identifying the nature of this error.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一行包含多余的逗号，表示可能存在数据摄取问题。然而，除了这个格式问题之外，条目本身似乎有效，因此在识别该错误的性质时，给出了`high`的置信度级别。
- en: 'Following our guidelines to compute the dirtiness score, we can adopt a methodical
    approach by introducing a `DataIssue` class in Python, designed to encapsulate
    various aspects of a data issue:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的指导方针计算数据脏度分数，我们可以通过引入一个Python中的`DataIssue`类来采用系统化的方法，该类旨在封装数据问题的各个方面：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To locate specific errors, a `numpy` array of size `(6, 5)` is utilised, where
    each element corresponds to a cell in the dataset. This array consists of 0s and
    1s, with 1 indicating a potential issue in the corresponding cell of the dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定位特定错误，使用一个大小为`(6, 5)`的`numpy`数组，其中每个元素对应数据集中的一个单元格。该数组由0和1组成，1表示数据集中相应单元格可能存在问题。
- en: 'All the identified data issues are instantiated hereafter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所有已识别的数据问题将在此之后实例化：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The categorisation of multiple data errors into specific `DataIssue` instances
    can be somewhat subjective, similar to the nuances involved in bug reporting in
    software development. The fields—`type_of_issue`, `expectation`, and `constraint_violated`—serve
    to elucidate the nature of the error, facilitating understanding during investigations
    or reviews.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个数据错误归类为特定的`DataIssue`实例可能带有一定的主观性，类似于软件开发中的错误报告过程。字段—`type_of_issue`、`expectation`和`constraint_violated`—有助于阐明错误的性质，便于在调查或审查时理解。
- en: For computing the dirtiness score, the critical elements are the locations of
    the errors and the associated confidence scores. In this example, the confidence
    scores are estimated based on the perceived certainty of an error's presence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据脏度分数时，关键元素是错误的位置和相关的置信度分数。在这个示例中，置信度分数是基于对错误存在性的感知确定的。
- en: '*Repeated issues pointing to the same cells significantly increase the likelihood
    of a problem being present there.*'
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*指向相同单元格的重复问题显著增加了问题存在的可能性。*'
- en: Now that we have all the information we need, let’s see how to calculate the
    dirtiness score for this small data set.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有所需的所有信息，让我们看看如何计算这个小数据集的脏度分数。
- en: Calculation of the Data Dirtiness Score
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算数据脏度分数
- en: '*The* Data Dirtiness Score *represents the* ***expected fraction of cells***
    *in a dataset* ***that contain errors****.*'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据脏污得分* *代表了* ***数据集中包含错误的单元格的预期比例***。*'
- en: The theory and calculation for this score are elaborated in the `Score Theory`
    section of the appendix.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 该得分的理论和计算详见附录中的`得分理论`部分。
- en: By using confidence scores for various issues as estimates for the independent
    probability of an error in each cell, we can apply fundamental probability principles
    to calculate the likelihood of an issue per cell, and consequently, the *Data
    Dirtiness Score*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用针对各个问题的置信得分作为每个单元格中错误独立概率的估计值，我们可以应用基本的概率原理来计算每个单元格出现问题的可能性，从而得出*数据脏污得分*。
- en: 'Below is a Python function to calculate this metric based on a list of identified
    data issues:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个Python函数，用于根据已识别的数据问题列表计算该指标：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s compute the score for the data set presented earlier:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算之前展示的数据集的得分：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Data Dirtiness Score: 31.87%*'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据脏污得分：31.87%*'
- en: To improve (reduce) this score, a natural step is to tackle the simplest errors,
    such as correcting duplicate commas used as separators in the last row.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善（降低）这一得分，一个自然的步骤是解决最简单的错误，例如纠正最后一行中作为分隔符的重复逗号。
- en: 'Here is the new version of the dataset:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集的新版本：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let’s recompute the score once again to see the improvement.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次重新计算得分，以查看改善效果。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Data Dirtiness Score: 15.21%*'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*数据脏污得分：15.21%*'
- en: Reevaluating the score post-correction reveals a significant improvement, halving
    the score due to the nature of the error affecting an entire row in a relatively
    small dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在纠正后重新评估得分，显示出显著的改善，由于错误影响了一个相对较小的数据集中的整个行，得分减少了一半。
- en: In conclusion, this measure provides a quantitative means of monitoring and
    improving the cleanliness of our dataset by correcting iteratively identified
    data errors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，这一度量为监控和改进数据集的清洁度提供了一种定量方法，通过迭代纠正已识别的数据错误。
- en: Next Steps and Challenges
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后续步骤与挑战
- en: 'Creating expectations or constraints for data can be challenging and costly
    due to the need for human labelling and domain knowledge. A solution is to automate
    the generation of constraints and data error detection, allowing humans to later
    review and adjust these automated constraints by either removing issues or modifying
    confidence scores. For that purpose, LLMs are really good candidates (cf. [Jellyfish:
    A Large Language Model for Data Preprocessing](https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10),
    [Can language models automate data wrangling?](http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf)
    or [Large Language Models as Data Preprocessors](https://arxiv.org/abs/2308.16361)).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '为数据创建预期或约束条件可能具有挑战性并且成本较高，因为它需要人工标注和领域知识。一个解决方案是自动化生成约束条件和数据错误检测，之后由人工审查并调整这些自动化的约束条件，可以通过删除问题或修改置信得分来完成。为此，LLMs（大语言模型）是非常好的候选者（参见
    [Jellyfish: A Large Language Model for Data Preprocessing](https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10)、[Can
    language models automate data wrangling?](http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf)
    或 [Large Language Models as Data Preprocessors](https://arxiv.org/abs/2308.16361)）。'
- en: The likelihood of certain constraints and violations isn’t always crystal-clear,
    which necessitates a confidence score to account for this uncertainty. Even experts
    might not always agree on specific data issues, so when automation is involved
    in detecting these issues, having an estimated likelihood becomes particularly
    useful.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 某些约束条件和违规情况的可能性并不总是非常明确，这就需要一个置信得分来考虑这种不确定性。即便是专家，对于特定的数据问题也可能并不总是达成一致，因此，当自动化技术被用于检测这些问题时，估算的可能性就显得特别有用。
- en: 'What about absent expectations or missed data errors? The effectiveness of
    error detection directly influences the cleanliness score and can lead to an overly
    optimistic value. However, there’s a counterargument to consider: errors that
    are more difficult to detect, and thus more concealed, might not be as critical
    in their impact on data usability or downstream applications. This suggests that
    such errors should be assigned a lower confidence score when identified as issues,
    reflecting their reduced significance. While this approach may not be without
    its flaws, it serves to limit the influence of these overlooked errors on the
    overall dirtiness score by weighting their importance accordingly.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于缺失的期望或漏掉的数据错误怎么办？错误检测的有效性直接影响清洁度得分，可能导致得分过于乐观。然而，也有一个反论点需要考虑：那些更难检测到、因此更隐蔽的错误，可能对数据可用性或下游应用的影响并不那么重要。这表明，当这些错误被识别为问题时，应该赋予较低的置信度得分，反映其较低的重要性。尽管这种方法并非没有缺陷，但它有助于通过相应地加权这些被忽视的错误的重要性，从而限制它们对总体脏污得分的影响。
- en: Another aspect to consider is the dynamic nature of the score. Addressing one
    issue could potentially affect other issues, raising questions about how to update
    the score efficiently without much hassle.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的方面是得分的动态特性。解决一个问题可能会影响其他问题，这引发了如何高效更新得分而不产生太多麻烦的问题。
- en: There’s also the question of whether to include indexes and column names as
    part of the dataset cells when calculating the cleanliness score, as their accuracy
    can also affect the data cleaning process (see for example [Column Type Annotation
    using ChatGPT](https://arxiv.org/abs/2306.00745)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个问题是，在计算清洁度得分时，是否应将索引和列名作为数据集单元的一部分，因为它们的准确性也会影响数据清理过程（例如，参见 [使用ChatGPT进行列类型标注](https://arxiv.org/abs/2306.00745)）。
- en: Future articles in this series will explore various related topics, including
    a taxonomy of data errors, leveraging LLMs for automated issue detection, and
    strategies for data correction and repair. Stay tuned then!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的未来文章将探讨与此相关的各种主题，包括数据错误的分类法、利用LLM进行自动化问题检测，以及数据修正和修复的策略。敬请关注！
- en: '-> Link to the 2nd article: [Data Quality Error Detection powered by LLMs](/automated-detection-of-data-quality-issues-54a3cb283a91).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: -> 第二篇文章链接：[LLMs驱动的数据质量错误检测](/automated-detection-of-data-quality-issues-54a3cb283a91)。
- en: References
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Can Foundation Models Wrangle Your Data?](https://arxiv.org/abs/2205.09911)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础模型能处理你的数据吗？](https://arxiv.org/abs/2205.09911)'
- en: '[Detecting Data Errors: Where are we and what needs to be done?](https://cs.uwaterloo.ca/~ilyas/papers/AbedjanVLDB2016.pdf)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检测数据错误：我们处于什么阶段，接下来需要做什么？](https://cs.uwaterloo.ca/~ilyas/papers/AbedjanVLDB2016.pdf)'
- en: '[Automatic Data Repair: Are We Ready to Deploy?](https://www.semanticscholar.org/reader/5191f08424a3ec0cdaf2b3285860216caca57463)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[自动化数据修复：我们准备好部署了吗？](https://www.semanticscholar.org/reader/5191f08424a3ec0cdaf2b3285860216caca57463)'
- en: '[HoloClean: Holistic Data Repairs with Probabilistic Inference](https://arxiv.org/abs/1702.00820)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HoloClean：通过概率推理进行全局数据修复](https://arxiv.org/abs/1702.00820)'
- en: '[Tidy Data | Journal of Statistical Software](https://www.jstatsoft.org/article/view/v059i10)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[整洁数据 | 统计软件期刊](https://www.jstatsoft.org/article/view/v059i10)'
- en: '[How to quantify Data Quality?](/how-to-quantify-data-quality-743721bdba03)'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何量化数据质量？](/how-to-quantify-data-quality-743721bdba03)'
- en: '[Cleaning Data for Effective Data Science](https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science)'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[有效数据科学的数据清理](https://github.com/PacktPublishing/Cleaning-Data-for-Effective-Data-Science)'
- en: '[Jellyfish: A Large Language Model for Data Preprocessing](https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[水母：一种用于数据预处理的大型语言模型](https://www.semanticscholar.org/reader/7e17ef56273063dfa838de30b7cc0546b2e5ee10)'
- en: '[Can language models automate data wrangling?](http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[语言模型能否自动化数据整理？](http://josephorallo.webs.upv.es/escrits/MLJ-DataWranglingAutomation.pdf)'
- en: '[Large Language Models as Data Preprocessors](https://arxiv.org/abs/2308.16361)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[作为数据预处理器的大型语言模型](https://arxiv.org/abs/2308.16361)'
- en: '[Column Type Annotation using ChatGPT](https://arxiv.org/abs/2306.00745)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用ChatGPT进行列类型标注](https://arxiv.org/abs/2306.00745)'
- en: Score theory
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 得分理论
- en: Let’s dive into the concept of calculating the *Data Dirtiness Score* for a
    dataset, denoted as 𝒟. This dataset comprises I rows, representing individuals,
    and J columns, representing different variables.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨如何计算数据集的*数据脏污得分*，表示为 𝒟。该数据集包含 I 行，代表个体，以及 J 列，代表不同的变量。
- en: 'We introduce a matrix X, which is of the same dimensions as 𝒟, with I rows
    and J columns:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入一个与 𝒟 维度相同的矩阵 X，具有 I 行和 J 列：
- en: '![](../Images/9e7f620081e61c7931d40d94f0cbba78.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e7f620081e61c7931d40d94f0cbba78.png)'
- en: In this matrix, each element X_{ij} follows a Bernoulli distribution with parameter
    π_{ij}. The value of X_{ij} is set to 0 if the cell (i, j) in dataset 𝒟 is free
    from data issues, and 1 if there is an issue, with the probability 𝔼[X_{ij}] =
    π_{ij} indicating the likelihood of an issue being present.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个矩阵中，每个元素 X_{ij} 遵循一个参数为 π_{ij} 的伯努利分布。如果数据集 𝒟 中单元格 (i, j) 没有数据问题，则 X_{ij}
    的值为 0；如果有问题，则 X_{ij} 的值为 1，概率 𝔼[X_{ij}] = π_{ij} 表示问题发生的可能性。
- en: 'Next, we define a random variable Y that represents the proportion of cells
    in 𝒟 that are problematic. The formula for Y is given by:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个随机变量 Y，表示数据集 𝒟 中存在问题的单元格的比例。Y 的公式如下：
- en: '![](../Images/5b81b04e0127fadd5e595f155ada72ae.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b81b04e0127fadd5e595f155ada72ae.png)'
- en: 'The *Data Dirtiness Score* is then the expected value of Y:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据脏污度分数*是 Y 的期望值：'
- en: '![](../Images/9751e5fc469f30e2848fb5cde58471e8.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9751e5fc469f30e2848fb5cde58471e8.png)'
- en: 'To connect this back to our earlier discussion, the link between the confidence
    scores for each cell’s data error and the probabilities π_{ij} is captured by
    the following relationship:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将此与我们之前的讨论联系起来，每个单元格数据错误的置信度分数与概率 π_{ij} 之间的关系由以下公式表示：
- en: '![](../Images/6dd87d19370c09f8103e0cae7236a483.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd87d19370c09f8103e0cae7236a483.png)'
- en: This means that the probability of a cell being error-free is calculated as
    the product of the complements of the confidence scores for potential errors in
    that cell.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，单元格无错误的概率是通过该单元格潜在错误的置信度分数的补数相乘计算出来的。
- en: If all confidence scores are set to 1, indicating absolute certainty of errors,
    the dirtiness score simplifies to the proportion of cells with errors in the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有的置信度分数都设置为 1，表示对错误的绝对确定性，则脏污度分数简化为数据集中有错误的单元格的比例。
- en: 'Calculating the dirtiness score or the cleanliness score for a dataset essentially
    yields the same insight, just from different perspectives. The formula for the
    *Data Cleanliness Score* is simply one minus the *Data Dirtiness Score*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数据集的脏污度分数或清洁度分数本质上给出了相同的见解，只是从不同的角度来看。*数据清洁度分数*的公式只是 1 减去 *数据脏污度分数*：
- en: '![](../Images/b96f59c6e83b5836924288130a2d89f7.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b96f59c6e83b5836924288130a2d89f7.png)'
- en: In this way, a dataset with no errors at all would have a cleanliness score
    of 100% and a dirtiness score of 0%.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，完全没有错误的数据集将具有 100% 的清洁度分数和 0% 的脏污度分数。
- en: Changes
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变更
- en: 'EDIT-2024–03–21: Convert confidence values to ordinal categories: `low`, `medium`,
    `high`, and `certain`. These represent probabilities of 0.25, 0.5, 0.75, and 1,
    respectively.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑-2024–03–21：将置信度值转换为序数类别：`低`、`中`、`高` 和 `确定`。这些分别表示概率 0.25、0.5、0.75 和 1。
