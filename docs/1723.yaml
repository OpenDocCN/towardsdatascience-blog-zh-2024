- en: 'VerifAI Project: Open Source Biomedical Question Answering with Verified Answers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VerifAI项目：开源生物医学问答系统，提供经过验证的答案
- en: 原文：[https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15](https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15](https://towardsdatascience.com/verifai-project-open-source-biomedical-question-answering-with-verified-answers-5417cd9003e0?source=collection_archive---------4-----------------------#2024-07-15)
- en: Experiences from building LLM-based (Mistral 7B) biomedical question-answering
    system on top of Qdrant and OpenSearch indices with hallucination detection method
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于LLM（Mistral 7B）的生物医学问答系统，构建在Qdrant和OpenSearch索引之上，并结合幻觉检测方法的经验。
- en: '[](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)[![Nikola
    Milosevic (Data Warrior)](../Images/ebea6501c00030561a59a4a12ab7a79a.png)](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)
    [Nikola Milosevic (Data Warrior)](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)[![Nikola
    Milosevic (Data Warrior)](../Images/ebea6501c00030561a59a4a12ab7a79a.png)](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)
    [Nikola Milosevic (Data Warrior)](https://datawarrior.medium.com/?source=post_page---byline--5417cd9003e0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)
    ·12 min read·Jul 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5417cd9003e0--------------------------------)
    ·阅读时间：12分钟·2024年7月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Last September (2023), we embarked on the development of the [VerifAI project](https://verifai-project.com/),
    after receiving funding from the [NGI Search](https://www.ngisearch.eu/view/Main/)
    funding scheme of Horizon Europe.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 去年9月（2023年），我们开始了[VerifAI项目](https://verifai-project.com/)，该项目在获得[NGI Search](https://www.ngisearch.eu/view/Main/)计划的资助后启动，该计划隶属于欧盟地平线计划。
- en: The idea of the project was to create a generative search engine for the biomedical
    domain, based on vetted documents (therefore we used a repository of biomedical
    journal publications called [PubMed](https://pubmed.ncbi.nlm.nih.gov/)), with
    an additional model that would verify the generated answer, by comparing the referenced
    article and generated claim. In domains such as biomedicine, but also in general,
    in sciences, there is a low tolerance for hallucinations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的想法是创建一个面向生物医学领域的生成搜索引擎，基于经过验证的文献（因此我们使用了一个名为[PubMed](https://pubmed.ncbi.nlm.nih.gov/)的生物医学期刊文章库），并添加一个模型，通过比较参考文章和生成的声明来验证生成的答案。在生物医学领域，甚至在科学领域，幻觉（错误信息）的容忍度非常低。
- en: While there are projects and products, such as Elicit or Perplexity, that do
    partially RAG (Retrieval-Augmented Generation) and can answer and reference documents
    for biomedical questions, there are a few factors that differentiate our project.
    Firstly, we focus at the moment on the biomedical documents. Secondly, as this
    is a funded project by the EU, we commit to open-source everything that we have
    created, from source code, models, model adapters, datasets, everything. Thirdly,
    no other product, that is available at the moment, does a posteriori verification
    of the generated answer, but they usually just rely on fairly simple RAG, which
    reduces hallucinations but does not remove them completely. One of the main aims
    of the project is to address the issue of so-called hallucinations. Hallucinations
    in Large Language Models (LLMs) refer to instances where the model generates text
    that is plausible-sounding but factually incorrect, misleading, or nonsensical.
    In this regard, the project adds to the live system's unique value.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像Elicit或Perplexity这样的项目和产品确实进行了一部分RAG（检索增强生成），能够回答生物医学问题并引用相关文档，但有几个因素使我们的项目与众不同。首先，我们目前专注于生物医学文档。其次，作为一个由欧盟资助的项目，我们承诺将所有创建的内容（包括源代码、模型、模型适配器、数据集等）开源。第三，目前没有其他产品能够对生成的答案进行事后验证，它们通常只是依赖相对简单的RAG，这可以减少幻觉现象，但无法完全消除它们。该项目的主要目标之一是解决所谓的幻觉问题。大语言模型（LLM）中的幻觉是指模型生成的文本看起来合理，但实际上是事实错误、误导性或无意义的。在这方面，该项目为实时系统增添了独特的价值。
- en: The project has been shared under the [AGPLv3 license](https://www.gnu.org/licenses/agpl-3.0.en.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目已根据[AGPLv3许可证](https://www.gnu.org/licenses/agpl-3.0.en.html)共享。
- en: Overall method
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整体方法
- en: The overall methodology that we have applied can be seen in the following figure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用的整体方法可以从下图中看到。
- en: '![](../Images/163f6581ef22f6654334708a5f9d3f80.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/163f6581ef22f6654334708a5f9d3f80.png)'
- en: The overall architecture of the system (image by authors)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的整体架构（作者提供的图像）
- en: When a user asks a question, the user query is transformed into a query, and
    the information retrieval engine is asked for the most relevant biomedical abstracts,
    indexed in PubMed, for the given question. In order to obtain the most relevant
    documents, we have created both a lexical index, based on [OpenSearch](https://opensearch.org/),
    and a vector/semantic search based on [Qdrant](https://qdrant.tech/). Namely,
    lexical search is great at retrieving relevant documents containing exact terms
    as the query, while semantic search helps us to search semantic space and retrieve
    documents that mean the same things, but may phrase it differently. The retrieved
    scores are normalized and a combination of documents from these two indices are
    retrieved (hybrid search). The top documents from the hybrid search, with questions,
    are passed into the context of a model for answer generation. In our case, we
    have fine-tuned the Mistral 7B-instruct model, using QLoRA, because this allowed
    us to host a fairly small well-performing model on a fairly cheap cloud instance
    (containing NVidia Tesla T4 GPU with 16GB of GPU RAM). After the answer is generated,
    the answer is parsed into sentences and references that support these sentences
    and passed to the separate model that checks whether the generated claim is supported
    by the content of the referenced abstract. This model classifies claims into supported,
    no-evidence, and contradicting classes. Finally, the answer, with highlighted
    claims that may not be fully supported by the abstracts, is presented to the user.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出问题时，用户查询会被转换成查询，信息检索引擎会根据给定的问题向PubMed索引中检索最相关的生物医学摘要。为了获取最相关的文档，我们创建了一个基于[OpenSearch](https://opensearch.org/)的词汇索引和一个基于[Qdrant](https://qdrant.tech/)的向量/语义搜索。具体来说，词汇搜索非常适合检索包含查询中确切术语的相关文档，而语义搜索帮助我们在语义空间中搜索，并检索那些意思相同但措辞不同的文档。检索得分会被归一化，然后从这两个索引中结合提取文档（混合搜索）。来自混合搜索的顶级文档和问题会传递给模型的上下文进行答案生成。在我们的案例中，我们使用QLoRA微调了Mistral
    7B-instruct模型，因为这样我们可以在一个相对便宜的云实例上托管一个表现良好的小型模型（该实例包含NVidia Tesla T4 GPU和16GB的GPU内存）。答案生成后，答案会被解析成句子，并引用支持这些句子的文献，然后传递给一个独立的模型，检查生成的声明是否得到引用摘要内容的支持。该模型将声明分类为支持、无证据和矛盾三类。最后，答案和可能未被摘要完全支持的声明将被呈现给用户。
- en: Therefore, the system has 3 components — information retrieval, answer generation,
    and answer verification. In the following sections, we will describe in more detail
    each of these sections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系统有三个组件——信息检索、答案生成和答案验证。在接下来的部分，我们将更详细地描述这些组件。
- en: Information retrieval
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息检索
- en: From the start of the project, we aimed at building a hybrid search, combining
    semantic and lexical search. The initial idea was to create it using a single
    software, however, that turned out not so easy, especially for the index of PubMed’s
    size. PubMed contains about 35 million documents, however, not all contain full
    abstracts. There are old documents, from the 1940s and 1950s that may not have
    abstracts, but as well some guide documents and similar that have only titles.
    We have indexed only the documents containing full abstracts and ended up with
    about 25 million documents. PubMed unpacked is about 120GB in size.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从项目开始，我们的目标是构建一个混合搜索，结合语义搜索和词汇搜索。最初的想法是使用单一软件来实现，然而，这并不是那么容易，特别是对于PubMed的索引大小来说。PubMed包含大约3500万篇文献，但并非所有文献都包含完整的摘要。有些较旧的文献，来自1940年代和1950年代，可能没有摘要，还有一些指南文档等，仅包含标题。我们只对包含完整摘要的文献进行了索引，最终得到了约2500万篇文献。解压后的PubMed大约为120GB大小。
- en: It is not problematic to create a well-performing index for lexical search in
    OpenSearch. This worked pretty much out of the box. We have indexed titles, and
    abstract texts, and added some metadata for filtering, like year of publication,
    authors, and journal. OpenSearch supports [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)
    as a vector store. So we tried to index our data with FAISS, but this was not
    possible, as the index was too large and we were running out of memory (we had
    a 64GB cloud instance for index). The indexing was done using MSMarco fine-tuned
    model based on DistilBERT ([sentence-transformers/msmarco-distilbert-base-tas-b](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b)).
    Since we learned that FAISS only supported the in-memory index, we needed to find
    another solution that would be able to store a part of the index on the hard drive.
    The solution was found in the Qdrant database, as it supports in-memory [mapping
    and on-disk storage of part of the index](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenSearch中创建一个高效的词汇搜索索引并不难。这几乎可以开箱即用。我们对标题、摘要文本进行了索引，并添加了一些用于过滤的元数据，如出版年份、作者和期刊。OpenSearch支持[FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)作为向量存储。因此，我们尝试使用FAISS来索引我们的数据，但由于索引太大，且内存不足（我们为索引配置了64GB的云实例），这一尝试未能成功。索引是使用基于DistilBERT的MSMarco微调模型进行的（[sentence-transformers/msmarco-distilbert-base-tas-b](https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b)）。由于我们了解到FAISS仅支持内存中的索引，因此我们需要找到一种解决方案，可以将部分索引存储在硬盘上。这个解决方案找到了——Qdrant数据库，它支持内存[映射和部分索引的磁盘存储](https://qdrant.tech/documentation/concepts/storage/#configuring-memmap-storage)。
- en: Another issue that appeared while creating the index was that once we did memory
    mapping and created the whole PubMed index, the query would be executed for a
    long time (almost 30 seconds). The problem was that calculations of dot products
    in 32-bit precision were taking a while for the index having 25 million documents
    (and potentially loading parts of the index from HDD). Therefore, we have made
    a search using only 8-bit precision, and we have reduced the time needed from
    about 30 seconds to less than a half second.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建索引时，出现的另一个问题是，一旦我们进行了内存映射并创建了整个PubMed索引，查询的执行时间会非常长（几乎30秒）。问题出在，使用32位精度计算点积时，对于包含2500万篇文献的索引来说，耗时较长（并且可能从硬盘加载索引部分）。因此，我们使用了仅采用8位精度的搜索方法，将所需时间从约30秒减少到了不到半秒。
- en: The lexical index contained whole abstracts, however, for the semantic index,
    documents needed to be split, because the transformer model we used for building
    the semantic index could swallow 512 tokens. Therefore the documents were split
    on full stop before the 512th token, and the same happened on every next 512 tokens.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇索引包含完整的摘要，而语义索引则需要对文献进行拆分，因为我们用来构建语义索引的变换器模型最多能处理512个tokens。因此，文献在第512个token之前会根据句号进行拆分，每当达到下一个512个tokens时，都会进行相同的操作。
- en: In order to create a combination of semantic and lexical search, we have normalized
    the outputs of the queries, by dividing all scores returned either from Qdrant
    or OpenSearch by the top score returned for the given document store. In that
    way, we have obtained two numbers, one for semantic and the other for lexical
    search, in the range between 0–1\. Then we tested the precision of retrieved most
    relevant documents in the top retrieved documents using the [BioASQ dataset](https://link.springer.com/content/pdf/10.1186/s12859-015-0564-6.pdf).
    The results can be seen in the table below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建语义搜索和词汇搜索的结合，我们对查询的输出进行了规范化，将Qdrant或OpenSearch返回的所有分数除以给定文档库中返回的最高分。这样，我们得到了两个数值，一个代表语义搜索，另一个代表词汇搜索，范围在0到1之间。然后，我们使用[BioASQ数据集](https://link.springer.com/content/pdf/10.1186/s12859-015-0564-6.pdf)测试了在检索的最相关文档中的精度。结果可以在下表中看到。
- en: '![](../Images/e0fe3ef1fc39780b52d3e81bb23e2fdc.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0fe3ef1fc39780b52d3e81bb23e2fdc.png)'
- en: The results of information retrieval, evaluating weights of semantic and lexical
    search. As it can be seen, best results we got for lexical search weight of 0.7
    and semantic 0.3\. Image from our paper accepted on BioNLP, and preprint available
    at [https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索的结果，评估语义和词汇搜索的权重。如图所示，最佳结果出现在词汇搜索权重为0.7，语义搜索权重为0.3时。图像来自我们已在BioNLP上接受的论文，预印本可在[https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)查看。
- en: We have done some re-ranking experiments using full precision, so you can see
    more details in the paper. But this has not been used in the application at the
    end. The overall conclusion was that lexical search does a pretty good job, and
    there is some contribution of semantic search, with the best performance obtained
    at weights of 0.7 for lexical search, and 0.3 for semantic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一些使用全精度的重新排序实验，因此可以在论文中看到更多细节。但这在最终的应用中没有使用。总体结论是，词汇搜索表现得相当不错，语义搜索也有一定的贡献，在词汇搜索权重为0.7，语义搜索权重为0.3时获得了最佳性能。
- en: Finally, we have built a query processing, where for lexical querying stopwords
    were excluded from the query, and search was performed in the lexical index, and
    similarity was calculated for the semantic index. Values for documents from both
    semantic and lexical indexes were normalized and summed up, and the top 10 documents
    were retrieved.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们构建了一个查询处理系统，对于词汇查询，查询中排除了停用词，并在词汇索引中进行了搜索，同时计算语义索引的相似度。来自语义和词汇索引的文档值都进行了规范化并求和，然后检索到前10个文档。
- en: Referenced answer generation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考答案生成
- en: Once the top 10 documents were retrieved, we could pass these documents to a
    generative model for referenced answer generation. We have tested several models.
    This can be done well with GPT4-Turbo models, and most of commercially available
    platforms would use GPT4 or Claude models. However, we wanted to create an open-source
    variant, where we do not depend on commercial models, having smaller and more
    efficient models while having also performance close to the performance of commercial
    models. Therefore, we have tested things with Mistral 7B instruct in both zero-shot
    regime and fine-tuned using 4bit QLora.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检索到前10个文档，我们可以将这些文档传递给生成模型来生成参考答案。我们已经测试了几种模型。使用GPT4-Turbo模型可以很好地完成这项工作，大多数商业平台也会使用GPT4或Claude模型。然而，我们希望创建一个开源版本，避免依赖商业模型，同时使用更小、更高效的模型，并且其性能接近商业模型。因此，我们在零-shot模式下，使用4bit
    QLora进行微调，测试了Mistral 7B instruct模型。
- en: In order to fine-tune Mistral, we needed to create a dataset for referenced
    question answering using PubMed. We created a dataset by randomly selecting questions
    from the PubMedQA dataset, then retrieved the top 10 relevant documents and used
    GPT-4 Turbo for referenced answer generation. We have called this dataset the
    [PQAref dataset and published it on HuggingFace](https://huggingface.co/datasets/BojanaBas/PQAref).
    Each sample in this dataset contains a question, a set of 10 documents, and a
    generated answer with referenced documents (based on 10 passed in context).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了微调Mistral，我们需要创建一个使用PubMed的参考问答数据集。我们通过随机选择PubMedQA数据集中的问题来创建数据集，然后检索前10个相关文档，并使用GPT-4
    Turbo生成参考答案。我们将这个数据集称为[PQAref数据集，并已发布在HuggingFace上](https://huggingface.co/datasets/BojanaBas/PQAref)。该数据集中的每个样本包含一个问题、一组10个文档和一个生成的答案，答案基于10个传入上下文的参考文档。
- en: Using this dataset, we have created a QLoRA adapter for Mistral-7B-instruct.
    This was trained on the [Serbian National AI platform](https://www.ai.gov.rs/)
    in the National Data Center of Serbia, using Nvidia A100 GPU. The training lasted
    around 32 hours.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个数据集，我们为Mistral-7B-instruct创建了一个QLoRA适配器。该适配器在[塞尔维亚国家AI平台](https://www.ai.gov.rs/)的塞尔维亚国家数据中心进行训练，使用了Nvidia
    A100 GPU，训练时长大约为32小时。
- en: 'We have performed an evaluation comparing Mistral 7B instruct v1, and Mistral
    7B instruct v2, with and without QLoRA fine-tuning (so without is zero-shot, based
    only on instruction, while with QLoRA we could save some tokens as instruction
    was not necessary for the prompt, as fine-tuning would make model do what is needed),
    and we compared it with GPT-4 Turbo (with prompt: “Answer the question using relevant
    abstracts provided, up to 300 words. Reference the statements with the provided
    abstract_id in brackets next to the statement.”). Several evaluations about the
    number of referenced documents and whether the referenced documents are relevant
    have been done. These results can be seen in the tables below.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项评估，比较了Mistral 7B instruct v1与Mistral 7B instruct v2，在有无QLoRA微调的情况下（没有QLoRA微调时为零-shot，仅基于指令，使用QLoRA时我们可以节省一些token，因为提示中不再需要指令，微调后的模型会根据需求执行任务），并且将其与GPT-4
    Turbo进行了比较（提示：“使用相关摘要回答问题，最多300字。参考所提供的abstract_id并将其放在陈述旁边的括号中。”）。我们进行了多项评估，关于引用文献的数量以及这些引用文献是否相关。以下表格展示了这些结果。
- en: '![](../Images/e0db2558d174c7bed72de6a1027c4193.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0db2558d174c7bed72de6a1027c4193.png)'
- en: The number of answers containing N references in various models. Image from
    our paper accepted on BioNLP, and preprint available at [https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 各种模型中包含N个引用文献的答案数量。图片来自我们在BioNLP接受的论文，预印本可见于[https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)
- en: From this table, it can be concluded that Mistral, especially the first version
    in zero-shot (0-M1), does not really often reference context documents, even though
    it was requested in the prompt. On the other hand, the second version showed much
    better performance, but it was far from GPT4-Turbo or fine-tuned Mistrals 7B.
    Fine-tuned Mistrals tended to cite more documents, even if the answer could be
    found in one of the documents, and added some additional information compared
    to GPT4-Turbo.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张表格可以得出结论，Mistral，特别是零-shot的第一版本（0-M1），即使提示中要求引用上下文文献，它也并不经常引用文献。另一方面，第二个版本表现出了更好的性能，但仍然远不及GPT4-Turbo或微调后的Mistral
    7B。微调后的Mistral往往会引用更多文献，即使答案可以从某一文献中找到，而且比GPT4-Turbo多出一些额外信息。
- en: '![](../Images/2f7c54fb55fd41528ce8fe8fe630aefc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f7c54fb55fd41528ce8fe8fe630aefc.png)'
- en: A number of referenced relevant documents for various models. Image from our
    paper accepted on BioNLP, and preprint available at [https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 各种模型中引用的相关文献数量。图片来自我们在BioNLP接受的论文，预印本可见于[https://arxiv.org/abs/2407.05015v1](https://arxiv.org/abs/2407.05015v1)
- en: As can be seen from the second table, GPT4-Turbo missed relevant references
    only once in the whole test set, while Mistral 7B-instruct v2 with fine-tuning
    missed a bit more, but showed still comparable performance, given much smaller
    model size.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 从第二个表格可以看出，GPT4-Turbo在整个测试集中只错过了一次相关引用，而Mistral 7B-instruct v2微调后错过的相关引用稍多，但考虑到其模型规模更小，表现仍然相当。
- en: We have also looked at several answers manually, to make sure the answers make
    sense. In the end, in the app, we are using Mistral 7B instruct v2 with a fine-tuned
    QLoRA adapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还手动查看了多个答案，以确保答案是合理的。最终，在应用中，我们使用的是Mistral 7B instruct v2，并配合微调的QLoRA适配器。
- en: Answer verification
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案验证
- en: The final part of the system is answer verification. For answer verification,
    we have developed several features, however, the main one is a system that verifies
    whether the generated claim is based on the abstract that was referenced. We have
    fine-tuned several models on the [SciFact dataset](https://allenai.org/data/scifact)
    from the Allen Institute for AI with several BERT and Roberta-based models.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 系统的最终部分是答案验证。对于答案验证，我们开发了多个功能，但主要的是一个系统，它验证生成的声明是否基于所引用的摘要。我们在[Allen Institute
    for AI](https://allenai.org/data/scifact)的[SciFact数据集](https://allenai.org/data/scifact)上对多个基于BERT和Roberta的模型进行了微调。
- en: In order to model input, we have parsed the answer to find sentences and related
    references. We have found that the first and last sentence in our system are often
    introduction or conclusion sentences, and may not be referenced. All other sentences
    should have a reference. If a sentence contained a reference, it was based on
    that PubMed document. If the sentence does not contain a reference, but the sentence
    before and after are referenced, we calculate the dot product of the embeddings
    of that sentence and sentences in 2 abstracts. Whichever abstract contains the
    sentence with the highest dot product is treated as the abstract that the sentence
    was based on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理模型输入，我们解析了答案以找到句子和相关参考文献。我们发现系统中的第一句和最后一句通常是引言或结论句，可能没有引用。所有其他句子都应该有引用。如果一个句子包含引用，它是基于该PubMed文档的。如果句子没有引用，但前后句有引用，我们计算该句子与2个摘要中的句子的嵌入点积。点积最大的摘要被认为是该句子的来源摘要。
- en: Once the answer is parsed and we have found all the abstracts the claims are
    based on, we pass it to the fine-tuned model. The input to the model was engineered
    in the following way
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦答案被解析，并且我们找到了所有与理赔相关的摘要，我们将其传递给微调后的模型。输入模型的方式如下所示：
- en: 'For deBERT-a models:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于deBERT-a模型：
- en: '[CLS]claim[SEP]evidence[SEP]'
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[CLS]claim[SEP]evidence[SEP]'
- en: 'For Roberta-based models:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于Roberta的模型：
- en: <s>claim</s></s>evidence</s>
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <s>claim</s></s>evidence</s>
- en: Where the claim is a generated claim from the generative component, and evidence
    is the text of concatenated title and abstract from referenced the PubMed document.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，claim是生成组件生成的理赔，evidence是来自PubMed文献中引用的标题和摘要拼接文本。
- en: 'We have evaluated the performance of our fine-tuned models and obtained the
    following results:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了微调模型的性能，并得到了以下结果：
- en: '![](../Images/41d8876e8eca04a94a98c3f4070923d2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41d8876e8eca04a94a98c3f4070923d2.png)'
- en: Evaluation of the models trained and tested on the SciFact dataset. Image by
    authors, publication submitted to the 16th International Conference on Knowledge
    Management and Information Systems
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在SciFact数据集上训练和测试的模型评估。图片由作者提供，论文已提交至第16届国际知识管理与信息系统会议。
- en: 'Often, when the models are fine-tuned on the same dataset as they are tested,
    the results are good. Therefore, we wanted to test it also on out-of-domain data.
    So we have selected the [HealthVer dataset](https://github.com/sarrouti/HealthVer),
    which is also in the healthcare domain used for claim verification. The results
    were the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当模型在与测试数据集相同的数据集上进行微调时，结果表现良好。因此，我们希望在域外数据上进行测试。所以我们选择了[HealthVer数据集](https://github.com/sarrouti/HealthVer)，它也属于医疗健康领域，主要用于理赔验证。测试结果如下：
- en: '![](../Images/d1e9c32a15dfed4cb8edbcc0e821e316.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1e9c32a15dfed4cb8edbcc0e821e316.png)'
- en: Results of testing on the HealthVer dataset. Image by authors, publication submitted
    to the 16th International Conference on Knowledge Management and Information Systems
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在HealthVer数据集上的测试结果。图片由作者提供，论文已提交至第16届国际知识管理与信息系统会议。
- en: We also evaluated the SciFact label prediction task using the GPT-4 model (with
    a prompt “Critically asses whether the statement is supported, contradicted or
    there is no evidence for the statement in the given abstract. Output SUPPORT if
    the statement is supported by the abstract. Output CONTRADICT if the statement
    is in contradiction with the abstract and output NO EVIDENCE if there is no evidence
    for the statement in the abstract.” ), resulting in a precision of 0.81, a recall
    of 0.80, and an F-1 score of 0.79\. Therefore, our model has better performance
    and due to the much lower number of parameters was more efficient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用GPT-4模型评估了SciFact标签预测任务（提示语为：“批判性地评估该声明是否得到支持、被反驳，或者在给定摘要中没有证据支持该声明。如果声明得到摘要的支持，请输出SUPPORT；如果声明与摘要矛盾，请输出CONTRADICT；如果摘要中没有支持该声明的证据，请输出NO
    EVIDENCE。”），得出了精确度为0.81、召回率为0.80、F-1分数为0.79的结果。因此，我们的模型表现更好，且由于参数数量显著更少，效率也更高。
- en: On top of the verification using this model, we have also calculated the closest
    sentence from the abstract using dot product similarity (using the same MSMarco
    model we use for semantic search). We visualize the closest sentence to the generated
    one on the user interface by hovering over a sentence.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用该模型进行验证的基础上，我们还通过点积相似度计算了摘要中与生成句子最接近的句子（使用与语义搜索相同的MSMarco模型）。我们通过在用户界面上悬停句子来可视化与生成句子最接近的句子。
- en: User interface
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用户界面
- en: 'We have developed a user interface, to which users can register, log in, and
    ask questions, where they will get referenced answers, links to PubMed, and verification
    by described posterior model. Here are a few screenshots of the user interface:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个用户界面，用户可以在其中注册、登录并提问，系统将提供参考答案、链接到PubMed以及通过描述的后验模型进行验证。以下是该用户界面的几个截图：
- en: '![](../Images/3e148129c2d3efd0e0c73620cf2f81cf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e148129c2d3efd0e0c73620cf2f81cf.png)'
- en: The user interface, while generating an answer. Screenshot by authors
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面正在生成答案时的截图。截图由作者提供
- en: '![](../Images/7ba6d960364f81265ca465c1d6d95169.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ba6d960364f81265ca465c1d6d95169.png)'
- en: Output, including verification. Screenshot by authors
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果，包括验证。截图由作者提供
- en: '![](../Images/26687e1879a7235178cfd547c073b2c8.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26687e1879a7235178cfd547c073b2c8.png)'
- en: Generating a different answer and the configuration window open. Screenshot
    by authors
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 生成不同答案并打开配置窗口的截图。截图由作者提供
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/60b1a4d98091e8f73da0e645399a2596.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60b1a4d98091e8f73da0e645399a2596.png)'
- en: VerifAI project logo. Logo by authors
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: VerifAI 项目logo。Logo由作者提供
- en: We are here to present our experience from building the VerifAI project, which
    is the biomedical generative question-answering engine with verifiable answers.
    We are open-sourcing whole code and models and we are opening the application,
    at least temporarily (depends on the budget for how long, and how and whether
    we can find a sustainable solution for hosting). In the next section, you can
    find the links to the application, website, code, and models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里分享构建VerifAI项目的经验，这是一个生物医学生成式问答引擎，能够提供可验证的答案。我们将开源整个代码和模型，并且暂时（具体时间取决于预算及是否能找到可持续的托管方案）开放应用程序。在下一部分中，您可以找到应用程序、网站、代码和模型的链接。
- en: The application is a result of the work of multiple people (see under Team section)
    and almost a year of research and development. We are happy and proud to present
    it now to a wider public and hope that people will enjoy it, and as well contribute
    to it, to make it more sustainable and better in the future.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序是多个人（请参见团队部分）和近一年研究与开发工作的成果。我们很高兴并自豪地将其呈现给更广泛的公众，并希望大家会喜欢它，同时也能为其做出贡献，推动它在未来变得更加可持续和更好。
- en: Cite our papers
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用我们的论文
- en: 'If you use any of the methodology, models, datasets, or are mentioning this
    project in your paper’s background section, please cite some of the following
    papers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在论文的背景部分使用了任何方法论、模型、数据集，或者提到了这个项目，请引用以下部分论文：
- en: '[Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano,
    Nikola Milošević, “Scientific QA System with Verifiable Answers”, The 6th International
    Open Search Symposium 2024](https://arxiv.org/pdf/2407.11485)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Adela Ljajić, Miloš Košprdić, Bojana Bašaragin, Darija Medvecki, Lorenzo Cassano,
    Nikola Milošević, “Scientific QA System with Verifiable Answers”, The 6th International
    Open Search Symposium 2024](https://arxiv.org/pdf/2407.11485)'
- en: '[Košprdić, M., Ljajić, A., Bašaragin, B., Medvecki, D., & Milošević, N. “Verif.
    ai: Towards an Open-Source Scientific Generative Question-Answering System with
    Referenced and Verifiable Answers.” The Sixteenth International Conference on
    Evolving Internet INTERNET 2024 (2024).](https://arxiv.org/pdf/2402.18589.pdf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Košprdić, M., Ljajić, A., Bašaragin, B., Medvecki, D., & Milošević, N. “Verif.
    ai: Towards an Open-Source Scientific Generative Question-Answering System with
    Referenced and Verifiable Answers.” The Sixteenth International Conference on
    Evolving Internet INTERNET 2024 (2024).](https://arxiv.org/pdf/2402.18589.pdf)'
- en: '[Bojana Bašaragin, Adela Ljajić, Darija Medvecki, Lorenzo Cassano, Miloš Košprdić,
    Nikola Milošević “How do you know that? Teaching Generative Language Models to
    Reference Answers to Biomedical Questions”, The 23rd BioNLP Workshop 2024, Colocated
    with ACL 2024](https://arxiv.org/abs/2407.05015)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bojana Bašaragin, Adela Ljajić, Darija Medvecki, Lorenzo Cassano, Miloš Košprdić,
    Nikola Milošević “How do you know that? Teaching Generative Language Models to
    Reference Answers to Biomedical Questions”, The 23rd BioNLP Workshop 2024, Colocated
    with ACL 2024](https://arxiv.org/abs/2407.05015)'
- en: Availability
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用性
- en: The application can be accessed and tried at [https://verifai-project.com/](https://verifai-project.com/)
    or [https://app.verifai-project.com/](https://app.verifai-project.com/). Users
    can here register, and ask questions to try our platform.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序可以通过 [https://verifai-project.com/](https://verifai-project.com/) 或 [https://app.verifai-project.com/](https://app.verifai-project.com/)
    访问和尝试。用户可以在此注册并提问，尝试我们的平台。
- en: The code of the project is available at GitHub at [https://github.com/nikolamilosevic86/verif.ai](https://github.com/nikolamilosevic86/verif.ai).
    The QLoRA adapters for Mistra 7B instruct can be found on HuggingFace at [https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10](https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10)
    and [https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10](https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10).
    A generated dataset for fine-tuning the generative component can be found also
    on HuggingFace at [https://huggingface.co/datasets/BojanaBas/PQAref](https://huggingface.co/datasets/BojanaBas/PQAref).
    A model for answer verification can be found at [https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT](https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的代码可以在GitHub上找到，链接为[https://github.com/nikolamilosevic86/verif.ai](https://github.com/nikolamilosevic86/verif.ai)。Mistra
    7B instruct的QLoRA适配器可以在HuggingFace上找到，链接为[https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10](https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.2-pqa-10)
    和 [https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10](https://huggingface.co/BojanaBas/Mistral-7B-Instruct-v0.1-pqa-10)。用于微调生成组件的生成数据集也可以在HuggingFace上找到，链接为[https://huggingface.co/datasets/BojanaBas/PQAref](https://huggingface.co/datasets/BojanaBas/PQAref)。一个用于答案验证的模型可以在[https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT](https://huggingface.co/MilosKosRad/TextualEntailment_DeBERTa_preprocessedSciFACT)找到。
- en: Team
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 团队
- en: The VerifAI project was developed as a collaborative project between Bayer Pharma
    R&D and the Institute for Artificial Intelligence Research and Development of
    Serbia, funded by the NGI Search project under grant agreement No 101069364\.
    The people involved are [Nikola Milosevic](https://www.linkedin.com/in/nikolamilosevic1986/),
    [Lorenzo Cassano](https://www.linkedin.com/in/lorenzo-cassano-360b631b1/), [Bojana
    Bašaragin](https://www.linkedin.com/in/bojana-basaragin/), [Miloš Košprdić](https://www.linkedin.com/in/milo%C5%A1-ko%C5%A1prdi%C4%87/),
    [Adela Ljajić](https://www.linkedin.com/in/adelacrnisanin/) and [Darija Medvecki](https://www.linkedin.com/in/darija-medvecki-b27468ab/).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: VerifAI项目是由拜耳制药研发部和塞尔维亚人工智能研究与发展研究所合作开发的，项目资金由NGI Search项目提供，资助协议号为101069364。参与人员包括[Nikola
    Milosevic](https://www.linkedin.com/in/nikolamilosevic1986/)、[Lorenzo Cassano](https://www.linkedin.com/in/lorenzo-cassano-360b631b1/)、[Bojana
    Bašaragin](https://www.linkedin.com/in/bojana-basaragin/)、[Miloš Košprdić](https://www.linkedin.com/in/milo%C5%A1-ko%C5%A1prdi%C4%87/)、[Adela
    Ljajić](https://www.linkedin.com/in/adelacrnisanin/) 和 [Darija Medvecki](https://www.linkedin.com/in/darija-medvecki-b27468ab/)。
- en: '![](../Images/3314065315cb101044ab1bf28aa16b5e.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3314065315cb101044ab1bf28aa16b5e.png)'
- en: If you would like to put faces to the team — Team (with some family members)
    at Prater Berlin Biergarten (image by authors, 10th July 2024)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想看到团队成员的面孔 — 团队（以及一些家人）在柏林普拉特啤酒花园的合影（作者提供，2024年7月10日）
