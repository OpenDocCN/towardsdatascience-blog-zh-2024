- en: The Math Behind KAN — Kolmogorov-Arnold Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: KAN 背后的数学 — 科尔莫戈罗夫-阿诺德网络
- en: 原文：[https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12](https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12](https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12)
- en: A new alternative to the classic Multi-Layer Perceptron is out. Why is it more
    accurate and interpretable? Math and Code Deep Dive.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种新的替代经典多层感知器的神经网络出现了。为什么它更准确且具有更好的可解释性？深入探讨数学与代码。
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    ·13 min read·Jun 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    ·13分钟阅读·2024年6月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/87b3c9f64523d22f8562872e8b827883.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87b3c9f64523d22f8562872e8b827883.png)'
- en: Image generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由DALL-E生成
- en: 'In today’s world of AI, neural networks drive countless innovations and advancements.
    At the heart of many breakthroughs is the Multi-Layer Perceptron (MLP), a type
    of neural network known for its ability to approximate complex functions. But
    as we push the boundaries of what AI can achieve, we must ask: Can we do better
    than the classic MLP?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今的人工智能世界中，神经网络推动了无数创新与进步。许多突破的核心是多层感知器（MLP），这是一种以逼近复杂函数能力著称的神经网络。但随着我们不断拓展人工智能的边界，我们不得不问：我们能比经典的多层感知器做得更好吗？
- en: Here’s Kolmogorov-Arnold Networks (KANs), a new approach to neural networks
    inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional MLPs,
    which use fixed activation functions at each neuron, KANs use learnable activation
    functions on the edges (weights) of the network. This simple shift opens up new
    possibilities in accuracy, interpretability, and efficiency.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是科尔莫戈罗夫-阿诺德网络（KAN），一种受科尔莫戈罗夫-阿诺德表示定理启发的新型神经网络方法。与传统的多层感知器（MLP）不同，后者在每个神经元上使用固定的激活函数，而KAN则在网络的边缘（权重）上使用可学习的激活函数。这个简单的变化为准确性、可解释性和效率开辟了新的可能性。
- en: This article explores why KANs are a revolutionary advancement in neural network
    design. We’ll dive into their mathematical foundations, highlight the key differences
    from MLPs, and show how KANs can outperform traditional methods.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了为什么KAN是神经网络设计中的革命性进展。我们将深入分析它们的数学基础，强调它们与MLP的关键差异，并展示KAN如何超越传统方法。
- en: '1: Limitations of MLPs'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1：多层感知器的局限性
