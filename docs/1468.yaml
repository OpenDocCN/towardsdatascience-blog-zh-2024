- en: The Math Behind KAN — Kolmogorov-Arnold Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12](https://towardsdatascience.com/the-math-behind-kan-kolmogorov-arnold-networks-7c12a164ba95?source=collection_archive---------0-----------------------#2024-06-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A new alternative to the classic Multi-Layer Perceptron is out. Why is it more
    accurate and interpretable? Math and Code Deep Dive.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--7c12a164ba95--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c12a164ba95--------------------------------)
    ·13 min read·Jun 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87b3c9f64523d22f8562872e8b827883.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: 'In today’s world of AI, neural networks drive countless innovations and advancements.
    At the heart of many breakthroughs is the Multi-Layer Perceptron (MLP), a type
    of neural network known for its ability to approximate complex functions. But
    as we push the boundaries of what AI can achieve, we must ask: Can we do better
    than the classic MLP?'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s Kolmogorov-Arnold Networks (KANs), a new approach to neural networks
    inspired by the Kolmogorov-Arnold representation theorem. Unlike traditional MLPs,
    which use fixed activation functions at each neuron, KANs use learnable activation
    functions on the edges (weights) of the network. This simple shift opens up new
    possibilities in accuracy, interpretability, and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: This article explores why KANs are a revolutionary advancement in neural network
    design. We’ll dive into their mathematical foundations, highlight the key differences
    from MLPs, and show how KANs can outperform traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: '1: Limitations of MLPs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
