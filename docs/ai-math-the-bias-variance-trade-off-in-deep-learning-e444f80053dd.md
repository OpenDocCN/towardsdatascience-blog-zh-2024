# AI 数学：深度学习中的偏差-方差权衡

> 原文：[https://towardsdatascience.com/ai-math-the-bias-variance-trade-off-in-deep-learning-e444f80053dd?source=collection_archive---------2-----------------------#2024-11-29](https://towardsdatascience.com/ai-math-the-bias-variance-trade-off-in-deep-learning-e444f80053dd?source=collection_archive---------2-----------------------#2024-11-29)

## 从经典统计学到深度学习的细微差别的视觉之旅

[](https://medium.com/@TarikDzekman?source=post_page---byline--e444f80053dd--------------------------------)[![Tarik Dzekman](../Images/0c66b22ecbdbbce79b2516e555c67432.png)](https://medium.com/@TarikDzekman?source=post_page---byline--e444f80053dd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e444f80053dd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e444f80053dd--------------------------------) [Tarik Dzekman](https://medium.com/@TarikDzekman?source=post_page---byline--e444f80053dd--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e444f80053dd--------------------------------) ·阅读时长47分钟·2024年11月29日

--

![](../Images/f11bbf2a6adb14782f156aecfeccb623.png)

来源：除非另有说明，所有图片均为作者提供。

在深度学习中，偏差-方差权衡并非简单明了，且通常可能不是我们应该关注的关键问题。要理解原因，我们需要通过推理统计学、经典统计学习方法以及机器学习的鲁棒性进行一次回顾。我们将在文章的最后讨论过度参数化和双重下降现象。

*建议背景：概率论、随机变量、统计学、线性代数、微积分、机器学习、深度学习。*

# 推理统计中的偏差与方差

*注意：在本节中，我们将为了视觉直觉而简化一些数学内容。鉴于我关注的是深度学习，推理统计的细节会让已经很长的文章更加冗长。*

假设你穿越回二战时期，成为盟军指挥部的一名统计学家。一名情报官员告诉你以下信息：

1.  德国人在他们的坦克上刻上连续的序列号。因此，序列号为115的坦克意味着它是第115辆生产的坦克。迄今为止，德国生产了一个未知数量的坦克（*N*）。

1.  当盟军摧毁一辆坦克时，我们可以在其上找到一个序列号。坦克的“可摧毁性”与其序列号无关。

1.  我们有一个序列号样本（大小为 *k*），*X = (x₁, x₂, … xₖ)*。

1.  我们需要利用这个样本来创建一个估计量 *N**。

![](../Images/ac8f081d8cfd8c2f6b4a3d170942e72e.png)

这就是所谓的[德国坦克问题](https://en.wikipedia.org/wiki/German_tank_problem)。本质上：

> 给定一个生成顺序序列号的制造过程，如何从一个随机样本中估计总的生产量？

## 探索一个估计量

我们将首先查看一个可能的估计量，并探索其数学性质：

![](../Images/b118b475c5a0062e92ed3a93cc8cca78.png)

+   *N**是我们对*N*的估计量

+   *X*是大小为*k*的随机样本

+   *m=max(X)是*样本中观察到的最大序列号

我们可以使用蒙特卡罗模拟来计算*N**的期望性能：

+   从对数正态分布中抽取*N*（均值=200，大方差）

+   从泊松分布中抽取*k*（*λ=20*）

+   对于10,000次迭代，从*[1..N]*中抽取样本k值并计算*N**

这模拟了一个*可能的世界*范围，其中样本数据被收集。下图显示了对于不同*N*、*k*和*N**值的模拟的100次迭代。

![](../Images/a63fceef9942ca861a065659aa104f71.png)

## 无偏估计量

我们可以看到，这些估计通常非常准确——有时会高估真实值，有时会低估真实值。我们可以绘制所有1万次迭代的误差，并查看它们是如何分布的：

![](../Images/2b193f2886e83468391ac9d831db1438.png)

该图表明*N**的平均误差为零。这是因为这是一个著名的无偏估计量。这意味着平均而言，误差会相互抵消，*N**在期望上接近*N*。即：在所有可能的世界中平均。

正式地，*N*的估计量的偏差表示为：

![](../Images/ff14a7382502e86cd1b2265da03a1cf2.png)

偏差是估计量在固定的*N*和*k*下，对*所有可能的*样本的期望（有符号）误差。如果期望误差为0，则表示估计量是无偏的。通常，偏差表示为仅对*X*的期望，而不是*X|N,k*。我使用了额外的下标仅仅是为了强调一个观点。

请注意，这有时写作：

![](../Images/db9ed9117d6e2a92217bd6821ed9f39c.png)

在这种情况下，我们可以证明额外的期望是不必要的。*N*是一个未知但具体的值，*N**的期望值也是如此。常数的期望值就是常数，因此我们可以省略额外的符号。

## 估计量的方差

方差量化了在不同可能的世界中估计量如何变化。我们的误差图显示估计值集中在0周围，稍有偏斜，这是由于对*N*和*k*的先验假设。如果我们观察*k/N*的比值，我们可以看到随着样本量越来越大，估计量的表现如何：

![](../Images/b0a5f69491abf909022f4a74307e87b0.png)

直观的结果是，对于一个无偏估计量，收集更大的样本会导致更准确的结果。*N**的真实方差是：

![](../Images/14baeea9bebe95b89c5dff933f3f5f92.png)

标准差(*N/k*)可以看作是一个随机样本中元素之间的平均间隔。例如：如果真实值是*N=200*，且样本大小是*k=10*，那么样本中值之间的平均间隔是*20*。因此，我们期望大多数估计值落在*200±40*的范围内。

可以证明这是任何无偏估计量能够达到的最小方差。在频率派统计学中，这被称为[均匀最小方差无偏估计量 (UMVUE)](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator)。换句话说：要想实现更低的方差，你需要一个有偏估计量。

从形式上讲，*N*的估计量的方差可以表示为：

![](../Images/19a154e0e7a92440478e456f56d04a57.png)

请注意，方差是围绕*估计值*的期望，而不是围绕真实值的期望。如果我们有一个有偏估计量，我们将评估围绕该有偏估计的分布。

测试你的理解：你明白为什么我们需要外部项的期望吗？*N*是一个随机变量，因此我们需要对所有可能的*X*进行期望，以获得一个具体值。

## 充分信息

你可能注意到我们的估计量有一个问题：它似乎丢弃了样本中的大量信息。如果我们的样本有**k**个值，为什么我们的估计量只使用**一个值**呢？

首先，给出一些简要定义：

+   “统计量”是数据的一个函数（通常是样本的函数）。

+   “充分统计量”是指包含我们尝试估计的总体参数的最大“信息”的统计量。

一旦我们知道最大值和样本大小*k*，就可以证明样本中没有额外的信息。原因与给定样本*X*时，*N*的[似然函数](https://en.wikipedia.org/wiki/Likelihood_function)有关。

**似然函数**

考虑所有可能的*k*大小子集* [1..N]*。对于任何给定的样本，*N*的唯一可能值范围是*[max(X), ∞]*。即，如果*N<max(X)*，则不可能得到包含*max(X)*的样本。获得任意一个*k*大小样本的概率基于从*N*个可能值中[选择](https://en.wikipedia.org/wiki/Combination)一个大小为*k*的集合的方式。下方展示了似然函数。请注意，固定样本的似然函数仅关注*k*和*m=max(X)*。

![](../Images/1b00bbed70f3e52d1aac79449858be98.png)

似然函数 *ℒ(θ;x)* 衡量在不同的*θ*（例如*N*）值下，观察值*x*的可能性。我们可以用它来找到一个*θ*值，该值使得观察到*x*的概率最大，而不会告诉我们关于*θ*本身的概率信息。

**最大似然**

假设*k=5*且*m=60*，那么*N ≥ 60*。最大似然发生在*N=m=60*。尽管大多数*N*值不太可能，但似然函数识别出*N=60*是此样本最可能的值。

![](../Images/b7910f6bfcb72a3ab6af91fa4456a647.png)

首先，注意到所有的*N*值都是非常不太可能的。然后，记住，对于固定的(*m, k*)值，似然函数告诉我们在每个可能的*N*值下，看到该*m*值的概率。仅仅因为*m=60*在*N=60*时最可能，并不意味着它是一个好的估计！

> **最可能的估计并不一定是最好的。**

**费舍尔信息**

[费舍尔信息](https://en.wikipedia.org/wiki/Fisher_information)量化了样本信息量。如果许多*N*的值很可能出现，则信息量较低；如果存在一个围绕真实值的尖锐似然峰值，**则信息量较高**。作为一个粗略的指南，费舍尔信息告诉我们，从随机样本中我们可能知道多少关于真实分布的信息。

**一个充分统计量**

“充分统计量”包含了关于所讨论参数的所有信息。我在这里不会讲证明，但一个统计量是充分的，如果它是最大似然估计（MLE）。如果MLE是有偏的，我们可以使用“偏差修正”来产生更好的估计，**但我们无法找到提供更多信息的其他统计量**。

**一个直观的解释**

并非所有的样本数据都提供有用的信息。具体到德国坦克问题，我们可以看到：

+   样本概率取决于*k*和*max⁡(X)*。

+   值得注意的是，*N*接近*max⁡(X)*的值，更可能生成包含*max(X)*的样本。

+   所有包含*max⁡(X)*的*k*-大小样本**是等可能的**。

+   所以，样本中关于真实*N*值的更多信息，除了知道*k*和*max(X)*，并不存在。

## 一个有偏估计量

使用*max(X)=m*作为估计量几乎总是会低估*N*，因为在一个样本中得到*N*的概率是*1/(N choose k)*。另一方面，如果我们确实得到一个包含*N*的样本，我们原始的估计量*N*可能会给出一个大的过高估计。假设*k=1*，且我们的样本恰好包含*N=1000*。那么我们的*N*估计值*2m-1=1999*将远远过大。

希望显而易见的是，这个理由非常糟糕，不能将*max(X)*作为*N*的估计量。为了验证这一点，我们来比较两个估计量的均方误差（MSE），看看它们的表现如何：

![](../Images/3802f34fe887a2ac22e0d5cd466086d9.png)

注意到*max(X)*估计量表现更差。请注意，几乎所有的误差都归因于其偏差。如果我们绘制估计值的分布，我们可以看到*max(X)*始终产生一个较窄范围的估计。

![](../Images/191dd43aa6a09050593499508cbda2ab.png)

我将跳过证明，我们依赖于可视化来看，*max(X)*的方差显著低于*N*。只需要记住，估计量方差的正确定义是围绕期望估计值的预期离散度。

## 偏差-方差分解

按惯例，我们试图最小化的总误差是均方误差（MSE）。如果你感兴趣，你可以阅读这篇关于[为什么使用MSE](https://stats.stackexchange.com/questions/470626/why-is-using-squared-error-the-standard-when-absolute-error-is-more-relevant-to)的讨论。这次我会省略下标，但记住我们是在对所有可能的样本计算期望值：

![](../Images/95ea308b6a311b24572fdc988827f983.png)

这可以被分解为一个偏差²项和一个方差项。这个推导是理解这一点很有用的。我们从引入*-E[N*]+E[N*]*开始，然后对项进行分组，并展开二次项：

![](../Images/db49180f69c7850086d6a8c03b3cd131.png)

最大的混淆可能出现在倒数第二行：

+   如果我们忽略冗余的期望值，那么左侧项就是偏差²。

+   展开并应用期望运算符后，中间项会变成0。

+   正确的项只是方差，取决于在平方结果之前减去哪个项。

更一般的推导可以在[Wikipedia 关于偏差-方差权衡的文章](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation)中找到。

总的预期误差是估计量的偏差误差和方差的组合。这里有一个微妙的问题：如果模型有偏差，为什么高方差有时能得到一个准确的答案？为什么总的预期误差是偏差²和方差的**总和**，而不是考虑到这一点的其他函数？

上面的分解解释了它是如何在数学上发生的，但可能没有直观地说明。为了建立直觉，考虑一下平方运算对高度不准确估计的影响。同时也要考虑到，偏差²本身不足以解释所有预期的平方误差。

## 一个最优的估计量？

我们已经展示了我们的估计量的*预期误差*。平均而言，给定一个随机样本，我们的估计量与生成该样本的真实值之间的差距有多大？一个始终偏差较大但预测范围较窄的估计量，可能比一个始终准确但预测范围更广的估计量要好。

我们能否在德国坦克问题中找到一个平衡点，在那里我们通过折衷偏差和方差来做出更好的估计？忽略常数项（+ C），这样一个函数大概是这样的：

![](../Images/dbe546c49e2e9fed4e5f9db734610d1a.png)

它将位于*g(k)=1*和*g(k)=(1+1/k)*之间。你能算出为什么吗？使用*1 * m* 是最大似然估计（MLE），它有偏差但方差较小。使用*(1+1/k)* 就是* N**，没有常数项。我们知道，*N** 是一个无偏估计量（UMVUE），但其方差比*m*大。所以在最大似然估计（MLE）和无偏最小方差估计量（UMVUE）之间，我们可以找到一个“最优”的估计量。

结果表明，在不知道*N*的真实值的情况下，我们无法找到最优的* g(k)* 函数，而*N*正是我们试图估计的数字！

维基百科页面上描述了贝叶斯推断技术，这需要对*N*设定先验分布。这个先验分布是你在分析时选择的内容。我们可以用它至少通过我们的世界知识设定合理的边界。例如，我们知道他们至少有*m*辆坦克，可能不超过100,000辆。但先验分布必须是主观的。在*m*到100,000区间内，分布应该是什么样的？应该是均匀分布吗？贝叶斯推断是一个引人入胜的话题，但我在这里就不讨论了。

最后，考虑到具有最低误差的估计量是有偏的。这是我们第一次发现，偏差-方差权衡并不总是最重要的考虑因素。为了推断的目的，我们可能需要从[统计风险](https://en.wikipedia.org/wiki/Statistical_risk)的角度来考虑问题，这可能会优先考虑无偏估计量而不是更精确的估计量。

## 盟军表现如何？

实际上，盟军确实使用了这里描述的技术，只是他们试图确定德国坦克的月生产量。当然，他们没有Python或运行蒙特卡洛模拟的能力。我们来看一下本文中使用的估计量与传统情报收集方法（即间谍活动）相比的表现：

```py
| Month       | N*    | Spying | German records |
|-------------|-------|--------|----------------|
| June 1940   | 169   | 1,000  | 122            |
| June 1941   | 244   | 1,550  | 271            |
| August 1942 | 327   | 1,550  | 342            |

Source: Wikipedia - The German Tank Problem
```

我们可以看到，统计估计的表现很好，且明显比间谍活动所做的估计更为精确。

## 反思

德国坦克问题是一个棘手的例子，我们跳过了很多对统计学家来说重要的数学细节。但我们介绍了一些关键概念：

+   估计量的均方误差（MSE）可以分解为偏差和方差。

+   偏差表示估计量的期望（带符号）误差，取所有可能样本（即所有可能世界）的平均值。

+   方差表示所有可能样本（即所有可能世界）中估计值的期望分布。

+   最好的估计量（即具有最低均方误差的估计量）很可能是有偏的。我们通过较低的方差来抵消偏差带来的误差，这意味着尽管估计量在期望上是有偏的，但估计值更有可能接近真实值。

+   总体参数的似然性关注的是该参数的哪些值使得样本最为可能。这与总体参数的概率无关。

+   最大似然估计（MLE）是样本的一个函数，用于识别最可能生成该样本的总体参数。

+   最大似然估计（MLE）不一定是最佳估计量。我们非常明显地看到，最可能的值可能与生成样本的真实值相距甚远。

+   费舍尔信息量是样本中包含的关于参数的信息量，大致通过似然图在真实值周围的曲率来衡量。

# 广义线性模型

从这里开始，我将使用在论文《[预测、估计与归因](https://med.stanford.edu/content/dam/sm/dbds/documents/biostats-workshop/paper-1-.pdf)》中描述的一个区分：

+   **预测**涉及从样本数据中构建的预测模型的经验精度。

+   **估计**涉及估计生成样本数据的分布的参数。

此外，我们还将考虑以下概念，这些概念在书籍《[统计学习的元素](https://hastie.su.domains/ElemStatLearn/)》中有更详细的描述：

+   一个统计过程创建了一个联合概率分布 *f(X,Y)*，其中粗体 ***X*** 或 ***Y*** 表示向量而不是标量。

+   训练数据 *D* 是从联合分布 *f(X,Y)* 中抽取的样本，包含形如 *(x,y)* 的元组。

+   一个预测模型 *h(x;D)* 在数据集 *D* 上训练，并根据观测值 *x∈X* 对目标变量 *y∈Y* 进行预测。它可以写作 *h(x;D)=E[Y | x∈X]*。

+   一个损失函数 *ℓ(y, h(x;D))* 计算模型在预测某个特定元组 *(x,y)* 的真实值 *y* 时的误差。对于回归问题，这通常是均方误差（MSE）。

此外，我引入了以下特定于本文的符号：

+   潜在变量 *Z* 是联合分布 *f(X,Y,Z)* 的一部分，但在训练数据 *D* 中从未观察到。因此，即使 *Z* 是完整分布的一部分，观察结果只能采取 *(x,y)* 的形式。

+   随机变量 *W* 代表内生抽样偏差。这意味着某些 *(x,y)* 组合可能较为稀疏，不太可能出现在我们的训练数据 *D* 中。这与外生抽样偏差相对立，后者指的是我们使用的抽样程序意味着并非所有观察结果都与 *f(X,Y)* 独立同分布（iid）。你可以通过阅读我的文章《[为何缩放有效](https://medium.com/towards-data-science/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18)》了解更多关于抽样偏差的影响。

## 示例问题 – 房价

我们将生成一个合成数据集，使用房屋的面积（平方米）来预测销售价值。这个看似简单的问题能够教会我们很多关于模型工作原理的知识。以下是一些附加的复杂性：

1.  有一个潜在变量影响着售价：房子距离海滩有多远？也许靠近海滩的房子更贵，但它们也更有可能只有 2-3 间卧室。

1.  任何训练样本 *D* 都存在内生偏差，因为有很少的小房子（1卧室）和特别大的房子（4个以上卧室），所以它们不太可能被挂牌出售。

在潜在变量和样本偏差之间，我们有在现实世界数据集中存在的复杂性。我们设想一个函数，它确定性地根据某些属性计算出售价格：

*f*(x,z)=y*，其中 *x*=面积，*z*=距离海滩的距离，*y*=售价

大小、与海滩的距离和价格之间的关系可以通过这个表面图捕捉到：

![](../Images/27196d0c8e469257d77cf0380ebc9a5d.png)

现在考虑你可能有两栋大小相同且与海滩距离相同的房屋，但它们的售价不同。这意味着我们变量之间的关系并非确定性。对于每一种（大小、距离、价格）组合，我们在训练数据中都有一定的概率密度来观察到具有这些值的房屋。这由联合概率密度函数 *f(X,Y,Z)* 给出。为了可视化这个联合密度，我们使用配对图：

![](../Images/dc5c3865eeff487953288a18aac5dd05.png)

如果我们唯一观察到的变量是房屋大小，那么与价格的关系并不简单。例如，假设我们取某一特定大小房屋的平均海滩距离。在这种情况下，这是一个难以计算的期望值。相反，我们可以使用模拟并应用一些平滑处理来逼近这种关系：

![](../Images/688311d4fb01e42e40f721b620eae0e0.png)

对于特别大的房屋，距离的影响会加剧。因此，靠近海滩的大房子比远离海滩的同等大小的房子要贵得多。两者都是昂贵的，但高端市场的方差差异显著。这将使得在尾部预测关系的真实形态变得困难。

此外，我们必须考虑样本中的内生偏差。被出售的概率（*W*）受到所有属性的影响，我们可以通过这个配对图展示这一点：

![](../Images/24d4bc0e712d8c941ef3d81fb40283f9.png)

我们如何看待这个新属性（*W*）呢？较小/较大的房屋建造得较少，因此出售的数量也较少。实际上，是否将房产挂牌出售受到许多因素的影响，包括人们的出售意愿。这种内生偏差通过使某些组合不太可能出现来影响我们的概率密度函数 *f(X, Z, Y)*，但它不会影响变量之间的关系 *f*(x,z)=y*。

我们调整配对图，展示了在考虑到某个特定房屋出现在市场上的内生偏差后，变量之间的更新关系。

![](../Images/e8ea99b74a458f0d44f23cc3faaab289.png)

注意到，房屋大小和价格之间的表观关系发生了轻微但可观察到的变化。

## 我们的模型捕捉到了什么？

让我们再看一下这个图表，它直接展示了价格和房屋大小之间的关系。

![](../Images/688311d4fb01e42e40f721b620eae0e0.png)

当我们分析一个模型的偏差/方差时，我们是在分析与这个函数的误差吗？**不，我们不是**。我们正在建立一个生成数据的统计过程模型——这个过程包括内生偏差。这意味着期望误差是基于从我们分布中所有可能样本的期望值。

换句话说：回归模型的偏差-方差权衡涉及的是该模型在所有可能世界中的期望误差。由于期望值是通过特定值出现的概率加权的，因此它会受到内生抽样偏差的影响。

令人奇怪的是，一栋房子被售出的概率居然会影响我们对房屋大小与售价之间关系的计算。然而，这个计算恰恰是偏差-方差权衡的核心。

## 回归误差分解

在德国坦克问题中，我们的样本概率是基于我们试图预测的值*f(X|N)*。在回归分析中，预测变量和目标值之间存在联合概率分布*f(X, Y)*。这意味着变量之间的关系有某种固有的变异性，这是无法被解释的。实际上，可能还有我们没有考虑到的潜在变量，但这是另一个话题。这种变异性导致了不可约的误差项，这就是为什么我们将其描述为在给定观察值*x*的情况下预测*y*的**期望值**。

请注意，这个不可约误差有时被称为“偶然性不确定性”（aleatoric uncertainty）。这与由于知识不足引起的“认识论不确定性”（epistemic uncertainty）相对立。一个未充分指定的模型可能导致认识论不确定性，但即使是完美的模型也必须面对偶然性不确定性。

这种新结构意味着，期望的均方误差（MSE）被分解为偏差、方差和一个不可约的误差项：

![](../Images/11c00d2bbebc417cb7a3be085e6ac6b7.png)

在这个分解中，我再次显示了期望的下标，以清楚地表明每个期望是基于什么条件的。新项（h-bar）是我们模型的期望值，经过所有可能的数据集的平均。这可以想象为在不同的世界中收集训练数据集，并创建一个模型集成，平均所有世界中的预测。

我们模型的期望误差需要是一个关于以下内容的积分：

1.  所有可能的数据集（*D*），我们可以用来训练我们的模型（*h*）

1.  所有可能的*x ∈ X*值（按其边际概率加权）

1.  所有可能的*y ∈ Y*值（以类似的权重加权）

![](../Images/274fd47f3232f53903d484a30f1f1adc.png)

有趣的是，这也是在固定大小训练集上的期望——样本大小可能依赖于变量的事实没有体现在这个分解中。

更重要的是，这个积分对于我们的问题是完全不可解的。实际上，**计算期望误差通常是不可解的**，对于非平凡问题尤为如此。即使我们知道用来生成这些合成数据的真实过程，这也依然成立。相反，我们将通过不同样本运行一些模拟，并通过平均误差来查看不同模型的表现。

## 模型复杂度

如果你对偏差-方差权衡有所了解，你可能知道偏差来源于“欠拟合”，而方差来源于“过拟合”。为什么一个过拟合的模型应该具有低偏差，或者一个欠拟合的模型应该具有低方差，这并不显而易见。这些术语*通常*与模型复杂性相关，但具体意味着什么呢？

这里有6种可能的世界，其中35套房子被拿出来销售。在每种情况下，我们使用多项式回归拟合来自[x⁰…x⁵]的项，并将预测的多项式与该大小的真实期望价格进行比较。请注意，不同的训练样本会导致截然不同的多项式预测：

![](../Images/1ec3bbc004ca74f3dbd03c19e7bb87f0.png)

但请记住——在偏差-方差权衡中，我们并不是在将我们的模型与真实关系进行评估。这个真实关系忽略了内生性采样偏差。相反，我们可以根据*W*的影响调整“真实”关系，来考虑被售出的概率。现在我们可以看到，与调整后的真实关系更接近的预测：

![](../Images/8a8183f60952e59bd1f69a026d08bd59.png)

我们可以通过模拟1,000个可能的世界来找到预测的期望值。这是根据房子大小计算的每个多项式度数的期望预测：

![](../Images/f75a8d392a6c7e78acd1489cddbeaf56.png)

请注意，这些模型在低端表现特别差。这完全是由于内生性采样偏差，因为我们不太可能看到许多特别小的房子待售。还要注意，这些模型在特别大的房子上也往往表现不佳，这既是内生性采样偏差的结果，也是潜在变量的影响。

现在我们引入模型函数*h*并增加一个额外的项*λ*，它代表了用于某一类模型的超参数。我们不再使用多项式的度数，而是让*λ*代表所使用的多项式项数的子集。对于我们的仿真，我们将对最多5项、多项式度数为10的所有组合进行穷举检查，并选择具有最佳训练误差的组合。理想情况下，这应该通过交叉验证来完成，但我们将跳过这一部分，因为它在深度学习中并不是一种有效的技术。同时，请注意，在5项和1000次仿真的情况下，穷举搜索已经相当慢了。

接下来我们引入一个函数*g(λ)=c*，它表示基于所选超参数的模型“复杂性”。在这种情况下，*g*只是恒等函数，复杂性完全关注于所使用的多项式项的子集。

固定模型架构下，复杂性变化的期望误差由以下公式给出：

![](../Images/7afb29a270a8a7c36d74afa5dd05560d.png)

现在，我们不再通过多项式度数计算期望预测，而是使用子集选择的大小。对1,000次仿真结果进行平均后，我们得到以下预测：

![](../Images/ed5a06b00534f0381b1842fb3a89fcd2.png)

此外，我们可以绘制总预期误差（按观察到该大小房屋的概率加权）并将误差分解为偏差和方差项：

![](../Images/d5309c6e8eb9c4e0b30455b23ecf49d3.png)

再次提醒，要获得预期误差，我们是对所有可能的世界进行平均。我们可以看到：

+   偏差²随着模型复杂度的增加而减少。

+   方差随着模型复杂度的增加而增加。

+   总误差先下降，达到最小值，然后上升。

+   在这个问题中，总误差也强烈受到不可约误差的影响。

利用一些假设，我们可以识别任何模型*h*的预期误差的一些属性。核心假设是：

+   在低复杂度时，总误差由偏差主导，而在高复杂度时，总误差由方差主导。在最小复杂度时偏差≫方差，在高复杂度时方差≫偏差。

+   作为复杂度的函数，偏差是单调递减的，方差是单调递增的。

+   复杂度函数*g*是可微的。

基于这些假设，我们可以预期大多数模型的表现类似于上述图形。首先，总误差下降到某个最优点，然后随着复杂度的增加，方差增大，总误差开始上升。为了找到最优复杂度，我们首先对误差分解函数关于复杂度求偏导：

![](../Images/d4bb1e09eeccbb39bd45fbd78da336a3.png)

拐点发生在偏导数为0时。

![](../Images/100c5ba3878fa784fe3ab876e7aae592.png)

在最优点，偏差平方的导数是方差的负值。没有进一步的假设，这实际上就是我们能说的关于最优误差的一切。例如，以下是一些随机的偏差和方差函数，恰好满足列出的假设。它们的导数互为倒数的点就是总误差最小化的点：

![](../Images/4948bbe43cf81ff7d3195054a28a3c3e.png)

如果我们添加一个额外的假设，即偏差和方差在最优点周围是对称的，那么我们可以将最低误差缩小到*Bias²(c*)=Var(c*)*。如果你尝试几个选项，你会发现最优点往往接近偏差平方和方差项相等的点。但是没有这个额外假设的话，这是不一定成立的。

## 含义

我们知道计算最优点是不可处理的。但一般认为，低偏差本质上会导致方差爆炸，这是模型复杂度的影响。想一想：其含义是，你无法拥有既表现良好又无偏的模型。

## 泛化误差

由于我们无法在所有可能的世界中平均计算，我们需要其他方法来计算模型的总体预期误差。泛化误差捕捉了模型在未见数据上的表现。这是模型在训练数据上的拟合程度与在底层数据分布上表现的差距。对于一个任意的损失函数*ℓ*，我们可以将泛化误差表述为：

![](../Images/6c8ed5cc03729c45c8d40cb0b5eaa6e3.png)

注意，即便如此，我们也无法计算模型在所有可能的*(x,y)*组合上的预期性能。我们通过收集一个新的独立数据集来进行评估，从而近似泛化误差。我们可以用不同的方式来评估性能：

1.  样本内误差：在用于拟合模型的数据上计算的训练误差。对于过拟合模型，这通常会误导性地偏低，并且无法捕捉泛化能力。

1.  样本外误差（OOS）：在一个与训练集分布相同的独立样本上的表现。这是评估泛化能力的黄金标准。

1.  分布外误差（OOD）：在不属于训练分布的数据上的表现。可以想象一个在城市区域训练的房价模型在乡村房屋上测试时——它很可能失败。

这些概念与我们在偏差-方差权衡中已经探讨过的内容有关。偏差较大的模型无法捕捉变量之间的关系，因此它们描述的关系无法很好地适应OOS（样本外）示例。但高方差模型可能会根据它们看到的样本给出截然不同的预测。即使它们可能有较低的偏差（期望值），**那也仅仅是因为它们误差的幅度相互抵消**。

现在让我们考虑两个与偏差和方差密切相关的概念：

+   **过拟合**最好理解为模型容量与训练数据可用性之间的结果。当一个模型的参数数量相对于训练数据的大小或多样性过多时，它不仅会拟合底层信号，还会拟合数据中的噪声。

+   另一方面，**欠拟合**是由于模型规格不足造成的后果。模型不够复杂，无法捕捉到底层分布的细节。这通常是因为相对于最佳拟合曲线的复杂度，模型的参数太少。

让我们来看一下从模拟中得到的一个可能的世界。在这里，我们放大了样本中大规模高价格部分的情况。注意到更复杂的模型试图画出一条曲线，基本上连接了所有观察到的点。如果样本稍微不同，这些曲线的形状可能会截然不同。另一方面，低复杂度的模型（例如* y = mx + b *或* y = b *线）无法捕捉到数据集尾部的弯曲。

![](../Images/efa6c9b4cd57665ca461d428c00b3d4b.png)

## 关于正则化的简要说明

L1 和 L2 正则化用于 Lasso 和 Ridge 回归是限制复杂性的有趣技术。它们并不是减少参数数量，而是鼓励较小的系数，从而产生平滑的曲线，这样更不容易在训练数据中的点之间产生震荡。这种做法会降低模型的复杂度，从而增加偏差。总体思路是，偏差的增加被减少的方差所弥补。关于这个话题已经写了整本教材，所以在这篇文章中我不再讨论正则化。

## 验证集和测试集

如果我们从偏差、方差和泛化误差的探索中能得到一条教训，那就是：模型必须在它从未见过的数据上进行评估。这个概念很直接，但其应用常常被误解。

验证集和测试集通过作为现实世界表现的代理来帮助减轻过拟合的风险。让我们从明确的区分开始：

+   **验证集：** 在模型开发过程中使用，用于调节超参数并选择表现最好的模型变体。

+   **测试集：** 一个完全独立的数据集，用于在所有训练和调优完成后评估最终模型。

使用这些数据集的目的是近似预测样本外的表现。但有一个陷阱。如果你过于频繁地使用验证集，它会成为训练过程的一部分，导致看不见的数据泄露问题。你可能会“过拟合”验证集上的超参数，从而未能捕捉到关系的真实本质。这就是为什么拥有单独的测试集来评估最终模型表现是有用的原因。测试集上的表现作为我们总体误差计算的代理。主要问题是：我们应该如何构建测试集？

## 尾部风险和分层

记住，估计需要了解分布的形状，而预测则只关注最大化经验精度。对于经验精度，我们需要考虑风险缓解。一个自动化的定价算法可能在预期中表现良好，但却可能带来重大的尾部风险。

显著低估高端住宅的价格会导致机会主义买家利用这些被低估的资产。显著高估高端住宅的价格会导致无人购买。现实世界的不对称性与预期值的对称性不匹配。

> **即使模型在*预期中*表现良好**，它在实际应用中也会失败得非常惨。

这就是为什么分层在设置测试集时可以成为一个至关重要的组成部分。它可能涉及从采样空间中过于密集的区域中删除一些例子，直到整个领域内的分布变得均匀。这个测试集将*不*是与我们的训练数据独立同分布的，因此它并不衡量我们之前看到的公式中的泛化误差。

另一种选择是使用不同的损失函数 *ℓ*（即不是均方误差（MSE），而是一个考虑到我们风险需求的损失函数）。这个损失函数可能会改变误差分解的动态，并可能偏向于显著欠拟合的模型。

## 我们的模型如何看待现实世界？

最后，考虑我们想要实现的目标。在深度学习中，我们的目标可能是训练通用智能体。偏差-方差权衡告诉我们[大型语言模型是否理解它们正在阅读的文本](https://medium.com/towards-data-science/what-do-large-language-models-understand-befdb4411b77)？没有什么。如果我们想评估我们的训练过程是否创建了世界的准确模型，我们需要考虑分布外（OOD）误差。对于任何有希望成为通用模型的系统，它们必须在OOD情境下工作。为此，我们需要离开统计学的范畴，最终进入机器学习的领域。

## 反思

在前一节中，我们学习了偏差和方差的核心概念。在本节中，我们面对的是一个更复杂的问题，阐明了在不同训练数据下，偏差和方差与模型期望性能之间的关系。

我们增加了一些复杂性，考虑潜在变量如何影响模型在尾部的表现——这可能导致尾部风险。我们还存在内生性采样偏差，这意味着预期误差的评估可能无法描述真实的潜在关系。

我们引入了验证集和测试集的概念，作为帮助确定模型在分布外（OOS）性能的方法，以测试模型的泛化误差。我们还讨论了抛弃独立同分布（iid）假设的替代测试集构建方法，这可能导致模型具有更低的尾部风险。

我们还引入了一些关键假设，一旦进入深度学习领域，这些假设将不再适用。在我们进入深度学习领域之前，我们将应用所有这些教训来设计稳健的机器学习算法。

# 稳健的机器学习

在深度学习中，我们常常处理大规模数据集和复杂的模型。这种组合可能导致模型训练时间长达数小时（有时甚至是数周或数月）。当面临为训练单个模型花费数小时的现实时，使用交叉验证等技术的前景令人却步。尽管如此，在训练过程结束时，我们通常对性能有较高要求，因为这需要大量的时间和计算资源投入。

## 稳健性的两种视角

本节的部分内容集中在论文[《机器学习稳健性：导论》](https://arxiv.org/pdf/2404.00897)中的观点。稳健模型被描述为那些在实际部署时仍然能表现良好的模型，尽管它们可能遇到与训练数据不同的输入。文中提供了以下有用的示例，说明了生产环境中输入可能发生变化的情况：

> **输入数据的变化和变动示例：**
> 
> — 输入特征或物体识别模式的变化，挑战了模型从训练数据中学到的归纳偏差。
> 
> — 由于自然发生的扭曲，如光照条件或其他环境因素，生产数据分布发生变化。
> 
> — 恶意输入的篡改，由攻击者故意引入，以迷惑模型甚至将其预测引导到期望的方向。
> 
> — 由于外部因素，如社会行为和经济状况的变化，导致的数据逐渐漂移。
> 
> **模型缺陷和稳定预测性能的威胁示例：**
> 
> — 开发和利用无关的模式和虚假的相关性，这些在生产环境中无法成立。
> 
> — 难以适应训练样本中通常被低估的边缘情况。
> 
> — 容易受到对过度参数化的现代机器学习模型的对抗性攻击和数据中毒的影响，这些攻击和中毒瞄准了模型的脆弱性。
> 
> — 模型无法很好地推广到逐渐漂移的数据，导致概念漂移，因为它学到的概念变得过时或不再能代表当前的数据分布。

我们将与论文[《基于偏差-方差权衡的鲁棒机器学习的数学基础》](https://arxiv.org/abs/2106.05522v4)进行对比。请注意，这篇论文已经撤回，因为“几条相关的定理和命题没有被提及”。然而，它仍然从偏差-方差权衡的角度提供了鲁棒性的有效概述。我们将首先阅读这篇论文，并考虑模型决策边界的形状如何受到复杂性和训练数据的影响。

## 分类的误差分解

在二分类中，我们训练模型来预测类别1（与类别0对比）的概率。这代表了在观察值*x*下，目标变量(*y∈{0,1}*)的期望值。总误差是预测概率与期望值之间的差异。单个项目的损失可以最简单地通过以下公式度量：

![](../Images/bfd9110f79e82ecae3bc39e5b66ac7ad.png)

这有效地度量了预测概率与真实类别之间的距离，并根据真实类别是否等于0或1动态调整。

我们注意到，[分类的偏差-方差分解](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)更加复杂。在《德国坦克问题》部分，我指出偏见模型仍然可能是正确的，因为方差可能（偶然地）将预测结果推向真实值。然而，当使用平方损失时，这一效应完全被事实所抵消，即对于高度错误的估计，期望损失大大增加。因此，高方差可能带来的任何潜在好处都会被显著偏离目标的估计所掩盖。

在二分类的情况下，这不一定成立。偏差、方差和总误差必须在（0,1）范围内。如果模型完全偏向某一类（bias=1），那么模型在期望上总是预测错误的类别。任何方差实际上都会使得正确预测更有可能！因此，在这种特定的情况下，*Err=Bias-Var*。

如果我们加上一个合理的假设，即偏差和方差的和必须小于或等于 1，那么我们就得到了标准的分解，只是总误差是 *Err=Bias+Var* 而不是 Bias²。

## 模型复杂度是复杂的。

在深度学习中，你可能认为模型复杂度完全取决于网络中的参数数量。但请考虑到神经网络是通过随机梯度下降训练的，并且需要时间来收敛到解决方案。为了让模型发生过拟合，它需要时间去学习一个将所有训练数据点连接起来的转换。因此，模型复杂度不仅仅是参数数量的函数，还包括在相同数据集上训练的轮数。

这意味着我们的函数 *g(λ)=c* 并不像多项式回归那样直接。此外，像早停法这样的技术通过在验证集上的错误率开始增加时停止训练，明确地解决了模型的方差问题。

根据论文，影响偏差和方差的超参数主要有三种类型：

+   **类型 I**：使用超参数直接平衡偏差和方差（例如，作为应用于正则化项（如权重衰减）的权重）。

+   **类型 II**：通过调整来自单个训练样本的损失信号间接影响偏差和方差（例如，减少或增加对大预测误差的惩罚）。

+   **类型 III**：控制影响模型复杂度的训练过程部分（例如，训练神经网络的训练轮数、早停法或决策树的深度）。

## 简单与困难样本

如果一个数据集在训练时导致模型有更大的期望泛化误差，那么这个数据集被认为是“更难”学习的。形式上：

![](../Images/62342f623665a3305d225f9dbedf3f0f.png)

*注意：“对于所有 λ”是一个强条件，可能并不总是成立。一个数据集在某些超参数下可能更难学习，但在其他超参数下则不然。*

我们做出假设，认为更困难数据集的最优复杂度（*c**）大于更简单数据集的最优复杂度。我们可以像这样绘制在两个数据集上训练的模型的期望误差：

![](../Images/18325b3f382d9c23a11628a8c5700739.png)

来源：《基于偏差-方差权衡的稳健机器学习数学基础》

根据我们的假设，“简单”数据集和“困难”数据集之间的区别会导致一个有趣的结果：在“更困难”的数据集上选择最优超参数必然会导致模型更复杂。

如果我们将训练数据分为“容易”和“困难”子集，我们可以使用类似的逻辑得出某些数据子集更难以学习。我们可以将这种逻辑扩展到将个别样本*(x,y)*分类为容易或困难。考虑以下导致样本难以学习的原因：

+   噪声标签（即标注错误的数据）

+   特征空间中的稀疏区域

+   必须复杂的分类边界

现在考虑[focal loss](https://arxiv.org/abs/1708.02002)，它可以表示为：

![](../Images/8de4246aa61464ebd7c5768ff9ed8ed6.png)

这类似于对特定样本应用损失加权，目的是在特征空间的难学部分给予模型更强的学习信号。一种常见的加权方法是按逆频率加权，这会给稀有类的样本分配更高的损失。焦点损失具有自动确定哪些样本难以学习的效果，基于模型的当前状态。模型的当前置信度被用来动态调整特征空间中困难区域的损失。因此，如果模型过于自信并且错误，这比模型自信且正确时传递的信号要强。

加权参数*γ*是一个类型II超参数的例子，它调整训练样本的损失信号。如果某个样本很难学习，焦点损失理想情况下会鼓励模型在特征空间的该部分变得更加复杂。然而，样本可能难以学习的原因有很多，因此这并不总是期望的效果。

## 决策边界的形状

在这里，我创建了一个二维数据集，其中包含简单的形状和重复的模式，作为决策边界。我还添加了一些“死区”，在这些区域，数据的采样变得更加困难。使用大约100,000个数据点，人类可以通过查看图表快速识别出边界应该是什么样子的。

![](../Images/45ae3cd336c95af90257ed635a81c33d.png)

尽管存在死区，你仍然可以轻松地看到边界，因为数十亿年的自然选择赋予了你一般的模式识别能力。对于从零开始训练的神经网络来说，情况就不那么容易了。在这个练习中，我们不会应用显式的正则化（权重衰减，dropout），因为这些方法会阻止网络过拟合训练数据。然而，值得注意的是，层归一化、跳跃连接，甚至随机梯度下降都可以作为**隐式正则化器**。

在这里，参数的数量（*p*）大致等于样本的数量（*N*）。我们将只关注训练损失，以观察模型如何过拟合。以下两个模型分别使用相当大的批量大小训练3000个周期。左边模型的预测边界使用标准的二元交叉熵损失，而右边模型使用焦点损失：

![](../Images/6a024af1917da95681b5f5376254cd2d.png)

第一点需要注意的是，尽管没有显式的正则化，但边界相对平滑。例如，在左上角，恰巧有一些稀疏的采样（偶然的），然而两个模型都倾向于切掉星形的一个尖端，而不是在每个点周围预测出一个更复杂的形状。这是一个重要的提醒，许多架构设计决定起到了隐式正则化的作用。

从我们的分析来看，我们期望焦点损失能够在自然复杂的区域预测出复杂的边界。理想情况下，这将是使用焦点损失的一个优势。但如果我们检查其中一个自然复杂的区域，我们会发现两个模型都未能识别出圆圈内部有一个额外的形状。

![](../Images/4ccf85133e8875fc508b606435e17543.png)

在稀疏数据（死区）区域，我们预期焦点损失会创建更复杂的边界。但这不一定是理想的。如果模型没有学习到数据的任何潜在模式，那么有无数种方式可以围绕稀疏点绘制边界。在这里，我们可以对比两个稀疏区域，发现焦点损失预测的边界比交叉熵更为复杂。

![](../Images/f7568d867ef9ca4a78a17332918eb0be.png)

最上面一行来自中央的星星，我们可以看到焦点损失（focal loss）对模式的学习更为深入。稀疏区域的预测边界更加复杂，但也更为准确。最下面一行来自右下角，我们可以看到预测的边界更为复杂，但它并没有学到关于形状的模式。由BCE（binary cross entropy）预测的平滑边界可能比焦点损失预测的奇怪形状更为理想。

这种定性分析并没有帮助我们决定哪个模型更好。我们如何量化它呢？这两种损失函数产生了不同的值，不能直接比较。相反，我们将比较预测的准确性。我们将使用标准的F1分数，但需要注意，不同的风险偏好可能会对召回率或精确度赋予额外的权重。

为了评估模型的泛化能力，我们使用与训练样本独立同分布（iid）的验证集。我们也可以使用早停法（early stopping）来防止两种方法的过拟合。如果我们比较两个模型的验证损失，我们会看到使用焦点损失的F1分数相对于二元交叉熵（binary cross entropy）有轻微提升。

+   **BCE 损失**: 0.936（验证集 F1）

+   **焦点损失**: 0.954（验证集 F1）

因此，似乎在应用于未见数据时，使用焦点损失训练的模型表现略好。到目前为止，一切顺利，对吧？

## iid 泛化问题

在标准的泛化定义中，假设未来的观测值与我们的训练分布是独立同分布的。但如果我们希望模型学习到生成数据的底层过程的有效表示，这并不会有所帮助。在这个例子中，这个过程涉及到决定决策边界的形状和对称性。如果我们的模型具有这些形状和对称性的内部表示，那么它应该在这些稀疏采样的“死区”中表现得同样好。

任何一个模型都永远无法在OOD（分布外）情况下工作，因为它们只见过来自单一分布的数据，无法进行泛化。而期待模型能够泛化是无理的。然而，我们可以专注于稀疏采样区域的鲁棒性。在《机器学习鲁棒性：入门》一文中，作者主要讨论了来自分布尾部的样本，这也是我们在房价模型中看到的情况。但在这里，我们有一种情况，采样是稀疏的，但与明确的“尾部”无关。我将继续将其称为“内生采样偏差”，以强调稀疏性不一定需要尾部的存在。

在这种鲁棒性的视角下，内生采样偏差是一种可能的情况，其中模型可能无法泛化。对于更强大的模型，我们也可以探索OOD和对抗性数据。考虑一个图像模型，它被训练来识别城市区域的物体，但在丛林中无法工作。这将是我们期望足够强大的模型能够在OOD情况下工作的情境。另一方面，对抗性样本则涉及向图像添加噪声，改变颜色的统计分布，这种变化人眼无法察觉，但会导致非鲁棒模型的错误分类。但构建能够抵抗对抗性和OOD扰动的模型超出了这篇已经很长的文章的范围。

## 对扰动的鲁棒性

那么我们如何量化这种鲁棒性呢？我们从准确度函数*A*（我们之前使用过F1得分）开始。然后我们考虑一个扰动函数*φ*，我们可以将其应用于单个数据点或整个数据集。请注意，这个扰动函数应该保持预测变量*x*和目标变量*y*之间的关系。（也就是说，我们不是故意标记错误的样本）。

考虑一个旨在预测任何城市房价的模型，OOD扰动可能涉及从训练数据中未包含的城市中获取样本。在我们的例子中，我们将关注一个修改版的数据集，这个数据集专门从稀疏区域进行采样。

模型（*h*）的鲁棒性评分（*R*）是衡量模型在扰动数据集下的表现，相较于清洁数据集的表现的指标：

![](../Images/e76d6b3ecd6f3f362297e0d9abedb811.png)

考虑这两种训练模型来预测决策边界：一种使用焦点损失（focal loss）训练，另一种使用二元交叉熵（binary cross entropy）训练。在与训练数据独立同分布（iid）的验证集上，焦点损失的表现略好一些。然而，我们使用该数据集进行了早停，这可能导致一些微妙的信息泄漏。让我们在以下方面进行比较：

1.  一个与训练集独立同分布的验证集，并用于早停。

1.  一个与训练集独立同分布的测试集。

1.  一个扰动过的（*φ*）测试集，我们仅从我所称的“死区”稀疏区域中抽样。

```py
| Loss Type  | Val (iid) F1  | Test (iid) F1   | Test (φ) F1 |   R(φ)  |
|------------|---------------|-----------------|-------------|---------|
| BCE Loss   |    0.936      |    0.959        |   0.834     | 0.869   |
| Focal Loss |    0.954      |    0.941        |   0.822     | 0.874   |
```

标准的偏差-方差分解表明，通过允许在困难样本上增加复杂度，焦点损失可能会得到更鲁棒的结果。我们知道这在所有情况下可能并不理想，因此我们在验证集上进行了评估以确认。到目前为止一切顺利。但现在我们看一下在扰动后的测试集上的表现，发现焦点损失的表现稍微差了一些！然而，我们也看到焦点损失的鲁棒性分数稍微高一些。*那这里到底发生了什么？*

我进行了多次实验，每次结果略有不同。这是我想要强调的一个令人惊讶的实例。偏差-方差分解是关于我们的模型在期望下（在不同可能的世界中）如何表现。相比之下，这种鲁棒性方法告诉我们在扰动下*这些特定模型*的表现。但我们可能还需要更多的考虑来进行模型选择。

这些结果中有许多微妙的教训：

1.  如果我们在验证集上做出重要决策（例如早停），那么拥有一个独立的测试集就变得至关重要。

1.  即使在相同的数据集上进行训练，我们也可能得到不同的结果。在训练神经网络时，存在多个随机性来源。我们将在本文最后一部分进一步讨论这一点。

1.  一个较弱的模型可能对扰动更加鲁棒。因此，模型选择需要考虑的不仅仅是鲁棒性分数。

1.  我们可能需要在多个扰动下评估模型，以做出明智的决策。

## 比较鲁棒性的方法

在一种关于鲁棒性的方法中，我们通过偏差-方差权衡的视角考虑超参数对模型性能的影响。我们可以利用这些知识理解不同类型的训练样本如何影响我们的训练过程。例如，我们知道错误标记的数据在使用焦点损失时特别不利。我们可以考虑是否可以将特别困难的样本从训练数据中排除，从而产生更鲁棒的模型。通过考虑超参数的类型以及它们如何影响偏差和方差，我们还能更好地理解正则化的作用。

另一个观点则大多忽略了偏差-方差权衡，关注我们的模型在扰动输入上的表现。对我们来说，这意味着专注于稀疏采样区域，但也可能包括分布外（OOD）数据和对抗数据。这种方法的一个缺点是它是评估性的，并不一定告诉我们如何构建更好的模型，除非我们训练更多（且更多样化）的数据。更大的缺点是，较弱的模型可能表现出更强的鲁棒性，因此我们不能仅仅使用鲁棒性得分来进行模型选择。

## 正则化与鲁棒性

如果我们采用标准的交叉熵损失训练的模型，我们可以绘制不同时间点上模型在不同指标上的表现：训练损失、验证损失、validation_*φ* 损失、验证准确度和验证_*φ* 准确度。我们可以比较在不同正则化类型存在下的训练过程，看看它如何影响泛化能力。

![](../Images/d7d858a6fad9c619970cc158619a8db0.png)

在这个特定问题中，我们可以做出一些不同寻常的观察。

1.  正如我们所预期的，在没有正则化的情况下，随着训练损失趋近于 0，验证损失开始增加。

1.  validation_*φ* 损失的增加要显著得多，因为它只包含来自稀疏“死区”的例子。

1.  但是，验证准确度并没有随着验证损失的增加而变得更差。这是怎么回事？这实际上是我在真实数据集中见过的情况。模型的准确度提高了，但它对输出的信心也在增强，所以当它出错时，损失非常高。使用模型的概率变得毫无用处，因为无论模型表现如何，它们都会趋向 99.99%。

1.  添加正则化可以防止验证损失膨胀，因为训练损失不能降到 0。然而，它也可能对验证准确度产生负面影响。

1.  添加 dropout 和权重衰减比仅使用 dropout 更好，但从准确度的角度来看，两者都比不使用正则化要差。

## 反思

如果你已经跟随我读到了这篇文章的这一部分，我希望你能理解偏差-方差权衡的局限性。理解模型复杂性与期望性能之间的典型关系始终是有用的，但我们已经看到了一些有趣的观察，挑战了默认的假设：

+   模型复杂性可以在特征空间的不同部分发生变化。因此，单一的复杂度与偏差/方差度量并不能始终捕捉到整个情况。

+   标准的泛化误差度量并没有捕捉到所有类型的泛化，尤其是在扰动下的鲁棒性缺失。

+   我们的训练样本中有些部分比其他部分更难学习，而且有多种方式可以将训练样本视为“困难”。在特征空间的自然复杂区域可能需要复杂性，但在稀疏区域则可能是有问题的。这种稀疏性可能是由内生的采样偏差驱动的，因此将性能与iid测试集进行比较可能会产生误导。

+   一如既往，我们需要考虑风险和风险最小化。如果你期望所有未来输入都与训练数据独立同分布，那么专注于稀疏区域或OOD数据可能是有害的，特别是当尾部风险不带来重大后果时。另一方面，我们已经看到尾部风险可能会带来独特的后果，因此为特定问题构建适当的测试集非常重要。

+   单纯测试模型对扰动的鲁棒性不足以用于模型选择。只有在适当的风险评估下，才能做出关于模型泛化能力的决策。

+   偏差-方差权衡只关注模型在可能世界中平均化后的期望损失。它并不一定告诉我们，使用硬分类边界时模型的准确度如何。这可能会导致一些反直觉的结果。

# 深度学习与过度参数化

让我们回顾一下对偏差-方差分解至关重要的一些假设：

+   在低复杂度下，总误差由偏差主导，而在高复杂度下，总误差由方差主导。在最小复杂度时，偏差远大于方差。

+   作为复杂性的函数，偏差是单调递减的，而方差是单调递增的。

+   复杂性函数*g*是可微分的。

事实证明，对于足够深的神经网络，前两个假设是错误的。最后一个假设可能只是为了简化一些计算而方便的虚构。我们不会质疑那个假设，但我们将重点关注前两个假设。

让我们简要回顾一下什么是过拟合：

+   当一个模型无法区分噪声（偶然性不确定性）与内在变异时，就会发生过拟合。这意味着一个训练过的模型在面对不同的训练数据时，可能会表现出截然不同的行为（即在不同可能世界中的方差）。

+   我们注意到模型过拟合时，它无法对未见的测试集进行泛化。这通常意味着模型在与训练数据独立同分布（iid）的测试数据上的表现。我们可能会关注不同的鲁棒性度量，从而构建一个具有OOS、分层、OOD或对抗性质的测试集。

到目前为止，我们假设获得真正低偏差的唯一方法是模型过于复杂。而且我们假设这种复杂性导致了不同数据训练的模型之间的高方差。我们还已确定，许多超参数对复杂性有贡献，包括随机梯度下降的迭代次数。

## 过度参数化与记忆化

你可能听说过大型神经网络可以简单地记住训练数据。但这是什么意思呢？在参数足够的情况下，模型不需要学习特征和输出之间的关系。相反，它可以存储一个函数，完美地响应每个训练示例的特征，完全独立地进行处理。这就像为每种特征组合写一个明确的if语句，并为该特征产生平均输出。考虑我们的决策边界数据集，其中每个示例都是完全可分的。这意味着训练集中的每个样本都能达到100%的准确率。

如果模型有足够的参数，那么梯度下降算法自然会使用所有这些空间进行记忆。一般认为，这比寻找特征与目标值之间的潜在关系要简单得多。这种情况通常发生在*p ≫ N*（可训练参数的数量远大于样本的数量）时。

但有两种情况，即使模型已经记住了训练数据，仍然能够学会泛化：

1.  参数太少会导致模型过于简单。增加更多的参数会导致模型复杂度达到看似最优的水平。继续增加参数会使模型表现更差，因为它开始拟合训练数据中的噪声。一旦参数的数量超过训练样本的数量，模型*可能*开始表现得更好。当*p ≫ N*时，模型会达到另一个最优点。

1.  训练模型直到训练和验证损失开始分离。随着模型记住训练数据，训练损失趋向于0，但验证损失却开始急剧增加并达到峰值。经过一段时间（较长）的训练后，验证损失开始下降。

这就是所谓的“双重下降”现象，其中额外的复杂度实际上有助于更好的泛化。

## 双重下降是否需要标签错误？

一种普遍共识是，标签噪声足够但不是双重下降发生的必要条件。例如，论文[揭示双重下降之谜](https://arxiv.org/pdf/2310.13572)发现，过度参数化的网络会学习将错误标签的类别分配给训练数据中的点，而不是学习忽略噪声。然而，模型可能会“隔离”这些点并学习围绕它们的普遍特征。它主要关注神经网络隐藏状态中学习到的特征，并表明这些特征的可分性即使在没有标签错误的情况下也能使标签变得嘈杂。

论文[《双重下降揭秘》](https://arxiv.org/pdf/2303.14151)描述了在广义线性模型中双重下降发生的几种必要条件。这些标准主要关注数据中的方差（而非模型方差），这使得模型很难正确学习预测变量和目标变量之间的关系。任何这些条件都可能导致双重下降：

1.  存在奇异值。

1.  测试集的分布未能有效地通过训练数据中最能解释方差的特征来捕捉。

1.  对于完美拟合的模型缺乏方差（即，完美拟合的模型似乎没有随机不确定性）。

这篇论文还通过以下可视化展示了一个玩具问题中的双重下降现象：

![](../Images/5d9ab5b8e05827b68246befb2167ca51.png)

来源：《双重下降揭秘：识别、解释与消除深度学习难题的来源》

相比之下，论文[《理解双重下降需要细致的偏差-方差分解》](https://proceedings.neurips.cc/paper_files/paper/2020/file/7d420e2b2939762031eed0447a9be19f-Paper.pdf)对不同噪声来源及其对方差的影响进行了详细的数学分析：

1.  采样——一般来说，将模型拟合到不同的数据集上会导致不同预测的模型（*V_D*）

1.  优化——参数初始化的影响，但可能也包括随机梯度下降的性质（*V_P*）。

1.  标签噪声——通常是被错误标记的样本（*V_ϵ***）。

1.  三个方差来源之间的潜在交互作用。

论文继续展示了这些方差项实际上是如何作为模型偏差的一部分贡献到总误差中的。此外，你可以先对V_D或V_P进行期望计算，这意味着根据你如何进行计算，你会得出不同的结论。一个恰当的分解需要理解总方差是如何通过三种方差来源之间的交互作用汇聚在一起的。结论是，虽然标签噪声加剧了双重下降，但它不是必需的。

## 正则化与双重下降

这些论文中的另一个共识是，正则化可以防止双重下降。但正如我们在前一部分中所看到的，这并不一定意味着正则化后的模型在未见数据上的泛化能力会更好。更有可能的是，正则化作为训练损失的“地板”，防止模型将训练损失降低到任意低的程度。但正如我们从偏差-方差权衡中所知道的，这可能限制了模型的复杂性，并引入了偏差。

## 反思

双重下降是一个有趣的现象，它挑战了本文中许多假设。我们可以看到，在适当的情况下，增加模型复杂度并不一定会降低模型的泛化能力。

我们是否应该将高度复杂的模型视为特殊情况，还是它们质疑整个偏差-方差权衡？就个人而言，我认为核心假设在大多数情况下成立，高度复杂的模型只是一个特殊情况。我认为偏差-方差权衡有其他缺陷，但核心假设往往是有效的。

# 结论

在统计推断和更典型的统计模型中，偏差-方差权衡相对简单。我没有讨论像决策树或支持向量机这样的其他机器学习方法，但我们讨论的许多内容仍然适用。然而，即使在这些环境中，我们也需要考虑比我们模型在所有可能世界中平均表现如何更多的因素。主要是因为我们是在将模型的表现与假设为iid的未来数据进行比较，这些数据假定与我们的训练集一致。

即使我们的模型只会看到类似于训练分布的数据，我们仍然可能面临尾部风险带来的重大后果。大多数[机器学习项目需要进行适当的风险评估](https://medium.com/management-matters/managing-risks-in-deploying-generative-ai-393254259497)，以了解错误的后果。我们应该构建符合适当风险框架的验证和测试集，而不是在iid假设下评估模型。

此外，假定具有通用能力的模型需要在OOD数据上进行评估。执行关键功能的模型需要进行对抗性评估。值得指出的是，在强化学习的环境中，偏差-方差权衡不一定有效。考虑[AI安全中的对齐问题](https://medium.com/towards-data-science/exploring-the-ai-alignment-problem-with-gridworlds-2683f2f5af38)，它考虑了超越明确陈述目标的模型性能。

我们还看到，在大规模过度参数化模型的情况下，关于过拟合和欠拟合的标准假设根本不成立。双重下降现象复杂且仍然没有完全理解。然而，它提供了一个关于信任强烈持有假设有效性的重大教训。

对于那些坚持看到这里的读者，我想最后在本文的不同部分之间做一个连接。在推断统计部分，我解释了费舍尔信息描述了样本可以包含关于样本所来自分布的信息量。在本文的多个部分中，我也提到过有无数种方式围绕稀疏采样点绘制决策边界。关于样本中是否有足够的信息来对稀疏区域得出结论，存在一个有趣的问题。

在我关于[为什么缩放有效](https://medium.com/towards-data-science/why-scaling-works-inductive-biases-vs-the-bitter-lesson-9c2782f99b18)的文章中，我谈到了归纳先验的概念。这是由我们选择的训练过程或模型架构引入的东西。这些归纳先验使得模型倾向于做出某些类型的推理。例如，正则化可能会鼓励模型做出平滑而不是崎岖的边界。通过不同类型的归纳先验，模型可能能从样本中提取比弱先验下更多的信息。例如，有方法可以鼓励对称性、平移不变性，甚至是检测重复模式。这些通常通过特征工程或通过架构决策（如卷积或注意力机制）来实现。

# 后记

我大约一年多前开始整理这篇文章的笔记。当时我有一个实验，焦点损失对从模型中获得良好性能至关重要。然后我接连做了几个实验，在没有明显原因的情况下，焦点损失的表现非常差。我开始深入挖掘偏差-方差权衡，这让我走进了一个死胡同。最终，我学到了更多关于双重下降的知识，并意识到偏差-方差权衡比我之前认为的要复杂得多。在那段时间里，我阅读并注释了几篇相关的论文，所有的笔记只是静静地积累在数字尘土中。

最近我意识到，多年来我读了很多糟糕的关于偏差-方差权衡的文章。我认为缺失的一个观念是，我们在计算“可能世界”中的期望。这个见解可能并不适合所有人，但对我来说似乎至关重要。

我还想评论一下一个流行的关于偏差与方差的可视化图，它使用箭术射击分布在靶心周围。我觉得这个可视化有误导性，因为它让人看起来偏差和方差是关于单一模型的个体预测。然而，偏差-方差误差分解背后的数学显然是关于在“可能世界”中对性能的平均值。我故意避免使用这个可视化图，正是因为这个原因。

我不确定有多少人会看到最后。我在开始写关于人工智能的内容之前很久就整理了这些笔记，并觉得应该好好利用它们。我也只是需要把这些想法从脑海里整理出来并写下来。所以，如果你已经看到最后，我希望你能从我的观察中获得一些启发。

# 参考文献

[1] “德国坦克问题，”Wikipedia, 2021年11月26日。[https://en.wikipedia.org/wiki/German_tank_problem](https://en.wikipedia.org/wiki/German_tank_problem)

[2] Wikipedia Contributors, “最小方差无偏估计量，”Wikipedia, 2019年11月09日。[https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator](https://en.wikipedia.org/wiki/Minimum-variance_unbiased_estimator)

[3] “似然函数,” Wikipedia, 2020年11月26日\. [https://en.wikipedia.org/wiki/Likelihood_function](https://en.wikipedia.org/wiki/Likelihood_function)

[4] “费舍尔信息,” Wikipedia, 2023年11月23日\. [https://en.wikipedia.org/wiki/Fisher_information](https://en.wikipedia.org/wiki/Fisher_information)

[5] Why, “为什么使用平方误差是标准做法，而绝对误差对于大多数问题更为相关？,” Cross Validated, 2020年6月5日\. [https://stats.stackexchange.com/questions/470626/w](https://stats.stackexchange.com/questions/470626/w)（访问日期：2024年11月26日）。

[6] Wikipedia 贡献者, “偏差–方差权衡,” Wikipedia, 2020年2月4日\. [https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)

[7] B. Efron, “预测、估计与归因,” International Statistical Review, 第88卷，第S1期, 2020年12月, doi: [https://doi.org/10.1111/insr.12409](https://doi.org/10.1111/insr.12409)。

[8] T. Hastie, R. Tibshirani, 和 J. H. Friedman, 《统计学习的元素》。Springer, 2009年。

[9] T. Dzekman, “Medium,” Medium, 2024年\. [https://medium.com/towards-data-science/why-scalin](https://medium.com/towards-data-science/why-scalin)（访问日期：2024年11月26日）。

[10] H. Braiek 和 F. Khomh, “机器学习鲁棒性：入门,” 2024年\. 可用: [https://arxiv.org/pdf/2404.00897](https://arxiv.org/pdf/2404.00897)

[11] O. Wu, W. Zhu, Y. Deng, H. Zhang, 和 Q. Hou, “基于偏差–方差权衡的鲁棒机器学习数学基础,” arXiv.org, 2021\. [https://arxiv.org/abs/2106.05522v4](https://arxiv.org/abs/2106.05522v4)（访问日期：2024年11月26日）。

[12] “bias_variance_decomp: 用于分类和回归损失的偏差–方差分解 — mlxtend,” rasbt.github.io. [https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp](https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/)

[13] T.-Y. Lin, P. Goyal, R. Girshick, K. He, 和 P. Dollár, “用于密集目标检测的焦点损失,” arXiv:1708.02002 [cs], 2018年2月, 可用: [https://arxiv.org/abs/1708.02002](https://arxiv.org/abs/1708.02002)

[14] Y. Gu, X. Zheng, 和 T. Aste, “解开双重下降之谜：通过学习特征空间的视角进行深入分析，” arXiv.org, 2023\. [https://arxiv.org/abs/2310.13572](https://arxiv.org/abs/2310.13572)（访问日期：2024年11月26日）。

[15] R. Schaeffer 等人, “双重下降解密：识别、解释与剖析深度学习难题的源头,” arXiv.org, 2023\. [https://arxiv.org/abs/2303.14151](https://arxiv.org/abs/2303.14151)（访问日期：2024年11月26日）。

[16] B. Adlam 和 J. Pennington, “理解双重下降需要细粒度的偏差–方差分解,” 神经信息处理系统, 第33卷, 第11022–11032页, 2020年1月。
