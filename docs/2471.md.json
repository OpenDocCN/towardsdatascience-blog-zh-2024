["```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Create dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\ndf = pd.DataFrame(dataset_dict)\n\n# One-hot encode 'Outlook' column\ndf = pd.get_dummies(df, columns=['Outlook'],prefix='',prefix_sep='')\n\n# Convert 'Wind' column to binary\ndf['Wind'] = df['Wind'].astype(int)\n\n# Rearrange columns\ncolumn_order = ['sunny', 'overcast', 'rain', 'Temperature', 'Humidity', 'Wind', 'Num_Players']\ndf = df[column_order]\n\n# Split features and target\nX, y = df.drop('Num_Players', axis=1), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n```", "```py\ndef calculate_split_mse(X_train, y_train, feature_name, split_point):\n    # Create DataFrame and sort by feature\n    analysis_df = pd.DataFrame({\n        'feature': X_train[feature_name],\n        'y_actual': y_train\n    }).sort_values('feature')\n\n    # Split data and calculate means\n    left_mask = analysis_df['feature'] <= split_point\n    left_mean = analysis_df[left_mask]['y_actual'].mean()\n    right_mean = analysis_df[~left_mask]['y_actual'].mean()\n\n    # Calculate squared differences\n    analysis_df['squared_diff'] = np.where(\n        left_mask,\n        (analysis_df['y_actual'] - left_mean) ** 2,\n        (analysis_df['y_actual'] - right_mean) ** 2\n    )\n\n    # Calculate MSEs and counts\n    left_mse = analysis_df[left_mask]['squared_diff'].mean()\n    right_mse = analysis_df[~left_mask]['squared_diff'].mean()\n    n_left = sum(left_mask)\n    n_right = len(analysis_df) - n_left\n\n    # Calculate weighted average MSE\n    weighted_mse = (n_left * left_mse + n_right * right_mse) / len(analysis_df)\n\n    # Print results\n    print(analysis_df)\n    print(f\"\\nResults for split at {split_point} on feature '{feature_name}':\")\n    print(f\"Left child MSE (n={n_left}, mean={left_mean:.2f}): {left_mse:.2f}\")\n    print(f\"Right child MSE (n={n_right}, mean={right_mean:.2f}): {right_mse:.2f}\")\n    print(f\"Weighted average MSE: {weighted_mse:.2f}\")\n\n# Example usage:\ncalculate_split_mse(X_train, y_train, 'Temperature', 73.5)\n```", "```py\ndef evaluate_all_splits(X_train, y_train):\n    \"\"\"Evaluate all possible split points using midpoints for all features\"\"\"\n    results = []\n\n    for feature in X_train.columns:\n        data = pd.DataFrame({'feature': X_train[feature], 'y_actual': y_train})\n        splits = [(a + b)/2 for a, b in zip(sorted(data['feature'].unique())[:-1], \n                                          sorted(data['feature'].unique())[1:])]\n\n        for split in splits:\n            left_mask = data['feature'] <= split\n            n_left = sum(left_mask)\n\n            if not (0 < n_left < len(data)): continue\n\n            left_mean = data[left_mask]['y_actual'].mean()\n            right_mean = data[~left_mask]['y_actual'].mean()\n\n            left_mse = ((data[left_mask]['y_actual'] - left_mean) ** 2).mean()\n            right_mse = ((data[~left_mask]['y_actual'] - right_mean) ** 2).mean()\n\n            weighted_mse = (n_left * left_mse + (len(data) - n_left) * right_mse) / len(data)\n\n            results.append({'Feature': feature, 'Split_Point': split, 'Weighted_MSE': weighted_mse})\n\n    return pd.DataFrame(results).round(2)\n\n# Example usage:\nresults = evaluate_all_splits(X_train, y_train)\nprint(results)\n```", "```py\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nimport matplotlib.pyplot as plt\n\n# Train the model\nregr = DecisionTreeRegressor(random_state=42)\nregr.fit(X_train, y_train)\n\n# Visualize the decision tree\nplt.figure(figsize=(26,8))\nplot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=False, fontsize=16, precision=2)\nplt.tight_layout()\nplt.show()\n```", "```py\n# Visualize the decision tree\nplt.figure(figsize=(26,8))\nplot_tree(regr, feature_names=X.columns, filled=True, rounded=True, impurity=True, fontsize=16, precision=2)\nplt.tight_layout()\nplt.show()\n```", "```py\n# Compute the cost-complexity pruning path\ntree = DecisionTreeRegressor(random_state=42)\neffective_alphas = tree.cost_complexity_pruning_path(X_train, y_train).ccp_alphas\nimpurities = tree.cost_complexity_pruning_path(X_train, y_train).impurities\n\n# Function to count leaf nodes\ncount_leaves = lambda tree: sum(tree.tree_.children_left[i] == tree.tree_.children_right[i] == -1 for i in range(tree.tree_.node_count))\n\n# Train trees and count leaves for each complexity parameter\nleaf_counts = [count_leaves(DecisionTreeRegressor(random_state=0, ccp_alpha=alpha).fit(X_train_scaled, y_train)) for alpha in effective_alphas]\n\n# Create DataFrame with analysis results\npruning_analysis = pd.DataFrame({\n    'total_leaf_impurities': impurities,\n    'leaf_count': leaf_counts,\n    'cost_function': [f\"{imp:.3f} + {leaves}α\" for imp, leaves in zip(impurities, leaf_counts)],\n    'effective_α': effective_alphas\n})\n\nprint(pruning_analysis)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import StandardScaler\n\n# Create dataset\ndataset_dict = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],\n    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Num_Players': [52,39,43,37,28,19,43,47,56,33,49,23,42,13,33,29,25,51,41,14,34,29,49,36,57,21,23,41]\n}\n\ndf = pd.DataFrame(dataset_dict)\n\n# One-hot encode 'Outlook' column\ndf = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\n\n# Convert 'Wind' column to binary\ndf['Wind'] = df['Wind'].astype(int)\n\n# Split data into features and target, then into training and test sets\nX, y = df.drop(columns='Num_Players'), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Initialize Decision Tree Regressor\ntree = DecisionTreeRegressor(random_state=42)\n\n# Get the cost complexity path, impurities, and effective alpha\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nprint(ccp_alphas)\nprint(impurities)\n\n# Train the final tree with the chosen alpha\nfinal_tree = DecisionTreeRegressor(random_state=42, ccp_alpha=0.1)\nfinal_tree.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = final_tree.predict(X_test)\n\n# Calculate and print RMSE\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"RMSE: {rmse:.4f}\")\n```"]