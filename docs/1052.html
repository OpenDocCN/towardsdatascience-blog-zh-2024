<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Relation Extraction with Llama3 Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Relation Extraction with Llama3 Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26">https://towardsdatascience.com/relation-extraction-with-llama3-models-f8bc41858b9e?source=collection_archive---------0-----------------------#2024-04-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c4de" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Enhanced relation extraction by fine-tuning Llama3–8B with a synthetic dataset created using Llama3–70B</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Silvia Onofrei" class="l ep by dd de cx" src="../Images/198b04b2063b4269eaff52402dc5f8d5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*sMBSQ7O4JfKYt-L7TGmlIQ.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@silviaonofrei?source=post_page---byline--f8bc41858b9e--------------------------------" rel="noopener follow">Silvia Onofrei</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f8bc41858b9e--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk ld le ab q ee lf lg" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lc"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lb lc">4</span></p></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lh k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al li an ao ap ie lj lk ll" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lm cn"><div class="l ae"><div class="ab cb"><div class="ln lo lp lq lr ls ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al li an ao ap ie lt lu lg lv lw lx ly lz s ma mb mc md me mf mg u mh mi mj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mn mo mp mq mr ms mk ml paragraph-image"><div role="button" tabindex="0" class="mt mu ed mv bh mw"><div class="mk ml mm"><img src="../Images/31614e3e1a26eb32ed70b3333c7d3913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yqsTaoZ28mrTWmCTBHt1GA.png"/></div></div><figcaption class="my mz na mk ml nb nc bf b bg z dx">Generated with DALL-E.</figcaption></figure><h1 id="578f" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Premise</h1><p id="4091" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Relation extraction (RE) is the task of extracting relationships from unstructured text to identify connections between various named entities. It is done in conjunction with named entity recognition (NER) and is an essential step in a natural langage processing pipeline. With the rise of Large Language Models (LLMs), traditional supervised approaches that involve tagging entity spans and classifying relationships (if any) between them are enhanced or entirely replaced by LLM-based approaches [<a class="af ov" href="https://arxiv.org/pdf/2305.05003.pdf" rel="noopener ugc nofollow" target="_blank">1</a>].</p><p id="63b6" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Llama3 is the most recent major release in the domain of GenerativeAI [<a class="af ov" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">2</a>]. The base model is available in two sizes, 8B and 70B, with a 400B model expected to be released soon. These models are available on the HuggingFace platform; see [<a class="af ov" href="https://huggingface.co/blog/llama3" rel="noopener ugc nofollow" target="_blank">3</a>] for details. The 70B variant powers Meta’s new chat website <a class="af ov" href="http://Meta.ai" rel="noopener ugc nofollow" target="_blank">Meta.ai</a> and exhibits performance comparable to ChatGPT. The 8B model is among the most performant in its class. The architecture of Llama3 is similar to that of Llama2, with the increase in performance primarily due to data upgrading. The model comes with an upgaded tokenizer and expanded context window. It is labelled as open-source, although only a small percentage of the data is released. Overall, it is an excellent model, and I cannot wait to give it a try.</p><p id="55fe" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Llama3–70B can produce amazing results, but due to its size it is impractical, prohibitively expensive and hard to use on local systems. Therefore, to leverage its capabilities, we have Llama3–70B teach the smaller Llama3–8B the task of relation extraction from unstructured text.</p><p id="78c0" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Specifically, with the help of Llama3–70B, we build a supervised fine-tuning dataset aimed at relation extraction. We then use this dataset to fine-tune Llama3–8B to enhance its relation extraction capabilities.</p><p id="1b10" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">To reproduce the code in the <a class="af ov" href="https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab Notebook </a>associated to this blog, you will need:</p><ul class=""><li id="5519" class="nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou pb pc pd bk">HuggingFace credentials (to save the fine-tuned model, optional) and Llama3 access, which can be obtained by following the instructions from one of the models’ cards;</li><li id="d5b6" class="nz oa fq ob b go pe od oe gr pf og oh oi pg ok ol om ph oo op oq pi os ot ou pb pc pd bk">A free <a class="af ov" href="https://console.groq.com" rel="noopener ugc nofollow" target="_blank">GroqCloud</a> account (you can loggin with a Google account) and a corresponding API Key.</li></ul><h1 id="dad6" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Workspace Setup</h1><p id="c47e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">For this project I used a Google Colab Pro equipped with an A100 GPU and a High-RAM setting.</p><p id="2b12" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We start by installing all the required libraries:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="6306" class="pn ne fq pk b bg po pp l pq pr">!pip install -q groq<br/>!pip install -U accelerate bitsandbytes datasets evaluate <br/>!pip install -U peft transformers trl </span></pre><p id="6ff6" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">I was very pleased to notice that the entire setup worked from the beginning without any dependencies issues or the need to install <code class="cx ps pt pu pk b">transformers</code> from the source, despite the novelty of the model.</p><p id="9e40" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We also need to give access Goggle Colab to the drive and files and set the working directory:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="7b8a" class="pn ne fq pk b bg po pp l pq pr"># For Google Colab settings<br/>from google.colab import userdata, drive<br/><br/># This will prompt for authorization<br/>drive.mount('/content/drive')<br/><br/># Set the working directory<br/>%cd '/content/drive/MyDrive/postedBlogs/llama3RE'</span></pre><p id="c76a" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">For those who wish to upload the model to the HuggingFace Hub, we need to upload the Hub credentials. In my case, these are stored in Google Colab secrets, which can be accessed via the key button on the left. This step is optional.</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="973e" class="pn ne fq pk b bg po pp l pq pr"># For Hugging Face Hub setting<br/>from huggingface_hub import login<br/><br/># Upload the HuggingFace token (should have WRITE access) from Colab secrets<br/>HF = userdata.get('HF')<br/><br/># This is needed to upload the model to HuggingFace<br/>login(token=HF,add_to_git_credential=True)</span></pre><p id="7155" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">I also added some path variables to simplify file access:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="99ff" class="pn ne fq pk b bg po pp l pq pr"># Create a path variable for the data folder<br/>data_path = '/content/drive/MyDrive/postedBlogs/llama3RE/datas/'<br/><br/># Full fine-tuning dataset<br/>sft_dataset_file = f'{data_path}sft_train_data.json'<br/><br/># Data collected from the the mini-test<br/>mini_data_path = f'{data_path}mini_data.json'<br/><br/># Test data containing all three outputs<br/>all_tests_data = f'{data_path}all_tests.json'<br/><br/># The adjusted training dataset<br/>train_data_path = f'{data_path}sft_train_data.json'<br/><br/># Create a path variable for the SFT model to be saved locally<br/>sft_model_path = '/content/drive/MyDrive/llama3RE/Llama3_RE/'</span></pre><p id="1754" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Now that our workspace is set up, we can move to the first step, which is to build a synthetic dataset for the task of relation extraction.</p><h1 id="82e3" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Creating a Synthetic Dataset for Relation Extraction with Llama3–70B</h1><p id="5176" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">There are several relation extraction datasets available, with the best-known being the <a class="af ov" href="https://paperswithcode.com/dataset/conll04" rel="noopener ugc nofollow" target="_blank">CoNLL04</a> dataset. Additionally, there are excellent datasets such as <a class="af ov" href="https://huggingface.co/datasets/web_nlg#dataset-card-for-webnlg" rel="noopener ugc nofollow" target="_blank">web_nlg</a>, available on HuggingFace, and <a class="af ov" href="https://github.com/allenai/SciREX?tab=readme-ov-file" rel="noopener ugc nofollow" target="_blank">SciREX</a> developed by AllenAI. However, most of these datasets come with restrictive licenses.</p><p id="41ab" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Inspired by the format of the <code class="cx ps pt pu pk b">web_nlg</code> dataset we will build our own dataset. This approach will be particularly useful if we plan to fine-tune a model trained on our dataset. To start, we need a collection of short sentences for our relation extraction task. We can compile this corpus in various ways.</p><h2 id="e323" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Gather a Collection of Sentences</h2><p id="ab0f" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We will use <a class="af ov" href="https://huggingface.co/datasets/databricks/databricks-dolly-15k" rel="noopener ugc nofollow" target="_blank">databricks-dolly-15k</a>, an open source dataset generated by Databricks employees in 2023. This dataset is designed for supervised fine-tuning and includes four features: instruction, context, response and category. After analyzing the eight categories, I decided to retain the first sentence of the context from the <code class="cx ps pt pu pk b">information_extraction</code> category. The data parsing steps are outlined below:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="9f90" class="pn ne fq pk b bg po pp l pq pr">from datasets import load_dataset<br/><br/># Load the dataset<br/>dataset = load_dataset("databricks/databricks-dolly-15k")<br/><br/># Choose the desired category from the dataset<br/>ie_category = [e for e in dataset["train"] if e["category"]=="information_extraction"]<br/><br/># Retain only the context from each instance<br/>ie_context = [e["context"] for e in ie_category]<br/><br/># Split the text into sentences (at the period) and keep the first sentence<br/>reduced_context = [text.split('.')[0] + '.' for text in ie_context]<br/><br/># Retain sequences of specified lengths only (use character length)<br/>sampler = [e for e in reduced_context if 30 &lt; len(e) &lt; 170]</span></pre><p id="f616" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The selection process yields a dataset comprising 1,041 sentences. Given that this is a mini-project, I did not handpick the sentences, and as a result, some samples may not be ideally suited for our task. In a project designated for production, I would carefully select only the most appropriate sentences. However, for the purposes of this project, this dataset will suffice.</p><h2 id="e158" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Format the Data</h2><p id="5575" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We first need to create a system message that will define the input prompt and instruct the model on how to generate the answers:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="f570" class="pn ne fq pk b bg po pp l pq pr">system_message = """You are an experienced annontator. <br/>Extract all entities and the relations between them from the following text. <br/>Write the answer as a triple entity1|relationship|entitity2. <br/>Do not add anything else.<br/>Example Text: Alice is from France.<br/>Answer: Alice|is from|France.<br/>"""</span></pre><p id="9283" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Since this is an experimental phase, I am keeping the demands on the model to a minimum. I did test several other prompts, including some that requested outputs in CoNLL format where entities are categorized, and the model performed quite well. However, for simplicity’s sake, we’ll stick to the basics for now.</p><p id="5ebe" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We also need to convert the data into a conversational format:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="7e82" class="pn ne fq pk b bg po pp l pq pr">messages = [[<br/>    {"role": "system","content": f"{system_message}"},<br/>    {"role": "user", "content": e}] for e in sampler]</span></pre><h2 id="1ec1" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">The Groq Client and API</h2><p id="ec84" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Llama3 was released just a few days ago, and the availability of API options is still limited. While a chat interface is available for Llama3–70B, this project requires an API that could process my 1,000 sentences with a couple lines of code. I found this excellent <a class="af ov" href="https://www.youtube.com/watch?v=ySwJT3Z1MFI" rel="noopener ugc nofollow" target="_blank">YouTube video</a> that explains how to use the GroqCloud API for free. For more details please refer to the video.</p><p id="e247" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Just a reminder: you’ll need to log in and retrieve a free API Key from the <a class="af ov" href="https://console.groq.com/playground" rel="noopener ugc nofollow" target="_blank">GroqCloud</a> website. My API key is already saved in the Google Colab secrets. We start by initializing the Groq client:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="da33" class="pn ne fq pk b bg po pp l pq pr">import os<br/>from groq import Groq<br/><br/>gclient = Groq(<br/>    api_key=userdata.get("GROQ"),<br/>)</span></pre><p id="1395" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Next we need to define a couple of helper functions that will enable us to interact with the <a class="af ov" href="http://meta.ai/" rel="noopener ugc nofollow" target="_blank">Meta.ai</a> chat interface effectively (these are adapted from the <a class="af ov" href="https://www.youtube.com/watch?v=ySwJT3Z1MFI" rel="noopener ugc nofollow" target="_blank">YouTube video</a>):</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="7dfa" class="pn ne fq pk b bg po pp l pq pr">import time<br/>from tqdm import tqdm<br/><br/>def process_data(prompt):<br/><br/>    """Send one request and retrieve model's generation."""<br/><br/>    chat_completion = gclient.chat.completions.create(<br/>        messages=prompt, # input prompt to send to the model<br/>        model="llama3-70b-8192", # according to GroqCloud labeling<br/>        temperature=0.5, # controls diversity<br/>        max_tokens=128, # max number tokens to generate<br/>        top_p=1, # proportion of likelihood weighted options to consider<br/>        stop=None, # string that signals to stop generating<br/>        stream=False, # if set partial messages are sent<br/>    )<br/>    return chat_completion.choices[0].message.content<br/><br/><br/>def send_messages(messages):<br/><br/>    """Process messages in batches with a pause between batches."""<br/>   <br/>   batch_size = 10<br/>    answers = []<br/><br/>    for i in tqdm(range(0, len(messages), batch_size)): # batches of size 10<br/><br/>        batch = messages[i:i+10]  # get the next batch of messages<br/><br/>        for message in batch:<br/>            output = process_data(message)<br/>            answers.append(output)<br/><br/>        if i + 10 &lt; len(messages):  # check if there are batches left<br/>            time.sleep(10)  # wait for 10 seconds<br/><br/>    return answers</span></pre><p id="e67a" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The first function <code class="cx ps pt pu pk b">process_data()</code> serves as a wrapper for the chat completion function of the Groq client. The second function <code class="cx ps pt pu pk b">send_messages()</code>, processes the data in small batches. If you follow the Settings link on the Groq playground page, you will find a link to <a class="af ov" href="https://console.groq.com/settings/limits" rel="noopener ugc nofollow" target="_blank">Limits</a> which details the conditions under which we can use the free API, including caps on the number of requests and generated tokens. To avoid exceedind these limits, I added a 10-seconds delay after each batch of 10 messages, although it wasn’t strictly necessary in my case. You might want to experiment with these settings.</p><p id="4dbc" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">What remains now is to generate our relation extraction data and integrate it with the initial dataset :</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="c235" class="pn ne fq pk b bg po pp l pq pr"># Data generation with Llama3-70B<br/>answers = send_messages(messages)<br/><br/># Combine input data with the generated dataset<br/>combined_dataset = [{'text': user, 'gold_re': output} for user, output in zip(sampler, answers)]</span></pre><h1 id="1cb4" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Evaluating Llama3–8B for Relation Extraction</h1><p id="de4d" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Before proceeding with fine-tuning the model, it’s important to evaluate its performance on several samples to determine if fine-tuning is indeed necessary.</p><h2 id="0a3d" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Building a Testing Dataset</h2><p id="0817" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We will select 20 samples from the dataset we just constructed and set them aside for testing. The remainder of the dataset will be used for fine-tuning.</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="ab7a" class="pn ne fq pk b bg po pp l pq pr">import random<br/>random.seed(17)<br/><br/># Select 20 random entries<br/>mini_data = random.sample(combined_dataset, 20)<br/><br/># Build conversational format<br/>parsed_mini_data = [[{'role': 'system', 'content': system_message},<br/>                     {'role': 'user', 'content': e['text']}] for e in mini_data]<br/><br/># Create the training set<br/>train_data = [item for item in combined_dataset if item not in mini_data]</span></pre><p id="27d2" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We will use the GroqCloud API and the utilities defined above, specifying <code class="cx ps pt pu pk b">model=llama3-8b-8192</code> while the rest of the function remains unchanged. In this case, we can directly process our small dataset without concern of exceeded the API limits.</p><p id="12d8" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Here is a sample output that provides the original <code class="cx ps pt pu pk b">text</code>, the Llama3-70B generation denoted <code class="cx ps pt pu pk b">gold_re</code> and the Llama3-8B hgeneration labelled <code class="cx ps pt pu pk b">test_re</code>.</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="12a9" class="pn ne fq pk b bg po pp l pq pr">{'text': 'Long before any knowledge of electricity existed, people were aware of shocks from electric fish.',<br/> 'gold_re': 'people|were aware of|shocks\nshocks|from|electric fish\nelectric fish|had|electricity',<br/> 'test_re': 'electric fish|were aware of|shocks'}</span></pre><p id="614f" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">For the full test dataset, please refer to the <a class="af ov" href="https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab notebook.</a></p><p id="dfc4" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Just from this example, it becomes clear that Llama3–8B could benefit from some improvements in its relation extraction capabilities. Let’s work on enhancing that.</p><h1 id="6f5a" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Supervised Fine Tuning of Llama3–8B</h1><p id="1cb9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We will utilize a full arsenal of techniques to assist us, including QLoRA and Flash Attention. I won’t delve into the specifics of choosing hyperparameters here, but if you’re interested in exploring further, check out these great references [<a class="af ov" href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms" rel="noopener ugc nofollow" target="_blank">4</a>] and [<a class="af ov" href="https://www.philschmid.de/fine-tune-llms-in-2024-with-trl" rel="noopener ugc nofollow" target="_blank">5</a>].</p><p id="62e8" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The A100 GPU supports Flash Attention and bfloat16, and it possesses about 40GB of memory, which is sufficient for our fine-tuning needs.</p><h2 id="9c4c" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk"><strong class="al">Preparing the SFT Dataset</strong></h2><p id="94fd" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We start by parsing the dataset into a conversational format, including a system message, input text and the desired answer, which we derive from the Llama3–70B generation. We then save it as a HuggingFace dataset:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="edf9" class="pn ne fq pk b bg po pp l pq pr">def create_conversation(sample):<br/>    return {<br/>        "messages": [<br/>            {"role": "system","content": system_message},<br/>            {"role": "user", "content": sample["text"]},<br/>            {"role": "assistant", "content": sample["gold_re"]}<br/>        ]<br/>    }<br/><br/>from datasets import load_dataset, Dataset<br/><br/>train_dataset = Dataset.from_list(train_data)<br/><br/># Transform to conversational format<br/>train_dataset = train_dataset.map(create_conversation,<br/>                      remove_columns=train_dataset.features,<br/>                      batched=False)</span></pre><h2 id="be88" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Choose the Model</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="de6f" class="pn ne fq pk b bg po pp l pq pr">model_id  =  "meta-llama/Meta-Llama-3-8B"</span></pre><h2 id="bad4" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Load the Tokenizer</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="4dc7" class="pn ne fq pk b bg po pp l pq pr">from transformers import AutoTokenizer<br/><br/># Tokenizer<br/>tokenizer = AutoTokenizer.from_pretrained(model_id,<br/>                                          use_fast=True,<br/>                                          trust_remote_code=True)<br/><br/>tokenizer.pad_token = tokenizer.eos_token<br/>tokenizer.pad_token_id =  tokenizer.eos_token_id<br/>tokenizer.padding_side = 'left'<br/><br/># Set a maximum length<br/>tokenizer.model_max_length = 512</span></pre><h2 id="1f54" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Choose Quantization Parameters</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="cfc6" class="pn ne fq pk b bg po pp l pq pr">from transformers import BitsAndBytesConfig<br/><br/>bnb_config = BitsAndBytesConfig(<br/>    load_in_4bit=True,<br/>    bnb_4bit_use_double_quant=True,<br/>    bnb_4bit_quant_type="nf4",<br/>    bnb_4bit_compute_dtype=torch.bfloat16<br/>)</span></pre><h2 id="499f" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Load the Model</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="9c06" class="pn ne fq pk b bg po pp l pq pr">from transformers import AutoModelForCausalLM<br/>from peft import prepare_model_for_kbit_training<br/>from trl import setup_chat_format<br/><br/>device_map = {"": torch.cuda.current_device()} if torch.cuda.is_available() else None<br/><br/>model = AutoModelForCausalLM.from_pretrained(<br/>    model_id,<br/>    device_map=device_map,<br/>    attn_implementation="flash_attention_2",<br/>    quantization_config=bnb_config<br/>)<br/><br/>model, tokenizer = setup_chat_format(model, tokenizer)<br/>model = prepare_model_for_kbit_training(model)</span></pre><h2 id="40f0" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">LoRA Configuration</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="4570" class="pn ne fq pk b bg po pp l pq pr">from peft import LoraConfig<br/><br/># According to Sebastian Raschka findings<br/>peft_config = LoraConfig(<br/>        lora_alpha=128, #32<br/>        lora_dropout=0.05,<br/>        r=256,  #16<br/>        bias="none",<br/>        target_modules=["q_proj", "o_proj", "gate_proj", "up_proj", <br/>          "down_proj", "k_proj", "v_proj"],<br/>        task_type="CAUSAL_LM",<br/>)</span></pre><p id="d432" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The best results are achieved when targeting all the linear layers. If memory constraints are a concern, opting for more standard values such as alpha=32 and rank=16 can be beneficial, as these settings result in significantly fewer parameters.</p><h2 id="9bcd" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Training Arguments</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="bf9b" class="pn ne fq pk b bg po pp l pq pr">from transformers import TrainingArguments<br/><br/># Adapted from  Phil Schmid blogpost<br/>args = TrainingArguments(<br/>    output_dir=sft_model_path,              # directory to save the model and repository id<br/>    num_train_epochs=2,                     # number of training epochs<br/>    per_device_train_batch_size=4,          # batch size per device during training<br/>    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass<br/>    gradient_checkpointing=True,            # use gradient checkpointing to save memory, use in distributed training<br/>    optim="adamw_8bit",                     # choose paged_adamw_8bit if not enough memory<br/>    logging_steps=10,                       # log every 10 steps<br/>    save_strategy="epoch",                  # save checkpoint every epoch<br/>    learning_rate=2e-4,                     # learning rate, based on QLoRA paper<br/>    bf16=True,                              # use bfloat16 precision<br/>    tf32=True,                              # use tf32 precision<br/>    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper<br/>    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper<br/>    lr_scheduler_type="constant",           # use constant learning rate scheduler<br/>    push_to_hub=True,                       # push model to Hugging Face hub<br/>    hub_model_id="llama3-8b-sft-qlora-re",<br/>    report_to="tensorboard",               # report metrics to tensorboard<br/>    )</span></pre><p id="4fa1" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">If you choose to save the model locally, you can omit the last three parameters. You may also need to adjust the <code class="cx ps pt pu pk b">per_device_batch_size</code> and <code class="cx ps pt pu pk b">gradient_accumulation_steps</code> to prevent Out of Memory (OOM) errors.</p><h2 id="3bf6" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">Initialize the Trainer and Train the Model</h2><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="778e" class="pn ne fq pk b bg po pp l pq pr">from trl import SFTTrainer<br/><br/>trainer = SFTTrainer(<br/>    model=model,<br/>    args=args,<br/>    train_dataset=sft_dataset,<br/>    peft_config=peft_config,<br/>    max_seq_length=512,<br/>    tokenizer=tokenizer,<br/>    packing=False, # True if the dataset is large<br/>    dataset_kwargs={<br/>        "add_special_tokens": False,  # the template adds the special tokens<br/>        "append_concat_token": False, # no need to add additional separator token<br/>    }<br/>)<br/><br/>trainer.train()<br/>trainer.save_model()</span></pre><p id="3c5f" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">The training, including model saving, took about 10 minutes.</p><p id="5abf" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Let’s clear the memory to prepare for inference tests. If you’re using a GPU with less memory and encounter CUDA Out of Memory (OOM) errors, you might need to restart the runtime.</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="2548" class="pn ne fq pk b bg po pp l pq pr">import torch<br/>import gc<br/>del model<br/>del tokenizer<br/>gc.collect()<br/>torch.cuda.empty_cache()</span></pre><h1 id="6907" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Inference with SFT Model</h1><p id="70ec" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this final step we will load the base model in half precision along with the Peft adapter. For this test, I have chosen not to merge the model with the adapter.</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="3cbe" class="pn ne fq pk b bg po pp l pq pr">from peft import AutoPeftModelForCausalLM<br/>from transformers import AutoTokenizer, pipeline<br/>import torch<br/><br/># HF model<br/>peft_model_id = "solanaO/llama3-8b-sft-qlora-re"<br/><br/># Load Model with PEFT adapter<br/>model = AutoPeftModelForCausalLM.from_pretrained(<br/>  peft_model_id,<br/>  device_map="auto",<br/>  torch_dtype=torch.float16,<br/>  offload_buffers=True<br/>)</span></pre><p id="aa50" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">Next, we load the tokenizer:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="7e65" class="pn ne fq pk b bg po pp l pq pr">okenizer = AutoTokenizer.from_pretrained(peft_model_id)<br/><br/>tokenizer.pad_token = tokenizer.eos_token<br/>tokenizer.pad_token_id =  tokenizer.eos_token_id</span></pre><p id="f0ed" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">And we build the text generation pipeline:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="ce4a" class="pn ne fq pk b bg po pp l pq pr">pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)</span></pre><p id="bf86" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We load the test dataset, which consists of the 20 samples we set aside previously, and format the data in a conversational style. However, this time we omit the assistant message and format it as a Hugging Face dataset:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="f375" class="pn ne fq pk b bg po pp l pq pr">def create_input_prompt(sample):<br/>    return {<br/>        "messages": [<br/>            {"role": "system","content": system_message},<br/>            {"role": "user", "content": sample["text"]},<br/>        ]<br/>    }<br/>    <br/>from datasets import Dataset<br/><br/>test_dataset = Dataset.from_list(mini_data)<br/><br/># Transform to conversational format<br/>test_dataset = test_dataset.map(create_input_prompt,<br/>                      remove_columns=test_dataset.features,<br/>                      batched=False)</span></pre><h2 id="5204" class="pv ne fq bf nf pw px py ni pz qa qb nl oi qc qd qe om qf qg qh oq qi qj qk ql bk">One Sample Test</h2><p id="695e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s generate relation extraction output using SFT Llama3–8B and compare it to the previous two outputs on a single instance:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="5f9a" class="pn ne fq pk b bg po pp l pq pr"> Generate the input prompt<br/>prompt = pipe.tokenizer.apply_chat_template(test_dataset[2]["messages"][:2],<br/>                                            tokenize=False,<br/>                                            add_generation_prompt=True)<br/># Generate the output<br/>outputs = pipe(prompt,<br/>              max_new_tokens=128,<br/>              do_sample=False,<br/>              temperature=0.1,<br/>              top_k=50,<br/>              top_p=0.1,<br/>              )<br/># Display the results<br/>print(f"Question: {test_dataset[2]['messages'][1]['content']}\n")<br/>print(f"Gold-RE: {test_sampler[2]['gold_re']}\n")<br/>print(f"LLama3-8B-RE: {test_sampler[2]['test_re']}\n")<br/>print(f"SFT-Llama3-8B-RE: {outputs[0]['generated_text'][len(prompt):].strip()}")</span></pre><p id="a9c7" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">We obtain the following:</p><pre class="mn mo mp mq mr pj pk pl bp pm bb bk"><span id="46c8" class="pn ne fq pk b bg po pp l pq pr">Question: Long before any knowledge of electricity existed, people were aware of shocks from electric fish.<br/><br/>Gold-RE: people|were aware of|shocks<br/>    shocks|from|electric fish<br/>    electric fish|had|electricity<br/><br/>LLama3-8B-RE: electric fish|were aware of|shocks<br/><br/>SFT-Llama3-8B-RE: people|were aware of|shocks<br/>         shocks|from|electric fish</span></pre><p id="6c27" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">In this example, we observe significant improvements in the relation extraction capabilities of Llama3–8B through fine-tuning. Despite the fine-tuning dataset being neither very clean nor particularly large, the results are impressive.</p><p id="6dbf" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">For the complete results on the 20-sample dataset, please refer to the <a class="af ov" href="https://github.com/SolanaO/Blogs_Content/blob/master/llama3_re/Llama3_RE_Inference_SFT.ipynb" rel="noopener ugc nofollow" target="_blank">Google Colab notebook</a>. Note that the inference test takes longer because we load the model in half-precision.</p><h1 id="b3ce" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusion</h1><p id="edc7" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In conclusion, by utilizing Llama3–70B and an available dataset, we successfully created a synthetic dataset which was then used to fine-tune Llama3–8B for a specific task. This process not only familiarized us with Llama3, but also allowed us to apply straightforward techniques from Hugging Face. We observed that working with Llama3 closely resembles the experience with Llama2, with the notable improvements being enhanced output quality and a more effective tokenizer.</p><p id="d035" class="pw-post-body-paragraph nz oa fq ob b go ow od oe gr ox og oh oi oy ok ol om oz oo op oq pa os ot ou fj bk">For those interested in pushing the boundaries further, consider challenging the model with more complex tasks such as categorizing entities and relationships, and using these classifications to build a knowledge graph.</p><h1 id="7e4e" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><ol class=""><li id="1de8" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou qm pc pd bk">Somin Wadhwa, Silvio Amir, Byron C. Wallace, Revisiting Relation Extraction in the era of Large Language Models, <a class="af ov" href="https://arxiv.org/pdf/2305.05003.pdf" rel="noopener ugc nofollow" target="_blank">arXiv.2305.05003</a> (2023).</li><li id="b20e" class="nz oa fq ob b go pe od oe gr pf og oh oi pg ok ol om ph oo op oq pi os ot ou qm pc pd bk">Meta, Introducing Meta Llama 3: The most capable openly available LLM to date, April 18, 2024 (<a class="af ov" href="https://ai.meta.com/blog/meta-llama-3/" rel="noopener ugc nofollow" target="_blank">link)</a>.</li><li id="cf17" class="nz oa fq ob b go pe od oe gr pf og oh oi pg ok ol om ph oo op oq pi os ot ou qm pc pd bk">Philipp Schmid, Omar Sanseviero, Pedro Cuenca, Youndes Belkada, Leandro von Werra, <a class="af ov" href="https://huggingface.co/blog/llama3" rel="noopener ugc nofollow" target="_blank">Welcome Llama 3 — Met’s new open LLM,</a> April 18, 2024.</li><li id="6154" class="nz oa fq ob b go pe od oe gr pf og oh oi pg ok ol om ph oo op oq pi os ot ou qm pc pd bk">Sebastian Raschka, <a class="af ov" href="https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms" rel="noopener ugc nofollow" target="_blank">Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</a>, Ahead of AI, Nov 19, 2023.</li><li id="4531" class="nz oa fq ob b go pe od oe gr pf og oh oi pg ok ol om ph oo op oq pi os ot ou qm pc pd bk">Philipp Schmid, <a class="af ov" href="https://www.philschmid.de/fine-tune-llms-in-2024-with-trl" rel="noopener ugc nofollow" target="_blank">How to Fine-Tune LLMs in 2024 with Hugging Face,</a> Jan 22, 2024.</li></ol><h1 id="c9f3" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Dataset</h1><p id="caaf" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><a class="af ov" href="https://huggingface.co/datasets/databricks/databricks-dolly-15k" rel="noopener ugc nofollow" target="_blank">databricks-dolly-15K</a> on Hugging Face platform (CC BY-SA 3.0)</p><h1 id="251f" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Full Code and Processed Data</h1><p id="4a54" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><a class="af ov" href="https://github.com/SolanaO/Blogs_Content/tree/master/llama3_re" rel="noopener ugc nofollow" target="_blank">Github Repo</a></p></div></div></div></div>    
</body>
</html>