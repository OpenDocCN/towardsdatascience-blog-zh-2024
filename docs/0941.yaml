- en: How to Encode Constraints to the Output of Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何为神经网络的输出编码约束
- en: 原文：[https://towardsdatascience.com/how-to-encode-constraints-to-the-output-of-neural-networks-9bce302b9687?source=collection_archive---------2-----------------------#2024-04-14](https://towardsdatascience.com/how-to-encode-constraints-to-the-output-of-neural-networks-9bce302b9687?source=collection_archive---------2-----------------------#2024-04-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-encode-constraints-to-the-output-of-neural-networks-9bce302b9687?source=collection_archive---------2-----------------------#2024-04-14](https://towardsdatascience.com/how-to-encode-constraints-to-the-output-of-neural-networks-9bce302b9687?source=collection_archive---------2-----------------------#2024-04-14)
- en: A summary of available approaches
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可用方法总结
- en: '[](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)[![Runzhong
    Wang](../Images/964d8ff22734d69fea6bb7256fe5d84d.png)](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)
    [Runzhong Wang](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)[![Runzhong
    Wang](../Images/964d8ff22734d69fea6bb7256fe5d84d.png)](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)
    [Runzhong Wang](https://medium.com/@runzhong.wang1?source=post_page---byline--9bce302b9687--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)
    ·12 min read·Apr 14, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9bce302b9687--------------------------------)
    ·12分钟阅读·2024年4月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5e6583bd3bcbd023d68ee777d90bee66.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e6583bd3bcbd023d68ee777d90bee66.png)'
- en: Image generated by ChatGPT based on this article’s content.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由ChatGPT根据本文内容生成。
- en: 'Neural networks are indeed powerful. However, as the application scope of neural
    networks moves from “standard” classification and regression tasks to more complex
    decision-making and AI for Science, one drawback is becoming increasingly apparent:
    the output of neural networks is usually unconstrained, or more precisely, constrained
    only by simple 0–1 bounds (Sigmoid activation function), non-negative constraints
    (ReLU activation function), or constraints that sum to one (Softmax activation
    function). These “standard” activation layers have been used to handle classification
    and regression problems and have witnessed the vigorous development of deep learning.
    However, as neural networks started to be widely used for decision-making, optimization
    solving, and other complex scientific problems, these “standard” activation layers
    are clearly no longer sufficient. This article will briefly discuss the current
    methodologies available that can add constraints to the output of neural networks,
    with some personal insights included. Feel free to critique and discuss any related
    topics.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络确实非常强大。然而，随着神经网络应用范围从“标准”的分类和回归任务扩展到更复杂的决策和科学AI，逐渐显现出一个缺点：神经网络的输出通常是没有约束的，或者更准确地说，通常仅受简单的0-1范围（Sigmoid激活函数）、非负约束（ReLU激活函数）或加和为1的约束（Softmax激活函数）限制。这些“标准”激活层曾用于处理分类和回归问题，并见证了深度学习的蓬勃发展。然而，随着神经网络开始广泛应用于决策、优化求解以及其他复杂的科学问题，这些“标准”激活层显然已经不再足够。本文将简要讨论当前可以为神经网络的输出添加约束的现有方法，并包含一些个人见解。欢迎批评和讨论相关话题。
- en: '[[中文版本(知乎)]](https://zhuanlan.zhihu.com/p/667124121)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[[中文版本(知乎)]](https://zhuanlan.zhihu.com/p/667124121)'
- en: If one shot doesn’t work, try multiple shots
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果一次不行，尝试多次
- en: 'If you are familiar with reinforcement learning, you may already know what
    I am talking about. Applying constraints to an n-dimensional vector seems difficult,
    but you can break an n-dimensional vector into n outputs. Each time an output
    is generated, you can manually write the code to restrict the action space for
    the next variable to ensure its value stays within a feasible domain. This so-called
    “autoregressive” method has obvious advantages: it is simple and can handle a
    rich variety of constraints (as long as you can write the code). However, its
    disadvantages are also clear: an n-dimensional vector requires n calls to the
    network’s forward computation, which is inefficient; moreover, this method usually
    needs to be modeled as a Markov Decision Process (MDP) and trained through reinforcement
    learning, so common challenges in reinforcement learning such as large action
    spaces, sparse reward functions, and long training times are also unavoidable.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉强化学习，你可能已经知道我在说什么。将约束应用于一个n维向量看似困难，但你可以将n维向量分解为n个输出。每次生成一个输出时，你可以手动编写代码，限制下一个变量的行动空间，以确保其值保持在一个可行的范围内。这种所谓的“自回归”方法有明显的优势：它简单并且能够处理各种约束（只要你能编写代码）。然而，它的缺点也很明显：一个n维向量需要进行n次网络前向计算调用，这效率较低；此外，这种方法通常需要建模为马尔可夫决策过程（MDP）并通过强化学习进行训练，因此强化学习中的常见挑战，如庞大的行动空间、稀疏奖励函数和长时间训练，也难以避免。
- en: In the domain of solving combinatorial optimization problems with neural networks,
    the autoregressive method coupled with reinforcement learning was once mainstream,
    but it is currently being replaced by more efficient methods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用神经网络解决组合优化问题的领域，自回归方法结合强化学习曾是主流，但目前正在被更高效的方法所取代。
- en: Perhaps… Let’s learn the constraints?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 或许…我们来学习一下约束条件吧？
- en: During training, a penalty term can be added to the objective function, representing
    the degree to which the current neural network output violates constraints. In
    the traditional optimization field, the Lagrangian dual method also offers a similar
    trick. Unfortunately, when applied to neural networks, these methods have so far
    only been proven on some simple constraints, and it is still unclear whether they
    are applicable to more complex constraints. One shortcoming is that inevitably
    some of the model’s capacity is used to learn how to meet corresponding constraints,
    thereby limiting the model’s ability in other directions (such as optimization
    solving).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，可以向目标函数中添加惩罚项，表示当前神经网络输出违反约束的程度。在传统的优化领域，拉格朗日对偶法也提供了类似的技巧。不幸的是，当应用到神经网络时，这些方法迄今为止只在一些简单的约束下得到了验证，目前尚不清楚它们是否适用于更复杂的约束。一个缺点是，模型的部分能力不得不用于学习如何满足相应的约束，从而限制了模型在其他方向（如优化求解）上的能力。
- en: 'For example, [*Karalias and Loukas, NeurIPS’21 “Erdo˝s Goes Neural: an Unsupervised
    Learning Framework for Combinatorial Optimization on Graphs”*](https://proceedings.neurips.cc/paper/2020/file/49f85a9ed090b20c8bed85a5923c669f-Paper.pdf)
    demonstrated that the so-called “box constraints”, where variable values lie between
    [a, b], can be learned through a penalty term, and the network can solve some
    relatively simple combinatorial optimization problems. However, our further study
    found that this methodology lacks generalization ability. In the training set,
    the neural network can maintain constraints well; but in the testing set, the
    constraints are almost completely lost. Moreover, although adding a penalty term
    in principle can apply to any constraint, it cannot handle more difficult constraints.
    Our paper [*Wang et al, ICLR’23 “Towards One-Shot Neural Combinatorial Optimization
    Solvers: Theoretical and Empirical Notes on the Cardinality-Constrained Case”*](https://openreview.net/pdf?id=h21yJhdzbwz)
    discusses the above phenomena and presents the theoretical analysis.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，[*Karalias和Loukas，NeurIPS’21“Erdo˝s Goes Neural: 一个无监督学习框架用于图上的组合优化”*](https://proceedings.neurips.cc/paper/2020/file/49f85a9ed090b20c8bed85a5923c669f-Paper.pdf)展示了所谓的“盒约束”，即变量值位于[a,
    b]之间，可以通过惩罚项学习，网络能够解决一些相对简单的组合优化问题。然而，我们的进一步研究发现，这种方法缺乏泛化能力。在训练集上，神经网络能够很好地维持约束；但是在测试集上，约束几乎完全丧失。此外，尽管在理论上添加惩罚项可以适用于任何约束，但它无法处理更复杂的约束。我们的论文[*Wang等，ICLR’23“朝向一次性神经组合优化求解器：基于基数约束的理论与实证分析”*](https://openreview.net/pdf?id=h21yJhdzbwz)讨论了上述现象并提供了理论分析。'
- en: 'On the other hand, the design philosophy of generative models, where outputs
    need to conform to a specific distribution, seems more suited to the “learning
    constraints” approach. [*Sun and Yang, NeurIPS’23 “DIFUSCO: Graph-based Diffusion
    Solvers for Combinatorial Optimization”*](https://proceedings.neurips.cc/paper_files/paper/2023/file/0ba520d93c3df592c83a611961314c98-Paper-Conference.pdf)
    showed that Diffusion models can output solutions that meet the constraints of
    the Traveling Salesman Problem (i.e., can output a complete route). We further
    presented [*Li et al, NeurIPS’23 “T2T: From Distribution Learning in Training
    to Gradient Search in Testing for Combinatorial Optimization”*](https://proceedings.neurips.cc/paper_files/paper/2023/file/0ba520d93c3df592c83a611961314c98-Paper-Conference.pdf),
    where the generative model (Diffusion) is responsible for meeting constraints,
    with another optimizer providing optimization guidance during the gradual denoising
    process of Diffusion. This strategy performed pretty well in experiments, surpassing
    all previous neural network solvers.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '另一方面，生成模型的设计理念要求输出符合特定分布，这似乎更适合“学习约束”方法。[*Sun和Yang, NeurIPS’23 “DIFUSCO: 基于图的扩散求解器用于组合优化”*](https://proceedings.neurips.cc/paper_files/paper/2023/file/0ba520d93c3df592c83a611961314c98-Paper-Conference.pdf)表明，扩散模型可以输出满足旅行商问题约束的解（即，能够输出完整的路径）。我们进一步展示了[*Li等人,
    NeurIPS’23 “T2T: 从训练中的分布学习到测试中的梯度搜索，用于组合优化”*](https://proceedings.neurips.cc/paper_files/paper/2023/file/0ba520d93c3df592c83a611961314c98-Paper-Conference.pdf)，其中生成模型（扩散模型）负责满足约束，另一个优化器则在扩散的逐步去噪过程中提供优化指导。这个策略在实验中表现得相当好，超越了所有之前的神经网络求解器。'
- en: 'Yet another interesting perspective: Solving a convex optimization problem'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个有趣的视角：求解一个凸优化问题
- en: Maybe you are concerned that autoregressive is too inefficient, and generative
    models may not solve your problem. You might be thinking about a neural network
    that does only one forward pass, and the output needs to meet the given constraints
    — is that possible?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你担心自回归模型效率过低，而生成模型可能无法解决你的问题。你可能在考虑一个只进行一次前向传播的神经网络，而输出需要满足给定的约束——这可能吗？
- en: The answer is yes. We can solve a convex optimization problem to project the
    neural network’s output into a feasible domain bounded by convex constraints.
    This methodology utilizes the property that a convex optimization problem is differentiable
    at its KKT conditions so that this projection step can be regarded as an activation
    layer, embeddable in an end-to-end neural network. This methodology was proposed
    and promoted by Zico Kolter’s group at CMU, and they currently offer the [cvxpylayers
    package](https://github.com/cvxgrp/cvxpylayers) to ease the implementation steps.
    The corresponding convex optimization problem is
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。我们可以求解一个凸优化问题，将神经网络的输出投影到由凸约束界定的可行域中。这种方法利用了凸优化问题在其KKT条件下可微的特性，因此这个投影步骤可以视为一个激活层，嵌入到端到端的神经网络中。这种方法由Zico
    Kolter的团队在CMU提出并推广，他们目前提供了[cvxpylayers包](https://github.com/cvxgrp/cvxpylayers)来简化实现步骤。相应的凸优化问题是
- en: where **y** is the unconstrained neural network output, **x** is the constrained
    neural network output. Because the purpose of this step is just a projection,
    a linear objective function can achieve this (adding an entropy regularizer is
    also reasonable). **Ax** ≤ **b** are the linear constraints you need to apply,
    which can also be quadratic or other convex constraints.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**y**是无约束的神经网络输出，**x**是有约束的神经网络输出。因为这一步的目的是仅仅进行投影，所以线性目标函数可以实现这一点（添加熵正则化也是合理的）。**Ax**
    ≤ **b**是你需要施加的线性约束，也可以是二次或其他凸约束。
- en: 'It is a personal note: there seem to be some [known issues](https://github.com/cvxgrp/cvxpylayers/issues/147),
    and it seems that this repository has not been updated/maintained for a long time
    (04/2024). I would truly appreciate it if anyone is willing to investigate what
    is going on.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个人的备注：似乎有一些[已知问题](https://github.com/cvxgrp/cvxpylayers/issues/147)，并且这个仓库似乎很长时间没有更新/维护了（04/2024）。如果有人愿意调查一下发生了什么，我将非常感激。
- en: 'For non-convex problems: Which gradient approximation do you prefer?'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于非凸问题：你更倾向于使用哪种梯度近似方法？
- en: Deriving gradients using KKT conditions is theoretically sound, but it cannot
    tackle non-convex or non-continuous problems. In fact, for non-continuous problems,
    when changes in problem parameters cause solution jumps, the real gradient becomes
    a delta function (i.e., infinite at the jump), which obviously can’t be used in
    training neural networks. Fortunately, there are some gradient approximation methods
    that can tackle this problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KKT条件推导梯度在理论上是可行的，但它无法解决非凸或不连续问题。事实上，对于不连续问题，当问题参数的变化导致解跳跃时，真实的梯度变成了一个δ函数（即在跳跃处无穷大），显然不能在神经网络训练中使用。幸运的是，有一些梯度近似方法可以解决这个问题。
- en: 'The Georg Martius group at Max Planck Institute introduced a black-box approximation
    method [*Vlastelica et al, ICLR’2020 “Differentiation of Blackbox Combinatorial
    Solvers”*](https://openreview.net/pdf?id=BkevoJSYPB), which views the solver as
    a black box. It first calls the solver once, then perturbs the problem parameters
    in a specific direction, and then calls the solver again. The residual between
    the outputs of the two solver calls serves as the approximate gradient. If this
    methodology is applied to the output of neural networks to enforce constraints,
    we can define an optimization problem with a linear objective function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 马克斯·普朗克研究所的Georg Martius小组提出了一种黑箱近似方法[*Vlastelica 等人，ICLR 2020 “黑箱组合求解器的微分”*](https://openreview.net/pdf?id=BkevoJSYPB)，将求解器视为黑箱。它首先调用一次求解器，然后沿特定方向扰动问题参数，再次调用求解器。两次求解器调用的输出之间的残差作为近似梯度。如果将这种方法应用于神经网络的输出以强制执行约束，我们可以定义一个线性目标函数的优化问题：
- en: where **y** is the unconstrained neural network output, and **x** is the constrained
    neural network output. Your next step is to implement an algorithm to solve the
    above problem (not necessarily to be optimal), and then it can be integrated into
    the black-box approximation framework. A drawback of the black-box approximation
    method is that it can only handle linear objective functions, but a linear objective
    function just happens to work if you are looking for some methods to enforce constraints;
    moreover, since it is just a gradient approximation method if the hyperparameters
    are not well-tuned, it might encounter sparse gradients and convergence issues.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**y**是未约束的神经网络输出，**x**是受约束的神经网络输出。你的下一步是实现一个算法来解决上述问题（不一定是最优的），然后可以将其集成到黑箱近似框架中。黑箱近似方法的一个缺点是它只能处理线性目标函数，但线性目标函数恰好在你寻找强制约束的方法时起作用；此外，由于它只是一个梯度近似方法，如果超参数没有调得很好，可能会遇到稀疏梯度和收敛问题。
- en: Another method for approximating gradients involves using a large amount of
    random noise perturbation, repeatedly calling the solver to estimate a gradient,
    as discussed in [*Berthet et al, NeurIPS’2020 “Learning with Differentiable Perturbed
    Optimizers”*](https://papers.nips.cc/paper/2020/file/6bb56208f672af0dd65451f869fedfd9-Paper.pdf).
    Theoretically, the gradient obtained this way should be similar to the gradient
    obtained through the LinSAT method (which will be discussed in the next section),
    being the gradient of an entropy-regularized linear objective function; however,
    in practice, this method requires a large number of random samples, which is kind
    of impractical (at least on my use cases).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种近似梯度的方法是使用大量随机噪声扰动，反复调用求解器来估计梯度，正如在[*Berthet 等人，NeurIPS 2020 “使用可微扰动优化器进行学习”*](https://papers.nips.cc/paper/2020/file/6bb56208f672af0dd65451f869fedfd9-Paper.pdf)中讨论的那样。从理论上讲，通过这种方式获得的梯度应该与通过LinSAT方法获得的梯度类似（将在下一节讨论），即一个熵正则化线性目标函数的梯度；然而，实际上，这种方法需要大量的随机样本，这在我的使用案例中有点不切实际。
- en: 'Self-promotion time: Projection without solving optimization'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自我推销时间：在不解决优化问题的情况下进行投影
- en: Whether it’s deriving gradients from KKT conditions for convex problems or approximating
    gradients for non-convex methods, both require calling/writing a solver, whereby
    the CPU-GPU communication could be a bottleneck because most solvers are usually
    designed and implemented for CPUs. Is there a way to project specific constraints
    directly on the GPU like an activation layer, without solving optimization problems
    explicitly?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是从KKT条件推导凸问题的梯度，还是近似非凸方法的梯度，都需要调用/编写求解器，因此CPU-GPU通信可能成为瓶颈，因为大多数求解器通常是为CPU设计和实现的。是否有一种方法可以像激活层一样直接在GPU上投影特定的约束，而不显式地解决优化问题？
- en: 'The answer is yes, and our [*Wang et al, ICML’2023 “LinSATNet: The Positive
    Linear Satisfiability Neural Networks”*](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf)
    presents a viable path and derives the convergence property of the algorithm.
    LinSAT stands for **Lin**ear **SAT**isfiability Network.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '答案是肯定的，我们的 [*Wang 等人, ICML''2023 “LinSATNet: 正线性可满足性神经网络”*](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf)
    论文提供了一条可行的路径，并推导了该算法的收敛性质。LinSAT 代表 **Lin**ear **SAT**isfiability Network（线性可满足性网络）。'
- en: LinSAT can be seen as an activation layer, allowing you to apply general positive
    linear constraints to the output of a neural network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LinSAT 可以看作是一个激活层，使您能够对神经网络的输出应用一般的正线性约束。
- en: '![](../Images/103e965ed6105a4a11aafc4e1a58ac6c.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/103e965ed6105a4a11aafc4e1a58ac6c.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The LinSAT layer is fully differentiable, and the real gradients are computed
    by autograd, just like other activation layers. Our implementation now supports
    PyTorch.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LinSAT 层是完全可微的，真实的梯度通过自动求导计算，就像其他激活层一样。我们的实现现在支持 PyTorch。
- en: You can install it by
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式安装：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And get started with
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 并开始使用
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A quick example
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的示例
- en: If you download and run the source code, you will find a simple example. In
    this example, we apply doubly stochastic constraints to a 3×3 matrix.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您下载并运行源代码，您会发现一个简单的示例。在这个示例中，我们对一个 3×3 的矩阵施加了双重随机约束。
- en: 'To run the example, first clone the repo:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例，首先克隆仓库：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Go into the repo, and run the example code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 进入仓库并运行示例代码：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, we try to enforce doubly-stochastic constraints to a 3×3 matrix.
    The doubly stochastic constraint means that all rows and columns of the matrix
    should sum to 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们尝试对一个 3×3 的矩阵施加双重随机约束。双重随机约束意味着矩阵的所有行和列的和都应为 1。
- en: 'The 3x3 matrix is flattened into a vector, and the following positive linear
    constraints are considered (for **Ex**=**f**):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3 矩阵被展平为一个向量，然后考虑以下正线性约束（对于 **Ex**=**f**）：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We randomly init **w** and regard it as the output of some neural networks:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机初始化 **w**，并将其视为某些神经网络的输出：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We also have a “ground-truth target” for the output of linsat_layer, which
    is a diagonal matrix in this example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个“真实目标”，它是 linsat_layer 输出的目标，在这个示例中，它是一个对角矩阵：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The forward/backward passes of LinSAT follow the standard PyTorch style and
    are readily integrated into existing deep learning pipelines.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: LinSAT 的前向/反向传播遵循标准的 PyTorch 风格，可以轻松集成到现有的深度学习管道中。
- en: 'The forward pass:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 前向传播：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The backward pass:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also set E as a sparse matrix to improve the time & memory efficiency
    (especially for large-sized input). Here is a dumb example (consider to construct
    E in sparse for the best efficiency):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将 E 设置为稀疏矩阵，以提高时间和内存效率（尤其是对于大尺寸输入）。以下是一个简单的示例（建议为了最佳效率构造稀疏形式的 E）：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can also do gradient-based optimization over w to make the output of linsat_layer
    closer to x_gt. This is what happens when you train a
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对 w 进行基于梯度的优化，使得 linsat_layer 的输出更接近 x_gt。这就是您训练时发生的情况。
- en: neural network.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络。
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And you are likely to see the loss decreasing during the training steps.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您可能会看到损失值逐步减小。
- en: For full API references, please check out [the GitHub repository](https://github.com/Thinklab-SJTU/LinSATNet?tab=readme-ov-file#api-reference).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有关完整的 API 参考，请查看 [GitHub 仓库](https://github.com/Thinklab-SJTU/LinSATNet?tab=readme-ov-file#api-reference)。
- en: How does LinSAT work?
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LinSAT 是如何工作的？
- en: Warning, tons of math ahead! You can safely skip this part if you are just using
    LinSAT.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 警告，接下来有大量数学内容！如果您只是使用 LinSAT，您可以安全跳过这一部分。
- en: If you want to learn more details and proofs, please refer to [the main paper](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf).
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果您想了解更多细节和证明，请参阅 [主论文](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf)。
- en: Here we introduce the mechanism inside LinSAT. It works by extending the Sinkhorn
    algorithm to multiple sets of marginals (to our best knowledge, we are the first
    to study Sinkhorn with multi-sets of marginals). The positive linear constraints
    are then enforced by transforming the constraints into marginals.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍 LinSAT 内部的机制。它通过将 Sinkhorn 算法扩展到多个集合的边际来工作（据我们所知，我们是第一个研究具有多集合边际的 Sinkhorn
    算法的人）。然后，通过将约束转化为边际，来强制执行正线性约束。
- en: Classic Sinkhorn with single-set marginals
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经典 Sinkhorn 单集边际
- en: Let’s start with the classic Sinkhorn algorithm. Given non-negative score matrix
    **S** with size *m×n*, and a set of marginal distributions on rows (non-negative
    vector **v** with size *m*) and columns (non-negative vector **u** with size *n*),
    where
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从经典的Sinkhorn算法开始。给定一个大小为*m×n*的非负评分矩阵**S**，以及一组行（大小为*m*的非负向量**v**）和列（大小为*n*的非负向量**u**）的边际分布，其中：
- en: the Sinkhorn algorithm outputs a normalized matrix Γ with size *m×n* and values
    in [0,1] so that
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Sinkhorn算法输出一个标准化矩阵Γ，大小为*m×n*，值域在[0,1]之间，因此
- en: Conceptually, Γᵢ ⱼ means the **proportion** of *u*ⱼ moved to *v*ᵢ.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，Γᵢ ⱼ表示**比例**，即从*u*ⱼ移动到*v*ᵢ的部分。
- en: 'The algorithm steps are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 算法步骤如下：
- en: Note that the above formulation is modified from the conventional Sinkhorn formulation.
    Γᵢ ⱼ uⱼ is equivalent to the elements in the “transport” matrix in papers such
    as [(Cuturi 2013](https://arxiv.org/pdf/1306.0895v1.pdf)). We prefer this new
    formulation as it generalizes smoothly to Sinkhorn with multi-set marginals in
    the following.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，上述公式是对传统Sinkhorn公式的修改。Γᵢ ⱼ uⱼ等价于“运输”矩阵中的元素，如[(Cuturi 2013)](https://arxiv.org/pdf/1306.0895v1.pdf)等论文所示。我们更倾向于采用这种新的公式，因为它能够平滑地扩展到以下带有多集边际分布的Sinkhorn算法。
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: To make a clearer comparison, the transportation matrix in [(Cuturi 2013)](https://arxiv.org/pdf/1306.0895v1.pdf)
    is **P** with size m×n, and the constraints are
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了更清晰的对比，[(Cuturi 2013)](https://arxiv.org/pdf/1306.0895v1.pdf)中的运输矩阵是**P**，大小为m×n，约束条件为：
- en: Pᵢ ⱼ means the **exact mass** moved from uⱼ to vᵢ.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pᵢ ⱼ表示从uⱼ到vᵢ移动的**精确质量**。
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The algorithm steps are:'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 算法步骤如下：
- en: Extended Sinkhorn with multi-set marginals
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展Sinkhorn算法与多集边际分布
- en: We discover that the Sinkhorn algorithm can generalize to multiple sets of marginals.
    Recall that Γᵢ ⱼ ∈ [0,1] means the proportion of *u*ⱼ moved to *v*ᵢ. Interestingly,
    it yields the same formulation if we simply replace **u**, **v** with another
    set of marginal distributions, suggesting the potential of extending the Sinkhorn
    algorithm to multiple sets of marginal distributions. Denote that there are *k*
    sets of marginal distributions that are jointly enforced to fit more complicated
    real-world scenarios. The sets of marginal distributions are
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现Sinkhorn算法可以推广到多个边际分布集。回顾一下，Γᵢ ⱼ ∈ [0,1]表示从*u*ⱼ移动到*v*ᵢ的比例。有趣的是，如果我们简单地将**u**、**v**替换为另一个边际分布集，得到的公式是相同的，这表明Sinkhorn算法有潜力扩展到多个边际分布集。设有*k*个边际分布集，这些边际分布集被联合施加约束以适应更复杂的现实场景。这些边际分布集为：
- en: 'and we have:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们得到：
- en: It assumes the existence of a normalized **Z** ∈ [0,1] with size *m×n*, s.t.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 假设存在一个标准化的**Z** ∈ [0,1]，大小为*m×n*，使得：
- en: 'i.e., the multiple sets of marginal distributions have a non-empty feasible
    region (you may understand the meaning of “non-empty feasible region” after reading
    the next section about how to handle positive linear constraints). Multiple sets
    of marginal distributions could be jointly enforced by traversing the Sinkhorn
    iterations over *k* sets of marginal distributions. The algorithm steps are:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 即，多个边际分布集有一个非空的可行区域（你可以在阅读下一节关于如何处理正线性约束时理解“非空可行区域”的含义）。通过遍历Sinkhorn迭代来联合执行多个边际分布集的约束，多个边际分布集可以共同施加约束。算法步骤如下：
- en: In [our paper](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf), we
    prove that the Sinkhorn algorithm for multi-set marginals shares the same convergence
    pattern with the classic Sinkhorn, and its underlying formulation is also similar
    to the classic Sinkhorn.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我们的论文](https://proceedings.mlr.press/v202/wang23at/wang23at.pdf)中，我们证明了多集边际分布的Sinkhorn算法与经典Sinkhorn算法具有相同的收敛模式，且其基础公式也与经典Sinkhorn相似。
- en: Transforming positive linear constraints into marginals
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将正线性约束转换为边际分布
- en: Then we show how to transform the positive linear constraints into marginals,
    which are handled by our proposed multi-set Sinkhorn.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们展示如何将正线性约束转换为边际分布，这些边际分布由我们提出的多集Sinkhorn处理。
- en: '**Encoding neural network’s output** For an *l*-length vector denoted as **y**
    (which can be the output of a neural network, also it is the input to **linsat_layer**),
    the following matrix is built'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码神经网络的输出** 对于一个长度为*l*的向量**y**（它可以是神经网络的输出，也可以是**linsat_layer**的输入），构建以下矩阵：'
- en: 'where **W** is of size 2 × (*l* + 1), and β is the dummy variable, the default
    is β = 0\. **y** is put at the upper-left region of **W**. The entropic regularizer
    is then enforced to control discreteness and handle potential negative inputs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**W**的大小为2 × (*l* + 1)，β是虚拟变量，默认值为β = 0。**y**位于**W**的左上区域。然后，施加熵正则化器以控制离散性并处理潜在的负输入：
- en: The score matrix **S** is taken as the input of Sinkhorn for multi-set marginals.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 得分矩阵 **S** 被作为 Sinkhorn 算法的输入来处理多集合边际。
- en: '**From linear constraints to marginals**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**从线性约束到边际**'
- en: '**1) Packing constraint** **Ax** ≤ **b**. Assuming that there is only one constraint,
    we rewrite the constraint as'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**1）打包约束** **Ax** ≤ **b**。假设只有一个约束，我们将其重写为'
- en: Following the “transportation” view of Sinkhorn, the output **x** *moves* at
    most *b* unit of mass from *a*₁*, a*₂*, …, aₗ*, and the dummy dimension allows
    the inequality by *moving* mass from the dummy dimension. It is also ensured that
    the sum of **u***ₚ*equals the sum of **v***ₚ*. The marginal distributions are
    defined as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循 Sinkhorn 的“运输”视角，输出 **x** *最多移动* *b* 单位的质量，从 *a*₁*, a₂*, …, aₗ*，并且虚拟维度允许通过
    *移动* 质量来实现不等式。还确保 **u***ₚ* 的总和等于 **v***ₚ* 的总和。边际分布被定义为
- en: '**2 ) Covering constraint** **Cx** ≥ **d**. Assuming that there is only one
    constraint, we rewrite the constraint as'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**2）覆盖约束** **Cx** ≥ **d**。假设只有一个约束，我们将其重写为'
- en: We introduce the multiplier
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了乘数
- en: because we always have
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们总是有
- en: (else the constraint is infeasible), and we cannot reach a feasible solution
    where all elements in **x** are 1s without this multiplier. Our formulation ensures
    that at least *d* unit of mass is *moved* from c₁*, c*₂*, …, cₗ* by **x**, thus
    representing the covering constraint of “greater than”. It is also ensured that
    the sum of **u_**c equals the sum of **v**_c. The marginal distributions are defined
    as
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （否则约束是不可行的），如果没有这个乘数，我们无法得到一个可行的解，其中所有 **x** 中的元素都是 1。我们的公式确保至少 *d* 单位的质量通过
    **x** 从 c₁*, c₂*, …, cₗ* 被 *移动*，从而表示“大于”的覆盖约束。还确保 **u_**c 的总和等于 **v**_c 的总和。边际分布被定义为
- en: '**3) Equality constraint** **Ex** = **f**. Representing the equality constraint
    is more straightforward. Assuming that there is only one constraint, we rewrite
    the constraint as'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**3）等式约束** **Ex** = **f**。表示等式约束更加直接。假设只有一个约束，我们将其重写为'
- en: The output **x** *moves* e₁*, e*₂*, …, eₗ* to *f*, and we need no dummy element
    in **u**ₑ because it is an equality constraint. It is also ensured that the sum
    of **u**ₑ equals the sum of **v**ₑ. The marginal distributions are defined as
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 **x** *移动* e₁*, e₂*, …, eₗ* 到 *f*，我们在 **u**ₑ 中不需要虚拟元素，因为这是一个等式约束。还确保 **u**ₑ
    的总和等于 **v**ₑ 的总和。边际分布被定义为
- en: After encoding all constraints and stacking them as multiple sets of marginals,
    we can call the Sinkhorn algorithm for multi-set marginals to encode the constraints.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在对所有约束进行编码并将它们堆叠为多个边际集合后，我们可以调用 Sinkhorn 算法来处理多集合边际，从而编码约束。
- en: Experimental Validation of LinSAT
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LinSAT 实验验证
- en: In our ICML paper, we validated the LinSATNet method for routing constraints
    beyond the general case (used for solving variants of the Traveling Salesman Problem),
    partial graph matching constraints (used in graph matching where only subsets
    of graphs match each other), and general linear constraints (used in specific
    preference with portfolio optimization). All these problems can be represented
    with positive linear constraints and handled using the LinSATNet method. In experiments,
    neural networks are capable of learning how to solve all three problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 ICML 论文中，我们验证了 LinSATNet 方法在路由约束方面的有效性，超出了普通情况（用于解决旅行商问题的变体）、部分图匹配约束（用于仅有部分图匹配的图匹配问题）和一般线性约束（用于特定的偏好与投资组合优化）。所有这些问题都可以用正线性约束表示，并通过
    LinSATNet 方法处理。在实验中，神经网络能够学习如何解决这三种问题。
- en: It should be noted that the LinSATNet method can only handle **positive linear
    constraints**, meaning that it is unable to handle constraints like *x*₁ — *x*₂
    ≤ 0 which contain negative terms. However, positive linear constraints already
    cover a vast array of scenarios. For each specific problem, the mathematical modeling
    is often not unique, and in many cases, a reasonable positive linear formulation
    could be found. In addition to the examples mentioned above, let the network output
    organic molecules (represented as graphs, ignoring hydrogen atoms, considering
    only the skeleton) can consider constraints such as C atoms having no more than
    4 bonds, O atoms having no more than 2 bonds.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，LinSATNet 方法只能处理**正线性约束**，这意味着它无法处理像 *x*₁ — *x*₂ ≤ 0 这样包含负项的约束。然而，正线性约束已经涵盖了大量的场景。对于每个具体问题，数学建模往往不是唯一的，在许多情况下，可以找到合理的正线性表达式。除了上述提到的示例外，让网络输出有机分子（表示为图，忽略氢原子，只考虑骨架结构）时，可以考虑像
    C 原子最多有 4 个键，O 原子最多有 2 个键这样的约束。
- en: Afterword
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后记
- en: Adding constraints to neural networks has a wide range of application scenarios,
    and so far, several methods are available. It’s important to note that there is
    no golden standard to judge their superiority over each other — the best method
    is usually relevant to a certain scenario.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 向神经网络添加约束具有广泛的应用场景，目前已有几种方法可供选择。需要注意的是，没有一个公认的标准来判断它们之间的优劣——最好的方法通常与特定场景相关。
- en: Of course, I recommend trying out LinSATNet! Anyway, it is as simple as an activation
    layer in your network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我推荐尝试 LinSATNet！反正它和网络中的激活层一样简单。
- en: 'If you found this article helpful, please feel free to cite:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章对你有帮助，欢迎随时引用：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: All aforementioned content has been discussed in this paper.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前述内容已经在本文中讨论过。
