["```py\nfrom ragas import evaluate\nfrom ragas.metrics import AnswerRelevancy, ContextRelevancy, Faithfulness\nimport weave\n\n@weave.op()\ndef evaluate_with_ragas(query, model_output):\n    # Put data into a Dataset object\n    data = {\n        \"question\": [query],\n        \"contexts\": [[model_output['context']]],\n        \"answer\": [model_output['answer']]\n    }\n    dataset = Dataset.from_dict(data)\n\n    # Define metrics to judge\n    metrics = [\n        AnswerRelevancy(),\n        ContextRelevancy(),\n        Faithfulness(),\n    ]\n\n    judge_model = ChatOpenAI(model=config['JUDGE_MODEL_NAME'])\n    embeddings_model = OpenAIEmbeddings(model=config['EMBEDDING_MODEL_NAME'])\n\n    evaluation = evaluate(dataset=dataset, metrics=metrics, llm=judge_model, embeddings=embeddings_model)\n\n    return {\n        \"answer_relevancy\": float(evaluation['answer_relevancy']),\n        \"context_relevancy\": float(evaluation['context_relevancy']),\n        \"faithfulness\": float(evaluation['faithfulness']),\n    }\n\ndef run_evaluation():\n    # Initialize chat model\n    model = rosebud_chat_model()\n\n    # Define evaluation questions\n    questions = [\n        {\"query\": \"Suggest a good movie based on a book.\"},  # Adaptations\n        {\"query\": \"Suggest a film for a cozy night in.\"},  # Mood-Based\n        {\"query\": \"What are some must-watch horror movies?\"},  # Genre-Specific\n        ...\n        # Total of 20 questions\n    ]\n\n    # Create Weave Evaluation object\n    evaluation = weave.Evaluation(dataset=questions, scorers=[evaluate_with_ragas])\n\n    # Run the evaluation\n    asyncio.run(evaluation.evaluate(model))\n\nif __name__ == \"__main__\":\n    weave.init('film-search')\n    run_evaluation()\n```", "```py\ndef start_log_feedback(feedback):\n    print(\"Logging feedback.\")\n    st.session_state.feedback_given = True\n    st.session_state.sentiment = feedback\n    thread = threading.Thread(target=log_feedback, args=(st.session_state.sentiment,\n                                                         st.session_state.query,\n                                                         st.session_state.query_constructor,\n                                                         st.session_state.context,\n                                                         st.session_state.response))\n    thread.start()\n\ndef log_feedback(sentiment, query, query_constructor, context, response):\n    ct = datetime.datetime.now()\n    wandb.init(project=\"film-search\",\n               name=f\"query: {ct}\")\n    table = wandb.Table(columns=[\"sentiment\", \"query\", \"query_constructor\", \"context\", \"response\"])\n    table.add_data(sentiment,\n                   query,\n                   query_constructor,\n                   context,\n                   response\n                   )\n    wandb.log({\"Query Log\": table})\n    wandb.finish()\n```", "```py\n{\n    \"query\": \"drama English dogs\", \n    \"filter\": {\n        \"operator\": \"and\", \n        \"arguments\": [\n            {\n                \"comparator\": \"eq\", \"attribute\": \"Genre\", \"value\": \"Drama\"\n            }, \n            {\n                \"comparator\": \"eq\", \"attribute\": \"Language\", \"value\": \"English\"\n            }, \n\n            {\n                \"comparator\": \"lt\", \"attribute\": \"Runtime (minutes)\", \"value\": 120\n            }\n        ]\n    },\n}\n```", "```py\n@task\ndef start():\n    \"\"\"\n    Start-up: check everything works or fail fast!\n    \"\"\"\n\n    # Print out some debug info\n    print(\"Starting flow!\")\n\n    # Ensure user has set the appropriate env variables\n    assert os.environ['LANGCHAIN_API_KEY']\n    assert os.environ['OPENAI_API_KEY']\n    ...\n\n@task(retries=3, retry_delay_seconds=[1, 10, 100])\ndef pull_data_to_csv(config):\n    TMBD_API_KEY = os.getenv('TMBD_API_KEY')\n    YEARS = range(config[\"years\"][0], config[\"years\"][-1] + 1)\n    CSV_HEADER = ['Title', 'Runtime (minutes)', 'Language', 'Overview', ...]\n\n    for year in YEARS:\n        # Grab list of ids for all films made in {YEAR}\n        movie_list = list(set(get_id_list(TMBD_API_KEY, year)))\n\n        FILE_NAME = f'./data/{year}_movie_collection_data.csv'\n\n        # Creating file\n        with open(FILE_NAME, 'w') as f:\n            writer = csv.writer(f)\n            writer.writerow(CSV_HEADER)\n\n        ...\n\n    print(\"Successfully pulled data from TMDB and created csv files in data/\")\n\n@task\ndef convert_csv_to_docs():\n    # Loading in data from all csv files\n    loader = DirectoryLoader(\n        ...\n        show_progress=True)\n\n    docs = loader.load()\n\n    metadata_field_info = [\n        AttributeInfo(name=\"Title\",\n                      description=\"The title of the movie\", type=\"string\"),\n        AttributeInfo(name=\"Runtime (minutes)\",\n                      description=\"The runtime of the movie in minutes\", type=\"integer\"),\n        ...\n    ]\n\n    def convert_to_list(doc, field):\n        if field in doc.metadata and doc.metadata[field] is not None:\n            doc.metadata[field] = [item.strip()\n                                   for item in doc.metadata[field].split(',')]\n\n    ...\n\n    fields_to_convert_list = ['Genre', 'Actors', 'Directors',\n                              'Production Companies', 'Stream', 'Buy', 'Rent']\n    ...\n\n    # Set 'overview' and 'keywords' as 'page_content' and other fields as 'metadata'\n    for doc in docs:\n        # Parse the page_content string into a dictionary\n        page_content_dict = dict(line.split(\": \", 1)\n                                 for line in doc.page_content.split(\"\\n\") if \": \" in line)\n\n        doc.page_content = (\n            'Title: ' + page_content_dict.get('Title') +\n            '. Overview: ' + page_content_dict.get('Overview') +\n            ...\n        )\n\n        ...\n\n    print(\"Successfully took csv files and created docs\")\n\n    return docs\n\n@task\ndef upload_docs_to_pinecone(docs, config):\n    # Create empty index\n    PINECONE_KEY, PINECONE_INDEX_NAME = os.getenv(\n        'PINECONE_API_KEY'), os.getenv('PINECONE_INDEX_NAME')\n\n    pc = Pinecone(api_key=PINECONE_KEY)\n\n    # Target index and check status\n    pc_index = pc.Index(PINECONE_INDEX_NAME)\n    print(pc_index.describe_index_stats())\n\n    embeddings = OpenAIEmbeddings(model=config['EMBEDDING_MODEL_NAME'])\n    namespace = \"film_search_prod\"\n\n    PineconeVectorStore.from_documents(\n        docs,\n        ...\n    )\n\n    print(\"Successfully uploaded docs to Pinecone vector store\")\n\n@task\ndef publish_dataset_to_weave(docs):\n    # Initialize Weave\n    weave.init('film-search')\n\n    rows = []\n    for doc in docs:\n        row = {\n            'Title': doc.metadata.get('Title'),\n            'Runtime (minutes)': doc.metadata.get('Runtime (minutes)'),\n             ...\n        }\n        rows.append(row)\n\n    dataset = Dataset(name='Movie Collection', rows=rows)\n    weave.publish(dataset)\n    print(\"Successfully published dataset to Weave\")\n\n@flow(log_prints=True)\ndef pinecone_flow():\n    with open('./config.json') as f:\n        config = json.load(f)\n\n    start()\n    pull_data_to_csv(config)\n    docs = convert_csv_to_docs()\n    upload_docs_to_pinecone(docs, config)\n    publish_dataset_to_weave(docs)\n\nif __name__ == \"__main__\":\n    pinecone_flow.deploy(\n        name=\"pinecone-flow-deployment\",\n        work_pool_name=\"my-aci-pool\",\n        cron=\"0 0 * * 0\",\n        image=DeploymentImage(\n            name=\"prefect-flows:latest\",\n            platform=\"linux/amd64\",\n        )\n    )\n```", "```py\nprefect work-pool create --type azure-container-instance:push --provision-infra my-aci-pool\nprefect deployment run 'get_repo_info/my-deployment'\n```"]