- en: How Google Used Your Data to Improve their Music AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28](https://towardsdatascience.com/how-google-used-your-data-to-improve-their-music-ai-8948a1e85491?source=collection_archive---------10-----------------------#2024-02-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MusicLM fine-tuned on user preferences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[![Max
    Hilsdorf](../Images/01da76c553e43d5ed6b6849bdbfd00da.png)](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    [Max Hilsdorf](https://medium.com/@maxhilsdorf?source=post_page---byline--8948a1e85491--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8948a1e85491--------------------------------)
    ·7 min read·Feb 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9180fef19f159a138075f11e75b4085a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Firmbee.com](https://unsplash.com/@firmbee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is MusicLM?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MusicLM, Google’s flagship text-to-music AI, was originally published in early
    2023\. Even in its basic version, it represented a major breakthrough and caught
    the music industry by surprise. However, a few weeks ago, MusicLM received a **significant
    update**. Here’s a side-by-side comparison for two selected prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt: “Dance music with a melodic synth line and arpeggiation”:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Old MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-7.wav)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-7.wav)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt: “a nostalgic tune played by accordion band”**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Old MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musiclm-27.wav)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**New MusicLM**](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav):
    [https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav](https://google-research.github.io/seanet/musiclm/rlhf/audio_samples/musicrlhf-ru-27.wav)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This increase in quality can be attributed to a new paper by Google Research
    titled: “MusicRL: Aligning Music Generation to Human Preferenc\es”. Apparently,
    this upgrade was considered so significant that they decided to rename the model.
    However, under the hood, MusicRL is identical to MusicLM in its key architecture.
    The only difference: **Finetuning**.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Finetuning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building an AI model from scratch, it starts with zero knowledge and essentially
    does random guessing. The model then extracts useful patterns through training
    on data and starts displaying increasingly intelligent behavior as training progresses.
    One downside to this approach is that **training from scratch requires a lot of
    data**. Finetuning is the idea that an existing model is used and adapted to a
    new task, or adapted to approach the same task differently. Because the model
    already has learned the most important patterns, **much less data is required**.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a powerful open-source LLM like Mistral7B can be trained from scratch
    by anyone, in principle. However, the amount of data required to produce even
    remotely useful outputs is gigantic. Instead, companies use the existing Mistral7B
    model and feed it a small amount of proprietary data to make it solve new tasks,
    whether that is writing SQL queries or classifying emails.
  prefs: []
  type: TYPE_NORMAL
- en: The **key takeawa**y is that finetuning does not change the fundamental structure
    of the model. It only adapts its internal logic slightly to perform better on
    a specific task. Now, let’s use this knowledge to understand how Google finetuned
    MusicLM on user data.
  prefs: []
  type: TYPE_NORMAL
- en: How Google Gathered User Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A few months after the MusicLM paper, a public demo was released as part of
    Google’s AI Test Kitchen. There, users could experiment with the text-to-music
    model for free. However, you might know the saying: **If the product is free,
    YOU are the product**. Unsurprisingly, Google is no exception to this rule. When
    using MusicLM’s public demo, you were occasionally confronted with two generated
    outputs and asked to state which one you prefer. Through this method, Google was
    able to gather **300,000 user preferences** within a couple of months.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64266a0a7a328012b4c1dbecbdaae7d6.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of the user preference ratings captured in the MusicLM public playground.
    Image taken from the [MusicRL paper](https://arxiv.org/pdf/2402.04229.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the screenshot, users were **not explicitly informed** that
    their preferences would be used for machine learning. While that may feel unfair,
    it is important to note that many of our actions in the internet are being used
    for ML training, whether it is our Google search history, our Instagram likes,
    or our private Spotify playlists. In comparison to these rather personal and sensitive
    cases, music preferences on the MusicLM playground seem negligible.
  prefs: []
  type: TYPE_NORMAL
- en: Example of User Data Collection on Linkedin Collaborative Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is good to be aware that user data collection for machine learning is happening
    all the time and usually without explicit consent. If you are on Linkedin, you
    might have been invited to contribute to so-called “collaborative articles”. Essentially,
    users are invited to provide tips on questions in their domain of expertise. Here
    is an example of a [collaborative article on how to write a successful folk song](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)
    (something I didn’t know I needed).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88b7b5725451f53f3869162771126314.png)'
  prefs: []
  type: TYPE_IMG
- en: Header of a [collaborative article](https://www.linkedin.com/advice/3/how-can-you-write-successful-folk-songs-skills-music-industry-w4i5e?trk=cah1)
    on songwriting. On the right side, I am asked to contribute to earn a “Top Voice”
    badge.
  prefs: []
  type: TYPE_NORMAL
- en: Users are incentivized to contribute, earning them a “Top Voice” badge on the
    platform. However, my impression is that **noone actually reads these articles**.
    This leads me to believe that these thousands of question-answer pairs are being
    used by Microsoft (owner of Linkedin) to **train an expert AI system** on these
    data. If my suspicion is accurate, I would find this example much more problematic
    than Google asking users for their favorite track.
  prefs: []
  type: TYPE_NORMAL
- en: '**But back to MusicLM!**'
  prefs: []
  type: TYPE_NORMAL
- en: How Google Took Advantage of this User Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next question is how Google was able to use this massive collection of user
    preferences to finetune MusicLM. The secret lies in a technique called **Reinforcement
    Learning from Human Feedback (RLHF)** which was one of the key breakthroughs of
    ChatGPT back in 2022\. In RLHF, human preferences are used to train an AI model
    that learns to imitate human preference decisions, resulting in an artificial
    human rater. Once this so-called **reward model** is trained, it can take in any
    two tracks and predict which one would most likely be preferred by human raters.
  prefs: []
  type: TYPE_NORMAL
- en: With the reward model set up, MusicLM could be finetuned to maximize the predicted
    user preference of its outputs. This means that the text-to-music model generated
    thousands of tracks, each track receiving a rating from the reward model. Through
    the iterative adaptation of the model weights, MusicLM learned to generate music
    that the artificial human rater “likes”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed2b47a614f28e51438c1ed647417056.png)'
  prefs: []
  type: TYPE_IMG
- en: RLHF explained. Image taken from [MusicRL](https://arxiv.org/abs/2402.04229)
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the finetuning on user preferences, MusicLM was also finetuned
    concerning two other criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Prompt Adherence**'
  prefs: []
  type: TYPE_NORMAL
- en: '[MuLan](https://research.google/pubs/mulan-a-joint-embedding-of-music-audio-and-natural-language/),
    Google’s proprietary text-to-audio embedding model was used to calculate the similarity
    between the user prompt and the generated audio. During finetuning, this adherence
    score was maximized.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Audio Quality**'
  prefs: []
  type: TYPE_NORMAL
- en: Google trained another reward model on user data to evaluate the subjective
    audio quality of its generated outputs. These user data seem to have been collected
    in separate surveys, not in MusicLM’s public demo.
  prefs: []
  type: TYPE_NORMAL
- en: How Much Better is the New MusicLM?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new, finetuned model seems to **reliably outperform the old MusicLM**, listen
    to the samples provided on the [demo page](https://google-research.github.io/seanet/musiclm/rlhf/).
    Of course, a selected public demo can be deceiving, as the authors are incentivized
    to showcase examples that make their new model look as good as possible. Hopefully,
    we will get to test out MusicRL in a public playground, soon.
  prefs: []
  type: TYPE_NORMAL
- en: However, the paper also provides a **quantitative assessment** of subjective
    quality. For this, Google conducted a study and asked users to compare two tracks
    generated for the same prompt, giving each track a score from 1 to 5\. Using this
    metric with the fancy-sounding name Mean Opinion Score (MOS), we can compare not
    only the number of direct comparison wins for each model, but also calculate the
    average rater score (MOS).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/645237ebf9e0b8db1060a1b315579d6e.png)'
  prefs: []
  type: TYPE_IMG
- en: Quantitative benchmarks. Image taken from [MusicRL](https://arxiv.org/abs/2402.04229)
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: Here, MusicLM represents the original MusicLM model. MusicRL-R was only finetuned
    for audio quality and prompt adherence. MusicRL-U was finetuned solely on human
    feedback (the reward model). Finally, MusicRL-RU was finetuned on all three objectives.
    Unsurprisingly, **MusicRL-RU beats all other models** in direct comparison as
    well as on the average ratings.
  prefs: []
  type: TYPE_NORMAL
- en: The paper also reports that MusicRL-RU, the fully finetuned model, beat MusicLM
    in 87% of direct comparisons. The importance of RLHF can be shown by analyzing
    the direct comparisons between MusicRL-R and MusicRL-RU. Here, the latter had
    a 66% win rate, reliably outperforming its competitor.
  prefs: []
  type: TYPE_NORMAL
- en: What are the Implications of This?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the difference in output quality is noticeable, qualitatively as well
    as quantitatively, the new MusicLM is **still quite far from human-level outputs**
    in most cases. Even on the public demo page, many generated outputs sound odd,
    rhythmically, fail to capture key elements of the prompt or suffer from unnatural-sounding
    instruments.
  prefs: []
  type: TYPE_NORMAL
- en: In my opinion, this paper is still significant, as it is the **first attempt
    at using RLHF for music generation**. RLHF has been used extensively in text generation
    for more than one year. But why has this taken so long? I suspect that collecting
    user feedback and finetuning the model is quite costly. Google likely released
    the public MusicLM demo with the primary intention of collecting user feedback.
    This was a smart move and gave them an edge over Meta, which has equally capable
    models, but no open platform to collect user data on.
  prefs: []
  type: TYPE_NORMAL
- en: All in all, Google has pushed itself ahead of the competition by leveraging
    proven finetuning methods borrowed from ChatGPT. While even with RLHF, the new
    MusicLM has still not reached human-level quality, Google can now maintain and
    update its reward model, **improving future generations of text-to-music models**
    with the same finetuning procedure.
  prefs: []
  type: TYPE_NORMAL
- en: It will be interesting to see if and when other competitors like Meta or Stability
    AI will be catching up. For us as users, all of this is just **great news**! We
    get free public demos and more capable models.
  prefs: []
  type: TYPE_NORMAL
- en: For musicians, the pace of the current developments may feel a little threatening
    — and for good reason. I expect to see **human-level text-to-music generation
    in the next 1–3 years**. By that, I mean text-to-music AI that is at least as
    capable at producing music as ChatGPT was at writing texts when it was released.
    Musicians must learn about AI and how it can already support them in their everyday
    work. As the music industry is being disrupted once again, curiosity and flexibility
    will be the primary key to success.
  prefs: []
  type: TYPE_NORMAL
- en: Interested in Music AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you liked this article, you might want to check out some of my other work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“3 Music AI Breakthroughs to Expect in 2024”](https://medium.com/towards-data-science/3-music-ai-breakthroughs-to-expect-in-2024-2d945ae6b5fd).
    Medium Blog'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“Where is Generative AI Music Now?”](https://www.youtube.com/watch?v=OLJi1b-B0i0).
    YouTube Interview on Sync My Music'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[“MusicLM — Has Google Solved AI Music Generation?”](https://medium.com/towards-data-science/musiclm-has-google-solved-ai-music-generation-c6859e76bc3c)
    Medium Blog Post'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also follow me on [Linkedin](https://www.linkedin.com/in/max-hilsdorf/)
    to stay updated about new papers and trends in Music AI.
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading this article!**'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Agostinelli et al., 2023\. MusicLM: Generating Music From Text. [https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cideron et al., 2024\. MusicRL: Aligning Music Generation to Human Preferences.
    [https://arxiv.org/abs/2402.04229](https://arxiv.org/abs/2402.04229)'
  prefs: []
  type: TYPE_NORMAL
