- en: A Complete Guide to BERT with Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT完整指南（附代码）
- en: 原文：[https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11?source=collection_archive---------0-----------------------#2024-05-13)
- en: History, Architecture, Pre-training, and Fine-tuning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 历史、架构、预训练和微调
- en: '[](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)[![Bradney
    Smith](../Images/32634347ac8cfd7c542eca402262fa81.png)](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)
    [Bradney Smith](https://medium.com/@bradneysmith?source=post_page---byline--9f87602e4a11--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)
    ·43 min read·May 13, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f87602e4a11--------------------------------)
    ·阅读时间43分钟·2024年5月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Part 4 in the “LLMs from Scratch” series — a complete guide to understanding
    and building Large Language Models. If you are interested in learning more about
    how these models work I encourage you to read:**'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**“从零开始构建大语言模型”系列的第4部分——完整指南，帮助你理解和构建大语言模型。如果你对这些模型是如何工作的感兴趣，我鼓励你阅读：**'
- en: '[Part 1: Tokenization — A Complete Guide](https://medium.com/p/cedc9f72de4e)'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第1部分：分词——完整指南](https://medium.com/p/cedc9f72de4e)'
- en: '[Part 2: Word Embeddings with word2vec from Scratch in Python](https://medium.com/p/eb9326c6ab7c/)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第2部分：从零开始使用word2vec进行词嵌入（Python版）](https://medium.com/p/eb9326c6ab7c/)'
- en: '[Part 3: Self-Attention Explained with Code](https://medium.com/p/d7a9f0f4d94e)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第3部分：自注意力机制与代码解析](https://medium.com/p/d7a9f0f4d94e)'
- en: '**Part 4: A Complete Guide to BERT with Code**'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4部分：BERT完整指南（附代码）**'
- en: '[Part 5: Mistral 7B Explained: Towards More Efficient Language Models](https://medium.com/p/7f9c6e6b7251)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第5部分：Mistral 7B 解析：走向更高效的语言模型](https://medium.com/p/7f9c6e6b7251)'
- en: '**Introduction**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: Bidirectional Encoder Representations from Transformers (BERT) is a Large Language
    Model (LLM) developed by Google AI Language which has made significant advancements
    in the field of Natural Language Processing (NLP). Many models in recent years
    have been inspired by or are direct improvements to BERT, such as RoBERTa, ALBERT,
    and DistilBERT to name a few. The original BERT model was released shortly after
    OpenAI’s Generative Pre-trained Transformer (GPT), with both building on the work
    of the Transformer architecture proposed the year prior. While GPT focused on
    Natural Language Generation (NLG), BERT prioritised Natural Language Understanding
    (NLU). These two developments reshaped the landscape of NLP, cementing themselves
    as notable milestones in the progression of machine learning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 双向编码器表示（BERT）是由谷歌AI语言团队开发的大型语言模型（LLM），在自然语言处理（NLP）领域取得了显著的进展。近年来，许多模型受到了BERT的启发，或是对BERT的直接改进，例如RoBERTa、ALBERT和DistilBERT等。原始的BERT模型是在OpenAI的生成式预训练变换器（GPT）之后不久发布的，二者都建立在前一年提出的变换器架构基础上。虽然GPT侧重于自然语言生成（NLG），但BERT则优先考虑自然语言理解（NLU）。这两项技术的发展重塑了NLP领域，成为机器学习进步的重要里程碑。
- en: The following article will explore the history of BERT, and detail the landscape
    at the time of its creation. This will give a complete picture of not only the
    architectural decisions made by the paper’s authors, but also an understanding
    of how to train and fine-tune BERT for use in industry and hobbyist applications.
    We will step through a detailed look at the architecture with diagrams and write
    code from scratch to fine-tune BERT on a sentiment analysis task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下文章将探讨BERT的历史，并详细描述其创建时的背景。这将全面展示论文作者所做的架构决策，并帮助理解如何训练和微调BERT，以便在工业和业余应用中使用。我们将通过详细的架构分析和图示，逐步介绍如何编写代码从零开始微调BERT进行情感分析任务。
- en: Contents
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: '[**1** — History and Key Features of BERT](#acda)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[**1** — BERT的历史与关键特性](#acda)'
- en: '[**2** — Architecture and Pre-training Objectives](#320d)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[**2** — 架构与预训练目标](#320d)'
- en: '[**3** — Fine-Tuning BERT for Sentiment Analysis](#ff89)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[**3** — 微调BERT进行情感分析](#ff89)'
- en: '[**4** — Conclusion](#86e2)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[**4** — 结论](#86e2)'
- en: '[**5** — Further Reading](#7f46)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[**5** — 深入阅读](#7f46)'
- en: 1 — History and Key Features of BERT
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 — BERT的历史与关键特性
- en: 'The BERT model can be defined by four main features:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型可以通过四个主要特性来定义：
- en: Encoder-only architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅编码器架构
- en: Pre-training approach
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练方法
- en: Model fine-tuning
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型微调
- en: Use of bidirectional context
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用双向上下文
- en: Each of these features were design choices made by the paper’s authors and can
    be understood by considering the time in which the model was created. The following
    section will walk through each of these features and show how they were either
    inspired by BERT’s contemporaries (the Transformer and GPT) or intended as an
    improvement to them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性是论文作者做出的设计选择，可以通过考虑模型创建时的背景来理解。以下部分将逐一介绍这些特性，并展示它们是如何受到BERT同时代模型（Transformer和GPT）的启发，或是作为对它们的改进而提出的。
- en: 1.1 — **Encoder-Only Architecture**
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 — **仅编码器架构**
- en: 'The debut of the Transformer in 2017 kickstarted a race to produce new models
    that built on its innovative design. OpenAI struck first in June 2018, creating
    GPT: a **decoder-only** model that excelled in NLG, eventually going on to power
    ChatGPT in later iterations. Google responded by releasing BERT four months later:
    an **encoder-only** model designed for NLU. Both of these architectures can produce
    very capable models, but the tasks they are able to perform are slightly different.
    An overview of each architecture is given below.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年Transformer的问世，开启了一场基于其创新设计生产新模型的竞赛。OpenAI于2018年6月率先推出了GPT：一个**仅解码器**模型，在自然语言生成（NLG）任务中表现卓越，最终推动了ChatGPT在后续版本中的发展。谷歌随后在四个月后发布了BERT：一个专为自然语言理解（NLU）设计的**仅编码器**模型。这两种架构都可以生成非常强大的模型，但它们能够执行的任务略有不同。下面是对每种架构的概述。
- en: '**Decoder-Only Models:**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅解码器模型：**'
- en: '**Goal:** Predict a new output sequence in response to an input sequence'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标：** 针对输入序列预测新的输出序列'
- en: '**Overview:** The decoder block in the Transformer is responsible for generating
    an output sequence based on the input provided to the encoder. Decoder-only models
    are constructed by omitting the encoder block entirely and stacking multiple decoders
    together in a single model. These models accept prompts as inputs and generate
    responses by predicting the next most probable word (or more specifically, token)
    one at a time in a task known as Next Token Prediction (NTP). As a result, decoder-only
    models excel in NLG tasks such as: conversational chatbots, machine translation,
    and code generation. These kinds of models are likely the most familiar to the
    general public due to the widespread use of ChatGPT which is powered by decoder-only
    models (GPT-3.5 and GPT-4).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述：** 在Transformer中，解码器模块负责根据提供给编码器的输入生成输出序列。仅解码器模型通过完全省略编码器模块，并将多个解码器堆叠在一个模型中来构建。这些模型接受提示作为输入，并通过预测下一个最可能的词（或更具体地说，token）逐个生成响应，这一任务被称为下一个词预测（Next
    Token Prediction，NTP）。因此，仅解码器模型在自然语言生成（NLG）任务中表现出色，例如：对话式聊天机器人、机器翻译和代码生成。这些模型可能是大众最为熟悉的，因为像ChatGPT这样的应用广泛使用了由仅解码器模型（GPT-3.5和GPT-4）驱动。'
- en: '**Encoder-Only Models:**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅编码器模型：**'
- en: '**Goal:** Make predictions about words within an input sequence'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标：** 对输入序列中的词进行预测'
- en: '**Overview:** The encoder block in the Transformer is responsible for accepting
    an input sequence, and creating rich, numeric vector representations for each
    word (or more specifically, each token). Encoder-only models omit the decoder
    and stack multiple Transformer encoders to produce a single model. These models
    do not accept prompts as such, but rather an input sequence for a prediction to
    be made upon (e.g. predicting a missing word within the sequence). Encoder-only
    models lack the decoder used to generate new words, and so are not used for chatbot
    applications in the way that GPT is used. Instead, encoder-only models are most
    often used for NLU tasks such as: Named Entity Recognition (NER) and sentiment
    analysis. The rich vector representations created by the encoder blocks are what
    give BERT a deep understanding of the input text. The BERT authors argued that
    this architectural choice would improve BERT’s performance compared to GPT, specifically
    writing that decoder-only architectures are:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概述：** Transformer 中的编码器块负责接收输入序列，并为每个单词（或更具体地说，为每个标记）创建丰富的数值向量表示。仅编码器模型省略了解码器，并堆叠多个
    Transformer 编码器来生成一个单一模型。这些模型并不接受提示，而是接受输入序列进行预测（例如，预测序列中的缺失单词）。仅编码器模型缺少用于生成新词的解码器，因此不会像
    GPT 那样用于聊天机器人应用。相反，仅编码器模型通常用于自然语言理解（NLU）任务，如命名实体识别（NER）和情感分析。编码器块创建的丰富向量表示赋予了
    BERT 对输入文本的深刻理解。BERT 的作者认为，这一架构选择相比 GPT 会提高 BERT 的表现，特别是他们指出，仅解码器架构的模型是：'
- en: “sub-optimal for sentence-level tasks, and could be very harmful when applying
    finetuning based approaches to token-level tasks such as question answering” [1]
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在句子级任务中表现 sub-optimal，并且在应用基于微调的方法处理如问答之类的标记级任务时可能非常有害” [1]
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** It is technically possible to generate text with BERT, but as we
    will see, this is not what the architecture was intended for, and the results
    do not rival decoder-only models in any way.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 从技术上讲，可以使用 BERT 生成文本，但正如我们将看到的，这并非该架构的初衷，且其结果在任何方面都无法与仅解码器模型相媲美。'
- en: '**Architecture Diagrams for the Transformer, GPT, and BERT:**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Transformer、GPT 和 BERT 的架构图：**'
- en: Below is an architecture diagram for the three models we have discussed so far.
    This has been created by adapting the architecture diagram from the original Transformer
    paper "Attention is All You Need" [2]. The number of encoder or decoder blocks
    for the model is denoted by `N`. In the original Transformer, `N` is equal to
    6 for the encoder and 6 for the decoder, since these are both made up of six encoder
    and decoder blocks stacked together respectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们迄今为止讨论的三种模型的架构图。该图是通过改编原始 Transformer 论文《Attention is All You Need》中的架构图
    [2] 创建的。模型中编码器或解码器块的数量由 `N` 表示。在原始 Transformer 中，`N` 对编码器和解码器都等于 6，因为它们分别由六个编码器和解码器块堆叠而成。
- en: '![](../Images/c6623d00dc4952aceb3bbacd507ca9f6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6623d00dc4952aceb3bbacd507ca9f6.png)'
- en: A comparison of the architectures for the Transformer, GPT, and BERT. Image
    adapted by author from the Transformer architecture diagram in the “Attention
    is All You Need” paper [2].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Transformer、GPT 和 BERT 架构的比较。该图由作者根据《Attention is All You Need》论文中的 Transformer
    架构图 [2] 改编而来。
- en: '**1.2 — Pre-training Approach**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.2 — 预训练方法**'
- en: 'GPT influenced the development of BERT in several ways. Not only was the model
    the first decoder-only Transformer derivative, but GPT also popularised model
    **pre-training**. Pre-training involves training a single large model to acquire
    a broad understanding of language (encompassing aspects such as word usage and
    grammatical patterns) in order to produce a task-agnostic **foundational model**.
    In the diagrams above, the foundational model is made up of the components below
    the linear layer (shown in purple). Once trained, copies of this foundational
    model can be **fine-tuned** to address specific tasks. Fine-tuning involves training
    only the linear layer: a small feedforward neural network, often called a **classification
    head** or just a **head**. The weights and biases in the remainder of the model
    (that is, the foundational portion) remained unchanged, or **frozen**.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GPT在多个方面影响了BERT的发展。这个模型不仅是第一个仅解码器的Transformer衍生模型，而且GPT还普及了模型的**预训练**。预训练包括训练一个大型模型，以获得对语言的广泛理解（涵盖词汇使用和语法模式等方面），从而生成一个任务无关的**基础模型**。在上述图示中，基础模型由线性层下方的组件（显示为紫色）构成。训练完成后，这个基础模型的副本可以被**微调**以应对特定任务。微调只涉及训练线性层：一个小型的前馈神经网络，通常被称为**分类头**或简称**头**。模型其余部分的权重和偏差（即基础部分）保持不变，或称为**冻结**。
- en: '**Analogy:**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**类比：**'
- en: To construct a brief analogy, consider a sentiment analysis task. Here, the
    goal is to classify text as either `positive` or `negative` based on the sentiment
    portrayed. For example, in some movie reviews, text such as `I loved this movie`
    would be classified as `positive` and text such as `I hated this movie` would
    be classified as `negative`. In the traditional approach to language modelling,
    you would likely train a new architecture from scratch specifically for this one
    task. You could think of this as teaching someone the English language from scratch
    by showing them movie reviews until eventually they are able to classify the sentiment
    found within them. This of course, would be slow, expensive, and require many
    training examples. Moreover, the resulting classifier would still only be proficient
    in this one task. In the pre-training approach, you take a generic model and fine-tune
    it for sentiment analysis. You can think of this as taking someone who is already
    fluent in English and simply showing them a small number of movie reviews to familiarise
    them with the current task. Hopefully, it is intuitive that the second approach
    is much more efficient.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个简短的类比，考虑情感分析任务。在这个任务中，目标是根据文本所表达的情感将其分类为`positive`（正面）或`negative`（负面）。例如，在一些电影评论中，像`I
    loved this movie`这样的文本会被分类为`positive`，而像`I hated this movie`这样的文本会被分类为`negative`。在传统的语言建模方法中，你可能需要从零开始训练一个新的架构，专门针对这个任务。你可以把这看作是通过展示电影评论来教某人英语，直到他们最终能够分类其中的情感。当然，这样的做法既慢又昂贵，并且需要大量的训练样本。而且，得到的分类器仍然只会精通这个任务。在预训练方法中，你会采用一个通用模型并对其进行微调，专门用于情感分析。你可以把这看作是将已经流利掌握英语的人带入情境，仅展示少量的电影评论，让他们熟悉当前的任务。显然，第二种方法要高效得多。
- en: '**Earlier Attempts at Pre-training:**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**早期的预训练尝试：**'
- en: The concept of pre-training was not invented by OpenAI, and had been explored
    by other researchers in the years prior. One notable example is the ELMo model
    (Embeddings from Language Models), developed by researchers at the Allen Institute
    [3]. Despite these earlier attempts, no other researchers were able to demonstrate
    the effectiveness of pre-training as convincingly as OpenAI in their seminal paper.
    In their own words, the team found that their
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的概念并非由OpenAI发明，早在之前的几年，其他研究人员也已探索过这一方向。一个显著的例子是ELMo模型（来自语言模型的嵌入），该模型由Allen
    Institute的研究人员开发[3]。尽管有这些早期尝试，但没有其他研究人员能像OpenAI在其开创性论文中那样有力地证明预训练的有效性。用他们自己的话说，团队发现他们的
- en: “task-agnostic model outperforms discriminatively trained models that use architectures
    specifically crafted for each task, significantly improving upon the state of
    the art” [4].
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “任务无关模型的表现超越了那些使用专门为每个任务精心设计架构的判别式训练模型，显著提高了当前的技术水平”[4]。
- en: This revelation firmly established the pre-training paradigm as the dominant
    approach to language modelling moving forward. In line with this trend, the BERT
    authors also fully adopted the pre-trained approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现坚定地确立了预训练范式作为未来语言建模的主导方法。顺应这一趋势，BERT的作者们也完全采用了预训练方法。
- en: '**1.3 — Model Fine-tuning**'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.3 — 模型微调**'
- en: '**Benefits of Fine-tuning:**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**微调的好处：**'
- en: Fine-tuning has become commonplace today, making it easy to overlook how recent
    it was that this approach rose to prominence. Prior to 2018, it was typical for
    a new model architecture to be introduced for each distinct NLP task. Transitioning
    to pre-training not only drastically decreased the training time and compute cost
    needed to develop a model, but also reduced the volume of training data required.
    Rather than completely redesigning and retraining a language model from scratch,
    a generic model like GPT could be fine-tuned with a small amount of task-specific
    data in a fraction of the time. Depending on the task, the classification head
    can be changed to contain a different number of output neurons. This is useful
    for classification tasks such as sentiment analysis. For example, if the desired
    output of a BERT model is to predict whether a review is `positive` or `negative`,
    the head can be changed to feature two output neurons. The activation of each
    indicates the probability of the review being `positive` or `negative` respectively.
    For a multi-class classification task with 10 classes, the head can be changed
    to have 10 neurons in the output layer, and so on. This makes BERT more versatile,
    allowing the foundational model to be used for various downstream tasks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 微调已经成为今天的常态，以至于我们容易忽略这种方法崭露头角的时间其实是很短的。2018年之前，通常每个不同的自然语言处理（NLP）任务都会引入一种新的模型架构。转向预训练不仅大幅减少了开发模型所需的训练时间和计算成本，而且还减少了所需的训练数据量。与其完全重新设计和从头开始重新训练一个语言模型，像GPT这样的通用模型只需少量任务特定的数据，就能在短时间内进行微调。根据任务的不同，分类头可以改变，以包含不同数量的输出神经元。这对于情感分析等分类任务非常有用。例如，如果BERT模型的目标输出是预测一个评论是`positive`还是`negative`，则可以将头部更改为包含两个输出神经元。每个神经元的激活表示该评论为`positive`或`negative`的概率。对于一个有10个类别的多类分类任务，头部可以改为有10个神经元的输出层，依此类推。这使得BERT更加多功能，使基础模型能够用于各种下游任务。
- en: '**Fine-tuning in BERT:**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT中的微调：**'
- en: 'BERT followed in the footsteps of GPT and also took this pre-training/fine-tuning
    approach. Google released two versions of BERT: Base and Large, offering users
    flexibility in model size based on hardware constraints. Both variants took around
    4 days to pre-train on many TPUs (tensor processing units), with BERT Base trained
    on 16 TPUs and BERT Large trained on 64 TPUs. For most researchers, hobbyists,
    and industry practitioners, this level of training would not be feasible. Hence,
    the idea of spending only a few hours fine-tuning a foundational model on a particular
    task remains a much more appealing alternative. The original BERT architecture
    has undergone thousands of fine-tuning iterations across various tasks and datasets,
    many of which are publicly accessible for download on platforms like Hugging Face
    [5].'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: BERT沿袭了GPT的做法，也采用了预训练/微调的方法。谷歌发布了两个版本的BERT：Base版和Large版，用户可以根据硬件限制选择不同的模型大小。两个版本都需要在多个TPU（张量处理单元）上预训练约4天，BERT
    Base在16个TPU上进行训练，BERT Large则在64个TPU上进行训练。对于大多数研究人员、爱好者和行业从业者来说，这种训练规模是不可行的。因此，花费仅几个小时在特定任务上对基础模型进行微调，依然是一个更具吸引力的选择。原始的BERT架构已经在多个任务和数据集上经历了数千次微调迭代，其中许多数据集可以通过Hugging
    Face等平台公开下载[5]。
- en: '**1.4 — Use of Bidirectional Context**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.4 — 使用双向上下文**'
- en: 'As a language model, BERT predicts the probability of observing certain words
    given that prior words have been observed. This fundamental aspect is shared by
    all language models, irrespective of their architecture and intended task. However,
    it’s the utilisation of these probabilities that gives the model its task-specific
    behaviour. For example, GPT is trained to predict the next most probable word
    in a sequence. That is, the model predicts the next word, given that the previous
    words have been observed. Other models might be trained on sentiment analysis,
    predicting the sentiment of an input sequence using a textual label such as `positive`
    or `negative`, and so on. Making any meaningful predictions about text requires
    the surrounding context to be understood, especially in NLU tasks. BERT ensures
    good understanding through one of its key properties: **bidirectionality**.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种语言模型，BERT预测在观察到前一个单词的基础上，某个词出现的概率。这一基本特性是所有语言模型所共有的，无论其架构或任务如何。然而，正是这些概率的使用赋予了模型任务特定的行为。例如，GPT被训练用来预测序列中下一个最可能的词。也就是说，给定已经观察到的前一个词，模型预测下一个词。其他模型可能被训练用于情感分析，预测输入序列的情感，使用诸如`positive`或`negative`之类的文本标签，等等。要对文本做出任何有意义的预测，都需要理解其上下文，尤其是在自然语言理解（NLU）任务中。BERT通过其关键特性之一——**双向性**——确保了对上下文的良好理解。
- en: Bidirectionality is perhaps BERT’s most significant feature and is pivotal to
    its high performance in NLU tasks, as well as being the driving reason behind
    the model’s encoder-only architecture. While the self-attention mechanism of Transformer
    encoders calculates bidirectional context, the same cannot be said for decoders
    which produce **unidirectional** context. The BERT authors argued that this lack
    of bidirectionality in GPT prevents it from achieving the same depth of language
    representation as BERT.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 双向性可能是BERT最重要的特性，也是其在自然语言理解任务中表现出色的关键所在，同时也是推动其编码器架构的原因。虽然Transformer编码器的自注意力机制计算双向上下文，但解码器则生成**单向**上下文，不能做到这一点。BERT的作者认为，GPT缺乏双向性使其无法像BERT那样达到相同深度的语言表示。
- en: '**Defining Bidirectionality:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义双向性：**'
- en: But what exactly does “bidirectional” context mean? Here, bidirectional denotes
    that each word in the input sequence can gain context from both preceding and
    succeeding words (called the left context and right context respectively). In
    technical terms, we say that the attention mechanism can **attend** to the preceding
    and subsequent tokens for each word. To break this down, recall that BERT only
    makes predictions about words *within* an input sequence, and does not generate
    new sequences like GPT. Therefore, when BERT predicts a word within the input
    sequence, it can incorporate contextual clues from all the surrounding words.
    This gives context in both directions, helping BERT to make more informed predictions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，究竟“双向”上下文是什么意思呢？在这里，双向性意味着输入序列中的每个词都可以从前后两个方向的词（分别称为左侧上下文和右侧上下文）获取上下文。在技术术语中，我们可以说，注意力机制可以**关注**每个词的前后词汇。为了进一步解释，回顾一下，BERT仅对输入序列中的词做出预测，而不像GPT那样生成新的序列。因此，当BERT预测输入序列中的某个词时，它可以结合周围词语的上下文线索。这为BERT提供了双向的上下文，帮助它做出更加准确的预测。
- en: Contrast this with decoder-only models like GPT, where the objective is to predict
    new words one at a time to generate an output sequence. Each predicted word can
    only leverage the context provided by preceding words (left context) as the subsequent
    words (right context) have not yet been generated. Therefore, these models are
    called **unidirectional**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅使用解码器的模型（如GPT）进行对比，在这些模型中，目标是逐个预测新词，以生成输出序列。每个预测的词只能利用前面词语提供的上下文（左侧上下文），因为后续的词（右侧上下文）尚未生成。因此，这些模型被称为**单向**模型。
- en: '![](../Images/4f9eb3300724e98c4926d726a87ec3f1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f9eb3300724e98c4926d726a87ec3f1.png)'
- en: A comparison of unidirectional and bidirectional context. Image by author.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 单向上下文和双向上下文的比较。图片来源：作者。
- en: '**Image Breakdown:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**图片解析：**'
- en: The image above shows an example of a typical BERT task using bidirectional
    context, and a typical GPT task using unidirectional context. For BERT, the task
    here is to predict the masked word indicated by `[MASK]`. Since this word has
    words to both the left and right, the words from either side can be used to provide
    context. If you, as a human, read this sentence with only the left or right context,
    you would probably struggle to predict the masked word yourself. However, with
    bidirectional context it becomes much more likely to guess that the masked word
    is `fishing`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个典型的 BERT 任务示例，使用双向上下文，而一个典型的 GPT 任务示例则使用单向上下文。对于 BERT，这里的任务是预测被 `[MASK]`
    标记的词。由于该词的两侧都有词语，左右两边的词语都可以用于提供上下文。如果你作为人类，只凭左侧或右侧的上下文来阅读这句话，可能会很难预测被遮掩的词。然而，使用双向上下文时，更有可能猜测出被遮掩的词是
    `fishing`。
- en: For GPT, the goal is to perform the classic NTP task. In this case, the objective
    is to generate a new sequence based on the context provided by the input sequence
    and the words already generated in the output. Given that the input sequence instructs
    the model to write a poem and the words generated so far are `Upon a`, you might
    predict that the next word is `river` followed by `bank`. With many potential
    candidate words, GPT (as a language model) calculates the likelihood of each word
    in its vocabulary appearing next and selects one of the most probable words based
    on its training data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT，目标是执行经典的自然语言推理（NTP）任务。在这种情况下，目标是基于输入序列提供的上下文以及输出中已生成的词语，生成一个新的序列。假设输入序列指示模型写一首诗，且当前已生成的词语是
    `Upon a`，你可能会预测下一个词是 `river`，接着是 `bank`。由于有许多潜在的候选词，GPT（作为一种语言模型）会计算其词汇表中每个词语出现的概率，并根据其训练数据选择最有可能的词语之一。
- en: '**1.5 — Limitations of BERT**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1.5 — BERT 的局限性**'
- en: 'As a bidirectional model, BERT suffers from two major drawbacks:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个双向模型，BERT 存在两个主要缺点：
- en: '**Increased Training Time:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**增加的训练时间：**'
- en: Bidirectionality in Transformer-based models was proposed as a direct improvement
    over the left-to-right context models prevalent at the time. The idea was that
    GPT could only gain contextual information about input sequences in a unidirectional
    manner and therefore lacked a complete grasp of the causal links between words.
    Bidirectional models, however, offer a broader understanding of the causal connections
    between words and so can potentially see better results on NLU tasks. Though bidirectional
    models had been explored in the past, their success was limited, as seen with
    bidirectional RNNs in the late 1990s [6]. Typically, these models demand more
    computational resources for training, so for the same computational power you
    could train a larger unidirectional model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的模型提出双向性作为对当时普遍存在的从左到右上下文模型的直接改进。其思想是 GPT 只能以单向的方式获取输入序列的上下文信息，因此缺乏对词语间因果关系的完整理解。然而，双向模型提供了对词语间因果联系的更广泛理解，因此可能在
    NLU 任务中表现更好。尽管双向模型在过去曾被探索过，但它们的成功是有限的，正如 1990 年代末期的双向 RNN 所示 [6]。通常，这些模型需要更多的计算资源进行训练，因此，在相同的计算能力下，你可以训练一个更大的单向模型。
- en: '**Poor Performance in Language Generation:**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言生成中的差劲表现：**'
- en: BERT was specifically designed to solve NLU tasks, opting to trade decoders
    and the ability to generate new sequences for encoders and the ability to develop
    rich understandings of input sequences. As a result, BERT is best suited to a
    subset of NLP tasks like NER, sentiment analysis and so on. Notably, BERT doesn’t
    accept prompts but rather processes sequences to formulate predictions about.
    While BERT can technically produce new output sequences, it is important to recognise
    the design differences between LLMs as we might think of them in the post-ChatGPT
    era, and the reality of BERT’s design.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 专门设计用于解决自然语言理解（NLU）任务，选择了使用编码器和处理输入序列以深入理解的能力，而放弃了解码器和生成新序列的能力。因此，BERT
    最适合处理一部分 NLP 任务，如命名实体识别（NER）、情感分析等。值得注意的是，BERT 不接受提示，而是处理序列来制定预测。虽然 BERT 在技术上可以生成新的输出序列，但我们需要认识到，在后
    ChatGPT 时代，我们对大语言模型（LLMs）的理解与 BERT 设计之间的差异。
- en: 2 — Architecture and Pre-training Objectives
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 — 架构与预训练目标
- en: '**2.1 — Overview of BERT’s Pre-training Objectives**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.1 — BERT 预训练目标概述**'
- en: 'Training a bidirectional model requires tasks that allow both the left and
    right context to be used in making predictions. Therefore, the authors carefully
    constructed two pre-training objectives to build up BERT’s understanding of language.
    These were: the **Masked Language Model** task (MLM), and the **Next Sentence
    Prediction** task (NSP). The training data for each was constructed from a scrape
    of all the English Wikipedia articles available at the time (2,500 million words),
    and an additional 11,038 books from the BookCorpus dataset (800 million words)
    [7]. The raw data was first preprocessed according to the specific tasks however,
    as described below.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个双向模型需要允许使用左侧和右侧上下文进行预测的任务。因此，作者精心构造了两个预训练目标，以建立BERT对语言的理解。这两个任务是：**掩码语言模型**（MLM）任务和**下一个句子预测**（NSP）任务。每个任务的训练数据都是从当时可用的所有英文维基百科文章（25亿个单词）以及额外的11,038本来自BookCorpus数据集的书籍（8亿个单词）中抓取的[7]。然而，原始数据首先根据具体任务进行预处理，如下所述。
- en: '**2.2 — Masked Language Modelling (MLM)**'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.2 — 掩码语言建模（MLM）**'
- en: '**Overview of MLM:**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLM概述：**'
- en: The Masked Language Modelling task was created to directly address the need
    for training a bidirectional model. To do so, the model must be trained to use
    both the left context and right context of an input sequence to make a prediction.
    This is achieved by randomly **masking** 15% of the words in the training data,
    and training BERT to predict the missing word. In the input sequence, the masked
    word is replaced with the `[MASK]` token. For example, consider that the sentence
    `A man was fishing on the river` exists in the raw training data found in the
    book corpus. When converting the raw text into training data for the MLM task,
    the word `fishing` might be randomly masked and replaced with the `[MASK]` token,
    giving the training input `A man was [MASK] on the river` with target `fishing`.
    Therefore, the goal of BERT is to predict the single missing word `fishing`, and
    not regenerate the input sequence with the missing word filled in. The masking
    process can be repeated for all the possible input sequences (e.g. sentences)
    when building up the training data for the MLM task. This task had existed previously
    in linguistics literature, and is referred to as the **Cloze** task [8]. However,
    in machine learning contexts, it is commonly referred to as MLM due to the popularity
    of BERT.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言建模（Masked Language Modelling, MLM）任务的创建是为了直接解决训练双向模型的需求。为了实现这一点，模型必须被训练成能够利用输入序列的左侧上下文和右侧上下文来进行预测。具体做法是随机**掩盖**训练数据中15%的单词，并训练BERT预测缺失的单词。在输入序列中，被掩盖的单词将被替换为`[MASK]`标记。例如，假设原始训练数据中有句子`A
    man was fishing on the river`，当将原始文本转换为MLM任务的训练数据时，单词`fishing`可能会被随机掩盖并替换为`[MASK]`标记，得到训练输入`A
    man was [MASK] on the river`，目标是`fishing`。因此，BERT的目标是预测缺失的单词`fishing`，而不是用缺失的单词填补输入序列。掩盖过程可以对所有可能的输入序列（例如句子）重复执行，以建立MLM任务的训练数据。这个任务在语言学文献中早已存在，被称为**完形填空（Cloze）**任务[8]。然而，在机器学习领域，由于BERT的流行，它通常被称为MLM。
- en: '**Mitigating Mismatches Between Pre-training and Fine-tuning:**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**缓解预训练与微调之间的不匹配：**'
- en: 'The authors noted however, that since the `[MASK]` token will only ever appear
    in the training data and not in live data (at inference time), there would be
    a mismatch between pre-training and fine-tuning. To mitigate this, not all masked
    words are replaced with the `[MASK]` token. Instead, the authors state that:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，作者指出，由于`[MASK]`标记只会出现在训练数据中，而不会出现在实际数据（推理时），因此会导致预训练与微调之间的不匹配。为了缓解这一问题，并非所有被掩盖的单词都会被替换为`[MASK]`标记。相反，作者表示：
- en: The training data generator chooses 15% of the token positions at random for
    prediction. If the i-th token is chosen, we replace the i-th token with (1) the
    `[MASK]` token 80% of the time (2) a random token 10% of the time (3) the unchanged
    i-th token 10% of the time.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 训练数据生成器随机选择15%的标记位置进行预测。如果选择了第i个标记，我们将第i个标记替换为（1）80%的时间替换为`[MASK]`标记（2）10%的时间替换为一个随机标记（3）10%的时间保留不变。
- en: '**Calculating the Error Between the Predicted Word and the Target Word:**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算预测单词与目标单词之间的误差：**'
- en: BERT will take in an input sequence of a maximum of 512 tokens for both BERT
    Base and BERT Large. If fewer than the maximum number of tokens are found in the
    sequence, then padding will be added using `[PAD]` tokens to reach the maximum
    count of 512\. The number of output tokens will also be exactly equal to the number
    of input tokens. If a masked token exists at position *i* in the input sequence,
    BERT’s prediction will lie at position *i* in the output sequence. All other tokens
    will be ignored for the purposes of training, and so updates to the models weights
    and biases will be calculated based on the error between the predicted token at
    position *i*, and the target token. The error is calculated using a loss function,
    which is typically the Cross Entropy Loss (Negative Log Likelihood) function,
    as we will see later.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 将接受一个最大包含 512 个标记的输入序列，无论是 BERT Base 还是 BERT Large。如果序列中的标记少于最大数量，则会通过使用
    `[PAD]` 标记进行填充，直到达到 512 的最大数量。输出标记的数量也将严格等于输入标记的数量。如果输入序列的第 *i* 个位置存在掩码标记，BERT
    的预测将位于输出序列的第 *i* 个位置。其他所有标记将在训练过程中被忽略，因此模型权重和偏差的更新将基于预测标记和目标标记之间的误差进行计算。误差通过损失函数进行计算，通常使用交叉熵损失（负对数似然）函数，正如我们稍后将看到的那样。
- en: '**2.3 — Next Sentence Prediction (NSP)**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.3 — 下一句预测（NSP）**'
- en: '**Overview:**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**概述：**'
- en: 'The second of BERT’s pre-training tasks is Next Sentence Prediction, in which
    the goal is to classify if one segment (typically a sentence) logically follows
    on from another. The choice of NSP as a pre-training task was made specifically
    to complement MLM and enhance BERT’s NLU capabilities, with the authors stating:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的第二个预训练任务是下一句预测（Next Sentence Prediction, NSP），其目标是判断一个片段（通常是句子）是否在逻辑上紧跟另一个片段。选择
    NSP 作为预训练任务，特意是为了补充 MLM（掩码语言模型）并增强 BERT 的自然语言理解（NLU）能力，作者表示：
- en: Many important downstream tasks such as Question Answering (QA) and Natural
    Language Inference (NLI) are based on understanding the relationship between two
    sentences, which is not directly captured by language modeling.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多重要的下游任务，如问答（QA）和自然语言推理（NLI），都基于理解两个句子之间的关系，而这一点并没有被语言模型直接捕捉到。
- en: 'By pre-training for NSP, BERT is able to develop an understanding of flow between
    sentences in prose text — an ability that is useful for a wide range of NLU problems,
    such as:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为 NSP 进行预训练，BERT 能够发展出对散文文本中句子之间流畅连接的理解——这一能力对于广泛的自然语言理解问题非常有用，例如：
- en: sentence pairs in paraphrasing
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 释义中的句子对
- en: hypothesis-premise pairs in entailment
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑蕴含中的假设-前提对
- en: question-passage pairs in question answering
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题-段落对（Question Passage Pairs）在问答任务中的应用
- en: '**Implementing NSP in BERT:**'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT 中实现 NSP：**'
- en: The input for NSP consists of the first and second segments (denoted A and B)
    separated by a `[SEP]` token with a second `[SEP]` token at the end. BERT actually
    expects at least one `[SEP]` token per input sequence to denote the end of the
    sequence, regardless of whether NSP is being performed or not. For this reason,
    the WordPiece tokenizer will append one of these tokens to the end of inputs for
    the MLM task as well as any other non-NSP task that do not feature one. NSP forms
    a classification problem, where the output corresponds to `IsNext` when segment
    A logically follows segment B, and `NotNext` when it does not. Training data can
    be easily generated from any monolingual corpus by selecting sentences with their
    next sentence 50% of the time, and a random sentence for the remaining 50% of
    sentences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: NSP 的输入由第一个和第二个片段（分别表示为 A 和 B）组成，两个片段之间用 `[SEP]` 标记分隔，末尾还会有第二个 `[SEP]` 标记。实际上，BERT
    期望每个输入序列中至少有一个 `[SEP]` 标记，用以标示序列的结束，无论是否执行 NSP。因此，WordPiece 分词器将在 MLM 任务以及任何不包含
    `[SEP]` 标记的非 NSP 任务中，将其中一个标记附加到输入的末尾。NSP 形成一个分类问题，当片段 A 逻辑上紧跟片段 B 时，输出为 `IsNext`；如果不是，则输出为
    `NotNext`。训练数据可以通过从任何单语语料库中生成：将句子与其后一句配对 50%，其余 50% 的句子则与随机句子配对。
- en: '**2.4 — Input Embeddings in BERT**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.4 — BERT 的输入嵌入**'
- en: 'The input embedding process for BERT is made up of three stages: positional
    encoding, segment embedding, and token embedding (as shown in the diagram below).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 的输入嵌入过程包括三个阶段：位置编码、片段嵌入和标记嵌入（如下图所示）。
- en: '**Positional Encoding:**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码：**'
- en: Just as with the Transformer model, positional information is injected into
    the embedding for each token. Unlike the Transformer however, the positional encodings
    in BERT are fixed and not generated by a function. This means that BERT is restricted
    to 512 tokens in its input sequence for both BERT Base and BERT Large.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Transformer模型一样，位置相关的信息被注入到每个标记的嵌入中。然而，与Transformer不同，BERT中的位置编码是固定的，并不是通过函数生成的。这意味着BERT在输入序列中限制为512个标记，无论是BERT
    Base还是BERT Large。
- en: '**Segment Embedding:**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**片段嵌入：**'
- en: Vectors encoding the segment that each token belongs to are also added. For
    the MLM pre-training task or any other non-NSP task (which feature only one `[SEP]`)
    token, all tokens in the input are considered to belong to segment A. For NSP
    tasks, all tokens after the second `[SEP]` are denoted as segment B.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标记所属片段的向量也会被加入。对于MLM预训练任务或任何其他非NSP任务（这些任务只包含一个`[SEP]`标记），输入中的所有标记都被视为属于片段A。对于NSP任务，第二个`[SEP]`之后的所有标记都被表示为片段B。
- en: '**Token Embedding:**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**标记嵌入：**'
- en: As with the original Transformer, the learned embedding for each token is then
    added to its positional and segment vectors to create the final embedding that
    will be passed to the self-attention mechanisms in BERT to add contextual information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始的Transformer一样，每个标记学习到的嵌入随后会与其位置和片段向量相加，创建最终的嵌入，这将传递给BERT中的自注意力机制，以加入上下文信息。
- en: '![](../Images/adef59c49c388ce9dbe42aa2c153829f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/adef59c49c388ce9dbe42aa2c153829f.png)'
- en: An overview of the BERT embedding process. Image taken from the BERT paper [1].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: BERT嵌入过程的概述。图像来自BERT论文[1]。
- en: '**2.5 — The Special Tokens**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.5 — 特殊标记**'
- en: 'In the image above, you may have noted that the input sequence has been prepended
    with a `[CLS]` (classification) token. This token is added to encapsulate a summary
    of the semantic meaning of the entire input sequence, and helps BERT to perform
    classification tasks. For example, in the sentiment analysis task, the `[CLS]`
    token in the final layer can be analysed to extract a prediction for whether the
    sentiment of the input sequence is `positive` or `negative`. `[CLS]` and `[PAD]`
    etc are examples of BERT’s **special tokens**. It’s important to note here that
    this is a BERT-specific feature, and so you should not expect to see these special
    tokens in models such as GPT. In total, BERT has five special tokens. A summary
    is provided below:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图像中，你可能注意到输入序列前面被加上了一个`[CLS]`（分类）标记。这个标记被添加用来封装整个输入序列的语义总结，帮助BERT执行分类任务。例如，在情感分析任务中，最终层中的`[CLS]`标记可以被分析以提取对输入序列情感是`正面`还是`负面`的预测。`[CLS]`和`[PAD]`等是BERT的**特殊标记**。这里需要注意的是，这是BERT特有的功能，因此你不应该期待在像GPT这样的模型中看到这些特殊标记。总的来说，BERT有五个特殊标记。下面提供了一个总结：
- en: '`[PAD]` (token ID: `0`) — a padding token used to bring the total number of
    tokens in an input sequence up to 512.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]`（标记ID：`0`）——一个填充标记，用于将输入序列中的总标记数填充至512。'
- en: '`[UNK]` (token ID: `100`) — an unknown token, used to represent a token that
    is not in BERT’s vocabulary.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[UNK]`（标记ID：`100`）——一个未知标记，用于表示BERT词汇表中没有的标记。'
- en: '`[CLS]` (token ID: `101`) — a classification token, one is expected at the
    beginning of every sequence, whether it is used or not. This token encapsulates
    the class information for classification tasks, and can be thought of as an aggregate
    sequence representation.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]`（标记ID：`101`）——一个分类标记，预期每个序列的开头都会有一个，无论是否使用。这个标记封装了分类任务的类信息，可以被认为是一个聚合的序列表示。'
- en: '`[SEP]` (token ID: `102`) — a separator token used to distinguish between two
    segments in a single input sequence (for example, in Next Sentence Prediction).
    At least one `[SEP]` token is expected per input sequence, with a maximum of two.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`（标记ID：`102`）——一个分隔符标记，用于区分单个输入序列中的两个片段（例如，在下一个句子预测任务中）。每个输入序列至少应该有一个`[SEP]`标记，最多可以有两个。'
- en: '`[MASK]` (token ID: `103`) — a mask token used to train BERT on the Masked
    Language Modelling task, or to perform inference on a masked sequence.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[MASK]`（标记ID：`103`）——一个掩码标记，用于在掩码语言建模任务中训练BERT，或在掩码序列上执行推理。'
- en: '**2.4 — Architecture Comparison for BERT Base and BERT Large**'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.4 — BERT Base和BERT Large的架构对比**'
- en: BERT Base and BERT Large are very similar from an architecture point-of-view,
    as you might expect. They both use the WordPiece tokenizer (and hence expect the
    same special tokens described earlier), and both have a maximum sequence length
    of 512 tokens. The vocabulary size for BERT is 30,522, with approximately 1,000
    of those tokens left as “unused”. The unused tokens are intentionally left blank
    to allow users to add custom tokens without having to retrain the entire tokenizer.
    This is useful when working with domain-specific vocabulary, such as medical and
    legal terminology. Both BERT Base and BERT Large have a higher number of embedding
    dimensions (*d_model*) compared to the original Transformer. This corresponds
    to the size of the learned vector representations for each token in the model’s
    vocabulary. For BERT Base *d_model* = 768, and for BERT Large *d_model* = 1024
    (double the original Transformer at 512).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，BERT Base 和 BERT Large 非常相似，正如你所期望的那样。它们都使用 WordPiece 分词器（因此也期望与前面描述的相同的特殊符号），并且都具有最大
    512 个词元的序列长度。BERT 的词汇表大小为 30,522，其中大约 1,000 个词元被标记为“未使用”。这些未使用的词元故意留空，以允许用户添加自定义词元，而无需重新训练整个分词器。这在处理特定领域的词汇时非常有用，比如医学和法律术语。BERT
    Base 和 BERT Large 都具有比原始 Transformer 更高的嵌入维度（*d_model*）。这对应于模型词汇中每个词元的学习向量表示的大小。对于
    BERT Base，*d_model* = 768；对于 BERT Large，*d_model* = 1024（是原始 Transformer 512 的两倍）。
- en: 'The two models mainly differ in four categories:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型主要在四个类别上有所不同：
- en: '**Number of encoder blocks,** `**N**`**:** the number of encoder blocks stacked
    on top of each other.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器块数量，** `**N**`**：** 堆叠在一起的编码器块的数量。'
- en: '**Number of attention heads per encoder block:** the attention heads calculate
    the contextual vector embeddings for the input sequence. Since BERT uses multi-head
    attention, this value refers to the number of heads per encoder layer.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个编码器块的注意力头数：** 注意力头计算输入序列的上下文向量嵌入。由于 BERT 使用多头注意力，这个值指的是每个编码器层的头数。'
- en: '**Size of hidden layer in feedforward network:** the linear layer consists
    of a hidden layer with a fixed number of neurons (e.g. 3072 for BERT Base) which
    feed into an output layer that can be of various sizes. The size of the output
    layer depends on the task. For instance, a binary classification problem will
    require just two output neurons, a multi-class classification problem with ten
    classes will require ten neurons, and so on.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈网络中隐藏层的大小：** 线性层由一个隐藏层组成，该隐藏层具有固定数量的神经元（例如 BERT Base 为 3072），然后传递到输出层，输出层的大小可以有所不同。输出层的大小取决于任务。例如，二分类问题只需要两个输出神经元，而一个有十个类别的多分类问题则需要十个神经元，以此类推。'
- en: '**Total parameters:** the total number of weights and biases in the model.
    At the time, a model with hundreds of millions was very large. However, by today’s
    standards, these values are comparatively small.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总参数量：** 模型中所有权重和偏置的总数。在当时，具有数亿参数的模型被认为非常大。然而，以今天的标准来看，这些值相对较小。'
- en: A comparison between BERT Base and BERT Large for each of these categories is
    shown in the image below.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 BERT Base 和 BERT Large 在这些类别中的对比。
- en: '![](../Images/8af52573f39fd836601eef13baf77957.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8af52573f39fd836601eef13baf77957.png)'
- en: A comparison between BERT Base and BERT Large. Image by author.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Base 和 BERT Large 的对比。图片来自作者。
- en: 3 — Fine-Tuning BERT for Sentiment Analysis
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 — 微调 BERT 进行情感分析
- en: This section covers a practical example of fine-tuning BERT in Python. The code
    takes the form of a task-agnostic fine-tuning pipeline, implemented in a Python
    class. We will then instantiate an object of this class and use it to fine-tune
    a BERT model on the sentiment analysis task. The class can be reused to fine-tune
    BERT on other tasks, such as Question Answering, Named Entity Recognition, and
    more. **Sections 3.1 to 3.5 walk through the fine-tuning process, and Section
    3.6 shows the full pipeline in its entirety.**
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了在 Python 中微调 BERT 的实际示例。代码采用任务无关的微调管道形式，使用 Python 类实现。接着，我们将实例化该类的对象，并用它来微调
    BERT 模型以进行情感分析任务。该类还可以重复使用，用于微调 BERT 完成其他任务，如问答、命名实体识别等。**第 3.1 节到 3.5 节逐步讲解了微调过程，第
    3.6 节展示了完整的管道。**
- en: '**3.1 — Load and Preprocess a Fine-Tuning Dataset**'
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.1 — 加载并预处理微调数据集**'
- en: The first step in fine-tuning is to select a dataset that is suitable for the
    specific task. In this example, we will use a sentiment analysis dataset provided
    by Stanford University. This dataset contains 50,000 online movie reviews from
    the Internet Movie Database (IMDb), with each review labelled as either `positive`
    or `negative`. You can download the dataset directly from the [Stanford University
    website](https://ai.stanford.edu/~amaas/data/sentiment/), or you can create a
    notebook on [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    and compare your work with others.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的第一步是选择一个适合特定任务的数据集。在这个示例中，我们将使用斯坦福大学提供的情感分析数据集。该数据集包含来自互联网电影数据库（IMDb）的50,000条在线电影评论，每条评论标注为`正面`或`负面`。您可以直接从[斯坦福大学网站](https://ai.stanford.edu/~amaas/data/sentiment/)下载该数据集，或者您可以在[Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)上创建一个笔记本，并将您的工作与他人进行比较。
- en: '[PRE0]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/dbc783b927e31a86bcb1bf6cc62aeb94.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbc783b927e31a86bcb1bf6cc62aeb94.png)'
- en: The first five rows of the IMDb dataset as shown in a Pandas DataFrame. Image
    by author.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 显示在Pandas DataFrame中的IMDb数据集的前五行。图片来自作者。
- en: Unlike earlier NLP models, Transformer-based models such as BERT require minimal
    preprocessing. Steps such as removing stop words and punctuation can prove counterproductive
    in some cases, since these elements provide BERT with valuable context for understanding
    the input sentences. Nevertheless, it is still important to inspect the text to
    check for any formatting issues or unwanted characters. Overall, the IMDb dataset
    is fairly clean. However, there appear to be some artefacts of the scraping process
    leftover, such as HTML break tags (`<br />`) and unnecessary whitespace, which
    should be removed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的NLP模型不同，基于Transformer的模型（如BERT）需要的预处理工作最小化。诸如去除停用词和标点符号等步骤，在某些情况下可能适得其反，因为这些元素为BERT提供了理解输入句子的宝贵上下文。尽管如此，仍然需要检查文本以查找任何格式问题或不必要的字符。总体而言，IMDb数据集相当干净。然而，似乎有一些抓取过程的遗留物，例如HTML换行标签（`<br
    />`）和不必要的空白，应该去除。
- en: '[PRE1]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Encode the Sentiment:**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码情感：**'
- en: The final step of the preprocessing is to encode the sentiment of each review
    as either `0` for `negative` or `1` for positive. These labels will be used to
    train the classification head later in the fine-tuning process.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的最后一步是将每个评论的情感编码为`0`表示`负面`或`1`表示`正面`。这些标签将用于稍后在微调过程中训练分类头。
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/f69c08f131a1534e3b654a28f0a7d813.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f69c08f131a1534e3b654a28f0a7d813.png)'
- en: The first five rows of the IMDb dataset after the sentiment column has been
    encoded. Image by author.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 编码情感列后的IMDb数据集前五行。图片来自作者。
- en: '**3.2 — Tokenize the Fine-Tuning Data**'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.2 — 分词微调数据**'
- en: 'Once preprocessed, the fine-tuning data can undergo tokenization. This process:
    splits the review text into individual tokens, adds the `[CLS]` and `[SEP]` special
    tokens, and handles padding. It’s important to select the appropriate tokenizer
    for the model, as different language models require different tokenization steps
    (e.g. GPT does not expect `[CLS]` and `[SEP]` tokens). We will use the `BertTokenizer`
    class from the Hugging Face `transformers` library, which is designed to be used
    with BERT-based models. For a more in-depth discussion of how tokenization works,
    see [Part 1 of this series](https://medium.com/p/cedc9f72de4e).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预处理完成，微调数据就可以进行分词处理。这个过程：将评论文本拆分成单独的标记，添加`[CLS]`和`[SEP]`特殊标记，并处理填充。选择合适的分词器非常重要，因为不同的语言模型需要不同的分词步骤（例如，GPT不需要`[CLS]`和`[SEP]`标记）。我们将使用来自Hugging
    Face `transformers`库的`BertTokenizer`类，它是专为BERT模型设计的。有关分词如何工作的更深入讨论，请参见[本系列的第1部分](https://medium.com/p/cedc9f72de4e)。
- en: 'Tokenizer classes in the `transformers` library provide a simple way to create
    pre-trained tokenizer models with the `from_pretrained` method. To use this feature:
    import and instantiate a tokenizer class, call the `from_pretrained` method, and
    pass in a string with the name of a tokenizer model hosted on the Hugging Face
    model repository. Alternatively, you can pass in the path to a directory containing
    the vocabulary files required by the tokenizer [9]. For our example, we will use
    a pre-trained tokenizer from the model repository. There are four main options
    when working with BERT, each of which use the vocabulary from Google’s pre-trained
    tokenizers. These are:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`库中的分词器类提供了一种简单的方法，可以使用`from_pretrained`方法创建预训练的分词器模型。使用此功能的方法是：导入并实例化一个分词器类，调用`from_pretrained`方法，并传入一个包含在Hugging
    Face模型库中托管的分词器模型名称的字符串。或者，你可以传入一个包含分词器所需词汇文件的目录的路径[9]。在我们的示例中，我们将使用来自模型库的预训练分词器。在使用BERT时，有四个主要选项，每个选项都使用来自Google预训练分词器的词汇表。这些选项是：'
- en: '`bert-base-uncased` — the vocabulary for the smaller version of BERT, which
    is NOT case sensitive (e.g. the tokens `Cat` and `cat` will be treated the same)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-uncased` — BERT的较小版本的词汇表，它对大小写不敏感（例如，`Cat`和`cat`会被视为相同的标记）'
- en: '`bert-base-cased` — the vocabulary for the smaller version of BERT, which IS
    case sensitive (e.g. the tokens `Cat` and `cat` will not be treated the same)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-base-cased` — BERT的较小版本的词汇表，它对大小写敏感（例如，`Cat`和`cat`不会被视为相同的标记）'
- en: '`bert-large-uncased` — the vocabulary for the larger version of BERT, which
    is NOT case sensitive (e.g. the tokens `Cat` and `cat` will be treated the same)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-large-uncased` — BERT的较大版本的词汇表，它对大小写不敏感（例如，`Cat`和`cat`会被视为相同的标记）'
- en: '`bert-large-cased` — the vocabulary for the larger version of BERT, which IS
    case sensitive (e.g. the tokens `Cat` and `cat` will not be treated the same)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bert-large-cased` — BERT的较大版本的词汇表，它对大小写敏感（例如，`Cat`和`cat`不会被视为相同的标记）'
- en: Both BERT Base and BERT Large use the same vocabulary, and so there is actually
    no difference between `bert-base-uncased` and `bert-large-uncased`, nor is there
    a difference between `bert-base-cased` and `bert-large-cased`. This may not be
    the same for other models, so it is best to use the same tokenizer and model size
    if you are unsure.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Base和BERT Large使用相同的词汇表，因此`bert-base-uncased`和`bert-large-uncased`之间实际上没有区别，`bert-base-cased`和`bert-large-cased`之间也没有区别。对于其他模型来说，情况可能不同，因此如果不确定，最好使用相同的分词器和模型大小。
- en: '**When to Use** `**cased**` **vs** `**uncased**`**:**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**何时使用** `**cased**` **与** `**uncased**`**:**'
- en: The decision between using `cased` and `uncased` depends on the nature of your
    dataset. The IMDb dataset contains text written by internet users who may be inconsistent
    with their use of capitalisation. For example, some users may omit capitalisation
    where it is expected, or use capitalisation for dramatic effect (to show excitement,
    frustration, etc). For this reason, we will choose to ignore case and use the
    `bert-base-uncased` tokenizer model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`cased`和`uncased`之间的选择取决于数据集的性质。IMDb数据集包含由互联网用户编写的文本，这些用户在使用大写字母时可能不一致。例如，一些用户可能会忽略应有的大写字母，或者为了强调情感（如表达兴奋、沮丧等）使用大写字母。因此，我们将选择忽略大小写，并使用`bert-base-uncased`分词器模型。
- en: Other situations may see a performance benefit by accounting for case. An example
    here may be in a Named Entity Recognition task, where the goal is to identify
    entities such as people, organisations, locations, etc in some input text. In
    this case, the presence of upper case letters can be extremely helpful in identifying
    if a word is someone’s name or a place, and so in this situation it may be more
    appropriate to choose `bert-base-cased`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，通过考虑大小写可能会获得性能上的优势。一个例子可能是在命名实体识别任务中，目标是识别输入文本中的实体，如人名、组织名、地点等。在这种情况下，大写字母的存在可以非常有助于识别一个单词是否为某个人的名字或一个地方的名称，因此在这种情况下，选择`bert-base-cased`可能更为合适。
- en: '[PRE4]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Encoding Process: Converting Text to Tokens to Token IDs**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码过程：将文本转换为标记再转换为标记ID**'
- en: 'Next, the tokenizer can be used to encode the cleaned fine-tuning data. This
    process will convert each review into a tensor of token IDs. For example, the
    review `I liked this movie` will be encoded by the following steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，可以使用分词器对清理后的微调数据进行编码。这个过程会将每条评论转换为一个标记ID的张量。例如，评论`I liked this movie`将通过以下步骤进行编码：
- en: 1\. Convert the review to lower case (since we are using `bert-base-uncased`)
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 将评论转换为小写（因为我们使用的是`bert-base-uncased`）
- en: '2\. Break the review down into individual tokens according to the `bert-base-uncased`
    vocabulary: `[''i'', ''liked'', ''this'', ''movie'']`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 根据`bert-base-uncased`词汇表将评论拆分为单独的token：`['i', 'liked', 'this', 'movie']`
- en: '2\. Add the special tokens expected by BERT: `[''[CLS]'', ''i'', ''liked'',
    ''this'', ''movie'', ''[SEP]'']`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 添加BERT期望的特殊token：`['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']`
- en: 3\. Convert the tokens to their token IDs, also according to the `bert-base-uncased`
    vocabulary (e.g. `[CLS]` -> `101`, `i` -> `1045`, etc)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 将token转换为它们的token ID，同样根据`bert-base-uncased`词汇表（例如`[CLS]` -> `101`，`i` ->
    `1045`，等等）
- en: 'The `encode` method of the `BertTokenizer` class encodes text using the above
    process, and can return the tensor of token IDs as PyTorch tensors, Tensorflow
    tensors, or NumPy arrays. The data type for the return tensor can be specified
    using the `return_tensors` argument, which takes the values: `pt`, `tf`, and `np`
    respectively.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`BertTokenizer`类的`encode`方法使用上述过程对文本进行编码，并可以返回PyTorch张量、TensorFlow张量或NumPy数组格式的token
    ID张量。返回张量的数据类型可以通过`return_tensors`参数来指定，取值为：`pt`、`tf`和`np`，分别对应PyTorch、TensorFlow和NumPy。'
- en: '**Note:** Token IDs are often called `input IDs` in Hugging Face, so you may
    see these terms used interchangeably.'
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 在Hugging Face中，token ID通常被称为`input IDs`，因此你可能会看到这两个术语交替使用。'
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Truncation and Padding:**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**截断与填充：**'
- en: Both BERT Base and BERT Large are designed to handle input sequences of exactly
    512 tokens. But what do you do when your input sequence doesn’t fit this limit?
    The answer is truncation and padding! Truncation reduces the number of tokens
    by simply removing any tokens beyond a certain length. In the `encode` method,
    you can set `truncation` to `True` and specify a `max_length` argument to enforce
    a length limit on all encoded sequences. Several of the entries in this dataset
    exceed the 512 token limit, and so the `max_length` parameter here has been set
    to 512 to extract the most amount of text possible from all reviews. If no review
    exceeds 512 tokens, the `max_length` parameter can be left unset and it will default
    to the model’s maximum length. Alternatively, you can still enforce a maximum
    length which is less than 512 to reduce training time during fine-tuning, albeit
    at the expense of model performance. For reviews shorter than 512 tokens (which
    is the majority here), padding tokens are added to extend the encoded review to
    512 tokens. This can be achieved by setting the padding `parameter` to `max_length`.
    Refer to the Hugging Face documentation for more details on the encode method
    [10].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Base和BERT Large都设计为处理恰好512个token的输入序列。但如果输入序列不符合这个限制，怎么办呢？答案是截断和填充！截断通过简单地删除超过特定长度的token来减少token的数量。在`encode`方法中，你可以将`truncation`设置为`True`，并指定一个`max_length`参数来强制对所有编码序列施加长度限制。数据集中有几条评论超过了512个token的限制，因此这里的`max_length`参数被设置为512，以从所有评论中提取尽可能多的文本。如果没有评论超过512个token，可以不设置`max_length`，它将默认使用模型的最大长度。或者，你也可以强制设置一个小于512的最大长度，从而在微调过程中减少训练时间，尽管这样会影响模型性能。对于长度小于512个token的评论（大多数评论是这样），会添加填充token，以将编码后的评论扩展到512个token。可以通过将填充`parameter`设置为`max_length`来实现。更多关于`encode`方法的细节，请参阅Hugging
    Face文档[10]。
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Using the Attention Mask with** `**encode_plus**`**:**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用注意力掩码与** `**encode_plus**`**：**'
- en: 'The example above shows the encoding for the first review in the dataset, which
    contains 119 padding tokens. If used in its current state for fine-tuning, BERT
    could attend to the padding tokens, potentially leading to a drop in performance.
    To address this, we can apply an attention mask that will instruct BERT to ignore
    certain tokens in the input (in this case the padding tokens). We can generate
    this attention mask by modifying the code above to use the `encode_plus` method,
    rather than the standard `encode` method. The `encode_plus` method returns a dictionary
    (called a Batch Encoder in Hugging Face), which contains the keys:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的示例展示了数据集中第一个评论的编码，其中包含119个填充token。如果直接用于微调，BERT可能会关注到这些填充token，从而可能导致性能下降。为了解决这个问题，我们可以应用一个注意力掩码，指示BERT忽略输入中的某些token（在这个例子中是填充token）。我们可以通过修改上面的代码，使用`encode_plus`方法，而不是标准的`encode`方法来生成这个注意力掩码。`encode_plus`方法返回一个字典（在Hugging
    Face中称为Batch Encoder），其中包含以下键：
- en: '`input_ids` — the same token IDs returned by the standard `encode` method'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_ids` — 与标准`encode`方法返回的token ID相同'
- en: '`token_type_ids` — the segment IDs used to distinguish between sentence A (id
    = 0) and sentence B (id = 1) in sentence pair tasks such as Next Sentence Prediction'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token_type_ids` — 用于区分句子A（id = 0）和句子B（id = 1）的段落ID，常用于下一个句子预测等句子对任务'
- en: '`attention_mask` — a list of 0s and 1s where 0 indicates that a token should
    be ignored during the attention process and 1 indicates a token should not be
    ignored'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`attention_mask` — 一个由0和1组成的列表，其中0表示在注意力过程中应该忽略该标记，而1表示该标记不应被忽略'
- en: '[PRE10]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Encode All Reviews:**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码所有评论：**'
- en: The last step for the tokenization stage is to encode all the reviews in the
    dataset and store the token IDs and corresponding attention masks as tensors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化阶段的最后一步是编码数据集中的所有评论，并将标记ID和对应的注意力掩码存储为张量。
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**3.3 — Create the Train and Validation DataLoaders**'
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.3 — 创建训练和验证DataLoader**'
- en: Now that each review has been encoded, we can split our data into a training
    set and a validation set. The validation set will be used to evaluate the effectiveness
    of the fine-tuning process as it happens, allowing us to monitor the performance
    throughout the process. We expect to see a decrease in loss (and consequently
    an increase in model accuracy) as the model undergoes further fine-tuning across
    **epochs**. An epoch refers to one full pass of the train data. The BERT authors
    recommend 2–4 epochs for fine-tuning [1], meaning that the classification head
    will see every review 2–4 times.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每条评论都已编码完成，我们可以将数据分割为训练集和验证集。验证集将用于在微调过程中评估其效果，使我们能够在整个过程中监控模型的性能。我们期望随着模型在**epochs**中进一步微调，损失下降（因此模型准确性提高）。一个epoch指的是对训练数据的完整遍历。BERT的作者推荐进行2到4个epochs的微调[1]，意味着分类头将看到每条评论2到4次。
- en: To partition the data, we can use the `train_test_split` function from SciKit-Learn’s
    `model_selection` package. This function requires the dataset we intend to split,
    the percentage of items to be allocated to the test set (or validation set in
    our case), and an optional argument for whether the data should be randomly shuffled.
    For reproducibility, we will set the shuffle parameter to `False`. For the `test_size`,
    we will choose a small value of 0.1 (equivalent to 10%). It is important to strike
    a balance between using enough data to validate the model and get an accurate
    picture of how it is performing, and retaining enough data for training the model
    and improving its performance. Therefore, smaller values such as `0.1` are often
    preferred. After the token IDs, attention masks, and labels have been split, we
    can group the training and validation tensors together in PyTorch TensorDatasets.
    We can then create a PyTorch DataLoader class for training and validation by dividing
    these TensorDatasets into batches. The BERT paper recommends batch sizes of 16
    or 32 (that is, presenting the model with 16 reviews and corresponding sentiment
    labels before recalculating the weights and biases in the classification head).
    Using DataLoaders will allow us to efficiently load the data into the model during
    the fine-tuning process by exploiting multiple CPU cores for parallelisation [11].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据进行划分，我们可以使用SciKit-Learn的`model_selection`包中的`train_test_split`函数。此函数需要我们打算划分的数据集、分配给测试集（或在我们的案例中是验证集）的项目百分比，以及一个可选参数，指示数据是否应随机打乱。为了确保可重复性，我们将`shuffle`参数设置为`False`。对于`test_size`，我们选择一个较小的值0.1（相当于10%）。在使用足够的数据来验证模型并准确评估其性能与保留足够数据来训练模型并提高其性能之间，需要找到平衡。因此，通常更倾向于选择较小的值，如`0.1`。在分割完标记ID、注意力掩码和标签后，我们可以将训练和验证的张量组合在一起，使用PyTorch的TensorDatasets。然后，我们可以通过将这些TensorDatasets分批来创建用于训练和验证的PyTorch
    DataLoader类。BERT论文推荐使用16或32的批量大小（即在重新计算分类头中的权重和偏置之前，模型需要先看到16条评论及其对应的情感标签）。使用DataLoader将使我们能够在微调过程中有效地加载数据，通过利用多个CPU核心进行并行化[11]。
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**3.4 — Instantiate a BERT Model**'
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.4 — 实例化一个BERT模型**'
- en: 'The next step is to load in a pre-trained BERT model for us to fine-tune. We
    can import a model from the Hugging Face model repository similarly to how we
    did with the tokenizer. Hugging Face has many versions of BERT with classification
    heads already attached, which makes this process very convenient. Some examples
    of models with pre-configured classification heads include:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是加载一个预训练的BERT模型以供我们进行微调。我们可以像导入标记器一样，从Hugging Face模型库中导入一个模型。Hugging Face有许多带有预配置分类头的BERT版本，这使得这个过程非常便捷。带有预配置分类头的模型示例包括：
- en: '`BertForMaskedLM`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForMaskedLM`'
- en: '`BertForNextSentencePrediction`'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForNextSentencePrediction`'
- en: '`BertForSequenceClassification`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForSequenceClassification`'
- en: '`BertForMultipleChoice`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForMultipleChoice`'
- en: '`BertForTokenClassification`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForTokenClassification`'
- en: '`BertForQuestionAnswering`'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BertForQuestionAnswering`'
- en: Of course, it is possible to import a headless BERT model and create your own
    classification head from scratch in PyTorch or Tensorflow. However in our case,
    we can simply import the `BertForSequenceClassification` model since this already
    contains the linear layer we need. This linear layer is initialised with random
    weights and biases, which will be trained during the fine-tuning process. Since
    BERT Base uses 768 embedding dimensions, the hidden layer contains 768 neurons
    which are connected to the final encoder block of the model. The number of output
    neurons is determined by the `num_labels` argument, and corresponds to the number
    of unique sentiment labels. The IMDb dataset features only `positive` and `negative`,
    and so the `num_labels` argument is set to `2`. For more complex sentiment analyses,
    perhaps including labels such as `neutral` or `mixed`, we can simply increase/decrease
    the `num_labels` value.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以在PyTorch或TensorFlow中导入一个无头BERT模型，并从头开始创建自己的分类头。然而，在我们的案例中，我们可以简单地导入`BertForSequenceClassification`模型，因为它已经包含了我们需要的线性层。这个线性层是使用随机权重和偏置初始化的，这些将在微调过程中进行训练。由于BERT
    Base使用768个嵌入维度，隐藏层包含768个神经元，这些神经元与模型的最终编码器块相连。输出神经元的数量由`num_labels`参数决定，并对应于唯一情感标签的数量。IMDb数据集只有`positive`和`negative`两类情感标签，因此`num_labels`参数被设置为`2`。对于更复杂的情感分析，可能包括如`neutral`或`mixed`等标签，我们可以简单地增大或减小`num_labels`的值。
- en: '**Note:** If you are interested in seeing how the pre-configured models are
    written in the source code, the `modelling_bert.py` file on the Hugging Face transformers
    repository shows the process of loading in a headless BERT model and adding the
    linear layer [12]. The linear layer is added in the `__init__` method of each
    class.'
  id: totrans-186
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你有兴趣查看预配置的模型如何在源代码中编写，Hugging Face的transformers库中的`modelling_bert.py`文件展示了如何加载一个无头BERT模型并添加线性层[12]。线性层是在每个类的`__init__`方法中添加的。'
- en: '[PRE14]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**3.5 — Instantiate an Optimizer, Loss Function, and Scheduler**'
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.5 — 实例化优化器、损失函数和调度器**'
- en: '**Optimizer:**'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器：**'
- en: After the classification head encounters a batch of training data, it updates
    the weights and biases in the linear layer to improve the model’s performance
    on those inputs. Across many batches and multiple epochs, the aim is for these
    weights and biases to converge towards optimal values. An **optimizer** is required
    to calculate the changes needed to each weight and bias, and can be imported from
    PyTorch’s `optim` package. Hugging Face use the AdamW optimizer in their examples,
    and so this is the optimizer we will use here [13].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类头处理完一批训练数据后，它会更新线性层中的权重和偏置，以提高模型在这些输入上的表现。在多个批次和多个训练轮次中，目标是让这些权重和偏置收敛到最优值。**优化器**用于计算每个权重和偏置所需的变化，可以从PyTorch的`optim`包中导入。Hugging
    Face在其示例中使用的是AdamW优化器，因此我们在这里也将使用该优化器[13]。
- en: '**Loss Function:**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数：**'
- en: The optimizer works by determining how changes to the weights and biases in
    the classification head will affect the loss against a scoring function called
    the **loss function**. Loss functions can be easily imported from PyTorch’s `nn`
    package, as shown below. Language models typically use the cross entropy loss
    function (also called the negative log likelihood function), and so this is the
    loss function we will use here.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器通过确定分类头中权重和偏置的变化如何影响与一个称为**损失函数**的评分函数的损失来工作。损失函数可以很容易地从PyTorch的`nn`包中导入，如下所示。语言模型通常使用交叉熵损失函数（也称为负对数似然函数），因此这也是我们在此使用的损失函数。
- en: '**Scheduler:**'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**调度器：**'
- en: A parameter called the **learning rate** is used to determine the size of the
    changes made to the weights and biases in the classification head. In early batches
    and epochs, large changes may prove advantageous since the randomly-initialised
    parameters will likely need substantial adjustments. However, as the training
    progresses, the weights and biases tend to improve, potentially making large changes
    counterproductive. Schedulers are designed to gradually decrease the learning
    rate as the training process continues, reducing the size of the changes made
    to each weight and bias in each optimizer step.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一个叫做**学习率**的参数用于确定分类头中权重和偏置变化的大小。在早期的批次和训练轮次中，较大的变化可能会证明是有利的，因为随机初始化的参数可能需要大量的调整。然而，随着训练的进行，权重和偏置趋于改善，过大的变化可能会适得其反。调度器的设计是随着训练的进行，逐渐减少学习率，从而减少每次优化步骤中对每个权重和偏置的变化大小。
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**3.6 — Fine-Tuning Loop**'
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.6 — 微调循环**'
- en: '**Utilise GPUs with CUDA:**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用带CUDA的GPU：**'
- en: Compute Unified Device Architecture (CUDA) is a computing platform created by
    NVIDIA to improve the performance of applications in various fields, such as scientific
    computing and engineering [14]. PyTorch’s `cuda` package allows developers to
    leverage the CUDA platform in Python and utilise their Graphical Processing Units
    (GPUs) for accelerated computing when training machine learning models. The `torch.cuda.is_available`
    command can be used to check if a GPU is available. If not, the code can default
    back to using the Central Processing Unit (CPU), with the caveat that this will
    take longer to train. In subsequent code snippets, we will use the PyTorch `Tensor.to`
    method to move tensors (containing the model weights and biases etc) to the GPU
    for faster calculations. If the device is set to `cpu` then the tensors will not
    be moved and the code will be unaffected.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 计算统一设备架构（CUDA）是由NVIDIA创建的计算平台，旨在提升各个领域应用的性能，如科学计算和工程应用[14]。PyTorch的`cuda`包允许开发者在Python中利用CUDA平台，并使用图形处理单元（GPU）进行加速计算，从而在训练机器学习模型时提高计算效率。可以使用`torch.cuda.is_available`命令来检查GPU是否可用。如果不可用，代码会默认使用中央处理单元（CPU），但需要注意，这将导致训练时间变长。在随后的代码片段中，我们将使用PyTorch的`Tensor.to`方法将张量（包含模型权重和偏置等）转移到GPU上，以加快计算速度。如果设备设置为`cpu`，则张量将不会被移动，代码也不会受到影响。
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The training process will take place over two for loops: an outer loop to repeat
    the process for each epoch (so that the model sees all the training data multiple
    times), and an inner loop to repeat the loss calculation and optimization step
    for each batch. To explain the training loop, consider the process in the steps
    below. The code for the training loop has been adapted from this fantastic blog
    post by Chris McCormick and Nick Ryan [15], which I highly recommend.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程将通过两个for循环进行：一个外循环，用于为每个epoch重复训练过程（这样模型可以多次看到所有训练数据）；一个内循环，用于对每个批次重复进行损失计算和优化步骤。为了解释训练循环，可以参考下面的步骤。训练循环的代码改编自Chris
    McCormick和Nick Ryan的这篇精彩博客文章[15]，我强烈推荐大家阅读。
- en: '**For each epoch:**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每个epoch：**'
- en: 1\. Switch the model to be in train mode using the `train` method on the model
    object. This will cause the model to behave differently than when in evaluation
    mode, and is especially useful when working with batchnorm and dropout layers.
    If you looked at the source code for the `BertForSequenceClassification`class
    earlier, you may have seen that the classification head does in fact contain a
    dropout layer, and so it is important we correctly distinguish between train and
    evaluation mode in our fine-tuning. These kinds of layers should only be active
    during training and not inference, and so the ability to switch between different
    modes for training and inference is a useful feature.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 使用模型对象的`train`方法将模型切换到训练模式。这将导致模型在训练模式下的行为与评估模式时有所不同，特别是在处理批归一化（batchnorm）和丢弃层（dropout
    layers）时非常有用。如果你之前查看过`BertForSequenceClassification`类的源代码，可能已经注意到分类头确实包含一个丢弃层，因此，在微调过程中正确区分训练模式和评估模式是非常重要的。这些层只应在训练时启用，而不应在推理时启用，因此能够在训练和推理模式之间切换是一个非常有用的功能。
- en: 2\. Set the training loss to 0 for the start of the epoch. This is used to track
    the loss of the model on the training data over subsequent epochs. The loss should
    decrease with each epoch if training is successful.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在epoch开始时将训练损失设为0。这个损失值用于跟踪模型在后续epoch中对训练数据的表现。如果训练成功，损失应该会随着每个epoch的进行而减少。
- en: '**For each batch:**'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**对于每个批次：**'
- en: As per the BERT authors’ recommendations, the training data for each epoch is
    split into batches. Loop through the training process for each batch.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 根据BERT作者的建议，每个epoch的训练数据会被分成若干个批次。对于每个批次，重复训练过程。
- en: 3\. Move the token IDs, attention masks, and labels to the GPU if available
    for faster processing, otherwise these will be kept on the CPU.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 如果GPU可用，将token ID、注意力掩码和标签移动到GPU上以加速处理，否则这些数据将保留在CPU上。
- en: 4\. Invoke the `zero_grad` method to reset the calculated gradients from the
    previous iteration of this loop. It might not be obvious why this is not the default
    behaviour in PyTorch, but some suggested reasons for this describe models such
    as Recurrent Neural Networks which require the gradients to not be reset between
    iterations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 调用`zero_grad`方法来重置上一轮迭代计算出的梯度。虽然在PyTorch中这不是默认行为，但一些原因解释了为什么像循环神经网络（RNN）这样的模型需要在迭代之间保留梯度，不能重置。
- en: 5\. Pass the batch to the model to calculate the logits (predictions based on
    the current classifier weights and biases) as well as the loss.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 将批次数据传递给模型以计算logits（基于当前分类器权重和偏置的预测值）以及损失值。
- en: 6\. Increment the total loss for the epoch. The loss is returned from the model
    as a PyTorch tensor, so extract the float value using the `item` method.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 增加该轮次的总损失值。损失值由模型以PyTorch张量形式返回，因此使用`item`方法提取浮动值。
- en: 7\. Perform a backward pass of the model and propagate the loss through the
    classifier head. This will allow the model to determine what adjustments to make
    to the weights and biases to improve its performance on the batch.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 执行模型的反向传播，并通过分类器头传播损失。这将使模型确定需要对权重和偏置进行哪些调整，以提高其在当前批次上的表现。
- en: 8\. Clip the gradients to be no larger than 1.0 so the model does not suffer
    from the exploding gradients problem.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. 将梯度裁剪到最大值为1.0，以避免模型遭遇梯度爆炸问题。
- en: 9\. Call the optimizer to take a step in the direction of the error surface
    as determined by the backward pass.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. 调用优化器，根据反向传播计算的误差面朝向进行一步优化。
- en: '**After training on each batch:**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**每个批次训练后：**'
- en: 10\. Calculate the average loss and time taken for training on the epoch.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. 计算训练过程中每轮的平均损失值和训练时间。
- en: '[PRE17]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The validation step takes place within the outer loop, so that the average validation
    loss is calculated for each epoch. As the number of epochs increases, we would
    expect to see the validation loss decrease and the classifier accuracy increase.
    The steps for the validation process are outlined below.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 验证步骤发生在外层循环中，因此会为每个epoch计算平均验证损失。随着epoch数的增加，我们预计验证损失会减少，分类器准确率会提高。验证过程的步骤如下所示。
- en: '**Validation step for the epoch:**'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**本轮验证步骤：**'
- en: 11\. Switch the model to evaluation mode using the `eval` method — this will
    deactivate the dropout layer.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 11\. 使用`eval`方法将模型切换到评估模式——这将禁用dropout层。
- en: 12\. Set the validation loss to 0\. This is used to track the loss of the model
    on the validation data over subsequent epochs. The loss should decrease with each
    epoch if training was successful.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 12\. 将验证损失初始化为0。这用于追踪模型在后续轮次中在验证数据上的损失。若训练成功，损失应随着每轮的进行而减小。
- en: 13\. Split the validation data into batches.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 13\. 将验证数据拆分成若干批次。
- en: '**For each batch:**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**每个批次：**'
- en: 14\. Move the token IDs, attention masks, and labels to the GPU if available
    for faster processing, otherwise these will be kept on the CPU.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 14\. 如果可用，将token IDs、注意力掩码和标签移动到GPU上以加速处理，否则这些数据将保留在CPU上。
- en: 15\. Invoke the `no_grad` method to instruct the model not to calculate the
    gradients since we will not be performing any optimization steps here, only inference.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 15\. 调用`no_grad`方法，指示模型不要计算梯度，因为在这里我们不会进行任何优化步骤，只进行推理。
- en: 16\. Pass the batch to the model to calculate the logits (predictions based
    on the current classifier weights and biases) as well as the loss.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 16\. 将批次数据传递给模型以计算logits（基于当前分类器权重和偏置的预测值）以及损失值。
- en: 17\. Extract the logits and labels from the model and move them to the CPU (if
    they are not already there).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 17\. 从模型中提取logits和标签，并将它们移动到CPU（如果它们尚未在CPU上）。
- en: 18\. Increment the loss and calculate the accuracy based on the true labels
    in the validation dataloader.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 18\. 增加损失值并根据验证数据加载器中的真实标签计算准确率。
- en: 19\. Calculate the average loss and accuracy.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 19\. 计算平均损失和准确率。
- en: '[PRE18]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The second-to-last line of the code snippet above uses the function `calculate_accuracy`
    which we have not yet defined, so let’s do that now. The accuracy of the model
    on the validation set is given by the fraction of correct predictions. Therefore,
    we can take the logits produced by the model, which are stored in the variable
    `logits`, and use this `argmax` function from NumPy. The `argmax` function will
    simply return the index of the element in the array that is the largest. If the
    logits for the text `I liked this movie` are `[0.08, 0.92]`, where `0.08` indicates
    the probability of the text being `negative` and `0.92` indicates the probability
    of the text being `positive`, the `argmax` function will return the index `1`
    since the model believes the text is more likely positive than it is negative.
    We can then compare the label `1` against our `labels` tensor we encoded earlier
    in Section 3.3 (line 19). Since the `logits` variable will contain the positive
    and negative probability values for every review in the batch (16 in total), the
    accuracy for the model will be calculated out of a maximum of 16 correct predictions.
    The code in the cell above shows the `val_accuracy` variable keeping track of
    every accuracy score, which we divide at the end of the validation to determine
    the average accuracy of the model on the validation data.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码片段倒数第二行使用了我们尚未定义的函数`calculate_accuracy`，所以现在让我们来定义它。模型在验证集上的准确度由正确预测的比例给出。因此，我们可以取模型生成的logits，这些值存储在变量`logits`中，并使用NumPy中的`argmax`函数。`argmax`函数将简单地返回数组中最大元素的索引。如果文本`I
    liked this movie`的logits是`[0.08, 0.92]`，其中`0.08`表示文本是`negative`的概率，`0.92`表示文本是`positive`的概率，那么`argmax`函数将返回索引`1`，因为模型认为该文本更可能是正面的而不是负面的。然后，我们可以将标签`1`与我们在第3.3节（第19行）之前编码的`labels`张量进行比较。由于`logits`变量将包含批次中每个评论的正负概率值（总共16个），因此模型的准确度将从最多16个正确预测中计算得出。上面单元格中的代码显示了`val_accuracy`变量跟踪每个准确度得分，我们在验证结束时对其进行除法运算，以确定模型在验证数据上的平均准确度。
- en: '[PRE19]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**3.7 — Complete Fine-tuning Pipeline**'
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.7 — 完整的微调管道**'
- en: And with that, we have completed the explanation of fine-tuning! The code below
    pulls everything above into a single, reusable class that can be used for any
    NLP task for BERT. Since the data preprocessing step is task-dependent, this has
    been taken outside of the fine-tuning class.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 至此，我们已经完成了微调的解释！下面的代码将上述所有内容汇总成一个单一、可重用的类，可以用于BERT的任何NLP任务。由于数据预处理步骤依赖于任务，因此它已被移出微调类之外。
- en: '**Preprocessing Function for Sentiment Analysis with the IMDb Dataset:**'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**情感分析的预处理函数（使用IMDb数据集）：**'
- en: '[PRE20]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Task-Agnostic Fine-tuning Pipeline Class:**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**任务无关的微调管道类：**'
- en: '[PRE21]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Example of Using the Class for Sentiment Analysis with the IMDb Dataset:**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用该类进行情感分析（使用IMDb数据集）示例：**'
- en: '[PRE22]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 4 —Conclusion
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 — 结论
- en: In this article, we have explored various aspects of BERT, including the landscape
    at the time of its creation, a detailed breakdown of the model architecture, and
    writing a task-agnostic fine-tuning pipeline, which we demonstrated using sentiment
    analysis. Despite being one of the earliest LLMs, BERT has remained relevant even
    today, and continues to find applications in both research and industry. Understanding
    BERT and its impact on the field of NLP sets a solid foundation for working with
    the latest state-of-the-art models. Pre-training and fine-tuning remain the dominant
    paradigm for LLMs, so hopefully this article has given some valuable insights
    you can take away and apply in your own projects!
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了BERT的各个方面，包括其创建时的背景、模型架构的详细解析，以及编写任务无关的微调管道，并通过情感分析进行了演示。尽管BERT是最早的LLM之一，但即使到今天，它仍然具有相关性，并继续在研究和工业中找到应用。理解BERT及其对自然语言处理领域的影响为使用最新的最先进模型奠定了坚实的基础。预训练和微调仍然是LLM的主导范式，因此希望本文能为你提供一些宝贵的见解，可以在自己的项目中加以应用！
- en: 5 — Further Reading
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 — 进一步阅读
- en: '[1] J. Devlin, M. Chang, K. Lee, and K. Toutanova, [BERT: Pre-training of Deep
    Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
    (2019), North American Chapter of the Association for Computational Linguistics'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] J. Devlin, M. Chang, K. Lee, 和 K. Toutanova，[BERT：用于语言理解的深度双向变换器预训练](https://arxiv.org/abs/1810.04805)（2019），计算语言学学会北美分会'
- en: '[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
    (2017), Advances in Neural Information Processing Systems 30 (NIPS 2017)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin, [注意力机制：一切你需要的](https://arxiv.org/pdf/1706.03762) (2017),
    神经信息处理系统进展 30（NIPS 2017）'
- en: '[3] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L.
    Zettlemoyer, [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365)
    (2018), Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, 和 L.
    Zettlemoyer, [深度上下文化的词表示](https://arxiv.org/pdf/1802.05365) (2018), 2018年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长篇论文）'
- en: '[4] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever (2018), [Improving
    Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf),'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever (2018), [通过生成预训练改进语言理解](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf),'
- en: '[5] Hugging Face, [Fine-Tuned BERT Models](https://huggingface.co/models?sort=trending&search=BERT)
    (2024), HuggingFace.co'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hugging Face, [微调后的BERT模型](https://huggingface.co/models?sort=trending&search=BERT)
    (2024), HuggingFace.co'
- en: '[6] M. Schuster and K. K. Paliwal, [Bidirectional recurrent neural networks](https://www.researchgate.net/publication/3316656_Bidirectional_recurrent_neural_networks)
    (1997), IEEE Transactions on Signal Processing 45'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] M. Schuster 和 K. K. Paliwal, [双向递归神经网络](https://www.researchgate.net/publication/3316656_Bidirectional_recurrent_neural_networks)
    (1997), IEEE 信号处理学报 45'
- en: '[7] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
    and S. Fidler, [Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books](https://arxiv.org/pdf/1506.06724v1.pdf)
    (2015), 2015 IEEE International Conference on Computer Vision (ICCV)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
    和 S. Fidler, [对齐书籍和电影：通过观看电影和阅读书籍实现类似故事的视觉解释](https://arxiv.org/pdf/1506.06724v1.pdf)
    (2015), 2015年IEEE计算机视觉国际会议（ICCV）'
- en: '[8] L. W. Taylor, [“Cloze Procedure”: A New Tool for Measuring Readability](https://gwern.net/doc/psychology/writing/1953-taylor.pdf)
    (1953), Journalism Quarterly, 30(4), 415–433.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] L. W. Taylor, [“完形填空程序”：衡量可读性的新工具](https://gwern.net/doc/psychology/writing/1953-taylor.pdf)
    (1953), 新闻学季刊，30(4)，415–433。'
- en: '[9] Hugging Face, [Pre-trained Tokenizers](https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained.pretrained_model_name_or_path)
    (2024) HuggingFace.co'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Hugging Face, [预训练分词器](https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained.pretrained_model_name_or_path)
    (2024) HuggingFace.co'
- en: '[10] Hugging Face, [Pre-trained Tokenizer Encode Method](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode)
    (2024) HuggingFace.co'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Hugging Face, [预训练分词器编码方法](https://huggingface.co/docs/transformers/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.encode)
    (2024) HuggingFace.co'
- en: '[11] T. Vo, [PyTorch DataLoader: Features, Benefits, and How to Use it](https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/#:~:text=There%20are%20several%20benefits%20of,time,%20especially%20for%20large%20datasets)
    (2023) SaturnCloud.io'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] T. Vo, [PyTorch DataLoader：特点、优势及如何使用](https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/#:~:text=There%20are%20several%20benefits%20of,time,%20especially%20for%20large%20datasets)
    (2023) SaturnCloud.io'
- en: '[12] Hugging Face, [Modelling BERT](https://github.com/huggingface/transformers/tree/v4.40.1)
    (2024) GitHub.com'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Hugging Face, [BERT建模](https://github.com/huggingface/transformers/tree/v4.40.1)
    (2024) GitHub.com'
- en: '[13] Hugging Face, [Run Glue](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2         d008813037968a9e58/examples/run_glue.py#L308),
    GitHub.com'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Hugging Face, [运行Glue](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L308),
    GitHub.com'
- en: '[14] NVIDIA, [CUDA Zone](https://developer.nvidia.com/cuda-zone#:~:text=CUDA%C2%AE%20is%20a%20parallel,harnessing%20the%20power%20of%20GPUs.)
    (2024), Developer.NVIDIA.com'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] NVIDIA, [CUDA专区](https://developer.nvidia.com/cuda-zone#:~:text=CUDA%C2%AE%20is%20a%20parallel,harnessing%20the%20power%20of%20GPUs.)
    (2024), Developer.NVIDIA.com'
- en: '[15] C. McCormick and N. Ryan, [BERT Fine-tuning](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model)
    (2019), McCormickML.com'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] C. McCormick 和 N. Ryan, [BERT微调](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model)
    (2019), McCormickML.com'
