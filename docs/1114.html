<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Turn Llama 3 into an Embedding Model with LLM2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Turn Llama 3 into an Embedding Model with LLM2Vec</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03">https://towardsdatascience.com/turn-llama-3-into-an-embedding-model-with-llm2vec-8448005f99aa?source=collection_archive---------1-----------------------#2024-05-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="68fe" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">RAG with Llama 3 for the generation and the retrieval</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Benjamin Marie" class="l ep by dd de cx" src="../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*sifLT7ybERpQ7SnaPwBDBQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@bnjmn_marie?source=post_page---byline--8448005f99aa--------------------------------" rel="noopener follow">Benjamin Marie</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8448005f99aa--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk ml"><img src="../Images/aa3e5f5e3c663da28873ad33454491ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*B7SXJRSsSYGozfPMPZ1_iA.png"/></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx">Generated with DALL-E</figcaption></figure><p id="9b91" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The embedding model is a critical component of retrieval-augmented generation (RAG) for large language models (LLMs). They encode the knowledge base and the query written by the user.</p><p id="fe96" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Using an embedding model trained or fine-tuned for the same domain as the LLM can significantly improve a RAG system. However, finding or training such an embedding model is often a difficult task as in-domain data are usually scarce.</p><p id="4a32" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">In this article, I show how to turn an LLM into a text embedding model using LLM2Vec. We will see how to do it with Llama 3 to create a RAG system that doesn’t need any other models than Llama 3.</p><h1 id="5703" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">LLM2Vec: Your LLM is Also an Embedding Model</h1><p id="7a3e" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">LLM2Vec is presented in this paper:</p><p id="b588" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><a class="af ov" href="https://arxiv.org/pdf/2404.05961.pdf" rel="noopener ugc nofollow" target="_blank">LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders</a></p><p id="f56c" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The official implementation is available on GitHub:</p><ul class=""><li id="e7bc" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt ow ox oy bk"><a class="af ov" href="https://github.com/McGill-NLP/llm2vec" rel="noopener ugc nofollow" target="_blank">McGill-NLP/llm2vec</a> (MIT license)</li></ul><p id="b359" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><em class="oz">How does LLM2Vec work?</em></p><p id="e2ce" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">LLMs are trained with a causal language modeling loss. They are trained to predict the next token given a sequence of tokens. Since the training examples are entire sequences of tokens, a causal attention mask is applied to the sequence so that when the model learns to predict a token, all the following tokens in the sequence are masked and don’t influence the attention computation. For instance, if we have the following sequence:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="90a8" class="pe nv fq pb b bg pf pg l ph pi">The cat is sleeping in the kitchen &lt;eos&gt;</span></pre><p id="0831" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">During training the following attention masks will be successively applied (under the mask, I only show the tokens that are not masked):</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="06fb" class="pe nv fq pb b bg pf pg l ph pi">1 0 0 0 0 0 0 0 0<br/>The <br/><br/>1 1 0 0 0 0 0 0<br/>The cat<br/><br/>1 1 1 0 0 0 0 0<br/>The cat is <br/><br/>1 1 1 1 0 0 0 0<br/>The cat is sleeping<br/><br/>1 1 1 1 1 0 0 0<br/>The cat is sleeping in <br/><br/>1 1 1 1 1 1 0 0<br/>The cat is sleeping in the <br/><br/>1 1 1 1 1 1 1 0<br/>The cat is sleeping in the kitchen<br/><br/>1 1 1 1 1 1 1 1<br/>The cat is sleeping in the kitchen &lt;eos&gt;</span></pre><p id="837a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The zeroes indicate that the token will not be considered for the attention computation.</p><p id="3d78" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">On the other hand, text embedding models are trained to encode entire sequence tokens and are bidirectional, i.e., they encode a sequence from left to right and right to left.</p><p id="3615" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The initial step in the LLM2Vec method to convert an LLM into an embedding model involves substituting the causal attention mask used in decoder-only LLMs with a matrix of all-ones. This alteration allows each token in the sequence to interact with all other tokens, effectively transforming it into a bidirectional LLM. However, decoder-only LLMs were not trained to encode future tokens, and thus, this straightforward modification degrades the quality of the representations. Nonetheless, it is possible to train the model to effectively use its new bidirectional attention capabilities.</p><p id="b242" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">They suggest employing a new masked next token prediction (MNTP) objective. MNTP merges the next token prediction objective with the masked language modeling used by BERT. To implement this, we take an arbitrary sequence x = (x1, x2, . . . , xN), mask a subset of the input tokens, and then train the model to predict these masked tokens using both past and future contexts. Importantly, when predicting a token that has been masked at position i, the loss is calculated using the logits from the token representation at the preceding position i − 1, rather than from the masked position itself as we will do for a BERT model.</p><p id="01ba" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Converting the LLM into a bidirectional model followed by MNTP training can adapt any decoder-only LLM into an encoder. However, these steps may not adequately address the need for sequence representations. Indeed, bidirectional encoders like BERT usually also incorporate a next-sentence prediction task in their pre-training while LLMs were not trained for this task. LLM2Vec addresses this missing capability by processing each input sequence twice through the model, each time with different randomly selected dropout masks, producing two distinct representations of the same sequence. The training objective is to improve the similarity between these two representations while reducing their similarity to representations of different sequences within the same training batch. They call this last step “Unsupervised Contrastive Learning” (SimCSE).</p><p id="7953" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The full process behind LLM2Vec is illustrated by this figure in the paper:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="pk pl ed pm bh pn"><div class="mj mk pj"><img src="../Images/8f30670da68e07a279a68d6d8c488f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SuTaGxJoxSBw8q6-.png"/></div></div><figcaption class="mt mu mv mj mk mw mx bf b bg z dx"><a class="af ov" href="https://arxiv.org/abs/2404.05961" rel="noopener ugc nofollow" target="_blank">source</a> (CC-BY)</figcaption></figure><p id="e4fc" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">They evaluated the models produced by LLM2Vec in various tasks and showed that they can outperform standard text embedding models. You will find the results in the sections 3 and 4 of the paper.</p><h1 id="d6c4" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Turning Llama 3 into a Text Embedding Model with LLM2Vec</h1><p id="9baa" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">My notebook showing how to convert Llama 3 into an embedding model is available here:</p><p id="34f6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk"><a class="af ov" href="https://newsletter.kaitchup.com/p/notebooks" rel="noopener ugc nofollow" target="_blank">Get the notebook (#65)</a></p><p id="ad80" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Converting an LLM to a text embedding model with LLM2Vec is fairly simple.</p><p id="92e3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">First, install the following packages:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="5b9a" class="pe nv fq pb b bg pf pg l ph pi">pip install llm2vec<br/>pip install flash-attn --no-build-isolation</span></pre><p id="9a63" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The llm2vec package will convert the LLM to an embedding model. flash-attn is the package for FlashAttention. It is not required but speeds up the training with MNTP. It works only with recent GPUs from the Ampere generation (RTX 3xxx/4xxx, A100, H100, etc.).</p><p id="de35" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Then, the conversion itself is performed by the following code:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="0cf2" class="pe nv fq pb b bg pf pg l ph pi">import torch<br/>from llm2vec import LLM2Vec<br/><br/>l2v = LLM2Vec.from_pretrained(<br/>    "meta-llama/Meta-Llama-3-8B",<br/>    device_map="cuda" if torch.cuda.is_available() else "cpu",<br/>    torch_dtype=torch.bfloat16,<br/>)<br/><br/>l2v.save("Llama-3-8B-Emb")</span></pre><p id="2a33" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">“torch_dtype=torch.bfloat16” is necessary to be able to run the conversion on a 24 GB GPU. If you don’t set it the model will be larger than the original model with float32 parameters.</p><p id="f2f6" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">I pushed this model to the hub in case you don’t want to do this conversion by yourself:</p><ul class=""><li id="6f6b" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt ow ox oy bk"><a class="af ov" href="https://huggingface.co/kaitchup/Llama-3-8B-llm2vec-Emb" rel="noopener ugc nofollow" target="_blank">kaitchup/Llama-3-8B-llm2vec-Emb</a></li></ul><p id="6496" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This model is ready to be used. You can plug it into a RAG pipeline. However, it won’t perform as well as a standard embedding model (in most cases).</p><p id="4506" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">We need to train Llama 3 8B with the MNTP objective. The authors also provide a script to do it:</p><ul class=""><li id="9014" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt ow ox oy bk"><a class="af ov" href="https://github.com/McGill-NLP/llm2vec/blob/main/experiments/run_mntp.py" rel="noopener ugc nofollow" target="_blank">experiments/run_mntp.py</a></li></ul><p id="339f" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">It currently supports models with the Llama and Mistral architectures.</p><p id="f129" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">To use it, clone the repository locally:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="8a4e" class="pe nv fq pb b bg pf pg l ph pi">git clone https://github.com/McGill-NLP/llm2vec.git</span></pre><p id="91e2" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The script expects one argument which is a configuration file in the JSON format. They propose several examples here:</p><ul class=""><li id="ac5a" class="my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt ow ox oy bk"><a class="af ov" href="https://github.com/McGill-NLP/llm2vec/tree/main/train_configs/mntp" rel="noopener ugc nofollow" target="_blank">train_configs/mntp</a></li></ul><p id="c7f9" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">For Llama 3 8B, I made the following configuration:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="8ec0" class="pe nv fq pb b bg pf pg l ph pi">JSON_CONFIG='''<br/>{<br/>    "model_name_or_path": "meta-llama/Meta-Llama-3-8B",<br/>    "dataset_name": "wikitext",<br/>    "dataset_config_name": "wikitext-103-raw-v1",<br/>    "per_device_train_batch_size": 1,<br/>    "per_device_eval_batch_size": 1,<br/>    "gradient_accumulation_steps": 16,<br/>    "do_train": true,<br/>    "do_eval": true,<br/>    "max_seq_length": 512,<br/>    "mask_token_type": "blank",<br/>    "data_collator_type": "all_mask",<br/>    "mlm_probability": 0.8,<br/>    "overwrite_output_dir": true,<br/>    "output_dir": "Llama-3-8B-llm2vec-MNTP-Emb",<br/>    "evaluation_strategy": "steps",<br/>    "eval_steps": 100,<br/>    "save_steps": 200,<br/>    "stop_after_n_steps": 1000,<br/>    "lora_r": 16,<br/>    "gradient_checkpointing": true,<br/>    "torch_dtype": "bfloat16",<br/>    "attn_implementation": "flash_attention_2"<br/>}<br/>'''<br/><br/>with open("mntp_config.json", 'w') as f:<br/>  f.write(JSON_CONFIG)</span></pre><p id="ac6a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">The script loads the model with bfloat16 parameters. I set the batch sizes per device to one so that the training can fit on a 24 GB GPU.</p><p id="5ad9" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">Then, we can start MNTP training:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="97e6" class="pe nv fq pb b bg pf pg l ph pi">python llm2vec/experiments/run_mntp.py mntp_config.json</span></pre><p id="843c" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">This should take 4 days using a 24 GB GPU, such as the L4 of Google Colab, for three epochs. One epoch could be enough if you can’t wait that long.</p><p id="ec83" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">After MNTP training, the model should yield much better results, especially for retrieval tasks.</p><p id="a80a" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">For the last step which is SimCSE training, the authors didn’t release their code yet but mentioned that they will.</p><h1 id="676d" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Setting up Llama 3 Text Embedding Model for RAG</h1><p id="3b0a" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">The embedding model created in the previous section is ready to be used in a RAG pipeline. For instance, you can load it with <a class="af ov" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">sentence-transformers</a> (Apache 2.0 license).</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="c396" class="pe nv fq pb b bg pf pg l ph pi">from sentence_transformers import SentenceTransformer<br/><br/>model = SentenceTransformer("kaitchup/Llama-3-8B-llm2vec-Emb")</span></pre><p id="0335" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">If you use <a class="af ov" href="https://github.com/run-llama/llama_index" rel="noopener ugc nofollow" target="_blank">LlamaIndex</a> (MIT license), you can set the HuggingFaceEmbedding model:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="2cc5" class="pe nv fq pb b bg pf pg l ph pi">Settings.embed_model = HuggingFaceEmbedding(model_name="kaitchup/Llama-3-8B-llm2vec-Emb", device='cpu')</span></pre><p id="51b3" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">I set device=’cpu’ but using the CPU makes the RAG system slower. You can remove this argument to run it on the GPU. However, note that the model is loaded in full precision. It doesn’t fit on a consumer GPU.</p><p id="05e5" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">I explain in detail how to set up a RAG system with LlamaIndex in this tutorial:</p><div class="po pp pq pr ps pt"><a href="https://newsletter.kaitchup.com/p/rag-for-mistral-7b-instruct-with?source=post_page-----8448005f99aa--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab ig"><div class="pv ab co cb pw px"><h2 class="bf fr hw z io py iq ir pz it iv fp bk">RAG for Mistral 7B Instruct with LlamaIndex and Transformers</h2><div class="qa l"><h3 class="bf b hw z io py iq ir pz it iv dx">RAG on budget</h3></div><div class="qb l"><p class="bf b dy z io py iq ir pz it iv dx">newsletter.kaitchup.com</p></div></div><div class="qc l"><div class="qd l qe qf qg qc qh lr pt"/></div></div></a></div><h1 id="1e93" class="nu nv fq bf nw nx ny gq nz oa ob gt oc od oe of og oh oi oj ok ol om on oo op bk">Conclusion</h1><p id="b5d1" class="pw-post-body-paragraph my mz fq na b go oq nc nd gr or nf ng nh os nj nk nl ot nn no np ou nr ns nt fj bk">With LLM2Vec, we can now use an LLM as a text embedding model. The conversion is simple and fast. Using one single model for both the generation and the retrieval in a RAG system is appealing as we don’t need to search for an additional embedding model.</p><p id="17ea" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">However, embedding models simply extracted from LLMs tend to underperform regular embedding models. The authors of LLM2Vec propose new training objectives, MNTP and SimCSE, to train the embedding model extracted from LLMs. I found this training to be costly but can yield significantly better embedding models according to the authors.</p></div></div></div><div class="ab cb qi qj qk ql" role="separator"><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo qp"/><span class="qm by bm qn qo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7421" class="pw-post-body-paragraph my mz fq na b go nb nc nd gr ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt fj bk">To support my work, consider subscribing to my newsletter for more articles/tutorials on recent advances in AI:</p><div class="po pp pq pr ps pt"><a href="https://newsletter.kaitchup.com/?source=post_page-----8448005f99aa--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pu ab ig"><div class="pv ab co cb pw px"><h2 class="bf fr hw z io py iq ir pz it iv fp bk">The Kaitchup - AI on a Budget | Benjamin Marie | Substack</h2><div class="qa l"><h3 class="bf b hw z io py iq ir pz it iv dx">Weekly tutorials, tips, and news on fine-tuning, running, and serving large language models on your computer. The…</h3></div><div class="qb l"><p class="bf b dy z io py iq ir pz it iv dx">newsletter.kaitchup.com</p></div></div><div class="qc l"><div class="qq l qe qf qg qc qh lr pt"/></div></div></a></div></div></div></div></div>    
</body>
</html>