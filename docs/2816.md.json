["```py\nclass AgentState(TypedDict):\n    \"\"\"\n    A dictionary representing the state of the research agent.\n\n    Attributes:\n        task (str): The description of the task to be performed.\n        plan (str): The research plan generated for the task.\n        draft (str): The current draft of the research report.\n        critique (str): The critique received for the draft.\n        content (List[str]): A list of content gathered during research.\n        revision_number (int): The current revision number of the draft.\n        max_revisions (int): The maximum number of revisions allowed.\n        finalized_state (bool): Indicates whether the report is finalized.\n    \"\"\"\n\n    task: str\n    plan: str\n    draft: str\n    critique: str\n    content: List[str]\n    editor_comment: str\n    revision_number: int\n    max_revisions: int\n    finalized_state: bool\n```", "```py\n def plan_node(self, state: AgentState) -> Dict[str, str]:\n        \"\"\"\n        Generate a research plan based on the current state.\n\n        Args:\n            state (AgentState): The current state of the research agent.\n\n        Returns:\n            Dict[str, str]: A dictionary containing the generated research plan.\n        \"\"\"\n        messages = [\n            SystemMessage(content=ResearchPlanPrompt.system_template),\n            HumanMessage(content=state[\"task\"]),\n        ]\n        response = self.model.invoke(messages)\n        return {\"plan\": response.content}\n```", "```py\nmodel = ChatOpenAI(\n    model=\"gpt-4o-mini\", temperature=0, api_key=secrets[\"OPENAI_API_KEY\"]\n)\n```", "```py\n@dataclass\nclass ResearchPlanPrompt:\n    system_template: str = \"\"\"\n    You are an expert writer tasked with creating a high-level outline for a research report.\n    Write such an outline for the user-provided topic. Include relevant notes or instructions for each section.\n    The style of the research report should be geared towards the educated public. It should be detailed enough to provide\n    a good level of understanding of the topic, but not unnecessarily dense. Think of it more like a whitepaper to be consumed \n    by a business leader rather than an academic journal article. \n    \"\"\"\n```", "```py\n def should_continue(state: AgentState) -> str:\n        \"\"\"\n        Determine whether the research process should continue based on the current state.\n\n        Args:\n            state (AgentState): The current state of the research agent.\n\n        Returns:\n            str: The next state to transition to (\"to_review\", \"accepted\", or \"rejected\").\n        \"\"\"\n        # always send to review if editor hasn't made comments yet\n        current_editor_comments = state.get(\"editor_comment\", [])\n        if not current_editor_comments:\n            return \"to_review\"\n\n        final_state = state.get(\"finalized_state\", False)\n        if final_state:\n            return \"accepted\"\n        elif state[\"revision_number\"] > state[\"max_revisions\"]:\n            logger.info(\"Revision number > max allowed revisions\")\n            return \"rejected\"\n        else:\n            return \"to_review\"\n```", "```py\nfrom research_assist.researcher.AgentComponents import (\n    AgentNodes,\n    AgentState,\n    AgentEdges,\n)\n# this is the predefined end node\nfrom langgraph.graph import END\n\nagent = StateGraph(AgentState)\nnodes = AgentNodes(model, searcher)\nedges = AgentEdges()\n\n## Nodes\nagent.add_node(\"initial_plan\", nodes.plan_node)\nagent.add_node(\"write\", nodes.generation_node)\nagent.add_node(\"review\", nodes.review_node)\nagent.add_node(\"do_research\", nodes.research_plan_node)\nagent.add_node(\"research_revise\", nodes.research_critique_node)\nagent.add_node(\"reject\", nodes.reject_node)\nagent.add_node(\"accept\", nodes.accept_node)\nagent.add_node(\"editor\", nodes.editor_node)\n\n## Edges\nagent.set_entry_point(\"initial_plan\")\nagent.add_edge(\"initial_plan\", \"do_research\")\nagent.add_edge(\"do_research\", \"write\")\nagent.add_edge(\"write\", \"editor\")\n\n## Conditional edges\nagent.add_conditional_edges(\n  \"editor\",\n  edges.should_continue,\n  {\"accepted\": \"accept\", \"to_review\": \"review\", \"rejected\": \"reject\"},\n)\nagent.add_edge(\"review\", \"research_revise\")\nagent.add_edge(\"research_revise\", \"write\")\nagent.add_edge(\"reject\", END)\nagent.add_edge(\"accept\", END)\n```", "```py\nfrom IPython.display import Image\n\nImage(agent.compile().get_graph().draw_png())\n```", "```py\nbrew install graphviz\npip install -U --no-cache-dir  \\\n        --config-settings=\"--global-option=build_ext\" \\\n        --config-settings=\"--global-option=-I$(brew --prefix graphviz)/include/\" \\\n        --config-settings=\"--global-option=-L$(brew --prefix graphviz)/lib/\" \\\n        pygraphviz\n```", "```py\ngraph = agent.compile()\nres = graph.invoke(\n    {\n        \"task\": \"What are the key trends in LLM research and application that you see in 2024\",\n        \"max_revisions\": 1,\n        \"revision_number\": 0,\n    }\n)\n```", "```py\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.checkpoint.memory import MemorySaver\nimport uuid\n\ncheckpointer = MemorySaver()\nin_memory_store = InMemoryStore()\ngraph = agent.compile(checkpointer=checkpointer, store=self.in_memory_store)\n\n# Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\nnamespace = (user_id, \"memories\")\n\nfor i, update in enumerate(graph.stream(\n  {\n     \"task\": task_description,\n     \"max_revisions\": max_revisions,\n     \"revision_number\": 0,\n  }, config, stream_mode=\"updates\"\n        )):\n   # print the data that just got generated \n   print(update)\n   memory_id = str(uuid.uuid4())\n   # store the data that just got generated in memory\n   self.in_memory_store.put(namespace, memory_id, {\"memory\": update})\n   results.append(update)\n```", "```py\n from langchain_core.messages import (\n    SystemMessage,\n    HumanMessage,\n)\nfrom research_assist.researcher.prompts import (\n    ResearchPlanPrompt,\n)\nfrom langchain_openai import ChatOpenAI\nfrom tavily import TavilyClient\n\nclass Queries(BaseModel):\n    \"\"\"\n    A model representing a list of search queries.\n\n    Attributes:\n        queries (List[str]): A list of search queries to be executed.\n    \"\"\"\n\n    queries: List[str]\n\n# set up task\ntask = \"\"\"\nWhat are the key trends in LLM reseach and application that you see in 2024\n\"\"\"\n\n# set up LLM and Tavily\nmodel = ChatOpenAI(\n    model=\"gpt-4o-mini\", temperature=0, api_key=secrets[\"OPENAI_API_KEY\"]\n)\ntavily = TavilyClient(api_key=secrets[\"TAVILY_API_KEY\"])\n\n# generate some queries relevant to the task\nqueries = agent.nodes.model.with_structured_output(Queries).invoke(\n            [\n                SystemMessage(content=ResearchPlanPrompt.system_template),\n                HumanMessage(content=task),\n            ]\n)\n```", "```py\n['key trends in LLM research 2024',\n 'LLM applications 2024',\n 'latest developments in LLM technology 2024',\n 'future of LLMs 2024',\n 'LLM research advancements 2024']\n```", "```py\nresponse = tavily.search(query=queries[0], max_results=2)\n```", "```py\nfrom research_assist.researcher.Agent import ResearchAgent, load_secrets\nfrom langchain_openai import ChatOpenAI\nfrom tavily import TavilyClient\n\nsecrets = load_secrets()\nmodel = ChatOpenAI(\n    model=\"gpt-4o-mini\", temperature=0, api_key=secrets[\"OPENAI_API_KEY\"]\n)\ntavily = TavilyClient(api_key=secrets[\"TAVILY_API_KEY\"])\n\nagent = ResearchAgent(model, tavily)\n```", "```py\ntask = \"\"\"\nWhat are the key trends in LLM reseach and application that you see in 2024\n\"\"\"\nresult = agent.run_task(task_description=task,max_revisions=3)\n```", "```py\nMarkdown(result[-3]['write']['draft'])\n```", "```py\nagent.in_memory_store.search((\"1\", \"memories\"))[-3].dict()\n```", "```py\neditor_comments = agent.in_memory_store.search((\"1\", \"memories\"))[-2].dict()\n```", "```py\n{'value': {'memory': {'editor': {'editor_comment': \n'The report has addressed the critiques by enhancing depth in key sections, \nadding clarity, and improving structure with subheadings. \nIt provides specific examples and discusses ethical considerations, \nmaking it a valuable resource. The revisions are sufficient for publication.',\n    'finalized_state': True}}},\n 'key': '9005ad06-c8eb-4c6f-bb94-e77f2bc867bc',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-11-11T06:09:46.170263+00:00',\n 'updated_at': '2024-11-11T06:09:46.170267+00:00'}\n```"]