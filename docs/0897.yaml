- en: Feature Engineering with Microsoft Fabric and PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08](https://towardsdatascience.com/feature-engineering-with-microsoft-fabric-and-pyspark-16d458018744?source=collection_archive---------9-----------------------#2024-04-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fabric Madness part 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[![Roger
    Noble](../Images/869b5b0f237f24b119ca6c41c2e31162.png)](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    [Roger Noble](https://medium.com/@roger_noble?source=post_page---byline--16d458018744--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--16d458018744--------------------------------)
    ·12 min read·Apr 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e76cf994c16355962435cec1d006594b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author and ChatGPT. “Design an illustration, focusing on a basketball
    player in action, this time the theme is on using pyspark to generate features
    for machine leaning models in a graphic novel style” prompt. ChatGPT, 4, OpenAI,
    4 April. 2024\. [https://chat.openai.com.](https://chat.openai.com./)
  prefs: []
  type: TYPE_NORMAL
- en: '*A Huge thanks to* [*Martim Chaves*](https://medium.com/@mgrc99) *who co-authored
    this post and developed the example scripts.*'
  prefs: []
  type: TYPE_NORMAL
- en: In our [previous post](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)
    we took a high level view of how to train a machine learning model in [Microsoft
    Fabric](https://www.microsoft.com/en-us/microsoft-fabric). In this post we wanted
    to dive deeper into the process of feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering is a crucial part of the development lifecycle for any Machine
    Learning (ML) systems. It is a step in the development cycle where raw data is
    processed to better represent its underlying structure and provide additional
    information that enhance our ML models. Feature engineering is both an art and
    a science. Even though there are specific steps that we can take to create good
    features, sometimes, it is only through experimentation that good results are
    achieved. Good features are crucial in guaranteeing a good system performance.
  prefs: []
  type: TYPE_NORMAL
- en: As datasets grow exponentially, traditional feature engineering may struggle
    with the size of very large datasets. This is where PySpark can help — as it is
    a scalable and efficient processing platform for massive datasets. A great thing
    about Fabric is that it makes using PySpark easy!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this post, we’ll be going over:'
  prefs: []
  type: TYPE_NORMAL
- en: How does PySpark Work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature Engineering in Action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this post, hopefully you’ll feel comfortable carrying out feature
    engineering with PySpark in Fabric. Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: How does PySpark work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark is a distributed computing system that allows for the processing of large
    datasets with speed and efficiency across a cluster of machines. It is built around
    the concept of a Resilient Distributed Dataset (RDD), which is a fault-tolerant
    collection of elements that can be operated on in parallel. RDDs are the fundamental
    data structure of Spark, and they allow for the distribution of data across a
    cluster of machines.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark is the Python API for Spark. It allows for the creation of Spark DataFrames,
    which are similar to Pandas DataFrames, but with the added benefit of being distributed
    across a cluster of machines. PySpark DataFrames are the core data structure in
    PySpark, and they allow for the manipulation of large datasets in a distributed
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: At the core of PySpark is the `SparkSession` object, which is what fundamentally
    interacts with Spark. This `SparkSession` is what allows for the creation of DataFrames,
    and other functionalities. Note that, when running a Notebook in Fabric, a `SparkSession`
    is automatically created for you, so you don't have to worry about that.
  prefs: []
  type: TYPE_NORMAL
- en: Having a rough idea of how PySpark works, let’s get to the basics.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Spark DataFrames may remind us of Pandas DataFrames due to their similarities,
    the syntax when using PySpark can be a bit different. In this section, we’ll go
    over some of the basics of PySpark, such as reading data, combining DataFrames,
    selecting columns, grouping data, joining DataFrames, and using functions.
  prefs: []
  type: TYPE_NORMAL
- en: The Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data we are looking at is from the 2024 US college basketball tournaments,
    which was obtained from the on-going *March Machine Learning Mania 2024* Kaggle
    competition, the details of which can be found [here](https://www.kaggle.com/competitions/march-machine-learning-mania-2024/overview),
    and is licensed under CC BY 4.0 [1]
  prefs: []
  type: TYPE_NORMAL
- en: Reading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned in the [previous post](https://medium.com/towards-data-science/fabric-madness-96b84dc5f241)
    of this series, the first step is usually to create a Lakehouse and upload some
    data. Then, when creating a Notebook, we can attach it to the created Lakehouse,
    and we’ll have access to the data stored there.
  prefs: []
  type: TYPE_NORMAL
- en: 'PySpark Dataframes can read various data formats, such as CSV, JSON, Parquet,
    and others. Our data is stored in CSV format, so we’ll be using that, like in
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, we’re reading the detailed results data set of the final
    women’s basketball college tournament matches. Note that the `"header"` option
    being true means that the names of the columns will be derived from the first
    row of the CSV file. The `inferSchema` option tells Spark to guess the data types
    of the columns - otherwise they would all be read as strings. `.cache()` is used
    to keep the DataFrame in memory.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re coming from Pandas, you may be wondering what the equivalent of `df.head()`
    is for PySpark - it's `df.show(5)`. The default for `.show()` is the top 20 rows,
    hence the need to specifically select 5.
  prefs: []
  type: TYPE_NORMAL
- en: Combining DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Combining DataFrames can be done in multiple ways. The first we will look at
    is a union, where the columns are the same for both DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, `unionByName` joins the two DataFrames by matching the names of the columns.
    Since both the women's and the men's *detailed match results* have the same columns,
    this is a good approach. Alternatively, there's also `union`, which combines two
    DataFrames, matching column positions.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Selecting columns from a DataFrame in PySpark can be done using the `.select()`
    method. We just have to indicate the name or names of the columns that are relevant
    as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output for `w_scores.show(5)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the output for `w_scores.show(5)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The columns can also be renamed when being selected using the `.alias()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Grouping Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Grouping allows us to carry out certain operations for the groups that exist
    within the data and is usually combined with a aggregation functions. We can use
    `.groupBy()` for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we are grouping by `"TeamID"`, meaning we're considering the
    groups of rows that have a distinct value for `"TeamID"`. For each of those groups,
    we're calculating the average of the `"Score"`. This way, we get the average score
    for each team.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output of `winners_average_scores.show(5)`, showing the average
    score of each team:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Joining Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Joining two DataFrames can be done using the `.join()` method. Joining is essentially
    extending the DataFrame by adding the columns of one DataFrame to another.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this example, both `stats_df` and `matches_df` were using `Season` and `TeamID`
    as unique identifiers for each row. Besides `Season` and `TeamID`, `stats_df`
    has other columns, such as statistics for each team during each season, whereas
    `matches_df` has information about the matches, such as date and location. This
    operation allows us to add those interesting statistics to the matches information!
  prefs: []
  type: TYPE_NORMAL
- en: Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several functions that PySpark provides that help us transform DataFrames.
    You can find the full list [here](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of a simple function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the code snippet above, a `"HighScore"` column is created when the score
    is higher than 80\. For each row in the `"Score"` column (indicated by the `.col()`
    function), the value `"Yes"` is chosen for the `"HighScore"` column if the `"Score"`
    value is larger than 80, determined by the `.when()` function. `.otherwise()`,
    the value chosen is `"No"`.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering in Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of PySpark and how it can be used, let’s
    go over how the regular season statistics features were created. These features
    were then used as inputs into our machine learning model to try to predict the
    outcome of the final tournament games.
  prefs: []
  type: TYPE_NORMAL
- en: The starting point was a DataFrame, `regular_data`, that contained match by
    match statistics for the *regular seasons*, which is the United States College
    Basketball Season that happens from November to March each year.
  prefs: []
  type: TYPE_NORMAL
- en: Each row in this DataFrame contained the season, the day the match was held,
    the ID of team 1, the ID of team 2, and other information such as the location
    of the match. Importantly, it also contained statistics for *each team* for that
    *specific match*, such as `"T1_FGM"`, meaning the Field Goals Made (FGM) for team
    1, or `"T2_OR"`, meaning the Offensive Rebounds (OR) of team 2.
  prefs: []
  type: TYPE_NORMAL
- en: The first step was selecting which columns would be used. These were columns
    that strictly contained in-game statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you’re interested, here’s what each statistic’s code means:'
  prefs: []
  type: TYPE_NORMAL
- en: 'FGM: Field Goals Made'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FGA: Field Goals Attempted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FGM3: Field Goals Made from the 3-point-line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FGA3: Field Goals Attempted for 3-point-line goals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OR: Offensive Rebounds. A rebounds is when the ball rebounds from the board
    when a goal is attempted, not getting in the net. If the team that *attempted*
    the goal gets possession of the ball, it’s called an “Offensive” rebound. Otherwise,
    it’s called a “Defensive” Rebound.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DR: Defensive Rebounds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ast: Assist, a pass that led directly to a goal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stl: Steal, when the possession of the ball is stolen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PF: Personal Foul, when a player makes a foul'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From there, a dictionary of *aggregation expressions* was created. Basically,
    for each column name in the previous list of columns, a function was stored that
    would calculate the mean of the column, and rename it, by adding a suffix, `"mean"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then, the data was grouped by `"Season"` and `"T1_TeamID"`, and the aggregation
    functions of the previously created dictionary were used as the argument for `.agg()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that the grouping was done by season and the **ID of team 1** — this means
    that `"T2_FGAmean"`, for example, will actually be the mean of the Field Goals
    Attempted made by the **opponents** of T1, not necessarily of a specific team.
    So, we actually need to rename the columns that are something like `"T2_FGAmean"`
    to something like `"T1_opponent_FGAmean"`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At this point, it’s important to mention that the `regular_data` DataFrame actually
    has **two** rows per each match that occurred. This is so that both teams can
    be "T1" and "T2", for each match. This little "trick" is what makes these statistics
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we “only” have the statistics for “T1”. We “need” the statistics for
    “T2” as well — “need” in quotations because there are no new statistics being
    calculated. We just need the same data, but with the columns having different
    names, so that for a match with “T1” and “T2”, we have statistics for both T1
    and T2\. So, we created a mirror DataFrame, where, instead of “T1&mldr;mean” and
    “T1_opponent_&mldr;mean”, we have “T2&mldr;mean” and “T2_opponent_&mldr;mean”.
    This is important because, later on, when we’re joining these regular season statistics
    to tournament matches, we’ll be able to have statistics for both team 1 **and**
    team 2.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, there are two DataFrames, with season statistics for “both” T1 and T2\.
    Since the final DataFrame will contain the “Season”, the “T1TeamID” and the “T2TeamID”,
    we can join these newly created features with a join!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Elo Ratings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First created by [Arpad Elo](https://en.wikipedia.org/wiki/Elo_rating_system),
    Elo is a rating system for zero-sum games (games where one player wins and the
    other loses), like basketball. With the Elo rating system, each team has an Elo
    rating, a value that generally conveys the team’s quality. At first, every team
    has the same Elo, and whenever they win, their Elo increases, and when they lose,
    their Elo decreases. A key characteristic of this system is that this value increases
    more with a win against a strong opponent than with a win against a weak opponent.
    Thus, it can be a very useful feature to have!
  prefs: []
  type: TYPE_NORMAL
- en: We wanted to capture the Elo rating of a team at the end of the regular season,
    and use that as feature for the tournament. To do this, we calculated the Elo
    for each team on a per match basis. To calculate Elo for this feature, we found
    it more straightforward to use Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Central to Elo is calculating the expected score for each team. It can be described
    in code like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Considering a team A and a team B, this function computes the expected score
    of team A against team B.
  prefs: []
  type: TYPE_NORMAL
- en: For each match, we would update the teams’ Elos. Note that the location of the
    match also played a part — winning at home was considered less impressive than
    winning away.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To apply the Elo rating system, we iterated through each season’s matches, initializing
    teams with a base rating and updating their ratings match by match. The final
    Elo available for each team in each season will, hopefully, be a good descriptor
    of the team’s quality.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Ideally, we wouldn’t calculate Elo changes on a match-by-match basis to determine
    each team’s final Elo for the season. However, we couldn’t come up with a better
    approach. Do you have any ideas? If so, let us know!
  prefs: []
  type: TYPE_NORMAL
- en: Value Added
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The feature engineering steps demonstrated show how we can transform raw data
    — regular season statistics — into valuable information with predictive power.
    It is reasonable to assume that a team’s performance during the regular season
    is indicative of its potential performance in the final tournaments. By calculating
    the mean of observed match-by-match statistics for both the teams and their opponents,
    along with each team’s Elo rating in their final match, we were able to create
    a dataset suitable for modelling. Then, models were trained to predict the outcome
    of tournament matches using these features, among others developed in a similar
    way. With these models, we only need the two team IDs to look up the mean of their
    regular season statistics and their Elos to feed into the model and predict a
    score!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we looked at some of the theory behind Spark and PySpark, how
    that can be applied, and a concrete practical example. We explored how feature
    engineering can be done in the case of sports data, creating regular season statistics
    to use as features for final tournament games. Hopefully you’ve found this interesting
    and helpful — happy feature engineering!
  prefs: []
  type: TYPE_NORMAL
- en: '**The full source code for this post and others in the series can be found**
    [**here**](https://dev.azure.com/nobledynamic/_git/FabricMadness)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://nobledynamic.com*](https://nobledynamic.com/posts/fabric-madness-2/)
    *on April 8, 2024.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Jeff Sonas, Ryan Holbrook, Addison Howard, Anju Kandru. (2024). March Machine
    Learning Mania 2024\. Kaggle. [https://kaggle.com/competitions/march-machine-learning-mania-2024](https://kaggle.com/competitions/march-machine-learning-mania-2024)'
  prefs: []
  type: TYPE_NORMAL
