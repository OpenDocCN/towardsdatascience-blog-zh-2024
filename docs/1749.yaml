- en: Data Curation Practices to Minimize Bias in Medical AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17](https://towardsdatascience.com/data-curation-practices-to-minimize-bias-in-medical-ai-379bf6983de2?source=collection_archive---------8-----------------------#2024-07-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ensuring fair and equitable healthcare outcomes from medical AI applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[![Fima
    Furman](../Images/5d25a93fa0bf4f5ebb2c7a684709635c.png)](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    [Fima Furman](https://medium.com/@fimafurman?source=post_page---byline--379bf6983de2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--379bf6983de2--------------------------------)
    ·8 min read·Jul 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75a75a38b2d02a467ceedd983e778149.png)'
  prefs: []
  type: TYPE_IMG
- en: Potential sources of bias in AI training data. Graphic created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '[AI bias](https://www.ibm.com/topics/ai-bias#:~:text=AI%20bias%2C%20also%20called%20machine,outputs%20and%20potentially%20harmful%20outcomes.)
    refers to discrimination when AI systems produce unequal outcomes for different
    groups due to bias in the training data. When not mitigated, biases in AI and
    machine learning models can systematize and exacerbate discrimination faced by
    historically marginalized groups by embedding discrimination within decision-making
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Issues in training data, such as unrepresentative or imbalanced datasets, historical
    prejudices embedded in the data, and flawed data collection methods, lead to biased
    models. For example, if a [loan decisioning application](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/)
    is trained on historical decisions, but Black loan applicants were systematically
    discriminated against in these historical decisions, then the model will embed
    this discriminatory pattern within its decisioning. Biases can also be introduced
    during the feature selection and engineering phases, where certain attributes
    may inadvertently act as proxies for sensitive characteristics such as race, gender,
    or socioeconomic status. For example, [race and zip code](https://engineering.cmu.edu/news-events/news/2018/12/11-datta-proxies.html)
    are strongly associated in America, so an algorithm trained using zip code data
    will indirectly embed information about race in its decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: AI in medical contexts involves using machine learning models and algorithms
    to aid diagnosis, treatment planning, and patient care. AI bias can be especially
    harmful in these situations, driving significant disparities in healthcare delivery
    and outcomes. For example, a [predictive model for skin cancer](https://healthcare-in-europe.com/en/news/ai-in-skin-cancer-detection-darker-skin-inferior-results.html)
    that has been trained predominantly on images of lighter skin tones may perform
    poorly on patients with darker skin. Such a system might cause misdiagnoses or
    delayed treatment for patients with darker skin, resulting in higher mortality
    rates. Given the high stakes in healthcare applications, data scientists must
    take action to mitigate AI bias in their applications. This article will focus
    on what data curation techniques data scientists can take to remove bias in training
    sets before models are trained.
  prefs: []
  type: TYPE_NORMAL
- en: How is AI Bias Measured?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To mitigate AI bias, it is important to understand how [model bias and fairness
    are defined](https://haas.berkeley.edu/wp-content/uploads/What-is-fairness_-EGAL2.pdf)
    (PDF) and measured. A fair/unbiased model ensures its predictions are equitable
    across different groups. This means that the model’s behavior, such as accuracy
    and selection probability, is comparable across subpopulations defined by sensitive
    features (e.g., race, gender, socioeconomic status).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using quantitative metrics for AI fairness/bias, we can measure and improve
    our own models. These metrics compare accuracy rates and selection probability
    between historically privileged groups and historically non-privileged groups.
    Three commonly used metrics to measure how fairly an AI model treats different
    groups are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical Parity Difference—**Compares the ratio of favorable outcomes
    between groups. This test shows that a model’s predictions are independent of
    sensitive group membership, aiming for equal selection rates across groups. It
    is useful in cases where an equal positive rate between groups is desired, such
    as hiring.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b27f17da60ccb8313d09b5ff2c7335a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Average Odds Difference —** Compares the disparity between false and true
    positive rates across different groups. This metric is stricter than Statistical
    Parity Difference because it seeks to ensure that false and true positive rates
    are equal between groups. It is useful in cases where both positive and negative
    errors are consequential, such as criminal justice.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19f2b6cfa46b7f20330df3ba95fb37e9.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Equal Opportunity Difference** — Compares the true positive rates between
    different groups. It checks that qualified individuals from different groups have
    an equal chance of being selected by an AI system. It does not account for false
    positive rates, potentially leading to disparities in incorrect positive predictions
    across groups.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d4c980dd327776116cc5b8ca233eeb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Data scientists can calculate these fairness/bias metrics on their models using
    a Python library such as Microsoft’s [Fairlearn](https://fairlearn.org/) package
    or IBM’s [AI Fairness 360](https://aif360.res.ibm.com/) Toolkit. For all these
    metrics, a value of zero represents a mathematically fair outcome.
  prefs: []
  type: TYPE_NORMAL
- en: What Data Curation Practices Can Minimize AI Bias?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To mitigate bias in AI training datasets, model builders have an arsenal of
    data curation techniques, which can be divided into quantitative (data transformation
    using mathematical packages) and qualitative (best practices for data collection).
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitative Practices**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[**Remove correlations with sensitive features**](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8c9e4419af21c19d12d483a1b4c4279.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of correlation removal in a sample dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Even if sensitive features (e.g., race, gender) are excluded from model training,
    other features may still be correlated with these sensitive features and introduce
    bias. For example, zip code strongly correlates with race in the United States.
    To ensure these features do not introduce hidden bias, data scientists should
    preprocess their inputs to remove the correlation between other input features
    and sensitive features.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done with [Fairlearn’s CorrelationRemover](https://fairlearn.org/v0.10/user_guide/mitigation/preprocessing.html)
    function. It mathematically transforms feature values to remove correlation while
    preserving most of the features' predictive value. See below for a sample code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[**Use re-weighting and re-sampling to create a balanced sample**](https://link.springer.com/article/10.1007/s10115-011-0463-8)'
  prefs: []
  type: TYPE_NORMAL
- en: Reweighting and resampling are similar processes that create a more balanced
    training dataset to correct for when specific groups are under or overrepresented
    in the input set. Reweighting involves assigning different weights to data samples
    to ensure that underrepresented groups have a proportionate impact on the model’s
    learning process. Resampling involves either oversampling minority class instances
    or undersampling majority class instances to achieve a balanced dataset.
  prefs: []
  type: TYPE_NORMAL
- en: If a sensitive group is underrepresented compared to the general population,
    data scientists can use [AI Fairness’s Reweighing](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/reweighing.py)
    function to transform the data input. See below for sample code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[**Transform feature values using a disparate impact remover**](https://arxiv.org/abs/1412.3756)'
  prefs: []
  type: TYPE_NORMAL
- en: Another technique to remove bias embedded in training data is transforming input
    features with a disparate impact remover. This technique adjusts feature values
    to increase fairness between groups defined by a sensitive feature while preserving
    the rank order of data within groups. This preserves the model's predictive capacity
    while mitigating bias.
  prefs: []
  type: TYPE_NORMAL
- en: To transform features to remove disparate impact, you can use [AI Fairness’s
    Disparate Impact Remover](https://github.com/Trusted-AI/AIF360/blob/main/aif360/algorithms/preprocessing/disparate_impact_remover.py).
    Note that this tool only transforms input data fairness with respect to a single
    protected attribute, so it cannot improve fairness across multiple sensitive features
    or at the intersection of sensitive features. See below for sample code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[**Leverage diverse expert data annotation to minimize labeling bias**](https://arxiv.org/abs/2312.10198)'
  prefs: []
  type: TYPE_NORMAL
- en: For supervised learning use cases, human data labeling of the response variable
    is often necessary. In these cases, imperfect human data labelers introduce their
    personal biases into the dataset, which are then learned by the machine. This
    is exacerbated when small, non-diverse groups of labelers do data annotation.
  prefs: []
  type: TYPE_NORMAL
- en: To minimize bias in the data annotation process, use a high-quality data annotation
    solution that leverages diverse expert opinions, such as [Centaur Labs](https://hubs.li/Q02GXtBT0).
    By algorithmically synthesizing multiple opinions using meritocratic measures
    of label confidence, such solutions mitigate the effect of individual bias and
    [drive huge gains in labeling accuracy](https://hubs.li/Q02GXBxn0) for your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73e3cdf4d69095d0aba30161e1608c6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration of how medical data labeling accuracy can improve by aggregating
    multiple expert opinions. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Practices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Implement inclusive and representative data collection practices**'
  prefs: []
  type: TYPE_NORMAL
- en: Medical AI training data must have sufficient sample sizes across all patient
    demographic groups and conditions to accurately make predictions for diverse groups
    of patients. To ensure datasets meet these needs, application builders should
    engage with relevant medical experts and stakeholders representing the affected
    patient population to define data requirements. Data scientists can use stratified
    sampling to ensure that their training set does not over or underrepresent groups
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists must also ensure that collection techniques do not bias data.
    For example, if medical imaging equipment is inconsistent across different samples,
    this would introduce systematic differences in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ensure data cleaning practices do not introduce bias**'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid creating bias during data cleaning, data scientists must handle missing
    data and impute values carefully. When a dataset has missing values for a sensitive
    feature like patient age, simple strategies such as imputing the mean age could
    skew the data, especially if certain age groups are underrepresented. Instead,
    techniques such as stratified imputation, where missing values are filled based
    on the distribution within relevant subgroups (e.g., imputing within age brackets
    or demographic categories). Advanced methods like [multiple imputation](https://www.publichealth.columbia.edu/research/population-health-methods/missing-data-and-multiple-imputation),
    which generates several plausible values and averages them to account for uncertainty,
    may also be appropriate depending on the situation. After performing data cleaning,
    data scientists should document the imputation process and ensure that the cleaned
    dataset remains representative and unbiased according to predefined standards.
  prefs: []
  type: TYPE_NORMAL
- en: '**Publish curation practices for stakeholder input**'
  prefs: []
  type: TYPE_NORMAL
- en: As data scientists develop their data curation procedure, they should publish
    them for stakeholder input to promote transparency and accountability. When stakeholders
    (e.g., patient group representatives, researchers, and ethicists) review and provide
    feedback on data curation methods, it helps identify and address potential sources
    of bias early in the development process. Furthermore, stakeholder engagement
    fosters trust and confidence in AI systems by demonstrating a commitment to ethical
    and inclusive practices. This trust is essential for driving post-deployment use
    of AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regularly audit and review input data and model performance**'
  prefs: []
  type: TYPE_NORMAL
- en: Regularly auditing and reviewing input data for live models ensures that bias
    does not develop in training sets over time. As medicine, patient demographics,
    and data sources evolve, previously unbiased models can become biased if the input
    data no longer represents the current population accurately. Continuous monitoring
    helps identify and correct any emerging biases, ensuring the model remains fair
    and effective.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4c7e7655123eabedbf45d564a27771ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Planet Volumes](https://unsplash.com/@planetvolumes) on [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists must take measures to minimize bias in their medical AI models
    to achieve equitable patient outcomes, drive stakeholder buy-in, and gain regulatory
    approval. Data scientists can leverage emerging tools from libraries such as [Fairlearn](https://fairlearn.org/)
    (Microsoft) or [AI Fairness 360 Toolkit](https://aif360.res.ibm.com/) (IBM) to
    measure and improve fairness in their AI models. While these tools and quantitative
    measures like Statistical Parity Difference are useful, developers must remember
    to take a holistic approach to fairness. This requires collaboration with experts
    and stakeholders from affected groups to understand patient populations and the
    impact of AI applications. If data scientists adhere to this practice, they will
    usher in a new era of just and superior healthcare for all.
  prefs: []
  type: TYPE_NORMAL
