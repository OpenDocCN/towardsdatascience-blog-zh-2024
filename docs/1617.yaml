- en: An Intuitive Overview of Weak Supervision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弱监督的直观概述
- en: 原文：[https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29](https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29](https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29)
- en: This is probably the solution to your next NLP problem.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这可能是解决你下一个NLP问题的方案。
- en: '[](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    ·8 min read·Jun 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    ·8分钟阅读·2024年6月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this story we introduce and broadly explore the topic of weak supervision
    in machine learning. Weak supervision is one learning paradigm in machine learning
    that started gaining notable attention in recent years. To wrap it up in a nutshell,
    full supervision requires that we have a training set *(x,y)* where *y* is the
    correct label for *x*; meanwhile, weak supervision assumes a general setting *(x,
    y’)* where *y’* does not have to be correct (i.e., it’s potentially incorrect;
    a weak label). Moreover, in weak supervision we can have multiple weak supervisors
    so one can have *(x, y’1,y’2,…,y’F)* for each examplewhere each *y’j* comes from
    a different source and is potentially incorrect.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我们介绍并广泛探讨机器学习中的弱监督主题。弱监督是机器学习中的一种学习范式，近年来开始引起了显著关注。简而言之，完全监督要求我们拥有一个训练集*(x,y)*，其中*y*是*x*的正确标签；而弱监督假设一个一般情境*(x,
    y’)*，其中*y’*不必是正确的（即，它可能是错误的；一个弱标签）。此外，在弱监督中，我们可以有多个弱监督者，因此对于每个示例，可以拥有*(x, y’1,y’2,…,y’F)*，其中每个*y’j*来自不同的来源，并且可能是错误的。
- en: '![](../Images/f305160664c917dc618a23c71681419e.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f305160664c917dc618a23c71681419e.png)'
- en: Giant Wide Featureless Monster Generated by DALLE
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 巨型宽泛无特征怪物，由DALLE生成
- en: Table of Contents
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: ∘ [Problem Statement](#58b3)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [问题陈述](#58b3)
- en: ∘ [General Framework](#3975)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [通用框架](#3975)
- en: ∘ [General Architecture](#fa9d)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [通用架构](#fa9d)
- en: ∘ [Snorkel](#306f)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [Snorkel](#306f)
- en: ∘ [Weak Supervision Example](#1174)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [弱监督示例](#1174)
- en: Problem Statement
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: In more practical terms, weak supervision goes towards solving what I like to
    call the supervised machine learning dilemma. If you are a business or a person
    with a new idea in machine learning you will need data. It’s often not that hard
    to collect many samples *(x1, x2, …, xm)* and sometimes, it can be even done programtically;
    however, the real dilemma is that you will need to hire human annotators to label
    this data and pay some $Z per label. The issue is not just that you may not know
    if the project is worth that much, it’s also that you may not afford hiring annotators
    to begin with as this process can be quite costy especially in fields such as
    law and medicine.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 更实用地说，弱监督旨在解决我所称之为监督式机器学习困境的问题。如果你是一个企业或有一个机器学习新想法的人，你将需要数据。通常，收集大量样本*(x1, x2,
    …, xm)*并不困难，有时甚至可以通过编程完成；然而，真正的困境在于，你需要雇佣人工标注员来标记这些数据，并为每个标签支付一定的$Z费用。问题不仅在于你可能无法确定项目是否值得花这么多钱，而且还在于你可能根本负担不起雇佣标注员的费用，因为这个过程在法律和医学等领域尤其昂贵。
- en: You may be thinking but how does weak supervision solve any of this? In simple
    terms, instead of paying annotators to give you labels, you ask them to give you
    some generic rules that can be sometimes inaccurate in labeling the data (which
    takes far less time and money). In some cases, it may be even trivial for your
    development team to figure out these rules themselves (e.g., if the task doesn’t
    require expert annotators).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，弱监督是如何解决这个问题的？简单来说，与其支付注释员来为你提供标签，不如让他们给你一些通用规则，这些规则在标注数据时有时会不准确（这会节省大量的时间和金钱）。在某些情况下，开发团队甚至可能很轻松地自己搞定这些规则（例如，如果任务不需要专家注释员的话）。
- en: Now let’s think of an example usecase. You are trying to build an NLP system
    that would mask words corresponding to sensitive information such as phone numbers,
    names and addresses. Instead of hiring people to label words in a corpus of sentences
    that you have collected, you write some functions that automatically label all
    the data based on whether the word is all numbers (likely but not certainly a
    phone number), whether the word starts with a capital letter while not in the
    beginning of the sentence (likely but not certainly a name) and etc. then training
    you system on the weakly labeled data. It may cross your mind that the trained
    model won’t be any better than such labeling sources but that’s incorrect; weak
    supervision models are by design meant to generalize beyond the labeling sources
    by knowing that there is uncertainty and often accounting for it in a way or another.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们思考一个具体的应用场景。你正在构建一个自然语言处理系统，用于屏蔽与敏感信息相关的单词，比如电话号码、姓名和地址。你不需要雇佣人员来标注你收集到的句子中的单词，而是编写一些函数，基于一些规则自动标注数据，比如判断单词是否全为数字（很可能是电话号码，但不一定），判断单词是否以大写字母开头而且不是句首（很可能是名字，但不一定），等等。然后，你可以用这些弱标签数据来训练你的系统。你可能会认为训练出的模型不会比这种标注源更好，但这是不正确的；弱监督模型的设计本意就是要超越标注源的局限，通过识别不确定性并以某种方式进行考虑，从而实现更好的泛化能力。
- en: '![](../Images/b927a0b732193a9b6ce78e93eef72030.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b927a0b732193a9b6ce78e93eef72030.png)'
- en: Engineering Planning Paper for Lab Experiment by DALLE
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室实验的工程规划论文，来自DALLE
- en: General Framework
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用框架
- en: Now let’s formally look at the framework of weak supervision as its employed
    in natural language processing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们正式地看一下弱监督在自然语言处理中的应用框架。
- en: ✦ **Given**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ✦ **给定条件**
- en: 'A set of *F* labeling functions *{L1 L2,…,LF}* where *Lj* assigns a weak (i.e.,
    potentially incorrect) label given an input *x* where any labeling function *Lj*
    may be any of:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一组*F*个标注函数 *{L1 L2,…,LF}*，其中*Lj*是一个弱标签函数，在给定输入*x*的情况下，任何标注函数*Lj*都可以是以下任意一种：
- en: Crowdsource annotator (sometimes they are not that accurate)
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 众包注释员（有时他们的准确性较低）
- en: Label due to distance supervision (i.e., extracted from another knowledge base)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于距离监督的标签（即，从另一个知识库中提取的标签）
- en: Weak model (e.g., inherently weak or trained on another task)
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 弱模型（例如，天生较弱或在其他任务上训练的模型）
- en: Heuristic function (e.g., label observation based on the existence of a keyword
    or pattern or defined by domain expert)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启发式函数（例如，根据关键字或模式的存在为观察项标注标签，或由领域专家定义）
- en: Gazetteers (e.g., label observation based on its occurrence in a specific list)
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 地名词典（例如，根据其在特定列表中的出现为观察项标注标签）
- en: LLM Invocation under a specific prompt P (recent work)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定提示P下调用大型语言模型（最近的研究成果）
- en: Any function in general that (preferably) performs better than random guess
    in guessing the label of x.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一般来说，任何能（最好）比随机猜测更好地预测*x*标签的函数。
- en: It’s generally assumed that *Li* may abstain from giving a label (e.g., a heuristic
    function such as *“if the word has numbers then label phone number else don’t
    label”*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常假设*Li*可能会选择不提供标签（例如，像*“如果单词含有数字，则标注为电话号码，否则不标注”*这样的启发式函数）。
- en: Suppose the training set has N examples, then this given is equivalent to an
    (N,F) weak label matrix in the case of sequence classification. For token classification
    with a sequence of length of T, it’s a (N,T,F) matrix of weak labels.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设训练集有N个样本，那么在序列分类的情况下，给定的弱标签矩阵等价于一个(N,F)的矩阵。对于长度为T的序列分类任务，它是一个(N,T,F)的弱标签矩阵。
- en: ✦ **Wanted**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ✦ **期望**
- en: To train a model M that effectively leverages the weakly labeled data along
    with any strong data if it exists.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型M，有效利用弱标签数据，并结合任何强标签数据（如果存在的话）。
- en: ✦ **Common NLP Tasks**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ✦ **常见的自然语言处理任务**
- en: '**Sequence classification** (e.g., sentiment analysis) or **token classification**
    (e.g., named entity recognition) where labeling functions are usually heuristic
    functions or gazetteers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列分类**（例如，情感分析）或 **标记分类**（例如，命名实体识别），其中标签函数通常是启发式函数或地名词典。'
- en: '**Low resource translation** *(x→y)* where labeling function(s) is usually
    a weaker translation model (e.g., a translation model in the reverse direction
    *(y→x)* to add more *(x,y)* translation pairs.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低资源翻译** *(x→y)*，其中标签函数通常是一个较弱的翻译模型（例如，采用反向方向的翻译模型 *(y→x)* 来增加更多的 *(x,y)*
    翻译对）。'
- en: General Architecture
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般架构
- en: 'For sequence or token classification tasks, the most common architecture in
    the literature plausibly takes this form:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于序列或标记分类任务，文献中最常见的架构通常采用以下形式：
- en: '![](../Images/fa685e9599cebd87977696c633a8173c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa685e9599cebd87977696c633a8173c.png)'
- en: 'Figure from Paper WRENCH: A Comprehensive Benchmark for Weak Supervision'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '来自论文的图示 WRENCH: 弱监督的综合基准'
- en: The **label model** learns to map the outputs from the label functions to probabilistic
    or deterministic labels which are used to train the end model. In other words,
    it takes the (N,F) or (N,T,F) label matrix discussed above and returns (N) or
    (N,T) matrix of labels (which are often probabilistic (i.e., soft) labels).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**标签模型**学习将标签函数的输出映射到概率性或确定性的标签，这些标签用于训练最终模型。换句话说，它接收上述讨论的 (N,F) 或 (N,T,F)
    标签矩阵，并返回 (N) 或 (N,T) 的标签矩阵（这些标签通常是概率性的（即软标签））。'
- en: The **end model** is used separately after this step and is just an ordinary
    classifier that operates on soft labels (cross-entropy loss allows that) produced
    by the label model. Some architectures use deep learning to merge label and end
    models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**最终模型**在此步骤后单独使用，仅是一个普通的分类器，操作的是标签模型生成的软标签（交叉熵损失函数能够处理这种情况）。某些架构使用深度学习将标签模型和最终模型合并。'
- en: Notice that once we have trained the label model, we use it to generate the
    labels for the end model and after that we no longer use the label model. In this
    sense, this is quite different from staking even if the label functions are other
    machine learning models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦我们训练了标签模型，我们就用它为最终模型生成标签，之后我们不再使用标签模型。从这个意义上说，这与堆叠模型是相当不同的，即使标签函数本身是其他机器学习模型。
- en: '**Another architecture**, which is the default in the case of translation (and
    less common for sequence/token classification), is to weight the weak examples
    (src, trg) pair based on their quality (usually only one labeling function for
    translation which is a weak model in the reverse direction as discussed earlier).
    Such weight can then be used in the loss function so the model learns more from
    better quality examples and less from lower quality ones. Approaches in this case
    attempt to devise methods to evaluate the quality of a specific example. One approach
    for example uses the roundtrip BLEU score (i.e., translates sentence to target
    then back to source) to estimate such weight.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**另一种架构**，这是翻译任务中的默认架构（对于序列/标记分类任务则不常见），是根据弱示例（源语言、目标语言）对的质量来加权（通常翻译任务只有一个标签函数，即反向模型，正如前面所讨论）。这种权重可以用于损失函数，使得模型从高质量的示例中学习得更多，而从低质量的示例中学习得更少。在这种情况下的方法尝试设计用于评估特定示例质量的方案。例如，一种方法使用回译
    BLEU 分数（即将句子翻译到目标语言，然后再翻译回源语言）来估算这种权重。'
- en: Snorkel
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Snorkel
- en: '![](../Images/208bb18b2ce2972bf52a32f3f4ad074e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/208bb18b2ce2972bf52a32f3f4ad074e.png)'
- en: 'Image From Snorkel: Rapid Training Data Creation with Weak Supervision'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Snorkel的图片：使用弱监督快速创建训练数据
- en: To see an example of how the label model can work, we can look at **Snorkel**
    which is arguably the most fundamental work in weaks supervision for sequence
    classification.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看标签模型如何工作，我们可以看看**Snorkel**，它无疑是序列分类中弱监督的最基础性工作。
- en: '![](../Images/711aff8f795ab3946ec977be1605cd02.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/711aff8f795ab3946ec977be1605cd02.png)'
- en: Equation from the Paper
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 来自论文的方程式
- en: In Snorkel, the authors were interested in finding ***P(yi|Λ(xi))*** where *Λ(xi)*
    is the weak label vector of the ith example. Clearly, once this probability is
    found, we can use it as soft label for the end model (because as we said cross
    entropy loss can handle soft labels). Also clearly, if we have *P(y, Λ(x))* then
    we can easily use to find *P(y|Λ(x))*.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在Snorkel中，作者的目标是找到 ***P(yi|Λ(xi))***，其中 *Λ(xi)* 是第i个示例的弱标签向量。显然，一旦找到了这个概率，我们可以将其作为最终模型的软标签（因为正如我们所说，交叉熵损失可以处理软标签）。同样显而易见的是，如果我们有
    *P(y, Λ(x))*，那么我们可以轻松地用它来求得 *P(y|Λ(x))*。
- en: We see from the equation above that they used the same hypothesis as logistic
    regression to model *P(y, Λ(x))* (Z is for normalization as in Sigmoid/Softmax).
    The difference is that instead of *w.x* we have *w.φ(Λ(xi),yi).* In particular,
    *φ(Λ(xi),yi)* is a vector of dimensionality *2F+|C|. F* is the number of labeling
    functions as mentioned earlier; meanwhile, C is the set of labeling function pairs
    that are correlated (thus, |C| is the number of correlated pairs). Authors refer
    to a method in another paper to automate constructing C which we won’t delve into
    here for brevity.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的方程中我们可以看到，他们使用了与逻辑回归相同的假设来建模 *P(y, Λ(x))*（Z用于归一化，类似于Sigmoid/Softmax）。不同之处在于，代替
    *w.x*，我们有 *w.φ(Λ(xi),yi)*。特别地，*φ(Λ(xi),yi)* 是一个维度为 *2F+|C|* 的向量。F 是之前提到的标注函数的数量；同时，C
    是一组相关的标注函数对（因此，|C| 是相关对的数量）。作者提到在另一篇论文中有一种方法来自动构建 C，这里为了简洁起见不再深入探讨。
- en: 'The vector ***φ(Λ(xi),yi)***contains:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 ***φ(Λ(xi),yi)*** 包含：
- en: '**F** binary elements to specify whether each of the labeling functions has
    abstained for given example'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**个二元元素，用于指定每个标注函数是否在给定示例中放弃'
- en: '**F** binary elements to specify whether each of the labeling functions is
    equal to the true label y (here y will be left as a variable; it’s an input to
    the distribution) given this example'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F**个二元元素，用于指定每个标注函数是否等于真实标签y（此处 y 作为变量，它是分布的输入）'
- en: '**C** binary elements to specify whether each correlated pair made the same
    vote given this example'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C**个二元元素用于指定给定此示例时，每一对相关的元素是否做出了相同的投票'
- en: 'They then train this label models (i.e., estimate the weights vector of length
    *2F+|C|*) by solving the following objective (minimizing negative log marginal
    likelihood):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 他们接着通过解决以下目标来训练这个标签模型（最小化负对数边际似然）：
- en: '![](../Images/6748b0cf02815e9b731a0b3d0eb4c5d1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6748b0cf02815e9b731a0b3d0eb4c5d1.png)'
- en: Equation from the Paper
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 论文中的方程
- en: Notice that they don’t need information about y as this objective is solved
    regardless of any specific value of it as indicated by the sum. If you look closely
    (undo the negative and the log) you may find that this is equivalent to finding
    the weights that maximize the probability for any of the true labels.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，他们不需要关于 y 的信息，因为这个目标是无论 y 的具体值如何都能解决的，如通过求和所示。如果你仔细观察（去掉负号和对数），你会发现这等同于找到那些最大化任何真实标签的概率的权重。
- en: Once the label model is trained, they use it to produce **N** soft labels ***P(y1|Λ(x1)),
    P(y2|Λ(x2)),…,P(yN|Λ(xN))*** and use that to normally train some discriminative
    model (i.e., a classifier).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦标签模型训练完成，他们使用该模型生成**N**个软标签 ***P(y1|Λ(x1)), P(y2|Λ(x2)),…,P(yN|Λ(xN))***，并利用这些标签来训练某些判别模型（即分类器）。
- en: Weak Supervision Example
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弱监督示例
- en: 'Snorkel has an excellent tutorial for spam classification [here](https://www.snorkel.org/use-cases/01-spam-tutorial).
    Skweak is another package (and paper) that is fundamental for weak supervision
    for token classification. This is an example on how to get started with Skweak
    as shown on [their Github](https://github.com/NorskRegnesentral/skweak):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Snorkel 提供了一个出色的垃圾邮件分类教程[在这里](https://www.snorkel.org/use-cases/01-spam-tutorial)。Skweak
    是另一个基本的弱监督标注包（以及相关论文），用于标记分类。下面是如何开始使用 Skweak 的示例，如在[他们的 GitHub](https://github.com/NorskRegnesentral/skweak)上所示：
- en: 'First define labeling functions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义标注函数：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Apply them on the corpus
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们应用于语料库
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Create and fit the label model
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并拟合标签模型
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then you can of course train a classifier on top of this on using the estimated
    soft labels.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你当然可以在此基础上使用估算的软标签训练分类器。
- en: In this article, we explored the problem addressed by weak supervision, provided
    a formal definition, and outlined the general architecture typically employed
    in this context. We also delved into Snorkel, one of the foundational models in
    weak supervision, and concluded with a practical example to illustrate how weak
    supervision can be applied.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了弱监督所解决的问题，提供了正式的定义，并概述了在这种背景下通常采用的一般架构。我们还深入研究了 Snorkel，这是弱监督中的基础模型之一，并以一个实际示例结束，说明了弱监督如何应用。
- en: '![](../Images/1e509242bf273839d2e1b1ac5652726d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e509242bf273839d2e1b1ac5652726d.png)'
- en: Jeep Going Away Bye by DALLE
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DALLE 创作的“Jeep Going Away Bye”
- en: Hope you found the article to be useful. Until next time, au revoir.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你觉得这篇文章有用。下次再见，au revoir。
- en: '**References**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Zhang, J. *et al.* (2021) *Wrench: A comprehensive benchmark for weak supervision*,
    *arXiv.org*. Available at: [https://arxiv.org/abs/2109.11377](https://arxiv.org/abs/2109.11377)
    .'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Zhang, J. *等* (2021) *Wrench: 一个全面的弱监督基准测试*, *arXiv.org*。可在以下网址获取：[https://arxiv.org/abs/2109.11377](https://arxiv.org/abs/2109.11377)。'
- en: '[2] Ratner, A. *et al.* (2017) *Snorkel: Rapid Training Data Creation with
    weak supervision*, *arXiv.org*. Available at: [https://arxiv.org/abs/1711.10160](https://arxiv.org/abs/1711.10160).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Ratner, A. *等* (2017) *Snorkel: 使用弱监督快速创建训练数据*, *arXiv.org*。可在以下网址获取：[https://arxiv.org/abs/1711.10160](https://arxiv.org/abs/1711.10160)。'
- en: '[3] NorskRegnesentral (2021) *NorskRegnesentral/skweak: Skweak: A software
    toolkit for weak supervision applied to NLP tasks*, *GitHub*. Available at: [https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] NorskRegnesentral (2021) *NorskRegnesentral/skweak: Skweak: 一个应用于NLP任务的弱监督软件工具包*,
    *GitHub*。可在以下网址获取：[https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak)。'
