- en: An Intuitive Overview of Weak Supervision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29](https://towardsdatascience.com/an-intuitive-overview-of-weak-supervision-215ab3db1591?source=collection_archive---------2-----------------------#2024-06-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is probably the solution to your next NLP problem.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--215ab3db1591--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--215ab3db1591--------------------------------)
    ·8 min read·Jun 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this story we introduce and broadly explore the topic of weak supervision
    in machine learning. Weak supervision is one learning paradigm in machine learning
    that started gaining notable attention in recent years. To wrap it up in a nutshell,
    full supervision requires that we have a training set *(x,y)* where *y* is the
    correct label for *x*; meanwhile, weak supervision assumes a general setting *(x,
    y’)* where *y’* does not have to be correct (i.e., it’s potentially incorrect;
    a weak label). Moreover, in weak supervision we can have multiple weak supervisors
    so one can have *(x, y’1,y’2,…,y’F)* for each examplewhere each *y’j* comes from
    a different source and is potentially incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f305160664c917dc618a23c71681419e.png)'
  prefs: []
  type: TYPE_IMG
- en: Giant Wide Featureless Monster Generated by DALLE
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ∘ [Problem Statement](#58b3)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [General Framework](#3975)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [General Architecture](#fa9d)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Snorkel](#306f)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Weak Supervision Example](#1174)
  prefs: []
  type: TYPE_NORMAL
- en: Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In more practical terms, weak supervision goes towards solving what I like to
    call the supervised machine learning dilemma. If you are a business or a person
    with a new idea in machine learning you will need data. It’s often not that hard
    to collect many samples *(x1, x2, …, xm)* and sometimes, it can be even done programtically;
    however, the real dilemma is that you will need to hire human annotators to label
    this data and pay some $Z per label. The issue is not just that you may not know
    if the project is worth that much, it’s also that you may not afford hiring annotators
    to begin with as this process can be quite costy especially in fields such as
    law and medicine.
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking but how does weak supervision solve any of this? In simple
    terms, instead of paying annotators to give you labels, you ask them to give you
    some generic rules that can be sometimes inaccurate in labeling the data (which
    takes far less time and money). In some cases, it may be even trivial for your
    development team to figure out these rules themselves (e.g., if the task doesn’t
    require expert annotators).
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s think of an example usecase. You are trying to build an NLP system
    that would mask words corresponding to sensitive information such as phone numbers,
    names and addresses. Instead of hiring people to label words in a corpus of sentences
    that you have collected, you write some functions that automatically label all
    the data based on whether the word is all numbers (likely but not certainly a
    phone number), whether the word starts with a capital letter while not in the
    beginning of the sentence (likely but not certainly a name) and etc. then training
    you system on the weakly labeled data. It may cross your mind that the trained
    model won’t be any better than such labeling sources but that’s incorrect; weak
    supervision models are by design meant to generalize beyond the labeling sources
    by knowing that there is uncertainty and often accounting for it in a way or another.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b927a0b732193a9b6ce78e93eef72030.png)'
  prefs: []
  type: TYPE_IMG
- en: Engineering Planning Paper for Lab Experiment by DALLE
  prefs: []
  type: TYPE_NORMAL
- en: General Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s formally look at the framework of weak supervision as its employed
    in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: ✦ **Given**
  prefs: []
  type: TYPE_NORMAL
- en: 'A set of *F* labeling functions *{L1 L2,…,LF}* where *Lj* assigns a weak (i.e.,
    potentially incorrect) label given an input *x* where any labeling function *Lj*
    may be any of:'
  prefs: []
  type: TYPE_NORMAL
- en: Crowdsource annotator (sometimes they are not that accurate)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Label due to distance supervision (i.e., extracted from another knowledge base)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Weak model (e.g., inherently weak or trained on another task)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Heuristic function (e.g., label observation based on the existence of a keyword
    or pattern or defined by domain expert)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gazetteers (e.g., label observation based on its occurrence in a specific list)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LLM Invocation under a specific prompt P (recent work)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any function in general that (preferably) performs better than random guess
    in guessing the label of x.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s generally assumed that *Li* may abstain from giving a label (e.g., a heuristic
    function such as *“if the word has numbers then label phone number else don’t
    label”*).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the training set has N examples, then this given is equivalent to an
    (N,F) weak label matrix in the case of sequence classification. For token classification
    with a sequence of length of T, it’s a (N,T,F) matrix of weak labels.
  prefs: []
  type: TYPE_NORMAL
- en: ✦ **Wanted**
  prefs: []
  type: TYPE_NORMAL
- en: To train a model M that effectively leverages the weakly labeled data along
    with any strong data if it exists.
  prefs: []
  type: TYPE_NORMAL
- en: ✦ **Common NLP Tasks**
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence classification** (e.g., sentiment analysis) or **token classification**
    (e.g., named entity recognition) where labeling functions are usually heuristic
    functions or gazetteers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low resource translation** *(x→y)* where labeling function(s) is usually
    a weaker translation model (e.g., a translation model in the reverse direction
    *(y→x)* to add more *(x,y)* translation pairs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For sequence or token classification tasks, the most common architecture in
    the literature plausibly takes this form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa685e9599cebd87977696c633a8173c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure from Paper WRENCH: A Comprehensive Benchmark for Weak Supervision'
  prefs: []
  type: TYPE_NORMAL
- en: The **label model** learns to map the outputs from the label functions to probabilistic
    or deterministic labels which are used to train the end model. In other words,
    it takes the (N,F) or (N,T,F) label matrix discussed above and returns (N) or
    (N,T) matrix of labels (which are often probabilistic (i.e., soft) labels).
  prefs: []
  type: TYPE_NORMAL
- en: The **end model** is used separately after this step and is just an ordinary
    classifier that operates on soft labels (cross-entropy loss allows that) produced
    by the label model. Some architectures use deep learning to merge label and end
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that once we have trained the label model, we use it to generate the
    labels for the end model and after that we no longer use the label model. In this
    sense, this is quite different from staking even if the label functions are other
    machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Another architecture**, which is the default in the case of translation (and
    less common for sequence/token classification), is to weight the weak examples
    (src, trg) pair based on their quality (usually only one labeling function for
    translation which is a weak model in the reverse direction as discussed earlier).
    Such weight can then be used in the loss function so the model learns more from
    better quality examples and less from lower quality ones. Approaches in this case
    attempt to devise methods to evaluate the quality of a specific example. One approach
    for example uses the roundtrip BLEU score (i.e., translates sentence to target
    then back to source) to estimate such weight.'
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/208bb18b2ce2972bf52a32f3f4ad074e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image From Snorkel: Rapid Training Data Creation with Weak Supervision'
  prefs: []
  type: TYPE_NORMAL
- en: To see an example of how the label model can work, we can look at **Snorkel**
    which is arguably the most fundamental work in weaks supervision for sequence
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/711aff8f795ab3946ec977be1605cd02.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation from the Paper
  prefs: []
  type: TYPE_NORMAL
- en: In Snorkel, the authors were interested in finding ***P(yi|Λ(xi))*** where *Λ(xi)*
    is the weak label vector of the ith example. Clearly, once this probability is
    found, we can use it as soft label for the end model (because as we said cross
    entropy loss can handle soft labels). Also clearly, if we have *P(y, Λ(x))* then
    we can easily use to find *P(y|Λ(x))*.
  prefs: []
  type: TYPE_NORMAL
- en: We see from the equation above that they used the same hypothesis as logistic
    regression to model *P(y, Λ(x))* (Z is for normalization as in Sigmoid/Softmax).
    The difference is that instead of *w.x* we have *w.φ(Λ(xi),yi).* In particular,
    *φ(Λ(xi),yi)* is a vector of dimensionality *2F+|C|. F* is the number of labeling
    functions as mentioned earlier; meanwhile, C is the set of labeling function pairs
    that are correlated (thus, |C| is the number of correlated pairs). Authors refer
    to a method in another paper to automate constructing C which we won’t delve into
    here for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector ***φ(Λ(xi),yi)***contains:'
  prefs: []
  type: TYPE_NORMAL
- en: '**F** binary elements to specify whether each of the labeling functions has
    abstained for given example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F** binary elements to specify whether each of the labeling functions is
    equal to the true label y (here y will be left as a variable; it’s an input to
    the distribution) given this example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**C** binary elements to specify whether each correlated pair made the same
    vote given this example'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They then train this label models (i.e., estimate the weights vector of length
    *2F+|C|*) by solving the following objective (minimizing negative log marginal
    likelihood):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6748b0cf02815e9b731a0b3d0eb4c5d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation from the Paper
  prefs: []
  type: TYPE_NORMAL
- en: Notice that they don’t need information about y as this objective is solved
    regardless of any specific value of it as indicated by the sum. If you look closely
    (undo the negative and the log) you may find that this is equivalent to finding
    the weights that maximize the probability for any of the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: Once the label model is trained, they use it to produce **N** soft labels ***P(y1|Λ(x1)),
    P(y2|Λ(x2)),…,P(yN|Λ(xN))*** and use that to normally train some discriminative
    model (i.e., a classifier).
  prefs: []
  type: TYPE_NORMAL
- en: Weak Supervision Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Snorkel has an excellent tutorial for spam classification [here](https://www.snorkel.org/use-cases/01-spam-tutorial).
    Skweak is another package (and paper) that is fundamental for weak supervision
    for token classification. This is an example on how to get started with Skweak
    as shown on [their Github](https://github.com/NorskRegnesentral/skweak):'
  prefs: []
  type: TYPE_NORMAL
- en: 'First define labeling functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Apply them on the corpus
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Create and fit the label model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Then you can of course train a classifier on top of this on using the estimated
    soft labels.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explored the problem addressed by weak supervision, provided
    a formal definition, and outlined the general architecture typically employed
    in this context. We also delved into Snorkel, one of the foundational models in
    weak supervision, and concluded with a practical example to illustrate how weak
    supervision can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e509242bf273839d2e1b1ac5652726d.png)'
  prefs: []
  type: TYPE_IMG
- en: Jeep Going Away Bye by DALLE
  prefs: []
  type: TYPE_NORMAL
- en: Hope you found the article to be useful. Until next time, au revoir.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Zhang, J. *et al.* (2021) *Wrench: A comprehensive benchmark for weak supervision*,
    *arXiv.org*. Available at: [https://arxiv.org/abs/2109.11377](https://arxiv.org/abs/2109.11377)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Ratner, A. *et al.* (2017) *Snorkel: Rapid Training Data Creation with
    weak supervision*, *arXiv.org*. Available at: [https://arxiv.org/abs/1711.10160](https://arxiv.org/abs/1711.10160).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] NorskRegnesentral (2021) *NorskRegnesentral/skweak: Skweak: A software
    toolkit for weak supervision applied to NLP tasks*, *GitHub*. Available at: [https://github.com/NorskRegnesentral/skweak](https://github.com/NorskRegnesentral/skweak).'
  prefs: []
  type: TYPE_NORMAL
