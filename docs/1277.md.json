["```py\n#%%writefile kernels.py\nimport numba\nfrom numba import cuda\n\nTHREADS_PER_BLOCK = 256\nBLOCKS_PER_GRID = 32 * 40\n\n@cuda.jit\ndef partial_reduce(array, partial_reduction):\n    i_start = cuda.grid(1)\n    threads_per_grid = cuda.blockDim.x * cuda.gridDim.x\n    s_thread = numba.float32(0.0)\n    for i_arr in range(i_start, array.size, threads_per_grid):\n        s_thread += array[i_arr]\n\n    s_block = cuda.shared.array((THREADS_PER_BLOCK,), numba.float32)\n    tid = cuda.threadIdx.x\n    s_block[tid] = s_thread\n    cuda.syncthreads()\n\n    i = cuda.blockDim.x // 2\n    while i > 0:\n        if tid < i:\n            s_block[tid] += s_block[tid + i]\n        cuda.syncthreads()\n        i //= 2\n\n    if tid == 0:\n        partial_reduction[cuda.blockIdx.x] = s_block[0]\n\n@cuda.jit\ndef single_thread_sum(partial_reduction, sum):\n    sum[0] = numba.float32(0.0)\n    for element in partial_reduction:\n        sum[0] += element\n\n@cuda.jit\ndef divide_by(array, val_array):\n    i_start = cuda.grid(1)\n    threads_per_grid = cuda.gridsize(1)\n    for i in range(i_start, array.size, threads_per_grid):\n        array[i] /= val_array[0]\n```", "```py\n#%%writefile run_v1.py\nimport argparse\nimport warnings\n\nimport numpy as np\nfrom numba import cuda\nfrom numba.core.errors import NumbaPerformanceWarning\n\nfrom kernels import (\n    BLOCKS_PER_GRID,\n    THREADS_PER_BLOCK,\n    divide_by,\n    partial_reduce,\n    single_thread_sum,\n)\n\n# Ignore NumbaPerformanceWarning\nwarnings.simplefilter(\"ignore\", category=NumbaPerformanceWarning)\n\ndef run(size):\n    # Define host array\n    a = np.ones(size, dtype=np.float32)\n    print(f\"Old sum: {a.sum():.3f}\")\n\n    # Array copy to device and array creation on the device.\n    dev_a = cuda.to_device(a)\n    dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)\n    dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)\n\n    # Launching kernels to normalize array\n    partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)\n    single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)\n    divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)\n\n    # Array copy to host\n    dev_a.copy_to_host(a)\n    cuda.synchronize()\n    print(f\"New sum: {a.sum():.3f}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Simple Example v1\")\n    parser.add_argument(\n        \"-n\",\n        \"--array-size\",\n        type=int,\n        default=100_000_000,\n        metavar=\"N\",\n        help=\"Array size\",\n    )\n\n    args = parser.parse_args()\n    run(size=args.array_size)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\n$ python run_v1.py\nOld sum: 100000000.000\nNew sum: 1.000\n```", "```py\n$ nsys profile \\\n  --trace cuda,osrt,nvtx \\\n  --gpu-metrics-device=all \\\n  --cuda-memory-usage true \\\n  --force-overwrite true \\\n  --output profile_run_v1 \\\n  python run_v1.py\nGPU 0: General Metrics for NVIDIA TU10x (any frequency)\nOld sum: 100000000.000\nNew sum: 1.000\nGenerating '/tmp/nsys-report-fb78.qdstrm'\n[1/1] [========================100%] profile_run_v1.nsys-rep\nGenerated:\n    /content/profile_run_v1.nsys-rep\n```", "```py\na = np.ones(size, dtype=np.float32)\n```", "```py\na = cuda.pinned_array(size, dtype=np.float32)\na[...] = 1.0\n```", "```py\n#%%writefile run_v2.py\nimport argparse\nimport warnings\n\nimport numpy as np\nimport nvtx\nfrom numba import cuda\nfrom numba.core.errors import NumbaPerformanceWarning\n\nfrom kernels import (\n    BLOCKS_PER_GRID,\n    THREADS_PER_BLOCK,\n    divide_by,\n    partial_reduce,\n    single_thread_sum,\n)\n\n# Ignore NumbaPerformanceWarning\nwarnings.simplefilter(\"ignore\", category=NumbaPerformanceWarning)\n\ndef run(size):\n    with nvtx.annotate(\"Compilation\", color=\"red\"):\n        dev_a = cuda.device_array((BLOCKS_PER_GRID,), dtype=np.float32)\n        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)\n        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)\n        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)\n        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)\n        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)\n\n    # Define host array\n    a = cuda.pinned_array(size, dtype=np.float32)\n    a[...]  = 1.0\n    print(f\"Old sum: {a.sum():.3f}\")\n\n    # Array copy to device and array creation on the device.\n    with nvtx.annotate(\"H2D Memory\", color=\"yellow\"):\n        dev_a = cuda.to_device(a)\n        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)\n        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)\n\n    # Launching kernels to normalize array\n    with nvtx.annotate(\"Kernels\", color=\"green\"):\n        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)\n        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)\n        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)\n\n    # Array copy to host\n    with nvtx.annotate(\"D2H Memory\", color=\"orange\"):\n        dev_a.copy_to_host(a)\n    cuda.synchronize()\n    print(f\"New sum: {a.sum():.3f}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Simple Example v2\")\n    parser.add_argument(\n        \"-n\",\n        \"--array-size\",\n        type=int,\n        default=100_000_000,\n        metavar=\"N\",\n        help=\"Array size\",\n    )\n\n    args = parser.parse_args()\n    run(size=args.array_size)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nwith nvtx.annotate(\"Region Title\", color=\"red\"):\n    ...\n```", "```py\n$ nsys profile \\\n  --trace cuda,osrt,nvtx \\\n  --gpu-metrics-device=all \\\n  --cuda-memory-usage true \\\n  --force-overwrite true \\\n  --output profile_run_v2 \\\n  python run_v2.py\nGPU 0: General Metrics for NVIDIA TU10x (any frequency)\nOld sum: 100000000.000\nNew sum: 1.000\nGenerating '/tmp/nsys-report-69ab.qdstrm'\n[1/1] [========================100%] profile_run_v2.nsys-rep\nGenerated:\n    /content/profile_run_v2.nsys-rep\n```", "```py\n#%%writefile run_v3_bug.py\nimport argparse\nimport warnings\nfrom math import ceil\n\nimport numpy as np\nimport nvtx\nfrom numba import cuda\nfrom numba.core.errors import NumbaPerformanceWarning\n\nfrom kernels import (\n    BLOCKS_PER_GRID,\n    THREADS_PER_BLOCK,\n    divide_by,\n    partial_reduce,\n    single_thread_sum,\n)\n\n# Ignore NumbaPerformanceWarning\nwarnings.simplefilter(\"ignore\", category=NumbaPerformanceWarning)\n\ndef run(size, nstreams):\n    with nvtx.annotate(\"Compilation\", color=\"red\"):\n        dev_a = cuda.device_array((BLOCKS_PER_GRID,), dtype=np.float32)\n        dev_a_reduce = cuda.device_array((BLOCKS_PER_GRID,), dtype=dev_a.dtype)\n        dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype)\n        partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_reduce)\n        single_thread_sum[1, 1](dev_a_reduce, dev_a_sum)\n        divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK](dev_a, dev_a_sum)\n\n    # Define host array\n    a = cuda.pinned_array(size, dtype=np.float32)\n    a[...] = 1.0\n\n    # Define regions for streams\n    step = ceil(size / nstreams)\n    starts = [i * step for i in range(nstreams)]\n    ends = [min(s + step, size) for s in starts]\n    print(f\"Old sum: {a.sum():.3f}\")\n\n    # Create streams\n    streams = [cuda.stream()] * nstreams\n\n    cpu_sums = [cuda.pinned_array(1, dtype=np.float32) for _ in range(nstreams)]\n    devs_a = []\n    with cuda.defer_cleanup():\n        for i, (stream, start, end) in enumerate(zip(streams, starts, ends)):\n            cpu_sums[i][...] = np.nan\n\n            # Array copy to device and array creation on the device.\n            with nvtx.annotate(f\"H2D Memory Stream {i}\", color=\"yellow\"):\n                dev_a = cuda.to_device(a[start:end], stream=stream)\n                dev_a_reduce = cuda.device_array(\n                    (BLOCKS_PER_GRID,), dtype=dev_a.dtype, stream=stream\n                )\n                dev_a_sum = cuda.device_array((1,), dtype=dev_a.dtype, stream=stream)\n            devs_a.append(dev_a)\n\n            # Launching kernels to sum array\n            with nvtx.annotate(f\"Sum Kernels Stream {i}\", color=\"green\"):\n                for _ in range(50):  # Make it spend more time in compute\n                    partial_reduce[BLOCKS_PER_GRID, THREADS_PER_BLOCK, stream](\n                    dev_a, dev_a_reduce\n                )\n                    single_thread_sum[1, 1, stream](dev_a_reduce, dev_a_sum)\n            with nvtx.annotate(f\"D2H Memory Stream {i}\", color=\"orange\"):\n                dev_a_sum.copy_to_host(cpu_sums[i], stream=stream)\n\n        # Ensure all streams are caught up\n        cuda.synchronize()\n\n        # Aggregate all 1D arrays into a single 1D array\n        a_sum_all = sum(cpu_sums)\n\n        # Send it to the GPU\n        with cuda.pinned(a_sum_all):\n            with nvtx.annotate(\"D2H Memory Default Stream\", color=\"orange\"):\n                dev_a_sum_all = cuda.to_device(a_sum_all)\n\n        # Normalize via streams\n        for i, (stream, start, end, dev_a) in enumerate(\n            zip(streams, starts, ends, devs_a)\n        ):\n            with nvtx.annotate(f\"Divide Kernel Stream {i}\", color=\"green\"):\n                divide_by[BLOCKS_PER_GRID, THREADS_PER_BLOCK, stream](\n                    dev_a, dev_a_sum_all\n                )\n\n            # Array copy to host\n            with nvtx.annotate(f\"D2H Memory Stream {i}\", color=\"orange\"):\n                dev_a.copy_to_host(a[start:end], stream=stream)\n\n        cuda.synchronize()\n        print(f\"New sum: {a.sum():.3f}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Simple Example v3\")\n    parser.add_argument(\n        \"-n\",\n        \"--array-size\",\n        type=int,\n        default=100_000_000,\n        metavar=\"N\",\n        help=\"Array size\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--streams\",\n        type=int,\n        default=4,\n        metavar=\"N\",\n        help=\"Array size\",\n    )\n\n    args = parser.parse_args()\n    run(size=args.array_size, nstreams=args.streams)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\n$ nsys profile \\\n  --trace cuda,osrt,nvtx \\\n  --gpu-metrics-device=all \\\n  --cuda-memory-usage true \\\n  --force-overwrite true \\\n  --output profile_run_v3_bug_4streams \\\n  python run_v3_bug.py -s 4\nGPU 0: General Metrics for NVIDIA TU10x (any frequency)\nOld sum: 100000000.000\nNew sum: 1.000\nGenerating '/tmp/nsys-report-a666.qdstrm'\n[1/1] [========================100%] profile_run_v3_bug_4streams.nsys-rep\nGenerated:\n    /content/profile_run_v3_bug_4streams.nsys-rep\n```", "```py\nstreams = [cuda.stream()] * nstreams\n```", "```py\nstreams = [cuda.stream() for _ in range(nstreams)]\n# Ensure they are all different\nassert all(s1.handle != s2.handle for s1, s2 in zip(streams[:-1], streams[1:]))\n```", "```py\n$  nsys profile \\\n  --trace cuda,osrt,nvtx \\\n  --gpu-metrics-device=all \\\n  --cuda-memory-usage true \\\n  --force-overwrite true \\\n  --output profile_run_v3_1stream \\\n  python run_v3.py -s 1\nGPU 0: General Metrics for NVIDIA TU10x (any frequency)\nOld sum: 100000000.000\nNew sum: 1.000\nGenerating '/tmp/nsys-report-de65.qdstrm'\n[1/1] [========================100%] profile_run_v3_1stream.nsys-rep\nGenerated:\n    /content/profile_run_v3_1stream.nsys-rep\n```", "```py\n$ nsys profile \\\n  --trace cuda,osrt,nvtx \\\n  --gpu-metrics-device=all \\\n  --cuda-memory-usage true \\\n  --force-overwrite true \\\n  --output profile_run_v3_8streams \\\n  python run_v3.py -s 8\nGPU 0: General Metrics for NVIDIA TU10x (any frequency)\nOld sum: 100000000.000\nNew sum: 1.000\nGenerating '/tmp/nsys-report-1fb7.qdstrm'\n[1/1] [========================100%] profile_run_v3_8streams.nsys-rep\nGenerated:\n    /content/profile_run_v3_8streams.nsys-rep\n```"]