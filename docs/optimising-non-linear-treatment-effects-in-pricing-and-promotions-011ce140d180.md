# ä¼˜åŒ–å®šä»·å’Œä¿ƒé”€ä¸­çš„éçº¿æ€§å¤„ç†æ•ˆåº”

> åŸæ–‡ï¼š[`towardsdatascience.com/optimising-non-linear-treatment-effects-in-pricing-and-promotions-011ce140d180?source=collection_archive---------2-----------------------#2024-05-24`](https://towardsdatascience.com/optimising-non-linear-treatment-effects-in-pricing-and-promotions-011ce140d180?source=collection_archive---------2-----------------------#2024-05-24)

## å› æœ AIï¼Œæ¢ç´¢å› æœæ¨ç†ä¸æœºå™¨å­¦ä¹ çš„ç»“åˆ

[](https://medium.com/@raz1470?source=post_page---byline--011ce140d180--------------------------------)![Ryan O'Sullivan](https://medium.com/@raz1470?source=post_page---byline--011ce140d180--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--011ce140d180--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--011ce140d180--------------------------------) [Ryan O'Sullivan](https://medium.com/@raz1470?source=post_page---byline--011ce140d180--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--011ce140d180--------------------------------) Â·12 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 5 æœˆ 24 æ—¥

--

![](img/10380a5d9fbe3aded646019e930eb6a1.png)

ç”±[Ernest Ojeh](https://unsplash.com/@namzo?utm_source=medium&utm_medium=referral)æ‹æ‘„ï¼Œ[å›¾ç‰‡æ¥æºäº Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

# è¿™ç³»åˆ—æ–‡ç« è®²äº†ä»€ä¹ˆï¼Ÿ

æ¬¢è¿æ¥åˆ°æˆ‘çš„å› æœ AI ç³»åˆ—æ–‡ç« ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å°†æ¢ç´¢å› æœæ¨ç†ä¸æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»“åˆã€‚ä½ å°†çœ‹åˆ°å¤šä¸ªä¸åŒä¸šåŠ¡æƒ…å¢ƒä¸‹çš„å®é™…åº”ç”¨ã€‚

åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†*ä½¿ç”¨åŒé‡æœºå™¨å­¦ä¹ å’Œçº¿æ€§è§„åˆ’æ¥ä¼˜åŒ–å¤„ç†ç­–ç•¥*ã€‚è¿™æ¬¡æˆ‘ä»¬å°†ç»§ç»­ä¼˜åŒ–çš„ä¸»é¢˜ï¼Œæ¢è®¨*ä¼˜åŒ–å®šä»·ä¸ä¿ƒé”€ä¸­çš„éçº¿æ€§å¤„ç†æ•ˆåº”*ã€‚

å¦‚æœä½ é”™è¿‡äº†ä¸Šä¸€ç¯‡å…³äºåŒé‡æœºå™¨å­¦ä¹ å’Œçº¿æ€§è§„åˆ’çš„æ–‡ç« ï¼Œå¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ï¼š

[](/using-double-machine-learning-and-linear-programming-to-optimise-treatment-strategies-920c20a29553?source=post_page-----011ce140d180--------------------------------) ## ä½¿ç”¨åŒé‡æœºå™¨å­¦ä¹ å’Œçº¿æ€§è§„åˆ’ä¼˜åŒ–å¤„ç†ç­–ç•¥

### å› æœ AIï¼Œæ¢ç´¢å› æœæ¨ç†ä¸æœºå™¨å­¦ä¹ çš„ç»“åˆ

towardsdatascience.com

# ä»‹ç»

æœ¬æ–‡å°†å±•ç¤ºæˆ‘ä»¬å¦‚ä½•ä¼˜åŒ–å®šä»·ä¸­çš„éçº¿æ€§å¤„ç†æ•ˆåº”ï¼ˆä½†è¿™äº›ç†å¿µä¹Ÿå¯ä»¥åº”ç”¨äºå¸‚åœºè¥é”€å’Œå…¶ä»–é¢†åŸŸï¼‰ã€‚

**åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å¸®åŠ©ä½ ç†è§£ï¼š**

+   ä¸ºä»€ä¹ˆåœ¨å®šä»·ä¸­éçº¿æ€§å¤„ç†æ•ˆåº”å¦‚æ­¤å¸¸è§ï¼Ÿ

+   æˆ‘ä»¬çš„å› æœäººå·¥æ™ºèƒ½å·¥å…·ç®±ä¸­æœ‰å“ªäº›å·¥å…·é€‚ç”¨äºä¼°ç®—éçº¿æ€§å¤„ç†æ•ˆåº”ï¼Ÿ

+   éçº¿æ€§ç¼–ç¨‹å¦‚ä½•ç”¨äºä¼˜åŒ–å®šä»·ï¼Ÿ

+   ä¸€ä¸ªä½¿ç”¨ Python çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œå±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•ç»“åˆå› æœäººå·¥æ™ºèƒ½å·¥å…·ç®±å’Œéçº¿æ€§ç¼–ç¨‹æ¥ä¼˜åŒ–å®šä»·é¢„ç®—ã€‚

å®Œæ•´çš„ç¬”è®°æœ¬å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼š

[](https://github.com/raz1470/causal_ai/blob/main/notebooks/using%20dml%20and%20lp%20to%20optimise%20treatment%20strategies.ipynb?source=post_page-----011ce140d180--------------------------------) [## causal_ai/notebooks/using dml and lp to optimise treatment strategies.ipynb at main Â·â€¦

### æœ¬é¡¹ç›®ä»‹ç»äº†å› æœäººå·¥æ™ºèƒ½ï¼ˆCausal AIï¼‰åŠå…¶å¦‚ä½•æ¨åŠ¨ä¸šåŠ¡ä»·å€¼ã€‚- causal_ai/notebooks/using dml and lp toâ€¦

github.com](https://github.com/raz1470/causal_ai/blob/main/notebooks/using%20dml%20and%20lp%20to%20optimise%20treatment%20strategies.ipynb?source=post_page-----011ce140d180--------------------------------)

# ä¸ºä»€ä¹ˆåœ¨å®šä»·ä¸­éçº¿æ€§å¤„ç†æ•ˆåº”å¦‚æ­¤å¸¸è§ï¼Ÿ

## é€’å‡æ”¶ç›Š

è®©æˆ‘ä»¬ä»¥é›¶å”®å•†è°ƒæ•´äº§å“ä»·æ ¼ä¸ºä¾‹ã€‚æœ€åˆï¼Œé™ä½ä»·æ ¼å¯èƒ½ä¼šå¯¼è‡´é”€å”®é‡æ˜¾è‘—å¢åŠ ã€‚ç„¶è€Œï¼Œéšç€ä»·æ ¼ç»§ç»­é™ä½ï¼Œé”€å”®çš„å¢é•¿å¯èƒ½ä¼šå¼€å§‹è¶‹äºå¹³ç¨³ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºé€’å‡æ”¶ç›Šã€‚å¦‚ä¸‹é¢æ‰€ç¤ºï¼Œé€’å‡æ”¶ç›Šçš„æ•ˆæœé€šå¸¸æ˜¯éçº¿æ€§çš„ã€‚

![](img/00d137dc0252c7fe3a474395358fd7c8.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾ç‰‡

é€’å‡æ”¶ç›Šå¯ä»¥åœ¨å®šä»·ä¹‹å¤–çš„å¤šä¸ªé¢†åŸŸè§‚å¯Ÿåˆ°ã€‚ä¸€äº›å¸¸è§çš„ä¾‹å­åŒ…æ‹¬ï¼š

+   è¥é”€ â€” å¢åŠ ç¤¾äº¤åª’ä½“æŠ•å…¥å¯ä»¥æé«˜å®¢æˆ·è·å–ï¼Œä½†éšç€æ—¶é—´çš„æ¨ç§»ï¼Œç„å‡†æ–°çš„ã€æœªå¼€å‘çš„å—ä¼—ä¼šå˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚

+   å†œä¸š â€” å‘ç”°åœ°æ·»åŠ è‚¥æ–™æœ€åˆå¯ä»¥æ˜¾è‘—æé«˜ä½œç‰©äº§é‡ï¼Œä½†è¿™ç§æ•ˆæœå¾ˆå¿«å°±ä¼šå¼€å§‹é€’å‡ã€‚

+   åˆ¶é€  â€” å‘ç”Ÿäº§è¿‡ç¨‹ä¸­æ·»åŠ æ›´å¤šå·¥äººå°†æé«˜æ•ˆç‡ï¼Œä½†æ¯å¢åŠ ä¸€åå·¥äººå¯¹æ•´ä½“äº§å‡ºçš„è´¡çŒ®å¯èƒ½ä¼šå‡å°‘ã€‚

è¿™è®©æˆ‘å¼€å§‹æ€è€ƒï¼Œå¦‚æœé€’å‡æ”¶ç›Šå¦‚æ­¤å¸¸è§ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„å› æœäººå·¥æ™ºèƒ½å·¥å…·ç®±ä¸­æœ‰å“ªäº›æŠ€æœ¯å¯ä»¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Ÿ

# æˆ‘ä»¬çš„å› æœäººå·¥æ™ºèƒ½å·¥å…·ç®±ä¸­æœ‰å“ªäº›æ–¹æ³•é€‚åˆä¼°ç®—éçº¿æ€§å¤„ç†æ•ˆåº”ï¼Ÿ

## å·¥å…·ç®±

æˆ‘ä»¬å°†æå‡ºä¸¤ä¸ªå…³é”®é—®é¢˜ï¼Œå¸®åŠ©æˆ‘ä»¬è¯†åˆ«å“ªäº›å› æœäººå·¥æ™ºèƒ½å·¥å…·ç®±ä¸­çš„æ–¹æ³•é€‚åˆè§£å†³å®šä»·é—®é¢˜ï¼š

+   å®ƒèƒ½å¤„ç†è¿ç»­æ€§å¤„ç†å—ï¼Ÿ

+   å®ƒèƒ½æ•æ‰éçº¿æ€§å¤„ç†æ•ˆåº”å—ï¼Ÿ

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•è¯„ä¼°æ¯ç§æ–¹æ³•é€‚ç”¨æ€§çš„æ€»ç»“ï¼š

+   å€¾å‘å¾—åˆ†åŒ¹é…ï¼ˆPSMï¼‰â€” å¤„ç†éœ€è¦æ˜¯äºŒå…ƒçš„ âŒ

+   å€¾å‘å¾—åˆ†é€†å‘åŒ¹é…ï¼ˆIPSMï¼‰â€” å¤„ç†éœ€è¦æ˜¯äºŒå…ƒçš„ âŒ

+   T å­¦ä¹ è€…ï¼ˆT-Learnerï¼‰â€” å¤„ç†éœ€è¦æ˜¯äºŒå…ƒçš„ âŒ

+   åŒé‡æœºå™¨å­¦ä¹ ï¼ˆDMLï¼‰â€” å¤„ç†æ•ˆåº”æ˜¯çº¿æ€§çš„ âŒ

+   åŒé‡é²æ£’å­¦ä¹ è€…ï¼ˆDRï¼‰â€” å¤„ç†éœ€è¦æ˜¯äºŒå…ƒçš„ âŒ

+   S-Learner â€” å¦‚æœä½¿ç”¨é€‚å½“çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆä¾‹å¦‚æ¢¯åº¦æå‡ï¼‰ï¼Œå®ƒå¯ä»¥å¤„ç†è¿ç»­å¤„ç†å’Œå¤„ç†ä¸ç»“æœä¹‹é—´çš„éçº¿æ€§å…³ç³»ğŸ’š

## S-Learner

S-Learner ä¸­çš„â€œSâ€æ¥è‡ªäºå®ƒæ˜¯ä¸€ä¸ªâ€œå•ä¸€æ¨¡å‹â€ã€‚ä¸€ä¸ªä»»æ„çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¢«ç”¨æ¥é¢„æµ‹ç»“æœï¼Œä½¿ç”¨å¤„ç†ã€æ··æ‚å› ç´ å’Œå…¶ä»–åå˜é‡ä½œä¸ºç‰¹å¾ã€‚è¿™ä¸ªæ¨¡å‹éšåè¢«ç”¨æ¥ä¼°è®¡åœ¨ä¸åŒå¤„ç†æ¡ä»¶ä¸‹æ½œåœ¨ç»“æœçš„å·®å¼‚ï¼ˆä»è€Œç»™æˆ‘ä»¬å¸¦æ¥å¤„ç†æ•ˆåº”ï¼‰ã€‚

S-Learner æœ‰è®¸å¤šä¼˜ç‚¹ï¼š

+   å®ƒå¯ä»¥å¤„ç†äºŒå…ƒå’Œè¿ç»­æ€§å¤„ç†ã€‚

+   å®ƒå¯ä»¥ä½¿ç”¨ä»»ä½•æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèµ‹äºˆæˆ‘ä»¬çµæ´»æ€§æ¥æ•æ‰ç‰¹å¾å’Œå¤„ç†ä¹‹é—´çš„éçº¿æ€§å…³ç³»ã€‚

ä¸€ä¸ªè­¦å‘Šï¼šæ­£åˆ™åŒ–åå·®ï¼ç°ä»£æœºå™¨å­¦ä¹ ç®—æ³•ä½¿ç”¨æ­£åˆ™åŒ–æ¥é˜²æ­¢è¿‡æ‹Ÿåˆâ€”â€”ä½†è¿™å¯èƒ½å¯¹å› æœé—®é¢˜äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ä»¥æ¢¯åº¦æå‡æ ‘æ–¹æ³•ä¸­çš„è¶…å‚æ•°*max features*ä¸ºä¾‹â€”â€”åœ¨å¤šä¸ªæ ‘ä¸­ï¼Œå¯èƒ½ä¼šå‡ºç°å¤„ç†æœªè¢«åŒ…å«åœ¨æ¨¡å‹ä¸­çš„æƒ…å†µã€‚è¿™ä¼šå‰Šå¼±å¤„ç†æ•ˆåº”ã€‚

åœ¨ä½¿ç”¨ S-Learner æ—¶ï¼Œæˆ‘å»ºè®®ä»”ç»†è€ƒè™‘æ­£åˆ™åŒ–å‚æ•°ï¼Œä¾‹å¦‚å°†*max features*è®¾ç½®ä¸º 1.0ï¼ˆæœ‰æ•ˆåœ°å…³é—­ç‰¹å¾æ­£åˆ™åŒ–ï¼‰ã€‚

# å¦‚ä½•ä½¿ç”¨éçº¿æ€§ç¼–ç¨‹æ¥ä¼˜åŒ–å®šä»·ï¼Ÿ

## ä»·æ ¼ä¼˜åŒ–

å‡è®¾æˆ‘ä»¬æœ‰å¤šç§äº§å“ï¼Œå¹¶ä¸”æƒ³è¦åœ¨ç»™å®šçš„ä¿ƒé”€é¢„ç®—ä¸‹ä¼˜åŒ–å®ƒä»¬çš„ä»·æ ¼ã€‚å¯¹äºæ¯ä¸ªäº§å“ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ª S-Learnerï¼ˆä½¿ç”¨æ¢¯åº¦æå‡ï¼‰ï¼Œå°†å¤„ç†è®¾ç½®ä¸ºæŠ˜æ‰£æ°´å¹³ï¼Œå°†ç»“æœè®¾ç½®ä¸ºæ€»è®¢å•æ•°ã€‚æˆ‘ä»¬çš„ S-Learner è¾“å‡ºä¸€ä¸ªå¤æ‚æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥ä¼°è®¡ä¸åŒæŠ˜æ‰£æ°´å¹³çš„æ•ˆåº”ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•ä¼˜åŒ–æ¯ä¸ªäº§å“çš„æŠ˜æ‰£æ°´å¹³å‘¢ï¼Ÿ

## å“åº”æ›²çº¿

ä¼˜åŒ–æŠ€æœ¯ï¼Œå¦‚çº¿æ€§ï¼ˆç”šè‡³éçº¿æ€§ï¼‰ç¼–ç¨‹ï¼Œä¾èµ–äºå“åº”çš„æ¸…æ™°å‡½æ•°å½¢å¼ã€‚åƒéšæœºæ£®æ—å’Œæ¢¯åº¦æå‡è¿™æ ·çš„æœºå™¨å­¦ä¹ æŠ€æœ¯å¹¶ä¸ä¼šç»™æˆ‘ä»¬æä¾›è¿™ä¸ªï¼ˆä¸çº¿æ€§å›å½’ä¸åŒï¼‰ã€‚ç„¶è€Œï¼Œå“åº”æ›²çº¿å¯ä»¥å°† S-Learner çš„è¾“å‡ºè½¬åŒ–ä¸ºä¸€ç§ç»¼åˆå½¢å¼ï¼Œå±•ç¤ºç»“æœå¦‚ä½•å“åº”å¤„ç†ã€‚

å¦‚æœä½ è¿˜ä¸å¤ªèƒ½æƒ³è±¡æˆ‘ä»¬å¦‚ä½•åˆ›å»ºå“åº”æ›²çº¿ï¼Œåˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬å°†åœ¨ Python æ¡ˆä¾‹ç ”ç©¶ä¸­è¯¦ç»†è®²è§£ï¼

## ç±³å“ˆåˆ©æ–¯-é—¨å†œæ–¹ç¨‹

æœ‰å‡ ç§æ–¹ç¨‹å¯ä»¥ç”¨æ¥å°† S-Learner æ˜ å°„åˆ°å“åº”æ›²çº¿ã€‚å…¶ä¸­ä¹‹ä¸€å°±æ˜¯ç±³å“ˆåˆ©æ–¯-é—¨å†œæ–¹ç¨‹ã€‚

ç±³å“ˆåˆ©æ–¯-é—¨å†œæ–¹ç¨‹é€šå¸¸ç”¨äºé…¶åŠ¨åŠ›å­¦ï¼ˆç ”ç©¶é…¶å‚¬åŒ–åŒ–å­¦ååº”çš„é€Ÿç‡ï¼‰ä¸­ï¼Œç”¨æ¥æè¿°é…¶ä¿ƒååº”çš„é€Ÿç‡ã€‚

![](img/103dde2faf7a1ea73b0d3fe4dd0b2798.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾ç‰‡

+   v â€” æ˜¯ååº”é€Ÿåº¦ï¼ˆè¿™æ˜¯æˆ‘ä»¬è½¬åŒ–åçš„å“åº”ï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„å®šä»·ç¤ºä¾‹ä¸­æ˜¯è®¢å•çš„æ€»æ•°ï¼‰

+   Vmax â€” æ˜¯æœ€å¤§ååº”é€Ÿåº¦ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º alphaï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼‰

+   Km â€” æ˜¯åº•ç‰©æµ“åº¦ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸º lambdaï¼Œè¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬éœ€è¦å­¦ä¹ çš„å‚æ•°ï¼‰

+   S â€” æ˜¯è¿ˆå…‹åˆ©æ–¯å¸¸æ•°ï¼ˆè¿™æ˜¯æˆ‘ä»¬çš„å¤„ç†å˜é‡ï¼Œæ‰€ä»¥åœ¨å®šä»·ç¤ºä¾‹ä¸­æ˜¯æŠ˜æ‰£æ°´å¹³ï¼‰

å®ƒçš„åŸç†ä¹Ÿå¯ä»¥åº”ç”¨äºå…¶ä»–é¢†åŸŸï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†é‚£äº›ç”±äºé¥±å’Œå› ç´ å¯¼è‡´è¾“å…¥å¢åŠ ä¸èƒ½æŒ‰æ¯”ä¾‹å¢åŠ è¾“å‡ºçš„ç³»ç»Ÿæ—¶ã€‚ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸åŒçš„ alpha å’Œ lambda å€¼å¦‚ä½•å½±å“æ›²çº¿ï¼š

```py
def michaelis_menten(x, alpha, lam):
    return alpha * x / (lam + x)
```

![](img/339c252eb69c29a11f08f39d2b0e2cbd.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

ä¸€æ—¦æˆ‘ä»¬è·å¾—äº†å“åº”æ›²çº¿ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥è€ƒè™‘ä¼˜åŒ–é—®é¢˜ã€‚è¿ˆå…‹åˆ©æ–¯-å­Ÿä¸œæ–¹ç¨‹ç»™å‡ºäº†ä¸€ä¸ªéçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼Œéçº¿æ€§è§„åˆ’æ˜¯ä¸€ä¸ªåˆé€‚çš„é€‰æ‹©ã€‚

## éçº¿æ€§è§„åˆ’

åœ¨æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« ä¸­æˆ‘ä»¬ä»‹ç»äº†çº¿æ€§è§„åˆ’ã€‚éçº¿æ€§è§„åˆ’ç±»ä¼¼ï¼Œä½†ç›®æ ‡å‡½æ•°å’Œ/æˆ–çº¦æŸæ¡ä»¶æœ¬è´¨ä¸Šæ˜¯éçº¿æ€§çš„ã€‚

åºåˆ—æœ€å°äºŒä¹˜æ³•è§„åˆ’ï¼ˆSLSQPï¼‰æ˜¯ä¸€ç§ç”¨äºè§£å†³éçº¿æ€§è§„åˆ’é—®é¢˜çš„ç®—æ³•ã€‚å®ƒå…è®¸åŒæ—¶å¤„ç†ç­‰å¼çº¦æŸå’Œä¸ç­‰å¼çº¦æŸï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ä½¿ç”¨åœºæ™¯ä¸­æ˜¯ä¸€ä¸ªåˆç†çš„é€‰æ‹©ã€‚

+   ç­‰å¼çº¦æŸï¼Œä¾‹å¦‚æ€»ä¿ƒé”€é¢„ç®—ç­‰äºÂ£100k

+   ä¸ç­‰å¼çº¦æŸï¼Œä¾‹å¦‚æ¯ä¸ªäº§å“çš„æŠ˜æ‰£åœ¨Â£1 åˆ°Â£10 ä¹‹é—´

SciPy æä¾›äº†ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ SLSQP å®ç°ï¼š

[](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html?source=post_page-----011ce140d180--------------------------------) [## minimize(method='SLSQP') - SciPy v1.13.0 Manual

### å¦‚æœ jac åœ¨['2-point', '3-point', 'cs']ä¸­ï¼Œä½¿ç”¨ç›¸å¯¹æ­¥é•¿è¿›è¡Œæ•°å€¼è¿‘ä¼¼ jacã€‚ç»å¯¹â€¦

[docs.scipy.org](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html?source=post_page-----011ce140d180--------------------------------)

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å±•ç¤º S-Learnerã€è¿ˆå…‹åˆ©æ–¯-å­Ÿä¸œæ–¹ç¨‹å’Œéçº¿æ€§è§„åˆ’ç»“åˆçš„å¼ºå¤§å¨åŠ›ï¼

# æ¡ˆä¾‹ç ”ç©¶

## èƒŒæ™¯

å†å²ä¸Šï¼Œä¿ƒé”€å›¢é˜Ÿä¸€ç›´ä¾é ä»–ä»¬çš„ä¸“å®¶åˆ¤æ–­æ¥ä¸ºä»–ä»¬çš„ä¸‰å¤§ä¸»æ‰“äº§å“è®¾ç½®æŠ˜æ‰£ã€‚è€ƒè™‘åˆ°å½“å‰çš„ç»æµçŠ¶å†µï¼Œä»–ä»¬è¢«è¿«å°†æ•´ä½“ä¿ƒé”€é¢„ç®—å‰Šå‡ 20%ã€‚äºæ˜¯ï¼Œä»–ä»¬æ±‚åŠ©äºæ•°æ®ç§‘å­¦å›¢é˜Ÿï¼Œå’¨è¯¢å¦‚ä½•åœ¨å‡å°‘è®¢å•é‡æŸå¤±çš„åŒæ—¶åšåˆ°è¿™ä¸€ç‚¹ã€‚

## æ•°æ®ç”Ÿæˆè¿‡ç¨‹

æˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ï¼š

+   4 ä¸ªä¸è®¢å•æ•°é‡æœ‰å¤æ‚å…³ç³»çš„ç‰¹å¾

+   ä¸€ä¸ªéµå¾ªè¿ˆå…‹åˆ©æ–¯-å­Ÿä¸œæ–¹ç¨‹çš„å¤„ç†æ•ˆæœ

```py
def data_generator(n, tau_weight, alpha, lam):

    # Set number of features
    p=4

    # Create features
    X = np.random.uniform(size=n * p).reshape((n, -1))

    # Nuisance parameters
    b = (
        np.sin(np.pi * X[:, 0])
        + 2 * (X[:, 1] - 0.5) ** 2
        + X[:, 2] * X[:, 3]
    )

    # Create treatment and treatment effect
    T = np.linspace(200, 10000, n)
    T_mm = michaelis_menten(T, alpha, lam) * tau_weight
    tau = T_mm / T

    # Calculate outcome
    y = b + T * tau + np.random.normal(size=n) * 0.5

    y_train = y
    X_train = np.hstack((X, T.reshape(-1, 1)))

    return y_train, X_train, T_mm, tau
```

X ç‰¹å¾æ˜¯æ··æ‚å˜é‡ï¼š

![](img/070ded84b57d5cde15c71bc2917b0210.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

æˆ‘ä»¬ä½¿ç”¨æ•°æ®ç”Ÿæˆå™¨ä¸ºä¸‰ä¸ªäº§å“åˆ›å»ºæ ·æœ¬ï¼Œæ¯ä¸ªäº§å“æœ‰ä¸åŒçš„å¤„ç†æ•ˆæœï¼š

```py
np.random.seed(1234)

n=100000

y_train_1, X_train_1, T_mm_1, tau_1 = data_generator(n, 1.00, 2, 5000)
y_train_2, X_train_2, T_mm_2, tau_2 = data_generator(n, 0.25, 2, 5000)
y_train_3, X_train_3, T_mm_3, tau_3 = data_generator(n, 2.00, 2, 5000)
```

## S-Learner

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ä»»ä½•æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå¹¶å°†å¤„ç†å’Œåå˜é‡ä½œä¸ºç‰¹å¾æ¥è®­ç»ƒä¸€ä¸ª S-Learnerï¼š

```py
def train_slearner(X_train, y_train):

    model = LGBMRegressor(random_state=42)
    model.fit(X_train, y_train)

    yhat_train = model.predict(X_train)

    mse_train = mean_squared_error(y_train, yhat_train)
    r2_train = r2_score(y_train, yhat_train)

    print(f'MSE on train set is {round(mse_train)}')
    print(f'R2 on train set is {round(r2_train, 2)}')

    return model, yhat_train
```

æˆ‘ä»¬ä¸ºæ¯ä¸ªäº§å“è®­ç»ƒä¸€ä¸ª S-Learnerï¼š

```py
np.random.seed(1234)

model_1, yhat_train_1 = train_slearner(X_train_1, y_train_1)
model_2, yhat_train_2 = train_slearner(X_train_2, y_train_2)
model_3, yhat_train_3 = train_slearner(X_train_3, y_train_3)
```

ç›®å‰è¿™åªæ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹â€”â€”ä¸‹é¢æˆ‘ä»¬å¯è§†åŒ–å®ƒåœ¨è¿™é¡¹å·¥ä½œä¸­çš„è¡¨ç°ï¼š

![](img/633b5e5ab9af4aa5319a9f6e195fe89e.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

## æå–å¤„ç†æ•ˆæœ

æ¥ä¸‹æ¥æˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„ S-learner æ¥æå–æ•´ä¸ªå¤„ç†å€¼èŒƒå›´ï¼ˆæŠ˜æ‰£é‡‘é¢ï¼‰çš„å¤„ç†æ•ˆæœï¼ŒåŒæ—¶å°†å…¶ä»–ç‰¹å¾ä¿æŒåœ¨å…¶å¹³å‡å€¼ã€‚

æˆ‘ä»¬é¦–å…ˆæå–æ•´ä¸ªå¤„ç†å€¼èŒƒå›´çš„é¢„æœŸç»“æœï¼ˆè®¢å•æ•°é‡ï¼‰ï¼š

```py
def extract_treated_effect(n, X_train, model):

    # Set features to mean value
    X_mean_mapping = {'X1': [X_train[:, 0].mean()] * n,
                      'X2': [X_train[:, 1].mean()] * n,
                      'X3': [X_train[:, 2].mean()] * n,
                      'X4': [X_train[:, 3].mean()] * n}

    # Create DataFrame
    df_scoring = pd.DataFrame(X_mean_mapping)

    # Add full range of treatment values
    df_scoring['T'] = X_train[:, 4].reshape(-1, 1)

    # Calculate outcome prediction for treated
    treated = model.predict(df_scoring)

    return treated, df_scoring
```

æˆ‘ä»¬å¯¹æ¯ä¸ªäº§å“æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
treated_1, df_scoring_1 = extract_treated_effect(n, X_train_1, model_1)
treated_2, df_scoring_2 = extract_treated_effect(n, X_train_2, model_2)
treated_3, df_scoring_3 = extract_treated_effect(n, X_train_3, model_3)
```

ç„¶åæˆ‘ä»¬æå–å½“å¤„ç†è®¾ç½®ä¸º 0 æ—¶çš„é¢„æœŸç»“æœï¼ˆè®¢å•æ•°é‡ï¼‰ï¼š

```py
def extract_untreated_effect(n, X_train, model):

    # Set features to mean value
    X_mean_mapping = {'X1': [X_train[:, 0].mean()] * n,
                      'X2': [X_train[:, 1].mean()] * n,
                      'X3': [X_train[:, 2].mean()] * n,
                      'X4': [X_train[:, 3].mean()] * n,
                      'T': [0] * n}

    # Create DataFrame
    df_scoring = pd.DataFrame(X_mean_mapping)

    # Add full range of treatment values
    df_scoring

    # Calculate outcome prediction for treated
    untreated = model.predict(df_scoring)

    return untreated
```

å†æ¬¡ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªäº§å“æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
untreated_1 = extract_untreated_effect(n, X_train_1, model_1)
untreated_2 = extract_untreated_effect(n, X_train_2, model_2)
untreated_3 = extract_untreated_effect(n, X_train_3, model_3)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥è®¡ç®—æ•´ä¸ªå¤„ç†å€¼èŒƒå›´çš„å¤„ç†æ•ˆæœï¼š

```py
treatment_effect_1 = treated_1 - untreated_1
treatment_effect_2 = treated_2 - untreated_2
treatment_effect_3 = treated_3 - untreated_3
```

å½“æˆ‘ä»¬å°†å…¶ä¸ä»æ•°æ®ç”Ÿæˆå™¨ä¿å­˜çš„å®é™…å¤„ç†æ•ˆæœè¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° S-Learner åœ¨ä¼°è®¡æ•´ä¸ªå¤„ç†å€¼èŒƒå›´çš„å¤„ç†æ•ˆæœæ–¹é¢éå¸¸æœ‰æ•ˆï¼š

![](img/b97c0b95c35e52cf9b9fac26fa824876.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†è¿™äº›å¤„ç†æ•ˆæœæ•°æ®ï¼Œå¯ä»¥ç”¨å®ƒä¸ºæ¯ä¸ªäº§å“æ„å»ºå“åº”æ›²çº¿ã€‚

## ç±³æ°-å­Ÿä¸œæ–¹ç¨‹ï¼ˆMichaelis-Mentonï¼‰

ä¸ºäº†æ„å»ºå“åº”æ›²çº¿ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›²çº¿æ‹Ÿåˆå·¥å…·ã€‚SciPy æœ‰ä¸€ä¸ªå¾ˆå¥½çš„å®ç°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒï¼š

[](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html?source=post_page-----011ce140d180--------------------------------) [## scipy.optimize.curve_fit - SciPy v1.13.0 æ‰‹å†Œ]

### scipy.optimize. curve_fit ( f , xdata , ydata , , , , , bounds = (-inf, inf) , , , * , , , ** kwargs ) [source] ä½¿ç”¨â€¦

docs.scipy.org](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html?source=post_page-----011ce140d180--------------------------------)

æˆ‘ä»¬é¦–å…ˆè®¾ç½®æˆ‘ä»¬æƒ³è¦å­¦ä¹ çš„å‡½æ•°ï¼š

```py
def michaelis_menten(x, alpha, lam):
    return alpha * x / (lam + x)
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ curve_fit æ¥å­¦ä¹  alpha å’Œ lambda å‚æ•°ï¼š

```py
def response_curves(treatment_effect, df_scoring):

    maxfev = 100000
    lam_initial_estimate = 0.001
    alpha_initial_estimate = max(treatment_effect)
    initial_guess = [alpha_initial_estimate, lam_initial_estimate]

    popt, pcov = curve_fit(michaelis_menten, df_scoring['T'], treatment_effect, p0=initial_guess, maxfev=maxfev)

    return popt, pcov
```

æˆ‘ä»¬å¯¹æ¯ä¸ªäº§å“æ‰§è¡Œæ­¤æ“ä½œï¼š

```py
popt_1, pcov_1 = response_curves(treatment_effect_1, df_scoring_1)
popt_2, pcov_2 = response_curves(treatment_effect_2, df_scoring_2)
popt_3, pcov_3 = response_curves(treatment_effect_3, df_scoring_3)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†å­¦ä¹ åˆ°çš„å‚æ•°è¾“å…¥åˆ°ç±³æ°å­Ÿä¸œæ–¹ç¨‹ä¸­ï¼Œå¸®åŠ©æˆ‘ä»¬å¯è§†åŒ–æ›²çº¿æ‹Ÿåˆçš„æ•ˆæœï¼š

```py
treatment_effect_curve_1 = michaelis_menten(df_scoring_1['T'], popt_1[0], popt_1[1])
treatment_effect_curve_2 = michaelis_menten(df_scoring_2['T'], popt_2[0], popt_2[1])
treatment_effect_curve_3 = michaelis_menten(df_scoring_3['T'], popt_3[0], popt_3[1])
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ›²çº¿æ‹Ÿåˆåšå¾—éå¸¸å¥½ï¼

![](img/fd743bbd492a4de4bf9bc40f0b9d8ad4.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ¯ä¸ªäº§å“çš„ alpha å’Œ lambda å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è€ƒè™‘éçº¿æ€§ä¼˜åŒ–â€¦â€¦

## éçº¿æ€§ç¼–ç¨‹

æˆ‘ä»¬é¦–å…ˆå¼€å§‹æ”¶é›†æ‰€æœ‰ä¼˜åŒ–æ‰€éœ€çš„ä¿¡æ¯ï¼š

+   æ‰€æœ‰äº§å“çš„åˆ—è¡¨

+   æ€»ä¿ƒé”€é¢„ç®—

+   æ¯ä¸ªäº§å“çš„é¢„ç®—èŒƒå›´

+   ä»ç±³æ°-å­Ÿä¸œååº”æ›²çº¿ä¸­æå–çš„æ¯ä¸ªäº§å“çš„å‚æ•°

```py
# List of products
products = ["product_1", "product_2", "product_3"]

# Set total budget to be the sum of the mean of each product reduced by 20%
total_budget = (df_scoring_1['T'].mean() + df_scoring_2['T'].mean() + df_scoring_3['T'].mean()) * 0.80

# Dictionary with min and max bounds for each product - set as +/-20% of max/min discount
budget_ranges = {"product_1": [df_scoring_1['T'].min() * 0.80, df_scoring_1['T'].max() * 1.2], 
                 "product_2": [df_scoring_2['T'].min() * 0.80, df_scoring_2['T'].max() * 1.2], 
                 "product_3": [df_scoring_3['T'].min() * 0.80, df_scoring_3['T'].max() * 1.2]}

# Dictionary with response curve parameters
parameters = {"product_1": [popt_1[0], popt_1[1]], 
              "product_2": [popt_2[0], popt_2[1]], 
              "product_3": [popt_3[0], popt_3[1]]}
```

æ¥ä¸‹æ¥æˆ‘ä»¬è®¾ç½®ç›®æ ‡å‡½æ•°â€”â€”æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è®¢å•æ•°ï¼Œä½†ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨æœ€å°åŒ–æ–¹æ³•ï¼Œå› æ­¤è¿”å›é¢„æœŸè®¢å•æ€»æ•°çš„è´Ÿå€¼ã€‚

```py
def objective_function(x, products, parameters):

    sum_orders = 0.0

    # Unpack parameters for each product and calculate expected orders
    for product, budget in zip(products, x, strict=False):
        L, k = parameters[product]
        sum_orders += michaelis_menten(budget, L, k)

    return -1 * sum_orders
```

æœ€åæˆ‘ä»¬å¯ä»¥è¿è¡Œä¼˜åŒ–ï¼Œç¡®å®šåˆ†é…ç»™æ¯ä¸ªäº§å“çš„æœ€ä¼˜é¢„ç®—ï¼š

```py
# Set initial guess by equally sharing out the total budget
initial_guess = [total_budget // len(products)] * len(products)

# Set the lower and upper bounds for each product
bounds = [budget_ranges[product] for product in products]

# Set the equality constraint - constraining the total budget
constraints = {"type": "eq", "fun": lambda x: np.sum(x) - total_budget}

# Run optimisation
result = minimize(
    lambda x: objective_function(x, products, parameters),
    initial_guess,
    method="SLSQP",
    bounds=bounds,
    constraints=constraints,
    options={'disp': True, 'maxiter': 1000, 'ftol': 1e-9},
)

# Extract results
optimal_treatment = {product: budget for product, budget in zip(products, result.x, strict=False)}
print(f'Optimal promo budget allocations: {optimal_treatment}')
print(f'Optimal orders: {round(result.fun * -1, 2)}')
```

è¾“å‡ºå‘æˆ‘ä»¬å±•ç¤ºäº†æ¯ä¸ªäº§å“çš„æœ€ä¼˜ä¿ƒé”€é¢„ç®—ï¼š

![](img/7468ce2e35ed95ef3fdd0f6432967360.png)

ç”¨æˆ·ç”Ÿæˆçš„å›¾åƒ

å¦‚æœä½ ä»”ç»†æ£€æŸ¥å“åº”æ›²çº¿ï¼Œä½ ä¼šå‘ç°ä¼˜åŒ–ç»“æœæ˜¯ç›´è§‚çš„ï¼š

+   ç¨å¾®å‡å°‘äº§å“ 1 çš„é¢„ç®—ã€‚

+   æ˜¾è‘—å‡å°‘äº§å“ 2 çš„é¢„ç®—ã€‚

+   æ˜¾è‘—å¢åŠ äº§å“ 3 çš„é¢„ç®—ã€‚

# ç»“è¯­ã€‚

ä»Šå¤©æˆ‘ä»¬è®¨è®ºäº† S-Learnerã€Michaelis-Menten æ–¹ç¨‹å’Œéçº¿æ€§è§„åˆ’çš„å¼ºå¤§ç»“åˆï¼ä»¥ä¸‹æ˜¯ä¸€äº›ç»“è¯­ï¼š

+   å¦‚å‰æ‰€è¿°ï¼Œä½¿ç”¨ S-Learner æ—¶è¦å°å¿ƒæ­£åˆ™åŒ–åå·®ï¼

+   S-Learner çš„ä¸€ä¸ªå¾ˆå¥½çš„æ›¿ä»£æ–¹æ³•æ˜¯ä½¿ç”¨ DMLï¼Œä½†åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰å¯¹å¤„ç†è¿›è¡Œè½¬æ¢â€”â€”ç„¶è€Œï¼Œè¿™æ„å‘³ç€ä½ éœ€è¦å¯¹å¤„ç†çš„å‡½æ•°å½¢å¼æœ‰ä¸€å®šçš„å…ˆéªŒçŸ¥è¯†ã€‚

+   æˆ‘é€‰æ‹©ä½¿ç”¨ Michaelis-Menten æ–¹ç¨‹æ¥æ„å»ºæˆ‘çš„å“åº”æ›²çº¿â€”â€”ç„¶è€Œï¼Œè¿™å¯èƒ½ä¸é€‚åˆä½ çš„é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡å…¶ä»–æ›´åˆé€‚çš„è½¬æ¢æ–¹æ³•æ¥æ›¿ä»£ã€‚

+   ä½¿ç”¨ SLSQP æ¥è§£å†³éçº¿æ€§è§„åˆ’é—®é¢˜å¯ä»¥è®©ä½ çµæ´»åœ°ä½¿ç”¨ç­‰å¼å’Œä¸ç­‰å¼çº¦æŸã€‚

+   ä½ æ”¶é›†çš„æ•°æ®å¾ˆå¯èƒ½æ˜¯è§‚å¯Ÿæ€§æ•°æ®â€”â€”è¿™å¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œå°¤å…¶æ˜¯åœ¨ä½ å°†æ”¶é›†åˆ°çš„æŠ˜æ‰£å€¼èŒƒå›´ä¸Šâ€”â€”è¿™äº›å€¼å¯èƒ½ä¼šé›†ä¸­åœ¨ä¸€ä¸ªç‰¹å®šçš„åŒºåŸŸã€‚ä½¿ç”¨æŸç§ Shapley æ–¹æ³•æ¥åˆ›å»ºç”¨äºç”Ÿæˆå“åº”æ›²çº¿çš„æ•°æ®ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹å¯èƒ½æ›´ä¸ºåˆé€‚ã€‚

+   æˆ‘é€‰æ‹©ä¸“æ³¨äºå®šä»·å’Œä¿ƒé”€ï¼Œä½†è¿™ä¸ªæ¡†æ¶å¯ä»¥æ‰©å±•åˆ°è¥é”€é¢„ç®—ã€‚

å¦‚æœä½ æƒ³ç»§ç»­æ·±å…¥äº†è§£å› æœ AIï¼Œå…³æ³¨æˆ‘â€”â€”åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•è¡¡é‡è¥é”€æ´»åŠ¨çš„å†…åœ¨å› æœå½±å“ã€‚
