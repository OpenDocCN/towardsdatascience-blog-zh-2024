- en: Create Mixtures of Experts with MergeKit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27](https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Combine multiple models into a single MoE*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    ¬∑9 min read¬∑Mar 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa74b784e835c9760158fe939603501a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks to the release of Mixtral, the **Mixture of Experts** (MoE) architecture
    has become popular in recent months. This architecture offers an interesting tradeoff:
    higher performance at the cost of increased VRAM usage. While Mixtral and other
    MoE architectures are pre-trained from scratch, another method of creating MoE
    has recently appeared. Thanks to Arcee‚Äôs [MergeKit](https://github.com/arcee-ai/mergekit)
    library, we now have a new way of creating MoEs by ensembling several pre-trained
    models. These are often referred to as **frankenMoEs** or **MoErges** to distinguish
    them from the pre-trained MoEs.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will detail how the MoE architecture works and how frankenMoEs
    are created. Finally, we will make our [own frankenMoE](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)
    with MergeKit and evaluate it on several benchmarks. The code is available on
    Google Colab in a wrapper called [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y).
  prefs: []
  type: TYPE_NORMAL
- en: Special thanks to [Charles Goddard](https://github.com/cg123), the creator of
    MergeKit, for proofreading this article.
  prefs: []
  type: TYPE_NORMAL
- en: üîÄ Introduction to MoEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Mixture of Experts is an architecture designed for improved efficiency and
    performance. It uses multiple specialized subnetworks, known as ‚Äú**experts**.‚Äù
    Unlike dense models, where the entire network is activated, MoEs only activate
    relevant experts based on the input. This results in faster training and more
    efficient inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two components at the core of an MoE model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sparse MoE Layers**: These replace the dense feed-forward network layers
    in the transformer architecture. Each MoE layer contains several experts, and
    only a subset of these experts are engaged for a given input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gate Network or Router**: This component determines which tokens are processed
    by which experts, ensuring that each part of the input is handled by the most
    suitable expert(s).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following example, we show how a Mistral-7B block is transformed into
    an MoE block with a sparse MoE layer (feedforward network 1, 2, and 3) and a router.
    This example represents an MoE with three experts, where two are currently engaged
    (FFN 1 and FFN 3).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ec46b2d6081bda34c1822c37edc0420.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: MoEs also come with their own set of challenges, especially in terms of fine-tuning
    and memory requirements. The fine-tuning process can be difficult due to the model‚Äôs
    complexity, with the need to **balance expert usage** during training to properly
    train the gating weights to select the most relevant ones. In terms of memory,
    even though only a fraction of the total parameters are used during inference,
    the entire model, including all experts, needs to be **loaded into memory**, which
    requires high VRAM capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, there are two essential parameters when it comes to MoEs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of experts** (`num_local_experts`): This determines the total number
    of experts in the architecture (e.g., 8 for Mixtral). The higher the number of
    experts, the higher the VRAM usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of experts/token** (`num_experts_per_tok`): This determines the number
    of experts that are engaged for each token and each layer (e.g., 2 for Mixtral).
    There is a tradeoff between a high number of experts per token for accuracy (but
    diminishing returns) vs. a low number for fast training and inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Historically, MoEs have underperformed dense models. However, the release of
    [Mixtral-8x7B](https://arxiv.org/abs/2401.04088) in December 2023 shook things
    up and showed impressive performance for its size. Additionally, GPT-4 is also
    rumored to be an MoE, which would make sense as it would be a lot cheaper to run
    and train for OpenAI compared to a dense model. In addition to these recent excellent
    MoEs, we now have a new way of creating MoEs with MergeKit: frankenMoEs, also
    called MoErges.'
  prefs: []
  type: TYPE_NORMAL
- en: üßü‚Äç‚ôÇÔ∏è True MoEs vs. frankenMoEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main difference between true MoEs and frankenMoEs is how they‚Äôre trained.
    In the case of true MoEs, the experts and the router are trained jointly. In the
    case of frankenMoEs, we upcycle existing models and initialize the router afterward.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we copy the weights of the layer norm and self-attention layers
    from a base model, and then copy the weights of the FFN layers found in each expert.
    This means that besides the FFNs, all the other parameters are shared. This explains
    why Mixtral-8x7B with eight experts doesn‚Äôt have 8*7 = 56B parameters, but about
    45B. This is also why using two experts per token gives the inference speed (FLOPs)
    of a 12B dense model instead of 14B.
  prefs: []
  type: TYPE_NORMAL
- en: 'FrankenMoEs are about selecting the most relevant experts and initializing
    them properly. MergeKit currently implements three ways of initializing the routers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Random**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L139-L142):
    Random weights. Be careful when using it as the same experts might be selected
    every time (it requires further fine-tuning or `num_local_experts = num_experts_per_tok`,
    which means you don''t need any routing).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Cheap embed**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L91C1-L109C37):
    It uses the raw embeddings of the input tokens directly and applies the same transformation
    across all layers. This method is computationally inexpensive and suitable for
    execution on less powerful hardware.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Hidden**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L70-L88):
    It creates hidden representations of a list of positive and negative prompts by
    extracting them from the last layer of the LLM. They are averaged and normalized
    to initialize the gates. More information about it is available on [Charles Goddard‚Äôs
    blog](https://goddard.blog/posts/clown-moe/).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can guess, the ‚Äúhidden‚Äù initialization is the most efficient to correctly
    route the tokens to the most relevant experts. In the next section, we will create
    our own frankenMoE using this technique.
  prefs: []
  type: TYPE_NORMAL
- en: üíª Creating a frankenMoE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create our frankenMoE, we need to select `n` experts. In this case, we will
    rely on Mistral-7B thanks to its popularity and relatively small size. However,
    eight experts like in Mixtral is quite a lot, as we need to fit all of them in
    memory. For efficiency, I'll only use four experts in this example, with two of
    them engaged for each token and each layer. In this case, we will end up with
    a model with 24.2B parameters instead of 4*7 = 28B parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, our goal is to create a well-rounded model that can do pretty much everything:
    write stories, explain articles, code in Python, etc. We can decompose this requirement
    into four tasks and select the best expert for each of them. This is how I decomposed
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chat model**: a general-purpose model that is used in most interactions.
    I used [mlabonne/AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B),
    which perfectly satisfies the requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code model**: a model capable of generating good code. I don‚Äôt have a lot
    of experience with Mistral-7B-based code models, but I found [beowolx/CodeNinja-1.0-OpenChat-7B](https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B)
    particularly good compared to others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Math model**: math is tricky for LLMs, which is why we want a model specialized
    in math. Thanks to its high MMLU and GMS8K scores, I chose [mlabonne/NeuralDaredevil-7B](https://huggingface.co/mlabonne/NeuralDaredevil-7B)
    for this purpose.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Role-play model**: The goal of this model is to write high-quality stories
    and conversations. I selected [SanjiWatsuki/Kunoichi-DPO-v2‚Äì7B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B)
    because of its good reputation and high MT-Bench score (8.51 vs. 8.30 for Mixtral).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we‚Äôve identified the experts we want to use, we can create the YAML
    configuration that MergeKit will use to create our frankenMoE. This uses the mixtral
    branch of MergeKit. You can find more information about how to write the configuration
    [on this page](https://github.com/arcee-ai/mergekit/blob/mixtral/docs/moe.md).
    Here is our version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For each expert, I provide five basic positive prompts. You can be a bit fancier
    and write entire sentences if you want. The best strategy consists of using real
    prompts that should trigger a particular expert. You can also add negative prompts
    to do the opposite.
  prefs: []
  type: TYPE_NORMAL
- en: Once this is ready, you can save your configuration as `config.yaml`. In the
    same folder, we will download and install the [mergekit](https://github.com/arcee-ai/mergekit)
    library (mixtral branch).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If your computer has enough RAM (roughly 24‚Äì32 GB of RAM), you can run the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don‚Äôt have enough RAM, you can shard the models instead as follows (it
    will take longer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command automatically downloads the experts and creates the frankenMoE
    in the `merge` directory. For the `hidden` gate mode, you can also use the `--load-in-4bit`
    and `--load-in-8bit` options to compute hidden states with lower precision.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can copy your configuration into [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y),
    a wrapper I made to simplify model merging. In this Colab notebook, you can input
    your model name, select the `mixtral` branch, specify your Hugging Face username/token,
    and run the cells. After creating your frankenMoE, it will also upload it to the
    Hugging Face Hub with a nicely formatted model card.
  prefs: []
  type: TYPE_NORMAL
- en: I called my model [Beyonder-4x7B-v3](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)
    and created [GGUF versions](https://huggingface.co/mlabonne/Beyonder-4x7B-v3-GGUF)
    of it using [AutoGGUF](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k).
    If you can‚Äôt run GGUF versions on your local machine, you can also perform inference
    using this [Colab notebook](https://colab.research.google.com/drive/1SIfwhpLttmoZxT604LGVXDOI9UKZ_1Aq?usp=sharing).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a good overview of its capabilities, it has been evaluated on three
    different benchmarks: Nous‚Äô benchmark suite, EQ-Bench, and the Open LLM Leaderboard.
    This model is not designed to excel in traditional benchmarks, as the code and
    role-playing models generally do not apply to those contexts. Nonetheless, it
    performs remarkably well thanks to strong general-purpose experts.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nous**: Beyonder-4x7B-v3 is one of the best models on Nous‚Äô benchmark suite
    (evaluation performed using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval))
    and significantly outperforms the v2\. See the entire leaderboard [here](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc0510c87454a244bd0866f260429868.png)'
  prefs: []
  type: TYPE_IMG
- en: '**EQ-Bench**: It‚Äôs also the best 4x7B model on the [EQ-Bench leaderboard](https://eqbench.com/),
    outperforming older versions of ChatGPT and Llama-2‚Äì70b-chat. Beyonder is very
    close to Mixtral-8x7B-Instruct-v0.1 and Gemini Pro, which are (supposedly) much
    bigger models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f42a3581a0111fe7994dc95948f26e92.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Open LLM Leaderboard**: Finally, it‚Äôs also a strong performer on the Open
    LLM Leaderboard, significantly outperforming the v2 model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a42d6294c40a6fc7f7467facc3c08fa.png)'
  prefs: []
  type: TYPE_IMG
- en: On top of these quantitative evaluations, I recommend checking the model‚Äôs outputs
    in a more qualitative way using a GGUF version on [LM Studio](https://lmstudio.ai/).
    A common way of testing these models is to gather a private set of questions and
    check their outputs. With this strategy, I found that Beyonder-4x7B-v3 is quite
    robust to changes in the user and system prompts compared to other models, including
    AlphaMonarch-7B. This is pretty cool as it improves the usefulness of the model
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: FrankenMoEs are a promising but still experimental approach. The trade-offs,
    like higher VRAM demand and slower inference speeds, can make it challenging to
    see their advantage over simpler merging techniques like SLERP or DARE TIES. Especially,
    when you use frankenMoEs with just two experts, they might not perform as well
    as if you had simply merged the two models. However, frankenMoEs excel in preserving
    knowledge, which can result in stronger models, as demonstrated by Beyonder-4x7B-v3\.
    With the right hardware, these drawbacks can be effectively mitigated.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced the Mixture of Experts architecture. Unlike traditional
    MoEs that are trained from scratch, MergeKit facilitates the creation of MoEs
    by ensembling experts, offering an innovative approach to improving model performance
    and efficiency. We detailed the process of creating a frankenMoE with MergeKit,
    highlighting the practical steps involved in selecting and combining different
    experts to produce a high-quality MoE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading this article. I encourage you to try to make your own FrankenMoEs
    using [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y):
    select a few models, create your config based Beyonder‚Äôs, and run the notebook
    to create your own models! If you liked this article, please follow me on [Hugging
    Face](https://huggingface.co/mlabonne) and X/Twitter [@maximelabonne](https://twitter.com/maximelabonne).'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Mixtral of Experts](https://arxiv.org/abs/2401.04088) by Jiang et al. (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mixture of Experts for Clowns](https://goddard.blog/posts/clown-moe/) by Charles
    Goddard (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mixture of Experts Explained](https://huggingface.co/blog/moe) by Sanseviero
    et al. (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Adaptive Mixture of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)
    by Jacobs et al. (1991)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints](https://arxiv.org/abs/2212.05055)
    by Komatsuzaki et al. (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learn more about machine learning and support my work with one click ‚Äî become
    a Medium member here:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
    [## Join Medium with my referral link ‚Äî Maxime Labonne'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
  prefs: []
  type: TYPE_NORMAL
