- en: Create Mixtures of Experts with MergeKit
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨MergeKitåˆ›å»ºä¸“å®¶æ··åˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27](https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27](https://towardsdatascience.com/create-mixtures-of-experts-with-mergekit-11b318c99562?source=collection_archive---------2-----------------------#2024-03-27)
- en: '*Combine multiple models into a single MoE*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*å°†å¤šä¸ªæ¨¡å‹ç»„åˆæˆä¸€ä¸ªMoE*'
- en: '[](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page---byline--11b318c99562--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    Â·9 min readÂ·Mar 27, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--11b318c99562--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2024å¹´3æœˆ27æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/aa74b784e835c9760158fe939603501a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa74b784e835c9760158fe939603501a.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Thanks to the release of Mixtral, the **Mixture of Experts** (MoE) architecture
    has become popular in recent months. This architecture offers an interesting tradeoff:
    higher performance at the cost of increased VRAM usage. While Mixtral and other
    MoE architectures are pre-trained from scratch, another method of creating MoE
    has recently appeared. Thanks to Arceeâ€™s [MergeKit](https://github.com/arcee-ai/mergekit)
    library, we now have a new way of creating MoEs by ensembling several pre-trained
    models. These are often referred to as **frankenMoEs** or **MoErges** to distinguish
    them from the pre-trained MoEs.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¾—ç›ŠäºMixtralçš„å‘å¸ƒï¼Œ**ä¸“å®¶æ··åˆ**ï¼ˆMoEï¼‰æ¶æ„åœ¨æœ€è¿‘å‡ ä¸ªæœˆå˜å¾—éå¸¸æµè¡Œã€‚è¿™ç§æ¶æ„æä¾›äº†ä¸€ç§æœ‰è¶£çš„æƒè¡¡ï¼šä»¥å¢åŠ VRAMä½¿ç”¨é‡ä¸ºä»£ä»·ï¼Œè·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚è™½ç„¶Mixtralå’Œå…¶ä»–MoEæ¶æ„æ˜¯ä»å¤´å¼€å§‹é¢„è®­ç»ƒçš„ï¼Œä½†æœ€è¿‘å‡ºç°äº†ä¸€ç§åˆ›å»ºMoEçš„å¦ä¸€ç§æ–¹æ³•ã€‚å€ŸåŠ©Arceeçš„[MergeKit](https://github.com/arcee-ai/mergekit)åº“ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡é›†æˆå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥åˆ›å»ºMoEã€‚è¿™äº›é€šå¸¸è¢«ç§°ä¸º**frankenMoE**æˆ–**MoErge**ï¼Œä»¥åŒºåˆ«äºé¢„è®­ç»ƒçš„MoEã€‚
- en: In this article, we will detail how the MoE architecture works and how frankenMoEs
    are created. Finally, we will make our [own frankenMoE](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)
    with MergeKit and evaluate it on several benchmarks. The code is available on
    Google Colab in a wrapper called [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†ä»‹ç»MoEæ¶æ„çš„å·¥ä½œåŸç†ä»¥åŠå¦‚ä½•åˆ›å»ºfrankenMoEã€‚æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨MergeKitåˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„[frankenMoE](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)å¹¶åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¯„ä¼°å®ƒã€‚ä»£ç å¯ä»¥é€šè¿‡Google
    Colabä¸­çš„ä¸€ä¸ªå°è£…ç¨‹åº[LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y)è·å–ã€‚
- en: Special thanks to [Charles Goddard](https://github.com/cg123), the creator of
    MergeKit, for proofreading this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ„Ÿè°¢[Charles Goddard](https://github.com/cg123)ï¼ŒMergeKitçš„åˆ›å»ºè€…ï¼Œæ„Ÿè°¢ä»–å¯¹æœ¬æ–‡çš„æ ¡å¯¹ã€‚
- en: ğŸ”€ Introduction to MoEs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”€ MoEç®€ä»‹
- en: A Mixture of Experts is an architecture designed for improved efficiency and
    performance. It uses multiple specialized subnetworks, known as â€œ**experts**.â€
    Unlike dense models, where the entire network is activated, MoEs only activate
    relevant experts based on the input. This results in faster training and more
    efficient inference.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“å®¶æ··åˆï¼ˆMoEï¼‰æ˜¯ä¸€ç§æ—¨åœ¨æé«˜æ•ˆç‡å’Œæ€§èƒ½çš„æ¶æ„ã€‚å®ƒä½¿ç”¨å¤šä¸ªä¸“é—¨çš„å­ç½‘ç»œï¼Œç§°ä¸ºâ€œ**ä¸“å®¶**â€ã€‚ä¸å¯†é›†å‹æ¨¡å‹ä¸åŒï¼Œåè€…ä¼šæ¿€æ´»æ•´ä¸ªç½‘ç»œï¼ŒMoEåªä¼šæ ¹æ®è¾“å…¥æ¿€æ´»ç›¸å…³çš„ä¸“å®¶ã€‚è¿™ä½¿å¾—è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ¨ç†æ›´åŠ é«˜æ•ˆã€‚
- en: 'There are two components at the core of an MoE model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MoEæ¨¡å‹çš„æ ¸å¿ƒæœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼š
- en: '**Sparse MoE Layers**: These replace the dense feed-forward network layers
    in the transformer architecture. Each MoE layer contains several experts, and
    only a subset of these experts are engaged for a given input.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¨€ç–MoEå±‚**: è¿™äº›å±‚æ›¿ä»£äº†å˜æ¢å™¨æ¶æ„ä¸­çš„å¯†é›†å‰é¦ˆç½‘ç»œå±‚ã€‚æ¯ä¸ªMoEå±‚åŒ…å«å¤šä¸ªä¸“å®¶ï¼Œè€Œæ¯æ¬¡è¾“å…¥ä»…ä¼šæ¿€æ´»å…¶ä¸­çš„ä¸€éƒ¨åˆ†ä¸“å®¶ã€‚'
- en: '**Gate Network or Router**: This component determines which tokens are processed
    by which experts, ensuring that each part of the input is handled by the most
    suitable expert(s).'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é—¨æ§ç½‘ç»œæˆ–è·¯ç”±å™¨**: è¯¥ç»„ä»¶å†³å®šå“ªäº›æ ‡è®°ç”±å“ªäº›ä¸“å®¶å¤„ç†ï¼Œç¡®ä¿è¾“å…¥çš„æ¯ä¸ªéƒ¨åˆ†éƒ½ç”±æœ€åˆé€‚çš„ä¸“å®¶å¤„ç†ã€‚'
- en: In the following example, we show how a Mistral-7B block is transformed into
    an MoE block with a sparse MoE layer (feedforward network 1, 2, and 3) and a router.
    This example represents an MoE with three experts, where two are currently engaged
    (FFN 1 and FFN 3).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å°†ä¸€ä¸ªMistral-7Bæ¨¡å—è½¬åŒ–ä¸ºå¸¦æœ‰ç¨€ç–MoEå±‚ï¼ˆå‰é¦ˆç½‘ç»œ1ã€2ã€3ï¼‰å’Œè·¯ç”±å™¨çš„MoEæ¨¡å—ã€‚è¿™ä¸ªç¤ºä¾‹ä»£è¡¨äº†ä¸€ä¸ªæ‹¥æœ‰ä¸‰ä¸ªä¸“å®¶çš„MoEï¼Œå…¶ä¸­ä¸¤ä¸ªä¸“å®¶æ­£åœ¨å‚ä¸ï¼ˆFFN
    1å’ŒFFN 3ï¼‰ã€‚
- en: '![](../Images/5ec46b2d6081bda34c1822c37edc0420.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ec46b2d6081bda34c1822c37edc0420.png)'
- en: Image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: MoEs also come with their own set of challenges, especially in terms of fine-tuning
    and memory requirements. The fine-tuning process can be difficult due to the modelâ€™s
    complexity, with the need to **balance expert usage** during training to properly
    train the gating weights to select the most relevant ones. In terms of memory,
    even though only a fraction of the total parameters are used during inference,
    the entire model, including all experts, needs to be **loaded into memory**, which
    requires high VRAM capacity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: MoEæ¨¡å‹ä¹Ÿæœ‰è‡ªå·±çš„ä¸€å¥—æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¾®è°ƒå’Œå†…å­˜éœ€æ±‚æ–¹é¢ã€‚ç”±äºæ¨¡å‹çš„å¤æ‚æ€§ï¼Œå¾®è°ƒè¿‡ç¨‹å¯èƒ½ä¼šå˜å¾—å›°éš¾ï¼Œéœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­**å¹³è¡¡ä¸“å®¶çš„ä½¿ç”¨**ï¼Œä»¥ä¾¿æ­£ç¡®è®­ç»ƒé—¨æ§æƒé‡ï¼Œä»è€Œé€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶ã€‚åœ¨å†…å­˜æ–¹é¢ï¼Œå°½ç®¡åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä»…ä½¿ç”¨äº†æ€»å‚æ•°çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…æ‹¬æ‰€æœ‰ä¸“å®¶ï¼Œéƒ½éœ€è¦**åŠ è½½åˆ°å†…å­˜ä¸­**ï¼Œè¿™éœ€è¦é«˜æ˜¾å­˜å®¹é‡ã€‚
- en: 'More specifically, there are two essential parameters when it comes to MoEs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼ŒMoEæœ‰ä¸¤ä¸ªå…³é”®å‚æ•°ï¼š
- en: '**Number of experts** (`num_local_experts`): This determines the total number
    of experts in the architecture (e.g., 8 for Mixtral). The higher the number of
    experts, the higher the VRAM usage.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸“å®¶æ•°é‡** (`num_local_experts`): è¯¥å‚æ•°å†³å®šæ¶æ„ä¸­ä¸“å®¶çš„æ€»æ•°ï¼ˆä¾‹å¦‚ï¼ŒMixtralä¸º8ï¼‰ã€‚ä¸“å®¶æ•°é‡è¶Šå¤šï¼Œæ˜¾å­˜ä½¿ç”¨è¶Šé«˜ã€‚'
- en: '**Number of experts/token** (`num_experts_per_tok`): This determines the number
    of experts that are engaged for each token and each layer (e.g., 2 for Mixtral).
    There is a tradeoff between a high number of experts per token for accuracy (but
    diminishing returns) vs. a low number for fast training and inference.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¯ä¸ªæ ‡è®°çš„ä¸“å®¶æ•°é‡** (`num_experts_per_tok`): è¯¥å‚æ•°å†³å®šæ¯ä¸ªæ ‡è®°å’Œæ¯ä¸ªå±‚æ¿€æ´»çš„ä¸“å®¶æ•°é‡ï¼ˆä¾‹å¦‚ï¼ŒMixtralä¸º2ï¼‰ã€‚åœ¨å‡†ç¡®æ€§å’Œè®­ç»ƒã€æ¨ç†é€Ÿåº¦ä¹‹é—´å­˜åœ¨æƒè¡¡ï¼šæ›´å¤šä¸“å®¶å¯ä»¥æé«˜å‡†ç¡®æ€§ï¼ˆä½†å›æŠ¥é€’å‡ï¼‰ï¼Œè€Œè¾ƒå°‘çš„ä¸“å®¶åˆ™æœ‰åŠ©äºå¿«é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚'
- en: 'Historically, MoEs have underperformed dense models. However, the release of
    [Mixtral-8x7B](https://arxiv.org/abs/2401.04088) in December 2023 shook things
    up and showed impressive performance for its size. Additionally, GPT-4 is also
    rumored to be an MoE, which would make sense as it would be a lot cheaper to run
    and train for OpenAI compared to a dense model. In addition to these recent excellent
    MoEs, we now have a new way of creating MoEs with MergeKit: frankenMoEs, also
    called MoErges.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å†å²ä¸Šï¼ŒMoEï¼ˆä¸“å®¶æ··åˆæ¨¡å‹ï¼‰è¡¨ç°ä¸å¦‚å¯†é›†æ¨¡å‹ã€‚ç„¶è€Œï¼Œ2023å¹´12æœˆå‘å¸ƒçš„[Mixtral-8x7B](https://arxiv.org/abs/2401.04088)éœ‡åŠ¨äº†è¿™ä¸€é¢†åŸŸï¼Œå¹¶å±•ç¤ºäº†å…¶åœ¨è§„æ¨¡ä¸Šçš„æƒŠäººè¡¨ç°ã€‚æ­¤å¤–ï¼ŒGPT-4ä¹Ÿè¢«ä¼ é—»ä¸ºä¸€ç§MoEæ¨¡å‹ï¼Œè¿™ä¹Ÿæœ‰å…¶é“ç†ï¼Œå› ä¸ºä¸å¯†é›†æ¨¡å‹ç›¸æ¯”ï¼Œè¿™ç§æ¨¡å‹åœ¨OpenAIè¿è¡Œå’Œè®­ç»ƒæ—¶ä¼šæ›´ä¾¿å®œã€‚é™¤äº†è¿™äº›è¿‘æœŸè¡¨ç°å‡ºè‰²çš„MoEæ¨¡å‹å¤–ï¼Œæˆ‘ä»¬ç°åœ¨è¿˜å¯ä»¥é€šè¿‡MergeKitåˆ›å»ºä¸€ç§æ–°çš„MoEæ–¹å¼ï¼šfrankenMoEï¼Œä¹Ÿè¢«ç§°ä¸ºMoErgesã€‚
- en: ğŸ§Ÿâ€â™‚ï¸ True MoEs vs. frankenMoEs
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ§Ÿâ€â™‚ï¸ çœŸæ­£çš„MoEä¸frankenMoE
- en: The main difference between true MoEs and frankenMoEs is how theyâ€™re trained.
    In the case of true MoEs, the experts and the router are trained jointly. In the
    case of frankenMoEs, we upcycle existing models and initialize the router afterward.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: çœŸæ­£çš„MoEä¸frankenMoEä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬çš„è®­ç»ƒæ–¹å¼ã€‚åœ¨çœŸæ­£çš„MoEä¸­ï¼Œä¸“å®¶å’Œè·¯ç”±å™¨æ˜¯å…±åŒè®­ç»ƒçš„ã€‚è€Œåœ¨frankenMoEä¸­ï¼Œæˆ‘ä»¬é‡æ–°åˆ©ç”¨ç°æœ‰æ¨¡å‹ï¼Œå¹¶åœ¨ä¹‹ååˆå§‹åŒ–è·¯ç”±å™¨ã€‚
- en: In other words, we copy the weights of the layer norm and self-attention layers
    from a base model, and then copy the weights of the FFN layers found in each expert.
    This means that besides the FFNs, all the other parameters are shared. This explains
    why Mixtral-8x7B with eight experts doesnâ€™t have 8*7 = 56B parameters, but about
    45B. This is also why using two experts per token gives the inference speed (FLOPs)
    of a 12B dense model instead of 14B.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬ä»åŸºç¡€æ¨¡å‹ä¸­å¤åˆ¶å±‚å½’ä¸€åŒ–å’Œè‡ªæ³¨æ„åŠ›å±‚çš„æƒé‡ï¼Œç„¶åå¤åˆ¶æ¯ä¸ªä¸“å®¶ä¸­æ‰¾åˆ°çš„FFNå±‚çš„æƒé‡ã€‚è¿™æ„å‘³ç€é™¤äº†FFNä¹‹å¤–ï¼Œæ‰€æœ‰å…¶ä»–å‚æ•°éƒ½æ˜¯å…±äº«çš„ã€‚è¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆå¸¦æœ‰å…«ä¸ªä¸“å®¶çš„Mixtral-8x7Bæ²¡æœ‰8*7
    = 56Bçš„å‚æ•°ï¼Œè€Œæ˜¯å¤§çº¦45Bã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ¯ä¸ªtokenä½¿ç”¨ä¸¤ä¸ªä¸“å®¶æ—¶ï¼Œæ¨ç†é€Ÿåº¦ï¼ˆFLOPsï¼‰ç›¸å½“äºä¸€ä¸ª12Bçš„å¯†é›†æ¨¡å‹ï¼Œè€Œä¸æ˜¯14Bçš„åŸå› ã€‚
- en: 'FrankenMoEs are about selecting the most relevant experts and initializing
    them properly. MergeKit currently implements three ways of initializing the routers:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: FrankenMoEçš„æ ¸å¿ƒæ˜¯é€‰æ‹©æœ€ç›¸å…³çš„ä¸“å®¶å¹¶æ­£ç¡®åˆå§‹åŒ–å®ƒä»¬ã€‚MergeKitç›®å‰å®ç°äº†ä¸‰ç§åˆå§‹åŒ–è·¯ç”±å™¨çš„æ–¹æ³•ï¼š
- en: '[**Random**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L139-L142):
    Random weights. Be careful when using it as the same experts might be selected
    every time (it requires further fine-tuning or `num_local_experts = num_experts_per_tok`,
    which means you don''t need any routing).'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**éšæœº**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L139-L142)ï¼šéšæœºæƒé‡ã€‚åœ¨ä½¿ç”¨æ—¶è¦å°å¿ƒï¼Œå› ä¸ºæ¯æ¬¡å¯èƒ½é€‰æ‹©ç›¸åŒçš„ä¸“å®¶ï¼ˆè¿™éœ€è¦è¿›ä¸€æ­¥çš„å¾®è°ƒæˆ–`num_local_experts
    = num_experts_per_tok`ï¼Œè¿™æ„å‘³ç€ä½ ä¸éœ€è¦ä»»ä½•è·¯ç”±ï¼‰ã€‚'
- en: '[**Cheap embed**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L91C1-L109C37):
    It uses the raw embeddings of the input tokens directly and applies the same transformation
    across all layers. This method is computationally inexpensive and suitable for
    execution on less powerful hardware.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**å»‰ä»·åµŒå…¥**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L91C1-L109C37)ï¼šç›´æ¥ä½¿ç”¨è¾“å…¥tokençš„åŸå§‹åµŒå…¥ï¼Œå¹¶å¯¹æ‰€æœ‰å±‚åº”ç”¨ç›¸åŒçš„å˜æ¢ã€‚è¯¥æ–¹æ³•è®¡ç®—å¼€é”€å°ï¼Œé€‚åˆåœ¨è¾ƒå¼±çš„ç¡¬ä»¶ä¸Šæ‰§è¡Œã€‚'
- en: '[**Hidden**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L70-L88):
    It creates hidden representations of a list of positive and negative prompts by
    extracting them from the last layer of the LLM. They are averaged and normalized
    to initialize the gates. More information about it is available on [Charles Goddardâ€™s
    blog](https://goddard.blog/posts/clown-moe/).'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**éšè—**](https://github.com/arcee-ai/mergekit/blob/9c691527f7192b5a2fc388555bfd3105e0898480/mergekit/scripts/mixtral_moe.py#L70-L88)ï¼šé€šè¿‡ä»LLMçš„æœ€åä¸€å±‚æå–ä¸€ç»„æ­£å‘å’Œè´Ÿå‘æç¤ºï¼Œåˆ›å»ºå®ƒä»¬çš„éšè—è¡¨ç¤ºã€‚è¿™äº›è¡¨ç¤ºä¼šè¢«å¹³å‡å¹¶å½’ä¸€åŒ–ä»¥åˆå§‹åŒ–é—¨æ§ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ[Charles
    Goddardçš„åšå®¢](https://goddard.blog/posts/clown-moe/)ã€‚'
- en: As you can guess, the â€œhiddenâ€ initialization is the most efficient to correctly
    route the tokens to the most relevant experts. In the next section, we will create
    our own frankenMoE using this technique.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€çŒœæµ‹ï¼Œâ€œéšè—â€åˆå§‹åŒ–æ˜¯æœ€æœ‰æ•ˆçš„ï¼Œå®ƒèƒ½å¤Ÿæ­£ç¡®åœ°å°†tokenè·¯ç”±åˆ°æœ€ç›¸å…³çš„ä¸“å®¶ã€‚åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸€æŠ€æœ¯åˆ›å»ºè‡ªå·±çš„frankenMoEã€‚
- en: ğŸ’» Creating a frankenMoE
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ’» åˆ›å»ºä¸€ä¸ªfrankenMoE
- en: To create our frankenMoE, we need to select `n` experts. In this case, we will
    rely on Mistral-7B thanks to its popularity and relatively small size. However,
    eight experts like in Mixtral is quite a lot, as we need to fit all of them in
    memory. For efficiency, I'll only use four experts in this example, with two of
    them engaged for each token and each layer. In this case, we will end up with
    a model with 24.2B parameters instead of 4*7 = 28B parameters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åˆ›å»ºæˆ‘ä»¬çš„frankenMoEï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©`n`ä¸ªä¸“å®¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä¾é Mistral-7Bï¼Œå› ä¸ºå®ƒéå¸¸æµè¡Œä¸”ç›¸å¯¹è¾ƒå°ã€‚ç„¶è€Œï¼ŒåƒMixtralä¸­çš„å…«ä¸ªä¸“å®¶å®é™…ä¸Šæœ‰ç‚¹å¤šï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬å…¨éƒ¨è£…å…¥å†…å­˜ã€‚ä¸ºäº†æé«˜æ•ˆç‡ï¼Œæœ¬ä¾‹ä¸­æˆ‘å°†åªä½¿ç”¨å››ä¸ªä¸“å®¶ï¼Œå…¶ä¸­æ¯ä¸ªtokenå’Œæ¯å±‚ä½¿ç”¨ä¸¤ä¸ªä¸“å®¶ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°ä¸€ä¸ªå…·æœ‰24.2Bå‚æ•°çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯4*7
    = 28Bçš„å‚æ•°ã€‚
- en: 'Here, our goal is to create a well-rounded model that can do pretty much everything:
    write stories, explain articles, code in Python, etc. We can decompose this requirement
    into four tasks and select the best expert for each of them. This is how I decomposed
    it:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªå…¨èƒ½æ¨¡å‹ï¼Œå¯ä»¥å®Œæˆå‡ ä¹æ‰€æœ‰ä»»åŠ¡ï¼šå†™æ•…äº‹ã€è§£é‡Šæ–‡ç« ã€ç¼–å†™Pythonä»£ç ç­‰ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªéœ€æ±‚åˆ†è§£æˆå››ä¸ªä»»åŠ¡ï¼Œå¹¶ä¸ºæ¯ä¸ªä»»åŠ¡é€‰æ‹©æœ€ä½³çš„ä¸“å®¶ã€‚ä»¥ä¸‹æ˜¯æˆ‘å¦‚ä½•è¿›è¡Œåˆ†è§£çš„ï¼š
- en: '**Chat model**: a general-purpose model that is used in most interactions.
    I used [mlabonne/AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B),
    which perfectly satisfies the requirements.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠå¤©æ¨¡å‹**ï¼šä¸€ç§ç”¨äºå¤§å¤šæ•°äº¤äº’çš„é€šç”¨æ¨¡å‹ã€‚æˆ‘ä½¿ç”¨äº†[mlabonne/AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B)ï¼Œå®ƒå®Œç¾æ»¡è¶³äº†éœ€æ±‚ã€‚'
- en: '**Code model**: a model capable of generating good code. I donâ€™t have a lot
    of experience with Mistral-7B-based code models, but I found [beowolx/CodeNinja-1.0-OpenChat-7B](https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B)
    particularly good compared to others.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»£ç æ¨¡å‹**ï¼šä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆè‰¯å¥½ä»£ç çš„æ¨¡å‹ã€‚æˆ‘æ²¡æœ‰å¤ªå¤šå…³äºåŸºäºMistral-7Bçš„ä»£ç æ¨¡å‹çš„ç»éªŒï¼Œä½†ç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼Œæˆ‘å‘ç°[beowolx/CodeNinja-1.0-OpenChat-7B](https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B)ç‰¹åˆ«å‡ºè‰²ã€‚'
- en: '**Math model**: math is tricky for LLMs, which is why we want a model specialized
    in math. Thanks to its high MMLU and GMS8K scores, I chose [mlabonne/NeuralDaredevil-7B](https://huggingface.co/mlabonne/NeuralDaredevil-7B)
    for this purpose.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°å­¦æ¨¡å‹**ï¼šæ•°å­¦å¯¹äºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥è¯´å¾ˆæ£˜æ‰‹ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¸“é—¨å¤„ç†æ•°å­¦çš„æ¨¡å‹ã€‚ç”±äºå…¶é«˜ MMLU å’Œ GMS8K åˆ†æ•°ï¼Œæˆ‘é€‰æ‹©äº†[mlabonne/NeuralDaredevil-7B](https://huggingface.co/mlabonne/NeuralDaredevil-7B)ä½œä¸ºè¿™ä¸ªç›®çš„çš„æ¨¡å‹ã€‚'
- en: '**Role-play model**: The goal of this model is to write high-quality stories
    and conversations. I selected [SanjiWatsuki/Kunoichi-DPO-v2â€“7B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B)
    because of its good reputation and high MT-Bench score (8.51 vs. 8.30 for Mixtral).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§’è‰²æ‰®æ¼”æ¨¡å‹**ï¼šè¿™ä¸ªæ¨¡å‹çš„ç›®æ ‡æ˜¯ç¼–å†™é«˜è´¨é‡çš„æ•…äº‹å’Œå¯¹è¯ã€‚æˆ‘é€‰æ‹©äº†[SanjiWatsuki/Kunoichi-DPO-v2â€“7B](https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B)ï¼Œå› ä¸ºå®ƒæœ‰è‰¯å¥½çš„å£°èª‰å’Œé«˜MT-Benchåˆ†æ•°ï¼ˆ8.51
    vs. Mixtralçš„8.30ï¼‰ã€‚'
- en: 'Now that weâ€™ve identified the experts we want to use, we can create the YAML
    configuration that MergeKit will use to create our frankenMoE. This uses the mixtral
    branch of MergeKit. You can find more information about how to write the configuration
    [on this page](https://github.com/arcee-ai/mergekit/blob/mixtral/docs/moe.md).
    Here is our version:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç¡®å®šäº†è¦ä½¿ç”¨çš„ä¸“å®¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºMergeKitå°†ç”¨æ¥åˆ›å»ºfrankenMoEçš„YAMLé…ç½®æ–‡ä»¶ã€‚è¿™ä½¿ç”¨äº†MergeKitçš„mixtralåˆ†æ”¯ã€‚ä½ å¯ä»¥åœ¨[è¿™ä¸ªé¡µé¢](https://github.com/arcee-ai/mergekit/blob/mixtral/docs/moe.md)ä¸Šæ‰¾åˆ°æœ‰å…³å¦‚ä½•ç¼–å†™é…ç½®çš„æ›´å¤šä¿¡æ¯ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„ç‰ˆæœ¬ï¼š
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For each expert, I provide five basic positive prompts. You can be a bit fancier
    and write entire sentences if you want. The best strategy consists of using real
    prompts that should trigger a particular expert. You can also add negative prompts
    to do the opposite.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªä¸“å®¶ï¼Œæˆ‘æä¾›äº†äº”ä¸ªåŸºæœ¬çš„æ­£å‘æç¤ºã€‚å¦‚æœä½ æ„¿æ„ï¼Œä¹Ÿå¯ä»¥å†™å®Œæ•´çš„å¥å­ã€‚æœ€å¥½çš„ç­–ç•¥æ˜¯ä½¿ç”¨èƒ½å¤Ÿè§¦å‘ç‰¹å®šä¸“å®¶çš„çœŸå®æç¤ºã€‚ä½ ä¹Ÿå¯ä»¥æ·»åŠ è´Ÿå‘æç¤ºä»¥è¾¾åˆ°ç›¸åæ•ˆæœã€‚
- en: Once this is ready, you can save your configuration as `config.yaml`. In the
    same folder, we will download and install the [mergekit](https://github.com/arcee-ai/mergekit)
    library (mixtral branch).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å‡†å¤‡å¥½ï¼Œä½ å¯ä»¥å°†é…ç½®ä¿å­˜ä¸º`config.yaml`ã€‚åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸­ï¼Œæˆ‘ä»¬å°†ä¸‹è½½å¹¶å®‰è£…[mergekit](https://github.com/arcee-ai/mergekit)åº“ï¼ˆmixtralåˆ†æ”¯ï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If your computer has enough RAM (roughly 24â€“32 GB of RAM), you can run the
    following command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çš„è®¡ç®—æœºæœ‰è¶³å¤Ÿçš„RAMï¼ˆå¤§çº¦24â€“32 GBçš„RAMï¼‰ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you donâ€™t have enough RAM, you can shard the models instead as follows (it
    will take longer):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ²¡æœ‰è¶³å¤Ÿçš„RAMï¼Œä½ å¯ä»¥å°è¯•å°†æ¨¡å‹åˆ†ç‰‡ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼ˆè¿™ä¼šèŠ±è´¹æ›´é•¿æ—¶é—´ï¼‰ï¼š
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command automatically downloads the experts and creates the frankenMoE
    in the `merge` directory. For the `hidden` gate mode, you can also use the `--load-in-4bit`
    and `--load-in-8bit` options to compute hidden states with lower precision.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‘½ä»¤ä¼šè‡ªåŠ¨ä¸‹è½½ä¸“å®¶æ¨¡å‹ï¼Œå¹¶åœ¨`merge`ç›®å½•ä¸­åˆ›å»ºfrankenMoEã€‚å¯¹äº`hidden`é—¨æ¨¡å¼ï¼Œä½ è¿˜å¯ä»¥ä½¿ç”¨`--load-in-4bit`å’Œ`--load-in-8bit`é€‰é¡¹ï¼Œä»¥æ›´ä½çš„ç²¾åº¦è®¡ç®—éšè—çŠ¶æ€ã€‚
- en: Alternatively, you can copy your configuration into [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y),
    a wrapper I made to simplify model merging. In this Colab notebook, you can input
    your model name, select the `mixtral` branch, specify your Hugging Face username/token,
    and run the cells. After creating your frankenMoE, it will also upload it to the
    Hugging Face Hub with a nicely formatted model card.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œä½ å¯ä»¥å°†ä½ çš„é…ç½®å¤åˆ¶åˆ°[LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y)ä¸­ï¼Œè¿™æ˜¯æˆ‘ä¸ºç®€åŒ–æ¨¡å‹åˆå¹¶è€Œåˆ¶ä½œçš„ä¸€ä¸ªåŒ…è£…å·¥å…·ã€‚åœ¨è¿™ä¸ªColabç¬”è®°æœ¬ä¸­ï¼Œä½ å¯ä»¥è¾“å…¥æ¨¡å‹åç§°ï¼Œé€‰æ‹©`mixtral`åˆ†æ”¯ï¼ŒæŒ‡å®šä½ çš„
    Hugging Face ç”¨æˆ·å/ä»¤ç‰Œï¼Œå¹¶è¿è¡Œç›¸åº”çš„å•å…ƒæ ¼ã€‚åœ¨åˆ›å»ºå®ŒfrankenMoEåï¼Œå®ƒè¿˜ä¼šå°†æ¨¡å‹ä¸Šä¼ åˆ° Hugging Face Hubï¼Œå¹¶é™„å¸¦ä¸€ä¸ªæ ¼å¼åŒ–è‰¯å¥½çš„æ¨¡å‹å¡ã€‚
- en: I called my model [Beyonder-4x7B-v3](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)
    and created [GGUF versions](https://huggingface.co/mlabonne/Beyonder-4x7B-v3-GGUF)
    of it using [AutoGGUF](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k).
    If you canâ€™t run GGUF versions on your local machine, you can also perform inference
    using this [Colab notebook](https://colab.research.google.com/drive/1SIfwhpLttmoZxT604LGVXDOI9UKZ_1Aq?usp=sharing).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æˆ‘çš„æ¨¡å‹å‘½åä¸º[Beyonder-4x7B-v3](https://huggingface.co/mlabonne/Beyonder-4x7B-v3)ï¼Œå¹¶ä½¿ç”¨[AutoGGUF](https://colab.research.google.com/drive/1P646NEg33BZy4BfLDNpTz0V0lwIU3CHu#scrollTo=fD24jJxq7t3k)åˆ›å»ºäº†[GGUFç‰ˆæœ¬](https://huggingface.co/mlabonne/Beyonder-4x7B-v3-GGUF)ã€‚å¦‚æœä½ æ— æ³•åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡ŒGGUFç‰ˆæœ¬ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ª[Colabç¬”è®°æœ¬](https://colab.research.google.com/drive/1SIfwhpLttmoZxT604LGVXDOI9UKZ_1Aq?usp=sharing)è¿›è¡Œæ¨ç†ã€‚
- en: 'To get a good overview of its capabilities, it has been evaluated on three
    different benchmarks: Nousâ€™ benchmark suite, EQ-Bench, and the Open LLM Leaderboard.
    This model is not designed to excel in traditional benchmarks, as the code and
    role-playing models generally do not apply to those contexts. Nonetheless, it
    performs remarkably well thanks to strong general-purpose experts.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å…¨é¢äº†è§£å…¶èƒ½åŠ›ï¼Œè¯¥æ¨¡å‹å·²ç»åœ¨ä¸‰ä¸ªä¸åŒçš„åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šNousçš„åŸºå‡†å¥—ä»¶ã€EQ-Benchå’ŒOpen LLMæ’è¡Œæ¦œã€‚è¿™ä¸ªæ¨¡å‹å¹¶ä¸æ˜¯ä¸ºäº†åœ¨ä¼ ç»ŸåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°çªå‡ºè€Œè®¾è®¡çš„ï¼Œå› ä¸ºä»£ç å’Œè§’è‰²æ‰®æ¼”æ¨¡å‹é€šå¸¸ä¸é€‚ç”¨äºè¿™äº›ä¸Šä¸‹æ–‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå‡­å€Ÿå¼ºå¤§çš„é€šç”¨ä¸“å®¶ï¼Œå®ƒçš„è¡¨ç°ä¾ç„¶éå¸¸å‡ºè‰²ã€‚
- en: '**Nous**: Beyonder-4x7B-v3 is one of the best models on Nousâ€™ benchmark suite
    (evaluation performed using [LLM AutoEval](https://github.com/mlabonne/llm-autoeval))
    and significantly outperforms the v2\. See the entire leaderboard [here](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nous**ï¼šBeyonder-4x7B-v3æ˜¯NousåŸºå‡†å¥—ä»¶ä¸­æœ€å¥½çš„æ¨¡å‹ä¹‹ä¸€ï¼ˆè¯„ä¼°ä½¿ç”¨[LLM AutoEval](https://github.com/mlabonne/llm-autoeval)è¿›è¡Œï¼‰ï¼Œå¹¶ä¸”æ˜æ˜¾è¶…è¶Šäº†v2ç‰ˆæœ¬ã€‚æŸ¥çœ‹å®Œæ•´çš„æ’è¡Œæ¦œ[è¿™é‡Œ](https://huggingface.co/spaces/mlabonne/Yet_Another_LLM_Leaderboard)ã€‚'
- en: '![](../Images/cc0510c87454a244bd0866f260429868.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc0510c87454a244bd0866f260429868.png)'
- en: '**EQ-Bench**: Itâ€™s also the best 4x7B model on the [EQ-Bench leaderboard](https://eqbench.com/),
    outperforming older versions of ChatGPT and Llama-2â€“70b-chat. Beyonder is very
    close to Mixtral-8x7B-Instruct-v0.1 and Gemini Pro, which are (supposedly) much
    bigger models.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**EQ-Bench**ï¼šå®ƒè¿˜æ˜¯[EQ-Benchæ’è¡Œæ¦œ](https://eqbench.com/)ä¸Šè¡¨ç°æœ€å¥½çš„4x7Bæ¨¡å‹ï¼Œè¶…è¶Šäº†æ—§ç‰ˆæœ¬çš„ChatGPTå’ŒLlama-2â€“70b-chatã€‚Beyonderä¸Mixtral-8x7B-Instruct-v0.1å’ŒGemini
    Proéå¸¸æ¥è¿‘ï¼Œåè€…ï¼ˆæ®è¯´ï¼‰æ˜¯æ›´å¤§çš„æ¨¡å‹ã€‚'
- en: '![](../Images/f42a3581a0111fe7994dc95948f26e92.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f42a3581a0111fe7994dc95948f26e92.png)'
- en: '**Open LLM Leaderboard**: Finally, itâ€™s also a strong performer on the Open
    LLM Leaderboard, significantly outperforming the v2 model.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Open LLMæ’è¡Œæ¦œ**ï¼šæœ€åï¼Œå®ƒåœ¨Open LLMæ’è¡Œæ¦œä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„è¡¨ç°è€…ï¼Œæ˜æ˜¾è¶…è¶Šäº†v2æ¨¡å‹ã€‚'
- en: '![](../Images/1a42d6294c40a6fc7f7467facc3c08fa.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a42d6294c40a6fc7f7467facc3c08fa.png)'
- en: On top of these quantitative evaluations, I recommend checking the modelâ€™s outputs
    in a more qualitative way using a GGUF version on [LM Studio](https://lmstudio.ai/).
    A common way of testing these models is to gather a private set of questions and
    check their outputs. With this strategy, I found that Beyonder-4x7B-v3 is quite
    robust to changes in the user and system prompts compared to other models, including
    AlphaMonarch-7B. This is pretty cool as it improves the usefulness of the model
    in general.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è¿™äº›å®šé‡è¯„ä¼°ï¼Œæˆ‘å»ºè®®ä½¿ç”¨[LM Studio](https://lmstudio.ai/)ä¸Šçš„GGUFç‰ˆæœ¬ï¼Œä»¥æ›´å®šæ€§çš„æ–¹å¼æ£€æŸ¥æ¨¡å‹çš„è¾“å‡ºã€‚æµ‹è¯•è¿™äº›æ¨¡å‹çš„å¸¸è§æ–¹æ³•æ˜¯æ”¶é›†ä¸€ç»„ç§äººé—®é¢˜å¹¶æ£€æŸ¥å…¶è¾“å‡ºã€‚é€šè¿‡è¿™ç§ç­–ç•¥ï¼Œæˆ‘å‘ç°Beyonder-4x7B-v3ç›¸æ¯”å…¶ä»–æ¨¡å‹ï¼ŒåŒ…æ‹¬AlphaMonarch-7Bï¼Œå¯¹äºç”¨æˆ·å’Œç³»ç»Ÿæç¤ºçš„å˜åŒ–è¡¨ç°å¾—ç›¸å½“ç¨³å¥ã€‚è¿™éå¸¸æ£’ï¼Œå› ä¸ºå®ƒæå‡äº†æ¨¡å‹çš„ä¸€èˆ¬å®ç”¨æ€§ã€‚
- en: FrankenMoEs are a promising but still experimental approach. The trade-offs,
    like higher VRAM demand and slower inference speeds, can make it challenging to
    see their advantage over simpler merging techniques like SLERP or DARE TIES. Especially,
    when you use frankenMoEs with just two experts, they might not perform as well
    as if you had simply merged the two models. However, frankenMoEs excel in preserving
    knowledge, which can result in stronger models, as demonstrated by Beyonder-4x7B-v3\.
    With the right hardware, these drawbacks can be effectively mitigated.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: FrankenMoEæ˜¯ä¸€ä¸ªæœ‰å‰æ™¯ä½†ä»å¤„äºå®éªŒé˜¶æ®µçš„æ–¹æ³•ã€‚å®ƒçš„æƒè¡¡ï¼Œå¦‚æ›´é«˜çš„æ˜¾å­˜éœ€æ±‚å’Œæ›´æ…¢çš„æ¨ç†é€Ÿåº¦ï¼Œå¯èƒ½ä¼šä½¿å¾—å®ƒåœ¨ä¸åƒSLERPæˆ–DARE TIESè¿™æ ·çš„ç®€å•èåˆæŠ€æœ¯ç›¸æ¯”æ—¶éš¾ä»¥æ˜¾ç°ä¼˜åŠ¿ã€‚å°¤å…¶æ˜¯å½“ä½ ä»…ä½¿ç”¨ä¸¤ä¸ªä¸“å®¶æ—¶ï¼Œå®ƒä»¬çš„è¡¨ç°å¯èƒ½ä¸å¦‚ç›´æ¥å°†è¿™ä¸¤ä¸ªæ¨¡å‹èåˆã€‚ç„¶è€Œï¼ŒfrankenMoEsåœ¨ä¿æŒçŸ¥è¯†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œè¿™å¯ä»¥ä½¿å¾—æ¨¡å‹æ›´å¼ºå¤§ï¼Œæ­£å¦‚Beyonder-4x7B-v3æ‰€å±•ç¤ºçš„é‚£æ ·ã€‚åªè¦æœ‰åˆé€‚çš„ç¡¬ä»¶ï¼Œè¿™äº›ç¼ºç‚¹å¯ä»¥æœ‰æ•ˆåœ°ç¼“è§£ã€‚
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we introduced the Mixture of Experts architecture. Unlike traditional
    MoEs that are trained from scratch, MergeKit facilitates the creation of MoEs
    by ensembling experts, offering an innovative approach to improving model performance
    and efficiency. We detailed the process of creating a frankenMoE with MergeKit,
    highlighting the practical steps involved in selecting and combining different
    experts to produce a high-quality MoE.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸“å®¶æ··åˆæ¶æ„ï¼ˆMixture of Expertsï¼‰ã€‚ä¸ä»é›¶å¼€å§‹è®­ç»ƒçš„ä¼ ç»ŸMoEä¸åŒï¼ŒMergeKité€šè¿‡é›†åˆä¸“å®¶æ¥ä¿ƒè¿›MoEçš„åˆ›å»ºï¼Œæä¾›äº†ä¸€ç§åˆ›æ–°çš„æ–¹æ³•æ¥æå‡æ¨¡å‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚æˆ‘ä»¬è¯¦ç»†è®²è§£äº†ä½¿ç”¨MergeKitåˆ›å»ºfrankenMoEçš„è¿‡ç¨‹ï¼Œå¼ºè°ƒäº†é€‰æ‹©å’Œç»„åˆä¸åŒä¸“å®¶ä»¥ç”Ÿæˆé«˜è´¨é‡MoEçš„å®é™…æ­¥éª¤ã€‚
- en: 'Thanks for reading this article. I encourage you to try to make your own FrankenMoEs
    using [LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y):
    select a few models, create your config based Beyonderâ€™s, and run the notebook
    to create your own models! If you liked this article, please follow me on [Hugging
    Face](https://huggingface.co/mlabonne) and X/Twitter [@maximelabonne](https://twitter.com/maximelabonne).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»æœ¬æ–‡ã€‚æˆ‘é¼“åŠ±ä½ å°è¯•ä½¿ç”¨[LazyMergeKit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb#scrollTo=d5mYzDo1q96y)ï¼šé€‰æ‹©å‡ ä¸ªæ¨¡å‹ï¼Œåˆ›å»ºåŸºäºBeyonderçš„é…ç½®ï¼Œå¹¶è¿è¡Œç¬”è®°æœ¬æ¥åˆ›å»ºä½ è‡ªå·±çš„æ¨¡å‹ï¼å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨[Hugging
    Face](https://huggingface.co/mlabonne)ä¸Šå…³æ³¨æˆ‘ï¼Œå¹¶åœ¨X/Twitterä¸Šå…³æ³¨[@maximelabonne](https://twitter.com/maximelabonne)ã€‚
- en: References
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Mixtral of Experts](https://arxiv.org/abs/2401.04088) by Jiang et al. (2023)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mixtralä¸“å®¶æ··åˆæ¨¡å‹](https://arxiv.org/abs/2401.04088) ç”±Jiangç­‰äººï¼ˆ2023å¹´ï¼‰æ’°å†™'
- en: '[Mixture of Experts for Clowns](https://goddard.blog/posts/clown-moe/) by Charles
    Goddard (2023)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å°ä¸‘çš„ä¸“å®¶æ··åˆæ¨¡å‹](https://goddard.blog/posts/clown-moe/) ç”±Charles Goddardï¼ˆ2023å¹´ï¼‰æ’°å†™'
- en: '[Mixture of Experts Explained](https://huggingface.co/blog/moe) by Sanseviero
    et al. (2023)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä¸“å®¶æ··åˆæ¨¡å‹è§£æ](https://huggingface.co/blog/moe) ç”±Sansevieroç­‰äººï¼ˆ2023å¹´ï¼‰æ’°å†™'
- en: '[Adaptive Mixture of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)
    by Jacobs et al. (1991)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å±€éƒ¨ä¸“å®¶çš„è‡ªé€‚åº”æ··åˆæ¨¡å‹](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) ç”±Jacobsç­‰äººï¼ˆ1991å¹´ï¼‰æ’°å†™'
- en: '[Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints](https://arxiv.org/abs/2212.05055)
    by Komatsuzaki et al. (2022)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ç¨€ç–ä¸Šé‡‡æ ·ï¼šä»å¯†é›†æ£€æŸ¥ç‚¹è®­ç»ƒä¸“å®¶æ··åˆæ¨¡å‹](https://arxiv.org/abs/2212.05055) ç”±Komatsuzakiç­‰äººï¼ˆ2022å¹´ï¼‰æ’°å†™'
- en: '*Learn more about machine learning and support my work with one click â€” become
    a Medium member here:*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç‚¹å‡»æ­¤å¤„äº†è§£æ›´å¤šå…³äºæœºå™¨å­¦ä¹ çš„å†…å®¹å¹¶æ”¯æŒæˆ‘çš„å·¥ä½œâ€”â€”æˆä¸ºMediumä¼šå‘˜ï¼š*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
    [## Join Medium with my referral link â€” Maxime Labonne'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥Medium â€” Maxime Labonne'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸ºMediumä¼šå‘˜ï¼Œä½ çš„éƒ¨åˆ†ä¼šå‘˜è´¹ç”¨å°†ç”¨äºæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼Œå¹¶ä¸”ä½ å°†å¯ä»¥å®Œå…¨è®¿é—®æ¯ä¸€ç¯‡æ•…äº‹â€¦
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----11b318c99562--------------------------------)
