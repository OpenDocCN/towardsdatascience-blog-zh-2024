- en: Building an Image Similarity Search Engine with FAISS and CLIP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23](https://towardsdatascience.com/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?source=collection_archive---------3-----------------------#2024-08-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A guided tutorial explaining how to search your image dataset with text or photo
    queries, using CLIP embedding and FAISS indexing.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--2211126d08fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2211126d08fa--------------------------------)
    ·6 min read·Aug 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6aa0f89a4ae8ff874b4620b8a4ef873b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image was generated by author on Flux-Pro platform
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wanted to find an image among your never-ending image dataset,
    but found it too tedious? In this tutorial we’ll build an image similarity search
    engine to easily find images using either a text query or a reference image. For
    your convenience, the complete code for this tutorial is provided at the bottom
    of the article as a **Colab notebook**.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have a paid Medium account, you can read for free[here](/building-an-image-similarity-search-engine-with-faiss-and-clip-2211126d08fa?sk=4d3ed082bd53b0e2ada2f660bd0da5ad).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pipeline Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The semantic meaning of an image can be represented by a numerical vector called
    an embedding. Comparing these low-dimensional embedding vectors, rather than the
    raw images, allows for efficient similarity searches. For each image in the dataset,
    we’ll create an embedding vector and store it in an index. When a text query or
    a reference image is provided, its embedding is generated and compared against
    the indexed embeddings to retrieve the most similar images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a brief overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedding:** The embeddings of the images are extracted using the CLIP model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Indexing**: The embeddings are stored as a FAISS index.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieval**: With FAISS, The embedding of the query is compared against the
    indexed embeddings to retrieve the most similar images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CLIP Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CLIP (Contrastive Language-Image Pre-training) model, developed by OpenAI,
    is a multi-modal vision and language model that maps images and text to the same
    latent space. Since we will use both image and text queries to search for images,
    we will use the CLIP model to embed our data. For further reading about CLIP,
    you can check out my previous article [here](/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1).
  prefs: []
  type: TYPE_NORMAL
- en: FAISS Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FAISS (Facebook AI Similarity Search) is an open-source library developed by
    Meta. It is built around the Index object that stores the database embedding vectors.
    FAISS enables efficient similarity search and clustering of dense vectors, and
    we will use it to index our dataset and retrieve the photos that resemble to the
    query.
  prefs: []
  type: TYPE_NORMAL
- en: Code Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Step 1 — Dataset Exploration**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create the image dataset for this tutorial I collected 52 images of varied
    topics from [Pexels](https://www.pexels.com/). To get the feeling, lets observe
    10 random images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5783da36e449663c611c2569c8921c43.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Step 2 — Extract CLIP Embeddings from the Image Dataset**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To extract CLIP embeddings, we‘ll first load the CLIP model using the HuggingFace
    SentenceTransformer library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll create a function that iterates through our dataset directory with
    `glob`, opens each image with `PIL Image.open`, and generates an embedding vector
    for each image with `CLIP model.encode`. It returns a list of the embedding vectors
    and a list of the paths of our images dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3 — Generate FAISS Index**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next step is to create a FAISS index from the embedding vectors list. FAISS
    offers various distance metrics for similarity search, including Inner Product
    (IP) and L2 (Euclidean) distance.
  prefs: []
  type: TYPE_NORMAL
- en: FAISS also offers various indexing options. It can use approximation or compression
    technique to handle large datasets efficiently while balancing search speed and
    accuracy. In this tutorial we will use a ‘Flat’ index, which performs a brute-force
    search by comparing the query vector against every single vector in the dataset,
    ensuring exact results at the cost of higher computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `faiss.IndexFlatIP` initializes an Index for Inner Product similarity, wrapped
    in an `faiss.IndexIDMap` to associate each vector with an ID. Next, the `index.add_with_ids`
    adds the vectors to the index with sequential ID’s, and the index is saved to
    disk along with the image paths.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index can be used immediately or saved to disk for future use .To load
    the FAISS index we will use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 4 — Retrieve Images by a Text Query or a Reference Image**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With our FAISS index built, we can now retrieve images using either text queries
    or reference images. If the query is an image path, the query is opened with `PIL
    Image.open`. Next, the query embedding vector is extracted with `CLIP model.encode`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Retrieval is happening on the `index.search` method. It implements a k-Nearest
    Neighbors (kNN) search to find the `k` most similar vectors to the query vector.
    We can adjust the value of k by changing the `top_k` parameter. The distance metric
    used in the kNN search in our implementation is the cosine similarity. The function
    returns the query and a list of retrieve images paths.
  prefs: []
  type: TYPE_NORMAL
- en: '**Search with a Text Query:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we are ready to examine the search results. The helper function `visualize_results`
    displays the results. You can fined it in the associated Colab notebook. Lets
    explore the retrieved most similar 3 images for the text query “ball” for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0982e9cc9e59715d51147782f62d60f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Retrieved images with the query: ‘a ball’'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the query ‘animal’ we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6beffd16ea96ed9928100a9d117bcb0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Retrieved images with the query: ‘animal’'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search with a Reference Image:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/015aeddcd340d75270dd41904b56d11d.png)'
  prefs: []
  type: TYPE_IMG
- en: Query and Retrieved images
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we get pretty cool results for an off-the-shelf pre-trained model.
    When we searched by a reference image of an eye painting, besides finding the
    original image, it found one match of eyeglass and one of a different painting.
    This demonstrates different aspects of the semantic meaning of the query image.
  prefs: []
  type: TYPE_NORMAL
- en: You can try other queries on the provided Colab notebook to see how the model
    performs with different text and image inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial we built a basic image similarity search engine using CLIP
    and FAISS. The retrieved images shared similar semantic meaning with the query,
    indicating the effectiveness of the approach. Though CLIP shows nice results for
    a Zero Shot model, it might exhibit low performance on Out-of-Distribution data,
    Fine-Grained tasks and inherit the natural bias of the data it was trained on.
    To overcome these limitations you can try other CLIP-like pre-trained models as
    in [OpenClip](https://github.com/mlfoundations/open_clip/tree/main), or fine-tune
    CLIP on your own custom dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it all the way here. Click 👍 to show your appreciation
    and raise the algorithm self esteem 🤓
  prefs: []
  type: TYPE_NORMAL
- en: '**Want to learn more?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full Code as Colab notebook:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Colab Notebook [Link](https://gist.github.com/Lihi-Gur-Arie/7cac63dbffde55449d2444e402d87bfc)
  prefs: []
  type: TYPE_NORMAL
