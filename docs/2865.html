<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Introducing ft-Q: Improving Vector Compression with Feature-Level Quantization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Introducing ft-Q: Improving Vector Compression with Feature-Level Quantization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introducing-ft-q-improving-vector-compression-with-feature-level-quantization-3c18470ed2ee?source=collection_archive---------6-----------------------#2024-11-26">https://towardsdatascience.com/introducing-ft-q-improving-vector-compression-with-feature-level-quantization-3c18470ed2ee?source=collection_archive---------6-----------------------#2024-11-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="01b3" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Quantization</h2><div/><div><h2 id="2117" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Pushing quantization to its limits by performing it at the feature level with ft-Quantization (ft-Q)</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ardito.bryan?source=post_page---byline--3c18470ed2ee--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Michelangiolo Mazzeschi" class="l ep by dd de cx" src="../Images/9211748ac638d2ed07679ac73ea17296.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3c18470ed2ee--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@ardito.bryan?source=post_page---byline--3c18470ed2ee--------------------------------" rel="noopener follow">Michelangiolo Mazzeschi</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3c18470ed2ee--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">2</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="314e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><em class="nk">***To understand this article, knowledge of </em><strong class="mq ga"><em class="nk">embeddings </em></strong><em class="nk">and </em><strong class="mq ga"><em class="nk">basic quantization </em></strong><em class="nk">is required. The implementation of this algorithm has been released on </em><a class="af nl" href="https://github.com/atlantis-nova/ft-Q" rel="noopener ugc nofollow" target="_blank"><em class="nk">GitHub</em></a><em class="nk"> and is fully open-source.</em></p><p id="baa6" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga"><em class="nk">UPDATE 24–20–12:</em></strong><em class="nk"> it has come to my attention that a similar approach already exists and is available in the sentence-transformers library under the parameters of </em><a class="af nl" href="https://sbert.net/docs/package_reference/sentence_transformer/quantization.html" rel="noopener ugc nofollow" target="_blank"><em class="nk">calibration embeddings</em></a><em class="nk"> (please cite the correct authors). However, when applied to binary quantization it is still wrong, as the </em><strong class="mq ga"><em class="nk">median</em> </strong><em class="nk">should be utilized to split the distribution in two perfect halves</em></p><p id="a004" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Since the dawn of LLMs, quantization has become one of the most popular memory-saving techniques for production-ready applications. Not long after, it has been popularized across vector databases, which have started using the same technology for compressing not only models but also vectors for retrieval purposes.</p><p id="ed99" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In this article, I will showcase the limitations of the current quantization algorithms and propose a <strong class="mq ga">new quantization approach (ft-Q)</strong> to address them.</p><h1 id="8932" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">What is Quantization and how does it work?</h1><p id="48be" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">Quantization is a <strong class="mq ga">memory-saving algorithm</strong> that lets us store numbers (both in-memory and in-disk) using a lower amount of bits. By default, when we store any number in memory, we use float32: this means that this number is stored using a combination of 32-bits (binary elements).</p><p id="a73d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For example, the integer 40 is stored as follows in a 32-bit object:</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo op"><img src="../Images/3de0fab2d22fc79957fb8000d8d713e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cWDZTyXwJV3Ri2f9GORKw.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">storing the number 40 in a 32-bit object, <strong class="bf no">image by Author</strong></figcaption></figure><p id="4764" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, we could decide to store the same number using fewer bits (cutting by half the memory usage), with a 16-bit object:</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo pg"><img src="../Images/bd0c31063855a84aa94e254701938943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mAubcQdd8JPei0n5STH2Qw.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">storing the number 40 in a 16-bit object, <strong class="bf no">image by Author</strong></figcaption></figure><p id="ea07" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">By quantization, we mean to store data using a lower number of bits (ex. 32 -&gt; 16, or 32 -&gt; 4), this is also known as casting. If we were to store 1GB of numbers (by default stored as 32-bit objects), if we decided to store them using 16-bit objects (hence, applying a quantization), the size of our data would be halved, resulting in 0.5GB.</p><h2 id="5565" class="ph nn fq bf no pi pj pk nr pl pm pn nu mx po pp pq nb pr ps pt nf pu pv pw fw bk">Is there a catch to quantization?</h2><p id="8b41" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">Saving this amount of storage looks incredible (as you understood, we could keep cutting until we reach the minimum amount of bits: 1-bit, also known as binary quantization. Our database size will be reduced by 32 times, <strong class="mq ga">from 1GB to 31.25MB!</strong>), but as you might have already understood, there is a catch.</p><p id="f5d9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Any number can be stored up to the limits allowed by all the possible combinations of bits. With a 32-bit quantization, you can store a maximum of 2³² numbers. There are so many possible combinations that we decided to include decimals when using 32-bits. For example, if we were to add a decimal to our initial number and store 40.12 in 32-bits, it would be using this combination of 1 and 0:</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="b1b2" class="qb nn fq py b bg qc qd l qe qf">01000010 00100000 01111010 11100001</span></pre><p id="4ccc" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">We have understood that with a 32-bit storage (given its large combination of possible values) we can pretty much encode each number, including its decimal points (to clarify, if you are new to quantization, the real number and decimal are not separated, 40.12 is converted as a whole into a combination of 32 binary numbers).</p><p id="2afe" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">If we keep diminishing the number of bits, all the possible combinations diminish exponentially. For example, 4-bit storage has a limit of 2⁴ combinations: we can only store 16 numbers (this does not leave much room to store decimals). With 1-bit storage, we can only store a single number, either a 1 or a 0.</p><p id="942c" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To put this into context, storing our initials 32-bit numbers into binary code would force us to convert all our numbers, such as 40.12 into either 0 or 1. In this scenario, this compression does not look very good.</p><h1 id="e444" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">How to make the best out of Quantization</h1><p id="f79b" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">We have seen how quantization results in an information loss. So, how can we make use of it, after all? When you look at the quantization of a single number (40.12 converted into 1), it seems there is no value that can derive from such an extreme level of quantization, there is simply too much loss.</p><p id="4317" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">However, when we apply this technique to a set of data such as vectors, the information loss is not as drastic as when applied to a single number. Vector search is a perfect example of where to apply quantization in a useful manner.</p><p id="b755" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">When we use an encoder, such as <strong class="mq ga">all-MiniLM-L6-v2</strong>, we store each sample (which was originally in the form of raw text) as a vector: a sequence of 384 numbers. The storage of millions of similar sequences, as you might have understood, is prohibitive, and we can use quantization to substantially diminish the size of the original vectors by a huge margin.</p><p id="c7c9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Perhaps, quantizing our vectors from 32-bit to 16-bit is not this big of a loss. But how about 4-bit or even binary quantization? Because our sets are relatively large (384 numbers each), this considerable complexity lets us reach a higher level of compression without resulting in excessive retrieval loss.</p><p id="dcf2" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">4-bit quantization</strong></p><p id="6d9d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The way we execute quantization is by looking at the data distribution of our flattened vector and choosing to map an equivalent interval with a lower number of bits. My favorite example is 4-bit quantization. With this degree of complexity, we can store 2⁴ = 16 numbers. But, as explained, all the numbers in our vectors are complex, each with several decimal points:</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="ead1" class="qb nn fq py b bg qc qd l qe qf">array([ 2.43655406e-02, -4.33481708e-02, -1.89688837e-03, -3.76498550e-02,<br/>       -8.96364748e-02,  2.96154656e-02, -5.79943173e-02,  1.87652372e-02,<br/>        1.87771711e-02,  6.30387887e-02, -3.23972516e-02, -1.46128759e-02,<br/>       -3.39277312e-02, -7.04369228e-03,  3.87261175e-02, -5.02494797e-02,<br/>        ...<br/>       -1.03239892e-02,  1.83096472e-02, -1.86534156e-03,  1.44851031e-02,<br/>       -6.21072948e-02, -4.46912572e-02, -1.57684386e-02,  8.28376040e-02,<br/>       -4.58770394e-02,  1.04658678e-01,  5.53084277e-02, -2.51113791e-02,<br/>        4.72703762e-02, -2.41811387e-03, -9.09169838e-02,  1.15215247e-02],<br/>      dtype=float32)</span></pre><p id="16d7" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">What we can do is map each of our numbers in the distribution into an interval that spans between [-8, 7] (16 possible numbers). To define the extreme of the interval, we can use the minimum and maximum values of the distribution we are quantizing.</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qg"><img src="../Images/20c08512fbff9dde9fecc9fdfd93b941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCwaeZ-ntgGQmdQ1nWrugQ.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">4-bit quantization: the grey area is an interval of integers between [-8, 7], don’t confuse it with bits. Any number of this interval will be later converted into a 4-bit object, <strong class="bf no">image by Author</strong></figcaption></figure><p id="7b26" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">For example, the minimum/maximum of the distribution is [-0.2, 0.2]. This means that -0.2 will be converted to -8, and 0.2 to 7. Each number in the distribution will have a quantized equivalent in the interval (ex. the first number in our example array (0.02436554) will be quantized to 0, as shown in the picture above).</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="1021" class="qb nn fq py b bg qc qd l qe qf">array([[-1, -3, -1, ...,  1, -2, -2],<br/>       [-6, -1, -2, ..., -2, -2, -3],<br/>       [ 0, -2, -4, ..., -1,  1, -2],<br/>       ...,<br/>       [ 3,  0, -5, ..., -5,  7,  0],<br/>       [-4, -5,  3, ..., -2, -2, -2],<br/>       [-1,  0, -2, ..., -1,  1, -3]], dtype=int4)</span></pre><p id="ba5f" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk"><strong class="mq ga">1-bit quantization</strong></p><p id="62d9" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The same principle applies to binary quantization but is much simpler. The rule is the following: each number of the distribution &lt; 0 becomes 0, and each number &gt; 0 becomes 1.</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qh"><img src="../Images/47ee5e68eaecf27187bc19d0f422e850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPmUqrOHp25c4kazDYv3zg.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">1-bit quantization, <strong class="bf no">image by Author</strong></figcaption></figure><h1 id="2189" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">Not all embeddings are built the same</h1><p id="f8ab" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">The principal issue with current quantization techniques is that they live on the assumption that all our values <strong class="mq ga">are based on a single distribution</strong>. That is why, when we use thresholds to define intervals (ex. minimum and maximum), we only use a single set derived from the totality of our data (which is modeled on a single distribution).</p></div></div><div class="ov"><div class="ab cb"><div class="lr qi ls qj lt qk cf ql cg qm ci bh"><figure class="oq or os ot ou ov qo qp paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qn"><img src="../Images/a6b19bc9727ad03a669bc57815c9d314.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*V6k70ccmlAyFV7xIxvEaWQ.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">distribution of all individual samples from a flattened encoded dataset, <strong class="bf no">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c176" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In an experiment, I have encoded 41.963 game descriptions into vectors<strong class="mq ga">. </strong>By looking at the data distributions of each feature, we can see that despite the efforts, there is no feature which is perfectly normalized: its mean could deviate from the target 0.</p></div></div><div class="ov"><div class="ab cb"><div class="lr qi ls qj lt qk cf ql cg qm ci bh"><figure class="oq or os ot ou ov qo qp paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qq"><img src="../Images/c11a41a92ae510cee2c12f27f3c166fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*y5f5dEloTYpuMzW3oZoA8Q.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">distributions of 20 random features of the encoded dataset, <strong class="bf no">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e65b" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In a few words, each feature can be modeled <strong class="mq ga">with a dedicated distribution</strong>. Because the data <strong class="mq ga">does not follow</strong> a single giant distribution, we can leverage the many ways this is organized by applying a quantization at the feature level. In addition, embeddings tend to encode each feature using similar values (otherwise, the mean will constantly be 0), which means there is a <strong class="mq ga">minimal chance of drift</strong> when encoding additional data.</p><p id="ee1d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To better explain the math, let us define two sets of values:<br/><strong class="mq ga">S</strong> = all the individual samples from the encoded dataset (41936 * 384)<br/><strong class="mq ga">Fₙ</strong> = all the individual samples from the encoded dataset belonging to a single feature (41936 * 1)</p><h1 id="9b1f" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">Feature 29: The Ugly Duckling</h1><p id="6571" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">In our sample dataset, each vector counts 384 features. However, by exploring the data one feature at a time, we can notice that some are not perfectly normalized but substantially skewed. Let us take <strong class="mq ga">F</strong>₂₉<strong class="mq ga"> </strong>as an example: the following plot shows the distribution of <strong class="mq ga">F</strong>₂₉<strong class="mq ga"> (</strong>41936<strong class="mq ga">) </strong>across our entire encoded dataset.</p></div></div><div class="ov"><div class="ab cb"><div class="lr qi ls qj lt qk cf ql cg qm ci bh"><figure class="oq or os ot ou ov qo qp paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qr"><img src="../Images/ab7ea6349f4adddc04c942c09c331cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*K8YP-xuhf5ub7zgf3oL9lA.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx"><strong class="bf no">F</strong>₂₉ distribution, <strong class="bf no">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ee99" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">As we can see from the plot, its distribution mean is around -0.07, and its edges are (-0.2, 0.1). I am confident, knowing how encoders behave, that no matter the amount of extra data we are going to feed the model, <strong class="mq ga">F</strong>₂₉ will always remain an Ugly Duckling, with its distribution untouched. The distribution only counts a few positive values.</p><h2 id="d43e" class="ph nn fq bf no pi pj pk nr pl pm pn nu mx po pp pq nb pr ps pt nf pu pv pw fw bk">Regular Quantization</h2><p id="1648" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">Now, let us apply <strong class="mq ga">binary quantization to the book, </strong>but only to <strong class="mq ga">F</strong>₂₉. I am choosing a binary approach because most of the information is lost, meaning there can be room for improvement using a different approach.</p><p id="e583" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To quantize values in a binary fashion we need to pick a single value that will work as a threshold when converting values to either 0 or 1. The easiest way is to pick 0 (~ the distribution mean of <strong class="mq ga">S</strong>). When working on the values of <strong class="mq ga">F</strong>₂₉, because most of them are negative, the majority will be quantized to 0, and only a few will be quantized to 1.</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qs"><img src="../Images/c06df47c158bea08f795a089f742a2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxVeDUeQltgpRxNxLYolBw.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">samples whose quantized value should be 1, but quantized as 0: 44% of <strong class="bf no">F</strong>₂₉, <strong class="bf no">image by Author</strong></figcaption></figure><p id="e2d6" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Let us explore the data further: 94% of the <strong class="mq ga">F</strong>₂₉ have been converted to 0, while our target in a perfectly normalized distribution is 50%. This means that 44% of <strong class="mq ga">F</strong>₂₉ (red area of the density distribution) has not been properly quantized.</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="161f" class="qb nn fq py b bg qc qd l qe qf"># we count the number of 0 over the total number of values<br/>&gt;&gt;&gt; 1-quantized_regular[:, 29].sum()/sample_vectors[:, 29].size<br/>0.9424122472338802 </span></pre><h2 id="d3f4" class="ph nn fq bf no pi pj pk nr pl pm pn nu mx po pp pq nb pr ps pt nf pu pv pw fw bk">ft-Quantization</h2><p id="c8a5" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">What if, instead of using 0 as a threshold (extracted from <strong class="mq ga">S</strong>) we were to use the <strong class="mq ga">F</strong>₂₉ distribution as a benchmark? Looking at <strong class="mq ga">F</strong>₂₉ distribution again, instead of 0, we would be using its mean ~ -0.07 and its extremes as the minimum/maximum of the interval ~ [-0.20, 0.10] (Please refer to the image below: I apologize for the image inaccuracy, the extremes are quite there, though). In simple words, ft-Q shifts the position of the <strong class="mq ga">reference quantization interval</strong> to better fit the real distribution of the data.</p></div></div><div class="ov"><div class="ab cb"><div class="lr qi ls qj lt qk cf ql cg qm ci bh"><figure class="oq or os ot ou ov qo qp paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qt"><img src="../Images/9972f8207b07902d3986059f72446860.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rGbMBcRKV3W1forC_SlvPg.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">visualization of ft-Q, the interval is adapted to the feature distribution, <strong class="bf no">image by Author</strong></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><blockquote class="qu qv qw"><p id="481f" class="mo mp nk mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">***Through the following article I am trying to introduce a new algorithm that, to the extent of my knowledge, I have been unable to find elsewhere. Note that the algorithm is different from FQ (feature quantization) that is used for training neural networks, this is algorithm is supposed to be used post-training.<br/><strong class="mq ga">I am open to criticism</strong> and <strong class="mq ga">welcome any feedback.</strong></p></blockquote><p id="ad22" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">After applying binary quantization to <strong class="mq ga">F</strong>₂₉, because the threshold has been updated, we can see how half the times the data will be quantized to 0, and the other half to 1, resulting in a more realistic representation of the data. By comparing the quantization results, ft-Q has converted 47% of the <strong class="mq ga">F</strong>₂₉ into 0, resulting in only 3% of values not being properly quantized.</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="de81" class="qb nn fq py b bg qc qd l qe qf"># we count the number of 0 over the total number of values<br/>&gt;&gt;&gt; 1-quantized_tfQ[:, 29].sum()/sample_vectors[:, 29].size<br/>0.46809423884013734</span></pre><p id="e78d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">To summarize, <strong class="mq ga">ft-Q</strong> (or ft-Quantization) encodes each feature individually, minimizing errors that can occur from non-normalized distributions.</p><h1 id="995a" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">When to use ft-Quantization</h1><p id="fc37" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">Realistically, no embedding is perfectly normalized, and there is a variance (despite it being minimal) across the feature distribution. However, now that we have identified the misplaced values, we can adjust them using <br/><strong class="mq ga">ft-Q</strong>.</p><h2 id="1f52" class="ph nn fq bf no pi pj pk nr pl pm pn nu mx po pp pq nb pr ps pt nf pu pv pw fw bk">Can ft-Q be applied to regular embeddings?</h2><p id="1362" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">When ft-Q is applied to regular embeddings we are not looking at a substantial enhancement.</p><pre class="oq or os ot ou px py pz bp qa bb bk"><span id="b8ee" class="qb nn fq py b bg qc qd l qe qf">&gt;&gt;&gt; err_regular = .5-quantized_regular.sum()/sample_vectors.size<br/>&gt;&gt;&gt; err_ftQ = .5-quantized_tfQ.sum()/sample_vectors.size<br/>&gt;&gt;&gt; err_total = abs(err_regular)-abs(err_ftQ)<br/>&gt;&gt;&gt; err_total<br/>0.012901293538566672</span></pre><p id="bd8e" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">In the case of <strong class="mq ga">all-MiniLM-L6-v2 </strong>we have reached a <strong class="mq ga">1.2% improvement</strong> (not remarkable, but still an upgrade).</p><h2 id="c144" class="ph nn fq bf no pi pj pk nr pl pm pn nu mx po pp pq nb pr ps pt nf pu pv pw fw bk">Where ft-Q shines: processed embeddings</h2><p id="d9ae" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">However, embeddings are not always used in their normalized form. Sometimes, there are use cases where encoding requires the embedding to be processed (ex. in the case of <a class="af nl" href="https://github.com/atlantis-nova/simtag" rel="noopener ugc nofollow" target="_blank">covariate encoding</a>). We can use the following theoretical diagram as a way to understand <strong class="mq ga">in which cases</strong> ft-Q can be better utilized:</p><figure class="oq or os ot ou ov on oo paragraph-image"><div role="button" tabindex="0" class="ow ox ed oy bh oz"><div class="on oo qx"><img src="../Images/0f1ce97a5812a6558b64afc4c9167d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vj3dwnzI7jPomE-lqJVXNQ.png"/></div></div><figcaption class="pb pc pd on oo pe pf bf b bg z dx">the more the feature skew from a perfect normalisation, the more ft-Q is effective, <strong class="bf no">image by Author</strong></figcaption></figure><p id="ac23" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">The vectors that are the result of an extra processing step are not necessarily normalized: we could <strong class="mq ga">normalize</strong> them again and only then apply <strong class="mq ga">quantization</strong>, but we can cast two birds with one stone by using ft-Q as a single operation (in addition to its small improvement even after a non-perfect normalization).</p><h1 id="edc6" class="nm nn fq bf no np nq gv nr ns nt gy nu nv nw nx ny nz oa ob oc od oe of og oh bk">Conclusion</h1><p id="627a" class="pw-post-body-paragraph mo mp fq mq b gt oi ms mt gw oj mv mw mx ok mz na nb ol nd ne nf om nh ni nj fj bk">In conclusion, this article attempts to propose a more granular approach to quantization. Originally, the reason for developing this algorithm was to solve performance issues of processed embeddings, but after proper experimentation, it has proved useful even in a regular scenario.</p><p id="6e3d" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">After the popularization of LLM and more complex vector databases, memory management and performance improvements are becoming increasingly relevant in the space of information retrieval, hence it is our responsibility to familiarize ourselves with them and propose new and better solutions.</p><p id="f391" class="pw-post-body-paragraph mo mp fq mq b gt mr ms mt gw mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj fj bk">Time will tell if new and smarter data compression approaches will join the scene. For now, you can make the best use of this algorithm.</p></div></div></div></div>    
</body>
</html>