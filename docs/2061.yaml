- en: Interpreting Weight Regularization In Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的权重正则化解释
- en: 原文：[https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23](https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23](https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23)
- en: Why do L1 and L2 regularization result in model sparsity and weight shrinkage?
    What about L3 regularization? Keep reading to find out more!
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么 L1 和 L2 正则化会导致模型稀疏和权重收缩？L3 正则化又会怎样呢？继续阅读，了解更多！
- en: '[](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    ·9 min read·Aug 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    ·阅读时长 9 分钟·2024年8月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/cfdaa36493feb5a8a12b275a6bd01de6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfdaa36493feb5a8a12b275a6bd01de6.png)'
- en: Photo by [D koi](https://unsplash.com/@dkoi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [D koi](https://unsplash.com/@dkoi?utm_source=medium&utm_medium=referral)
    于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [Naresh Singh](https://medium.com/@brocolishbroxoli) 合著。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: After reading this article, you’ll be very well equipped with the tools and
    reasoning capability to think about the effects of any Lk regularization term
    and decide if it applies to your situation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本文后，您将具备足够的工具和推理能力来思考任何 Lk 正则化项的效果，并决定它是否适用于您的情况。
- en: What is regularization in machine learning?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的正则化是什么？
- en: Let’s look at some definitions on the internet and generalize based on those.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看互联网上的一些定义，并基于这些定义进行概括。
- en: Regularization is a set of methods for reducing overfitting in machine learning
    models. Typically, regularization trades a marginal decrease in training accuracy
    for an increase in generalizability. ([IBM](https://www.ibm.com/topics/regularization))
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化是一组减少机器学习模型过拟合的方法。通常，正则化通过牺牲训练准确度的微小下降来换取模型泛化能力的提高。 ([IBM](https://www.ibm.com/topics/regularization))
- en: Regularization makes models stable across different subsets of the data. It
    reduces the sensitivity of model outputs to minor changes in the training set.
    ([geeksforgeeks](https://www.geeksforgeeks.org/regularization-in-machine-learning/))
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化使得模型在不同的数据子集上保持稳定。它减少了模型输出对训练集细微变化的敏感度。 ([geeksforgeeks](https://www.geeksforgeeks.org/regularization-in-machine-learning/))
- en: Regularization in machine learning serves as a method to forestall a model from
    overfitting. ([simplilearn](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning))
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习中的正则化是一种防止模型过拟合的方法。 ([simplilearn](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning))
- en: In general, regularization is a technique to prevent the model from overfitting
    and to allow the model to generalize its predictions on unseen data. Let’s look
    at the role of weight regularization in particular.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，正则化是一种防止模型过拟合的技术，使模型能够在未见过的数据上进行泛化预测。接下来，我们特别讨论一下权重正则化的作用。
- en: Why use weight regularization?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么使用权重正则化？
- en: One could employ many forms of regularization while training a machine learning
    model. Weight regularization is one such technique, which is the focus of this
    article. Weight regularization means applying some constraints on the learnable
    weights of your machine learning model so that they allow the model to generalize
    to unseen inputs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，可以采用多种形式的正则化。权重正则化就是一种技术，本文将重点讨论它。权重正则化意味着对机器学习模型中可学习的权重施加一些约束，以使它们能够使模型对未见过的输入具有较好的泛化能力。
- en: Weight regularization improves the performance of neural networks by penalizing
    the weight matrices of nodes. This penalty discourages the model from having large
    parameter (weight) values. It helps control the model’s ability to fit the noise
    in the training data. Typically, the biases in the machine learning model are
    not subject to regularization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 权重正则化通过惩罚节点的权重矩阵来提高神经网络的性能。这种惩罚抑制了模型拥有大参数（权重）值的情况。它有助于控制模型拟合训练数据噪声的能力。通常，机器学习模型中的偏置项不受正则化约束。
- en: How is regularization implemented in deep neural networks?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在深度神经网络中，正则化是如何实现的？
- en: 'Typically, a regularization loss is added to the model’s loss during training.
    It allows us to control the model’s weights during training. The formula looks
    like this:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，正则化损失会在训练过程中添加到模型的损失中。它允许我们在训练过程中控制模型的权重。公式如下所示：
- en: '![](../Images/a90e3dad21a91d2138379b77ff6d1722.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a90e3dad21a91d2138379b77ff6d1722.png)'
- en: '*Figure-1: Total loss as a sum of the model loss and regularization loss. k
    is a floating point value and indicates the regularization norm. Alpha is the
    weighting factor for the regularization loss.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*图-1：总损失是模型损失和正则化损失的总和。k 是一个浮动值，表示正则化范数。Alpha 是正则化损失的加权因子。*'
- en: Typical values of k used in practice are 1 and 2\. These are called the L1 and
    L2 regularization schemes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中常用的 k 值是 1 和 2。这些被称为 L1 和 L2 正则化方案。
- en: But why do we use just these two values for the most part, when in fact there
    are infinitely many values of k one could use? Let’s answer this question with
    an interpretation of the L1 and L2 regularization schemes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为什么我们大多数时候只使用这两个值，事实上 k 还有无数个可能的值？让我们通过对 L1 和 L2 正则化方案的解释来回答这个问题。
- en: Interpretation of different weight regularization types
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同权重正则化类型的解释
- en: The two most common types of regularization used for machine learning models
    are L1 and L2 regularization. We will start with these two, and continue to discuss
    some unusual regularization types such as L0.5 and L3 regularization. We will
    take a look at the gradients of the regularization losses and plot them to intuitively
    understand how they affect the model weights.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习模型中，最常用的两种正则化类型是 L1 和 L2 正则化。我们将从这两种开始，并继续讨论一些不常见的正则化类型，如 L0.5 和 L3 正则化。我们将查看正则化损失的梯度，并将其绘制出来，以直观地理解它们如何影响模型的权重。
- en: L1 regularization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 正则化
- en: L1 regularization adds the average of the absolute value of the weights together
    as the regularization loss.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化通过将权重的绝对值的平均值加起来作为正则化损失。
- en: '![](../Images/a84db670c58566cdf15db472a25fe29e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a84db670c58566cdf15db472a25fe29e.png)'
- en: '*Figure-2: L1 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*图-2：L1 正则化损失及其对每个权重 Wi 的偏导数。*'
- en: It has the effect of adjusting the weights by a constant (in this case alpha
    times the learning rate) in the direction that minimizes the loss. Figure 3 shows
    a graphical representation of the function and its derivative.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过一个常数（在此情况下为 alpha 乘以学习率）来调整权重，方向是最小化损失的方向。图 3 显示了该函数及其导数的图示。
- en: '![](../Images/35237be3c01400c997680a60a2efce90.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35237be3c01400c997680a60a2efce90.png)'
- en: '*Figure-3: The blue line is |w| and the red line is the derivative of |w|.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*图-3：蓝线表示 |w|，红线表示 |w| 的导数。*'
- en: You can see that the derivative of the L1 norm is a constant (depending on the
    sign of w), which means that the gradient of this function only depends on the
    sign of w and not its magnitude. The gradient of the L1 norm is not defined at
    w=0.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，L1 范数的导数是常数（取决于 w 的符号），这意味着该函数的梯度仅取决于 w 的符号，而不取决于其大小。L1 范数在 w=0 时的梯度是未定义的。
- en: It means that the weights are moved towards zero by a constant value at each
    step during backpropagation. Throughout training, it has the effect of driving
    the weights to converge at zero. That is why the L1 regularization makes a model
    sparse (i.e. some of the weights become 0). It might cause a problem in some cases
    if it ends up making a model too sparse. The L2 regularization does not have this
    side-effect. Let’s discuss it in the next section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在每次反向传播步骤中，权重将通过一个常数值向零移动。在整个训练过程中，它有助于将权重收敛到零。这就是为什么 L1 正则化会使模型变得稀疏（即一些权重变为
    0）。在某些情况下，如果它使模型过于稀疏，可能会导致问题。L2 正则化则没有这种副作用。我们将在下一节讨论这一点。
- en: L2 regularization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L2 正则化
- en: L2 regularization adds the average of the square of the absolute value of the
    weights together as the regularization loss.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化将权重绝对值的平方的平均值加总作为正则化损失。
- en: '![](../Images/be2f17e6a46e85c79baec3a21ab72ff2.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be2f17e6a46e85c79baec3a21ab72ff2.png)'
- en: '*Figure-4: L2 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4：L2 正则化损失及其相对于每个权重 Wi 的偏导数。*'
- en: It has the effect of adjusting each weight by a multiple of the weight itself
    in the direction that minimizes the loss. Figure 5 shows a graphical representation
    of the function and its derivative.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它有助于在使损失最小化的方向上，通过权重本身的倍数调整每个权重。图5展示了该函数及其导数的图形表示。
- en: '![](../Images/a227e0b16c814d3b10454ca35b0f1c79.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a227e0b16c814d3b10454ca35b0f1c79.png)'
- en: '*Figure-5: The blue line is pow(|w|, 2) and the red line is the derivative
    of pow(|w|, 2).*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5：蓝线是 pow(|w|, 2)，红线是 pow(|w|, 2) 的导数。*'
- en: You can see that the derivative of the L2 norm is just the sign-adjusted square
    root of the norm itself. The gradient of the L2 norm depends on both the sign
    and magnitude of the weight.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，L2 范数的导数只是该范数本身的符号调整后的平方根。L2 范数的梯度依赖于权重的符号和大小。
- en: This means that at every gradient update step, the weights will be adjusted
    toward zero by an amount that is proportional to the weight’s value. Over time,
    this has the effect of drawing the weights toward zero, but never exactly zero,
    since subtracting a constant factor of a value from the value itself never makes
    the result exactly zero unless it is zero to begin with. The L2 norm is commonly
    used for weight decay during machine learning model training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在每次梯度更新步骤中，权重将根据权重值的大小按比例调整到接近零。随着时间的推移，这会将权重拉向零，但永远不会完全为零，因为从一个值中减去常数因子本身，除非该值本来就为零，否则结果永远不会精确为零。L2
    范数通常用于机器学习模型训练中的权重衰减。
- en: Let’s consider L0.5 regularization next.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来考虑 L0.5 正则化。
- en: L0.5 regularization
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L0.5 正则化
- en: L0.5 regularization adds the average of the square root of the absolute value
    of the weights together as the regularization loss.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: L0.5 正则化将权重绝对值的平方根的平均值加总作为正则化损失。
- en: '![](../Images/8501ea9721f5d5a6de06bb936551ff76.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8501ea9721f5d5a6de06bb936551ff76.png)'
- en: '*Figure-6: L0.5 regularization loss and its partial derivative with respect
    to each weight Wi.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6：L0.5 正则化损失及其相对于每个权重 Wi 的偏导数。*'
- en: This has the effect of adjusting each weight by a multiple (in this case alpha
    times the learning rate) of the inverse square root of the weight itself in the
    direction that minimizes the loss. Figure 7 shows a graph of the function and
    its derivative.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在每次反向传播步骤中，权重将通过一个常数倍（在此情况下为 alpha 乘以学习率）调整，调整的方向是使损失最小化，并且调整量是权重自身的平方根的倒数。图7展示了该函数及其导数的图形。
- en: '![](../Images/ae7d191a6f2b39386785954cf1368a2a.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae7d191a6f2b39386785954cf1368a2a.png)'
- en: '*Figure-7: The blue line is pow(|w|, 0.5) and the red line is the derivative
    of pow(|w|, 0.5).*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7：蓝线是 pow(|w|, 0.5)，红线是 pow(|w|, 0.5) 的导数。*'
- en: 'You can see that the derivative of the L0.5 norm is a discontinuous function,
    which peaks at the positive values of w close to 0 and it reaches negative infinity
    for the negative values of w close to 0\. Further, we can draw the following conclusions
    from the graph:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，L0.5 范数的导数是一个不连续的函数，在接近零的正值 w 处达到峰值，并且在接近零的负值 w 处达到负无穷大。此外，我们可以从图中得出以下结论：
- en: As |w| tends to 0, the magnitude of the gradient tends to infinity. During backpropagation,
    these values of w will quickly swing to past 0 because large gradients will cause
    a large change in the value of w. In other words, negative w will become positive
    and vice-versa. This cycle of flip flops will continue to repeat itself.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当|w|趋近于0时，梯度的大小趋向于无穷大。在反向传播过程中，这些w值会迅速跳过0，因为大的梯度会导致w值发生剧烈变化。换句话说，负的w将变为正，反之亦然。这种反转的循环会持续重复。
- en: As |w| increases, the magnitude of the gradient decreases. These values of w
    are stable because of small gradients. However, with each backpropagation step,
    the value of w will be drawn closer to 0.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当|w|增加时，梯度的大小会减小。这些w值是稳定的，因为梯度较小。然而，每一次反向传播步骤后，w的值都会被拉向0。
- en: This is hardly what one would want from a weight regularization routine, so
    it’s safe to say that L0.5 isn’t a great weight regularizer. Let’s consider L3
    regularization next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况几乎不是人们希望从权重正则化过程中得到的结果，因此可以安全地说L0.5不是一个好的权重正则化器。接下来我们考虑L3正则化。
- en: L3 regularization
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L3正则化
- en: L3 regularization adds the average of the cube of the absolute value of the
    weights together as the regularization loss.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: L3正则化将权重绝对值的立方的平均值作为正则化损失进行求和。
- en: '![](../Images/6b8d495a27d3a0632a9fde9d0cebc941.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b8d495a27d3a0632a9fde9d0cebc941.png)'
- en: '*Figure-8: L3 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8：L3正则化损失及其相对于每个权重Wi的偏导数。*'
- en: This has the effect of adjusting each weight by a multiple (in this case alpha
    times the learning rate) of the square of the weight itself in the direction that
    minimizes the loss.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这会通过一个倍数（在这种情况下是alpha倍学习率）调整每个权重的平方值，朝着最小化损失的方向进行调整。
- en: Graphically, this is what the function and its derivative look like.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从图形上看，这就是函数及其导数的样子。
- en: '![](../Images/2e6f04cb8b21dd0b936850641a9544a4.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e6f04cb8b21dd0b936850641a9544a4.png)'
- en: '*Figure-9: The blue line is pow(|w|, 3) and the red line is the derivative
    of pow(|w|, 3).*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9：蓝线表示pow(|w|, 3)，红线表示pow(|w|, 3)的导数。*'
- en: To really understand what’s going on here, we need to zoom in to the chart around
    the w=0 point.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了真正理解这里发生了什么，我们需要放大w=0点附近的图表。
- en: '![](../Images/c0a7a0ef895dae233cc3d4dea17b637e.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0a7a0ef895dae233cc3d4dea17b637e.png)'
- en: '*Figure-10: The blue line is pow(|w|, 3) and the red line is the derivative
    of pow(|w|, 3), zoomed in at small values of w around 0.0.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10：蓝线表示pow(|w|, 3)，红线表示pow(|w|, 3)的导数，放大显示w值接近0.0时的情况。*'
- en: You can see that the derivative of the L3 norm is a continuous and differentiable
    function (despite the presence of |w| in the derivative), which has a large magnitude
    at large values of w and a small magnitude for small values of w.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，L3范数的导数是一个连续且可微的函数（尽管导数中有|w|），它在大值w时具有较大的幅度，而在小值w时具有较小的幅度。
- en: Interestingly, the gradient is very close to zero for very small values of w
    around the 0.0 mark.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对于非常小的w值，接近0.0时，梯度几乎为零。
- en: The interpretation of the gradient for L3 is interesting.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: L3的梯度解释很有趣。
- en: For large values of w, the magnitude of the gradient is large. During backpropagation,
    these values will be pushed towards 0.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于较大的w值，梯度的大小很大。在反向传播过程中，这些值将被推向0。
- en: Once the weight w reaches an inflection point (close to 0.0), the gradient almost
    vanishes, and the weights will stop getting updated.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦权重w达到拐点（接近0.0），梯度几乎消失，权重将停止更新。
- en: The effect is that it will drive the weights with large magnitudes close to
    0, but not exactly 0.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其效果是将大幅度的权重拉近0，但不会完全变为0。
- en: Let’s consider higher norms to see how this plays out in the limiting case.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑更高的范数，看看在极限情况下会发生什么。
- en: Beyond L3 regularization
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越L3正则化
- en: To understand what happens for Linfinity, we need to see what happens in the
    case of the L10 regularization case.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解Linfinity的情况，我们需要看看L10正则化的情况。
- en: '![](../Images/55222800c0aa95904a497c6587a5d7cd.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55222800c0aa95904a497c6587a5d7cd.png)'
- en: '*Figure-11: The blue line is pow(|w|, 10) and the red line is the derivative
    of pow(|w|, 10), zoomed in at small values of w around 0.0.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11：蓝线表示pow(|w|, 10)，红线表示pow(|w|, 10)的导数，放大显示w值接近0.0时的情况。*'
- en: One can see that the gradients for values of |w| < 0.5 are extremely small,
    which means that regularization won’t be effective for those values of w.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，当|w| < 0.5时，梯度非常小，这意味着对于这些w值，正则化将不会起作用。
- en: Exercise
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Based on everything we saw above, L1 and L2 regularization are fairly practical
    based on what you want to achieve. As an exercise, try to reason about the behavior
    of the L1.5 regularization, whose chart is shown below.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们上述的所有观察，L1 和 L2 正则化根据你想要实现的目标是相当实用的。作为一个练习，尝试推理一下 L1.5 正则化的行为，其图表如下所示。
- en: '![](../Images/b9e04e33914d0b5caa9b6d71ee0ec907.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9e04e33914d0b5caa9b6d71ee0ec907.png)'
- en: '*Figure-12: The blue line is pow(|w|, 1.5) and the red line is the derivative
    of pow(|w|, 1.5).*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图12：蓝线是 pow(|w|, 1.5)，红线是 pow(|w|, 1.5) 的导数。*'
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We took a visual and intuitive look at the L1 and L2 (and in general Lk) regularization
    terms to understand why L1 regularization results in sparse model weights and
    L2 regularization results in model weights close to 0\. Framing the solution as
    inspecting the resulting gradients is extremely valuable during this exercise.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从视觉和直观的角度审视了 L1 和 L2（以及一般的 Lk）正则化项，以理解为什么 L1 正则化会导致稀疏的模型权重，而 L2 正则化会使模型权重接近
    0。在本次练习中，将解决方案框定为检查结果的梯度是非常有价值的。
- en: We explored L0.5, L3, and L10 regularization terms and graphically, and you
    (the reader) reasoned about regularization terms between L1 and L2 regularization,
    and developed an intuitive understanding of what implications it would have on
    a model’s weights.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探索了 L0.5、L3 和 L10 正则化项，并以图形方式呈现，你（读者）推理了 L1 和 L2 正则化之间的正则化项，并形成了对它们对模型权重影响的直观理解。
- en: We hope that this article has added to your toolbox of tricks you can use when
    considering regularization strategies during model training to fine-tuning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望本文能够为你在模型训练和微调时考虑正则化策略时提供更多的技巧。
- en: All the charts in this article were created using the online desmos graphing
    calculator. [Here is a link](https://www.desmos.com/calculator/hp5bjnh8ul) to
    the functions used in case you wish to play with them.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有图表均使用在线 Desmos 图形计算器创建。[这是一个链接](https://www.desmos.com/calculator/hp5bjnh8ul)，供你在需要时玩弄这些函数。
- en: All the images were created by the author(s) unless otherwise mentioned.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由作者创建，除非另有说明。
- en: References
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: We found the following articles useful while researching the topic, and we hope
    that you find them useful too!
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究这一主题时，我们发现以下文章非常有用，我们希望你也能从中受益！
- en: '[Stackexchange discussion](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Stackexchange 讨论](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms)'
- en: '[TDS: Demystifying L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TDS: 解密 L1 和 L2 正则化（第 3 部分）](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
- en: '[Visual explanation of L1 and L2 regularization](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[L1 和 L2 正则化的视觉解释](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)'
- en: Deep Learning by Ian Goodfellow
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ian Goodfellow 的《深度学习》
- en: An introduction to statistical learning by Gareth James
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gareth James 的《统计学习简介》
