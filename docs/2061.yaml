- en: Interpreting Weight Regularization In Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23](https://towardsdatascience.com/interpreting-weight-regularization-in-machine-learning-99f2677f7ef5?source=collection_archive---------7-----------------------#2024-08-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why do L1 and L2 regularization result in model sparsity and weight shrinkage?
    What about L3 regularization? Keep reading to find out more!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page---byline--99f2677f7ef5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--99f2677f7ef5--------------------------------)
    ·9 min read·Aug 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfdaa36493feb5a8a12b275a6bd01de6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [D koi](https://unsplash.com/@dkoi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Co-authored with [Naresh Singh](https://medium.com/@brocolishbroxoli).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this article, you’ll be very well equipped with the tools and
    reasoning capability to think about the effects of any Lk regularization term
    and decide if it applies to your situation.
  prefs: []
  type: TYPE_NORMAL
- en: What is regularization in machine learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at some definitions on the internet and generalize based on those.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization is a set of methods for reducing overfitting in machine learning
    models. Typically, regularization trades a marginal decrease in training accuracy
    for an increase in generalizability. ([IBM](https://www.ibm.com/topics/regularization))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regularization makes models stable across different subsets of the data. It
    reduces the sensitivity of model outputs to minor changes in the training set.
    ([geeksforgeeks](https://www.geeksforgeeks.org/regularization-in-machine-learning/))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regularization in machine learning serves as a method to forestall a model from
    overfitting. ([simplilearn](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning))
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, regularization is a technique to prevent the model from overfitting
    and to allow the model to generalize its predictions on unseen data. Let’s look
    at the role of weight regularization in particular.
  prefs: []
  type: TYPE_NORMAL
- en: Why use weight regularization?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One could employ many forms of regularization while training a machine learning
    model. Weight regularization is one such technique, which is the focus of this
    article. Weight regularization means applying some constraints on the learnable
    weights of your machine learning model so that they allow the model to generalize
    to unseen inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Weight regularization improves the performance of neural networks by penalizing
    the weight matrices of nodes. This penalty discourages the model from having large
    parameter (weight) values. It helps control the model’s ability to fit the noise
    in the training data. Typically, the biases in the machine learning model are
    not subject to regularization.
  prefs: []
  type: TYPE_NORMAL
- en: How is regularization implemented in deep neural networks?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, a regularization loss is added to the model’s loss during training.
    It allows us to control the model’s weights during training. The formula looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a90e3dad21a91d2138379b77ff6d1722.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-1: Total loss as a sum of the model loss and regularization loss. k
    is a floating point value and indicates the regularization norm. Alpha is the
    weighting factor for the regularization loss.*'
  prefs: []
  type: TYPE_NORMAL
- en: Typical values of k used in practice are 1 and 2\. These are called the L1 and
    L2 regularization schemes.
  prefs: []
  type: TYPE_NORMAL
- en: But why do we use just these two values for the most part, when in fact there
    are infinitely many values of k one could use? Let’s answer this question with
    an interpretation of the L1 and L2 regularization schemes.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretation of different weight regularization types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The two most common types of regularization used for machine learning models
    are L1 and L2 regularization. We will start with these two, and continue to discuss
    some unusual regularization types such as L0.5 and L3 regularization. We will
    take a look at the gradients of the regularization losses and plot them to intuitively
    understand how they affect the model weights.
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L1 regularization adds the average of the absolute value of the weights together
    as the regularization loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a84db670c58566cdf15db472a25fe29e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-2: L1 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  prefs: []
  type: TYPE_NORMAL
- en: It has the effect of adjusting the weights by a constant (in this case alpha
    times the learning rate) in the direction that minimizes the loss. Figure 3 shows
    a graphical representation of the function and its derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/35237be3c01400c997680a60a2efce90.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-3: The blue line is |w| and the red line is the derivative of |w|.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the derivative of the L1 norm is a constant (depending on the
    sign of w), which means that the gradient of this function only depends on the
    sign of w and not its magnitude. The gradient of the L1 norm is not defined at
    w=0.
  prefs: []
  type: TYPE_NORMAL
- en: It means that the weights are moved towards zero by a constant value at each
    step during backpropagation. Throughout training, it has the effect of driving
    the weights to converge at zero. That is why the L1 regularization makes a model
    sparse (i.e. some of the weights become 0). It might cause a problem in some cases
    if it ends up making a model too sparse. The L2 regularization does not have this
    side-effect. Let’s discuss it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L2 regularization adds the average of the square of the absolute value of the
    weights together as the regularization loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be2f17e6a46e85c79baec3a21ab72ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-4: L2 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  prefs: []
  type: TYPE_NORMAL
- en: It has the effect of adjusting each weight by a multiple of the weight itself
    in the direction that minimizes the loss. Figure 5 shows a graphical representation
    of the function and its derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a227e0b16c814d3b10454ca35b0f1c79.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-5: The blue line is pow(|w|, 2) and the red line is the derivative
    of pow(|w|, 2).*'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the derivative of the L2 norm is just the sign-adjusted square
    root of the norm itself. The gradient of the L2 norm depends on both the sign
    and magnitude of the weight.
  prefs: []
  type: TYPE_NORMAL
- en: This means that at every gradient update step, the weights will be adjusted
    toward zero by an amount that is proportional to the weight’s value. Over time,
    this has the effect of drawing the weights toward zero, but never exactly zero,
    since subtracting a constant factor of a value from the value itself never makes
    the result exactly zero unless it is zero to begin with. The L2 norm is commonly
    used for weight decay during machine learning model training.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider L0.5 regularization next.
  prefs: []
  type: TYPE_NORMAL
- en: L0.5 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L0.5 regularization adds the average of the square root of the absolute value
    of the weights together as the regularization loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8501ea9721f5d5a6de06bb936551ff76.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-6: L0.5 regularization loss and its partial derivative with respect
    to each weight Wi.*'
  prefs: []
  type: TYPE_NORMAL
- en: This has the effect of adjusting each weight by a multiple (in this case alpha
    times the learning rate) of the inverse square root of the weight itself in the
    direction that minimizes the loss. Figure 7 shows a graph of the function and
    its derivative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae7d191a6f2b39386785954cf1368a2a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-7: The blue line is pow(|w|, 0.5) and the red line is the derivative
    of pow(|w|, 0.5).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the derivative of the L0.5 norm is a discontinuous function,
    which peaks at the positive values of w close to 0 and it reaches negative infinity
    for the negative values of w close to 0\. Further, we can draw the following conclusions
    from the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: As |w| tends to 0, the magnitude of the gradient tends to infinity. During backpropagation,
    these values of w will quickly swing to past 0 because large gradients will cause
    a large change in the value of w. In other words, negative w will become positive
    and vice-versa. This cycle of flip flops will continue to repeat itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As |w| increases, the magnitude of the gradient decreases. These values of w
    are stable because of small gradients. However, with each backpropagation step,
    the value of w will be drawn closer to 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is hardly what one would want from a weight regularization routine, so
    it’s safe to say that L0.5 isn’t a great weight regularizer. Let’s consider L3
    regularization next.
  prefs: []
  type: TYPE_NORMAL
- en: L3 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L3 regularization adds the average of the cube of the absolute value of the
    weights together as the regularization loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b8d495a27d3a0632a9fde9d0cebc941.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-8: L3 regularization loss and its partial derivative with respect to
    each weight Wi.*'
  prefs: []
  type: TYPE_NORMAL
- en: This has the effect of adjusting each weight by a multiple (in this case alpha
    times the learning rate) of the square of the weight itself in the direction that
    minimizes the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Graphically, this is what the function and its derivative look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e6f04cb8b21dd0b936850641a9544a4.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-9: The blue line is pow(|w|, 3) and the red line is the derivative
    of pow(|w|, 3).*'
  prefs: []
  type: TYPE_NORMAL
- en: To really understand what’s going on here, we need to zoom in to the chart around
    the w=0 point.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0a7a0ef895dae233cc3d4dea17b637e.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-10: The blue line is pow(|w|, 3) and the red line is the derivative
    of pow(|w|, 3), zoomed in at small values of w around 0.0.*'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the derivative of the L3 norm is a continuous and differentiable
    function (despite the presence of |w| in the derivative), which has a large magnitude
    at large values of w and a small magnitude for small values of w.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, the gradient is very close to zero for very small values of w
    around the 0.0 mark.
  prefs: []
  type: TYPE_NORMAL
- en: The interpretation of the gradient for L3 is interesting.
  prefs: []
  type: TYPE_NORMAL
- en: For large values of w, the magnitude of the gradient is large. During backpropagation,
    these values will be pushed towards 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the weight w reaches an inflection point (close to 0.0), the gradient almost
    vanishes, and the weights will stop getting updated.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The effect is that it will drive the weights with large magnitudes close to
    0, but not exactly 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider higher norms to see how this plays out in the limiting case.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond L3 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand what happens for Linfinity, we need to see what happens in the
    case of the L10 regularization case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55222800c0aa95904a497c6587a5d7cd.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-11: The blue line is pow(|w|, 10) and the red line is the derivative
    of pow(|w|, 10), zoomed in at small values of w around 0.0.*'
  prefs: []
  type: TYPE_NORMAL
- en: One can see that the gradients for values of |w| < 0.5 are extremely small,
    which means that regularization won’t be effective for those values of w.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on everything we saw above, L1 and L2 regularization are fairly practical
    based on what you want to achieve. As an exercise, try to reason about the behavior
    of the L1.5 regularization, whose chart is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9e04e33914d0b5caa9b6d71ee0ec907.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure-12: The blue line is pow(|w|, 1.5) and the red line is the derivative
    of pow(|w|, 1.5).*'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a visual and intuitive look at the L1 and L2 (and in general Lk) regularization
    terms to understand why L1 regularization results in sparse model weights and
    L2 regularization results in model weights close to 0\. Framing the solution as
    inspecting the resulting gradients is extremely valuable during this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: We explored L0.5, L3, and L10 regularization terms and graphically, and you
    (the reader) reasoned about regularization terms between L1 and L2 regularization,
    and developed an intuitive understanding of what implications it would have on
    a model’s weights.
  prefs: []
  type: TYPE_NORMAL
- en: We hope that this article has added to your toolbox of tricks you can use when
    considering regularization strategies during model training to fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: All the charts in this article were created using the online desmos graphing
    calculator. [Here is a link](https://www.desmos.com/calculator/hp5bjnh8ul) to
    the functions used in case you wish to play with them.
  prefs: []
  type: TYPE_NORMAL
- en: All the images were created by the author(s) unless otherwise mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We found the following articles useful while researching the topic, and we hope
    that you find them useful too!
  prefs: []
  type: TYPE_NORMAL
- en: '[Stackexchange discussion](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[TDS: Demystifying L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Visual explanation of L1 and L2 regularization](https://www.linkedin.com/pulse/intuitive-visual-explanation-differences-between-l1-l2-xiaoli-chen/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deep Learning by Ian Goodfellow
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An introduction to statistical learning by Gareth James
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
