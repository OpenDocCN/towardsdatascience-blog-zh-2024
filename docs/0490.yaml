- en: Optimized Deployment of Mistral7B on Amazon SageMaker Real-Time Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimized-deployment-of-mistral7b-on-amazon-sagemaker-real-time-inference-e820629f15dd?source=collection_archive---------5-----------------------#2024-02-21](https://towardsdatascience.com/optimized-deployment-of-mistral7b-on-amazon-sagemaker-real-time-inference-e820629f15dd?source=collection_archive---------5-----------------------#2024-02-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Utilize large model inference containers powered by DJL Serving & Nvidia TensorRT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--e820629f15dd--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--e820629f15dd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e820629f15dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e820629f15dd--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--e820629f15dd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e820629f15dd--------------------------------)
    ·9 min read·Feb 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c7701b317d1f4292baac6499df4b53c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/a-close-up-of-a-human-brain-on-a-white-surface-9A9TcXEsy6c)
    by [Kommers](https://unsplash.com/@kommers)
  prefs: []
  type: TYPE_NORMAL
- en: The Generative AI space continues to expand at an unprecedented rate, with the
    introduction of more Large Language Model (LLM) families by the day. Within each
    family there are also varying sizes of each model, for instances there’s Llama7b,
    Llama13B, and Llama70B. Regardless of the model that you select, the same challenges
    arise for hosting these LLMs for inference.
  prefs: []
  type: TYPE_NORMAL
- en: The size of these LLMs continue to be the most pressing challenge, as it’s very
    difficult/impossible to fit many of these LLMs onto a single GPU. There are a
    few different approaches to tackling this problem, such as model partitioning.
    With model partitioning you can use techniques such as [Pipeline or Tensor Parallelism](https://colossalai.org/docs/concepts/paradigms_of_parallelism/#:~:text=There%20are%20generally%20two%20types,to%20parallelize%20computation%20between%20layers.)
    to essentially shard the model across multiple GPUs. Outside of model partitioning,
    other popular approaches include [Quantization](https://huggingface.co/docs/optimum/concept_guides/quantization)
    of model weights to a lower precision to reduce the model size itself at a cost
    of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: While the model size is a large challenge in itself, there is also the challenge
    of retaining the previous inference/attention in Text Generation for [Decoder
    based models](/llms-and-transformers-from-scratch-the-decoder-d533008629c5). Text
    Generation with these models is not as simple as traditional…
  prefs: []
  type: TYPE_NORMAL
