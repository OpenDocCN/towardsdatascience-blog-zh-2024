["```py\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams\nclient = QdrantClient(path=\"qdrant/\")\ncollection_name = \"MyCollection\"\nif client.collection_exists(collection_name):\n    client.delete_collection(collection_name)\n\nclient.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))\nqdrant = Qdrant(client, collection_name, hf)\n```", "```py\nsentence-transformers/msmarco-bert-base-dot-v5\n```", "```py\nclient.create_collection(collection_name,vectors_config=VectorParams(size=768, distance=Distance.DOT))\n```", "```py\n model_name = \"sentence-transformers/msmarco-bert-base-dot-v5\"\n    model_kwargs = {'device': 'cpu'}\n    encode_kwargs = {'normalize_embeddings': True}\n    hf = HuggingFaceEmbeddings(\n        model_name=model_name,\n        model_kwargs=model_kwargs,\n        encode_kwargs=encode_kwargs\n    )\n```", "```py\nfrom langchain_text_splitters import TokenTextSplitter\ntext_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\ntexts = text_splitter.split_text(file_content)\nmetadata = []\nfor i in range(0,len(texts)):\n    metadata.append({\"path\":file})\nqdrant.add_texts(texts,metadatas=metadata)\n```", "```py\ndef get_files(dir):\n    file_list = []\n    for f in listdir(dir):\n        if isfile(join(dir,f)):\n            file_list.append(join(dir,f))\n        elif isdir(join(dir,f)):\n            file_list= file_list + get_files(join(dir,f))\n    return file_list\n```", "```py\nimport docx\ndef getTextFromWord(filename):\n    doc = docx.Document(filename)\n    fullText = []\n    for para in doc.paragraphs:\n        fullText.append(para.text)\n    return '\\n'.join(fullText)\n```", "```py\nfrom pptx import Presentation\ndef getTextFromPPTX(filename):\n    prs = Presentation(filename)\n    fullText = []\n    for slide in prs.slides:\n        for shape in slide.shapes:\n            fullText.append(shape.text)\n    return '\\n'.join(fullText)\n```", "```py\nf = open(file,'r')\nfile_content = f.read()\nf.close()\n```", "```py\nreader = PyPDF2.PdfReader(file)\nfor i in range(0,len(reader.pages)):\n    file_content = file_content + \" \"+reader.pages[i].extract_text()\n```", "```py\nfile_content = \"\"\n    for file in onlyfiles:\n        file_content = \"\"\n        if file.endswith(\".pdf\"):\n            print(\"indexing \"+file)\n            reader = PyPDF2.PdfReader(file)\n            for i in range(0,len(reader.pages)):\n                file_content = file_content + \" \"+reader.pages[i].extract_text()\n        elif file.endswith(\".txt\"):\n            print(\"indexing \" + file)\n            f = open(file,'r')\n            file_content = f.read()\n            f.close()\n        elif file.endswith(\".docx\"):\n            print(\"indexing \" + file)\n            file_content = getTextFromWord(file)\n        elif file.endswith(\".pptx\"):\n            print(\"indexing \" + file)\n            file_content = getTextFromPPTX(file)\n        else:\n            continue\n        text_splitter = TokenTextSplitter(chunk_size=500, chunk_overlap=50)\n        texts = text_splitter.split_text(file_content)\n        metadata = []\n        for i in range(0,len(texts)):\n            metadata.append({\"path\":file})\n        qdrant.add_texts(texts,metadatas=metadata)\n    print(onlyfiles)\n    print(\"Finished indexing!\")\n```", "```py\nfrom fastapi import FastAPI\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_qdrant import Qdrant\nfrom qdrant_client import QdrantClient\nfrom pydantic import BaseModel\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport environment_var\nimport os\nfrom openai import OpenAI\n\nclass Item(BaseModel):\n    query: str\n    def __init__(self, query: str) -> None:\n        super().__init__(query=query)\n```", "```py\nmodel_name = \"sentence-transformers/msmarco-bert-base-dot-v5\"\nmodel_kwargs = {'device': 'cpu'}\nencode_kwargs = {'normalize_embeddings': True}\nhf = HuggingFaceEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)\n\nos.environ[\"HF_TOKEN\"] = environment_var.hf_token\nuse_nvidia_api = False\nuse_quantized = True\nif environment_var.nvidia_key !=\"\":\n    client_ai = OpenAI(\n        base_url=\"https://integrate.api.nvidia.com/v1\",\n        api_key=environment_var.nvidia_key\n    )\n    use_nvidia_api = True\nelif use_quantized:\n    model_id = \"Kameshr/LLAMA-3-Quantized\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\nelse:\n    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n    )\n```", "```py\nclient = QdrantClient(path=\"qdrant/\")\ncollection_name = \"MyCollection\"\nqdrant = Qdrant(client, collection_name, hf)\n```", "```py\napp = FastAPI()\n\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n```", "```py\n@app.post(\"/search\")\ndef search(Item:Item):\n    query = Item.query\n    search_result = qdrant.similarity_search(\n        query=query, k=10\n    )\n    i = 0\n    list_res = []\n    for res in search_result:\n        list_res.append({\"id\":i,\"path\":res.metadata.get(\"path\"),\"content\":res.page_content})\n    return list_res\n\n@app.post(\"/ask_localai\")\nasync def ask_localai(Item:Item):\n    query = Item.query\n    search_result = qdrant.similarity_search(\n        query=query, k=10\n    )\n    i = 0\n    list_res = []\n    context = \"\"\n    mappings = {}\n    i = 0\n    for res in search_result:\n        context = context + str(i)+\"\\n\"+res.page_content+\"\\n\\n\"\n        mappings[i] = res.metadata.get(\"path\")\n        list_res.append({\"id\":i,\"path\":res.metadata.get(\"path\"),\"content\":res.page_content})\n        i = i +1\n\n    rolemsg = {\"role\": \"system\",\n               \"content\": \"Answer user's question using documents given in the context. In the context are documents that should contain an answer. Please always reference document id (in squere brackets, for example [0],[1]) of the document that was used to make a claim. Use as many citations and documents as it is necessary to answer question.\"}\n    messages = [\n        rolemsg,\n        {\"role\": \"user\", \"content\": \"Documents:\\n\"+context+\"\\n\\nQuestion: \"+query},\n    ]\n    if use_nvidia_api:\n        completion = client_ai.chat.completions.create(\n            model=\"meta/llama3-70b-instruct\",\n            messages=messages,\n            temperature=0.5,\n            top_p=1,\n            max_tokens=1024,\n            stream=False\n        )\n        response = completion.choices[0].message.content\n    else:\n        input_ids = tokenizer.apply_chat_template(\n                messages,\n                add_generation_prompt=True,\n                return_tensors=\"pt\"\n            ).to(model.device)\n\n        terminators = [\n            tokenizer.eos_token_id,\n            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n            ]\n\n        outputs = model.generate(\n            input_ids,\n            max_new_tokens=256,\n            eos_token_id=terminators,\n            do_sample=True,\n            temperature=0.2,\n            top_p=0.9,\n        )\n        response = tokenizer.decode(outputs[0][input_ids.shape[-1]:])\n    return {\"context\":list_res,\"answer\":response}\n```", "```py\nimport uvicorn\n\nif __name__==\"__main__\":\n    uvicorn.run(\"api:app\",host='0.0.0.0', port=8000, reload=False,  workers=3)\n```", "```py\nimport re\nimport streamlit as st\nimport requests\nimport json\nst.title('_:blue[Local GenAI Search]_ :sunglasses:')\nquestion = st.text_input(\"Ask a question based on your local files\", \"\")\nif st.button(\"Ask a question\"):\n    st.write(\"The current question is \\\"\", question+\"\\\"\")\n    url = \"http://127.0.0.1:8000/ask_localai\"\n\n    payload = json.dumps({\n      \"query\": question\n    })\n    headers = {\n      'Accept': 'application/json',\n      'Content-Type': 'application/json'\n    }\n\n    response = requests.request(\"POST\", url, headers=headers, data=payload)\n\n    answer = json.loads(response.text)[\"answer\"]\n    rege = re.compile(\"\\[Document\\ [0-9]+\\]|\\[[0-9]+\\]\")\n    m = rege.findall(answer)\n    num = []\n    for n in m:\n        num = num + [int(s) for s in re.findall(r'\\b\\d+\\b', n)]\n\n    st.markdown(answer)\n    documents = json.loads(response.text)['context']\n    show_docs = []\n    for n in num:\n        for doc in documents:\n            if int(doc['id']) == n:\n                show_docs.append(doc)\n    a = 1244\n    for doc in show_docs:\n        with st.expander(str(doc['id'])+\" - \"+doc['path']):\n            st.write(doc['content'])\n            with open(doc['path'], 'rb') as f:\n                st.download_button(\"Downlaod file\", f, file_name=doc['path'].split('/')[-1],key=a\n                )\n                a = a + 1\n```"]