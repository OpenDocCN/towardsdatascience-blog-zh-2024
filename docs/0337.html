<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Encoding Categorical Variables: A Deep Dive into Target Encoding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Encoding Categorical Variables: A Deep Dive into Target Encoding</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/encoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753?source=collection_archive---------2-----------------------#2024-02-05">https://towardsdatascience.com/encoding-categorical-variables-a-deep-dive-into-target-encoding-2862217c2753?source=collection_archive---------2-----------------------#2024-02-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="9c14" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Data comes in different shapes and forms. One of those shapes and forms is known as categorical data.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Juan Jose Munoz" class="l ep by dd de cx" src="../Images/b42d72e9e2a2eaf11da5465e9b041d53.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YeUOkLrhcC48xk632N_gjw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2862217c2753--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@juanjosemunozp?source=post_page---byline--2862217c2753--------------------------------" rel="noopener follow">Juan Jose Munoz</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2862217c2753--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1578" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">This poses a problem because most Machine Learning algorithms use only numerical data as input</strong>. However, categorical data is usually not a challenge to deal with, thanks to simple, well-defined functions that transform them into numerical values. If you have taken any data science course, you will be familiar with the one hot encoding strategy for categorical features. This strategy is great when your features have limited categories. However, you will run into some issues when dealing with high cardinal features (features with many categories)</p><p id="131b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Here is how you can use target encoding to transform Categorical features into numerical values.</strong></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/f5faebf9812d3395ba1b4d55a6ddf0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7zCJa5LG048EJDIz"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Photo by <a class="af ny" href="https://unsplash.com/@sonika_agarwal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sonika Agarwal</a> on <a class="af ny" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="f477" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">The problem with One Hot encoding</h1><p id="b7c9" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><strong class="ml fr">Early in any data science course, you are introduced to one hot encoding as a key strategy to deal with categorical values</strong>, and rightfully so, as this strategy works really well on low cardinal features (features with limited categories).</p><p id="1306" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">In a nutshell, One hot encoding transforms each category into a binary vector,</strong> where the corresponding category is marked as ‘True’ or ‘1’, and all other categories are marked with ‘False’ or ‘0’.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="4686" class="pe oa fq pb b bg pf pg l ph pi">import pandas as pd<br/><br/># Sample categorical data<br/>data = {'Category': ['Red', 'Green', 'Blue', 'Red', 'Green']}<br/><br/># Create a DataFrame<br/>df = pd.DataFrame(data)<br/><br/># Perform one-hot encoding<br/>one_hot_encoded = pd.get_dummies(df['Category'])<br/><br/># Display the result<br/>print(one_hot_encoded)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pj"><img src="../Images/ba9c89d63e6cba7e5db81567deaf9b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*y0suYUXfsaPHwRcS1dC05Q.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">One hot encoding output — we could improve this by dropping one column because if we know Blue and Green, we can figure the value of Red. Image by author</figcaption></figure><p id="7eeb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While this works great for features with limited categories <em class="pk">(Less than 10–20 categories)</em>, as the number of categories increases, the one-hot encoded vectors become longer and sparser, potentially leading to increased memory usage and computational complexity, let’s look at an example.</p><p id="a4ca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pk">The below code uses Amazon Employee Access data, made publicity available in kaggle: </em><a class="af ny" href="https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge" rel="noopener ugc nofollow" target="_blank"><em class="pk">https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge</em></a></p><p id="b6f7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The data contains eight categorical feature columns indicating characteristics of the required resource, role, and workgroup of the employee at Amazon.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="da48" class="pe oa fq pb b bg pf pg l ph pi">data.info()</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pl"><img src="../Images/5e6c072ac9109edba362fc86ca6496ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*nmLY9pXhTV3pN0xZj067TQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Column information. Image by author</figcaption></figure><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="dd1b" class="pe oa fq pb b bg pf pg l ph pi"># Display the number of unique values in each column<br/>unique_values_per_column = data.nunique()<br/><br/>print("Number of unique values in each column:")<br/>print(unique_values_per_column)</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pm"><img src="../Images/3272325aaa9c9f09c59e5aa935a95c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*3TsBdlBWa351Ge518SVnaQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The eight features have high cardinality. Image by author</figcaption></figure><p id="3f6d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Using one hot encoding could be challenging in a dataset like this due to the high number of distinct categories for each feature.</strong></p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="bbe1" class="pe oa fq pb b bg pf pg l ph pi">#Initial data memory usage<br/>memory_usage = data.memory_usage(deep=True)<br/>total_memory_usage = memory_usage.sum()<br/>print(f"\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB")</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pn"><img src="../Images/c46013a222c086c67eca8bdf08167d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*YTSnmbhrO1FyKf6FULMCtA.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The initial dataset is 11.24 MB. Image by author</figcaption></figure><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="e877" class="pe oa fq pb b bg pf pg l ph pi">#one-hot encoding categorical features<br/>data_encoded = pd.get_dummies(data, <br/>                              columns=data.select_dtypes(include='object').columns,<br/>                              drop_first=True)<br/><br/>data_encoded.shape</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng po"><img src="../Images/c18fd40a3d00ecb79244ef3e78d51923.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*MGEzjYuzEnxp-tXDMu9NAg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">After on-hot encoding, the dataset has 15 618 columns. Image by author</figcaption></figure><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pp"><img src="../Images/8d92598376535d840d3f52d59a26e239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_LxfI7w568e50w4ommA4A.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The resulting data set is highly sparse, meaning it contains a lot of 0s and 1. Image by author</figcaption></figure><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="426d" class="pe oa fq pb b bg pf pg l ph pi"># Memory usage for the one-hot encoded dataset<br/>memory_usage = data_encoded.memory_usage(deep=True)<br/>total_memory_usage = memory_usage.sum()<br/>print(f"\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB")</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pq"><img src="../Images/ca2ecf4f83515a06eae3394bcd8aa012.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*HMhIIfH5C6mRPdg7Gqm_0Q.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Dataset memory usage increased to 488.08 MB due to the increased number of columns. Image by author</figcaption></figure><p id="fac4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As you can see, one-hot encoding is not a viable solution to deal with high cardinal categorical features, as it significantly increases the size of the dataset.</p><p id="0092" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">In cases with high cardinal features, target encoding is a better option.</strong></p><h1 id="5d14" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Target encoding — overview of basic principle</h1><p id="f751" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">Target encoding transforms a categorical feature into a numeric feature without adding any extra columns, avoiding turning the dataset into a larger and sparser dataset.</p><p id="a75b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Target encoding works by converting each category of a categorical feature into its corresponding expected value.</strong> The approach to calculating the expected value will depend on the value you are trying to predict.</p><blockquote class="pr ps pt"><p id="cbc5" class="mj mk pk ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For Regression problems, the expected value is simply the average value for that category.</p><p id="d142" class="mj mk pk ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For Classification problems, the expected value is the conditional probability given that category.</p></blockquote><p id="afe2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In both cases, we can get the results by simply using the ‘group_by’ function in pandas.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="e4d2" class="pe oa fq pb b bg pf pg l ph pi">#Example of how to calculate the expected value for Target encoding of a Binary outcome<br/>expected_values = data.groupby('ROLE_TITLE')['ACTION'].value_counts(normalize=True).unstack()<br/>expected_values</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng pu"><img src="../Images/492473834fd253f24f7d6a3fe7c7dcb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*iEAaB5O9a3_4uuzVuUFtbQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The resulting table indicates the probability of each `ACTION` outcome by unique `Role_title` ID. Image by author</figcaption></figure><p id="fc42" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The resulting table indicates the probability of each “<em class="pk">ACTION”</em> outcome by unique “<em class="pk">ROLE_TITLE</em>” id. All that is left to do is replace the “<em class="pk">ROLE_TITLE</em>” id with the values from the probability of “ACTION” being 1 in the original dataset. <em class="pk">(i.e instead of category 117879 the dataset will show 0.889331)</em></p><p id="90ee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">While this can give us an intuition of how target encoding works, using this simple method runs the risk of overfitting</strong>. Especially for rare categories, as in those cases, target encoding will essentially provide the target value to the model. Also, the above method can only deal with seen categories, so if your test data has a new category, it won’t be able to handle it.</p><p id="c68a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">To avoid those errors, you need to make the target encoding transformer more robust.</strong></p><h1 id="a838" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Defining a Target encoding class</h1><p id="5943" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">To make target encoding more robust, you can create a custom transformer class and integrate it with scikit-learn so that it can be used in any model pipeline.</p><p id="7add" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pk">NOTE: The below code is taken from the book “The Kaggle Book” and can be found in Kaggle: </em><a class="af ny" href="https://www.kaggle.com/code/lucamassaron/meta-features-and-target-encoding" rel="noopener ugc nofollow" target="_blank"><em class="pk">https://www.kaggle.com/code/lucamassaron/meta-features-and-target-encoding</em></a></p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="092c" class="pe oa fq pb b bg pf pg l ph pi">import numpy as np<br/>import pandas as pd<br/><br/>from sklearn.base import BaseEstimator, TransformerMixin<br/><br/>class TargetEncode(BaseEstimator, TransformerMixin):<br/>    <br/>    def __init__(self, categories='auto', k=1, f=1, <br/>                 noise_level=0, random_state=None):<br/>        if type(categories)==str and categories!='auto':<br/>            self.categories = [categories]<br/>        else:<br/>            self.categories = categories<br/>        self.k = k<br/>        self.f = f<br/>        self.noise_level = noise_level<br/>        self.encodings = dict()<br/>        self.prior = None<br/>        self.random_state = random_state<br/>        <br/>    def add_noise(self, series, noise_level):<br/>        return series * (1 + noise_level *   <br/>                         np.random.randn(len(series)))<br/>        <br/>    def fit(self, X, y=None):<br/>        if type(self.categories)=='auto':<br/>            self.categories = np.where(X.dtypes == type(object()))[0]<br/>        <br/>        temp = X.loc[:, self.categories].copy()<br/>        temp['target'] = y<br/>        self.prior = np.mean(y)<br/>        for variable in self.categories:<br/>            avg = (temp.groupby(by=variable)['target']<br/>                       .agg(['mean', 'count']))<br/>            # Compute smoothing <br/>            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /                 <br/>                         self.f)))<br/>            # The bigger the count the less full_avg is accounted<br/>            self.encodings[variable] = dict(self.prior * (1 -  <br/>                             smoothing) + avg['mean'] * smoothing)<br/>            <br/>        return self<br/>    <br/>    def transform(self, X):<br/>        Xt = X.copy()<br/>        for variable in self.categories:<br/>            Xt[variable].replace(self.encodings[variable], <br/>                                 inplace=True)<br/>            unknown_value = {value:self.prior for value in <br/>                             X[variable].unique() <br/>                             if value not in <br/>                             self.encodings[variable].keys()}<br/>            if len(unknown_value) &gt; 0:<br/>                Xt[variable].replace(unknown_value, inplace=True)<br/>            Xt[variable] = Xt[variable].astype(float)<br/>            if self.noise_level &gt; 0:<br/>                if self.random_state is not None:<br/>                    np.random.seed(self.random_state)<br/>                Xt[variable] = self.add_noise(Xt[variable], <br/>                                              self.noise_level)<br/>        return Xt<br/>    <br/>    def fit_transform(self, X, y=None):<br/>        self.fit(X, y)<br/>        return self.transform(X)</span></pre><p id="fa99" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It might look daunting at first, but let’s break down each part of the code to understand how to create a robust Target encoder.</p><h2 id="304b" class="pv oa fq bf ob pw px py oe pz qa qb oh ms qc qd qe mw qf qg qh na qi qj qk ql bk">Class Definition</h2><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="1359" class="pe oa fq pb b bg pf pg l ph pi">class TargetEncode(BaseEstimator, TransformerMixin):</span></pre><p id="1cd9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This first step ensures that you can use this transformer class in scikit-learn pipelines for data preprocessing, feature engineering, and machine learning workflows. It achieves this by inheriting the scikit-learn classes <em class="pk">BaseEstimator</em> and <em class="pk">TransformerMixin</em>.</p><p id="7c82" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Inheritance allows the <em class="pk">TargetEncode</em> class to reuse or override methods and attributes defined in the base classes, in this case, <em class="pk">BaseEstimator </em>and<em class="pk"> TransformerMixin</em></p><p id="22fa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pk">BaseEstimator</em> is a base class for all scikit-learn estimators. Estimators are objects in scikit-learn with a “fit” method for training on data and a “predict” method for making predictions.</p><p id="9d6a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pk">TransformerMixin</em> is a mixin class for transformers in scikit-learn, it provides additional methods such as “fit_transform”, which combines fitting and transforming in a single step.</p><p id="5b4b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Inheriting from <em class="pk">BaseEstimator</em> &amp; <em class="pk">TransformerMixin,</em> allows TargetEncode to implement these methods, making it compatible with the scikit-learn API.</strong></p><h2 id="e6ff" class="pv oa fq bf ob pw px py oe pz qa qb oh ms qc qd qe mw qf qg qh na qi qj qk ql bk">Defining the constructor</h2><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="05bd" class="pe oa fq pb b bg pf pg l ph pi">def __init__(self, categories='auto', k=1, f=1, <br/>                 noise_level=0, random_state=None):<br/>        if type(categories)==str and categories!='auto':<br/>            self.categories = [categories]<br/>        else:<br/>            self.categories = categories<br/>        self.k = k<br/>        self.f = f<br/>        self.noise_level = noise_level<br/>        self.encodings = dict()<br/>        self.prior = None<br/>        self.random_state = random_state</span></pre><p id="1cd7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This second step defines the constructor for the <em class="pk">“TargetEncode</em>” class and initializes the instance variables with default or user-specified values.</p><p id="b74d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The “<em class="pk">categories</em>” parameter determines which columns in the input data should be considered as categorical variables for target encoding. It is Set by default to ‘auto’ to automatically identify categorical columns during the fitting process.</p><p id="8076" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The parameters k, f, and noise_level control the smoothing effect during target encoding and the level of noise added during transformation.</p><h2 id="949b" class="pv oa fq bf ob pw px py oe pz qa qb oh ms qc qd qe mw qf qg qh na qi qj qk ql bk">Adding noise</h2><p id="1746" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><strong class="ml fr">This next step is very important to avoid overfitting</strong>.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="b447" class="pe oa fq pb b bg pf pg l ph pi">def add_noise(self, series, noise_level):<br/>        return series * (1 + noise_level *   <br/>                         np.random.randn(len(series)))</span></pre><p id="9fe0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The “<em class="pk">add_noise</em>” method adds random noise to introduce variability and prevent overfitting during the transformation phase.</p><p id="8426" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="pk">“np.random.randn(len(series))”</em> generates an array of random numbers from a standard normal distribution (mean = 0, standard deviation = 1).</p><p id="17f1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Multiplying this array by “<em class="pk">noise_level” s</em>cales the random noise based on the specified noise level.”</p><p id="63f0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">This step contributes to the robustness and generalization capabilities of the target encoding process.</strong></p><h2 id="87e7" class="pv oa fq bf ob pw px py oe pz qa qb oh ms qc qd qe mw qf qg qh na qi qj qk ql bk">Fitting the Target encoder</h2><p id="3ced" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">This part of the code trains the target encoder on the provided data by calculating the target encodings for categorical columns and storing them for later use during transformation.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="dc4f" class="pe oa fq pb b bg pf pg l ph pi">def fit(self, X, y=None):<br/>        if type(self.categories)=='auto':<br/>            self.categories = np.where(X.dtypes == type(object()))[0]<br/>        <br/>        temp = X.loc[:, self.categories].copy()<br/>        temp['target'] = y<br/>        self.prior = np.mean(y)<br/>        for variable in self.categories:<br/>            avg = (temp.groupby(by=variable)['target']<br/>                       .agg(['mean', 'count']))<br/>            # Compute smoothing <br/>            smoothing = (1 / (1 + np.exp(-(avg['count'] - self.k) /                 <br/>                         self.f)))<br/>            # The bigger the count the less full_avg is accounted<br/>            self.encodings[variable] = dict(self.prior * (1 -  <br/>                             smoothing) + avg['mean'] * smoothing)</span></pre><p id="446e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The smoothing term helps prevent overfitting, especially when dealing with categories with small samples.</p><p id="481c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The method follows the scikit-learn convention for fit methods in transformers.</strong></p><p id="8640" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It starts by checking and identifying the categorical columns and creating a temporary DataFrame, containing only the selected categorical columns from the input X and the target variable y.</p><p id="ea9c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The prior mean of the target variable is calculated and stored in the prior attribute. <strong class="ml fr">This represents the overall mean of the target variable across the entire dataset.</strong></p><p id="e4a0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Then, it calculates the mean and count of the target variable for each category using the group-by method, as seen previously.</strong></p><p id="42db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There is an additional smoothing step to prevent overfitting on categories with small numbers of samples. Smoothing is calculated based on the number of samples in each category. The larger the count, the less the smoothing effect.</p><p id="ba4f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The calculated encodings for each category in the current variable are stored in the encodings dictionary. This dictionary will be used later during the transformation phase.</p><h2 id="f2a3" class="pv oa fq bf ob pw px py oe pz qa qb oh ms qc qd qe mw qf qg qh na qi qj qk ql bk">Transforming the data</h2><p id="16ac" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">This part of the code replaces the original categorical values with their corresponding target-encoded values stored in <em class="pk">self.encodings.</em></p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="b63f" class="pe oa fq pb b bg pf pg l ph pi">def transform(self, X):<br/>        Xt = X.copy()<br/>        for variable in self.categories:<br/>            Xt[variable].replace(self.encodings[variable], <br/>                                 inplace=True)<br/>            unknown_value = {value:self.prior for value in <br/>                             X[variable].unique() <br/>                             if value not in <br/>                             self.encodings[variable].keys()}<br/>            if len(unknown_value) &gt; 0:<br/>                Xt[variable].replace(unknown_value, inplace=True)<br/>            Xt[variable] = Xt[variable].astype(float)<br/>            if self.noise_level &gt; 0:<br/>                if self.random_state is not None:<br/>                    np.random.seed(self.random_state)<br/>                Xt[variable] = self.add_noise(Xt[variable], <br/>                                              self.noise_level)<br/>        return Xt</span></pre><p id="b44b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This step has an additional robustness check to ensure the target encoder can handle new or unseen categories. <strong class="ml fr">For those new or unknown categories, it replaces them with the mean of the target variable</strong> stored in the prior_mean variable.</p><p id="7cab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you need more robustness against overfitting, you can set up a <em class="pk">noise_level</em> greater than 0 to add random noise to the encoded values.</p><p id="55e1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <em class="pk">fit_transform</em> method combines the functionality of fitting and transforming the data by first fitting the transformer to the training data and then transforming it based on the calculated encodings.</p><p id="8a24" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that you understand how the code works, let’s see it in action.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="cf28" class="pe oa fq pb b bg pf pg l ph pi">#Instantiate TargetEncode class<br/>te = TargetEncode(categories='ROLE_TITLE')<br/>te.fit(data, data['ACTION'])<br/>te.transform(data[['ROLE_TITLE']])</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qm"><img src="../Images/22681eff29fda72f848991a453fed087.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*MBGYTcP-wfgqI9Ua9J1M1w.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output with Target encoded Role title. Image by author</figcaption></figure><p id="47cc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Target encoder replaced each “<em class="pk">ROLE_TITLE</em>” id with the probability of each category. Now, let's do the same for all features and check the memory usage after using Target Encoding.</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="5cdf" class="pe oa fq pb b bg pf pg l ph pi">y = data['ACTION']<br/>features = data.drop('ACTION',axis=1)<br/><br/>te = TargetEncode(categories=features.columns)<br/>te.fit(features,y)<br/>te_data = te.transform(features)<br/><br/>te_data.head()</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qn"><img src="../Images/16cffc6f388731f36b184f0376183964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jaWrdE-7OgZnaR7YpbQxwQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output, Target encoded features. Image by author</figcaption></figure><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="cb3f" class="pe oa fq pb b bg pf pg l ph pi">memory_usage = te_data.memory_usage(deep=True)<br/>total_memory_usage = memory_usage.sum()<br/>print(f"\nTotal memory usage of the DataFrame: {total_memory_usage / (1024 ** 2):.2f} MB")</span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qo"><img src="../Images/9d24943bd0681c1ba70420d2f47e6bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*1_XH_KWlZ-h_5V_894CgoQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The resulting dataset only uses 2.25 MB, compared to 488.08 MB from the one-hot encoder. Image by author</figcaption></figure><p id="f838" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Target encoding successfully transformed the categorical data into numerical without creating extra columns or increasing memory usage.</p><h1 id="d41f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Target encoding with SciKitLearn API</h1><p id="31c4" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk"><strong class="ml fr">So far we have created our own target encoder class, however you don’t have to do this anymore.</strong></p><p id="0375" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In scikit-learn version 1.3 release, somewhere around June 2023, they introduced the Target Encoder class to their API. Here is how you can use target encoding with Scikit Learn</p><pre class="ni nj nk nl nm pa pb pc bp pd bb bk"><span id="86f2" class="pe oa fq pb b bg pf pg l ph pi">from sklearn.preprocessing import TargetEncoder<br/><br/>#Splitting the data<br/>y = data['ACTION']<br/>features = data.drop('ACTION',axis=1)<br/><br/>#Specify the target type<br/>te = TargetEncoder(smooth="auto",target_type='binary')<br/>X_trans = te.fit_transform(features, y)<br/><br/>#Creating a Dataframe<br/>features_encoded = pd.DataFrame(X_trans, columns = features.columns)<br/></span></pre><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qp"><img src="../Images/1aaf7df1af07f04088bc8380e62d514f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTEfoD6EJzLoOiZAPZx8-g.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Output from sklearn Target Encoder transformation. Image by author</figcaption></figure><p id="ecde" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that we are getting slightly different results from the manual Target encoder class because of the smooth parameter and randomness on the noise level.</p><p id="c23c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As you see, sklearn makes it easy to run target encoding transformations. <strong class="ml fr">However, it is important to understand how the transformation works under the hood first to understand and explain the output.</strong></p><p id="ed8b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While Target encoding is a powerful encoding method, it’s important to consider the specific requirements and characteristics of your dataset and choose the encoding method that best suits your needs and the requirements of the machine learning algorithm you plan to use.</p><h1 id="2be9" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><p id="21e4" class="pw-post-body-paragraph mj mk fq ml b go ov mn mo gr ow mq mr ms ox mu mv mw oy my mz na oz nc nd ne fj bk">[1] Banachewicz, K. &amp; Massaron, L. (2022). <em class="pk">The Kaggle Book: Data Analysis and Machine Learning for Competitive Data Science</em>. Packt&gt;</p><p id="7fcf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] Massaron, L. (2022, January). Amazon Employee Access Challenge. Retrieved February 1, 2024, from <a class="af ny" href="https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/datasets/lucamassaron/amazon-employee-access-challenge</a></p><p id="8d8c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] Massaron, L. Meta-features and target encoding. Retrieved February 1, 2024, from <a class="af ny" href="https://www.kaggle.com/luca-massaron/meta-features-and-target-encoding" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/luca-massaron/meta-features-and-target-encoding</a></p><p id="29cb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] Scikit-learn.<code class="cx qq qr qs pb b">sklearn.preprocessing.TargetEncoder</code>. In scikit-learn: Machine learning in Python (Version 1.3). Retrieved February 1, 2024, from <a class="af ny" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html</a></p></div></div></div></div>    
</body>
</html>