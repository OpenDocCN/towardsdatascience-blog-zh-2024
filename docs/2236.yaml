- en: Deploying your Llama Model via vLLM using SageMaker Endpoint
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12](https://towardsdatascience.com/deploying-your-llama-model-via-vllm-using-sagemaker-endpoint-f02b424da124?source=collection_archive---------8-----------------------#2024-09-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leveraging AWS’s MLOps platform to serve your LLM models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[![Jake
    Teo](../Images/9687f43822fab69befb750a8ec58516d.png)](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    [Jake Teo](https://medium.com/@teosiyang?source=post_page---byline--f02b424da124--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f02b424da124--------------------------------)
    ·8 min read·Sep 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a52a22e39707bd0217ce2e9ce9454883.png)'
  prefs: []
  type: TYPE_IMG
- en: Instances in an MLOps workflow that require an inference endpoint (created by
    author).
  prefs: []
  type: TYPE_NORMAL
- en: In any machine learning project, the goal is to train a model that can be used
    by others to derive a good prediction. To do that, the model needs to be served
    for inference. Several parts in this workflow require this inference endpoint,
    namely, for model evaluation, before releasing it to the development, staging,
    and finally production environment for the end-users to consume.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will demonstrate how to deploy the latest LLM and serving
    technologies, namely Llama and vLLM, using AWS’s SageMaker endpoint and its DJL
    image. What are these components and how do they make up an inference endpoint?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c015c60e62b49d2c87dc9949dcd2dd07.png)'
  prefs: []
  type: TYPE_IMG
- en: How each of these components together serves the model in AWS. SageMaker endpoint
    is the GPU instance, DJL is the template Docker image, and vLLM is the model server
    (created by author).
  prefs: []
  type: TYPE_NORMAL
- en: '**SageMaker** is an AWS service that consists of a large suite of tools and
    services to manage a machine learning lifecycle. Its inference service is known
    as SageMaker endpoint. Under the hood, it is essentially a virtual machine self-managed
    by AWS.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DJL** (Deep Java Library) is an open-source library developed by AWS used
    to develop LLM inference docker images, including vLLM [2]. This image is used
    in…'
  prefs: []
  type: TYPE_NORMAL
