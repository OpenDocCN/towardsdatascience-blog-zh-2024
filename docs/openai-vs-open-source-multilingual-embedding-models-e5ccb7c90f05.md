# OpenAI ä¸å¼€æºå¤šè¯­è¨€åµŒå…¥æ¨¡å‹

> åŸæ–‡ï¼š[https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24](https://towardsdatascience.com/openai-vs-open-source-multilingual-embedding-models-e5ccb7c90f05?source=collection_archive---------0-----------------------#2024-02-24)

## é€‰æ‹©æœ€é€‚åˆä½ æ•°æ®çš„æ¨¡å‹

[](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Yann-AÃ«l Le Borgne](../Images/acc1c8b32373d7f345064b89b51869fd.png)](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------) [Yann-AÃ«l Le Borgne](https://ya-lb.medium.com/?source=post_page---byline--e5ccb7c90f05--------------------------------)

Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e5ccb7c90f05--------------------------------) Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2024å¹´2æœˆ24æ—¥

--

![](../Images/d4106437a23e5edaba980ce8486937d9.png)

æˆ‘ä»¬å°†ä½¿ç”¨æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆä½œä¸ºæˆ‘ä»¬çš„åµŒå…¥æ¨¡å‹æ¯”è¾ƒçš„æ•°æ®è¯­æ–™åº“ã€‚å›¾åƒç”±Dall-E 3ç”Ÿæˆã€‚

OpenAIæœ€è¿‘å‘å¸ƒäº†ä»–ä»¬çš„æ–°ä¸€ä»£åµŒå…¥æ¨¡å‹ï¼Œç§°ä¸º*embedding v3*ï¼Œä»–ä»¬[æè¿°](https://openai.com/blog/new-embedding-models-and-api-updates)è¿™äº›æ¨¡å‹æ˜¯æ€§èƒ½æœ€å¼ºçš„åµŒå…¥æ¨¡å‹ï¼Œå…·æœ‰æ›´é«˜çš„å¤šè¯­è¨€æ€§èƒ½ã€‚è¯¥æ¨¡å‹æœ‰ä¸¤ç§ç±»å‹ï¼šä¸€ç§è¾ƒå°çš„ï¼Œç§°ä¸º`text-embedding-3-small`ï¼Œå¦ä¸€ç§è¾ƒå¤§ä¸”æ›´å¼ºå¤§çš„ï¼Œç§°ä¸º`text-embedding-3-large`ã€‚

å…³äºè¿™äº›æ¨¡å‹çš„è®¾è®¡å’Œè®­ç»ƒæ–¹å¼ï¼Œå…¬å¼€çš„ä¿¡æ¯éå¸¸å°‘ã€‚ä¸ä»–ä»¬ä¹‹å‰çš„åµŒå…¥æ¨¡å‹å‘å¸ƒï¼ˆ2022å¹´12æœˆï¼Œada-002æ¨¡å‹ç±»ï¼‰ä¸€æ ·ï¼ŒOpenAIå†æ¬¡é€‰æ‹©äº†ä¸€ç§é—­æºçš„æ–¹æ³•ï¼Œæ¨¡å‹åªèƒ½é€šè¿‡ä»˜è´¹APIè®¿é—®ã€‚

ä½†æ˜¯ï¼Œæ€§èƒ½çœŸçš„é‚£ä¹ˆå¥½å—ï¼Œå€¼å¾—ä»˜è´¹å—ï¼Ÿ

**è¿™ç¯‡æ–‡ç« çš„åŠ¨æœºæ˜¯é€šè¿‡å®è¯æ¯”è¾ƒè¿™äº›æ–°æ¨¡å‹ä¸å®ƒä»¬çš„å¼€æºå¯¹æ‰‹çš„è¡¨ç°**ã€‚æˆ‘ä»¬å°†ä¾èµ–ä¸€ä¸ªæ•°æ®æ£€ç´¢å·¥ä½œæµï¼Œåœ¨è¯¥å·¥ä½œæµä¸­ï¼Œå¿…é¡»æ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ‰¾åˆ°è¯­æ–™åº“ä¸­æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚

æˆ‘ä»¬çš„è¯­æ–™åº“å°†æ˜¯[æ¬§æ´²äººå·¥æ™ºèƒ½æ³•æ¡ˆ](https://artificialintelligenceact.eu/)ï¼Œè¯¥æ³•æ¡ˆç›®å‰æ­£å¤„äºæœ€ç»ˆéªŒè¯é˜¶æ®µã€‚è¿™ä¸ªè¯­æ–™åº“çš„ä¸€ä¸ªæœ‰è¶£ç‰¹ç‚¹æ˜¯ï¼Œé™¤äº†å®ƒæ˜¯å…¨çƒé¦–ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„æ³•å¾‹æ¡†æ¶å¤–ï¼Œå®ƒè¿˜æä¾›24ç§è¯­è¨€ç‰ˆæœ¬ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿæ¯”è¾ƒ**ä¸åŒè¯­è¨€å®¶æ—ä¹‹é—´çš„æ•°æ®æ£€ç´¢å‡†ç¡®æ€§**ã€‚

æœ¬æ–‡å°†æ¶µç›–ä»¥ä¸‹ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š

+   ä»å¤šè¯­è¨€æ–‡æœ¬è¯­æ–™åº“ä¸­ç”Ÿæˆä¸€ä¸ªå®šåˆ¶çš„åˆæˆé—®ç­”æ•°æ®é›†

+   æ¯”è¾ƒOpenAIä¸æœ€å…ˆè¿›çš„å¼€æºåµŒå…¥æ¨¡å‹åœ¨æ­¤è‡ªå®šä¹‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®æ€§ã€‚

ä¸ºäº†é‡ç°æœ¬æ–‡ä¸­å±•ç¤ºçš„ç»“æœï¼Œä»£ç å’Œæ•°æ®å·²ç»å…¬å¼€åœ¨[è¿™ä¸ªGithubä»“åº“](https://github.com/Yannael/multilingual-embeddings)ä¸­ã€‚è¯·æ³¨æ„ï¼Œã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹ä½œä¸ºç¤ºä¾‹ä½¿ç”¨ï¼Œæœ¬æ–‡ä¸­éµå¾ªçš„æ–¹æ³•å¯ä»¥é€‚åº”å…¶ä»–æ•°æ®è¯­æ–™åº“ã€‚

# ç”Ÿæˆè‡ªå®šä¹‰çš„é—®ç­”æ•°æ®é›†

è®©æˆ‘ä»¬é¦–å…ˆå¼€å§‹ç”Ÿæˆè‡ªå®šä¹‰æ•°æ®ä¸Šçš„é—®ç­”æ•°æ®é›†ï¼ˆQ/Aï¼‰ï¼Œç”¨äºè¯„ä¼°ä¸åŒåµŒå…¥æ¨¡å‹çš„æ€§èƒ½ã€‚ç”Ÿæˆè‡ªå®šä¹‰é—®ç­”æ•°æ®é›†æœ‰ä¸¤ä¸ªå¥½å¤„ã€‚é¦–å…ˆï¼Œå®ƒé¿å…äº†åè§ï¼Œç¡®ä¿æ•°æ®é›†æœªå‚ä¸åµŒå…¥æ¨¡å‹çš„è®­ç»ƒï¼Œè¿™ç§æƒ…å†µå¯èƒ½å‘ç”Ÿåœ¨å‚è€ƒåŸºå‡†ä¸Šï¼Œå¦‚[MTEB](https://huggingface.co/spaces/mteb/leaderboard)ã€‚å…¶æ¬¡ï¼Œå®ƒä½¿å¾—è¯„ä¼°å¯ä»¥æ ¹æ®ç‰¹å®šæ•°æ®è¯­æ–™åº“è¿›è¡Œå®šåˆ¶ï¼Œè¿™åœ¨æ£€ç´¢å¢å¼ºåº”ç”¨ï¼ˆRAGï¼‰ç­‰æƒ…å†µä¸‹å°¤ä¸ºé‡è¦ã€‚

æˆ‘ä»¬å°†éµå¾ª[Llama Indexæ–‡æ¡£ä¸­](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)å»ºè®®çš„ç®€å•æµç¨‹ã€‚é¦–å…ˆå°†è¯­æ–™åº“æ‹†åˆ†æˆå¤šä¸ªå—ã€‚ç„¶åï¼Œå¯¹äºæ¯ä¸ªå—ï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä¸€ç»„åˆæˆé—®é¢˜ï¼Œä½¿å¾—é—®é¢˜çš„ç­”æ¡ˆä½äºç›¸åº”çš„å—ä¸­ã€‚è¯¥è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![](../Images/debcd01ca6179cf42bb71ec7c684627d.png)

ä¸ºä½ çš„æ•°æ®ç”Ÿæˆé—®ç­”æ•°æ®é›†ï¼Œæ–¹æ³•å‚è€ƒ[Llama Index](https://blog.llamaindex.ai/fine-tuning-embeddings-for-rag-with-synthetic-data-e534409a3971)

å®ç°è¿™ä¸ªç­–ç•¥åœ¨ä½¿ç”¨åƒLlama Indexè¿™æ ·çš„LLMæ•°æ®æ¡†æ¶æ—¶éå¸¸ç›´æ¥ã€‚è¯­æ–™åº“çš„åŠ è½½å’Œæ–‡æœ¬çš„æ‹†åˆ†å¯ä»¥é€šè¿‡é«˜é˜¶å‡½æ•°æ–¹ä¾¿åœ°å®Œæˆï¼Œå¦‚ä¸‹é¢çš„ä»£ç æ‰€ç¤ºã€‚

```py
from llama_index.readers.web import SimpleWebPageReader
from llama_index.core.node_parser import SentenceSplitter

language = "EN"
url_doc = "https://eur-lex.europa.eu/legal-content/"+language+"/TXT/HTML/?uri=CELEX:52021PC0206"

documents = SimpleWebPageReader(html_to_text=True).load_data([url_doc])

parser = SentenceSplitter(chunk_size=1000)
nodes = parser.get_nodes_from_documents(documents, show_progress=True)
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œè¯­æ–™åº“æ˜¯ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹çš„è‹±æ–‡ç‰ˆï¼Œç›´æ¥ä»ç½‘ç»œä¸Šè·å–ï¼Œä½¿ç”¨çš„æ˜¯è¿™ä¸ª[å®˜æ–¹ç½‘å€](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯2021å¹´4æœˆçš„è‰æ¡ˆç‰ˆæœ¬ï¼Œå› ä¸ºæœ€ç»ˆç‰ˆæœ¬å°šæœªæä¾›æ‰€æœ‰æ¬§ç›Ÿè¯­è¨€ã€‚åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œç½‘å€ä¸­çš„è‹±æ–‡å¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–23ç§æ¬§ç›Ÿå®˜æ–¹è¯­è¨€ä¸­çš„ä»»ä½•ä¸€ç§ï¼Œä»¥è·å–ä¸åŒè¯­è¨€çš„æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼ŒBGä»£è¡¨ä¿åŠ åˆ©äºšè¯­ï¼ŒESä»£è¡¨è¥¿ç­ç‰™è¯­ï¼ŒCSä»£è¡¨æ·å…‹è¯­ï¼Œç­‰ç­‰ï¼‰ã€‚

![](../Images/b228519bd3b36c7cec689ffd01b3226c.png)

ä¸‹è½½ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆã€‹åœ¨24ç§å®˜æ–¹æ¬§ç›Ÿè¯­è¨€ä¸­çš„é“¾æ¥ï¼ˆæ¥è‡ª[æ¬§ç›Ÿå®˜ç½‘](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206)ï¼‰

æˆ‘ä»¬ä½¿ç”¨SentenceSplitterå¯¹è±¡å°†æ–‡æ¡£æ‹†åˆ†æˆ1000ä¸ªæ ‡è®°çš„å—ã€‚å¯¹äºè‹±æ–‡æ–‡æœ¬ï¼Œè¿™å¤§çº¦ä¼šç”Ÿæˆ100ä¸ªå—ã€‚

ç„¶åï¼Œæ¯ä¸ªæ–‡æ¡£å—ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™ä»¥ä¸‹æç¤ºï¼ˆ[Llama Indexåº“ä¸­å»ºè®®çš„é»˜è®¤æç¤º](https://github.com/run-llama/llama_index/blob/c058f2531ea86ee74822cb1421ceaeee7098a99f/llama_index/finetuning/embeddings/common.py#L51)ï¼‰ï¼š

```py
prompts={}
prompts["EN"] = """\
Context information is below.

---------------------
{context_str}
---------------------

Given the context information and not prior knowledge, generate only questions based on the below query.

You are a Teacher/ Professor. Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination.
The questions should be diverse in nature across the document. Restrict the questions to the context information provided."
"""
```

è¯¥æç¤ºçš„ç›®çš„æ˜¯ç”Ÿæˆå…³äºæ–‡æ¡£å—çš„é—®é¢˜ï¼Œå°±åƒè€å¸ˆåœ¨å‡†å¤‡å³å°†åˆ°æ¥çš„å°æµ‹éªŒä¸€æ ·ã€‚æ¯ä¸ªæ–‡æ¡£å—è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡é€šè¿‡å‚æ•°â€˜num_questions_per_chunkâ€™ä¼ é€’ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºä¸¤ä¸ªã€‚ç„¶åï¼Œå¯ä»¥é€šè¿‡è°ƒç”¨Llama Indexåº“ä¸­çš„generate_qa_embedding_pairsæ¥ç”Ÿæˆé—®é¢˜ï¼š

```py
from llama_index.llms import OpenAI
from llama_index.legacy.finetuning import generate_qa_embedding_pairs

qa_dataset = generate_qa_embedding_pairs(
    llm=OpenAI(model="gpt-3.5-turbo-0125",additional_kwargs={'seed':42}),
    nodes=nodes,
    qa_generate_prompt_tmpl = prompts[language],
    num_questions_per_chunk=2
)
```

æˆ‘ä»¬åœ¨æ­¤ä»»åŠ¡ä¸­ä¾èµ–äºOpenAIçš„GPT-3.5-turbo-0125æ¨¡å‹ï¼Œæ ¹æ®OpenAIçš„è¯´æ³•ï¼Œè¿™æ˜¯è¯¥ç³»åˆ—çš„æ——èˆ°æ¨¡å‹ï¼Œæ”¯æŒ16Kçš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶é’ˆå¯¹å¯¹è¯è¿›è¡Œäº†ä¼˜åŒ–ï¼ˆ[https://platform.openai.com/docs/models/gpt-3-5-turbo](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fmodels%2Fgpt-3-5-turbo)ï¼‰ã€‚

ç”Ÿæˆçš„å¯¹è±¡â€˜qa_datasetâ€™åŒ…å«é—®é¢˜å’Œç­”æ¡ˆï¼ˆæ–‡æ¡£å—ï¼‰å¯¹ã€‚ä½œä¸ºç”Ÿæˆé—®é¢˜çš„ç¤ºä¾‹ï¼Œä»¥ä¸‹æ˜¯å‰ä¸¤ä¸ªé—®é¢˜çš„ç»“æœï¼ˆå…¶ä¸­â€˜ç­”æ¡ˆâ€™æ˜¯ç¬¬ä¸€ä¸ªæ–‡æ¡£å—çš„æ–‡æœ¬ï¼‰ï¼š

> 1) æ ¹æ®è¯´æ˜æ€§å¤‡å¿˜å½•ï¼Œå…³äºäººå·¥æ™ºèƒ½çš„ç»Ÿä¸€è§„åˆ™ææ¡ˆï¼ˆäººå·¥æ™ºèƒ½æ³•æ¡ˆï¼‰çš„ä¸»è¦ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
> 
> 2) æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå…³äºäººå·¥æ™ºèƒ½çš„ç»Ÿä¸€è§„åˆ™ææ¡ˆå¦‚ä½•åœ¨ä¿ƒè¿›äººå·¥æ™ºèƒ½åœ¨æ¬§ç›Ÿçš„åº”ç”¨çš„åŒæ—¶ï¼Œè§£å†³ä¸ä½¿ç”¨äººå·¥æ™ºèƒ½ç›¸å…³çš„é£é™©ï¼Ÿ

æ–‡æ¡£å—å’Œé—®é¢˜çš„æ•°é‡å–å†³äºè¯­è¨€ï¼Œä»è‹±è¯­çš„çº¦100ä¸ªæ–‡æ¡£å—å’Œ200ä¸ªé—®é¢˜ï¼Œåˆ°åŒˆç‰™åˆ©è¯­çš„200ä¸ªæ–‡æ¡£å—å’Œ400ä¸ªé—®é¢˜ä¸ç­‰ã€‚

# OpenAIåµŒå…¥æ¨¡å‹çš„è¯„ä¼°

æˆ‘ä»¬çš„è¯„ä¼°å‡½æ•°éµå¾ª[Llama Indexæ–‡æ¡£](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding.html)ï¼Œç”±ä¸¤ä¸ªä¸»è¦æ­¥éª¤ç»„æˆã€‚é¦–å…ˆï¼Œæ‰€æœ‰ç­”æ¡ˆï¼ˆæ–‡æ¡£å—ï¼‰çš„åµŒå…¥å­˜å‚¨åœ¨ä¸€ä¸ªVectorStoreIndexä¸­ï¼Œä»¥ä¾¿é«˜æ•ˆæ£€ç´¢ã€‚ç„¶åï¼Œè¯„ä¼°å‡½æ•°å¾ªç¯éå†æ‰€æœ‰æŸ¥è¯¢ï¼Œæ£€ç´¢æœ€ç›¸ä¼¼çš„å‰kä¸ªæ–‡æ¡£ï¼Œå¹¶é€šè¿‡MRRï¼ˆ[å¹³å‡å€’æ•°æ’å](https://en.wikipedia.org/wiki/Mean_reciprocal_rank)ï¼‰æ¥è¯„ä¼°æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚

```py
def evaluate(dataset, embed_model, insert_batch_size=1000, top_k=5):
    # Get corpus, queries, and relevant documents from the qa_dataset object
    corpus = dataset.corpus
    queries = dataset.queries
    relevant_docs = dataset.relevant_docs

    # Create TextNode objects for each document in the corpus and create a VectorStoreIndex to efficiently store and retrieve embeddings
    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]
    index = VectorStoreIndex(
        nodes, embed_model=embed_model, insert_batch_size=insert_batch_size
    )
    retriever = index.as_retriever(similarity_top_k=top_k)

    # Prepare to collect evaluation results
    eval_results = []

    # Iterate over each query in the dataset to evaluate retrieval performance
    for query_id, query in tqdm(queries.items()):
        # Retrieve the top_k most similar documents for the current query and extract the IDs of the retrieved documents
        retrieved_nodes = retriever.retrieve(query)
        retrieved_ids = [node.node.node_id for node in retrieved_nodes]

        # Check if the expected document was among the retrieved documents
        expected_id = relevant_docs[query_id][0]
        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc per query

        # Calculate the Mean Reciprocal Rank (MRR) and append to results
        if is_hit:
            rank = retrieved_ids.index(expected_id) + 1
            mrr = 1 / rank
        else:
            mrr = 0
        eval_results.append(mrr)

    # Return the average MRR across all queries as the final evaluation metric
    return np.average(eval_results)
```

åµŒå…¥æ¨¡å‹é€šè¿‡`embed_model`å‚æ•°ä¼ é€’ç»™è¯„ä¼°å‡½æ•°ï¼Œå¯¹äºOpenAIæ¨¡å‹æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆå§‹åŒ–äº†æ¨¡å‹åç§°å’Œæ¨¡å‹ç»´åº¦çš„OpenAIEmbeddingå¯¹è±¡ã€‚

```py
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(model=model_spec['model_name'],
                              dimensions=model_spec['dimensions'])
```

`dimensions` APIå‚æ•°å¯ä»¥ç¼©çŸ­åµŒå…¥ï¼ˆå³ç§»é™¤åºåˆ—æœ«å°¾çš„ä¸€äº›æ•°å­—ï¼‰ï¼Œè€Œä¸ä¼šå¤±å»å…¶è¡¨ç¤ºæ¦‚å¿µçš„ç‰¹æ€§ã€‚OpenAIä¾‹å¦‚åœ¨å®ƒä»¬çš„[å…¬å‘Š](https://openai.com/blog/new-embedding-models-and-api-updates)ä¸­å»ºè®®ï¼Œåœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­ï¼ŒåµŒå…¥å¯ä»¥ç¼©çŸ­åˆ°256çš„å¤§å°ï¼ŒåŒæ—¶ä»ç„¶ä¼˜äºæœªç¼©çŸ­çš„`text-embedding-ada-002`åµŒå…¥ï¼ˆå…¶å¤§å°ä¸º1536ï¼‰ã€‚

æˆ‘ä»¬åœ¨å››ä¸ªä¸åŒçš„OpenAIåµŒå…¥æ¨¡å‹ä¸Šè¿è¡Œäº†è¯„ä¼°å‡½æ•°ï¼š

+   ä¸¤ä¸ªç‰ˆæœ¬çš„`text-embedding-3-large`ï¼šä¸€ä¸ªæ˜¯æœ€ä½ç»´åº¦ï¼ˆ256ï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯æœ€é«˜ç»´åº¦ï¼ˆ3072ï¼‰ã€‚å®ƒä»¬åˆ†åˆ«ç§°ä¸ºâ€˜OAI-large-256â€™å’Œâ€˜OAI-large-3072â€™ã€‚

+   OAI-smallï¼š`text-embedding-3-small`åµŒå…¥æ¨¡å‹ï¼Œç»´åº¦ä¸º1536ã€‚

+   OAI-ada-002ï¼šä¼ ç»Ÿçš„`text-embedding-ada-002`æ¨¡å‹ï¼Œç»´åº¦ä¸º1536ã€‚

æ¯ä¸ªæ¨¡å‹éƒ½åœ¨å››ç§ä¸åŒçš„è¯­è¨€ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼šè‹±è¯­ï¼ˆENï¼‰ã€æ³•è¯­ï¼ˆFRï¼‰ã€æ·å…‹è¯­ï¼ˆCSï¼‰å’ŒåŒˆç‰™åˆ©è¯­ï¼ˆHUï¼‰ï¼Œåˆ†åˆ«æ¶µç›–äº†æ—¥è€³æ›¼è¯­ç³»ã€ç½—æ›¼è¯­ç³»ã€æ–¯æ‹‰å¤«è¯­ç³»å’Œä¹Œæ‹‰å°”è¯­ç³»çš„ç¤ºä¾‹ã€‚

```py
embeddings_model_spec = {
}

embeddings_model_spec['OAI-Large-256']={'model_name':'text-embedding-3-large','dimensions':256}
embeddings_model_spec['OAI-Large-3072']={'model_name':'text-embedding-3-large','dimensions':3072}
embeddings_model_spec['OAI-Small']={'model_name':'text-embedding-3-small','dimensions':1536}
embeddings_model_spec['OAI-ada-002']={'model_name':'text-embedding-ada-002','dimensions':None}

results = []

languages = ["EN", "FR", "CS", "HU"]

# Loop through all languages
for language in languages:

    # Load dataset
    file_name=language+"_dataset.json"
    qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)

    # Loop through all models
    for model_name, model_spec in embeddings_model_spec.items():

        # Get model
        embed_model = OpenAIEmbedding(model=model_spec['model_name'],
                                      dimensions=model_spec['dimensions'])

        # Assess embedding score (in terms of MRR)
        score = evaluate(qa_dataset, embed_model)

        results.append([language, model_name, score])

df_results = pd.DataFrame(results, columns = ["Language" ,"Embedding model", "MRR"])
```

ç»“æœä¸­çš„å‡†ç¡®åº¦ï¼ˆä»¥MRRè¡¡é‡ï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š

![](../Images/e85932db410223b301977602a81afcd7.png)

OpenAIæ¨¡å‹è¡¨ç°çš„æ€»ç»“

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¯¹äºå¤§å‹æ¨¡å‹ï¼Œéšç€åµŒå…¥å¤§å°å¢å¤§åˆ°3072ï¼Œè¡¨ç°æœ‰æ‰€æå‡ã€‚ä¸å°å‹å’Œä¼ ç»ŸAdaæ¨¡å‹ç›¸æ¯”ï¼Œå¤§å‹æ¨¡å‹çš„è¡¨ç°è™½æœ‰æ‰€æå‡ï¼Œä½†ä¾ç„¶æ¯”æˆ‘ä»¬é¢„æœŸçš„è¦å°ã€‚ä¸ºè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜åœ¨ä¸‹æ–¹æŠ¥å‘Šäº†OpenAIæ¨¡å‹åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚

![](../Images/d7f95f6dd6fc1fa806136b1f0a8236bd.png)

OpenAIåµŒå…¥æ¨¡å‹çš„è¡¨ç°ï¼Œè¯¦è§å®ƒä»¬çš„[å®˜æ–¹å…¬å‘Š](https://openai.com/blog/new-embedding-models-and-api-updates)ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æˆ‘ä»¬çš„è¯„ä¼°ä¸­ï¼Œå¤§å‹ã€å°å‹å’ŒAdaæ¨¡å‹ä¹‹é—´çš„è¡¨ç°å·®å¼‚ï¼Œæ¯”MTEBåŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„å·®å¼‚è¦å°ï¼Œè¿™åæ˜ äº†ä¸€ä¸ªäº‹å®ï¼Œå³åœ¨å¤§å‹åŸºå‡†æµ‹è¯•ä¸­è§‚å¯Ÿåˆ°çš„å¹³å‡è¡¨ç°ï¼Œå¹¶ä¸ä¸€å®šèƒ½åæ˜ åœ¨å®šåˆ¶æ•°æ®é›†ä¸Šè·å¾—çš„ç»“æœã€‚

# å¼€æºåµŒå…¥æ¨¡å‹çš„è¯„ä¼°

å…³äºåµŒå…¥çš„å¼€æºç ”ç©¶éå¸¸æ´»è·ƒï¼Œæ–°çš„æ¨¡å‹å®šæœŸå‘å¸ƒã€‚ä¸€ä¸ªä¸é”™çš„ä¿æŒæœ€æ–°å‘å¸ƒæ¨¡å‹çš„åœ°æ–¹æ˜¯[Hugging Face ğŸ˜Š MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)ã€‚

æœ¬æ–‡ä¸­çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ç»„æœ€è¿‘å‘å¸ƒçš„å››ç§åµŒå…¥æ¨¡å‹ï¼ˆ2024å¹´ï¼‰ã€‚é€‰æ‹©æ ‡å‡†æ˜¯å®ƒä»¬åœ¨MTEBæ’è¡Œæ¦œä¸Šçš„å¹³å‡å¾—åˆ†ä»¥åŠå¤„ç†å¤šè¯­è¨€æ•°æ®çš„èƒ½åŠ›ã€‚ä»¥ä¸‹æ˜¯æ‰€é€‰æ¨¡å‹çš„ä¸»è¦ç‰¹å¾æ€»ç»“ã€‚

æ‰€é€‰çš„å¼€æºåµŒå…¥æ¨¡å‹

+   [***E5-Mistral-7B-instruct***](https://huggingface.co/intfloat/e5-mistral-7b-instruct)(E5-mistral-7b)ï¼šå¾®è½¯çš„è¿™æ¬¾E5åµŒå…¥æ¨¡å‹æ˜¯ä»[Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)åˆå§‹åŒ–çš„ï¼Œå¹¶åœ¨å¤šè¯­è¨€æ•°æ®é›†çš„æ··åˆä¸Šè¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹åœ¨MTEBæ’è¡Œæ¦œä¸Šè¡¨ç°æœ€ä½³ï¼Œä½†ä¹Ÿæ˜¯æœ€å¤§çš„æ¨¡å‹ï¼ˆ14GBï¼‰ã€‚

+   [***multilingual-e5-large-instruct***](https://huggingface.co/intfloat/multilingual-e5-large-instruct)(ML-E5-large)ï¼šå¾®è½¯çš„å¦ä¸€æ¬¾E5æ¨¡å‹ï¼Œæ—¨åœ¨æ›´å¥½åœ°å¤„ç†å¤šè¯­è¨€æ•°æ®ã€‚å®ƒæ˜¯ä»[xlm-roberta-large](https://huggingface.co/xlm-roberta-large)åˆå§‹åŒ–çš„ï¼Œå¹¶åœ¨å¤šè¯­è¨€æ•°æ®é›†çš„æ··åˆä¸Šè¿›è¡Œè®­ç»ƒã€‚å®ƒæ¯”E5-Mistralå°å¾—å¤šï¼ˆå°10å€ï¼‰ï¼Œä½†ä¸Šä¸‹æ–‡å¤§å°ä¹Ÿæ›´å°ï¼ˆ514ï¼‰ã€‚

+   [***BGE-M3***](https://huggingface.co/BAAI/bge-m3)ï¼šè¯¥æ¨¡å‹ç”±åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢è®¾è®¡ï¼Œæ˜¯å…¶é’ˆå¯¹å¤šè¯­è¨€æ•°æ®çš„æœ€å…ˆè¿›åµŒå…¥æ¨¡å‹ï¼Œæ”¯æŒ100å¤šç§å·¥ä½œè¯­è¨€ã€‚æˆªè‡³2024å¹´2æœˆ22æ—¥ï¼Œå®ƒå°šæœªåœ¨MTEBæ’è¡Œæ¦œä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚

+   [***nomic-embed-text-v1***](https://huggingface.co/nomic-ai/nomic-embed-text-v1)(Nomic-Embed)ï¼šè¯¥æ¨¡å‹ç”±[Nomic](https://home.nomic.ai/)è®¾è®¡ï¼Œå£°ç§°åœ¨æ€§èƒ½ä¸Šä¼˜äºOpenAI Ada-002å’Œtext-embedding-3-smallï¼ŒåŒæ—¶ä»…ä¸º0.55GBå¤§å°ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¯¥æ¨¡å‹æ˜¯ç¬¬ä¸€ä¸ªå®Œå…¨å¯å¤ç°å’Œå¯å®¡è®¡çš„æ¨¡å‹ï¼ˆå¼€æ”¾æ•°æ®å’Œå¼€æºè®­ç»ƒä»£ç ï¼‰ã€‚

è¯„ä¼°è¿™äº›å¼€æºæ¨¡å‹çš„ä»£ç ä¸ç”¨äºOpenAIæ¨¡å‹çš„ä»£ç ç±»ä¼¼ã€‚ä¸»è¦çš„å˜åŒ–åœ¨äºæ¨¡å‹è§„æ ¼ï¼Œå¿…é¡»æŒ‡å®šé™„åŠ çš„ç»†èŠ‚ï¼Œå¦‚æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦å’Œæ± åŒ–ç±»å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹å››ç§è¯­è¨€ä¸­çš„æ¯ç§æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼š

```py
embeddings_model_spec = {
}

embeddings_model_spec['E5-mistral-7b']={'model_name':'intfloat/e5-mistral-7b-instruct','max_length':32768, 'pooling_type':'last_token', 
                                        'normalize': True, 'batch_size':1, 'kwargs': {'load_in_4bit':True, 'bnb_4bit_compute_dtype':torch.float16}}
embeddings_model_spec['ML-E5-large']={'model_name':'intfloat/multilingual-e5-large','max_length':512, 'pooling_type':'mean', 
                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}
embeddings_model_spec['BGE-M3']={'model_name':'BAAI/bge-m3','max_length':8192, 'pooling_type':'cls', 
                                 'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}
embeddings_model_spec['Nomic-Embed']={'model_name':'nomic-ai/nomic-embed-text-v1','max_length':8192, 'pooling_type':'mean', 
                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'trust_remote_code' : True}}

results = []

languages = ["EN", "FR", "CS", "HU"]

# Loop through all models
for model_name, model_spec in embeddings_model_spec.items():

    print("Processing model : "+str(model_spec))

    # Get model
    tokenizer = AutoTokenizer.from_pretrained(model_spec['model_name'])
    embed_model = AutoModel.from_pretrained(model_spec['model_name'], **model_spec['kwargs'])

    if model_name=="Nomic-Embed":
        embed_model.to('cuda')

    # Loop through all languages
    for language in languages:

        # Load dataset
        file_name=language+"_dataset.json"
        qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)

        start_time_assessment=time.time()

        # Assess embedding score (in terms of hit rate at k=5)
        score = evaluate(qa_dataset, tokenizer, embed_model, model_spec['normalize'], model_spec['max_length'], model_spec['pooling_type'])

        # Get duration of score assessment
        duration_assessment = time.time()-start_time_assessment

        results.append([language, model_name, score, duration_assessment])

df_results = pd.DataFrame(results, columns = ["Language" ,"Embedding model", "MRR", "Duration"])
```

ä¸‹é¢æŠ¥å‘Šäº†ä»¥MRRä¸ºæ ‡å‡†çš„å‡†ç¡®åº¦ç»“æœã€‚

![](../Images/f77ab24c4d15f7334aa730420a285de6.png)

å¼€æºæ¨¡å‹çš„æ€§èƒ½æ€»ç»“

BGE-M3è¡¨ç°æœ€ä½³ï¼Œå¹³å‡è¡¨ç°ç´§éšå…¶åçš„æ˜¯ML-E5-Largeã€E5-mistral-7bå’ŒNomic-Embedã€‚BGE-M3æ¨¡å‹å°šæœªåœ¨MTEBæ’è¡Œæ¦œä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜å®ƒå¯èƒ½ä¼šæ’åœ¨å…¶ä»–æ¨¡å‹ä¹‹å‰ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè™½ç„¶BGE-M3æ˜¯é’ˆå¯¹å¤šè¯­è¨€æ•°æ®è¿›è¡Œä¼˜åŒ–çš„ï¼Œä½†å®ƒåœ¨è‹±è¯­ä¸Šçš„è¡¨ç°ä¼˜äºå…¶ä»–æ¨¡å‹ã€‚

æˆ‘ä»¬è¿˜æŠ¥å‘Šäº†æ¯ä¸ªåµŒå…¥æ¨¡å‹çš„å¤„ç†æ—¶é—´ã€‚

![](../Images/026e18ce209b4a9150cc60b529e46e89.png)

é€šè¿‡è‹±æ–‡é—®ç­”æ•°æ®é›†çš„å¤„ç†æ—¶é—´ï¼ˆä»¥ç§’ä¸ºå•ä½ï¼‰

E5-mistral-7bæ¯”å…¶ä»–æ¨¡å‹å¤§10å€ä»¥ä¸Šï¼Œæ¯«ä¸æ„å¤–åœ°æ˜¯æœ€æ…¢çš„æ¨¡å‹ã€‚

# ç»“è®º

è®©æˆ‘ä»¬å°†è¿™å…«ä¸ªæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½å¹¶æ’å±•ç¤ºåœ¨ä¸€ä¸ªå›¾ä¸­ã€‚

![](../Images/2098db260ef495cac46c32a79518bcc3.png)

å…«ä¸ªæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½æ€»ç»“

ä»è¿™äº›ç»“æœä¸­å¯ä»¥å¾—å‡ºä»¥ä¸‹å…³é”®è§‚å¯Ÿï¼š

+   **å¼€æºæ¨¡å‹çš„è¡¨ç°æœ€å¥½**ã€‚ç”±åŒ—äº¬äººå·¥æ™ºèƒ½å­¦ä¼šå¼€å‘çš„ [BGE-M3](https://huggingface.co/BAAI/bge-m3) æ¨¡å‹è¡¨ç°æœ€ä½³ã€‚è¯¥æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ä¸ OpenAI æ¨¡å‹ç›¸åŒï¼ˆ8Kï¼‰ï¼Œå¤§å°ä¸º 2.2GBã€‚

+   **OpenAI æ¨¡å‹èŒƒå›´çš„ä¸€è‡´æ€§**ã€‚å¤§ï¼ˆ3072ï¼‰ã€å°å‹å’Œé—ç•™ç‰ˆ OpenAI æ¨¡å‹çš„è¡¨ç°éå¸¸ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œå‡å°‘å¤§æ¨¡å‹ï¼ˆ256ï¼‰çš„åµŒå…¥å¤§å°ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

+   **è¯­è¨€æ•æ„Ÿæ€§**ã€‚å‡ ä¹æ‰€æœ‰æ¨¡å‹ï¼ˆé™¤äº† ML-E5-largeï¼‰åœ¨è‹±è¯­ä¸Šçš„è¡¨ç°æœ€å¥½ã€‚åœ¨æ·å…‹è¯­å’ŒåŒˆç‰™åˆ©è¯­ç­‰è¯­è¨€ä¸­ï¼Œæ€§èƒ½å·®å¼‚è¾ƒå¤§ã€‚

é‚£ä¹ˆï¼Œä½ æ˜¯åº”è¯¥é€‰æ‹©ä»˜è´¹çš„ OpenAI è®¢é˜…ï¼Œè¿˜æ˜¯æ‰˜ç®¡ä¸€ä¸ªå¼€æºåµŒå…¥æ¨¡å‹ï¼Ÿ

OpenAI çš„ [æœ€è¿‘ä»·æ ¼è°ƒæ•´](https://openai.com/pricing)ä½¿å¾—è®¿é—®å…¶ API æ›´åŠ å®æƒ ï¼Œç°åœ¨æ¯ç™¾ä¸‡ä¸ª token çš„è´¹ç”¨ä¸º $0.13ã€‚å¤„ç†æ¯æœˆç™¾ä¸‡æ¬¡æŸ¥è¯¢ï¼ˆå‡è®¾æ¯æ¬¡æŸ¥è¯¢å¤§çº¦æ¶‰åŠ 1K tokenï¼‰å¤§çº¦éœ€è¦ $130ã€‚æ ¹æ®ä½ çš„ä½¿ç”¨åœºæ™¯ï¼Œç§Ÿç”¨å’Œç»´æŠ¤è‡ªå·±çš„åµŒå…¥æœåŠ¡å™¨å¯èƒ½ä¸å…·æœ‰æˆæœ¬æ•ˆç›Šã€‚

ç„¶è€Œï¼Œæˆæœ¬æ•ˆç›Šå¹¶éå”¯ä¸€çš„è€ƒè™‘å› ç´ ã€‚å…¶ä»–å› ç´ å¦‚å»¶è¿Ÿã€éšç§å’Œæ•°æ®å¤„ç†å·¥ä½œæµçš„æ§åˆ¶ä¹Ÿå¯èƒ½éœ€è¦è¢«è€ƒè™‘ã€‚å¼€æºæ¨¡å‹æä¾›äº†å®Œå…¨æ§åˆ¶æ•°æ®çš„ä¼˜åŠ¿ï¼Œä»è€Œå¢å¼ºäº†éšç§æ€§å’Œå®šåˆ¶æ€§ã€‚å¦ä¸€æ–¹é¢ï¼ŒOpenAI çš„ API å­˜åœ¨å»¶è¿Ÿé—®é¢˜ï¼Œæœ‰æ—¶ä¼šå¯¼è‡´å“åº”æ—¶é—´å»¶é•¿ã€‚

æ€»ç»“æ¥è¯´ï¼Œé€‰æ‹©å¼€æºæ¨¡å‹è¿˜æ˜¯åƒ OpenAI è¿™æ ·çš„ä¸“æœ‰è§£å†³æ–¹æ¡ˆå¹¶ä¸æ˜¯ä¸€ä¸ªç®€å•çš„ç­”æ¡ˆã€‚å¼€æºåµŒå…¥æ¨¡å‹æä¾›äº†ä»¤äººä¿¡æœçš„é€‰æ‹©ï¼Œç»“åˆäº†æ€§èƒ½å’Œå¯¹æ•°æ®çš„æ›´å¤§æ§åˆ¶ã€‚ç›¸åï¼ŒOpenAI çš„äº§å“å¯èƒ½ä»ç„¶å¸å¼•é‚£äº›ä¼˜å…ˆè€ƒè™‘ä¾¿åˆ©æ€§çš„ç”¨æˆ·ï¼Œå°¤å…¶æ˜¯å½“éšç§é—®é¢˜ä¸æ˜¯ä¸»è¦å…³æ³¨ç‚¹æ—¶ã€‚

# æœ‰ç”¨çš„é“¾æ¥

+   é…å¥—çš„ Github ä»“åº“ï¼š[https://github.com/Yannael/multilingual-embeddings](https://github.com/Yannael/multilingual-embeddings)

+   [ä½ æƒ³çŸ¥é“çš„å…³äºå¥å­åµŒå…¥çš„ä¸€åˆ‡ï¼ˆä¹Ÿè®¸è¿˜æœ‰æ›´å¤šï¼‰](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/)

+   [OpenAI åšå®¢å…¬å‘Šï¼šæ–°çš„åµŒå…¥æ¨¡å‹å’Œ API æ›´æ–°](https://openai.com/blog/new-embedding-models-and-api-updates)

+   [åµŒå…¥ï¼šOpenAI æŒ‡å—](https://platform.openai.com/docs/guides/embeddings/embedding-models)

+   [MTEB: å¤§è§„æ¨¡æ–‡æœ¬åµŒå…¥åŸºå‡†](https://arxiv.org/abs/2210.07316) å’Œ [Hugging Face MTEB æ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)

+   [æ–‡æœ¬åµŒå…¥ï¼šå…¨é¢æŒ‡å—](/text-embeddings-comprehensive-guide-afd97fce8fb5)

+   [å®è·µè€…æŒ‡å—ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰](https://cameronrwolfe.substack.com/p/a-practitioners-guide-to-retrieval)

+   [å¦‚ä½•ä¸ºä½ çš„ RAG æ‰¾åˆ°æœ€ä½³çš„å¤šè¯­è¨€åµŒå…¥æ¨¡å‹](/how-to-find-the-best-multilingual-embedding-model-for-your-rag-40325c308ebb)

æ³¨æ„ï¼š

+   é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›

+   ã€Šæ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆè‰æ¡ˆã€‹æ ¹æ®[å§”å‘˜ä¼šçš„æ–‡ä»¶é‡ç”¨æ”¿ç­–](https://eur-lex.europa.eu/content/legal-notice/legal-notice.html)å‘å¸ƒï¼ŒåŸºäº[å†³å®š 2011/833/EU](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32011D0833)ï¼Œå¯ç”¨äºå•†ä¸šæˆ–éå•†ä¸šç›®çš„ã€‚

*å–œæ¬¢è¿™ç¯‡æ–‡ç« å—ï¼Ÿåˆ†äº«ä½ çš„æƒ³æ³•ï¼Œç»™å®ƒç‚¹ä¸ªèµï¼Œæˆ–* [*åœ¨ LinkedIn ä¸Šä¸æˆ‘è”ç³»*](https://www.linkedin.com/in/yannaelb/)*ã€‚*
