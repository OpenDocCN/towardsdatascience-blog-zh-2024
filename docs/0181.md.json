["```py\ngit clone https://github.com/HamzaG737/data-engineering-project.git\n```", "```py\n├── LICENSE\n├── README.md\n├── airflow\n│   ├── Dockerfile\n│   ├── __init__.py\n│   └── dags\n│       ├── __init__.py\n│       └── dag_kafka_spark.py\n├── data\n│   └── last_processed.json\n├── docker-compose-airflow.yaml\n├── docker-compose.yml\n├── kafka\n├── requirements.txt\n├── spark\n│   └── Dockerfile\n└── src\n    ├── __init__.py\n    ├── constants.py\n    ├── kafka_client\n    │   ├── __init__.py\n    │   └── kafka_stream_data.py\n    └── spark_pgsql\n        └── spark_streaming.py\n```", "```py\npip install -r requirements.txt\n```", "```py\n{last_processed:\"2023-11-22\"}\n```", "```py\nversion: '3'\n\nservices:\n  kafka:\n    image: 'bitnami/kafka:latest'\n    ports:\n      - '9094:9094'\n    networks:\n      - airflow-kafka\n    environment:\n      - KAFKA_CFG_NODE_ID=0\n      - KAFKA_CFG_PROCESS_ROLES=controller,broker\n      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094\n      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,EXTERNAL://localhost:9094\n      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,EXTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT\n      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093\n      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER\n    volumes:\n      - ./kafka:/bitnami/kafka\n\n  kafka-ui:\n    container_name: kafka-ui-1\n    image: provectuslabs/kafka-ui:latest\n    ports:\n      - 8800:8080  \n    depends_on:\n      - kafka\n    environment:\n      KAFKA_CLUSTERS_0_NAME: local\n      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: PLAINTEXT://kafka:9092\n      DYNAMIC_CONFIG_ENABLED: 'true'\n    networks:\n      - airflow-kafka\n\nnetworks:\n  airflow-kafka:\n    external: true\n```", "```py\ndocker network create airflow-kafka\n```", "```py\ndocker-compose up \n```", "```py\npython scripts/create_table.py\n```", "```py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n)\nfrom pyspark.sql.functions import from_json, col\nfrom src.constants import POSTGRES_URL, POSTGRES_PROPERTIES, DB_FIELDS\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO, format=\"%(asctime)s:%(funcName)s:%(levelname)s:%(message)s\"\n)\n\ndef create_spark_session() -> SparkSession:\n    spark = (\n        SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n        .config(\n            \"spark.jars.packages\",\n            \"org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n\n        )\n        .getOrCreate()\n    )\n\n    logging.info(\"Spark session created successfully\")\n    return spark\n\ndef create_initial_dataframe(spark_session):\n    \"\"\"\n    Reads the streaming data and creates the initial dataframe accordingly.\n    \"\"\"\n    try:\n        # Gets the streaming data from topic random_names\n        df = (\n            spark_session.readStream.format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n            .option(\"subscribe\", \"rappel_conso\")\n            .option(\"startingOffsets\", \"earliest\")\n            .load()\n        )\n        logging.info(\"Initial dataframe created successfully\")\n    except Exception as e:\n        logging.warning(f\"Initial dataframe couldn't be created due to exception: {e}\")\n        raise\n\n    return df\n\ndef create_final_dataframe(df):\n    \"\"\"\n    Modifies the initial dataframe, and creates the final dataframe.\n    \"\"\"\n    schema = StructType(\n        [StructField(field_name, StringType(), True) for field_name in DB_FIELDS]\n    )\n    df_out = (\n        df.selectExpr(\"CAST(value AS STRING)\")\n        .select(from_json(col(\"value\"), schema).alias(\"data\"))\n        .select(\"data.*\")\n    )\n    return df_out\n\ndef start_streaming(df_parsed, spark):\n    \"\"\"\n    Starts the streaming to table spark_streaming.rappel_conso in postgres\n    \"\"\"\n    # Read existing data from PostgreSQL\n    existing_data_df = spark.read.jdbc(\n        POSTGRES_URL, \"rappel_conso\", properties=POSTGRES_PROPERTIES\n    )\n\n    unique_column = \"reference_fiche\"\n\n    logging.info(\"Start streaming ...\")\n    query = df_parsed.writeStream.foreachBatch(\n        lambda batch_df, _: (\n            batch_df.join(\n                existing_data_df, batch_df[unique_column] == existing_data_df[unique_column], \"leftanti\"\n            )\n            .write.jdbc(\n                POSTGRES_URL, \"rappel_conso\", \"append\", properties=POSTGRES_PROPERTIES\n            )\n        )\n    ).trigger(once=True) \\\n        .start()\n\n    return query.awaitTermination()\n\ndef write_to_postgres():\n    spark = create_spark_session()\n    df = create_initial_dataframe(spark)\n    df_final = create_final_dataframe(df)\n    start_streaming(df_final, spark=spark)\n\nif __name__ == \"__main__\":\n    write_to_postgres()\n```", "```py\ndef create_spark_session() -> SparkSession:\n    spark = (\n        SparkSession.builder.appName(\"PostgreSQL Connection with PySpark\")\n        .config(\n            \"spark.jars.packages\",\n            \"org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\",\n\n        )\n        .getOrCreate()\n    )\n\n    logging.info(\"Spark session created successfully\")\n    return spark\n```", "```py\ndef create_initial_dataframe(spark_session):\n    \"\"\"\n    Reads the streaming data and creates the initial dataframe accordingly.\n    \"\"\"\n    try:\n        # Gets the streaming data from topic random_names\n        df = (\n            spark_session.readStream.format(\"kafka\")\n            .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n            .option(\"subscribe\", \"rappel_conso\")\n            .option(\"startingOffsets\", \"earliest\")\n            .load()\n        )\n        logging.info(\"Initial dataframe created successfully\")\n    except Exception as e:\n        logging.warning(f\"Initial dataframe couldn't be created due to exception: {e}\")\n        raise\n\n    return df\n```", "```py\ndef create_final_dataframe(df):\n    \"\"\"\n    Modifies the initial dataframe, and creates the final dataframe.\n    \"\"\"\n    schema = StructType(\n        [StructField(field_name, StringType(), True) for field_name in DB_FIELDS]\n    )\n    df_out = (\n        df.selectExpr(\"CAST(value AS STRING)\")\n        .select(from_json(col(\"value\"), schema).alias(\"data\"))\n        .select(\"data.*\")\n    )\n    return df_out\n```", "```py\ndef start_streaming(df_parsed, spark):\n    \"\"\"\n    Starts the streaming to table spark_streaming.rappel_conso in postgres\n    \"\"\"\n    # Read existing data from PostgreSQL\n    existing_data_df = spark.read.jdbc(\n        POSTGRES_URL, \"rappel_conso\", properties=POSTGRES_PROPERTIES\n    )\n\n    unique_column = \"reference_fiche\"\n\n    logging.info(\"Start streaming ...\")\n    query = df_parsed.writeStream.foreachBatch(\n        lambda batch_df, _: (\n            batch_df.join(\n                existing_data_df, batch_df[unique_column] == existing_data_df[unique_column], \"leftanti\"\n            )\n            .write.jdbc(\n                POSTGRES_URL, \"rappel_conso\", \"append\", properties=POSTGRES_PROPERTIES\n            )\n        )\n    ).trigger(once=True) \\\n        .start()\n\n    return query.awaitTermination()\n```", "```py\nFROM bitnami/spark:latest\n\nWORKDIR /opt/bitnami/spark\n\nRUN pip install py4j\n\nCOPY ./src/spark_pgsql/spark_streaming.py ./spark_streaming.py\nCOPY ./src/constants.py ./src/constants.py\n\nENV POSTGRES_DOCKER_USER=host.docker.internal\nARG POSTGRES_PASSWORD\nENV POSTGRES_PASSWORD=$POSTGRES_PASSWORD\n```", "```py\ndocker build -f spark/Dockerfile -t rappel-conso/spark:latest --build-arg POSTGRES_PASSWORD=$POSTGRES_PASSWORD  .\n```", "```py\nstart_date = datetime.today() - timedelta(days=1)\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"start_date\": start_date,\n    \"retries\": 1,  # number of retries before failing the task\n    \"retry_delay\": timedelta(seconds=5),\n}\n\nwith DAG(\n    dag_id=\"kafka_spark_dag\",\n    default_args=default_args,\n    schedule_interval=timedelta(days=1),\n    catchup=False,\n) as dag:\n\n    kafka_stream_task = PythonOperator(\n        task_id=\"kafka_data_stream\",\n        python_callable=stream,\n        dag=dag,\n    )\n\n    spark_stream_task = DockerOperator(\n        task_id=\"pyspark_consumer\",\n        image=\"rappel-conso/spark:latest\",\n        api_version=\"auto\",\n        auto_remove=True,\n        command=\"./bin/spark-submit --master local[*] --packages org.postgresql:postgresql:42.5.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 ./spark_streaming.py\",\n        docker_url='tcp://docker-proxy:2375',\n        environment={'SPARK_LOCAL_HOSTNAME': 'localhost'},\n        network_mode=\"airflow-kafka\",\n        dag=dag,\n    )\n\n    kafka_stream_task >> spark_stream_task\n```", "```py\n docker-proxy:\n    image: bobrik/socat\n    command: \"TCP4-LISTEN:2375,fork,reuseaddr UNIX-CONNECT:/var/run/docker.sock\"\n    ports:\n      - \"2376:2375\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    networks:\n      - airflow-kafka\n```", "```py\nversion: '3.8'\nx-airflow-common:\n  &airflow-common\n  build:\n    context: .\n    dockerfile: ./airflow_resources/Dockerfile\n  image: de-project/airflow:latest\n```", "```py\nvolumes:\n  - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n  - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n  - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n  - ./src:/opt/airflow/dags/src\n  - ./data/last_processed.json:/opt/airflow/data/last_processed.json\nuser: \"${AIRFLOW_UID:-50000}:0\"\nnetworks:\n  - airflow-kafka\n```", "```py\necho -e \"AIRFLOW_UID=$(id -u)\\nAIRFLOW_PROJ_DIR=\\\"./airflow_resources\\\"\" > .env\n```", "```py\n docker compose -f docker-compose-airflow.yaml up\n```", "```py\nSELECT count(*) FROM rappel_conso_table\n```"]