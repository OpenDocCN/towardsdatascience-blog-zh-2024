["```py\nimport numpy as np\nimport torch\nimport cv2\nimport os\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n```", "```py\ndata_dir=r\"LabPicsV1//\" # Path to LabPics1 dataset folder\ndata=[] # list of files in dataset\nfor ff, name in enumerate(os.listdir(data_dir+\"Simple/Train/Image/\")):  # go over all folder annotation\n    data.append({\"image\":data_dir+\"Simple/Train/Image/\"+name,\"annotation\":data_dir+\"Simple/Train/Instance/\"+name[:-4]+\".png\"})\n```", "```py\ndef read_batch(data): # read random image and its annotaion from  the dataset (LabPics)\n\n   #  select image\n\n        ent  = data[np.random.randint(len(data))] # choose random entry\n        Img = cv2.imread(ent[\"image\"])[...,::-1]  # read image\n        ann_map = cv2.imread(ent[\"annotation\"]) # read annotation\n\n   # resize image\n\n        r = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\n        Img = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\n        ann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n\n   # merge vessels and materials annotations\n\n        mat_map = ann_map[:,:,0] # material annotation map\n        ves_map = ann_map[:,:,2] # vessel  annotaion map\n        mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merged map\n\n   # Get binary masks and points\n\n        inds = np.unique(mat_map)[1:] # load all indices\n        points= []\n        masks = [] \n        for ind in inds:\n            mask=(mat_map == ind).astype(np.uint8) # make binary mask\n            masks.append(mask)\n            coords = np.argwhere(mask > 0) # get all coordinates in mask\n            yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n            points.append([[yx[1], yx[0]]])\n        return Img,np.array(masks),np.array(points), np.ones([len(masks),1])\n```", "```py\nent  = data[np.random.randint(len(data))] # choose random entry\nImg = cv2.imread(ent[\"image\"])[...,::-1]  # read image\nann_map = cv2.imread(ent[\"annotation\"]) # read annotation\n```", "```py\nr = np.min([1024 / Img.shape[1], 1024 / Img.shape[0]]) # scalling factor\nImg = cv2.resize(Img, (int(Img.shape[1] * r), int(Img.shape[0] * r)))\nann_map = cv2.resize(ann_map, (int(ann_map.shape[1] * r), int(ann_map.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n```", "```py\n mat_map = ann_map[:,:,0] # material annotation map\n  ves_map = ann_map[:,:,2] # vessel  annotaion map\n  mat_map[mat_map==0] = ves_map[mat_map==0]*(mat_map.max()+1) # merged map\n```", "```py\ninds = np.unique(mat_map)[1:] # list of all indices in map\npoints= [] # list of all points (one for each mask)\nmasks = [] # list of all masks\nfor ind in inds:\n            mask = (mat_map == ind).astype(np.uint8) # make binary mask for index ind\n            masks.append(mask)\n            coords = np.argwhere(mask > 0) # get all coordinates in mask\n            yx = np.array(coords[np.random.randint(len(coords))]) # choose random point/coordinate\n            points.append([[yx[1], yx[0]]])\nreturn Img,np.array(masks),np.array(points), np.ones([len(masks),1])\n```", "```py\nsam2_checkpoint = \"sam2_hiera_small.pt\" # path to model weight\nmodel_cfg = \"sam2_hiera_s.yaml\" # model config\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\") # load model\npredictor = SAM2ImagePredictor(sam2_model) # load net\n```", "```py\npredictor.model.sam_mask_decoder.train(True) # enable training of mask decoder \npredictor.model.sam_prompt_encoder.train(True) # enable training of prompt encoder\n```", "```py\noptimizer=torch.optim.AdamW(params=predictor.model.parameters(),lr=1e-5,weight_decay=4e-5)\n```", "```py\nscaler = torch.cuda.amp.GradScaler() # set mixed precision\n```", "```py\nfor itr in range(100000):\n    with torch.cuda.amp.autocast(): # cast to mix precision\n            image,mask,input_point, input_label = read_batch(data) # load data batch\n            if mask.shape[0]==0: continue # ignore empty batches\n            predictor.set_image(image) # apply SAM image encoder to the image\n```", "```py\nwith torch.cuda.amp.autocast():\n```", "```py\nimage,mask,input_point, input_label = read_batch(data)\n```", "```py\npredictor.set_image(image)\n```", "```py\n mask_input, unnorm_coords, labels, unnorm_box = predictor._prep_prompts(input_point, input_label, box=None, mask_logits=None, normalize_coords=True)\n  sparse_embeddings, dense_embeddings = predictor.model.sam_prompt_encoder(points=(unnorm_coords, labels),boxes=None,masks=None,)\n```", "```py\nbatched_mode = unnorm_coords.shape[0] > 1 # multi mask prediction\nhigh_res_features = [feat_level[-1].unsqueeze(0) for feat_level in predictor._features[\"high_res_feats\"]]\nlow_res_masks, prd_scores, _, _ = predictor.model.sam_mask_decoder(image_embeddings=predictor._features[\"image_embed\"][-1].unsqueeze(0),image_pe=predictor.model.sam_prompt_encoder.get_dense_pe(),sparse_prompt_embeddings=sparse_embeddings,dense_prompt_embeddings=dense_embeddings,multimask_output=True,repeat_image=batched_mode,high_res_features=high_res_features,)\nprd_masks = predictor._transforms.postprocess_masks(low_res_masks, predictor._orig_hw[-1])# Upscale the masks to the original image resolution\n```", "```py\nprd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map\n```", "```py\nprd_mask = torch.sigmoid(prd_masks[:, 0])# Turn logit map to probability map\n```", "```py\nseg_loss = (-gt_mask * torch.log(prd_mask + 0.00001) - (1 - gt_mask) * torch.log((1 - prd_mask) + 0.00001)).mean() # cross entropy loss \n```", "```py\ninter = (gt_mask * (prd_mask > 0.5)).sum(1).sum(1)\n```", "```py\niou = inter / (gt_mask.sum(1).sum(1) + (prd_mask > 0.5).sum(1).sum(1) - inter)\n```", "```py\nscore_loss = torch.abs(prd_scores[:, 0] - iou).mean()\n```", "```py\nloss = seg_loss+score_loss*0.05  # mix losses\n```", "```py\npredictor.model.zero_grad() # empty gradient\nscaler.scale(loss).backward()  # Backpropogate\nscaler.step(optimizer)\nscaler.update() # Mix precision\n```", "```py\nif itr%1000==0: torch.save(predictor.model.state_dict(), \"model.torch\") # save model \n```", "```py\n if itr==0: mean_iou=0\nmean_iou = mean_iou * 0.99 + 0.01 * np.mean(iou.cpu().detach().numpy())\nprint(\"step)\",itr, \"Accuracy(IOU)=\",mean_iou)\n```", "```py\nimport numpy as np\nimport torch\nimport cv2\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\n# use bfloat16 for the entire script (memory efficient)\ntorch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n```", "```py\nimage_path = r\"sample_image.jpg\" # path to image\nmask_path = r\"sample_mask.png\" # path to mask, the mask will define the image region to segment\ndef read_image(image_path, mask_path): # read and resize image and mask\n        img = cv2.imread(image_path)[...,::-1]  # read image as rgb\n        mask = cv2.imread(mask_path,0) # mask of the region we want to segment\n\n        # Resize image to maximum size of 1024\n\n        r = np.min([1024 / img.shape[1], 1024 / img.shape[0]])\n        img = cv2.resize(img, (int(img.shape[1] * r), int(img.shape[0] * r)))\n        mask = cv2.resize(mask, (int(mask.shape[1] * r), int(mask.shape[0] * r)),interpolation=cv2.INTER_NEAREST)\n        return img, mask\nimage,mask = read_image(image_path, mask_path)\n```", "```py\nnum_samples = 30 # number of points/segment to sample\ndef get_points(mask,num_points): # Sample points inside the input mask\n        points=[]\n        for i in range(num_points):\n            coords = np.argwhere(mask > 0)\n            yx = np.array(coords[np.random.randint(len(coords))])\n            points.append([[yx[1], yx[0]]])\n        return np.array(points)\ninput_points = get_points(mask,num_samples)\n```", "```py\n# Load model you need to have pretrained model already made\nsam2_checkpoint = \"sam2_hiera_small.pt\" \nmodel_cfg = \"sam2_hiera_s.yaml\" \nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\npredictor = SAM2ImagePredictor(sam2_model)\n```", "```py\npredictor.model.load_state_dict(torch.load(\"model.torch\"))\n```", "```py\nwith torch.no_grad(): # prevent the net from caclulate gradient (more efficient inference)\n        predictor.set_image(image) # image encoder\n        masks, scores, logits = predictor.predict(  # prompt encoder + mask decoder\n            point_coords=input_points,\n            point_labels=np.ones([input_points.shape[0],1])\n        )\n```", "```py\nmasks=masks[:,0].astype(bool)\nshorted_masks = masks[np.argsort(scores[:,0])][::-1].astype(bool)\n```", "```py\nseg_map = np.zeros_like(shorted_masks[0],dtype=np.uint8)\noccupancy_mask = np.zeros_like(shorted_masks[0],dtype=bool)\n```", "```py\nfor i in range(shorted_masks.shape[0]):\n    mask = shorted_masks[i]\n    if (mask*occupancy_mask).sum()/mask.sum()>0.15: continue \n    mask[occupancy_mask]=0\n    seg_map[mask]=i+1\n    occupancy_mask[mask]=1\n```", "```py\nrgb_image = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint8)\nfor id_class in range(1,seg_map.max()+1):\n    rgb_image[seg_map == id_class] = [np.random.randint(255), np.random.randint(255), np.random.randint(255)]\n```", "```py\ncv2.imshow(\"annotation\",rgb_image)\ncv2.imshow(\"mix\",(rgb_image/2+image/2).astype(np.uint8))\ncv2.imshow(\"image\",image)\ncv2.waitKey()\n```"]