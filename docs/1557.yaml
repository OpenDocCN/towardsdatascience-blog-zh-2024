- en: 'Beyond Kleinberg’s Impossibility Theorem of Clustering: My Study Note of a
    Pragmatic Clustering Evaluation Framework'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-kleinbergs-impossibility-theorem-of-clustering-a-pragmatic-clustering-evaluation-framework-3b25eccf37f2?source=collection_archive---------10-----------------------#2024-06-21](https://towardsdatascience.com/beyond-kleinbergs-impossibility-theorem-of-clustering-a-pragmatic-clustering-evaluation-framework-3b25eccf37f2?source=collection_archive---------10-----------------------#2024-06-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article explores a pragmatic evaluation framework for clustering under
    the constraint of Kleinberg’s Impossibility Theorem of Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://deeporigami.medium.com/?source=post_page---byline--3b25eccf37f2--------------------------------)[![Michio
    Suginoo](../Images/15e4a70d17d163889cc902bf4409931a.png)](https://deeporigami.medium.com/?source=post_page---byline--3b25eccf37f2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3b25eccf37f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3b25eccf37f2--------------------------------)
    [Michio Suginoo](https://deeporigami.medium.com/?source=post_page---byline--3b25eccf37f2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3b25eccf37f2--------------------------------)
    ·12 min read·Jun 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94a20207398170b1d86652eeb8655152.png)'
  prefs: []
  type: TYPE_IMG
- en: Processed by the Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In his paper “**an Impossibility Theorem of Clustering**” published in 2002,
    Jon Kleinberg articulated that there is no clustering model that can satisfy all
    three desirable axioms of clustering simultaneously: scale invariance, richness,
    and consistency. (Kleinberg, 2002)'
  prefs: []
  type: TYPE_NORMAL
- en: What do those three axioms mean? Here is an interpretation of the three axioms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale invariant means: a clustering algorithm should generate the same results
    when all distances among datapoints are scaled by the factor of a constant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richness means: the clustering algorithm should demonstrate high capability
    in generating all possible partitions of a given dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistency means: when we enhance a set of clusters by increasing the inter-cluster
    distances and decreasing intra-cluster distances, the clustering algorithm should
    generate the same results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To cut a long story short, Kleinberg demonstrated that a mathematically satisfactory
    clustering algorithm is non-existent.
  prefs: []
  type: TYPE_NORMAL
- en: This might be a (the?) death sentence for clustering analysis to some theoretical
    fundamentalists.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, I encountered one academic paper challenging the validity of
    Kleinberg Impossibility Theorem. I would not step into that territory. But if
    you are interested in that, here you are: “[*On the Discrepancy Between Kleinberg’s
    Clustering Axioms and k-Means Clustering Algorithm Behavior*](https://arxiv.org/pdf/1702.04577).”'
  prefs: []
  type: TYPE_NORMAL
- en: Whatever the ground-truth may be, ever since Kleinberg released his impossibility
    theorem, many methodologies for clustering evaluation have been proposed from
    the field of engineering (e.g. applied mathematics, information theory, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: To pursue pragmatism in filling the gap between theoretical/scientific limitations
    and practical functionality is the domain of engineering.
  prefs: []
  type: TYPE_NORMAL
- en: As a matter of fact, it seems that there is no universally accepted scientific
    theory that explains why airplanes can fly. Here is [an article about it](https://www.scientificamerican.com/video/no-one-can-explain-why-planes-stay-in-the-air/).
    In the absence of scientific theory, I have survived many flights, thanks to the
    art of engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5a9b6dbad132e9861dca3247d35aea6.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Museums Victoria](https://unsplash.com/@museumsvictoria?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In appreciating the engineering spirit of pragmatism, I would need a reasonably
    good framework to fill the gap between Kleinberg’s Impossible Theorem and our
    daily practical applications for clustering analysis.
  prefs: []
  type: TYPE_NORMAL
- en: If airplanes can fly without universally accepted scientific theory, we can
    do clustering!
  prefs: []
  type: TYPE_NORMAL
- en: Maybe... why not!
  prefs: []
  type: TYPE_NORMAL
- en: 'Easier said than done, under Kleinberg’s Impossibility Theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: How can we evaluate the results of clustering algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we select algorithms best suited for a given objective?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A comprehensive understanding of these sorts of very simple questions remains
    elusive, at least to me.
  prefs: []
  type: TYPE_NORMAL
- en: In this backdrop, I encountered a paper written by Palacio-Niño & Berzal (2019),
    “[*Evaluation Metrics for Unsupervised Learning Algorithms*](https://arxiv.org/abs/1905.05667)”,
    in which they outlined a clustering validation framework in their attempt to better
    evaluate the quality of clustering performances under the mathematical limitations
    posed by “the Impossible Theorem”. Yes, they are quite conscious of Kleinberg’s
    Impossible Theorem in prescribing their framework.
  prefs: []
  type: TYPE_NORMAL
- en: To promote our pragmatic use of clustering algorithms, I thought that it would
    be constructive to share my study note about the pragmatic evaluation framework
    in this article.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is my note, there are many modifications here and there for my own
    personal purposes deviating from the details of the paper written by Palacio-Niño
    & Berzal. In addition, because this article rather intends to paint an overall
    structure of the proposed clustering validation framework, it does not intend
    to get into details. If you wish, please read [the full text of their original
    paper online](https://arxiv.org/abs/1905.05667) to fill the gap between my article
    and their original paper.
  prefs: []
  type: TYPE_NORMAL
- en: As the final precaution, I don’t profess that this is a comprehensive or a standard
    guide of clustering validation framework. But I hope that novices to clustering
    analysis find it useful as a guide to shape their own clustering validation framework.
  prefs: []
  type: TYPE_NORMAL
- en: No more, no less.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start.
  prefs: []
  type: TYPE_NORMAL
- en: '***An Overall Framework of Cluster Validation***'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the structure of their framework. We can see four stages of validation:'
  prefs: []
  type: TYPE_NORMAL
- en: preliminary evaluation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: internal validation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: external validation,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: relative validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s check them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '*1\. Preliminary evaluation:*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of this process is to simply confirm the presence of clusters
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: This process applies the framework of hypothesis testing to assess the presence
    of clustering tendency in the dataset. This process set the null hypothesis that
    the dataset is purely random so that there is no cluster tendency in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Since hypothesis testing can be treated as a standalone subject, I will put
    it aside from this article going forward.
  prefs: []
  type: TYPE_NORMAL
- en: '*2\. Internal Validation or Unsupervised Validation*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of the internal validation is to assess the quality of the clustering
    structure solely based on the given dataset without any external information about
    the ground-truth labels. In other words, when we do not have any advanced knowledge
    of the ground-truth labels, internal validation is a sole option.
  prefs: []
  type: TYPE_NORMAL
- en: A typical objective of clustering internal validation is to discover clusters
    that maximize intra-cluster similarities and minimize inter-cluster similarities.
    For this purpose, internal criteria are designed to measure intra-cluster similarities
    and inter-cluster dispersion. Simple!
  prefs: []
  type: TYPE_NORMAL
- en: 'That said, there is a catch:'
  prefs: []
  type: TYPE_NORMAL
- en: “*good scores on an internal criterion do not necessarily translate into good
    effectiveness in an application.*” (Manning et al., 2008)
  prefs: []
  type: TYPE_NORMAL
- en: A better score on internal criterion do not necessarily guarantee a better effectiveness
    of the resulting model. Internal validation is not enough!
  prefs: []
  type: TYPE_NORMAL
- en: So, what would we do then?
  prefs: []
  type: TYPE_NORMAL
- en: This is the very reason why we do need external validations.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. External Validation or Supervised Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In contrast to the internal validation, the external validation requires external
    class labels: ideally the ground-truth label, if not, potentially its representative
    surrogate. Because the very first reason why we use unsupervised clustering algorithm
    is because we do not have any idea about the labels to begin with, the idea of
    external validation appears absurd, paradoxical, or at least counterintuitive.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, when we have external information about the class labels — e.g.
    a set of results from a benchmark model or a gold standard model — we can implement
    the external validation.
  prefs: []
  type: TYPE_NORMAL
- en: Because we use reference class labels for validation, the objectives of external
    validation would naturally converge to the general validation framework of supervised
    classification analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In broader terms, this category includes the model selection and human judgement.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Relative Validation:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, relative validation.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an illustration of relative validation.
  prefs: []
  type: TYPE_NORMAL
- en: Particularly for the class of partitioning clustering (e.g. K-Means), the setting
    of the number of clusters is an important starting point to determine the configuration
    of algorithm, because it would materially affect the result of clustering.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, for this class of clustering algorithms, the number of clusters
    is a hyperparameter of the algorithm. In this context, the number of clusters
    needs to be optimized from the perspective of algorithm’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is the optimization needs to be simultaneously implemented
    together with other hyperparameters that determine the configuration of algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: It requires comparisons to understand how a set of hyperparameters settings
    can affect the algorithmic configuration. This type of relative validation is
    typically treated within the domain of parameter optimization. Since parameter
    optimization of machine learning algorithm is a big subject of machine learning
    training (model development), I will put it aside from this article going forward.
  prefs: []
  type: TYPE_NORMAL
- en: By now, we have a fair idea about the overall profile of their validation framework.
  prefs: []
  type: TYPE_NORMAL
- en: Next, a relevant question is “What sort of metrics shall we use for each validation?”
  prefs: []
  type: TYPE_NORMAL
- en: In this context, I gathered some metrics as examples for the internal validation
    and the external validation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics for Internal & External Validations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s focus on internal validation and external validation. Below, I will
    list some metrics of my choice with hyper-links where you can trace their definitions
    and formulas in details.
  prefs: []
  type: TYPE_NORMAL
- en: Since I will not cover the formulas for these metrics, the readers are advised
    to trace the hyper-links provided below to find them out!
  prefs: []
  type: TYPE_NORMAL
- en: A. Metrics used for Internal Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of the internal validation is to establish the quality of the
    clustering structure solely based on the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Classification of Internal evaluation methods:*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Internal validation methods can be categorized accordingly to the classes of
    clustering methodologies. A typical classification of clustering can be formulated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning methods (e.g. K-means),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical methods (e.g. agglomerative clustering),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density base methods (e.g. DBSCAN), and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the rest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, I cover the first two: partitioning clustering and hierarchical clustering.'
  prefs: []
  type: TYPE_NORMAL
- en: '***a) Partitioning Methods: e.g. K-means***'
  prefs: []
  type: TYPE_NORMAL
- en: 'For partitioning methods, there are three basis of evaluation metrics: cohesion,
    separation, and their hybrid.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cohesion:*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cohesion evaluates the closeness of the inner-cluster data structure. The lower
    the value of cohesion metrics, the better quality the clusters are. An example
    of cohesion metrics is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSW: Sum of Squared Errors Within Cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Separation:*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Separation is an inter-cluster metrics and evaluates the dispersion of the
    inter-cluster data structure. The idea behind a separation metric is to maximize
    the distance between clusters. An example of cohesion metrics is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SSB: Sum of Squared Errors between Clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hybrid of both cohesion and separation:*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hybrid type quantifies the level of separation and cohesion in a single metric.
    Here is a list of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'i) [The silhouette coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html):
    in the range of [-1, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: This metrics is a relative measure of the inter-cluster distance with neighboring
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a general interpretation of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best value: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The worst value: -1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Values near 0: overlapping clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Negative values: high possibility that a sample is assigned to a wrong cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric: [https://www.geeksforgeeks.org/silhouette-index-cluster-validity-index-set-2/?ref=ml_lbp](https://www.geeksforgeeks.org/silhouette-index-cluster-validity-index-set-2/?ref=ml_lbp)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ii) [The Calisnki-Harabasz coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html):'
  prefs: []
  type: TYPE_NORMAL
- en: Also known as the Variance Ratio Criterion, this metrics measures the ratio
    of the sum of inter-clusters dispersion and of intra-cluster dispersion for all
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given assignment of clusters, the higher the value of the metric, the
    better the clustering result is: since a higher value indicates that the resulting
    clusters are compact and well-separated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric: [https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp](https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp)'
  prefs: []
  type: TYPE_NORMAL
- en: 'iii) [Dann Index](https://github.com/jqmviegas/jqm_cvi/tree/master):'
  prefs: []
  type: TYPE_NORMAL
- en: For a given assignment of clusters, a higher Dunn index indicates better clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric: [https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp](https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp)'
  prefs: []
  type: TYPE_NORMAL
- en: 'iv) [Davies Bouldin Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score):'
  prefs: []
  type: TYPE_NORMAL
- en: The metric measures the ratio of intra-cluster similarity to inter-cluster similarity.
    Logically, a higher metric suggests a denser intra-cluster structure and a more
    separated inter-cluster structure, thus, a better clustering result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric: [https://www.geeksforgeeks.org/davies-bouldin-index/](https://www.geeksforgeeks.org/davies-bouldin-index/)'
  prefs: []
  type: TYPE_NORMAL
- en: '***b) Hierarchical Methods: e.g. agglomerate clustering algorithm***'
  prefs: []
  type: TYPE_NORMAL
- en: i) Human judgement based on visual representation of dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Although Palacio-Niño & Berzal did not include human judgement; it is one of
    the most useful tools for internal validation for hierarchical clustering based
    on dendrogram.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, the co-authors listed the following two correlation coefficient metrics
    specialized in evaluating the results of a hierarchical clustering.
  prefs: []
  type: TYPE_NORMAL
- en: For both, their higher values indicate better results. Both take values in the
    range of [-1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: 'ii) [The Cophenetic Correlation Coefficient](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cophenet.html)
    (CPCC): [-1, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: It measures distance between observations in the hierarchical clustering defined
    by the linkage.
  prefs: []
  type: TYPE_NORMAL
- en: 'iii) Hubert Statistic: [-1, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: A higher Hubert value corresponds to a better clustering of data.
  prefs: []
  type: TYPE_NORMAL
- en: '***c) Potential Category: Self-supervised learning***'
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning can generate the feature representations which can
    be used for clustering. Self-supervised learnings have no explicit labels in the
    dataset but use the input data itself as labels for learning. Palacio-Niño & Berzal
    did not include self-supervised framework, such as autoencoder and GANs, for their
    proposal in this section. Well, they are not clustering algorithm per se. Nevertheless,
    I will keep this particular domain pending for my note. Time will tell if any
    specialized metrics emerge from this particular domain.
  prefs: []
  type: TYPE_NORMAL
- en: Before closing the section of internal validation, here is a caveat from Gere
    (2023).
  prefs: []
  type: TYPE_NORMAL
- en: “*Choosing the proper hierarchical clustering algorithm and number of clusters
    is always a key question … . In many cases, researchers do not publish any reason
    why it was chosen a given distance measure and linkage rule along with cluster
    numbers. The reason behind this could be that different cluster validation and
    comparison techniques give contradictory results in most cases. … The results
    of the validation methods deviate, suggesting that clustering depends heavily
    on the data set in question. Although Euclidean distance, Ward’s method seems
    a safe choice, testing, and validation of different clustering combinations is
    strongly suggested.*”
  prefs: []
  type: TYPE_NORMAL
- en: Yes, it is a hard task.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to external validation.
  prefs: []
  type: TYPE_NORMAL
- en: B. Metrics used for External Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Repeatedly, a better score on internal criterion do not necessarily guarantee
    a better effectiveness of the resulting model. (Manning et al., 2008) In this
    context, it would be imperative for us to explore external validation.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the internal validation, the external validation requires external
    class labels. When we have such external information — the ground-truth labels
    as an idea option or their surrogates as a practical option such as the results
    from benchmark models — the external validation objective of clustering converges
    to that of supervised learning by design.
  prefs: []
  type: TYPE_NORMAL
- en: 'The co-authors listed three classes of external validation methodologies: matching
    sets, peer-to-peer correlation, and information theory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of them, in one way or another, compare two sets of cluster results: the
    one obtained from the clustering algorithm under evaluation, call it *C*; the
    other, call it *P*, from an external reference — another benchmark algorithm or,
    if possible, the ground-truth classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Matching Sets:***'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This class of methods identifies the relationship between each predicted cluster
    in ***C*** and its corresponding external reference classes of ***P***. Some of
    them are popular validation metrics for supervised classification. I will just
    list up some metrics in this category here. Please follow their hyper-links for
    further details.
  prefs: []
  type: TYPE_NORMAL
- en: 'a) [classification accuracy](https://stackoverflow.com/questions/34047540/python-clustering-purity-metric):'
  prefs: []
  type: TYPE_NORMAL
- en: 'b) [purity](https://stackoverflow.com/questions/34047540/python-clustering-purity-metric):'
  prefs: []
  type: TYPE_NORMAL
- en: 'c) [precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#precision-score):'
  prefs: []
  type: TYPE_NORMAL
- en: 'd) [recall score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#recall-score):'
  prefs: []
  type: TYPE_NORMAL
- en: 'e) [F-measure](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '***2\. Peer-to-peer correlation:***'
  prefs: []
  type: TYPE_NORMAL
- en: 'This class of metrics is a group of similarity measures between a pair of equivalent
    partitions resulted from two different methods, ***C*** and ***P***. Logically,
    the higher the similarity, the better the clustering result: in the sense that
    the predicted cluster classes resembles to the reference class labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'a) [Jaccard Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html):
    [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'It compares the external reference class labels and the predicted labels by
    measuring the overlap between these 2 sets: the ratio of the size of the intersection
    to that of the union of the two labels sets.'
  prefs: []
  type: TYPE_NORMAL
- en: A higher the metric, the more correlated these two sets are.
  prefs: []
  type: TYPE_NORMAL
- en: 'b) [Rand Index](https://clusteringjl.readthedocs.io/en/latest/randindex.html):
    [0, 1]'
  prefs: []
  type: TYPE_NORMAL
- en: “[*From a mathematical standpoint, Rand index is related to the accuracy, but
    is applicable even when class labels are not used.*](https://clusteringjl.readthedocs.io/en/latest/randindex.html)”
  prefs: []
  type: TYPE_NORMAL
- en: Here is how to interpret the result of the metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value, 0: No agreement between the two sets of clustering results, ***C***
    and ***P***.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The value, 1: Complete agreement between the two sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.geeksforgeeks.org/rand-index-in-machine-learning/?ref=ml_lbp](https://www.geeksforgeeks.org/rand-index-in-machine-learning/?ref=ml_lbp)'
  prefs: []
  type: TYPE_NORMAL
- en: c) [Folkes Mallows coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score)
  prefs: []
  type: TYPE_NORMAL
- en: It measures “[*the geometric mean between of the precision and recall*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score)*.*”
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a use case example of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.geeksforgeeks.org/ml-fowlkes-mallows-score/](https://www.geeksforgeeks.org/ml-fowlkes-mallows-score/)'
  prefs: []
  type: TYPE_NORMAL
- en: '***3\. Information Theory:***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have another class of metrics from information theory. There are two
    basis for this class of metrics: entropy and mutual information.'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is“*a reciprocal measure of purity that allows us to measure the degree
    of disorder in the clustering results.*”
  prefs: []
  type: TYPE_NORMAL
- en: Mutual Information measures “*the reduction in uncertainty about the clustering
    results given knowledge of the prior partition.*”
  prefs: []
  type: TYPE_NORMAL
- en: And we have the following metrics as examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[Adjusted Mutual Information Score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Variation of Information](https://clusteringjl.readthedocs.io/en/latest/varinfo.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[V-Measure](https://clusteringjl.readthedocs.io/en/latest/vmeasure.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. ***Model Selection Metrics:***
  prefs: []
  type: TYPE_NORMAL
- en: For external validation, I further would like to add model selection metrics
    below from another reference. (Karlsson et al., 2019)
  prefs: []
  type: TYPE_NORMAL
- en: '[Akaike’s information criterion (AIC)](https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.aic.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[the Hannan–Quinn information criterion (HQC)](https://www.statsmodels.org/dev/generated/statsmodels.tools.eval_measures.hqic.html)
    and'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[the Bayesian information criterion (BIC)](https://www.statsmodels.org/dev/generated/statsmodels.tools.eval_measures.bic.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use them to compare their values among multiple results. The result with
    the lowest values of these metrics is considered to be the best fit. Nevertheless,
    standalone these metrics cannot tell the quality of one single result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a precaution for the use of these model selection metrics. For any
    of those information criteria to be valid in evaluating models, it requires a
    certain set of preconditions: low multicollinearity, sufficient sample sizes,
    and good fitting of models with high R-squared metrics. When any of these conditions
    is not met, the reliability of these metrics could materially be impaired. (Karlsson
    et al., 2019)'
  prefs: []
  type: TYPE_NORMAL
- en: That’s all for this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would not profess that what I covered here is comprehensive or even a gold
    standard. Actually, there are different approaches. For example, R has a clustering
    validation package called [***clValid***](https://cran.r-project.org/web/packages/clValid/vignettes/clValid.pdf),
    which uses different approaches: from “internal”, “stability”, and “biological”
    modes. And I think that ***clValid*** is a wonderful tool.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that, I rather would hope that this article serves as a useful starting
    guide for novices to clustering analysis in shaping their own clustering evaluation
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: Repeatedly, the intention of this article is to paint an overview of a potentially
    pragmatic evaluation framework for clustering under the theoretical constraints
    characterized by Kleinberg’s Impossible Theorem of Clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'At last, but not least, remember the following aphorism:'
  prefs: []
  type: TYPE_NORMAL
- en: “[All models are wrong, but some are useful](https://www.lacan.upc.edu/admoreWeb/2018/05/all-models-are-wrong-but-some-are-useful-george-e-p-box/)”
  prefs: []
  type: TYPE_NORMAL
- en: This aphorism should continue resonating in our mind when we deal with any model.
    By the way, the aphorism is often associated with the renowned statistician of
    history, George E. P. Box.
  prefs: []
  type: TYPE_NORMAL
- en: Given the imperfect conditions that we live in, let’s promote together practical
    knowledge in the spirit of Aristotelian [*Phronesis*](https://en.wikipedia.org/wiki/Phronesis#:~:text=Phronesis%20(Ancient%20Greek%3A%20%CF%86%CF%81%CF%8C%CE%BD%CE%B7%CF%83%E1%BF%90%CF%82%2C,discussion%20in%20ancient%20Greek%20philosophy.).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: Michio Suginoo
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Davies_bouldin_score*. (n.d.). Scikit-Learn. Retrieved June 20, 2024, from
    [https://scikit-learn/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html](https://scikit-learn/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gere, A. (2023). Recommendations for validating hierarchical clustering in consumer
    sensory projects. *Current Research in Food Science*, *6*, 100522\. [https://doi.org/10.1016/j.crfs.2023.100522](https://doi.org/10.1016/j.crfs.2023.100522)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karlsson, P. S., Behrenz, L., & Shukur, G. (2019). Performances of Model Selection
    Criteria When Variables are Ill Conditioned. *Computational Economics*, *54*(1),
    77–98\. [https://doi.org/10.1007/s10614-017-9682-8](https://doi.org/10.1007/s10614-017-9682-8)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kleinberg, J. (2002). An Impossibility Theorem for Clustering. *Advances in
    Neural Information Processing Systems*, *15*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manning, C. D., Raghavan, P., & Schütze, H. (2008). *Introduction to Information
    Retrieval/ Flat Clustering/ Evaluation of clustering*. [Https://Nlp.Stanford.Edu/IR-Book.](http://Https://Nlp.Stanford.Edu/IR-Book.)
    [https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Palacio-Niño, J.-O., & Berzal, F. (2019). *Evaluation Metrics for Unsupervised
    Learning Algorithms* (arXiv:1905.05667). arXiv. [http://arxiv.org/abs/1905.05667](http://arxiv.org/abs/1905.05667)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
