- en: 'Dimensionality Reduction Made Simple: PCA Theory and Scikit-Learn Implementation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/dimensionality-reduction-made-simple-pca-theory-and-scikit-learn-implementation-9d07a388df9e?source=collection_archive---------6-----------------------#2024-02-07](https://towardsdatascience.com/dimensionality-reduction-made-simple-pca-theory-and-scikit-learn-implementation-9d07a388df9e?source=collection_archive---------6-----------------------#2024-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tame the Curse of Dimensionality! Learn Dimensionality Reduction (PCA) and implement
    it with Python and Scikit-Learn.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page---byline--9d07a388df9e--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page---byline--9d07a388df9e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9d07a388df9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9d07a388df9e--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page---byline--9d07a388df9e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9d07a388df9e--------------------------------)
    ·11 min read·Feb 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd97fa20ef6b63759949e85573f2a094.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/3DkouQeZjp4).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the novel [Flatland](https://en.wikipedia.org/wiki/Flatland), characters
    living in a two-dimensional world find themselves perplexed and unable to comprehend
    when they encounter a three-dimensional being. I use this analogy to illustrate
    how similar phenomena occur in Machine Learning when dealing with problems involving
    thousands or even millions of dimensions (i.e. features): **surprising phenomena
    happen**, which have **disastrous implications** on our Machine Learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m sure you felt stunned, at least once, by the **huge number of features**
    involved in modern Machine Learning problems. Every Data Science practitioner,
    sooner or later, will face this challenge. This article will explore the theoretical
    foundations and the Python implementation of the most used Dimensionality Reduction
    algorithm: [**Principal Component Analysis**](https://en.wikipedia.org/wiki/Principal_component_analysis#:~:text=Principal%20component%20analysis%20(PCA)%20is,the%20visualization%20of%20multidimensional%20data.)
    (PCA).'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need to reduce the number of features?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Datasets involving thousands or even millions of features are common nowadays.
    Adding new features to a dataset can bring in valuable information, however, they
    will **slow the training process** and make…
  prefs: []
  type: TYPE_NORMAL
