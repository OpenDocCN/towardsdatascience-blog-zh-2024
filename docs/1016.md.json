["```py\nInstructions: Does Speaker 2's answer mean yes or no?\nOutput labels: no, yes\nInput: Speaker 1: \"You do this often?\" Speaker 2: \"It's my first time.\"\nOutput:\n```", "```py\nclass Component:\n    def __init__(self, **kwargs): pass\nclass Metaprompt(Component): pass\nclass Paragraph(Component): pass\nclass InputData(Component): pass\n\nprompt = Metaprompt(\n    children=[\n        Paragraph(text=\"Instructions: \"),\n        Paragraph(\n            id=\"instructions\",\n            text=\"Does Speaker 2's answer mean yes or no?\",\n        ),\n        Paragraph(id=\"labels\", text=\"Output labels: yes, no\"),\n        InputData(),\n        Paragraph(text=\"Output: \"),\n    ]\n)\n```", "```py\nimport pyglove as pg\n@pg.symbolize\nclass Component:\n    def __init__(self, **kwargs): pass\n```", "```py\nprompt.rebind({'children[1].text': 'Does the response mean yes?'})\nprompt.rebind({'children[2]': pg.MISSING_VALUE})\nprint(prompt)\n```", "```py\nMetaprompt(\n  children = [\n    0 : Paragraph(\n      text = 'Instructions: '\n    ),\n    1 : Paragraph(\n      id = 'instructions',\n      text = 'Does the response mean yes?'\n    ),\n    2 : InputData(),\n    3 : Paragraph(\n      text = 'Output: '\n    )\n  ]\n)\n```", "```py\ndef starting_prompt():\n    instructions = MetaPrompt(\n        Paragraph(text=\"Instructions: \"),\n        Paragraph(\n            id=\"instructions\",\n            text=\"Does Speaker 2's answer mean yes or no?\",\n        ),\n        Paragraph(id=\"labels\", text=\"Output labels: yes, no\"),\n        InputData(),\n        Paragraph(text=\"Output: \"),\n    )\n    return Output(instructions.with_extractor())\n```", "```py\nmydata = DataTable.from_records(\n    records, # list of {\"input\": <>, \"output\": <>}\n    constants={\"instructions\": default_instructions}, \n)\n```", "```py\ndef accuracy(y_true: DataTable, y_pred: DataTable) -> EvaluationScore:\n    y_true = y_true.outputs.normalized_values()\n    y_pred = y_pred.outputs.normalized_values()\n    n_correct = sum([y_p == y_t for y_p, y_t in zip(y_pred, y_true)])\n    return EvaluationScore(n_correct / len(y_true))\n```", "```py\nmutation_operators = BagOfMutators(\n    starting_prompt=StartingPrompt(d_train),\n    InduceInstructions({\"id\": \"instructions\"}, d_train),\n    Paraphrase({\"id\": \"instructions\"}),\n)\n```", "```py\nrunner = OpenAIChat(\n    model_id=\"gpt-3.5-turbo-16k\",\n    api_config={\"api_key\": YOUR_KEY},\n    cache=\"cache.tsv\",\n)\nprompt_optimizer = BeamSearch(runner, mutation_operators, accuracy, depth=6)\ntransformed = prompt_optimizer.fit_transform(d_train)\n```", "```py\n...\nParagraph(\n    \"Consider the dialogue, context, and background \"\n    \"information provided to determine the most suitable output label\",\n    id=\"instructions\",\n)\n...\n```", "```py\n...\nParagraph(\n    \"\",\n    id=\"instructions\",\n)\n...\n```", "```py\nclass RagStartingPrompt:\n    def __init__(self, dtrain, examples, embedding_runner):\n        self._examples = examples\n        self._dtrain = dtrain\n        self._embedding_runner = embedding_runner\n\n    def __call__(self, return_raw=False):\n        structure = [\n            Section(\"Syntax\", self._dtrain.constants[\"list_of_operators\"]),\n            Section(\n                \"Examples\",\n                EmbeddingFewshotExamples(\n                    self._embedding_runner, self._examples, 5\n                ),\n            ),\n            Section(\n                \"Complete and output in the same format as above\",\n                InputData(),\n            ),\n        ]\n        instructions = MetaPrompt(\n            structure,\n            render_as=\"markdown\",\n            data_formatter=JSONDataFormatter(),\n        )  \n        return Output(\n            instructions.with_extractor(),\n            on_error=\"empty_result\",\n        )\n```"]