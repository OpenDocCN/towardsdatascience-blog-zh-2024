<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Perception-Inspired Graph Convolution for Music Understanding Tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Perception-Inspired Graph Convolution for Music Understanding Tasks</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perception-inspired-graph-convolution-for-music-understanding-tasks-4d2ba1be48e7?source=collection_archive---------3-----------------------#2024-07-09">https://towardsdatascience.com/perception-inspired-graph-convolution-for-music-understanding-tasks-4d2ba1be48e7?source=collection_archive---------3-----------------------#2024-07-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4521" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This article discusses MusGConv, a perception-inspired graph convolution block for symbolic musical applications</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://manoskary.medium.com/?source=post_page---byline--4d2ba1be48e7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Emmanouil Karystinaios" class="l ep by dd de cx" src="../Images/120d889f330aa7b433a0668a1224e1c8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*OJefyxSpqBkD14wGUsHeFA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4d2ba1be48e7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://manoskary.medium.com/?source=post_page---byline--4d2ba1be48e7--------------------------------" rel="noopener follow">Emmanouil Karystinaios</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4d2ba1be48e7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/38ce2a0126823d36efafd032cdd049bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqKhNPjFBvj3WE5gDvMqsA.png"/></div></div></figure><h1 id="0c89" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Introduction</h1><p id="8c61" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">In the field of Music Information Research (MIR), the challenge of understanding and processing musical scores has continuously been introduced to new methods and approaches. Most recently many graph-based techniques have been proposed as a way to target music understanding tasks such as voice separation, cadence detection, composer classification, and Roman numeral analysis.</p><p id="8f10" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">This blog post covers one of my recent papers in which I introduced a new graph convolutional block, called <strong class="nv fr">MusGConv</strong>, designed specifically for processing music score data. <strong class="nv fr">MusGConv</strong> takes advantage of music perceptual principles to improve the efficiency and the performance of graph convolution in Graph Neural Networks applied to music understanding tasks.</p><h1 id="c56a" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Understanding the Problem</h1><p id="ac88" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Traditional approaches in MIR often rely on audio or symbolic representations of music. While audio captures the intensity of sound waves over time, symbolic representations like MIDI files or musical scores encode discrete musical events. Symbolic representations are particularly valuable as they provide higher-level information essential for tasks such as music analysis and generation.</p><p id="c737" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">However, existing techniques based on symbolic music representations often borrow from computer vision (CV) or natural language processing (NLP) methodologies. For instance, representing music as a “pianoroll” in a matrix format and treating it similarly to an image, or, representing music as a series of tokens and treating it with sequential models or transformers. These approaches, though effective, could fall short in fully capturing the complex, multi-dimensional nature of music, which includes hierarchical note relation and intricate pitch-temporal relationships. Some recent approaches have been proposed to model the musical score as a graph and apply Graph Neural Networks to solve various tasks.</p><h2 id="fbbb" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">The Musical Score as a Graph</h2><p id="200d" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">The fundamental idea of GNN-based approaches to musical scores is to model a musical score as a graph where notes are the vertices and edges are built from the temporal relations between the notes. To create a graph from a musical score we can consider four types of edges (see Figure below for a visualization of the graph on the score):</p><ul class=""><li id="a22a" class="nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo pl pm pn bk"><em class="po">onset edges</em>: connect notes that share the same onset;</li><li id="7232" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pl pm pn bk"><em class="po">consecutive edges</em> (or <em class="po">next edges</em>): connect a note x to a note y if the offset of x corresponds to the onset of y;</li><li id="5d90" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pl pm pn bk"><em class="po">during edges: </em>connect a note x to a note y if the onset of y falls within the onset and offset of x;</li><li id="8510" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pl pm pn bk"><em class="po">rest edges</em> (or <em class="po">silence edges</em>): connect the last notes before a rest to the first ones after it.</li></ul><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pu"><img src="../Images/6987757e6a0d20c7eb72defcf27a721e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ebbxC6NrruwNoLByrOikg.png"/></div></div></figure><p id="a81a" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">A GNN can treat the graph created from the notes and these four types of relations.</p><h1 id="75e6" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Introducing MusGConv</h1><p id="bc86" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">MusGConv is designed to leverage music score graphs and enhance them by incorporating principles of music perception into the graph convolution process. It focuses on two fundamental dimensions of music: pitch and rhythm, considering both their relative and absolute representations.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/2dd885f5d8623cb8a5d42c7a43098506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RwSHjas9W5N7sUYTL-EakA.png"/></div></div></figure><p id="4eb2" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">Absolute representations refer to features that can be attributed to each note individually such as the note’s pitch or spelling, its duration or any other feature. On the other hand, relative features are computed between pairs of notes, such as the music interval between two notes, their onset difference, i.e. the time on which they occur, etc.</p><h2 id="6e51" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Key Features of MusGConv</h2><ol class=""><li id="7ee7" class="nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo pw pm pn bk"><strong class="nv fr">Edge Feature Computation</strong>: MusGConv computes edge features based on the distances between notes in terms of onset, duration, and pitch. The edge features can be normalized to ensure they are more effective for Neural Network computations.</li><li id="a50c" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Relative and Absolute Representations</strong>: By considering both relative features (distance between pitches as edge features) and absolute values (actual pitch and timing as node features), MusGConv can adapt and use the representation that is more relevant depending on the occasion.</li><li id="5440" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Integration with Graph Neural Networks</strong>: The MusGConv block integrates easily with existing GNN architectures with almost no additional computational cost and can be used to improve musical understanding tasks such as voice separation, harmonic analysis, cadence detection, or composer identification.</li></ol><p id="1224" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">The importance and coexistence of the relative and absolute representations can be understood from a transpositional perspective in music. Imagine the same music content transposed. Then, the intervalic relations between notes stay the same but the pitch of each note is altered.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk px"><img src="../Images/0cf1ef46a42f17b5c98c0178aeb597d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UG7SYNwc5kL1mbN7FophPA.png"/></div></div><figcaption class="py pz qa mj mk qb qc bf b bg z dx">Same content transposed by a major third. The relation between the notes between the top and the bottom are the same but the absolute pitch is changed.</figcaption></figure><h1 id="d777" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Understanding Message Passing in Graph Neural Networks (GNNs)</h1><p id="0a5a" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">To fully understand the inner workings of the MusGConv convolution block it is important to first explain the principles of Message Passing.</p><h2 id="cda1" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">What is Message Passing?</h2><p id="8a98" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">In the context of GNNs, message passing is a process where vertices within a graph exchange information with their neighbors to update their own representations. This exchange allows each node to gather contextual information from the graph, which is then used to for predictive tasks.</p><p id="7f33" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">The message passing process is defined by the following steps:</p><ol class=""><li id="3350" class="nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo pw pm pn bk"><strong class="nv fr">Initialization</strong>: Each node is assigned to a feature vector, which can include some important properties. For example in a musical score, this could include pitch, duration, and onset time for each node/note.</li><li id="7120" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Message Generation</strong>: Each node generates a message to send to its neighbors. The message typically includes the node’s current feature vector and any edge features that describe the relationship between the nodes. A message can be for example a linear transformation of the neighbor’s node features.</li><li id="50e5" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Message Aggregation</strong>: Each node collects messages from its neighbors. The aggregation function is usually a permutation invariant function such as sum, mean, or max and it combines these messages into a single vector, ensuring that the node captures information from its entire neighborhood.</li><li id="cd24" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Node Update</strong>: The aggregated message is used to update the node’s feature vector. This update often involves applying a neural network layer (like a fully connected layer) followed by a non-linear activation function (such as ReLU).</li><li id="cc0a" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Iteration</strong>: Steps 2–4 are repeated for a specified number of iterations or layers, allowing information to propagate through the graph. With each iteration, nodes incorporate information from progressively larger neighborhoods.</li></ol><h2 id="c8be" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Message Passing in MusGConv</h2><p id="97bf" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">MusGConv alters the standard message passing process mainly by incorporating both absolute features as node features and relative musical features as edge features. This design is tailored to fit the nature of musical data.</p><p id="3abd" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">The MusGConv convolution is defined by the following steps:</p><ol class=""><li id="db85" class="nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo pw pm pn bk"><strong class="nv fr">Edge Features Computation</strong>: In MusGConv, edge features are computed as the difference between notes in terms of onset, duration, and pitch. Additionally, pitch-class intervals (distances between notes without considering the octave) are included, providing an reductive but effective method to quantify music intervals.</li><li id="e620" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Message Computation</strong>: The message within the MusGConv includes the source node’s current feature vector but also the afformentioned edge features from the source to the destination node, allowing the network to leverage both absolute and relative information of the neighbors during message passing.</li><li id="2c00" class="nt nu fq nv b go pp nx ny gr pq oa ob oc pr oe of og ps oi oj ok pt om on oo pw pm pn bk"><strong class="nv fr">Aggregation and Update</strong>: MusGConv uses sum as the aggregation function, however, it concatenates the current node representation with the sum of its neighbor messages.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/8fbdc41387d6265bde062a84600ceedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*xqN7ofw0xtR_pXYcj_rZQQ.png"/></div><figcaption class="py pz qa mj mk qb qc bf b bg z dx">The MusGConv graph convolutional block.</figcaption></figure><p id="598a" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">By designing the message passing mechanism in this way, MusGConv attempts to preserve the relative perceptual properties of music (such as intervals and rhythms), leading to more meaningful representations of musical data.</p><p id="2238" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">Should edge features are absent or deliberately not provided then MusGConv computes the edge features between two nodes as the absolute difference between their node features. The version of MusGConv with the edges features is named MusGConv(+EF) in the experiments.</p><h1 id="6791" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Applications and Experiments</h1><p id="22ad" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">To demonstrate the potential of MusGConv I discuss below the tasks and the experiments conducted in the paper. All models independent of the task are designed with the pipeline shown in the figure below. When MusGConv is employed the GNN blocks are replaced by MusGConv blocks.</p><p id="d068" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">I decided to apply MusGConv to four tasks: voice separation, composer classification, Roman numeral analysis, and cadence detection. Each one of these tasks presents a different taxonomy from a graph learning perspective. Voice separation is a link prediction task, composer classification is a global classification task, cadence detection is a node classification task, and Roman numeral analysis can be viewed as a subgraph classification task. Therefore we are exploring the suitability of MusGConv not only from a musical analysis perspective but through out the spectrum of graph deep learning task taxonomy.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qe ln qf lo qg cf qh cg qi ci bh"><figure class="mm mn mo mp mq mr qk ql paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/e30351b46580c8dd2b21e7e34d8e00f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*x9BCcjKvOxOpyCULHuj65Q.png"/></div></div><figcaption class="py pz qa mj mk qb qc bf b bg z dx">Example of a general graph pipeline for symbolic music understanding tasks</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ca61" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Voice Separation</h2><p id="ef32" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Voice separation is the detection of individual monophonic streams within a polyphonic music excerpt. Previous methods had employed GNNs to solve this task. From a GNN perspective, voice separation can be viewed as link prediction task, i.e. for every pair of notes we predict if they are connected by an edge or not. The product the link prediction process should be a graph where consecutive notes in the same voice are ought to be connected. Then voices are the connected components of the predicted graph. I point the readers to <a class="af qm" href="https://arxiv.org/abs/2304.14848" rel="noopener ugc nofollow" target="_blank">this paper</a> for more information on voice separation using GNNs.</p><p id="a18d" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">For voice separation the pipeline of the above figure applies to the GNN encoder part of the architecture. The link prediction part takes place as the task specific module of the pipeline. To use MusGConv it is sufficient to replace the convolution blocks of the GNN encoder with MusGConv. This simple substitution results in more accurate prediction making less mistakes.</p><p id="4165" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">Since the interpretation of deep learning systems is not exactly trivial, it is not easy to pinpoint the reason for the improved performance. From a musical perspective consecutive notes in the same voice should tend to have smaller relative pitch difference. The design of MusGConv definitely outlines the pitch differences with the relative edge features. However, I would need to also say, from individual observations that music does not strictly follow any rules.</p><h2 id="e73c" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Composer Classification</h2><p id="a6c1" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Composer classification is the process of identifying a composer based on some music excerpt. Previous GNN-based approaches for this task receive a score graph as input similarly to the pipeline shown above and then they include some global pooling layer that collapses the graph of the music excerpt to a vector. From that vector then the classification process applied where classes are the predefined composers.</p><p id="87d2" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">Yet again, MusGConv is easy to implement by replacing the GNN convolutional blocks. In the experiments, using MusGConv was indeed very beneficial in solving this task. My intuition is that relative features in combination with the absolute give better insights to compositional style.</p><h2 id="923c" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Roman Numeral Analysis</h2><p id="5d87" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Roman numeral analysis is a method for harmonic analysis where chords are represented as Roman numerals. The task for predicting the Roman numerals is a fairly complex one. Previous architectures used a mixture of GNNs and Sequential models. Additionally, Roman numeral analysis is a multi-task classification problem, typically a Roman numeral is broken down to individual simpler tasks in order to reduce the class vocabulary of unique Roman numerals. Finally, the graph-based architecture of Roman numeral analysis also includes a onset contraction layer after the graph convolution that transforms the graph to an ordered sequence. This onset contraction layer, contracts groups of notes that occur at the same time and they are assigned to the same label during classification. Therefore, it can be viewed as a subgraph classification task. I would reckon that the explication of this model would merit its own post, therefore, I would suggest reading <a class="af qm" href="https://arxiv.org/abs/2307.03544" rel="noopener ugc nofollow" target="_blank">the paper </a>for more insights.</p><p id="4bc2" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">Nevertheless, the general graph pipeline in the figure is still applicable. The sequential models together with the multitask classification process and the onset contraction module entirely belong to the task-specific box. However, replacing the Graph Convolutional Blocks with MusGConv blocks does not seem to have an effect on this task and architecture. I attribute this to the fact that the task and the model architecture are simply too complex.</p><h2 id="32a2" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Cadence Detection</h2><p id="26cd" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Finally, let’s discuss cadence detection. Detecting cadences can be viewed as similar to detecting phrase endings and it is an important aspect of music analysis. Previous methods for cadence detection employed GNNs with an encoder-decoder GNN architecture. Each note which by now we know that also corresponds to one node in the graph is classified to being a cadence note or not. The cadence detection task includes a lot of peculiarities such as very heavy class imbalances as well as annotation ambiguities. If you are interested I would again suggest to check out <a class="af qm" href="https://arxiv.org/abs/2208.14819" rel="noopener ugc nofollow" target="_blank">this paper</a>.</p><p id="7299" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">The use of MusGConv convolution in the encoder of can be beneficial for detecting cadences. I believe that the combination of relative and absolute features and the design of MusGConv can keep track of voice leading patterns that often occur around cadences.</p><h2 id="78b5" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk">Results and Evaluation</h2><p id="817e" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">Extensive experiments have shown that MusGConv can outperform state-of-the-art models across the aforementioned music understanding tasks. The table below summarizes the improvements:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/d72d396c9948e25565c95504855c632a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckiuj3j59wcUyJIpFeUmEA.png"/></div></div><figcaption class="py pz qa mj mk qb qc bf b bg z dx">(F1) stands for macro F1 score otherwise simple Accuracy score is shown.</figcaption></figure><p id="a87d" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">However soulless a table can be, I would prefer not to fully get into any more details in the spirit of keeping this blog post lively and towards a discussion. Therefore, I invite you to check out the original <a class="af qm" href="https://arxiv.org/pdf/2405.09224" rel="noopener ugc nofollow" target="_blank">paper</a> for more details on the results and datasets.</p><h1 id="43e6" class="mx my fq bf mz na nb gq nc nd ne gt nf ng nh ni nj nk nl nm nn no np nq nr ns bk">Summary and Discussion</h1><p id="6979" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk"><strong class="nv fr">MusGConv</strong> is a graph convolutional block for music. It offers a simple perception-inspired approach to graph convolution that results to performance improvement of GNNs when applied to music understanding tasks. Its simplicity is the key to its effectiveness. In some tasks, it is very beneficial is some others not so much. The inductive bias of the relative and absolute features in music is a neat trick to magically improve your GNN results but my advice is to always take it with a pinch of salt. Try out MusGConv by all means but also do not forget about all the other cool graph convolutional block possibilities.</p><p id="aa8f" class="pw-post-body-paragraph nt nu fq nv b go op nx ny gr oq oa ob oc or oe of og os oi oj ok ot om on oo fj bk">If you are interested in trying <strong class="nv fr">MusGConv</strong>, the code and models are available on <a class="af qm" href="https://github.com/manoskary/musgconv" rel="noopener ugc nofollow" target="_blank"><strong class="nv fr">GitHub</strong></a>.</p><h2 id="f8d9" class="ou my fq bf mz ov ow ox nc oy oz pa nf oc pb pc pd og pe pf pg ok ph pi pj pk bk"><strong class="al">Notes and Acknowledgments</strong></h2><p id="a035" class="pw-post-body-paragraph nt nu fq nv b go nw nx ny gr nz oa ob oc od oe of og oh oi oj ok ol om on oo fj bk">All images in this post are by the author. I would like to thank Francesco Foscarin my co-author of the original paper for his contributions to this work.</p></div></div></div></div>    
</body>
</html>