<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Spoiler Alert: The Magic of RAG Does Not Come from AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Spoiler Alert: The Magic of RAG Does Not Come from AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spoiler-alert-the-magic-of-rag-does-not-come-from-ai-8a0ed2ad4800?source=collection_archive---------0-----------------------#2024-11-17">https://towardsdatascience.com/spoiler-alert-the-magic-of-rag-does-not-come-from-ai-8a0ed2ad4800?source=collection_archive---------0-----------------------#2024-11-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a860" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Why retrieval, not generation, makes RAG systems magical</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@frankw_usa?source=post_page---byline--8a0ed2ad4800--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Frank Wittkampf" class="l ep by dd de cx" src="../Images/3dbd69f8ef648074fa170fac451645fd.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NKA6NcV6ulN5ingXiivgKQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8a0ed2ad4800--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@frankw_usa?source=post_page---byline--8a0ed2ad4800--------------------------------" rel="noopener follow">Frank Wittkampf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8a0ed2ad4800--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="938c" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Quick POCs</h2><p id="212d" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Most quick proof of concepts (POCs) which allow a user to explore data with the help of conversational AI simply blow you away. It feels like pure magic when you can all of a sudden talk to your documents, or data, or code base.</p><p id="49bc" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">These POCs work wonders on small datasets with a limited count of docs. However, as with almost anything when you bring it to production, you quickly run into problems at scale. When you do a deep dive and you inspect the answers the AI gives you, you notice:</p><ul class=""><li id="4e22" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz of og oh bk">Your agent doesn’t reply with complete information. It missed some important pieces of data</li><li id="b6ac" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk">Your agent doesn’t reliably give the same answer</li><li id="735b" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk">Your agent isn’t able to tell you how and where it got which information, making the answer significantly less useful</li></ul><p id="0439" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">It turns out that the <strong class="nj fr">real magic in RAG</strong> does not happen in the generative AI step, but in the process of retrieval and composition. Once you dive in, it’s pretty obvious why…</p><p id="f138" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk"><em class="on">* RAG = Retrieval Augmented Generation — </em><a class="af oo" href="https://en.wikipedia.org/wiki/Retrieval-augmented_generation" rel="noopener ugc nofollow" target="_blank"><em class="on">Wikipedia Definition of RAG</em></a></p><figure class="os ot ou ov ow ox op oq paragraph-image"><div class="op oq or"><img src="../Images/1546c06f10cd9e6f67fbd43ec17c1601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*JKXH3z6q5S_CIrofFt37pw.png"/></div><figcaption class="oz pa pb op oq pc pd bf b bg z dx">RAG process — Illustration</figcaption></figure><h1 id="76d2" class="pe mk fq bf ml pf pg gq mp ph pi gt mt pj pk pl pm pn po pp pq pr ps pt pu pv bk">So, how does a RAG-enabled AI agent answer a question?</h1><p id="462a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">A quick recap of how a simple RAG process works:</p><ol class=""><li id="c378" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz pw og oh bk">It all starts with a <strong class="nj fr">query</strong>. The user asked a question, or some system is trying to answer a question.</li><li id="5d7d" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz pw og oh bk">A <strong class="nj fr">search</strong> is done with the query. Mostly you’d embed the query and do a similarity search, but you can also do a classic elastic search or a combination of both, or a straight lookup of information</li><li id="4c91" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz pw og oh bk">The search result is a set of <strong class="nj fr">documents</strong> (or document snippets, but let’s simply call them documents for now)</li><li id="58d3" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz pw og oh bk">The documents and the essence of the query are combined into some easily readable <strong class="nj fr">context</strong> so that the AI can work with it</li><li id="11ef" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz pw og oh bk">The <strong class="nj fr">AI interprets</strong> the question and the documents and generates an answer</li><li id="8933" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz pw og oh bk">Ideally this answer is <strong class="nj fr">fact checked</strong>, to see if the AI based the answer on the documents, and/or if it is appropriate for the audience</li></ol><h1 id="d883" class="pe mk fq bf ml pf pg gq mp ph pi gt mt pj pk pl pm pn po pp pq pr ps pt pu pv bk">Where’s the magic?</h1><p id="3427" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The dirty little secret is that the essence of the RAG process is that you have to provide the answer to the AI (before it even does anything), so that it is able to give you the reply that you’re looking for.</p><p id="59d8" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">In other words:</p><ul class=""><li id="2c44" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz of og oh bk">the work that the AI does (step 5) is <strong class="nj fr">apply judgement, and properly articulate the answer</strong></li><li id="919a" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk">the work that the engineer does (step 3 and 4) is <strong class="nj fr">find the answer and compose it such that AI can digest it</strong></li></ul><p id="585c" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Which is more important? The answer is, of course, it depends, because if judgement is the critical element, then the AI model does all the magic. But for an endless amount of business use cases, finding and properly composing the pieces that make up the answer, is the more important part.</p><h1 id="54c1" class="pe mk fq bf ml pf pg gq mp ph pi gt mt pj pk pl pm pn po pp pq pr ps pt pu pv bk">What are the typical engineering problems to solve if you want proper RAG process?</h1><p id="58cc" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The first set of problems to solve when running a RAG process are the data ingestion, splitting, chunking, document interpretation issues. I’ve written about a few of these in <a class="af oo" href="https://medium.com/@frankw_usa" rel="noopener">prior articles</a>, but am ignoring them here. For now let’s assume you have properly solved your data ingestion, you have a lovely vector store or search index.</p><p id="bbf8" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Typical challenges:</p><ul class=""><li id="b6c8" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz of og oh bk"><strong class="nj fr">Duplication </strong>— Even the simplest production systems often have duplicate documents. More so when your system is large, you have extensive users or tenants, you connect to multiple data sources, or you deal with versioning, etc.</li><li id="4ab5" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Near duplication</strong> — Documents which largely contain the same data, but with minor changes. There are two types of near duplication: <br/> — Meaningful — E.g. a small correction, or a minor addition, e.g. a date field with an update<br/> — Meaningless — E.g.: minor punctuation, syntax, or spacing differences, or just differences introduced by timing or intake processing</li><li id="0c78" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Volume</strong> — Some queries have a very large relevant response data set</li><li id="48eb" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Data freshness vs quality</strong> — Which snippets of the response data set have the most high quality content for the AI to use vs which snippets are most relevant from a time (freshness) perspective?</li><li id="ca8f" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Data variety</strong> — How do we ensure a variety of search results such that the AI is properly informed?</li><li id="8b43" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Query phrasing and ambiguity</strong> — The prompt that triggered the RAG flow, might not be phrased in such a way that it yields the optimal result, or might even be ambiguous</li><li id="cd32" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Response Personalization</strong> — The query might require a different response based on who asks it</li></ul><p id="58c5" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">This list goes on, but you get the gist.</p><h1 id="ef9c" class="pe mk fq bf ml pf pg gq mp ph pi gt mt pj pk pl pm pn po pp pq pr ps pt pu pv bk">Sidebar: Don’t unlimited context windows solve this?</h1><p id="f394" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Short answer: no.</p><p id="a071" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">The cost and performance impact of using extremely large context windows shouldn’t be underestimated (you easily 10x or 100x your per query cost), not including any follow up interaction that the user/system has.</p><p id="72ce" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">However, putting that aside. Imagine the following situation.</p><blockquote class="px py pz"><p id="bbc1" class="nh ni on nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We put Anne in room with a piece of paper. The paper says: *patient Joe: complex foot fracture.* Now we ask Anne, does the patient have a foot fracture? Her answer is “yes, he does”.<br/><br/>Now we give Anne a hundred pages of medical history on Joe. Her answer becomes “well, depending on what time you are referring to, he had …”<br/><br/>Now we give Anne thousands of pages on all the patients in the clinic…</p></blockquote><p id="c48e" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">What you quickly notice, is that how we define the question (or the prompt in our case) starts to get very important. <strong class="nj fr">The larger the context window, the more nuance the query needs.</strong></p><p id="5bfc" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Additionally, <strong class="nj fr">the larger the context window,</strong> <strong class="nj fr">the universe of possible answers grows.</strong> This can be a positive thing, but in practice, it’s a method that invites lazy engineering behavior, and is likely to reduce the capabilities of your application if not handled intelligently.</p><h1 id="b0d2" class="pe mk fq bf ml pf pg gq mp ph pi gt mt pj pk pl pm pn po pp pq pr ps pt pu pv bk">Suggested approaches</h1><p id="5a4a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">As you scale a RAG system from POC to production, here’s how to address typical data challenges with specific solutions.</p><h2 id="09e3" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Duplication</h2><p id="5f2d" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Duplication is inevitable in multi-source systems. By using fingerprinting (hashing content), document IDs, or semantic hashing, you can identify exact duplicates at ingestion and prevent redundant content. However, consolidating metadata across duplicates can also be valuable; this lets users know that certain content appears in multiple sources, which can add credibility or highlight repetition in the dataset.</p><pre class="os ot ou ov ow qa qb qc bp qd bb bk"><span id="42ee" class="qe mk fq qb b bg qf qg l qh qi"># Fingerprinting for deduplication<br/>def fingerprint(doc_content):<br/>    return hashlib.md5(doc_content.encode()).hexdigest()<br/><br/># Store fingerprints and filter duplicates, while consolidating metadata<br/>fingerprints = {}<br/>unique_docs = []<br/>for doc in docs:<br/>    fp = fingerprint(doc['content'])<br/>    if fp not in fingerprints:<br/>        fingerprints[fp] = [doc]<br/>        unique_docs.append(doc)<br/>    else:<br/>        fingerprints[fp].append(doc)  # Consolidate sources</span></pre><h2 id="8dc1" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Near Duplication</h2><p id="7fb4" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Near-duplicate documents (similar but not identical) often contain important updates or small additions. Given that a minor change, like a status update, can carry critical information, freshness becomes crucial when filtering near duplicates. A practical approach is to use cosine similarity for initial detection, then retain the freshest version within each group of near-duplicates while flagging any meaningful updates.</p><pre class="os ot ou ov ow qa qb qc bp qd bb bk"><span id="3414" class="qe mk fq qb b bg qf qg l qh qi">from sklearn.metrics.pairwise import cosine_similarity<br/>from sklearn.cluster import DBSCAN<br/>import numpy as np<br/><br/># Cluster embeddings with DBSCAN to find near duplicates<br/>clustering = DBSCAN(eps=0.1, min_samples=2, metric="cosine").fit(doc_embeddings)<br/><br/># Organize documents by cluster label<br/>clustered_docs = {}<br/>for idx, label in enumerate(clustering.labels_):<br/> if label == -1:<br/> continue<br/> if label not in clustered_docs:<br/> clustered_docs[label] = []<br/> clustered_docs[label].append(docs[idx])<br/><br/># Filter clusters to retain only the freshest document in each cluster<br/>filtered_docs = []<br/>for cluster_docs in clustered_docs.values():<br/> # Choose the document with the most recent timestamp or highest relevance<br/> freshest_doc = max(cluster_docs, key=lambda d: d['timestamp'])<br/> filtered_docs.append(freshest_doc)</span></pre><h2 id="e608" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Volume</h2><p id="395e" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">When a query returns a high volume of relevant documents, effective handling is key. One approach is a **layered strategy**:</p><ul class=""><li id="13d9" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz of og oh bk"><strong class="nj fr">Theme Extraction</strong>: Preprocess documents to extract specific themes or summaries.</li><li id="2aa4" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Top-k Filtering</strong>: After synthesis, filter the summarized content based on relevance scores.</li><li id="a085" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Relevance Scoring</strong>: Use similarity metrics (e.g., BM25 or cosine similarity) to prioritize the top documents before retrieval.</li></ul><p id="d6e2" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">This approach reduces the workload by retrieving synthesized information that’s more manageable for the AI. Other strategies could involve batching documents by theme or pre-grouping summaries to further streamline retrieval.</p><h2 id="7d47" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Data Freshness vs. Quality</h2><p id="4cf9" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Balancing quality with freshness is essential, especially in fast-evolving datasets. Many scoring approaches are possible, but here’s a general tactic:</p><ul class=""><li id="b5e7" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz of og oh bk"><strong class="nj fr">Composite Scoring</strong>: Calculate a quality score using factors like source reliability, content depth, and user engagement.</li><li id="b5a3" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Recency Weighting</strong>: Adjust the score with a timestamp weight to emphasize freshness.</li><li id="2777" class="nh ni fq nj b go oi nl nm gr oj no np mu ok nr ns my ol nu nv nc om nx ny nz of og oh bk"><strong class="nj fr">Filter by Threshold</strong>: Only documents meeting a combined quality and recency threshold proceed to retrieval.</li></ul><p id="fd82" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Other strategies could involve scoring only high-quality sources or applying decay factors to older documents.</p><h2 id="249f" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Data Variety</h2><p id="c970" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Ensuring diverse data sources in retrieval helps create a balanced response. Grouping documents by source (e.g., different databases, authors, or content types) and selecting top snippets from each source is one effective method. Other approaches include scoring by unique perspectives or applying diversity constraints to avoid over-reliance on any single document or perspective.</p><pre class="os ot ou ov ow qa qb qc bp qd bb bk"><span id="15c9" class="qe mk fq qb b bg qf qg l qh qi"># Ensure variety by grouping and selecting top snippets per source<br/><br/>from itertools import groupby<br/><br/>k = 3  # Number of top snippets per source<br/>docs = sorted(docs, key=lambda d: d['source'])<br/><br/>grouped_docs = {key: list(group)[:k] for key, group in groupby(docs, key=lambda d: d['source'])}<br/>diverse_docs = [doc for docs in grouped_docs.values() for doc in docs]</span></pre><h2 id="2b5b" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Query Phrasing and Ambiguity</h2><p id="b3f2" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Ambiguous queries can lead to suboptimal retrieval results. Using the exact user prompt is mostly not be the best way to retrieve the results they require. E.g. there might have been an information exchange earlier on in the chat which is relevant. Or the user pasted a large amount of text with a question about it.</p><p id="d261" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">To ensure that you use a refined query, one approach is to ensure that a RAG tool provided to the model asks it to rephrase the question into a more detailed search query, similar to how one might carefully craft a search query for Google. This approach improves alignment between the user’s intent and the RAG retrieval process. The phrasing below is suboptimal, but it provides the gist of it:</p><pre class="os ot ou ov ow qa qb qc bp qd bb bk"><span id="7e54" class="qe mk fq qb b bg qf qg l qh qi">tools = [{ <br/>  "name": "search_our_database", <br/>  "description": "Search our internal company database for relevant documents", <br/>  "parameters": { <br/>   "type": "object", <br/>   "properties": { <br/>    "query": { <br/>     "type": "string", <br/>     "description": "A search query, like you would for a google search, in sentence form. Take care to provide any important nuance to the question." <br/>    } <br/>   }, <br/>   "required": ["query"] <br/>  } <br/>}]</span></pre><h2 id="b779" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Response Personalization</h2><p id="7f62" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">For tailored responses, integrate user-specific context directly into the RAG context composition. By adding a user-specific layer to the final context, you allow the AI to take into account individual preferences, permissions, or history without altering the core retrieval process.</p></div></div></div><div class="ab cb qj qk ql qm" role="separator"><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp qq"/><span class="qn by bm qo qp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9bdd" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">By addressing these data challenges, your RAG system can evolve from a compelling POC into a reliable production-grade solution. Ultimately, the effectiveness of RAG relies more on careful engineering than on the AI model itself. While AI can generate fluent answers, the real magic lies in how well we retrieve and structure information. So the next time you’re impressed by an AI system’s conversational abilities, remember that it’s likely the result of an expertly designed retrieval process working behind the scenes.</p><p id="156e" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">I hope this article provided you some insight into the RAG process, and why the magic that you experience when talking to your data isn’t necessarily coming from the AI model, but is largely dependent on the design of your retrieval process.</p><p id="6949" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Please comment with your thoughts.</p></div></div></div></div>    
</body>
</html>