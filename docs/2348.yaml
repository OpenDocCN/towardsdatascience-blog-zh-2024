- en: LLM Fine-Tuning — FAQs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26](https://towardsdatascience.com/llm-fine-tuning-faqs-200442827c99?source=collection_archive---------8-----------------------#2024-09-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Answering the most common questions I received as an AI consultant
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--200442827c99--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--200442827c99--------------------------------)
    ·7 min read·Sep 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Last year, I posted an article on [fine-tuning large language models (LLMs)](/fine-tuning-large-language-models-llms-23473d763b91).
    To my surprise, this turned out to be one of my most-read blogs ever, and it led
    to dozens of conversations with clients about their fine-tuning questions and
    AI projects. Here, I will summarize these conversations' most frequently asked
    questions and my responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eca379049c372187ac5c9bd345ce5417.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Canva.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is Fine-tuning?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I like to define [**fine-tuning**](/fine-tuning-large-language-models-llms-23473d763b91)
    as **taking an existing (pre-trained) model and training at least 1 model parameter
    to adapt it to a particular use case**.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note the “*training at least 1 model parameter*” part of the
    definition. Some will define fine-tuning without this nuance (including me at
    times). However, this distinguishes fine-tuning from approaches like prompt engineering
    or prefix-tuning, which adapt a model’s behavior without modifying its internal
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
    [## Fine-Tuning Large Language Models (LLMs)'
  prefs: []
  type: TYPE_NORMAL
- en: A conceptual overview with example Python code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/fine-tuning-large-language-models-llms-23473d763b91?source=post_page-----200442827c99--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**When NOT to Fine-tune**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
