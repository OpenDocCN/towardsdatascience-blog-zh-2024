- en: Apache Hadoop and Apache Spark for Big Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08](https://towardsdatascience.com/apache-hadoop-and-apache-spark-for-big-data-analysis-daaf659fd0ee?source=collection_archive---------5-----------------------#2024-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A complete guide to big data analysis using Apache Hadoop (HDFS) and PySpark
    library in Python on game reviews on the Steam gaming platform.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[![Rindhuja
    Treesa Johnson](../Images/15d2bfb802395968ea23faff62cc623a.png)](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    [Rindhuja Treesa Johnson](https://medium.com/@rindhuj1?source=post_page---byline--daaf659fd0ee--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--daaf659fd0ee--------------------------------)
    ·14 min read·May 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: With over 100 zettabytes (= 10¹²GB) of [data produced every year](https://www.statista.com/statistics/871513/worldwide-data-created/)
    around the world, the significance of handling big data is one of the most required
    skills today. Data Analysis, itself, could be defined as the ability to handle
    big data and derive insights from the never-ending and exponentially growing data.
    Apache Hadoop and Apache Spark are two of the basic tools that help us untangle
    the limitless possibilities hidden in large datasets. [Apache Hadoop](https://hadoop.apache.org/)
    enables us to streamline data storage and distributed computing with its Distributed
    File System (HDFS) and the MapReduce-based parallel processing of data. [Apache
    Spark](https://spark.apache.org/) is a big data analytics engine capable of EDA,
    SQL analytics, Streaming, Machine Learning, and Graph processing compatible with
    the major programming languages through its APIs. Both when combined form an exceptional
    environment for dealing with big data with the available computational resources
    — just a personal computer in most cases!
  prefs: []
  type: TYPE_NORMAL
- en: Let us unfold the power of [Big Data and Apache Hadoop](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722)
    with a simple analysis project implemented using Apache Spark in Python.
  prefs: []
  type: TYPE_NORMAL
- en: To begin with, let’s dive into the installation of Hadoop Distributed File System
    and Apache Spark on a MacOS. I am using a MacBook Air with macOS Sonoma with an
    M1 chip.
  prefs: []
  type: TYPE_NORMAL
- en: '**Jump to the section —**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Installing Hadoop Distributed File System](#b292)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Installing Apache Spark](#2773)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Steam Review Analysis using PySpark](#32ea)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[What next?](#2580)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. Installing Hadoop Distributed File System
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks to [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)
    for the amazing article that helped me with the [installation of Hadoop on my
    Mac](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed). I
    seamlessly installed and ran Hadoop following his steps which I will show you
    here as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**a. Installing HomeBrew**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I use [Homebrew](https://brew.sh/) for installing applications on my Mac for
    ease. It can be directly installed on the system with the below code —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once it is installed, you can run the simple code below to verify the installation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0f0f02df5fc0a74936bb9fba42d896e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: However, you will likely encounter an error saying, `command not found`, this
    is because the homebrew will be installed in a different location (Figure 2) and
    it is not executable from the current directory. For it to function, we add a
    path environment variable for the brew, i.e., adding homebrew to the .bash_profile.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/594e71aa1c71a70fe1615dba45a9b210.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: You can avoid this step by using the full path to Homebrew in your commands,
    however, it might become a hustle at later stages, so not recommended!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, when you try, `brew --version`, it should show the Homebrew version correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '**b. Installing Hadoop**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Disclaimer! Hadoop is a Java-based application and is supported by a Java Development
    Kit (JDK) version older than 11, preferably 8 or 11\. Install JDK before continuing.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Thanks to [Code With Arjun](https://medium.com/u/bdd6bb8fb77f?source=post_page---user_mention--daaf659fd0ee--------------------------------)
    again for this video on JDK installation on MacBook M1.
  prefs: []
  type: TYPE_NORMAL
- en: Guide to Installing JDK
  prefs: []
  type: TYPE_NORMAL
- en: Now, we install the Hadoop on our system using the brew command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This command should install Hadoop seamlessly. Similar to the steps followed
    while installing HomeBrew, we should edit the path environment variable for Java
    in the Hadoop folder. The environment variable settings for the installed version
    of Hadoop can be found in the Hadoop folder within HomeBrew. You can use `which
    hadoop` command to find the location of the Hadoop installation folder. Once you
    locate the folder, you can find the variable settings at the below location. The
    below command takes you to the required folder for editing the variable settings
    (Check the Hadoop version you installed to avoid errors).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can view the files in this folder using the `ls` command. We will edit the
    `hadoop-env.sh` to enable the proper running of Hadoop on the system.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9433c954e59f2cbaaf96c35b2a1fcac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have to find the path variable for Java to edit the `hadoop-ev.sh` file
    using the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e3082e79171b59a742c2462151b8efa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: We can open the `hadoop-env.sh` file in any text editor. I used VI editor, you
    can use any editor for the purpose. We can copy and paste the path — `Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home`
    at the `export JAVA_HOME =` position.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bbeff2e0db5eba54d035987932f6209a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: hadoop-env.sh file opened in VI Text Editor'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we edit the four XML files in the Hadoop folder.
  prefs: []
  type: TYPE_NORMAL
- en: '`core-site.xml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`hdfs-site.xml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`mapred-site.xml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`yarn-site.xml`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With this, we have successfully completed the installation and configuration
    of HDFS on the local. To make the data on Hadoop accessible with Remote login,
    we can go to Sharing in the General settings and enable `Remote Login`. You can
    edit the user access by clicking on the info icon.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b09bae4523e0679b1275c841247e5aff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Enable Remote Access. Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s run Hadoop!
  prefs: []
  type: TYPE_NORMAL
- en: Execute the following commands
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fb2fa78c2639a628f5ee27da07acfa27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Initiating Hadoop and viewing the nodes and resources running. Image
    by Author'
  prefs: []
  type: TYPE_NORMAL
- en: We are all set! Now let’s create a directory in HDFS and add the data will be
    working on. Let’s quickly take a look at our data source and details.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The [Steam Reviews Dataset 2021](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021)
    ***(***[***License: GPL 2***](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html)***)***
    is a collection of reviews from about 21 million gamers covering over 300 different
    games in the year 2021\. the data is extracted using Steam’s API — [Steamworks](https://partner.steamgames.com/doc/store/getreviews)
    — using the Get List function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The dataset consists of 23 columns and 21.7 million rows with a size of 8.17
    GB (that is big!). The data consists of reviews in different languages and a boolean
    column that tells if the player recommends the game to other players. We will
    be focusing on how to handle this big data locally using HDFS and analyze it using
    Apache Spark in Python using the PySpark library.
  prefs: []
  type: TYPE_NORMAL
- en: '**c. Uploading Data into HDFS**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Firstly, we create a directory in the HDFS using the `mkdir` command. *It will
    throw an error if we try to add a file directly to a non-existing folder.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will add the data file to the folder `steam_analysis` using the `put`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Apache Hadoop also uses a user interface available at [http://localhost:9870/](http://localhost:9870/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a158c45d8a4f833962c62d1c16d19215.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: HDFS User Interface at localhost:9870\. Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the uploaded files as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b57932add5847d2cf0d9f63a7bfad1cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Navigating files in HDFS. Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Once the data interaction is over, we can use `stop-all.sh` command to stop
    all the Apache Hadoop daemons.
  prefs: []
  type: TYPE_NORMAL
- en: Let us move to the next step — Installing Apache Spark
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Installing Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Hadoop takes care of data storage (HDFS) and parallel processing (MapReduce)
    of the data for faster execution. [Apache Spark](https://spark.apache.org/) is
    a multi-language compatible analytical engine designed to deal with big data analysis.
    We will run the Apache Spark on Python in Jupyter IDE.
  prefs: []
  type: TYPE_NORMAL
- en: After installing and running HDFS, the installation of Apache Spark for Python
    is a piece of cake. PySpark is the Python API for Apache Spark that can be installed
    using the `pip` method in the Jupyter Notebook. PySpark is the Spark Core API
    with its four components — Spark SQL, Spark ML Library, Spark Streaming, and GraphX.
    Moreover, we can access the Hadoop files through PySpark by initializing the installation
    with the required Hadoop version.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let’s get started with the Big Data Analytics!
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Steam Review Analysis using PySpark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Steam](https://store.steampowered.com/about/) is an online gaming platform
    that hosts over 30,000 games streaming across the world with over 100 million
    players. Besides gaming, the platform allows the players to provide reviews for
    the games they play, a great resource for the platform to improve customer experience
    and for the gaming companies to work on to keep the players on edge. We used this
    review data provided by the platform publicly available on [Kaggle](https://www.kaggle.com/datasets/najzeko/steam-reviews-2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. a. Data Extraction from HDFS**'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the PySpark library to access, clean, and analyze the data. To start,
    we connect the PySpark session to Hadoop using the local host address.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. b. Data Cleaning and Pre-Processing**'
  prefs: []
  type: TYPE_NORMAL
- en: We can start by taking a look at the dataset. Similar to the pandas.head() function
    in Pandas, PySpark has the SparkSession.show() function that gives a glimpse of
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we will remove the reviews column in the dataset as we do not plan
    on performing any NLP on the dataset. Also, the reviews are in different languages
    making any sentiment analysis based on the review difficult.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/52636915bc9dc3c68b59c2d3238eeb54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The Structure of the Schema'
  prefs: []
  type: TYPE_NORMAL
- en: We have a huge dataset with us with 23 attributes with NULL values for different
    attributes which does not make sense to consider any imputation. Therefore, I
    have removed the records with NULL values. However, this is not a recommended
    approach. You can evaluate the importance of the available attributes and remove
    the irrelevant ones, then try imputing data points to the NULL values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We still have almost 17 million records in the dataset!
  prefs: []
  type: TYPE_NORMAL
- en: Now, we focus on the variable names of the dataset as in Figure 11\. We can
    see that the attributes have a few characters like dot(.) that are unacceptable
    as Python identifiers. Also, we change the data type of the date and time attributes.
    So we change these using the following code —
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a42764009ddbe5123c2a75a7f5cf4e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A glimpse of the Steam review Analysis dataset. Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is clean and ready for analysis!
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. c. Exploratory Data Analysis**'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is rich in information with over 20 variables. We can analyze the
    data from different perspectives. Therefore, we will be splitting the data into
    different PySpark data frames and caching them to run the analysis faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '***i. Games Analysis***'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will try to understand the review and recommendation patterns
    for different games. We will consider the *number of reviews* analogous to the
    popularity of the game and the *number of* ***True*** *recommendations* analogous
    to the gamer’s preference for the game.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Most Popular Games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Finding the Most Recommended Games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b62fe5c6d97a4793d51ff4f7c653c876.png)![](../Images/405b141436af05e056a8c5953a809287.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Shows the pie charts for popular and recommended games. Images by
    Author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Insights***'
  prefs: []
  type: TYPE_NORMAL
- en: Player Unknown’s Battlegrounds (PUBG) is the most popular and most recommended
    game of 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the second positions for the two categories are held by Grand Theft
    Auto V (GTA V) and Stardew Valley respectively. This shows that being popular
    does not mean all the players recommend the game to another player.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same pattern is observed with other games also. However, the number of reviews
    for a game significantly affects this trend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***ii. Demographic Analysis***'
  prefs: []
  type: TYPE_NORMAL
- en: We will find the demography, especially, the locality of the gamers using the
    `data_demo` data frame. This analysis will help us understand the popular languages
    used for review and languages used by reviewers of popular games. We can use this
    trend to determine the demographic influence and sentiments of the players to
    be used for recommending new games in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Popular Review Languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Finding Review Languages of Popular Games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/45704b3736bfb990e3200b620c4af498.png)![](../Images/da796a9979022ac0928317fb693f394f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Language Popularity; Language Popularity among Popular games. Images
    by Author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Insights***'
  prefs: []
  type: TYPE_NORMAL
- en: English is the most popular language used by reviewers followed by Schinese
    and Russian
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schinese is the most widely used language for the most popular game (PUBG),
    whereas, English is widely used for the second most popular game (GTA V) and almost
    all others!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The popularity of a game seems to have roots in the area of origin. [PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html)
    is a product of a South Korean gaming company and we observe that it has the Korean
    language among one of the highly used.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time, author, and review analyses are also performed on this data, however,
    do not give any actionable insights. Feel free to visit the [GitHub repository
    for the full project](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark)
    documentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. d. Game Recommendation using Spark ML Library**'
  prefs: []
  type: TYPE_NORMAL
- en: We have reached the last stage of this project, where we will implement the
    [Alternating Least Squares (ALS)](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)
    machine-learning algorithm from the Spark ML Library. This model utilizes the
    collaborative filtering technique to recommend games based on player’s behavior,
    i.e., the games they played before. This algorithm identifies the game selection
    pattern for players who play each available game on the Steam App.
  prefs: []
  type: TYPE_NORMAL
- en: For the algorithm to work,
  prefs: []
  type: TYPE_NORMAL
- en: We require three variables — the independent variable, target variable(s) —
    depending on the number of recommendations, here 5, and a rating variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We encode the games and the authors to make the computation easier. We also
    convert the `boolean`recommended column into a rating column with ***True = 5,
    and False = 1.***
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, we will be recommending 5 new games for each played game and therefore
    we consider the data of the players who have played more than five for modeling
    the algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s jump to the modeling and recommending part!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/84c72ca4d73022d09883d5a231ebc3bb.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c613673f7ba9009d76ebd17d9d48ec25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The game list with the corresponding index for reference. Image
    by Author'
  prefs: []
  type: TYPE_NORMAL
- en: '***Implementing ALS Algorithm***'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3362790f58d8e6be83a3fdd37556267f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The recommendation and rating generated for each author based on
    their gaming history. Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: We can cross-match the indices from Figure 16 to find the games recommended
    for each player. Thus, we implemented a basic recommendation system using the
    Spark Core ML Library.
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. e. Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we could successfully implement the following —
  prefs: []
  type: TYPE_NORMAL
- en: Download and install the Hadoop ecosystem — HDFS and MapReduce — to store, access,
    and extract big data efficiently, and implement big data analytics much faster
    using a personal computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the Apache Spark API for Python (PySpark) and integrate it with the
    Hadoop ecosystem, enabling us to carry out big data analytics and some machine-learning
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The games and demographic analysis gave us some insights that can be used to
    improve the gaming experience and control the player churn. Keeping the players
    updated and informed about the trends in their peers should be a priority for
    the Steam platform. Suggestions like “most played”, “most played in your region”,
    “most recommended”, and “don’t miss out on these new games” can keep the players
    active.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Steam Application can use the ALS recommendation system to recommend new
    games to existing players based on their profile and keep them engaged and afresh.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. What Next?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implement Natural Language Processing techniques in the review column, for different
    languages to extract the essence of the reviews and improve the gaming experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steam can report bugs in the games based on the reviews. Developing an AI algorithm
    that captures the review content, categorizes it, and sends it to appropriate
    personnel could do wonders for the platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comment what you think can be done more!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5\. References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Hadoop. [Apache Hadoop](https://hadoop.apache.org/)*. Apache Hadoop*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statista. (2021). [Volume of data/information created, captured, copied, and
    consumed worldwide from 2010 to 2020, with forecasts from 2021 to 2025](https://www.statista.com/statistics/871513/worldwide-data-created/)
    *statista*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dey, R. (2023). [A Beginner’s Guide to Big Data and Hadoop Distributed File
    System (HDFS)](https://medium.com/@roshmitadey/a-beginners-guide-to-big-data-and-hadoop-distributed-file-system-hdfs-b5c324d3c722).
    *Medium*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code with Arjun (2021). [Install Hadoop on Mac OS (MacBook M1)](https://codewitharjun.medium.com/install-hadoop-on-macos-efe7c860c3ed).
    *Medium*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark. [PySpark Installation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).
    *Apache Spark*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark. [Collaborative Filtering with ALS)](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html).
    *Apache Spark*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s Uncover it. (2023). [PUBG](https://letsuncoverhistory.blogspot.com/2023/06/History-and-The-Future-of-PubG.html).
    *Let’s Uncover It*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the complete big data analysis project in my [GitHub repository](https://github.com/Rindhujatreesa/Big_Data_Processing_Projects/tree/main/Steam_Review_Analysis_with_HDFS_%26_PySpark).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s connect on [LinkedIn](https://www.linkedin.com/in/rindhuja-johnson/) and
    discuss more!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you found this article useful, clap, share, and comment!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
