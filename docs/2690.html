<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Least Squares Regression, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Least Squares Regression, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05">https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="51f3" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">REGRESSION ALGORITHM</h2><div/><div><h2 id="9e0b" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Gliding through points to minimize squares</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ii lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3cff" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">When people start learning about data analysis, they usually begin with linear regression. There’s a good reason for this — it’s one of the most useful and straightforward ways to understand how regression works. The most common approaches to linear regression are called “Least Squares Methods” — these work by finding patterns in data by minimizing the squared differences between predictions and actual values. The most basic type is <strong class="mp ga">Ordinary Least Squares</strong> (OLS), which finds the best way to draw a straight line through your data points.</p><p id="7310" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Sometimes, though, OLS isn’t enough — especially when your data has many related features that can make the results unstable. That’s where <strong class="mp ga">Ridge regression</strong> comes in. Ridge regression does the same job as OLS but adds a special control that helps prevent the model from becoming too sensitive to any single feature.</p><p id="1d00" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Here, we’ll glide through two key types of Least Squares regression, exploring how these algorithms smoothly slide through your data points and see their differences in theory.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/e13851516ec1dfc98b92fd91fe5661c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cc1uSMR8mhDbQ7fu3qNKgQ.gif"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="8f53" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">Definition</h1><p id="39a4" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Linear Regression is a statistical method that predicts numerical values using a linear equation. It models the relationship between a dependent variable and one or more independent variables by fitting a straight line (or plane, in multiple dimensions) through the data points. The model calculates coefficients for each feature, representing their impact on the outcome. To get a result, you input your data’s feature values into the linear equation to compute the predicted value.</p><h1 id="72ff" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">📊 Dataset Used</h1><p id="f572" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">To illustrate our concepts, we’ll use <a class="af pd" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">our standard dataset</a> that predicts the number of golfers visiting on a given day. This dataset includes variables like weather outlook, temperature, humidity, and wind conditions.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/8f70777fcdcfbe631c352c17a649d816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSbHIOe7ZL6BF1uwPTCOZQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Columns: ‘Outlook’ (one-hot encoded to sunny, overcast, rain), ‘Temperature’ (in Fahrenheit), ‘Humidity’ (in %), ‘Wind’ (Yes/No) and ‘Number of Players’ (numerical, target feature)</figcaption></figure><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="7398" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'],prefix='',prefix_sep='')<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data into features and target, then into training and test sets<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)</span></pre><p id="3e93" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">While it is not mandatory, to effectively use Linear Regression — including Ridge Regression — we can standardize the numerical features first.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/856bb5024939a0acbe57928f62346faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFZZ90PWRFqs2YVlYqie_w.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Standard scaling is applied to ‘Temperature’ and ‘Humidity’ while the one-hot encoding is applied to ‘Outlook’ and ‘Wind’</figcaption></figure><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="cb52" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'])<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/><br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)</span></pre><h1 id="ccaa" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">Main Mechanism</h1><p id="3c47" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Linear Regression predicts numbers by making a straight line (or hyperplane) from the data:</p><ol class=""><li id="dff0" class="mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni po pp pq bk">The model finds the best line by making the gaps between the real values and the line’s predicted values as small as possible. This is called “least squares.”</li><li id="365f" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">Each input gets a number (coefficient/weight) that shows how much it changes the final answer. There’s also a starting number (intercept/bias) that’s used when all inputs are zero.</li><li id="a857" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">To predict a new answer, the model takes each input, multiplies it by its number, adds all these up, plus adds the starting number. This gives you the predicted answer.</li></ol><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/c72b0e92063ef36ac8fb47688848a377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycpFoYTIl7Aan2UGSAK6fQ.png"/></div></div></figure><h1 id="89da" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk"><strong class="al">Ordinary Least Squares (OLS) Regression</strong></h1><p id="fe23" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Let’s start with Ordinary Least Squares (OLS) — the fundamental approach to linear regression. The goal of OLS is to find the best-fitting line through our data points. We do this by measuring how “wrong” our predictions are compared to actual values, and then finding the line that makes these errors as small as possible. When we say “error,” we mean the vertical distance between each point and our line — in other words, how far off our predictions are from reality. Let’s see what happened in 2D case first.</p><h2 id="c866" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">In 2D Case</h2><p id="1c85" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">In 2D case, we can imagine the linear regression algorithm like this:</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/6c8ce9540dd0d3e926482943741c9ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kciC_4-ZxFYMSO-6-yksmA.png"/></div></div></figure><p id="b634" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Here’s the explanation of the process above:</p><p id="c629" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">1.We start with a training set, where each row has:<br/>· <em class="qm">x </em>: our input feature (the numbers 1, 2, 3, 1, 2)<br/>· <em class="qm">y </em>: our target values (0, 1, 1, 2, 3)</p><p id="5add" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">2. We can plot these points on a scatter plot and we want to find a line <em class="qm">y</em> = <em class="qm">β</em>₀ + <em class="qm">β</em>₁<em class="qm">x</em> that best fits these points</p><p id="7836" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">3. For any given line (any <em class="qm">β</em>₀ and <em class="qm">β</em>₁), we can measure how good it is by:<br/>· Calculating the vertical distance (<em class="qm">d</em>₁, <em class="qm">d</em>₂,<em class="qm"> d</em>₃, <em class="qm">d</em>₄, <em class="qm">d</em>₅) from each point to the line<br/>· These distances are |<em class="qm">y</em> — (<em class="qm">β</em>₀ + <em class="qm">β</em>₁<em class="qm">x</em>)| for each point</p><p id="27f5" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">4. Our optimization goal is to find <em class="qm">β</em>₀ and<em class="qm"> β</em>₁ that minimize the sum of squared distances: <em class="qm">d</em>₁² + <em class="qm">d</em>₂² + <em class="qm">d</em>₃² + <em class="qm">d</em>₄² + <em class="qm">d</em>₅². In vector notation, this is written as ||<em class="qm">y </em>— <em class="qm">Xβ</em>||², where <em class="qm">X</em> = [1 <em class="qm">x</em>] contains our input data (with 1’s for the intercept) and <em class="qm">β</em> = [<em class="qm">β</em>₀ <em class="qm">β</em>₁]ᵀ contains our coefficients.</p><p id="ad18" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">5. The optimal solution has a closed form: <em class="qm">β</em> = (<em class="qm">X</em>ᵀ<em class="qm">X</em>)⁻¹<em class="qm">X</em>ᵀy. Calculating this we get<em class="qm"> β</em>₀ = -0.196 (intercept), <em class="qm">β</em>₁ = 0.761 (slope).</p><p id="908d" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This vector notation makes the formula more compact and shows that we’re really working with matrices and vectors rather than individual points. We will see more details of our calculation next in the multidimensional case.</p><h2 id="3162" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">In Multidimensional Case (📊 Dataset)</h2><p id="2f5a" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Again, the goal of OLS is to find coefficients (<em class="qm">β</em>) that minimize the squared differences between our predictions and actual values. Mathematically, we express this as <strong class="mp ga">minimizing</strong> ||<em class="qm">y</em> — <em class="qm">Xβ</em>||², where <em class="qm">X</em> is our data matrix and <em class="qm">y</em> contains our target values.</p><p id="b41b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The training process follows these key steps:</p><h2 id="2ce7" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Training Step</h2><p id="f292" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">1. Prepare our data matrix <em class="qm">X</em>. This involves adding a column of ones to account for the bias/intercept term (<em class="qm">β</em>₀).</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qn"><img src="../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*if-7FlRvuq0LyHAz8L43CA.png"/></div></div></figure><p id="0e42" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">2. Instead of iteratively searching for the best coefficients, we can compute them directly using the normal equation:<br/><em class="qm">β</em> = (<em class="qm">X</em>ᵀ<em class="qm">X</em>)⁻¹<em class="qm">X</em>ᵀ<em class="qm">y</em></p><p id="93a4" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">where:<br/>· <em class="qm">β </em>is the vector of estimated coefficients,<br/>· <em class="qm">X</em> is the dataset matrix(including a column for the intercept),<br/>· <em class="qm">y</em> is the label,<br/>· <em class="qm">X</em>ᵀ represents the transpose of matrix <em class="qm">X</em>,<br/>· ⁻¹ represents the inverse of the matrix.</p><p id="ff25" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s break this down:</p><p id="b1a0" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">a. We multiply <em class="qm">X</em>ᵀ (<em class="qm">X</em> transpose) by <em class="qm">X</em>, giving us a square matrix</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/41c746d7c63229c8463c4b4451d547c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSBkGGPrwi-yO377JihvRg.png"/></div></div></figure><p id="9e15" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">b. We compute the inverse of this matrix</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qo"><img src="../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TB6Z0EhByi2p0nqUcfDTSw.png"/></div></div></figure><p id="4b2f" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">c. We compute <em class="qm">X</em>ᵀ<em class="qm">y</em></p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qp"><img src="../Images/56478c8b91a65d73a45096748d575778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqljD0Lxqo9bZ2JrFzmB3w.png"/></div></div></figure><p id="9df2" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">d. We multiply (<em class="qm">X</em>ᵀ<em class="qm">X</em>)⁻¹ and <em class="qm">X</em>ᵀ<em class="qm">y </em>to get our coefficients</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qq"><img src="../Images/963c4e9a98a88c168f9c90739b1fc1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYYLpFPJZxxMqgUozAiJIg.png"/></div></div></figure><h2 id="0cbf" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Test Step</h2><p id="a1ed" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Once we have our coefficients, making predictions is straightforward: we simply multiply our new data point by these coefficients to get our prediction.</p><p id="e0a8" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In matrix notation, for a new data point <em class="qm">x</em>*, the prediction <em class="qm">y</em>* is calculated as <br/><em class="qm">y</em>* = <em class="qm">x</em>*<em class="qm">β</em> = [1, x₁, x₂, …, xₚ] × [β₀, β₁, β₂, …, βₚ]ᵀ, <br/>where <em class="qm">β</em>₀ is the intercept and <em class="qm">β</em>₁ through <em class="qm">β</em>ₚ are the coefficients for each feature.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/fc075c4c8801b373f49a757ac742fef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCzgdei9vOr0tJ0BZNVguQ.png"/></div></div></figure><h2 id="e039" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Evaluation Step</h2><p id="9732" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">We can do the same process for all data points. For our dataset, here’s the final result with the RMSE as well.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/c0f9605d793360d6c403abdb3192e0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lzf6VMZOU_PBQe7J1LAudA.png"/></div></div></figure><h1 id="807f" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk"><strong class="al">Ridge Regression</strong></h1><p id="abec" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Now, let’s consider Ridge Regression, which builds upon OLS by addressing some of its limitations. The key insight of Ridge Regression is that sometimes the optimal OLS solution <strong class="mp ga">involves very large coefficients</strong>, which can lead to overfitting.</p><p id="d554" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Ridge Regression adds a penalty term (<em class="qm">λ</em>||<em class="qm">β</em>||²) to the objective function. This term discourages large coefficients by adding their squared values to what we’re minimizing. The full objective becomes:</p><p id="739b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">min ||<em class="qm">y</em> — <em class="qm">X</em>β||² + λ||<em class="qm">β</em>||²</p><p id="5376" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The <em class="qm">λ</em> (lambda) parameter controls how much we penalize large coefficients. When <em class="qm">λ</em> = 0, we get OLS; as<em class="qm"> λ</em> increases, the coefficients shrink toward zero (but never quite reach it).</p><h2 id="da91" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Training Step</h2><ol class=""><li id="5757" class="mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni po pp pq bk">Just like OLS, prepare our data matrix <em class="qm">X</em>. This involves adding a column of ones to account for the intercept term (<em class="qm">β</em>₀).</li><li id="7b05" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">The training process for Ridge follows a similar pattern to OLS, but with a modification. The closed-form solution becomes:<br/><em class="qm">β</em> = (<em class="qm">X</em>ᵀ<em class="qm">X</em>+ λI)⁻¹<em class="qm">X</em>ᵀ<em class="qm">y</em></li></ol><p id="d8de" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">where:<br/>· <em class="qm">I</em> is the identity matrix (with the first element, corresponding to <em class="qm">β</em>₀, sometimes set to 0 to exclude the intercept from regularization in some implementations),<br/>· λ is the regularization value.<br/>· <em class="qm">Y</em> is the vector of observed dependent variable values.<br/>· Other symbols remain as defined in the OLS section.</p><p id="e5d3" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s break this down:</p><p id="cebb" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">a. We add <em class="qm">λ</em>I to <em class="qm">X</em>ᵀ<em class="qm">X. </em>The value of <em class="qm">λ </em>can be any positive number (say 0.1).</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qr"><img src="../Images/1f631555a5c3da55dcc0e5c818fdc266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byo0j5v6j2Ch4UicKJOsPQ.png"/></div></div></figure><p id="83c4" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">b. We compute the inverse of this matrix. The benefits of adding λI to <em class="qm">X</em>ᵀ<em class="qm">X</em> before inversion are:<br/>· Makes the matrix invertible, even if <em class="qm">X</em>ᵀ<em class="qm">X</em> isn’t (solving a key numerical problem with OLS)<br/>· Shrinks the coefficients proportionally to <em class="qm">λ</em></p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qs"><img src="../Images/41521c73d61c93a7d2f3d061ed957b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6SKc3lY7TJzqVCNO-kJnQ.png"/></div></div></figure><p id="305b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">c. We multiply (<em class="qm">X</em>ᵀ<em class="qm">X</em>+ <em class="qm">λI</em>)⁻¹ and <em class="qm">X</em>ᵀ<em class="qm">y </em>to get our coefficients</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qt"><img src="../Images/e4ed2341b5407401f67dd250d1b98f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dapWwY3uzL7_8cpkEqgjw.png"/></div></div></figure><h2 id="0e05" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk"><strong class="al">Test Step</strong></h2><p id="8877" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">The prediction process remains the same as OLS — multiply new data points by the coefficients. The difference lies in the coefficients themselves, which are typically smaller and more stable than their OLS counterparts.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/918f8768c3d930ee52fc25e57f1c38bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qh985VeQj7xrRftIkhGuGQ.png"/></div></div></figure><h2 id="fe75" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Evaluation Step</h2><p id="f9f0" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">We can do the same process for all data points. For our dataset, here’s the final result with the RMSE as well.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/d89851ba086f08012ca77a947dd7e06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mj3z3Zw9HMGKxbUF0VVVcQ.png"/></div></div></figure><h2 id="2227" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk"><strong class="al">Final Remarks: Choosing Between OLS and Ridge</strong></h2><p id="a65a" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">The choice between OLS and Ridge often depends on your data:</p><ul class=""><li id="7eaa" class="mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni qu pp pq bk">Use OLS when you have well-behaved data with little multicollinearity and enough samples (relative to features)</li><li id="1f70" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni qu pp pq bk">Use Ridge when you have:<br/>- Many features (relative to samples)<br/>- Multicollinearity in your features<br/>- Signs of overfitting with OLS</li></ul><p id="5ebe" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">With Ridge, you’ll need to choose <em class="qm">λ</em>. Start with a range of values (often logarithmically spaced) and choose the one that gives the best validation performance.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qv"><img src="../Images/0d0c60a41bac36424c7f233554495189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DBCP3wgCWh3BNpBjgg_lQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Apparantly, the default value <em class="qw">λ = 1 gives the best RMSE for our dataset.</em></figcaption></figure><h1 id="b312" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">🌟 OLS and Ridge Regression Code Summarized</h1><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="1ed7" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.linear_model import Ridge<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df = df[['sunny','overcast','rain','Temperature','Humidity','Wind','Num_Players']]<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/><br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)<br/><br/># Initialize and train the model<br/>#model = LinearRegression() # Option 1: OLS Regression<br/>model = Ridge(alpha=0.1)  # Option 2: Ridge Regression (alpha is the regularization strength, equivalent to λ)<br/><br/># Fit the model<br/>model.fit(X_train_scaled, y_train)<br/><br/># Make predictions<br/>y_pred = model.predict(X_test_scaled)<br/><br/># Calculate and print RMSE<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"RMSE: {rmse:.4f}")<br/><br/># Additional information about the model<br/>print("\nModel Coefficients:")<br/>print(f"Intercept    : {model.intercept_:.2f}")<br/>for feature, coef in zip(X_train_scaled.columns, model.coef_):<br/>    print(f"{feature:13}: {coef:.2f}")</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qx"><img src="../Images/ff9ff4226a31e1ad21380175a7da9bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhkLJJCIbYurDgydtzcerg.png"/></div></div></figure></div></div></div><div class="ab cb qy qz ra rb" role="separator"><span class="rc by bm rd re rf"/><span class="rc by bm rd re rf"/><span class="rc by bm rd re"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="806f" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Further Reading</h2><p id="9d91" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">For a detailed explanation of <a class="af pd" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">OLS Linear Regression</a> and <a class="af pd" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener ugc nofollow" target="_blank">Ridge Regression</a>, and its implementation in scikit-learn, readers can refer to their official documentation. It provides comprehensive information on their usage and parameters.</p><h2 id="d13a" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Technical Environment</h2><p id="909b" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="f399" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">About the Illustrations</h2><p id="0d40" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="fb1f" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙍𝙚𝙜𝙧𝙚𝙨𝙨𝙞𝙤𝙣 𝘼𝙡𝙜𝙤𝙧𝙞𝙩𝙝𝙢𝙨 𝙝𝙚𝙧𝙚:</p><div class="rg rh ri rj rk"><div role="button" tabindex="0" class="ab bx cp kj it rl rm bp rn lv ao"><div class="ro l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rp rq cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rp rq em n ay ub"/></div><div class="rr l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ru hp l"><h2 class="bf ga wh ic it wi iv iw wj iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wk wl wm wn wo lh wp wq um ii wr ws wt uq ur us ep bm ut ny" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wu l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sd dz se it ab sf il ed"><div class="ed rx bx ry rz"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This “dummy” doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rx bx kk sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sc sb"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:</p><div class="rg rh ri rj rk"><div role="button" tabindex="0" class="ab bx cp kj it rl rm bp rn lv ao"><div class="ro l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rp rq cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rp rq em n ay ub"/></div><div class="rr l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ru hp l"><h2 class="bf ga wh ic it wi iv iw wj iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wk wl wm wn wo lh wp wq um ii wr ws wt uq ur us ep bm ut ny" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wu l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sd dz se it ab sf il ed"><div class="ed rx bx ry rz"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rx bx kk sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sc sb"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>