<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Least Squares Regression, Explained: A Visual Guide with Code Examples for Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Least Squares Regression, Explained: A Visual Guide with Code Examples for Beginners</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05">https://towardsdatascience.com/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=collection_archive---------1-----------------------#2024-11-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="51f3" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">REGRESSION ALGORITHM</h2><div/><div><h2 id="9e0b" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Gliding through points to minimize squares</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--2e5ad011eae4--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">Â·</span></span></div><span data-testid="storyPublishDate">Nov 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ii lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3cff" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">When people start learning about data analysis, they usually begin with linear regression. Thereâ€™s a good reason for this â€” itâ€™s one of the most useful and straightforward ways to understand how regression works. The most common approaches to linear regression are called â€œLeast Squares Methodsâ€ â€” these work by finding patterns in data by minimizing the squared differences between predictions and actual values. The most basic type is <strong class="mp ga">Ordinary Least Squares</strong> (OLS), which finds the best way to draw a straight line through your data points.</p><p id="7310" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Sometimes, though, OLS isnâ€™t enough â€” especially when your data has many related features that can make the results unstable. Thatâ€™s where <strong class="mp ga">Ridge regression</strong> comes in. Ridge regression does the same job as OLS but adds a special control that helps prevent the model from becoming too sensitive to any single feature.</p><p id="1d00" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Here, weâ€™ll glide through two key types of Least Squares regression, exploring how these algorithms smoothly slide through your data points and see their differences in theory.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/e13851516ec1dfc98b92fd91fe5661c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cc1uSMR8mhDbQ7fu3qNKgQ.gif"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="8f53" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">Definition</h1><p id="39a4" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Linear Regression is a statistical method that predicts numerical values using a linear equation. It models the relationship between a dependent variable and one or more independent variables by fitting a straight line (or plane, in multiple dimensions) through the data points. The model calculates coefficients for each feature, representing their impact on the outcome. To get a result, you input your dataâ€™s feature values into the linear equation to compute the predicted value.</p><h1 id="72ff" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">ğŸ“Š Dataset Used</h1><p id="f572" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">To illustrate our concepts, weâ€™ll use <a class="af pd" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">our standard dataset</a> that predicts the number of golfers visiting on a given day. This dataset includes variables like weather outlook, temperature, humidity, and wind conditions.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/8f70777fcdcfbe631c352c17a649d816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSbHIOe7ZL6BF1uwPTCOZQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Columns: â€˜Outlookâ€™ (one-hot encoded to sunny, overcast, rain), â€˜Temperatureâ€™ (in Fahrenheit), â€˜Humidityâ€™ (in %), â€˜Windâ€™ (Yes/No) and â€˜Number of Playersâ€™ (numerical, target feature)</figcaption></figure><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="7398" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/><br/># Create dataset<br/>dataset_dict = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temp.': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],<br/>    'Humid.': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, 14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/>df = pd.DataFrame(dataset_dict)<br/><br/># One-hot encode 'Outlook' column<br/>df = pd.get_dummies(df, columns=['Outlook'],prefix='',prefix_sep='')<br/><br/># Convert 'Wind' column to binary<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data into features and target, then into training and test sets<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)</span></pre><p id="3e93" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">While it is not mandatory, to effectively use Linear Regression â€” including Ridge Regression â€” we can standardize the numerical features first.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/856bb5024939a0acbe57928f62346faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFZZ90PWRFqs2YVlYqie_w.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Standard scaling is applied to â€˜Temperatureâ€™ and â€˜Humidityâ€™ while the one-hot encoding is applied to â€˜Outlookâ€™ and â€˜Windâ€™</figcaption></figure><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="cb52" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'])<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/><br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)</span></pre><h1 id="ccaa" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">Main Mechanism</h1><p id="3c47" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Linear Regression predicts numbers by making a straight line (or hyperplane) from the data:</p><ol class=""><li id="dff0" class="mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni po pp pq bk">The model finds the best line by making the gaps between the real values and the lineâ€™s predicted values as small as possible. This is called â€œleast squares.â€</li><li id="365f" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">Each input gets a number (coefficient/weight) that shows how much it changes the final answer. Thereâ€™s also a starting number (intercept/bias) thatâ€™s used when all inputs are zero.</li><li id="a857" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">To predict a new answer, the model takes each input, multiplies it by its number, adds all these up, plus adds the starting number. This gives you the predicted answer.</li></ol><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/c72b0e92063ef36ac8fb47688848a377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ycpFoYTIl7Aan2UGSAK6fQ.png"/></div></div></figure><h1 id="89da" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk"><strong class="al">Ordinary Least Squares (OLS) Regression</strong></h1><p id="fe23" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Letâ€™s start with Ordinary Least Squares (OLS) â€” the fundamental approach to linear regression. The goal of OLS is to find the best-fitting line through our data points. We do this by measuring how â€œwrongâ€ our predictions are compared to actual values, and then finding the line that makes these errors as small as possible. When we say â€œerror,â€ we mean the vertical distance between each point and our line â€” in other words, how far off our predictions are from reality. Letâ€™s see what happened in 2D case first.</p><h2 id="c866" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">In 2D Case</h2><p id="1c85" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">In 2D case, we can imagine the linear regression algorithm like this:</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/6c8ce9540dd0d3e926482943741c9ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kciC_4-ZxFYMSO-6-yksmA.png"/></div></div></figure><p id="b634" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Hereâ€™s the explanation of the process above:</p><p id="c629" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">1.We start with a training set, where each row has:<br/>Â· <em class="qm">x </em>: our input feature (the numbers 1, 2, 3, 1, 2)<br/>Â· <em class="qm">y </em>: our target values (0, 1, 1, 2, 3)</p><p id="5add" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">2. We can plot these points on a scatter plot and we want to find a line <em class="qm">y</em> = <em class="qm">Î²</em>â‚€ + <em class="qm">Î²</em>â‚<em class="qm">x</em> that best fits these points</p><p id="7836" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">3. For any given line (any <em class="qm">Î²</em>â‚€ and <em class="qm">Î²</em>â‚), we can measure how good it is by:<br/>Â· Calculating the vertical distance (<em class="qm">d</em>â‚, <em class="qm">d</em>â‚‚,<em class="qm"> d</em>â‚ƒ, <em class="qm">d</em>â‚„, <em class="qm">d</em>â‚…) from each point to the line<br/>Â· These distances are |<em class="qm">y</em> â€” (<em class="qm">Î²</em>â‚€ + <em class="qm">Î²</em>â‚<em class="qm">x</em>)| for each point</p><p id="27f5" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">4. Our optimization goal is to find <em class="qm">Î²</em>â‚€ and<em class="qm"> Î²</em>â‚ that minimize the sum of squared distances: <em class="qm">d</em>â‚Â² + <em class="qm">d</em>â‚‚Â² + <em class="qm">d</em>â‚ƒÂ² + <em class="qm">d</em>â‚„Â² + <em class="qm">d</em>â‚…Â². In vector notation, this is written as ||<em class="qm">y </em>â€” <em class="qm">XÎ²</em>||Â², where <em class="qm">X</em> = [1 <em class="qm">x</em>] contains our input data (with 1â€™s for the intercept) and <em class="qm">Î²</em> = [<em class="qm">Î²</em>â‚€ <em class="qm">Î²</em>â‚]áµ€ contains our coefficients.</p><p id="ad18" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">5. The optimal solution has a closed form: <em class="qm">Î²</em> = (<em class="qm">X</em>áµ€<em class="qm">X</em>)â»Â¹<em class="qm">X</em>áµ€y. Calculating this we get<em class="qm"> Î²</em>â‚€ = -0.196 (intercept), <em class="qm">Î²</em>â‚ = 0.761 (slope).</p><p id="908d" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This vector notation makes the formula more compact and shows that weâ€™re really working with matrices and vectors rather than individual points. We will see more details of our calculation next in the multidimensional case.</p><h2 id="3162" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">In Multidimensional Case (ğŸ“Š Dataset)</h2><p id="2f5a" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Again, the goal of OLS is to find coefficients (<em class="qm">Î²</em>) that minimize the squared differences between our predictions and actual values. Mathematically, we express this as <strong class="mp ga">minimizing</strong> ||<em class="qm">y</em> â€” <em class="qm">XÎ²</em>||Â², where <em class="qm">X</em> is our data matrix and <em class="qm">y</em> contains our target values.</p><p id="b41b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The training process follows these key steps:</p><h2 id="2ce7" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Training Step</h2><p id="f292" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">1. Prepare our data matrix <em class="qm">X</em>. This involves adding a column of ones to account for the bias/intercept term (<em class="qm">Î²</em>â‚€).</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qn"><img src="../Images/94f0dc6a1292fe7a1cd4d41f648b377b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*if-7FlRvuq0LyHAz8L43CA.png"/></div></div></figure><p id="0e42" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">2. Instead of iteratively searching for the best coefficients, we can compute them directly using the normal equation:<br/><em class="qm">Î²</em> = (<em class="qm">X</em>áµ€<em class="qm">X</em>)â»Â¹<em class="qm">X</em>áµ€<em class="qm">y</em></p><p id="93a4" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">where:<br/>Â· <em class="qm">Î² </em>is the vector of estimated coefficients,<br/>Â· <em class="qm">X</em> is the dataset matrix(including a column for the intercept),<br/>Â· <em class="qm">y</em> is the label,<br/>Â· <em class="qm">X</em>áµ€ represents the transpose of matrix <em class="qm">X</em>,<br/>Â· â»Â¹ represents the inverse of the matrix.</p><p id="ff25" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Letâ€™s break this down:</p><p id="b1a0" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">a. We multiply <em class="qm">X</em>áµ€ (<em class="qm">X</em> transpose) by <em class="qm">X</em>, giving us a square matrix</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/41c746d7c63229c8463c4b4451d547c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rSBkGGPrwi-yO377JihvRg.png"/></div></div></figure><p id="9e15" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">b. We compute the inverse of this matrix</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qo"><img src="../Images/ec4c7598c2579c5b2aed1e6f0ba14c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TB6Z0EhByi2p0nqUcfDTSw.png"/></div></div></figure><p id="4b2f" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">c. We compute <em class="qm">X</em>áµ€<em class="qm">y</em></p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qp"><img src="../Images/56478c8b91a65d73a45096748d575778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqljD0Lxqo9bZ2JrFzmB3w.png"/></div></div></figure><p id="9df2" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">d. We multiply (<em class="qm">X</em>áµ€<em class="qm">X</em>)â»Â¹ and <em class="qm">X</em>áµ€<em class="qm">y </em>to get our coefficients</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qq"><img src="../Images/963c4e9a98a88c168f9c90739b1fc1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fYYLpFPJZxxMqgUozAiJIg.png"/></div></div></figure><h2 id="0cbf" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Test Step</h2><p id="a1ed" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Once we have our coefficients, making predictions is straightforward: we simply multiply our new data point by these coefficients to get our prediction.</p><p id="e0a8" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In matrix notation, for a new data point <em class="qm">x</em>*, the prediction <em class="qm">y</em>* is calculated as <br/><em class="qm">y</em>* = <em class="qm">x</em>*<em class="qm">Î²</em> = [1, xâ‚, xâ‚‚, â€¦, xâ‚š] Ã— [Î²â‚€, Î²â‚, Î²â‚‚, â€¦, Î²â‚š]áµ€, <br/>where <em class="qm">Î²</em>â‚€ is the intercept and <em class="qm">Î²</em>â‚ through <em class="qm">Î²</em>â‚š are the coefficients for each feature.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/fc075c4c8801b373f49a757ac742fef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TCzgdei9vOr0tJ0BZNVguQ.png"/></div></div></figure><h2 id="e039" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Evaluation Step</h2><p id="9732" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">We can do the same process for all data points. For our dataset, hereâ€™s the final result with the RMSE as well.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/c0f9605d793360d6c403abdb3192e0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lzf6VMZOU_PBQe7J1LAudA.png"/></div></div></figure><h1 id="807f" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk"><strong class="al">Ridge Regression</strong></h1><p id="abec" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Now, letâ€™s consider Ridge Regression, which builds upon OLS by addressing some of its limitations. The key insight of Ridge Regression is that sometimes the optimal OLS solution <strong class="mp ga">involves very large coefficients</strong>, which can lead to overfitting.</p><p id="d554" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Ridge Regression adds a penalty term (<em class="qm">Î»</em>||<em class="qm">Î²</em>||Â²) to the objective function. This term discourages large coefficients by adding their squared values to what weâ€™re minimizing. The full objective becomes:</p><p id="739b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">min ||<em class="qm">y</em> â€” <em class="qm">X</em>Î²||Â² + Î»||<em class="qm">Î²</em>||Â²</p><p id="5376" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The <em class="qm">Î»</em> (lambda) parameter controls how much we penalize large coefficients. When <em class="qm">Î»</em> = 0, we get OLS; as<em class="qm"> Î»</em> increases, the coefficients shrink toward zero (but never quite reach it).</p><h2 id="da91" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Training Step</h2><ol class=""><li id="5757" class="mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni po pp pq bk">Just like OLS, prepare our data matrix <em class="qm">X</em>. This involves adding a column of ones to account for the intercept term (<em class="qm">Î²</em>â‚€).</li><li id="7b05" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni po pp pq bk">The training process for Ridge follows a similar pattern to OLS, but with a modification. The closed-form solution becomes:<br/><em class="qm">Î²</em> = (<em class="qm">X</em>áµ€<em class="qm">X</em>+ Î»I)â»Â¹<em class="qm">X</em>áµ€<em class="qm">y</em></li></ol><p id="d8de" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">where:<br/>Â· <em class="qm">I</em> is the identity matrix (with the first element, corresponding to <em class="qm">Î²</em>â‚€, sometimes set to 0 to exclude the intercept from regularization in some implementations),<br/>Â· Î» is the regularization value.<br/>Â· <em class="qm">Y</em> is the vector of observed dependent variable values.<br/>Â· Other symbols remain as defined in the OLS section.</p><p id="e5d3" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Letâ€™s break this down:</p><p id="cebb" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">a. We add <em class="qm">Î»</em>I to <em class="qm">X</em>áµ€<em class="qm">X. </em>The value of <em class="qm">Î» </em>can be any positive number (say 0.1).</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qr"><img src="../Images/1f631555a5c3da55dcc0e5c818fdc266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*byo0j5v6j2Ch4UicKJOsPQ.png"/></div></div></figure><p id="83c4" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">b. We compute the inverse of this matrix. The benefits of adding Î»I to <em class="qm">X</em>áµ€<em class="qm">X</em> before inversion are:<br/>Â· Makes the matrix invertible, even if <em class="qm">X</em>áµ€<em class="qm">X</em> isnâ€™t (solving a key numerical problem with OLS)<br/>Â· Shrinks the coefficients proportionally to <em class="qm">Î»</em></p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qs"><img src="../Images/41521c73d61c93a7d2f3d061ed957b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6SKc3lY7TJzqVCNO-kJnQ.png"/></div></div></figure><p id="305b" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">c. We multiply (<em class="qm">X</em>áµ€<em class="qm">X</em>+ <em class="qm">Î»I</em>)â»Â¹ and <em class="qm">X</em>áµ€<em class="qm">y </em>to get our coefficients</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qt"><img src="../Images/e4ed2341b5407401f67dd250d1b98f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dapWwY3uzL7_8cpkEqgjw.png"/></div></div></figure><h2 id="0e05" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk"><strong class="al">Test Step</strong></h2><p id="8877" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">The prediction process remains the same as OLS â€” multiply new data points by the coefficients. The difference lies in the coefficients themselves, which are typically smaller and more stable than their OLS counterparts.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/918f8768c3d930ee52fc25e57f1c38bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qh985VeQj7xrRftIkhGuGQ.png"/></div></div></figure><h2 id="fe75" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Evaluation Step</h2><p id="f9f0" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">We can do the same process for all data points. For our dataset, hereâ€™s the final result with the RMSE as well.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pe"><img src="../Images/d89851ba086f08012ca77a947dd7e06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mj3z3Zw9HMGKxbUF0VVVcQ.png"/></div></div></figure><h2 id="2227" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk"><strong class="al">Final Remarks: Choosing Between OLS and Ridge</strong></h2><p id="a65a" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">The choice between OLS and Ridge often depends on your data:</p><ul class=""><li id="7eaa" class="mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni qu pp pq bk">Use OLS when you have well-behaved data with little multicollinearity and enough samples (relative to features)</li><li id="1f70" class="mn mo fq mp b gt pr mr ms gw ps mu mv mw pt my mz na pu nc nd ne pv ng nh ni qu pp pq bk">Use Ridge when you have:<br/>- Many features (relative to samples)<br/>- Multicollinearity in your features<br/>- Signs of overfitting with OLS</li></ul><p id="5ebe" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">With Ridge, youâ€™ll need to choose <em class="qm">Î»</em>. Start with a range of values (often logarithmically spaced) and choose the one that gives the best validation performance.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qv"><img src="../Images/0d0c60a41bac36424c7f233554495189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DBCP3wgCWh3BNpBjgg_lQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Apparantly, the default value <em class="qw">Î» = 1 gives the best RMSE for our dataset.</em></figcaption></figure><h1 id="b312" class="oc od fq bf oe of og gv oh oi oj gy ok ol om on oo op oq or os ot ou ov ow ox bk">ğŸŒŸ OLS and Ridge Regression Code Summarized</h1><pre class="nm nn no np nq pf pg ph bp pi bb bk"><span id="1ed7" class="pj od fq pg b bg pk pl l pm pn">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.linear_model import Ridge<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df = df[['sunny','overcast','rain','Temperature','Humidity','Wind','Num_Players']]<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/><br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)<br/><br/># Initialize and train the model<br/>#model = LinearRegression() # Option 1: OLS Regression<br/>model = Ridge(alpha=0.1)  # Option 2: Ridge Regression (alpha is the regularization strength, equivalent to Î»)<br/><br/># Fit the model<br/>model.fit(X_train_scaled, y_train)<br/><br/># Make predictions<br/>y_pred = model.predict(X_test_scaled)<br/><br/># Calculate and print RMSE<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"RMSE: {rmse:.4f}")<br/><br/># Additional information about the model<br/>print("\nModel Coefficients:")<br/>print(f"Intercept    : {model.intercept_:.2f}")<br/>for feature, coef in zip(X_train_scaled.columns, model.coef_):<br/>    print(f"{feature:13}: {coef:.2f}")</span></pre><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qx"><img src="../Images/ff9ff4226a31e1ad21380175a7da9bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhkLJJCIbYurDgydtzcerg.png"/></div></div></figure></div></div></div><div class="ab cb qy qz ra rb" role="separator"><span class="rc by bm rd re rf"/><span class="rc by bm rd re rf"/><span class="rc by bm rd re"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="806f" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Further Reading</h2><p id="9d91" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">For a detailed explanation of <a class="af pd" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">OLS Linear Regression</a> and <a class="af pd" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener ugc nofollow" target="_blank">Ridge Regression</a>, and its implementation in scikit-learn, readers can refer to their official documentation. It provides comprehensive information on their usage and parameters.</p><h2 id="d13a" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">Technical Environment</h2><p id="909b" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="f399" class="pw od fq bf oe px py pz oh qa qb qc ok mw qd qe qf na qg qh qi ne qj qk ql fw bk">About the Illustrations</h2><p id="0d40" class="pw-post-body-paragraph mn mo fq mp b gt oy mr ms gw oz mu mv mw pa my mz na pb nc nd ne pc ng nh ni fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="fb1f" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ğ™šğ™œğ™§ğ™šğ™¨ğ™¨ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:</p><div class="rg rh ri rj rk"><div role="button" tabindex="0" class="ab bx cp kj it rl rm bp rn lv ao"><div class="ro l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rp rq cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rp rq em n ay ub"/></div><div class="rr l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ru hp l"><h2 class="bf ga wh ic it wi iv iw wj iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wk wl wm wn wo lh wp wq um ii wr ws wt uq ur us ep bm ut ny" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wu l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sd dz se it ab sf il ed"><div class="ed rx bx ry rz"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed rx bx kk sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sc sb"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph mn mo fq mp b gt mq mr ms gw mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:</p><div class="rg rh ri rj rk"><div role="button" tabindex="0" class="ab bx cp kj it rl rm bp rn lv ao"><div class="ro l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rp rq cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rp rq em n ay ub"/></div><div class="rr l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq ru hp l"><h2 class="bf ga wh ic it wi iv iw wj iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wk wl wm wn wo lh wp wq um ii wr ws wt uq ur us ep bm ut ny" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----2e5ad011eae4--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wu l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sd dz se it ab sf il ed"><div class="ed rx bx ry rz"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed rx bx kk sa sb"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sc sb"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>