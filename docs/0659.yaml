- en: 'Human Pose Tracking with MediaPipe in 2D and 3D: Rerun Showcase'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=collection_archive---------1-----------------------#2024-03-11](https://towardsdatascience.com/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=collection_archive---------1-----------------------#2024-03-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to easily visualise MediaPipe’s human pose tracking with Rerun
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://andreasnaoum.medium.com/?source=post_page---byline--125053cfe64f--------------------------------)[![Andreas
    Naoum](../Images/e14d545f270170877e0af31572275e17.png)](https://andreasnaoum.medium.com/?source=post_page---byline--125053cfe64f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--125053cfe64f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--125053cfe64f--------------------------------)
    [Andreas Naoum](https://andreasnaoum.medium.com/?source=post_page---byline--125053cfe64f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--125053cfe64f--------------------------------)
    ·7 min read·Mar 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8588c83a6d99bada222874b26a1f256.png)'
  prefs: []
  type: TYPE_IMG
- en: Human Pose Tracking | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explore a use case that leverages the power of [MediaPipe](https://developers.google.com/mediapipe)
    for tracking human poses in both 2D and 3D. What makes this exploration even more
    fascinating is the visualisation aspect powered by the open-source visualisation
    tool [Rerun](https://www.rerun.io), which provides a holistic view of human poses
    in action.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, you’ll be guided to use MediaPipe to track human poses in
    2D and 3D, and explore the visualisation capabilities of Rerun.
  prefs: []
  type: TYPE_NORMAL
- en: Human Pose Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human pose tracking is a task in computer vision that focuses on identifying
    key body locations, analysing posture, and categorising movements. At the heart
    of this technology is a pre-trained machine-learning model to assess the visual
    input and recognise landmarks on the body in both image coordinates and 3D world
    coordinates. The use cases and applications of this technology include but are
    not limited to Human-Computer Interaction, Sports Analysis, Gaming, Virtual Reality,
    Augmented Reality, Health, etc.
  prefs: []
  type: TYPE_NORMAL
- en: It would be good to have a perfect model, but unfortunately, the current models
    are still imperfect. Although datasets could have a variety of body types, the
    human body differs among individuals. The uniqueness of each individual’s body
    poses a challenge, particularly for those with non-standard arm and leg dimensions,
    which may result in lower accuracy when using this technology. When considering
    the integration of this technology into systems, it is crucial to acknowledge
    the possibility of inaccuracies. Hopefully, ongoing efforts within the scientific
    community will pave the way for the development of more robust models.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond lack of accuracy, ethical and legal considerations emerge from utilising
    this technology. For instance, capturing human body poses in public spaces could
    potentially invade privacy rights if individuals have not given their consent.
    It’s crucial to take into account any ethical and legal concerns before implementing
    this technology in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites & Setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Begin by installing the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Track Human Pose using MediaPipe
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d027aae91cc4189161358f87f836a4f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Image via [Pose Landmark Detection Guide](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)
    by [Google](https://about.google/brand-resource-center/) [1]
  prefs: []
  type: TYPE_NORMAL
- en: '[**MediaPipe Python**](https://mediapipe-studio.webapps.google.com/home) is
    a handy tool for developers looking to integrate on-device ML solutions for computer
    vision and machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the code below, [MediaPipe pose landmark detection](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)
    was utilised for detecting landmarks of human bodies in an image. This model can
    detect body pose landmarks as both image coordinates and 3D world coordinates.
    Once you have successfully run the ML model, you can use the image coordinates
    and the 3D world coordinates to visualise the output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Visualise the output of MediaPipe using Rerun
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0ffbd838ee50a8f7b64114417b1e978b.png)'
  prefs: []
  type: TYPE_IMG
- en: Rerun Viewer | Image via [Rerun Docs](https://www.rerun.io/docs/reference/viewer/overview)
    [2]
  prefs: []
  type: TYPE_NORMAL
- en: '[Rerun](https://www.rerun.io) serves as a visualisation tool for multi-modal
    data. Through the [Rerun Viewer](https://www.rerun.io/docs/reference/viewer/overview),
    you can build layouts, customise visualisations and interact with your data. The
    rest part of this section details how you can log and present data using the Rerun
    SDK to visualise it within the Rerun Viewer'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bba414c2f083ce237872945da1c80150.png)'
  prefs: []
  type: TYPE_IMG
- en: Pose Landmarker Model | Image via [Pose Landmark Detection Guide](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)
    by [Google](https://about.google/brand-resource-center/) [1]
  prefs: []
  type: TYPE_NORMAL
- en: In both 2D and 3D points, specifying connections between points is essential.
    Defining these connections automatically renders lines between them. Using the
    information provided by MediaPipe, you can get the pose points connections from
    the `POSE_CONNECTIONS` set and then set them as keypoint connections using [Annotation
    Context](https://www.rerun.io/docs/concepts/annotation-context).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Image Coordinates — 2D Positions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/20e233978eee38de893191d9b35b30f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualising Human Pose as 2D Points | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Visualising the body pose landmarks on the video appears to be a good choice.
    To achieve that, you need to follow the rerun documentation for Entities and Components.
    [The Entity Path Hierarchy](https://www.rerun.io/docs/concepts/entity-path) page
    describes how to log multiple Components on the same Entity. For example, you
    can create the ‘video’ entity and include the components ‘video/rgb’ for the video
    and ‘video/pose’ for the body pose. If you’re aiming to use that for a video,
    you need the concept of [Timelines](https://www.rerun.io/docs/concepts/timelines).
    Each frame can be associated with the appropriate data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a function that can visualise the 2D points on the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3D World Coordinates — 3D Points
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/fc0f27b3622dd6cffc6cbae4a70cb5fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualising Human Pose as 3D Points | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Why settle on 2D points when you have 3D Points? Create a new entity, name it
    “Person”, and log the 3D points. It’s done! You just created a 3D presentation
    of the human body pose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Source Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The tutorial focuses on the main parts of the Human Pose Tracking example. For
    those who prefer a hands-on approach, the full source code for this example is
    available on [GitHub](https://github.com/rerun-io/rerun/tree/latest/examples/python/human_pose_tracking).
    Feel free to explore, modify, and understand the inner workings of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Tips & Suggestions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. Compress the image for efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can boost the overall procedure speed by compressing the logged images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Limit Memory Use
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re logging more data than can be fitted into your RAM, it will start
    dropping the old data. The default limit is 75% of your system RAM. If you want
    to increase that you could use the command line argument — memory-limit. More
    information about memory limits can be found on Rerun’s [How To Limit Memory Use](https://www.rerun.io/docs/howto/limit-ram)
    page.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Customise Visualisations for your needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/98f7548f01e6161352b1194f993aaf09.png)'
  prefs: []
  type: TYPE_IMG
- en: Customise Rerun Viewer | Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Beyond Human Pose Tracking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*If you found this article useful and insightful, there’s more!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Similar articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----125053cfe64f--------------------------------)
    [## Real-Time Face and Face Landmark Detection with MediaPipe: Rerun Showcase'
  prefs: []
  type: TYPE_NORMAL
- en: How to easily visualise MediaPipe’s face and face landmark detection in 2D and
    3D with Rerun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'ai.gopubby.com](https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----125053cfe64f--------------------------------)
    [](/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=post_page-----125053cfe64f--------------------------------)
    [## Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase'
  prefs: []
  type: TYPE_NORMAL
- en: How to visualise MediaPipe’s Hand Tracking and Gesture Recognition with Rerun
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=post_page-----125053cfe64f--------------------------------)
    ![Andreas Naoum](../Images/5af57533919b28694a1f6a0caf923221.png)
  prefs: []
  type: TYPE_NORMAL
- en: '[Andreas Naoum](https://andreasnaoum.medium.com/?source=post_page-----125053cfe64f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Data Visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://andreasnaoum.medium.com/list/multimodal-data-visualizations-48083691fa4b?source=post_page-----125053cfe64f--------------------------------)5
    stories![](../Images/afd598e10c2627e4d16424dd5903e02f.png)![](../Images/6ce441267a7e6f182643f87d802ee4ac.png)![](../Images/431a1a3f42709b335dddbdfeecffb9ef.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '*I regularly share tutorials on visualisation for computer vision and robotics.
    Follow me for future updates!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [***LinkedIn***](http://www.linkedin.com/in/andreas-naoum)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [Pose Landmark Detection Guide](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)
    by [Google](https://about.google/brand-resource-center/), Portions of this page
    are reproduced from work created and [shared by Google](https://developers.google.com/readme/policies)
    and used according to terms described in the [Creative Commons 4.0 Attribution
    License](https://creativecommons.org/licenses/by/4.0/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Rerun Docs](https://www.rerun.io/docs/reference/viewer/overview) by [Rerun](https://www.rerun.io)
    under [MIT license](https://github.com/rerun-io/rerun/blob/main/LICENSE-MIT)'
  prefs: []
  type: TYPE_NORMAL
