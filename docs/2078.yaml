- en: 'Missing Value Imputation, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb?source=collection_archive---------0-----------------------#2024-08-27](https://towardsdatascience.com/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb?source=collection_archive---------0-----------------------#2024-08-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DATA PREPROCESSING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One (tiny) dataset, six imputation methods?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--93e0726284eb--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--93e0726284eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--93e0726284eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--93e0726284eb--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--93e0726284eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--93e0726284eb--------------------------------)
    Â·13 min readÂ·Aug 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f42d507518dfcdd0f8722ea80b8fec0e.png)'
  prefs: []
  type: TYPE_IMG
- en: '`â›³ï¸ More [DATA PREPROCESSING](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4),
    explained: â–¶ [Missing Value Imputation](/missing-value-imputation-explained-a-visual-guide-with-code-examples-for-beginners-93e0726284eb)
    Â· [Categorical Encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    Â· [Data Scaling](/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    Â· [Discretization](/discretization-explained-a-visual-guide-with-code-examples-for-beginners-f056af9102fa?gi=c1bf25229f86)
    Â· [Oversampling & Undersampling](/oversampling-and-undersampling-explained-a-visual-guide-with-mini-2d-dataset-1155577d3091)
    Â· [Data Leakage in Preprocessing](/data-leakage-in-preprocessing-explained-a-visual-guide-with-code-examples-33cbf07507b7)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s talk about something that every data scientist, analyst, or curious number-cruncher
    has to deal with sooner or later: missing values. Now, I know what youâ€™re thinking
    â€” â€œOh great, another missing value guide.â€ But hear me out. Iâ€™m going to show
    you how to tackle this problem using not one, not two, but six different imputation
    methods, all on a single dataset (with helpful visuals as well!). By the end of
    this, youâ€™ll see why domain knowledge is worth its weight in gold (something even
    our AI friends might struggle to replicate).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29c4e53115cb3858f193adc68a724447.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: What Are Missing Values and Why Do They Occur?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into our dataset and imputation methods, letâ€™s take a moment to
    understand what missing values are and why theyâ€™re such a common headache in data
    science.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Missing Values?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Missing values, often represented as NaN (Not a Number) in pandas or NULL in
    databases, are essentially *holes in your dataset*. Theyâ€™re the empty cells in
    your spreadsheet, the blanks in your survey responses, the data points that got
    away. In the world of data, not all absences are created equal, and understanding
    the nature of your missing values is crucial for deciding how to handle them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77e04b185445651e16249a07fbddc447.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Why Do Missing Values Occur?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Missing values can sneak into your data for a variety of reasons. Here are
    some common reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Entry Errors**: Sometimes, itâ€™s just human error. Someone might forget
    to input a value or accidentally delete one.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sensor Malfunctions**: In IoT or scientific experiments, a faulty sensor
    might fail to record data at certain times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Survey Non-Response**: In surveys, respondents might skip questions theyâ€™re
    uncomfortable answering or donâ€™t understand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Merged Datasets**: When combining data from multiple sources, some entries
    might not have corresponding values in all datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Corruption**: During data transfer or storage, some values might get
    corrupted and become unreadable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Intentional Omissions**: Some data might be intentionally left out due to
    privacy concerns or irrelevance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sampling Issues**: The data collection method might systematically miss certain
    types of data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Time-Sensitive Data**: In time series data, values might be missing for periods
    when data wasnâ€™t collected (e.g., weekends, holidays).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Types of Missing Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding the type of missing data youâ€™re dealing with can help you choose
    the most appropriate imputation method. Statisticians generally categorize missing
    data into three types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing Completely at Random (MCAR)**: The missingness is *totally random*
    and doesnâ€™t depend on any other variable. For example, if a lab sample was accidentally
    dropped.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Missing at Random (MAR)**: The probability of missing data *depends on other
    observed variables* but not on the missing data itself. For example, men might
    be less likely to answer questions about emotions in a survey.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Missing Not at Random (MNAR)**: The missingness *depends on the value of
    the missing data itself*. For example, people with high incomes might be less
    likely to report their income in a survey.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/4cc65a492703a3c1624d09b257c15bbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Why Care About Missing Values?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Missing values can significantly impact your analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: They can introduce bias if not handled properly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Many machine learning algorithms canâ€™t handle missing values out of the box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They can lead to loss of important information if instances with missing values
    are simply discarded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improperly handled missing values can lead to incorrect conclusions or predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thatâ€™s why itâ€™s crucial to have a solid strategy for dealing with missing values.
    And thatâ€™s exactly what weâ€™re going to explore in this article!
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First things first, letâ€™s introduce our dataset. Weâ€™ll be working with a golf
    course dataset that tracks various factors affecting the crowdedness of the course.
    This dataset has a bit of everything â€” numerical data, categorical data, and yes,
    plenty of missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c57995a7970551a87f83242506716ab.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset is artificially made by the author (inspired by [1]) to promote
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, our dataset contains 20 rows and 8 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Date: The date of the observation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weekday: Day of the week (0â€“6, where 0 is Monday)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Holiday: Boolean indicating if itâ€™s a holiday (0 or 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Temp: Temperature in Celsius'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Humidity: Humidity percentage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wind: Wind condition (0 or 1, possibly indicating calm or windy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Outlook: Weather outlook (sunny, overcast, or rainy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crowdedness: Percentage of course occupancy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And look at that! Weâ€™ve got missing values in every column except Date and Weekday.
    Perfect for our imputation party.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our dataset loaded, letâ€™s tackle these missing values with
    six different imputation methods. Weâ€™ll use a different strategy for each type
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Listwise Deletion'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Listwise deletion, also known as complete case analysis, involves removing entire
    rows that contain any missing values. This method is simple and preserves the
    distribution of the data, but it can lead to a significant loss of information
    if many rows contain missing values.
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: Listwise deletion is often used when the number of missing
    values is small and the data is missing completely at random (MCAR). Itâ€™s also
    useful when you need a complete dataset for certain analyses that canâ€™t handle
    missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using listwise deletion for rows that have at least
    4 missing values. These rows might not provide enough reliable information, and
    removing them can help us focus on the more complete data points. However, weâ€™re
    being cautious and only removing rows with significant missing data to preserve
    as much information as possible.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac3d3f78bea999b4a77b406489df405b.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Weâ€™ve removed 2 rows that had too many missing values. Now letâ€™s move on to
    imputing the remaining missing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: Simple Imputation â€” Mean and Mode'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simple imputation involves replacing missing values with a summary statistic
    of the observed values. Common approaches include using the mean, median, or mode
    of the non-missing values in a column.
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: Mean imputation is often used for continuous variables when
    the data is missing at random and the distribution is roughly symmetric. Mode
    imputation is typically used for categorical variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using mean imputation for Humidity and mode imputation
    for Holiday. For Humidity, assuming the missing values are random, the mean provides
    a reasonable estimate of the typical humidity. For Holiday, since itâ€™s a binary
    variable (holiday or not), the mode gives us the most common state, which is a
    sensible guess for missing values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb3cbf0666616f5ece9d7ebafe1801bc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 3: Linear Interpolation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear interpolation estimates missing values by assuming a linear relationship
    between known data points. Itâ€™s particularly useful for time series data or data
    with a natural ordering.
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: Linear interpolation is often used for time series data,
    where missing values can be estimated based on the values before and after them.
    Itâ€™s also useful for any data where thereâ€™s expected to be a roughly linear relationship
    between adjacent points.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using linear interpolation for Temperature. Since temperature
    tends to change gradually over time and our data is ordered by date, linear interpolation
    can provide reasonable estimates for the missing temperature values based on the
    temperatures recorded on nearby days.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99ad8cebf6fd2d63877900c2f3b71339.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 4: Forward/Backward Fill'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forward fill (or â€œlast observation carried forwardâ€) propagates the last known
    value forward to fill gaps, while backward fill does the opposite. This method
    assumes that the missing value is likely to be similar to the nearest known value.
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: Forward/backward fill is often used for time series data,
    especially when the value is likely to remain constant until changed (like in
    financial data) or when the most recent known value is the best guess for the
    current state.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using a combination of forward and backward fill for
    Outlook. Weather conditions often persist for several days, so itâ€™s reasonable
    to assume that a missing Outlook value might be similar to the Outlook of the
    previous or following day.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f76370e6ad62f0c847556ff0e1e83395.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 5: Constant Value Imputation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This method involves replacing all missing values in a variable with a specific
    constant value. This constant could be chosen based on domain knowledge or a safe
    default value.
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: Constant value imputation is often used when thereâ€™s a logical
    default value for missing data, or when you want to explicitly flag that a value
    was missing (by using a value outside the normal range of the data).'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using constant value imputation for the Wind column,
    replacing missing values with -1\. This approach explicitly flags imputed values
    (since -1 is outside the normal 0â€“1 range for Wind) and it preserves the information
    that these values were originally missing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc42e0bc88d6142a226e70ec4a52840e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Method 6: KNN Imputation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[K-Nearest Neighbors](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    (KNN) imputation estimates missing values by finding the K most similar samples
    in the dataset (just like KNN as Classification Algorithm) and using their values
    to impute the missing data. This method can capture complex relationships between
    variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ğŸ‘ Common Use**: KNN imputation is versatile and can be used for both continuous
    and categorical variables. Itâ€™s particularly useful when there are expected to
    be complex relationships between variables that simpler methods might miss.'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Our Case**: Weâ€™re using KNN imputation for Crowdedness. Crowdedness likely
    depends on a combination of factors (like temperature, holiday status, etc.),
    and KNN can capture these complex relationships to provide more accurate estimates
    of missing crowdedness values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a053cca0cd994e3d358188a79e82229d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Conclusion: The Power of Choice (and Knowledge)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, there you have it! Six different ways to handle missing values, all applied
    to our golf course dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/512ea00ed33319c250f4dc0e98dc11f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s recap how each method tackled our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Listwise Deletion**: Helped us focus on more complete data points by removing
    rows with extensive missing values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Simple Imputation**: Filled in Humidity with average values and Holiday with
    the most common occurrence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Linear Interpolation**: Estimated missing Temperature values based on the
    trend of surrounding days.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forward/Backward Fill**: Guessed missing Outlook values from adjacent days,
    reflecting the persistence of weather patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Constant Value Imputation**: Flagged missing Wind data with -1, preserving
    the fact that these values were originally unknown.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**KNN Imputation**: Estimated Crowdedness based on similar days, capturing
    complex relationships between variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each method tells a different story about our missing data, and the â€œrightâ€
    choice depends on what we know about our golf course operations and what questions
    weâ€™re trying to answer.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway? Donâ€™t just blindly apply imputation methods. Understand your
    data, consider the context, and choose the method that makes the most sense for
    your specific situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'âš ï¸ Warning: The Purpose and Limitations of Missing Value Imputation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While weâ€™ve explored various imputation techniques, we need to understand their
    purpose and limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Not a Magic Solution**: Imputation is not a cure-all for missing data. Itâ€™s
    a tool to make your data usable, **not to create perfect data**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Potential for Bias**: Imputed values are educated guesses. They can introduce
    bias if not done carefully, especially if the data is Not Missing At Random (NMAR).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss of Uncertainty**: Most simple imputation methods donâ€™t account for the
    uncertainty in the missing values, which can lead to overconfident models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data Distortion**: Aggressive imputation can distort relationships in your
    data. Always check if imputation has significantly altered your dataâ€™s distribution
    or correlations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Document Your Process**: Always clearly document your imputation methods.
    This transparency is crucial for reproducibility and for others to understand
    potential biases in your results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, the goal of imputation is to make your data usable while minimizing bias
    and information loss. Itâ€™s not about creating perfect data, but about making the
    best use of the information you have. Always approach imputation with caution
    and critical thinking.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ Missing Value Imputation Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81e368a2e5f6c346e3d721bd30cf1575.png)'
  prefs: []
  type: TYPE_IMG
- en: For a concise visual summary of Missing Values Imputation, check out [the companion
    Instagram post.](https://www.instagram.com/p/C_Kh78qSzoz/)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] T. M. Mitchell, [Machine Learning](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)
    (1997), McGraw-Hill Science/Engineering/Math, pp. 59'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¿ğ™–ğ™©ğ™– ğ™‹ğ™§ğ™šğ™¥ğ™§ğ™¤ğ™˜ğ™šğ™¨ğ™¨ğ™ğ™£ğ™œ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data Preprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/data-preprocessing-17a2c49b44e4?source=post_page-----93e0726284eb--------------------------------)6
    stories![](../Images/f7ead0fb9a8dc2823d7a43d67a1c6932.png)![Cartoon illustration
    of two figures embracing, with letters â€˜Aâ€™, â€˜Bâ€™, â€˜Câ€™ and numbers â€˜1â€™, â€˜2â€™, â€˜3â€™
    floating around them. A pink heart hovers above, symbolizing affection. The background
    is a pixelated pattern of blue and green squares, representing data or encoding.
    This image metaphorically depicts the concept of encoding categorical data, where
    categories (ABC) are transformed into numerical representations (123).](../Images/72bb3a287a9ca4c5e7a3871e234bcc4b.png)![A
    cartoon illustration representing data scaling in machine learning. A tall woman
    (representing a numerical feature with a large range) is shown shrinking into
    a child (representing the same feature after scaling to a smaller range). A red
    arrow indicates the shrinking process, and yellow sparkles around the child signify
    the positive impact of scaling.](../Images/d261b2c52a3cafe266d1962d4dbabdbd.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----93e0726284eb--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----93e0726284eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----93e0726284eb--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)'
  prefs: []
  type: TYPE_NORMAL
