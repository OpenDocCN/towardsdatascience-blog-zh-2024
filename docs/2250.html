<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Creating Task-Oriented Dialog systems with LangGraph and LangChain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Creating Task-Oriented Dialog systems with LangGraph and LangChain</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-task-oriented-dialog-systems-with-langgraph-and-langchain-fada6c9c4983?source=collection_archive---------2-----------------------#2024-09-14">https://towardsdatascience.com/creating-task-oriented-dialog-systems-with-langgraph-and-langchain-fada6c9c4983?source=collection_archive---------2-----------------------#2024-09-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="abe8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Yet another LangGraph tutorial</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mesquitadeh?source=post_page---byline--fada6c9c4983--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Déborah Mesquita" class="l ep by dd de cx" src="../Images/3b77b7eb569e24f2679875429173daf1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Ikjkskv8dznK8GVC093MLA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--fada6c9c4983--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mesquitadeh?source=post_page---byline--fada6c9c4983--------------------------------" rel="noopener follow">Déborah Mesquita</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--fada6c9c4983--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/df4dbf4e422ff2e27cc77246f75e6d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ouWBgwxkE2U-5aPwpJ7x5g.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by <a class="af nj" href="https://unsplash.com/pt-br/@kaleidico?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Kaleidico</a> on <a class="af nj" href="https://unsplash.com/pt-br/fotografias/duas-pessoas-desenhando-no-quadro-branco-26MJGnCM0Wc?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f1f6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">A Task-Oriented Dialogue system (ToD) is a system that assists users in <strong class="nm fr">achieving a particular task</strong>, such as booking a restaurant, planning a travel itinerary or ordering delivery food.</p><p id="e7e0" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We know that we instruct LLMs using prompts, but how can we implement these ToD systems so that <strong class="nm fr">the conversation always revolves around the task we want the users to achieve</strong>? One way of doing that is by using <strong class="nm fr">prompts</strong>, <strong class="nm fr">memory </strong>and <strong class="nm fr">tool calling. </strong>FortunatelyLangChain + LangGraph can help us tie all these things together.</p><p id="cb21" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In this article, you’ll learn how to build a Task Oriented Dialogue System that helps users create User Stories with a high level of quality. The system is all based on LangGraph’s <a class="af nj" href="https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting/" rel="noopener ugc nofollow" target="_blank">Prompt Generation from User Requirements </a>tutorial.</p><h1 id="ffe3" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Why do we need to use LangGraph?</h1><p id="c68e" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">In this tutorial we assume you already know how to use LangChain. A User Story has some components like objective, success criteria, plan of execution and deliverables. The user should provide each of them, and we need to “hold their hand” into providing them one by one. Doing that using only LangChain would require a lot of ifs and elses.</p><p id="697b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With LangGraph we can use a graph abstraction to <strong class="nm fr">create cycles </strong>to control the dialogue. It also has <strong class="nm fr">built-in persistence</strong>, so we don’t need to worry about actively tracking the interactions that happen within the graph.</p><p id="d903" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The main LangGraph abstraction is the <a class="af nj" href="https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph" rel="noopener ugc nofollow" target="_blank"><strong class="nm fr">StateGraph</strong></a>, which is used to create graph workflows. Each graph needs to be initialized with a <strong class="nm fr">state_schema</strong>: a schema class that each node of the graph uses to read and write information.</p><p id="0eff" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The flow of our system will consist of rounds of <em class="ph">LLM </em>and <em class="ph">user </em>messages. The main loop will contain these steps:</p><ol class=""><li id="8127" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of pi pj pk bk">User says something</li><li id="026c" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of pi pj pk bk">LLM reads the messages of the state and decides if it’s ready to create the User Story or if the user should respond again</li></ol><p id="fb41" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Our system is simple so the schema consists only of the messages that were exchanged in the dialogue.</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="1bf0" class="pu oh fq pr b bg pv pw l px py">from langgraph.graph.message import add_messages<br/><br/>class StateSchema(TypedDict):<br/>    messages: Annotated[list, add_messages]</span></pre><p id="0512" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The <strong class="nm fr">add_messages </strong>method is used to merge the output messages from each <strong class="nm fr">node </strong>into the existing list of messages in the graph’s state.</p><p id="8b04" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Speaking about nodes, another two main LangGraph concepts are <strong class="nm fr">Nodes</strong> and <strong class="nm fr">Edges</strong>. Each <strong class="nm fr">node </strong>of the graph runs a function and each <strong class="nm fr">edge </strong>controls the flow of one node to another. We also have <strong class="nm fr">START </strong>and <strong class="nm fr">END </strong>virtual nodes to tell the graph where to start the execution and where the execution should end.</p><p id="e1de" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To run the system we’ll use the <code class="cx pz qa qb pr b">.stream()</code> method. After we build the graph and compile it, <strong class="nm fr">each round of interaction will go through the START until the END of the graph</strong> and the path it takes (which nodes should run or not) is controlled by our workflow combined with the state of the graph. The following code has the main flow of our system:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="443e" class="pu oh fq pr b bg pv pw l px py">config = {"configurable": {"thread_id": str(uuid.uuid4())}}<br/><br/>while True:<br/>    user = input("User (q/Q to quit): ")<br/>    if user in {"q", "Q"}:<br/>        print("AI: Byebye")<br/>        break<br/>    output = None<br/>    for output in graph.stream(<br/>        {"messages": [HumanMessage(content=user)]}, config=config, stream_mode="updates"<br/>    ):<br/>        last_message = next(iter(output.values()))["messages"][-1]<br/>        last_message.pretty_print()<br/><br/>    if output and "prompt" in output:<br/>        print("Done!")</span></pre><p id="5ab1" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">At each interaction (if the user didn’t type “q” or “Q” to quit) we run graph.stream() passing the message of the user using the “updates” stream_mode, which streams the updates of the state after each step of the graph (<a class="af nj" href="https://langchain-ai.github.io/langgraph/concepts/low_level/#stream-and-astream" rel="noopener ugc nofollow" target="_blank">https://langchain-ai.github.io/langgraph/concepts/low_level/#stream-and-astream</a>). We then get this last message from the state_schema and print it.</p><p id="40ca" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">In this tutorial we’ll still learn how to create the nodes and edges of the graph, but first let’s talk more about the architecture of ToD systems in general and learn how to implement one with <strong class="nm fr">LLMs</strong>, <strong class="nm fr">prompts </strong>and <strong class="nm fr">tool calling</strong>.</p><h1 id="6973" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">The Architecture of ToD systems</h1><p id="4bda" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">The main components of a framework to build <strong class="nm fr">End-to-End Task-Oriented Dialogue systems</strong> are [1]:</p><ol class=""><li id="277c" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of pi pj pk bk">Natural Language Understanding (NLU) for <strong class="nm fr">extracting the intent and key slots of users</strong></li><li id="2270" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of pi pj pk bk">Dialogue State Tracking (DST) for <strong class="nm fr">tracing users’ belief state</strong> given dialogue</li><li id="2fcb" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of pi pj pk bk">Dialogue Policy Learning (DPL) to <strong class="nm fr">determine the next step to take</strong></li><li id="71f6" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of pi pj pk bk">Natural Language Generation (NLG) for <strong class="nm fr">generating dialogue system response</strong></li></ol><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq qc"><img src="../Images/a4be2cd1978b86ea567eaae32cea0cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*CqNUPqw97b_8Y0v3zBzNbQ.png"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Main components of a ToD system (image from Qin, Libo, et al [1])</figcaption></figure><p id="296c" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">By using LLMs, we can combine some of these components into only one. The <strong class="nm fr">NLP </strong>and the <strong class="nm fr">NLG </strong>components are easy peasy to implement using LLMs since understanding and generating dialogue responses are their specialty.</p><p id="f9d2" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We can implement the Dialogue State Tracking (<strong class="nm fr">DST</strong>) and the Dialogue Policy Learning (<strong class="nm fr">DPL</strong>) by using LangChain’s <strong class="nm fr">SystemMessage </strong>to prime the AI behavior and always pass this message every time we interact with the LLM. The state of the dialogue should also always be passed to the LLM at every interaction with the model. This means that we will make sure the dialogue is always centered around the task we want the user to complete by <strong class="nm fr">always telling the LLM what the goal of the dialogue is and how it should behave.</strong> We’ll do that first by using a prompt:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="0d65" class="pu oh fq pr b bg pv pw l px py">prompt_system_task = """Your job is to gather information from the user about the User Story they need to create.<br/><br/>You should obtain the following information from them:<br/><br/>- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.<br/>- Success criteria the sucess criteria of the user story<br/>- Plan_of_execution: the plan of execution of the initiative<br/>- Deliverables: the deliverables of the initiative<br/><br/>If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. <br/>Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.<br/>Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. <br/>Always remind them that if they do not know how to answer something, you can help them.<br/><br/>After you are able to discern all the information, call the relevant tool."""</span></pre><p id="8d8d" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">And then appending this prompt everytime we send a message to the LLM:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="1d45" class="pu oh fq pr b bg pv pw l px py">def domain_state_tracker(messages):<br/>    return [SystemMessage(content=prompt_system_task)] + messages</span></pre><p id="1934" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Another important concept of our ToD system LLM implementation is<strong class="nm fr"> tool calling</strong>. If you read the last sentence of the <strong class="nm fr">prompt_system_task</strong> again it says “<em class="ph">After you are able to discern all the information, call the relevant tool</em>”. This way, we are telling the LLM that when it decides that the user provided all the User Story parameters, <strong class="nm fr">it should call the tool to create the User Story</strong>. Our tool for that will be created using a Pydantic model with the User Story parameters.</p><p id="f62f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">By using only the prompt and tool calling, we can control our ToD system. Beautiful right? Actually we also need to use <strong class="nm fr">the state of the graph</strong> to make all this work. Let’s do it in the next section, where we’ll finally build the ToD system.</p><h1 id="8e89" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Creating the dialogue system to build User Stories</h1><p id="901b" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">Alright, time to do some coding. First we’ll specify which LLM model we’ll use, then set the prompt and bind the tool to generate the User Story:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="5047" class="pu oh fq pr b bg pv pw l px py">import os<br/>from dotenv import load_dotenv, find_dotenv<br/><br/>from langchain_openai import AzureChatOpenAI<br/>from langchain_core.pydantic_v1 import BaseModel<br/>from typing import List, Literal, Annotated<br/><br/>_ = load_dotenv(find_dotenv()) # read local .env file<br/><br/>llm = AzureChatOpenAI(azure_deployment=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT_NAME"),<br/>                    openai_api_version="2023-09-01-preview",<br/>                    openai_api_type="azure",<br/>                    openai_api_key=os.environ.get('AZURE_OPENAI_API_KEY'),<br/>                    azure_endpoint=os.environ.get('AZURE_OPENAI_ENDPOINT'),<br/>                    temperature=0)<br/><br/>prompt_system_task = """Your job is to gather information from the user about the User Story they need to create.<br/><br/>You should obtain the following information from them:<br/><br/>- Objective: the goal of the user story. should be concrete enough to be developed in 2 weeks.<br/>- Success criteria the sucess criteria of the user story<br/>- Plan_of_execution: the plan of execution of the initiative<br/><br/>If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess. <br/>Whenever the user responds to one of the criteria, evaluate if it is detailed enough to be a criterion of a User Story. If not, ask questions to help the user better detail the criterion.<br/>Do not overwhelm the user with too many questions at once; ask for the information you need in a way that they do not have to write much in each response. <br/>Always remind them that if they do not know how to answer something, you can help them.<br/><br/>After you are able to discern all the information, call the relevant tool."""<br/><br/>class UserStoryCriteria(BaseModel):<br/>    """Instructions on how to prompt the LLM."""<br/>    objective: str<br/>    success_criteria: str<br/>    plan_of_execution: str<br/><br/>llm_with_tool = llm.bind_tools([UserStoryCriteria])</span></pre><p id="db4e" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">As we were talking earlier, the state of our graph consists only of the messages exchanged and a flag to know if the user story was created or not. Let’s create the graph first using <strong class="nm fr">StateGraph </strong>and this schema:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="7a84" class="pu oh fq pr b bg pv pw l px py">from langgraph.graph import StateGraph, START, END<br/>from langgraph.graph.message import add_messages<br/><br/>class StateSchema(TypedDict):<br/>    messages: Annotated[list, add_messages]<br/><br/>workflow = StateGraph(StateSchema)</span></pre><p id="c4e0" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The next image shows the structure of the final graph:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq qd"><img src="../Images/a2eb92c3d942ba8fb79c921ee2362301.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*7MtwrMSBJeak5s5xImVqGA.png"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">The structure of the ToD graph to create User Stories (image created by the author)</figcaption></figure><p id="0a14" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">At the top we have a <strong class="nm fr">talk_to_user</strong> node. This node can either:</p><ul class=""><li id="c5e6" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qe pj pk bk">Finalize the dialogue (go to the <strong class="nm fr">finalize_dialogue</strong> node)</li><li id="a6e6" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of qe pj pk bk">Decide that it’s time to wait for the user input (go to the <strong class="nm fr">END </strong>node)</li></ul><p id="49c0" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Since the main loop runs forever (<em class="ph">while True</em>), every time the graph reaches the END node, it waits for the user input again. This will become more clear when we create the loop.</p><p id="ee91" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Let’s create the nodes of the graph, starting with the <strong class="nm fr">talk_to_user </strong>node. This node needs to keep track of the task (maintaing the main prompt during all the conversation) and also keep the message exchanges because it’s where the state of the dialogue is stored. This state also keeps which parameters of the User Story are already filled or not using the messages. So this node should add the SystemMessage every time and append the new message from the LLM:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="71dc" class="pu oh fq pr b bg pv pw l px py">def domain_state_tracker(messages):<br/>    return [SystemMessage(content=prompt_system_task)] + messages<br/><br/>def call_llm(state: StateSchema):<br/>    """<br/>    talk_to_user node function, adds the prompt_system_task to the messages,<br/>    calls the LLM and returns the response<br/>    """<br/>    messages = domain_state_tracker(state["messages"])<br/>    response = llm_with_tool.invoke(messages)<br/>    return {"messages": [response]}</span></pre><p id="8fe6" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Now we can add the <strong class="nm fr">talk_to_user</strong> node to this graph. We’ll do that by giving it a name and then passing the function we’ve created:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="3d1b" class="pu oh fq pr b bg pv pw l px py">workflow.add_node("talk_to_user", call_llm)</span></pre><p id="7e37" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This node should be the first node to run in the graph, so let’s specify that with an <strong class="nm fr">edge</strong>:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="e637" class="pu oh fq pr b bg pv pw l px py">workflow.add_edge(START, "talk_to_user")</span></pre><p id="658b" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">So far the graph looks like this:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div class="mp mq qf"><img src="../Images/da757765b057ce82f01f6af98a41e93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*koHO_mpkliIUoTDYBvjWHQ.png"/></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Our graph with only one node (image created by the author)</figcaption></figure><p id="35d1" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">To control the flow of the graph, we’ll also use the message classes from LangChain. We have four types of messages:</p><ul class=""><li id="c015" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qe pj pk bk"><strong class="nm fr">SystemMessage: </strong>message for priming AI behavior</li><li id="40ee" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of qe pj pk bk"><strong class="nm fr">HumanMessage: </strong>message from a human</li><li id="728a" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of qe pj pk bk"><strong class="nm fr">AIMessage: </strong>the message returned from a chat model as a response to a prompt</li><li id="dedc" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of qe pj pk bk"><strong class="nm fr">ToolMessage: </strong>message containing the result of a tool invocation, used for passing the result of executing a tool back to a model</li></ul><p id="65b9" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">We’ll use <strong class="nm fr">the type</strong> of the last message of the<strong class="nm fr"> graph state</strong> to control the flow on the <strong class="nm fr">talk_to_user</strong> node. If the last message is an <strong class="nm fr"><em class="ph">AIMessage </em></strong>and it has the <strong class="nm fr"><em class="ph">tool_calls </em></strong>key, then we’ll go to the <strong class="nm fr">finalize_dialogue </strong>node because it’s time to create the User Story. Otherwise, we should go to the <strong class="nm fr">END </strong>node because we’ll restart the loop since it’s time for the user to answer.</p><p id="9ac3" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">The <strong class="nm fr">finalize_dialogue </strong>node should build the <strong class="nm fr">ToolMessage </strong>to pass the result to the model. The <strong class="nm fr">tool_call_id </strong>field is used to associate the tool call request with the tool call response. Let’s create this node and add it to the graph:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="6a43" class="pu oh fq pr b bg pv pw l px py">def finalize_dialogue(state: StateSchema):<br/>    """<br/>    Add a tool message to the history so the graph can see that it`s time to create the user story<br/>    """<br/>    return {<br/>        "messages": [<br/>            ToolMessage(<br/>                content="Prompt generated!",<br/>                tool_call_id=state["messages"][-1].tool_calls[0]["id"],<br/>            )<br/>        ]<br/>    }<br/><br/>workflow.add_node("finalize_dialogue", finalize_dialogue)</span></pre><p id="35d7" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Now let’s create the last node, the <strong class="nm fr">create_user_story </strong>one. This node will call the LLM using the prompt to create the User Story and the information that was gathered during the conversation. If the model decided that it was time to call the tool then the values of the key <strong class="nm fr">tool_calls </strong>should have all the info to create the User Story.</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="b9f6" class="pu oh fq pr b bg pv pw l px py">prompt_generate_user_story = """Based on the following requirements, write a good user story:<br/><br/>{reqs}"""<br/><br/>def build_prompt_to_generate_user_story(messages: list):<br/>    tool_call = None<br/>    other_msgs = []<br/>    for m in messages:<br/>        if isinstance(m, AIMessage) and m.tool_calls: #tool_calls is from the OpenAI API<br/>            tool_call = m.tool_calls[0]["args"]<br/>        elif isinstance(m, ToolMessage):<br/>            continue<br/>        elif tool_call is not None:<br/>            other_msgs.append(m)<br/>    return [SystemMessage(content=prompt_generate_user_story.format(reqs=tool_call))] + other_msgs<br/><br/><br/>def call_model_to_generate_user_story(state):<br/>    messages = build_prompt_to_generate_user_story(state["messages"])<br/>    response = llm.invoke(messages)<br/>    return {"messages": [response]}<br/><br/>workflow.add_node("create_user_story", call_model_to_generate_user_story)</span></pre><p id="e07f" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With all the nodes are created, it’s time to add the <strong class="nm fr">edges</strong>. We’ll add a conditional edge to the <strong class="nm fr">talk_to_user </strong>node. Remember that this node can either:</p><ul class=""><li id="5cca" class="nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of qe pj pk bk">Finalize the dialogue if it’s time to call the tool (go to the <strong class="nm fr">finalize_dialogue</strong> node)</li><li id="8760" class="nk nl fq nm b go pl no np gr pm nr ns nt pn nv nw nx po nz oa ob pp od oe of qe pj pk bk">Decide that we need to gather user input (go to the <strong class="nm fr">END </strong>node)</li></ul><p id="e9e5" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">This means that we’ll only check if the last message is an AIMessage and has the tool_calls key; otherwise we should go to the END node. Let’s create a function to check this and add it as an edge:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="71a0" class="pu oh fq pr b bg pv pw l px py">def define_next_action(state) -&gt; Literal["finalize_dialogue", END]:<br/>    messages = state["messages"]<br/><br/>    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:<br/>        return "finalize_dialogue"<br/>    else:<br/>        return END<br/><br/>workflow.add_conditional_edges("talk_to_user", define_next_action)</span></pre><p id="2167" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Now let’s add the other edges:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="8b89" class="pu oh fq pr b bg pv pw l px py">workflow.add_edge("finalize_dialogue", "create_user_story")<br/>workflow.add_edge("create_user_story", END)</span></pre><p id="ec16" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With that the graph workflow is done. Time to compile the graph and create the loop to run it:</p><pre class="ms mt mu mv mw pq pr ps bp pt bb bk"><span id="4ac0" class="pu oh fq pr b bg pv pw l px py">memory = MemorySaver()<br/>graph = workflow.compile(checkpointer=memory)<br/><br/>config = {"configurable": {"thread_id": str(uuid.uuid4())}}<br/><br/>while True:<br/>    user = input("User (q/Q to quit): ")<br/>    if user in {"q", "Q"}:<br/>        print("AI: Byebye")<br/>        break<br/>    output = None<br/>    for output in graph.stream(<br/>        {"messages": [HumanMessage(content=user)]}, config=config, stream_mode="updates"<br/>    ):<br/>        last_message = next(iter(output.values()))["messages"][-1]<br/>        last_message.pretty_print()<br/><br/>    if output and "create_user_story" in output:<br/>        print("User story created!")</span></pre><p id="aac1" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Let’s finally test the system:</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qg"><img src="../Images/f88a5dbb64636f7ecc2afae4a90569d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cD8uuOYdkK6f4hqiIgR0A.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">The assistant in action (image created by the author)</figcaption></figure><h1 id="0c90" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Final Thoughts</h1><p id="48e9" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">With LangGraph and LangChain we can build systems that guide users through structured interactions reducing the complexity to create them by using the LLMs to help us control the conditional logic.</p><p id="5512" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">With the combination of prompts, memory management, and tool calling we can create intuitive and effective dialogue systems, opening new possibilities for user interaction and task automation.</p><p id="5b75" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">I hope that this tutorial help you better understand how to use LangGraph (I’ve spend a couple of days banging my head on the wall to understand how all the pieces of the library work together).</p><p id="9b03" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">All the code of this tutorial can be found here: <a class="af nj" href="https://github.com/dmesquita/task_oriented_dialogue_system_langgraph" rel="noopener ugc nofollow" target="_blank">dmesquita/task_oriented_dialogue_system_langgraph (github.com)</a></p><p id="6c6e" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">Thanks for reading!</p><h1 id="0c0b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">References</h1><p id="377e" class="pw-post-body-paragraph nk nl fq nm b go pc no np gr pd nr ns nt pe nv nw nx pf nz oa ob pg od oe of fj bk">[1] Qin, Libo, et al. “End-to-end task-oriented dialogue: A survey of tasks, methods, and future directions.” <em class="ph">arXiv preprint arXiv:2311.09008</em> (2023).</p><p id="fa32" class="pw-post-body-paragraph nk nl fq nm b go nn no np gr nq nr ns nt nu nv nw nx ny nz oa ob oc od oe of fj bk">[2] <em class="ph">Prompt generation from user requirements</em>. Available at: <a class="af nj" href="https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting" rel="noopener ugc nofollow" target="_blank">https://langchain-ai.github.io/langgraph/tutorials/chatbots/information-gather-prompting</a></p></div></div></div></div>    
</body>
</html>