<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A visual explanation of LLM hyperparameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A visual explanation of LLM hyperparameters</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29">https://towardsdatascience.com/a-visual-explanation-of-llm-hyperparameters-daf61d3b006e?source=collection_archive---------3-----------------------#2024-08-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="26ab" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understand temperature, Top-k, Top-p, Frequency &amp; Precense Penalty once and for all.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jenn J." class="l ep by dd de cx" src="../Images/d2ef3b8f454d4f7a974edd5a965a80e8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*WlHipDuero5XpQkoxWyZYw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jenn-j-dev?source=post_page---byline--daf61d3b006e--------------------------------" rel="noopener follow">Jenn J.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--daf61d3b006e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6215" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Getting a handle on temperature, Top-k, Top-p, frequency, and presence penalties can be a bit of a challenge, especially when you’re just starting out with LLM hyperparameters. Terms like “Top-k” and “presence penalty” can feel a bit overwhelming at first.</p><p id="b3ee" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When you look up “Top-k,” you might find a definition like: “Top-k sampling limits the model’s selection of the next word to only the top-k most probable options, based on their predicted probabilities.” That’s a lot to take in! But how does this actually help when you’re working on prompt engineering?</p><p id="3e92" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you’re anything like me and learn best with visuals, let’s break these down together and make these concepts easy to understand once and for all.</p><h1 id="79a3" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">LLMs under the hood</h1><p id="5a55" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Before we dive into LLM hyperparameters, let’s do a quick thought experiment. Imagine hearing the phrase “A cup of …”. Most of us would expect the next word to be something like “coffee” (or “tea” if you’re a tea person!) You probably wouldn’t think of “stars” or “courage” right away.</p><p id="496b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">What’s happening here is that we’re instinctively <em class="of">predicting</em> the most likely words to follow “A cup of …”, with “coffee” being a much higher likelihood than “stars”.</p><p id="84f4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is similar to how LLMs work — they calculate the probabilities of possible next words and choose one based on those probabilities.</p><p id="55b6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So on a high level, the hyperparameters are ways to tune <em class="of">how we select the next probable words</em>.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cc86" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let’s start with the most common hyperparameter:</p><h1 id="8b34" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Temperature</h1><p id="a281" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Temperature controls the randomness of the models’ output. A lower temperature makes the output more deterministic, favoring more likely words, while a higher temperature allows for more creativity by considering less likely words.</p><p id="8a7d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In our “A cup of…” example, setting the temperature to 0 makes the model favor the most likely word, which is “coffee”.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op oq"><img src="../Images/869825d95b5bfdc9c79b14e9d1b13356.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHNwK9UBbrvVe8_Idcu7EA.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image provided by the author</figcaption></figure><p id="e3fc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As temperature increases, the sampling probabilities between different words start to even out, prompting the model to generate highly unusal or unexpected outputs.</p><p id="fd66" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Note, that setting the temperature to 0 still doesn’t make the model completely deterministic, though it gets very close.</p><p id="c2d4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases</strong></p><ul class=""><li id="0c1e" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Low temperature (e.g. 0.2): Ideal for tasks requiring precise and predictable results, such as technical writing or formal documentation.</li><li id="f195" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">High temperature (e.g. 0.8 or above): Useful for creative tasks like storytelling, poetry, or brainstorming</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a40b" class="ne nf fq bf ng nh pp gq nj nk pq gt nm nn pr np nq nr ps nt nu nv pt nx ny nz bk">Max Tokens</h1><p id="9999" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Max tokens define the maximum number of tokens (which can be words or parts of words) the model can generate in its responses. Tokens are the smallest units of text that a model processes.</p><p id="d8e3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Relationship between tokens and words:</strong></p><ul class=""><li id="71e6" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">1 word = 1~2 tokens: In English, a typical word is usually split into 1 to 2 tokens. For example, simple words like “cat” might be a single token, while more complex words like “unbelievable” might be split into multiple tokens.</li><li id="0492" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">The general rule of thumb: You can roughly estimate the number of words by dividing tokens by 1.5 (as a rough average).</li></ul><p id="e159" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases</strong></p><ul class=""><li id="b6af" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Low max tokens (e.g. 50): Ideal for tasks requiring brief responses, such as headlines, short summaries, or concise answers. (Be careful that the model might cut off the output response)</li><li id="4372" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">High max tokens (e.g. 500): Useful for generating longer content like articles, stories, or detailed explanations.</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2245" class="ne nf fq bf ng nh pp gq nj nk pq gt nm nn pr np nq nr ps nt nu nv pt nx ny nz bk">Top-k</h1><p id="e49e" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Top-k sampling restricts the model from selecting from the top <em class="of">k</em> most likely next words. By narrowing the choices, it helps reduce the chances of generating irrelevant or nonsensical outputs.</p><p id="6235" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the diagram below, if we set <em class="of">k</em> to 2, the model will only consider the two most likely next words — in this case, ‘coffee’ and ‘courage.’ These two words are then resampled, with their probabilities adjusted to sum to 1, ensuring one of them is chosen.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op pu"><img src="../Images/72bad6505f466125adcde5e07054ea70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*374nBkpu2vH81XxbKD96Bg.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image provided by the author.</figcaption></figure><p id="d458" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases:</strong></p><ul class=""><li id="452c" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Low k (e.g., k=10): Best for structured tasks where you want to maintain focus and coherence, such as summarization or coding.</li><li id="8600" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">High k (e.g., k=50): Suitable for creative or exploratory tasks where you want to introduce more variability without losing coherence.</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0e78" class="ne nf fq bf ng nh pp gq nj nk pq gt nm nn pr np nq nr ps nt nu nv pt nx ny nz bk">Top-p</h1><p id="d6aa" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Top-p sampling selects the smallest set of words whose combined probability exceeds a threshold <em class="of">p</em> (e.g., 0.9), allowing for a more context-sensitive choice of words.</p><p id="4c77" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the diagram below, we start with the most probable word, ‘coffee,’ which has a probability of 0.6. Since this is less than our threshold of <em class="of">p</em> = 0.9, we add the next word, ‘courage,’ with a probability of 0.2. Together, these give us a total probability of 0.8, which is still below 0.9. Finally, we consider the word ‘dreams’ with a probability of 0.13, bringing the total to 0.93, which exceeds 0.9. At this point, we stop, having selected the first two most probable words.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op pv"><img src="../Images/9444be711d7c56d878173c9fa83df048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqKeizk-ODwudYjnQrO3VA.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image provided by the author.</figcaption></figure><p id="630a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases:</strong></p><ul class=""><li id="fc8e" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Low p (e.g., p=0.5): Effective for tasks that require concise and to-the-point outputs, like news headlines or instructional text.</li><li id="f424" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">High p (e.g., p=0.95): Useful for more open-ended tasks, such as dialogue generation or creative content, where a wider variety of responses is desirable.</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bed1" class="ne nf fq bf ng nh pp gq nj nk pq gt nm nn pr np nq nr ps nt nu nv pt nx ny nz bk">Frequency Penalty</h1><p id="945e" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">A frequency penalty reduces the likelihood of the model repeating the same word within the text, promoting diversity and minimizing redundancy in the output. By applying this penalty, the model is encouraged to introduce new words instead of reusing ones that have already appeared.</p><p id="3c49" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The frequency penalty is calculated using the formula:</p><p id="7b06" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Adjusted probability = initial probability / (1 + frequency penalty * count of appearance)</strong></p><p id="6792" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For example, let’s say that the word “sun” has a probability of 0.5, and it has already appeared twice in the text. If we set the frequency penalty to 1, the adjusted probability for “sun” would be:</p><p id="63f0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Adjusted probability = 0.5 / (1 + 1 * 2) = 0.5 / 3 = 0.16</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op pw"><img src="../Images/416077203419aadf31c7749d6fa2f540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z1VeRHdraE9AXrJTCoWlcw.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image provided by the author.</figcaption></figure><p id="dbe8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases:</strong></p><ul class=""><li id="8c08" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">High Penalty (e.g., 1.0): Ideal for generating content where repetition would be distracting or undesirable, such as essays or research papers.</li><li id="586f" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">Low Penalty (e.g., 0.0): Useful when repetition might be necessary or beneficial, such as in poetry, mantras, or certain marketing slogans.</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f1dc" class="ne nf fq bf ng nh pp gq nj nk pq gt nm nn pr np nq nr ps nt nu nv pt nx ny nz bk">Presence Penalty</h1><p id="765a" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">The presence penalty is similar to the frequency penalty but with one key difference: it penalizes the model for reusing any word or phrase that has already been mentioned, <em class="of">regardless of how often it appears</em>.</p><p id="483e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In other words, repeating the word 2 times is as bad as repeating it 20 times.</p><p id="9841" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The formula for adjusting the probability with a presence penalty is:<br/><strong class="mk fr">Adjusted probability = initial probability / (1 + presence penalty * presence)</strong></p><p id="1ca9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let’s revisit the earlier example with the word “sun”. Instead of multiplying the penalty by the frequency of how many times “sun” has appeared, we simply check whether it has appeared at all — in this case, it has, so we count it as 1.</p><p id="cdc1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If we set the presence penalty to 1, the adjusted probability would be:</p><p id="6bb4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Adjusted probability = 0.5 / (1 + 1 * 1) = 0.5 / 2 = 0.25</p><p id="12b7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This reduction makes it less likely for the model to choose “sun” again, encouraging the use of new words or phrases, even if “sun” has only appeared once in the text.</p><figure class="or os ot ou ov ow oo op paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="oo op px"><img src="../Images/ebaf6807ca9661e8deec2354ea0a7683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoVtpFQKzSPbMAWAnaml3A.png"/></div></div><figcaption class="pc pd pe oo op pf pg bf b bg z dx">Image provided by author.</figcaption></figure><p id="07f4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Use cases:</strong></p><ul class=""><li id="51b3" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">High Penalty (e.g., 1.0): Great for exploratory or brainstorming sessions where you want the model to keep introducing new ideas or topics.</li><li id="c316" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">Low Penalty (e.g., 0.0): Suitable for tasks where reinforcement of key terms or ideas is important, such as technical documentation or instructional material.</li></ul><h1 id="4607" class="ne nf fq bf ng nh ni gq nj nk nl gt nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Frequency and Presence Penalties often go hand-in-hand</h1><p id="dd69" class="pw-post-body-paragraph mi mj fq mk b go oa mm mn gr ob mp mq mr oc mt mu mv od mx my mz oe nb nc nd fj bk">Now that we’ve gone over the basics, let’s dive into how frequency and presence penalties are often used together. Just a heads-up, though — they’re powerful tools, but it’s important to use them with a bit of caution to get the best results.</p><p id="f852" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">When to use them:</strong></p><ul class=""><li id="d7ea" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Content Generation</li><li id="27ec" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">Preventing Redundancy</li></ul><p id="d3d6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">When to not use them</strong></p><ul class=""><li id="1f93" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ph pi pj bk">Technical Writing: In technical documentation or specific instructions where consistent terminology is crucial, using these penalties might be counterproductive.</li><li id="583e" class="mi mj fq mk b go pk mm mn gr pl mp mq mr pm mt mu mv pn mx my mz po nb nc nd ph pi pj bk">Brand messaging: If you’re generating content that relies heavily on a specific brand tone or key phrases, reducing repetition might dilute the brand’s voice.</li></ul></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="afe9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">By now, you should have a clearer picture of how temperature, Top-k, Top-p, frequency, and presence penalties work together to shape the output of your language model. And if it still feels a bit tricky, that’s totally okay — these concepts can take some time to fully click. Just keep experimenting and exploring, and you’ll get the hang of it before you know it.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e676" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you find visual content like this helpful and want more, we’d love to see you in our <a class="af py" href="https://discord.gg/ETGYYsswTb" rel="noopener ugc nofollow" target="_blank">Discord community.</a> It’s a space where we share ideas, help each other out, and learn together.</p></div></div></div></div>    
</body>
</html>