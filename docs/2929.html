<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Interpret Matrix Expressions — Transformations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Interpret Matrix Expressions — Transformations</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=collection_archive---------2-----------------------#2024-12-04">https://towardsdatascience.com/how-to-interpret-matrix-expressions-transformations-a5e6871cd224?source=collection_archive---------2-----------------------#2024-12-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bb31" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Matrix algebra for a data scientist</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jaroslaw.drapala?source=post_page---byline--a5e6871cd224--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jaroslaw Drapala" class="l ep by dd de cx" src="../Images/34de3c52fc32005e36930135254ae45e.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*UOcQjVU5X3yqZH0NoxpOpA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a5e6871cd224--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jaroslaw.drapala?source=post_page---byline--a5e6871cd224--------------------------------" rel="noopener follow">Jaroslaw Drapala</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a5e6871cd224--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">23 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4070d1bfcdecc387b4685ebc7c1ab93b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*57Fx9wZ-ds46JqFq"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@ballonandon?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ben Allan</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="de4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>his article begins a series for anyone who finds matrix algebra overwhelming. My goal is to turn <em class="oi">what you’re afraid of</em> into <em class="oi">what you’re fascinated by</em>. You’ll find it especially helpful if you want <strong class="nf fr">to understand machine learning concepts and methods</strong>.</p><h2 id="3aed" class="oj ok fq bf ol om on oo op oq or os ot nm ou ov ow nq ox oy oz nu pa pb pc pd bk"><strong class="al">Table of contents:</strong></h2><ol class=""><li id="d07b" class="nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny pj pk pl bk">Introduction</li><li id="583b" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Prerequisites</li><li id="2e17" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Matrix-vector multiplication</li><li id="0d89" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Transposition</li><li id="a621" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Composition of transformations</li><li id="5302" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Inverse transformation</li><li id="0c51" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Non-invertible transformations</li><li id="ceb3" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Determinant</li><li id="cde9" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Non-square matrices</li><li id="9ec2" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Inverse and Transpose: similarities and differences</li><li id="2693" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Translation by a vector</li><li id="3f71" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny pj pk pl bk">Final words</li></ol><h1 id="e9e1" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">1. Introduction</h1><p id="ba07" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You’ve probably noticed that while it’s easy to find materials explaining matrix computation algorithms, it’s harder to find ones that teach <strong class="nf fr">how to interpret complex matrix expressions</strong>. I’m addressing this gap with my series, focused on <strong class="nf fr">the part of matrix algebra that is most commonly used by data scientists</strong>.</p><p id="82ec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We’ll focus more on concrete examples rather than general formulas. I’d rather sacrifice generality for the sake of clarity and readability. I’ll often appeal to your imagination and intuition, hoping my materials will inspire you to explore more formal resources on these topics. For precise definitions and general formulas, I’d recommend you look at some good textbooks: the classic one on linear algebra¹ and the other focused on machine learning².</p><p id="919a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This part will teach you</p><blockquote class="qj"><p id="ce3c" class="qk ql fq bf qm qn qo qp qq qr qs ny dx">to see a matrix as a representation of the transformation applied to data.</p></blockquote><p id="cd8c" class="pw-post-body-paragraph nd ne fq nf b go qt nh ni gr qu nk nl nm qv no np nq qw ns nt nu qx nw nx ny fj bk">Let’s get started then — let me take the lead through the world of matrices.</p><h1 id="a05c" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">2. Prerequisites</h1><p id="44e6" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">I’m guessing you can handle the expressions that follow.</p><p id="7a8d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is <strong class="nf fr">the dot product</strong> written using a row vector and a column vector:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/4cc9cca6b5d353563d474f66a9748940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZJjpe-Ca_-UZMzhLHyrkw.png"/></div></div></figure><p id="f4dc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">A matrix</strong> is a rectangular array of symbols arranged in rows and columns. Here is an example of a matrix with two rows and three columns:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/73ceb88688619cab30df270d88da304c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhaXuuwdxo2zTIcnrOmmvw.png"/></div></div></figure><p id="89fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can view it as <strong class="nf fr">a sequence of columns</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/52f93ad2eec4d242106deb5d97e6d084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qbuf2DkAoanB15iIcru8ZA.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/744dcfeea4c9ef2f971961a72eb40350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MbP2blUaomHoKiYRRwoc0A.png"/></div></div></figure><p id="16de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">or <strong class="nf fr">a sequence of rows</strong> stacked one on top of another:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/08bd1c9f80d39bb52e42131cb03da09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaRvptlxqYhs2vJ_fEGJqg.png"/></div></div></figure><p id="3663" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As you can see, I used superscripts for rows and subscripts for columns. In machine learning, it’s important to clearly distinguish between observations, represented as vectors, and features, which are arranged in rows.</p><p id="5356" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Other interesting ways to represent this matrix are <strong class="nf fr">A</strong><em class="oi">₂</em>ₓ<em class="oi">₃ </em>and <strong class="nf fr">A</strong>[<em class="oi">aᵢ</em>⁽<em class="oi">ʲ </em>⁾].</p><p id="1fb7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Multiplying</strong> two matrices <strong class="nf fr">A </strong>and <strong class="nf fr">B </strong>results in a third matrix <strong class="nf fr">C </strong>= <strong class="nf fr">AB</strong> containing the scalar products of each row of <strong class="nf fr">A </strong>with each column of <strong class="nf fr">B</strong>, arranged accordingly. Below is an example for <strong class="nf fr">C</strong><em class="oi">₂</em>ₓ<em class="oi">₂</em><strong class="nf fr"> </strong>= <strong class="nf fr">A</strong><em class="oi">₂</em>ₓ<em class="oi">₃</em><strong class="nf fr">B</strong><em class="oi">₃</em>ₓ<em class="oi">₂.</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/e80a49676c39ee0ae27f395a452273c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ChtiiLBMCSRu770ChVGkA.png"/></div></div></figure><p id="f190" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where c<em class="oi">ᵢ</em>⁽<em class="oi">ʲ </em>⁾ is the scalar product of the <em class="oi">i</em>-th column of the matrix <strong class="nf fr">B</strong> and the <em class="oi">j</em>-th row of matrix <strong class="nf fr">A</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/7346d6bad0c3048bd4216aa3384a1ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QClUtix68I2z3Dr291luA.png"/></div></div></figure><p id="4107" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note that this definition of multiplication requires the number of rows of <em class="oi">the left matrix</em> to match the number of columns of <em class="oi">the right matrix</em>. In other words, <strong class="nf fr">the inner dimensions of the matrices must match</strong>.</p><p id="99e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Make sure you can manually multiply matrices with arbitrary entries. You can use the following code to check the result or to practice multiplying matrices.</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="3993" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/># Matrices to be multiplied<br/>A = [<br/>    [ 1, 0, 2],<br/>    [-2, 1, 1]<br/>]<br/><br/>B = [<br/>    [ 0, 3],<br/>    [-3, 1],<br/>    [-2, 2]<br/>]<br/><br/># Convert to numpy array<br/>A = np.array(A)<br/>B = np.array(B)<br/><br/># Multiply A by B (if possible)<br/>try:<br/>    C = A @ B<br/>    print(f'A B = \n{C}\n')<br/>except:<br/>    print("""ValueError:<br/>The number of rows in matrix A does not match <br/>the number of columns in matrix B<br/>""")<br/><br/>#  and in the reverse order, B by A (if possible)<br/>try:<br/>    D = B @ A<br/>    print(f'B A =\n{D}')<br/>except:<br/>    print("""ValueError:<br/>The number of rows in matrix B does not match <br/>the number of columns in matrix A<br/>""")</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="47d6" class="ri ok fq rf b bg rj rk l rl rm">A B = <br/>[[-4  7]<br/> [-5 -3]]<br/><br/>B A =<br/>[[-6  3  3]<br/> [-5  1 -5]<br/> [-6  2 -2]]</span></pre><h1 id="3507" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">3. Matrix-vector multiplication</h1><p id="a77c" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">In this section, I will explain the effect of matrix multiplication on vectors. The vector <strong class="nf fr">x</strong> is multiplied by the matrix <strong class="nf fr">A</strong>, producing a new vector <strong class="nf fr">y</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ro"><img src="../Images/a9ad5f2b2d114991258d0e67129daed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozDrOJvawV8hJnfRVV1lNA.png"/></div></div></figure><p id="5955" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is a common operation in data science, as it enables <strong class="nf fr">a linear transformation of data</strong>. The use of matrices to represent linear transformations is highly advantageous, as you will soon see in the following examples.</p><p id="83e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Below, you can see your grid space and your standard basis vectors: blue for the <em class="oi">x</em>⁽¹⁾ direction and magenta for the <em class="oi">x</em>⁽²⁾ direction.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/ede926b8ba98086d998573ee431a351e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UhFmLhFLHnuC28sMGasT8w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Standard basis in a Grid Space</figcaption></figure><p id="358b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A good starting point is to work with transformations that map two-dimensional vectors <strong class="nf fr">x</strong> into two-dimensional vectors <strong class="nf fr">y</strong> in the same grid space.</p><blockquote class="rq rr rs"><p id="1a66" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Describing the desired transformation is a simple trick. You just need to say how the coordinates of the basis vectors change after the transformation and use these new coordinates as the columns of the matrix <strong class="nf fr">A.</strong></p></blockquote><p id="fce2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As an example, consider a linear transformation that produces the effect illustrated below. The standard basis vectors are drawn lightly, while the transformed vectors are shown more clearly.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/f09b964655b91e8cb825ac84b1bad995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLmw2TcmgaPXSQUjf8PMWg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Standard basis transformed by matrix <strong class="bf ol">A</strong></figcaption></figure><p id="b98a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the comparison of the basis vectors before and after the transformation, you can observe that the transformation involves a 45-degree counterclockwise rotation about the origin, along with an elongation of the vectors.</p><p id="bfb9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This effect can be achieved using the matrix <strong class="nf fr">A</strong>, composed as follows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rt"><img src="../Images/f66cf341722a3e4849efd40ec7b764d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxVObeo1hmb9s5QRjhJ5AQ.png"/></div></div></figure><p id="2db3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The first column of the matrix contains the coordinates of the first basis vector after the transformation, and the second column contains those of the second basis vector.</p><p id="f380" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The equation (1) then takes the form</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ru"><img src="../Images/6e4894b7838ae46f335906226c11b293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1eR1hdtG7FMFRc9TArhvuw.png"/></div></div></figure><p id="fa97" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s take two example points <strong class="nf fr">x</strong>₁and <strong class="nf fr">x</strong>₂ :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rv"><img src="../Images/1c42eca1c9c3c50fef3476bf341dc03a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gDhnZiCTRv7BZg3imR8mqQ.png"/></div></div></figure><p id="4937" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and transform them into the vectors <strong class="nf fr">y</strong>₁​ and <strong class="nf fr">y</strong>₂ :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rw"><img src="../Images/641645e3f7274bd47d636b82ec3792ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5A7vMKfJHPfXMt-_Ehu0KA.png"/></div></div></figure><p id="6109" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I encourage you to do these calculations by hand first, and then switch to using a program like this:</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="f8a6" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/># Transformation matrix<br/>A = np.array([<br/>    [1, -1],<br/>    [1,  1]<br/>])<br/><br/># Points (vectors) to be transformed using matrix A<br/>points = [<br/>    np.array([1, 1/2]),<br/>    np.array([-1/4, 5/4])<br/>]<br/><br/># Print out the transformed points (vectors)<br/>for i, x in enumerate(points):<br/>    y = A @ x<br/>    print(f'y_{i} = {y}')</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="6b66" class="ri ok fq rf b bg rj rk l rl rm">y_0 = [0.5 1.5]<br/>y_1 = [-1.5  1. ]</span></pre><p id="d318" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The plot below shows the results.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/620eb5e9122de60f3e382ce50c0b132b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGgoW-nTpAVBqOYUl2BUsw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Points transformed by matrix <strong class="bf ol">A</strong></figcaption></figure><p id="8243" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <strong class="nf fr">x</strong> points are gray and smaller, while their transformed counterparts <strong class="nf fr">y</strong> have black edges and are bigger. If you’d prefer to think of these points as arrowheads, here’s the corresponding illustration:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/dd158ce434f6441a15216165f2c6c75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7TA3I-dmrCdgCvbp9Gvraw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Vectors transformed by matrix <strong class="bf ol">A</strong></figcaption></figure><p id="f327" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now you can see more clearly that the points have been rotated around the origin and pushed a little away.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e05b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s examine another matrix:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sf"><img src="../Images/919cdaab273f68fd87846d0a6341dafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jjW2j7gXTPkLcI5JNOAgw.png"/></div></div></figure><p id="61db" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and see how the transformation</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sg"><img src="../Images/64340ef07ff0a3727dc510b69eef38d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KA92sPAt4MtpjAyYIC4nIg.png"/></div></div></figure><p id="7fc5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">affects the points on the grid lines:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/47bdd0080d88de5d02d7215b668a6ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4ExyXEOXf6BK9zuIZaicQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Grid lines transformed by matrix <strong class="bf ol">B</strong></figcaption></figure><p id="0eb6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Compare the result with that obtained using <strong class="nf fr">B</strong>/2, which corresponds to dividing all elements of the matrix <strong class="nf fr">B</strong> by 2:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/257a75bda25f1cc28659d6452bd54163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFDcpD5v07TstcvCCD9Pog.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Grid lines transformed by matrix <strong class="bf ol">B</strong>/2</figcaption></figure><p id="45a0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In general, a linear transformation:</p><ul class=""><li id="d1f1" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny si pk pl bk">ensures that straight lines remain straight,</li><li id="4227" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny si pk pl bk">keeps parallel lines parallel,</li><li id="47bd" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny si pk pl bk">scales the distances between them by a uniform factor.</li></ul><p id="4c91" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To keep things concise, I’ll use ‘<em class="oi">transformation </em><strong class="nf fr"><em class="oi">A</em></strong>‘ throughout the text instead of the full phrase ‘<em class="oi">transformation represented by matrix </em><strong class="nf fr"><em class="oi">A</em></strong>’.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="01a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s return to the matrix</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sf"><img src="../Images/919cdaab273f68fd87846d0a6341dafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jjW2j7gXTPkLcI5JNOAgw.png"/></div></div></figure><p id="d3e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and apply the transformation to a few sample points.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/70fa29bb0ad01ed57cce3b360d5f3e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RW0xa1Vwtb8g_skReutz9g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The effects of transformation <strong class="bf ol">B</strong> on various input vectors</figcaption></figure><p id="4b21" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice the following:</p><ul class=""><li id="10a7" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny si pk pl bk">point <strong class="nf fr">x</strong>₁​ has been rotated counterclockwise and brought closer to the origin,</li><li id="bb34" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny si pk pl bk">point <strong class="nf fr">x</strong>₂​, on the other hand, has been rotated clockwise and pushed away from the origin,</li><li id="ee54" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny si pk pl bk">point <strong class="nf fr">x</strong>₃​ has only been scaled down, meaning it’s moved closer to the origin while keeping its direction,</li><li id="d769" class="nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny si pk pl bk">point <strong class="nf fr">x</strong>₄ has undergone a similar transformation, but has been scaled up.</li></ul><p id="fa20" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The transformation compresses in the <em class="oi">x</em>⁽¹⁾-direction and stretches in the <em class="oi">x</em>⁽²⁾-direction. You can think of the grid lines as behaving like an accordion.</p><p id="1796" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Directions such as those represented by the vectors <strong class="nf fr">x</strong>₃ and <strong class="nf fr">x</strong>₄ play an important role in machine learning, but that’s a story for another time.</p><p id="0d7e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For now, we can call them <strong class="nf fr"><em class="oi">eigen-directions</em></strong>, because vectors along these directions might only be scaled by the transformation, without being rotated. Every transformation, except for rotations, has its own set of eigen-directions.</p><h1 id="e3db" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">4. Transposition</h1><p id="2e3a" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Recall that the transformation matrix is constructed by stacking the transformed basis vectors in columns. Perhaps you’d like to see what happens if we <strong class="nf fr">swap the rows and columns</strong> afterwards (the transposition).</p><p id="7ec0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let us take, for example, the matrix</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sj"><img src="../Images/1c78255ae8debe430d0f94d514b8c906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNsRF2B5TGfS99qjd-bWJA.png"/></div></div></figure><p id="d13d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <strong class="nf fr">A</strong>ᵀ stands for the transposed matrix.</p><p id="f35c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From a geometric perspective, <em class="oi">the coordinates of the first</em> new basis vector come from <em class="oi">the first coordinates of all</em> the old basis vectors, the second from the second coordinates, and so on.</p><p id="1bb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In NumPy, it’s as simple as that:</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="4add" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/>A = np.array([<br/>    [1, -1],<br/>    [1 , 1]<br/>    ])<br/><br/>print(f'A transposed:\n{A.T}')</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="fca1" class="ri ok fq rf b bg rj rk l rl rm">A transposed:<br/>[[ 1  1]<br/> [-1  1]]</span></pre><p id="6bbb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I must disappoint you now, as I cannot provide a simple rule that expresses the relationship between the transformations <strong class="nf fr">A</strong> and <strong class="nf fr">A</strong>ᵀ in just a few words.</p><p id="6f2d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Instead, let me show you a property shared by both the original and transposed transformations, which will come in handy later.</p><p id="624c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here is the geometric interpretation of the transformation represented by the matrix <strong class="nf fr">A</strong>. The area shaded in gray is called <strong class="nf fr">the parallelogram</strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sk"><img src="../Images/cc4da6b270ad59593b66ecd3c2ef9aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fUU7d7iV2PFC5g1oyc6Sw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Parallelogram spanned by the basis vectors transformed by matrix <strong class="bf ol">A</strong></figcaption></figure><p id="17c1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Compare this with the transformation obtained by applying the matrix <strong class="nf fr">A</strong>ᵀ:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sk"><img src="../Images/583dbc483553575dedb38801354a87bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WsE7AkmpjID29RP3ykgwQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Parallelogram spanned by the basis vectors transformed by matrix <strong class="bf ol">A</strong>ᵀ</figcaption></figure><p id="e162" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let us consider another transformation that applies entirely different scales to the unit vectors:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sl"><img src="../Images/a1f9c9f8bf7aa8baf397057e38941807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jw9jY8mm7R0uAQ_H_5ulDA.png"/></div></div></figure><p id="99ce" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The parallelogram associated with the matrix <strong class="nf fr">B</strong> is much narrower now:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/ce38dffa87078dc19015a4f85f9eff0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtVQfsbsF0-u1qIu4QGQIQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Parallelogram spanned by the basis vectors transformed by matrix <strong class="bf ol">B</strong></figcaption></figure><p id="e82d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">but it turns out that it is the same size as that for the matrix <strong class="nf fr">B</strong>ᵀ:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/cc8e5ec58859440e7782dd669d504972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViAAZisJ-I-lL0CItlUzoQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Parallelogram spanned by the basis vectors transformed by matrix <strong class="bf ol">B</strong>ᵀ</figcaption></figure><p id="3ac8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let me put it this way: you have a set of numbers to assign to the components of your vectors. If you assign a larger number to one component, you’ll need to use smaller numbers for the others. In other words, the total length of the vectors that make up the parallelogram stays the same. I know this reasoning is a bit vague, so if you’re looking for more rigorous proofs, check the literature in the references section.</p><p id="22c5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And here’s the kicker at the end of this section: the area of the parallelograms can be found by calculating <strong class="nf fr">the determinant</strong> of the matrix. What’s more, <em class="oi">the determinant of the matrix and its transpose are identical.</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sm"><img src="../Images/bf71b3faf8c4ed8e5cefabaf78bd14c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rlJHShHX_RCriBz-vEzSw.png"/></div></div></figure><p id="1feb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">More on the determinant in the upcoming sections.</p><h1 id="cdb8" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">5. Composition of transformations</h1><p id="7a74" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You can apply a sequence of transformations — for example, start by applying <strong class="nf fr">A</strong> to the vector <strong class="nf fr">x</strong>, and then pass the result through <strong class="nf fr">B</strong>. This can be done by first multiplying the vector <strong class="nf fr">x </strong>by the matrix <strong class="nf fr">A</strong>, and then multiplying the result by the matrix <strong class="nf fr">B</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sn"><img src="../Images/73685f1fe740347c23835e6168605d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QlSiFeHzuh2kblRa9AOMTA.png"/></div></div></figure><p id="797c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can multiply the matrices <strong class="nf fr">B </strong>and <strong class="nf fr">A </strong>to obtain the matrix <strong class="nf fr">C</strong> for further use:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk so"><img src="../Images/a037c7b0cac6eeaba19b52edb0676319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edOo4oS7qlXfJwvAhSuezw.png"/></div></div></figure><p id="6664" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is the effect of the transformation represented by the matrix <strong class="nf fr">C</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/02125514614c2a36bce4c5c3554da5eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vw8-k6-imP46lDSUecKBTg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation described by the composite matrix <strong class="bf ol">BA</strong></figcaption></figure><p id="6123" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can perform the transformations in reverse order: first apply <strong class="nf fr">B</strong>, then apply <strong class="nf fr">A</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sn"><img src="../Images/0c26a616bd0701edcb29b3ed70669815.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu0GCOmhTuBRZ4h4bSSRhA.png"/></div></div></figure><p id="f7f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let <strong class="nf fr">D</strong> represent the sequence of multiplications performed in this order:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sp"><img src="../Images/fdfb4328d6ef70c8e8a91f2804af571e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9J3xOj28SqypdiEYjMaoA.png"/></div></div></figure><p id="65de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And this is how it affects the grid lines:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/0c89d832d86f8f2760a4d66e95a16555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBfKXUuVm0J268FIZL6yxQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation described by the composite matrix <strong class="bf ol">AB</strong></figcaption></figure><p id="3655" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">So, you can see for yourself that <strong class="nf fr">the order of matrix multiplication matters</strong>.</p><p id="c84f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There’s a cool property with <strong class="nf fr">the transpose of a composite transformation</strong>. Check out what happens when we multiply <strong class="nf fr">A </strong>by <strong class="nf fr">B</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/e80a49676c39ee0ae27f395a452273c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ChtiiLBMCSRu770ChVGkA.png"/></div></div></figure><p id="515d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and then transpose the result, which means we’ll apply (<strong class="nf fr">AB</strong>)ᵀ:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sq"><img src="../Images/79c2894f91e4ef014cd9c69a778c1421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICWKPywsopTrYJwslM_s8w.png"/></div></div></figure><p id="cc29" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can easily extend this observation to the following rule:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sr"><img src="../Images/7052600e654ee7c2d565c38b444116f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5Yq2EWfffwrrUqk7TpLQg.png"/></div></div></figure><p id="99b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To finish off this section, consider the inverse problem: is it possible to recover matrices <strong class="nf fr">A</strong> and <strong class="nf fr">B</strong> given only <strong class="nf fr">C </strong>= <strong class="nf fr">AB</strong>?</p><p id="e62b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is <strong class="nf fr">matrix factorization</strong>, which, as you might expect, doesn’t have a unique solution. Matrix factorization is a powerful technique that can provide insight into transformations, as they may be expressed as a composition of simpler, elementary transformations. But that’s a topic for another time.</p><h1 id="89b9" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">6. Inverse transformation</h1><p id="c0bd" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You can easily construct a matrix representing a <strong class="nf fr">do-nothing transformation</strong> that leaves the standard basis vectors unchanged:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ss"><img src="../Images/028f2bbe2eefcad91f2fb3710204af8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_f-Gha50m-o5RLF-VBw6Q.png"/></div></div></figure><p id="c50c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is commonly referred to as <strong class="nf fr">the identity matrix</strong>.</p><p id="26dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take a matrix <strong class="nf fr">A</strong> and consider the transformation that undoes its effects. The matrix representing this transformation is <strong class="nf fr">A</strong>⁻¹. Specifically, when applied after or before <strong class="nf fr">A</strong>, it yields the identity matrix <strong class="nf fr">I</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk st"><img src="../Images/278a35da36dddd4d8ee7104d5b3d5733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euWd0sl3KFAAywo2AkKE2A.png"/></div></div></figure><p id="c160" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">There are many resources that explain how to calculate the inverse by hand. I recommend learning <a class="af nc" href="https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html" rel="noopener ugc nofollow" target="_blank">Gauss-Jordan method</a> because it involves simple row manipulations on the augmented matrix. At each step, you can swap two rows, rescale any row, or add to a selected row a weighted sum of the remaining rows.</p><p id="4adf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take the following matrix as an example for hand calculations:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk su"><img src="../Images/5dcbd283762d34da02b7985a74828053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t71Pc8v18V88WhaxdHmp1w.png"/></div></div></figure><p id="0172" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You should get the inverse matrix:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sv"><img src="../Images/04d869f86b763751c24a5bfdf843fd18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGsMqICk4gpWM_AT-I-l7g.png"/></div></div></figure><p id="6883" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Verify by hand that equation (4) holds. You can also do this in NumPy.</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="34d0" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/>A = np.array([<br/>    [1, -1],<br/>    [1 , 1]<br/>    ])<br/><br/>print(f'Inverse of A:\n{np.linalg.inv(A)}')</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="64df" class="ri ok fq rf b bg rj rk l rl rm">Inverse of A:<br/>[[ 0.5  0.5]<br/> [-0.5  0.5]]</span></pre><p id="6f87" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take a look at how the two transformations differ in the illustrations below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/6841442f62d2e5076f36dd59040ad1e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTRv2d4MfyxArQvo0Yc4pg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/40d8f35857e15029116e004221c177f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-zfWNVHMSQO0wUibUydLmw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong>⁻¹</figcaption></figure><p id="c15d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At first glance, it’s not obvious that one transformation reverses the effects of the other.</p><p id="0614" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, in these plots, you might notice a fascinating and far-reaching <strong class="nf fr">connection between the transformation and its inverse</strong>.</p><p id="5217" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take a close look at the first illustration, which shows the effect of transformation <strong class="nf fr">A</strong> on the basis vectors. The original unit vectors are depicted semi-transparently, while their transformed counterparts, resulting from multiplication by matrix <strong class="nf fr">A</strong>, are drawn clearly and solidly. Now, imagine that these newly drawn vectors are the basis vectors you use to describe the space, and you perceive the original space from their perspective. Then, the original basis vectors will appear smaller and, secondly, will be oriented towards the east. And this is exactly what the second illustration shows, demonstrating the effect of the transformation <strong class="nf fr">A</strong>⁻¹.</p><p id="64c6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is a preview of an upcoming topic I’ll cover in the next article about <em class="oi">using matrices to represent different perspectives on data</em>.</p><p id="685a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">All of this sounds great, but there’s a catch: <strong class="nf fr">some transformations can’t be reversed</strong>.</p><h1 id="b297" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">7. Non-invertible transformations</h1><p id="a17c" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">The workhorse of the next experiment will be the matrix with 1s on the diagonal and <em class="oi">b</em> on the antidiagonal:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sw"><img src="../Images/25a3ab7eb1f2fd31c21e71336ca8abf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j0AVtGlKRpfM-sERpFlX0w.png"/></div></div></figure><p id="a4a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <em class="oi">b</em> is a fraction in the interval (0, 1). This matrix is, by definition, symmetrical, as it happens to be identical to its own transpose: <strong class="nf fr">A</strong>=<strong class="nf fr">A</strong>ᵀ, but I’m just mentioning this by the way; it’s not particularly relevant here.</p><p id="4e12" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Invert this matrix using the Gauss-Jordan method, and you will get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sx"><img src="../Images/68a558db28b1988e8a1dff107e943aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-sL-bt-5bZt056aZmdFlw.png"/></div></div></figure><p id="175f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can easily find online the rules for calculating the determinant of 2x2 matrices, which will give</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sy"><img src="../Images/c3a5f7cf3fe401637edefe31ba794594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KB7PRUA1BvmSZOKA5FXLNQ.png"/></div></div></figure><p id="b6e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is no coincidence. In general, it holds that</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sz"><img src="../Images/18c6939aaad77b3e9af3c75059ab77a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3R7yQvP8mUmQZcYwqZHEA.png"/></div></div></figure><p id="be32" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that when <em class="oi">b</em> = 0, the two matrices are identical. This is no surprise, as <strong class="nf fr">A</strong> reduces to the identity matrix <strong class="nf fr">I</strong>.</p><p id="a9b5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Things get tricky when <em class="oi">b </em>= 1, as the det(<strong class="nf fr">A) </strong>= 0 and det<strong class="nf fr">(A</strong>⁻¹) becomes infinite. As a result, <strong class="nf fr">A</strong>⁻¹ does not exist for a matrix <strong class="nf fr">A</strong> consisting entirely of 1s. In algebra classes, teachers often warn you about a zero determinant. However, when we consider where the matrix comes from, it becomes apparent that an infinite determinant can also occur, resulting in <em class="oi">a fatal error</em>. Anyway,</p><blockquote class="rq rr rs"><p id="77e1" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">a zero determinant means the transformation is non-ivertible.</p></blockquote></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f48d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, the stage is set for experiments with different values of <em class="oi">b</em>. We’ve just seen how calculations fail at the limits, so let’s now visually investigate what happens as we carefully approach them.</p><p id="1b4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We start with <em class="oi">b </em>= ½​ and end up near 1.</p><p id="acb1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Step 1)</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ta"><img src="../Images/eeace7b7e5bd790b47317dff9520bf3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T_64p59e_pKyKkG2SJgL4Q.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/0c0250b06e9b3484f32e994538d49b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCLA1Aun47RFnK9gPNaQIA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/07ae76e1916792ee122d8cf9e4f8f7ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kUptuu69L5s5fDvOgyOjRA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong>⁻¹</figcaption></figure><p id="061e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Step 2)</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tb"><img src="../Images/32a130667236785712ba893d8dc5385e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uFJYvcxRRQfxR90053sVlg.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/1dacc8d80f399a3697998a94b3b87bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H05C27h3OoRl_Z4s8e0vmQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/75c303220d28ee0aa41f487cdc6a6144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ab7enCENR3MxbIr4l0aVw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong>⁻¹</figcaption></figure><p id="a5ce" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Recall that <strong class="nf fr">the determinant of the matrix representing the transformation corresponds to the area of the parallelogram</strong> formed by the transformed basis vectors.</p><p id="2c08" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is in line with the illustrations: the smaller the area of the parallelogram for transformation <strong class="nf fr">A</strong>, the larger it becomes for transformation <strong class="nf fr">A</strong>⁻¹. What follows is: the narrower the basis for transformation <strong class="nf fr">A</strong>, the wider it is for its inverse. Note also that I had to extend the range on the axes because the basis vectors for transformation <strong class="nf fr">A</strong> are getting longer.</p><p id="33b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">By the way, notice that</p><blockquote class="rq rr rs"><p id="5df9" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">the transformation <strong class="nf fr">A</strong> has the same eigen-directions as <strong class="nf fr">A</strong>⁻¹.</p></blockquote><p id="9efb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Step 3) <em class="oi">Almost there…</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tc"><img src="../Images/762d7551521b13fdada37d678056e7a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YL88GpqhQ6nP9CSXshnFsQ.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/51eb1cc88dc35bd03432b484d5d8ce04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9ho8NlEe3QtP_ndWWw81A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong></figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/88d3105ae207249f20bb3ad02e74fcec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kiJZ_fSk-8a7Q9PSZ9Q3Lg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Transformation <strong class="bf ol">A</strong>⁻¹</figcaption></figure><p id="d8c6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The gridlines are squeezed so much that they almost overlap, which eventually happens when <em class="oi">b</em> hits 1. The basis vectors of are stretched so far that they go beyond the axis limits. When <em class="oi">b</em> reaches exactly 1, both basis vectors lie on the same line.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="1f5c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Having seen the previous illustrations, you’re now ready to guess the effect of applying a non-invertible transformation to the vectors. Take a moment to think it through first, then either try running a computational experiment or check out the results I’ve provided below.</p><p id="92c4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="629b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="2415" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="4d7b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Think of it this way.</p><p id="bd50" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When the basis vectors are not parallel, meaning they form an angle other than 0 or 180 degrees, you can use them to address any point on the entire plane (mathematicians say that the vectors <strong class="nf fr"><em class="oi">span </em></strong>the plane). Otherwise, the entire plane can no longer be spanned, and only points along the line covered by the basis vectors can be addressed.</p><p id="97eb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="370f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="561a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">.</p><p id="724b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is what it looks like when you apply the non-invertible transformation to randomly selected points:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sh"><img src="../Images/c9043605861f4f0c4cdf577eca6881d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bweUieUouNwgTejqF3wDLg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A non-invertible matrix <strong class="bf ol">A</strong> reduces the dimensionality of the data</figcaption></figure><p id="8b38" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A consequence of applying a non-invertible transformation is that the two-dimensional space collapses to a one-dimensional subspace. After the transformation, it is no longer possible to uniquely recover the original coordinates of the points.</p><p id="fd3f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Take a look at the entries of matrix <strong class="nf fr">A</strong>. When <em class="oi">b</em> = 1, both columns (and rows) are identical, implying that the transformation matrix effectively behaves as if it were a 1 by 2 matrix, mapping two-dimensional vectors to a scalar.</p><p id="0226" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can easily verify that the problem would be the same if one row were a multiple of the other. This can be further generalized for matrices of any dimensions: if any row can be expressed as a weighted sum (<em class="oi">linear combination</em>) of the others, it implies that a dimension collapses. The reason is that such a vector lies within the space spanned by the other vectors, so it does not provide any additional ability to address points beyond those that can already be addressed. You may consider this vector <strong class="nf fr"><em class="oi">redundant</em></strong>.</p><p id="4f12" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From section 4 on transposition, we can infer that <strong class="nf fr">if there are redundant rows, there must be an equal number of redundant columns</strong>.</p><h1 id="ccd3" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">8. Determinant</h1><p id="4ae7" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You might now ask if there’s a non-geometrical way to verify whether the columns or rows of the matrix are redundant.</p><p id="eb73" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Recall the parallelograms from Section 4 and the scalar quantity known as the determinant. I mentioned that</p><blockquote class="rq rr rs"><p id="8f2b" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">the determinant of a matrix indicates how the area of a unit parallelogram changes under the transformation.</p></blockquote><p id="e1d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The exact definition of the determinant is somewhat tricky, but as you’ve already seen, its graphical interpretation should not cause any problems.</p><p id="527a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I will demonstrate the behavior of two transformations represented by matrices:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk td"><img src="../Images/983afc1e1e9040443ea782b7f54a953c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GG9_PB8u6kKyPnJ_9_wSkQ.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/5188224ee675e76f85ee6713653134e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_l8NNTtL1z90gAq4teTexw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">det(<strong class="bf ol">A</strong>) = 2</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/65e63a7a0e367989ad88d94d9c41f7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7lq36yOjCrHjF7EiVsgzA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">det(<strong class="bf ol">B</strong>) = -3/4</figcaption></figure><p id="56b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The magnitude of the determinant indicates how much the transformation stretches (if greater than 1) or shrinks (if less than 1) the space overall. While the transformation may stretch along one direction and compress along another, the overall effect is given by the value of the determinant.</p><p id="c278" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Also, a negative determinant indicates a reflection; note that matrix <strong class="nf fr">B</strong> reverses the order of the basis vectors.</p><p id="f6e6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A parallelogram with zero area corresponds to a transformation that collapses a dimension, meaning <strong class="nf fr">the determinant can be used to test for redundancy in the basis vectors of a matrix</strong>.</p><p id="fd3e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Since the determinant measures the area of a parallelogram under a transformation, we can apply it to a sequence of transformations. If det(<strong class="nf fr">A</strong>) and det(<strong class="nf fr">B</strong>) represent the scaling factors of unit areas for transformations <strong class="nf fr">A</strong> and <strong class="nf fr">B</strong>, then the scaling factor for the unit area after applying both transformations sequentially, that is, <strong class="nf fr">AB</strong>, is equal to det(<strong class="nf fr">AB</strong>). As both transformations act independently and one after the other, the total effect is given by det(<strong class="nf fr">AB</strong>) = det(<strong class="nf fr">A</strong>) det(<strong class="nf fr">B</strong>). Substituting matrix <strong class="nf fr">A</strong>⁻¹ for matrix <strong class="nf fr">B </strong>and noting that det(<strong class="nf fr">I</strong>) = 1 leads to equation (5) introduced in the previous section.</p><p id="23d3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here’s how you can calculate the determinant using NumPy:</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="85c6" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/>A = np.array([<br/>    [-1/2, 1/4],<br/>    [2, 1/2]<br/>    ])<br/><br/>print(f'det(A) = {np.linalg.det(A)}')</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="93ec" class="ri ok fq rf b bg rj rk l rl rm">det(A) = -0.75</span></pre><h1 id="c935" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">9. Non-square matrices</h1><p id="015d" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Until now, we’ve focused on square matrices, and you’ve developed a geometric intuition of the transformations they represent. Now is a great time to expand these skills to <strong class="nf fr">matrices with any number of rows and columns</strong>.</p><h2 id="0d1d" class="oj ok fq bf ol om on oo op oq or os ot nm ou ov ow nq ox oy oz nu pa pb pc pd bk">Wide matrices</h2><p id="4a1e" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">This is an example of <strong class="nf fr">a wide matrix</strong>, which has more columns than rows:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk te"><img src="../Images/0642b7a3ca839f9e87d0d3f3f53b968d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dk-065tymz9ROS7cYTATsA.png"/></div></div></figure><p id="a4a6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the perspective of equation (1), <strong class="nf fr">y</strong> = <strong class="nf fr">Ax</strong>, it maps three-dimensional vectors <strong class="nf fr">x</strong> to two-dimensional vectors <strong class="nf fr">y</strong>.</p><p id="ab04" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In such a case, one column can always be expressed as a multiple of another or as a weighted sum of the others. For example, the third column here equals 3/4 times the first column plus 5/4 times the second.</p><p id="e2cc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Once the vector <strong class="nf fr">x</strong> has been transformed into <strong class="nf fr">y</strong>, it’s no longer possible to reconstruct the original <strong class="nf fr">x</strong> from <strong class="nf fr">y</strong>. We say that the transformation <strong class="nf fr">reduces the dimensionality of the input data</strong>. These types of transformations are very important in machine learning.</p><p id="0fe9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sometimes, a wide matrix disguises itself as a square matrix, but you can reveal it by checking whether its determinant is zero. We’ve had this situation before, remember?</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0520" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can use the matrix <strong class="nf fr">A</strong> to create two different square matrices. Try deriving the following result yourself:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tf"><img src="../Images/80f2cae52ac384fcb9146da7c1fa37a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quRfcEdg7diUmUY5uMdj2w.png"/></div></div></figure><p id="e24f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and also determinants (I recommend simplified formulas for working with <a class="af nc" href="https://brilliant.org/wiki/expansion-of-determinants/" rel="noopener ugc nofollow" target="_blank">2×2</a> and <a class="af nc" href="https://en.wikipedia.org/wiki/Rule_of_Sarrus" rel="noopener ugc nofollow" target="_blank">3×3</a> matrices):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tg"><img src="../Images/24633e4af77690f9c245aa92ac05590f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDYa_0cFfC9Ktg2OtggRuQ.png"/></div></div></figure><p id="a706" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The matrix <strong class="nf fr">A</strong>ᵀ<strong class="nf fr">A</strong> is composed of the dot products of all possible pairs of columns from matrix <strong class="nf fr">A</strong>, some of which are definitely redundant, thereby transferring this redundancy to <strong class="nf fr">A</strong>ᵀ<strong class="nf fr">A</strong>.</p><p id="cde0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Matrix <strong class="nf fr">AA</strong>ᵀ, on the other hand, contains only the dot products of the rows of matrix <strong class="nf fr">A</strong>, which are fewer in number than the columns. Therefore, the vectors that make up matrix <strong class="nf fr">AA</strong>ᵀ are most likely (though not entirely guaranteed) linearly independent, meaning that one vector cannot be expressed as a multiple of another or as a weighted sum of the others.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5edf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">What would happen if you insisted on determining <strong class="nf fr">x</strong> from <strong class="nf fr">y</strong>, which was previously computed as <strong class="nf fr">y</strong> = <strong class="nf fr">Ax</strong>? You could left-multiply both sides by <strong class="nf fr">A</strong>⁻¹ to get equation <strong class="nf fr">A</strong>⁻¹<strong class="nf fr">y</strong> = <strong class="nf fr">A</strong>⁻¹<strong class="nf fr">Ax</strong> and, since <strong class="nf fr">A</strong>⁻¹<strong class="nf fr">A = I</strong>, obtain <strong class="nf fr">x</strong> = <strong class="nf fr">A</strong>⁻¹<strong class="nf fr">y</strong>. But this would fail from the very beginning, because matrix <strong class="nf fr">A</strong>⁻¹, being non-square, is certainly non-invertible (at least not in the sense that was previously introduced).</p><p id="20a7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">However, you can extend the original equation <strong class="nf fr">y</strong> = <strong class="nf fr">Ax </strong>to include a square matrix where it’s needed. You just need to left-multiply matrix <strong class="nf fr">A</strong>ᵀ on both sides of the equation, yielding <strong class="nf fr">A</strong>ᵀ<strong class="nf fr">y</strong> = <strong class="nf fr">A</strong>ᵀ<strong class="nf fr">Ax</strong>. On the right, we now have a square matrix <strong class="nf fr">A</strong>ᵀ<strong class="nf fr">A</strong>. Unfortunately, we’ve already seen that its determinant is zero, so it appears that we have once again failed to reconstruct <strong class="nf fr">x</strong> from <strong class="nf fr">y</strong>.</p><h2 id="fcb5" class="oj ok fq bf ol om on oo op oq or os ot nm ou ov ow nq ox oy oz nu pa pb pc pd bk">Tall matrices</h2><p id="efa9" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">Here is an example of a <strong class="nf fr">tall matrix</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk th"><img src="../Images/a4113e8d05109a6962e31b4e136ed46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OmPHmE6E0OFHhbEvMXZKqg.png"/></div></div></figure><p id="84bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">that maps two-dimensional vectors <strong class="nf fr">x </strong>into three-dimensional vectors <strong class="nf fr">y</strong>. I made a third row by simply squaring the entries of the first row. While this type of extension doesn’t add any new information to the data, it can surprisingly improve the performance of certain machine learning models.</p><p id="6705" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You might think that, unlike wide matrices, tall matrices allow the reconstruction of the original <strong class="nf fr">x</strong> from <strong class="nf fr">y</strong>, where <strong class="nf fr">y</strong> = <strong class="nf fr">Bx</strong>, since no information is discarded — only added.</p><p id="017a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">And you’d be right! Look at what happens when we left-multiply by matrix <strong class="nf fr">B</strong>ᵀ, just like we tried before, but without success: <strong class="nf fr">B</strong>ᵀ<strong class="nf fr">y</strong> = <strong class="nf fr">B</strong>ᵀ<strong class="nf fr">Bx</strong>. This time, matrix <strong class="nf fr">B</strong>ᵀ<strong class="nf fr">B</strong> is invertible, so we can left-multiply by its inverse:</p><p id="3c54" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">(B</strong>ᵀ<strong class="nf fr">B)</strong>⁻¹<strong class="nf fr">B</strong>ᵀ<strong class="nf fr">y</strong> = <strong class="nf fr">(B</strong>ᵀ<strong class="nf fr">B)</strong>⁻¹<strong class="nf fr">(B</strong>ᵀ<strong class="nf fr">B)x</strong></p><p id="12d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and finally obtain:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ti"><img src="../Images/f7a2a74813ff1fef6d8bbd655f4619c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5JZF-nDmJMyyy1zSNrpJQ.png"/></div></div></figure><p id="e2d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is how it works in Python:</p><pre class="mm mn mo mp mq re rf rg bp rh bb bk"><span id="408b" class="ri ok fq rf b bg rj rk l rl rm">import numpy as np<br/><br/># Tall matrix<br/>B = [<br/>    [2, -3],<br/>    [1 , 0],<br/>    [3, -3]<br/>]<br/><br/># Convert to numpy array<br/>B = np.array(B)<br/><br/># A column vector from a lower-dimensional space<br/>x = np.array([-3,1]).reshape(2,-1)<br/><br/># Calculate its corresponding vector in a higher-dimensional space<br/>y = B @ x<br/><br/>reconstructed_x = np.linalg.inv(B.T @ B) @ B.T @ y<br/><br/>print(reconstructed_x)</span></pre><pre class="rn re rf rg bp rh bb bk"><span id="cf19" class="ri ok fq rf b bg rj rk l rl rm">[[-3.]<br/> [ 1.]]</span></pre></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="97e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To summarize: the determinant measures the redundancy (or linear independence) of the columns and rows of a matrix. However, it only makes sense when applied to square matrices. Non-square matrices represent transformations between spaces of different dimensions and necessarily have linearly dependent columns or rows. If the target dimension is higher than the input dimension, it’s possible to reconstruct lower-dimensional vectors from higher-dimensional ones.</p><h1 id="b71b" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">10. Inverse and Transpose: similarities and differences</h1><p id="93b1" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You’ve certainly noticed that the inverse and transpose operations play a key role in matrix algebra. In this section, we bring together the most useful identities related to these operations.</p><p id="5366" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Whenever I apply the inverse operator, I assume that the matrix being operated on is square.</p><p id="aa26" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We’ll start with the obvious one that hasn’t appeared yet.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tj"><img src="../Images/ec979c5c54f3212d0be3d698b6e191b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*had_eyWjowCYo7WLJDnGiA.png"/></div></div></figure><p id="b36a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here are the previously given identities (2) and (5), placed side by side:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tk"><img src="../Images/147f2a2d2ee0053d27fe2ee37563b518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-ehBK-Vk5jbhPRj_kWe7A.png"/></div></div></figure></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="af46" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s walk through the following reasoning, starting with the identity from equation (4), where <strong class="nf fr">A</strong> is replaced by the composite <strong class="nf fr">AB</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tl"><img src="../Images/7d0da538c70be86d7770cfa67ef8182d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7ZomFe42oREnHzAAFRz7Q.png"/></div></div></figure><p id="7718" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The parentheses on the right are not needed. After removing them, I right-multiply both sides by the matrix <strong class="nf fr">B</strong>⁻¹ and then by <strong class="nf fr">A</strong>⁻¹.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tm"><img src="../Images/c71c62e2e8f8911aea2ae49a4b949246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RuG3AVsxkr9L-6rLLVnww.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tn"><img src="../Images/7711ca2ab945fb221b91d72f15936b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AvCzfaP2FM_vCvi4wkOFmA.png"/></div></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk to"><img src="../Images/db6b16f6655d291f2de92931dc97448f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFaif1ZAcL-vFC9OpwKRZw.png"/></div></div></figure><p id="e5bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thus, we observe the next similarity between inversion and transposition (see equation (3)):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tp"><img src="../Images/10eaf1e5d985dceca4fe6a9e09a7c33a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RYRdRfw91fi8FDo6MSKqg.png"/></div></div></figure></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="852f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You might be disappointed now, as the following only applies to transposition.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tq"><img src="../Images/2587c3c2b63f52999284d1f62a5b1816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P8-6nvHXbKegYM8rkZkNBg.png"/></div></div></figure><p id="f853" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But imagine if <strong class="nf fr">A</strong> and <strong class="nf fr">B</strong> were scalars. The same for the inverse would be a mathematical scandal!</p><p id="2afb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For a change, the identity in equation (4) works only for the inverse:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tr"><img src="../Images/468ae0bc1d92eed062816ab96280ed17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PSUCikhyzsVfgsb6nYtzlA.png"/></div></div></figure></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="aec4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I’ll finish off this section by discussing the interplay between inversion and transposition.</p><p id="7639" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">From the last equation, along with equation (3), we get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ts"><img src="../Images/39ef86a68fba2679de18655cbb169a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wfIgNiuFhuE1PFfuHSv0_A.png"/></div></div></figure><p id="e638" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Keep in mind that <strong class="nf fr">I</strong>ᵀ = <strong class="nf fr">I</strong>. Right-multiplying by the inverse of <strong class="nf fr">A</strong>ᵀ yields the following identity:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tt"><img src="../Images/3ab87eb0e07fe889ebd9802afde5b1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_dPTqwGlxshw5swFsH51fw.png"/></div></div></figure><h1 id="34d3" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">11. Translation by a vector</h1><p id="fa84" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">You might be wondering why I’m focusing only on the operation of multiplying a vector by a matrix, while neglecting the translation of a vector by adding another vector.</p><p id="e6bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">One reason is purely mathematical. Linear operations offer significant advantages, such as ease of transformation, simplicity of expressions, and algorithmic efficiency.</p><p id="ef16" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A key property of linear operations is that a linear combination of inputs leads to a linear combination of outputs:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tu"><img src="../Images/5a0cacd0a12470f99c0fbb5d74385709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pp4zA9l-0902vWyaeNYRvQ.png"/></div></div></figure><p id="9a4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">where <em class="oi">α</em> <em class="oi">, β</em> are real scalars, and <em class="oi">Lin </em>represents a linear operation.</p><p id="728c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s first examine the matrix-vector multiplication operator <em class="oi">Lin</em>[<strong class="nf fr">x</strong>] = <strong class="nf fr">Ax</strong> from equation (1):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tv"><img src="../Images/7fa1ced0afa32aacbed05cb51a10d0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzfhRBXzLZC_uA9kw8cdVw.png"/></div></div></figure><p id="114e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This confirms that matrix-vector multiplication is a linear operation.</p><p id="fdfa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, let’s consider a more general transformation, which involves a shift by a vector <strong class="nf fr">b</strong>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tw"><img src="../Images/011c6dfe8c7ba8d4d26c0489900894d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*shneBIccfcUFmeLZjgHBDg.png"/></div></div></figure><p id="82bf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Plug in a weighted sum and see what comes out.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk tx"><img src="../Images/9ba9c54b156a84f7cd1ec877c294e76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgZjH1T_9ClerrfAWiZFWA.png"/></div></div></figure><p id="99f2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">You can see that adding <strong class="nf fr">b</strong> disrupts the linearity. Operations like this are called <strong class="nf fr">affine </strong>to differentiate them from linear ones.</p><p id="1a1d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Don’t worry though — there’s a simple way to eliminate the need for translation. Simply shift the data beforehand, for example, by centering it, so that the vector <strong class="nf fr">b</strong> becomes zero. This is a common approach in data science.</p><p id="d6b9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Therefore, the data scientist only needs to worry about matrix-vector multiplication.</p><h1 id="2458" class="pr ok fq bf ol ps pt gq op pu pv gt ot pw px py pz qa qb qc qd qe qf qg qh qi bk">12. Final words</h1><p id="455b" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">I hope that linear algebra seems easier to understand now, and that you’ve got a sense of how interesting it can be.</p><p id="a9e3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">If I’ve sparked your interest in learning more, that’s great! But even if it’s just that you feel more confident with the course material, that’s still a win.</p><p id="c400" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Bear in mind that this is more of a semi-formal introduction to the subject. For more rigorous definitions and proofs, you might need to look at specialised literature.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f872" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="oi">Unless otherwise noted, all images are by the author</em></p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="212e" class="pr ok fq bf ol ps ty gq op pu tz gt ot pw ua py pz qa ub qc qd qe uc qg qh qi bk">References</h1><p id="b026" class="pw-post-body-paragraph nd ne fq nf b go pe nh ni gr pf nk nl nm pg no np nq ph ns nt nu pi nw nx ny fj bk">[1] Gilbert Strang. <em class="oi">Introduction to linear algebra</em>. Wellesley-Cambridge Press, 2022.</p><p id="2752" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] Marc Peter Deisenroth, A. Aldo Faisal, Cheng Soon Ong. <em class="oi">Mathematics for machine learning</em>. Cambridge University Press, 2020.</p></div></div></div></div>    
</body>
</html>