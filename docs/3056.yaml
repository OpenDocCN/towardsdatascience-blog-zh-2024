- en: An Agentic Approach to Reducing LLM Hallucinations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少LLM幻觉的代理方法
- en: 原文：[https://towardsdatascience.com/an-agentic-approach-to-reducing-llm-hallucinations-f7ffd6eedcf2?source=collection_archive---------1-----------------------#2024-12-22](https://towardsdatascience.com/an-agentic-approach-to-reducing-llm-hallucinations-f7ffd6eedcf2?source=collection_archive---------1-----------------------#2024-12-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-agentic-approach-to-reducing-llm-hallucinations-f7ffd6eedcf2?source=collection_archive---------1-----------------------#2024-12-22](https://towardsdatascience.com/an-agentic-approach-to-reducing-llm-hallucinations-f7ffd6eedcf2?source=collection_archive---------1-----------------------#2024-12-22)
- en: Simple techniques to alleviate LLM hallucinations using LangGraph
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LangGraph缓解LLM幻觉的简单技巧
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--f7ffd6eedcf2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)
    ·8 min read·Dec 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f7ffd6eedcf2--------------------------------)
    ·8分钟阅读·2024年12月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/630e9d5af9ca9f6e315c751334d7460c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/630e9d5af9ca9f6e315c751334d7460c.png)'
- en: Photo by [Greg Rakozy](https://unsplash.com/@grakozy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Greg Rakozy](https://unsplash.com/@grakozy?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you’ve worked with LLMs, you know they can sometimes hallucinate. This means
    they generate text that’s either nonsensical or contradicts the input data. It’s
    a common issue that can hurts the reliability of LLM-powered applications.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经使用过LLM，你就会知道它们有时会产生幻觉。这意味着它们生成的文本要么没有意义，要么与输入数据相矛盾。这是一个常见的问题，会影响LLM驱动应用程序的可靠性。
- en: In this post, we’ll explore a few simple techniques to reduce the likelihood
    of hallucinations. By following these tips, you can (hopefully) improve the accuracy
    of your AI applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探索一些简单的技巧，来减少幻觉发生的可能性。通过遵循这些建议，你可以（希望）提高AI应用程序的准确性。
- en: 'There are multiple types of hallucinations:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉有多种类型：
- en: '[Intrinsic hallucinations](https://arxiv.org/pdf/2311.05232): the LLM’s response
    contradicts the user-provided context. This is when the response is verifiably
    wrong withing the current context.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[内在幻觉](https://arxiv.org/pdf/2311.05232)：LLM的回答与用户提供的上下文相矛盾。这是指在当前上下文中，回答是可以被验证为错误的。'
- en: '[Extrinsic hallucinations](https://arxiv.org/pdf/2311.05232): the LLM’s response
    cannot be verified using the user-provided context. This is when the response
    may or may not be wrong but we have no way of confirming that using the current
    context.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[外在幻觉](https://arxiv.org/pdf/2311.05232)：LLM的回答无法通过用户提供的上下文进行验证。这是指回答可能正确也可能错误，但我们无法通过当前上下文确认其正确性。'
- en: 'Incoherent hallucinations: the LLM’s response does not answer the question
    or does not make sense. This is when the LLM is unable to follow the instructions.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无关的幻觉：LLM的回答没有回答问题或没有意义。这是指LLM无法遵循指示。
- en: In this post, we will target all the types mentioned above.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将针对上述所有类型进行讨论。
- en: We will list out a set of tips and tricks that work in different ways in reducing
    hallucinations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将列出一组有效的技巧和方法，这些方法可以以不同方式减少幻觉。
- en: 'Tip 1: Use Grounding'
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示1：使用基础数据
- en: Grounding is using in-domain relevant additional context in the input of the
    LLM when asking it to do a task. This gives the LLM the information it needs to
    correctly answer the question and reduces the likelihood of a hallucination. This
    is one the reason we use Retrieval augmented generation (RAG).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 归属（Grounding）是在向LLM提出任务时，输入包含领域相关的附加上下文。这为LLM提供了正确回答问题所需的信息，并减少了幻觉的可能性。这也是我们使用增强检索生成（RAG）的原因之一。
- en: For example asking the LLM a math question OR asking it the same question while
    providing it with relevant sections of a math book will yield different results,
    with the second option being more likely to be right.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，向LLM提问一个数学问题，或者在提供相关数学书籍的章节后向它提问，会得到不同的结果，第二种方式更有可能得到正确答案。
- en: 'Here is an example of such implementation in one of my previous tutorials where
    I provide document-extracted context when asking a question:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我之前教程中的一个实现示例，当我在提问时提供了从文档中提取的上下文：
- en: '[](/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=post_page-----f7ffd6eedcf2--------------------------------)
    [## Build a Document AI pipeline for ANY type of PDF With Gemini'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=post_page-----f7ffd6eedcf2--------------------------------)
    [## 使用Gemini构建适用于任何类型PDF的文档AI管道'
- en: Tables, Images, figures or handwriting are not problem anymore ! Full Code provided.
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 表格、图片、图形或手写内容不再是问题！完整代码已提供。
- en: towardsdatascience.com](/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=post_page-----f7ffd6eedcf2--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=post_page-----f7ffd6eedcf2--------------------------------)
- en: 'Tip 2: Use structured outputs'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示 2：使用结构化输出
- en: Using structured outputs means forcing the LLM to output valid JSON or YAML
    text. This will allow you to reduce the useless ramblings and get “straight-to-the-point”
    answers about what you need from the LLM. It also will help with the next tips
    as it makes the LLM responses easier to verify.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结构化输出意味着强制LLM输出有效的JSON或YAML文本。这将帮助你减少无意义的废话，得到“直截了当”的答案，同时也有助于下一个提示，因为它使LLM的回应更容易验证。
- en: 'Here is how you can do this with Gemini’s API:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何通过Gemini的API实现这一点：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Where “prepare_schema_for_gemini” is a utility function that prepares the schema
    to match Gemini’s weird requirements. You can find its definition here: [code](https://github.com/CVxTz/document_ai_agents/blob/498d8ee6e8597f8ba43b336c64178d186461dba0/document_ai_agents/schema_utils.py#L38).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“prepare_schema_for_gemini”是一个工具函数，用来准备模式以匹配Gemini的特殊要求。你可以在这里找到它的定义：[code](https://github.com/CVxTz/document_ai_agents/blob/498d8ee6e8597f8ba43b336c64178d186461dba0/document_ai_agents/schema_utils.py#L38)。
- en: This code defines a Pydantic schema and sends this schema as part of the query
    in the field “response_schema”. This forces the LLM to follow this schema in its
    response and makes it easier to parse its output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码定义了一个Pydantic模式，并将该模式作为查询的一部分传递给字段“response_schema”。这迫使LLM在回应时遵循这个模式，并使其输出更容易解析。
- en: 'Tip 3: Use chain of thoughts and better prompting'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示 3：使用思维链和更好的提示
- en: Sometimes, giving the LLM the space to work out its response, before committing
    to a final answer, can help produce better quality responses. This technique is
    called Chain-of-thoughts and is widely used as it is effective and very easy to
    implement.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，给LLM一些空间来推敲其回答，在最终给出答案之前，能帮助产生更高质量的回答。这种技术称为思维链（Chain-of-thoughts），它被广泛使用，因为它有效且非常容易实现。
- en: We can also explicitly ask the LLM to answer with “N/A” if it can’t find enough
    context to produce a quality response. This will give it an easy way out instead
    of trying to respond to questions it has no answer to.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以明确要求LLM如果无法找到足够的上下文来生成高质量的回答时，回答为“N/A”。这样它就有了一个简单的退出方式，而不是尝试回答自己无法回答的问题。
- en: 'For example, lets look into this simple question and context:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看这个简单的问题和上下文：
- en: '**Context**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景**'
- en: 'Thomas Jefferson (April 13 [O.S. April 2], 1743 — July 4, 1826) was an American
    statesman, planter, diplomat, lawyer, architect, philosopher, and Founding Father
    who served as the third president of the United States from 1801 to 1809.[6] He
    was the primary author of the Declaration of Independence. Following the American
    Revolutionary War and before becoming president in 1801, Jefferson was the nation’s
    first U.S. secretary of state under George Washington and then the nation’s second
    vice president under John Adams. Jefferson was a leading proponent of democracy,
    republicanism, and natural rights, and he produced formative documents and decisions
    at the state, national, and international levels. (Source: Wikipedia)'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 托马斯·杰斐逊（1743年4月13日[公历4月2日]—1826年7月4日）是美国政治家、种植园主、外交家、律师、建筑师、哲学家和开国元勋，曾于1801至1809年担任美国第三任总统。[6]
    他是《独立宣言》的主要起草人。美国独立战争后，杰斐逊在1801年成为总统之前，曾担任乔治·华盛顿政府的首任美国国务卿，随后成为约翰·亚当斯政府的第二任副总统。杰斐逊是民主、共和主义和自然权利的主要倡导者，并在州、国家和国际层面制定了具有重要影响的文件和决策。（来源：维基百科）
- en: '**Question**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**'
- en: What year did davis jefferson die?
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 戴维斯·杰斐逊是哪一年去世的？
- en: 'A naive approach yields:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法得到如下结果：
- en: '**Response**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**回应**'
- en: answer=’1826'
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案=‘1826’
- en: Which is obviously false as Jefferson Davis is not even mentioned in the context
    at all. It was Thomas Jefferson that died in 1826.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 显然这是错误的，因为杰斐逊·戴维斯在上下文中根本没有被提到。死于1826年的是托马斯·杰斐逊。
- en: 'If we change the schema of the response to use chain-of-thoughts to:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将回应的模式改为使用思维链：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are also adding more details about what we expect as output when the question
    is not answerable using the context “Answer with ‘N/A’ if answer is not found”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在添加更多细节，说明当问题无法使用上下文回答时，我们期待的输出是什么——“如果没有找到答案，请回复‘N/A’”。
- en: 'With this new approach, we get the following **rationale** (remember, chain-of-thought):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种新方法，我们得到了以下**推理**（记住，思维链）：
- en: The provided text discusses Thomas Jefferson, not Jefferson Davis. No information
    about the death of Jefferson Davis is included.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提供的文本讨论的是托马斯·杰斐逊，而不是杰斐逊·戴维斯。没有包含任何关于杰斐逊·戴维斯死亡的信息。
- en: 'And the final **answer**:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的**答案**：
- en: answer=’N/A’
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 答案=‘N/A’
- en: Great ! But can we use a more general approach to hallucination detection?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！但是我们能否使用一种更通用的方法来检测幻觉？
- en: We can, with Agents!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用代理！
- en: 'Tip 4: Use an Agentic approach'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示4：使用代理方法
- en: 'We will build a simple agent that implements a three-step process:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个简单的代理，实施一个三步流程：
- en: The first step is to include the context and ask the question to the LLM in
    order to get the first candidate response and the relevant context that it had
    used for its answer.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一步是包含上下文并向LLM提问，以便获得第一个候选回应及其用于回答的相关上下文。
- en: The second step is to reformulate the question and the first candidate response
    as a declarative statement.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二步是将问题和第一个候选回答重新表述为陈述句。
- en: 'The third step is to ask the LLM to verify whether or not the relevant context
    **entails** the candidate response. It is called “Self-verification”: [https://arxiv.org/pdf/2212.09561](https://arxiv.org/pdf/2212.09561)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三步是让LLM验证相关上下文是否**蕴含**候选回应。这被称为“自我验证”：[https://arxiv.org/pdf/2212.09561](https://arxiv.org/pdf/2212.09561)
- en: In order to implement this, we define three nodes in LangGraph. The first node
    will ask the question while including the context, the second node will reformulate
    it using the LLM and the third node will check the entailment of the statement
    in relation to the input context.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们在LangGraph中定义了三个节点。第一个节点将提问并包含上下文，第二个节点将使用LLM进行重述，第三个节点将检查该声明与输入上下文的蕴含关系。
- en: 'The first node can be defined as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个节点可以定义如下：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And the second one as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个示例如下：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The third one as:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个节点如下：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Full code in [https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码请见[https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
- en: Notice how each node uses its own schema for structured output and its own prompt.
    This is possible due to the flexibility of both Gemini’s API and LangGraph.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个节点都使用其自己的模式进行结构化输出，并使用自己的提示。这是由于Gemini的API和LangGraph的灵活性所实现的。
- en: Lets work through this code using the same example as above ➡️
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过与上述相同的示例来演示这段代码 ➡️
- en: '*(Note: we are not using chain-of-thought on the first prompt so that the verification
    gets triggered for our tests.)*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*(注意：我们在第一个提示中没有使用思维链，以便触发验证进行测试。)*'
- en: '**Context**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文**'
- en: 'Thomas Jefferson (April 13 [O.S. April 2], 1743 — July 4, 1826) was an American
    statesman, planter, diplomat, lawyer, architect, philosopher, and Founding Father
    who served as the third president of the United States from 1801 to 1809.[6] He
    was the primary author of the Declaration of Independence. Following the American
    Revolutionary War and before becoming president in 1801, Jefferson was the nation’s
    first U.S. secretary of state under George Washington and then the nation’s second
    vice president under John Adams. Jefferson was a leading proponent of democracy,
    republicanism, and natural rights, and he produced formative documents and decisions
    at the state, national, and international levels. (Source: Wikipedia)'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 托马斯·杰斐逊（1743年4月13日[公历4月2日] — 1826年7月4日）是美国政治家、种植园主、外交官、律师、建筑师、哲学家和建国父亲之一，曾于1801年至1809年担任美国第三任总统。[6]
    他是《独立宣言》的主要作者。在美国独立战争后，托马斯·杰斐逊在1801年成为总统之前，曾担任乔治·华盛顿总统政府下的第一任美国国务卿，并在约翰·亚当斯总统政府下担任第二任副总统。杰斐逊是民主主义、共和主义和自然权利的主要支持者，他在州级、国家级和国际层面上起草了具有开创性的文件和决策。（来源：维基百科）
- en: '**Question**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**'
- en: What year did davis jefferson die?
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 戴维斯·杰斐逊是什么时候去世的？
- en: '**First node result (First answer):**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一节点结果（第一次回答）：**'
- en: '**relevant_context**=’Thomas Jefferson (April 13 [O.S. April 2], 1743 — July
    4, 1826) was an American statesman, planter, diplomat, lawyer, architect, philosopher,
    and Founding Father who served as the third president of the United States from
    1801 to 1809.’'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**相关上下文**=‘托马斯·杰斐逊（1743年4月13日[公历4月2日] — 1826年7月4日）是美国政治家、种植园主、外交官、律师、建筑师、哲学家和建国父亲之一，曾于1801年至1809年担任美国第三任总统。’'
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**answer=’1826''**'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**答案**=‘1826’'
- en: '**Second node result (Answer Reformulation):**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二节点结果（答案重述）：**'
- en: '**declarative_answer**=’Davis Jefferson died in 1826'''
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**陈述性答案**=‘戴维斯·杰斐逊于1826年去世’'
- en: '**Third node result (Verification):**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三节点结果（验证）：**'
- en: '**rationale**=’The context states that Thomas Jefferson died in 1826\. The
    assertion states that Davis Jefferson died in 1826\. The context does not mention
    Davis Jefferson, only Thomas Jefferson.’'
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**推理**=‘上下文中提到托马斯·杰斐逊于1826年去世。声明称戴维斯·杰斐逊于1826年去世。上下文中没有提到戴维斯·杰斐逊，只有托马斯·杰斐逊。’'
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**entailment**=’No’'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**蕴含**=‘否’'
- en: So the verification step **rejected** (*No entailment between the two*) the
    initial answer. We can now avoid returning a hallucination to the user.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，验证步骤**拒绝**了初始答案（*两者之间没有蕴含*）。现在我们可以避免将幻觉返回给用户。
- en: 'Bonus Tip : Use stronger models'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示：使用更强大的模型
- en: 'This tip is not always easy to apply due to budget or latency limitations but
    you should know that stronger LLMs are less prone to hallucination. So, if possible,
    go for a more powerful LLM for your most sensitive use cases. You can check a
    benchmark of hallucinations here: [https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard).
    We can see that the top models in this benchmark (least hallucinations) also ranks
    at the top of conventional NLP leader boards.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预算或延迟限制，这个提示并不总是容易应用，但你应该知道，较强大的LLM较不容易产生幻觉。因此，如果可能的话，针对最敏感的使用场景，选择更强大的LLM。你可以在这里查看幻觉的基准测试：[https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard)。我们可以看到，在这个基准测试中，排名最前的模型（最少幻觉）也排名在传统NLP排行榜的前列。
- en: '![](../Images/28143041199f3dd4c5130c62985a0f62.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28143041199f3dd4c5130c62985a0f62.png)'
- en: 'Source: [https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard)
    Source License: Apache 2.0'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://github.com/vectara/hallucination-leaderboard](https://github.com/vectara/hallucination-leaderboard)
    来源许可证：Apache 2.0
- en: Conclusion
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial, we explored strategies to improve the reliability of LLM outputs
    by reducing the hallucination rate. The main recommendations include careful formatting
    and prompting to guide LLM calls and using a workflow based approach where Agents
    are designed to verify their own answers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们探讨了通过降低幻觉率来提高大型语言模型（LLM）输出可靠性的策略。主要建议包括仔细格式化和提示，以指导LLM的调用，并采用基于工作流的方法，在这种方法中，代理会被设计为验证自己的答案。
- en: 'This involves multiple steps:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括多个步骤：
- en: Retrieving the exact context elements used by the LLM to generate the answer.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索LLM用于生成答案的确切上下文元素。
- en: Reformulating the answer for easier verification (In declarative form).
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将答案重新表述为更易验证的形式（以陈述句形式）。
- en: Instructing the LLM to check for consistency between the context and the reformulated
    answer.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指示LLM检查上下文与重述答案之间的一致性。
- en: While all these tips can significantly improve accuracy, you should remember
    that no method is foolproof. There’s always a risk of rejecting valid answers
    if the LLM is overly conservative during verification or missing real hallucination
    cases. Therefore, rigorous evaluation of your specific LLM workflows is still
    essential.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有这些技巧都可以显著提高准确性，但你应该记住没有任何方法是万无一失的。如果LLM在验证过程中过于保守，可能会拒绝有效的答案，或者遗漏真实的幻觉案例，因此，仍然需要对你的特定LLM工作流进行严格评估。
- en: Full code in [https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码在[https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
- en: Thank you for reading !
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感谢阅读！
