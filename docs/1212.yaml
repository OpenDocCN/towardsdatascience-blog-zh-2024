- en: How Does the Segment-Anything Model’s (SAM’s) Encoder Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b?source=collection_archive---------5-----------------------#2024-05-14](https://towardsdatascience.com/how-does-the-segment-anything-models-sam-s-encoder-work-003a8a6e3f8b?source=collection_archive---------5-----------------------#2024-05-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A deep dive into how image content embedding, sine and cosine positional embedding,
    guidance click embedding, and dense mask embedding are produced
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--003a8a6e3f8b--------------------------------)
    ·16 min read·May 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8fa7cce26e763966c76f98d3a33a940.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [benjamin lehman](https://unsplash.com/@abject?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article explains how the Segment-Anything model’s (SAM) encoder works.
    SAM uses a [ImageEncoderViT](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py)
    class to produce content embeddings for the input image. It also uses a [PromptEncoder](https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/prompt_encoder.py)
    class to produce embeddings for:'
  prefs: []
  type: TYPE_NORMAL
- en: guidance clicks, both positive and negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bounding boxes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image positional encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: image dense mask encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ImageEncoderViT is a standard ViT network, so I won’t explain it in detail
    here. This article only focuses on how PromptEncoder produces the above embeddings.
    Those embeddings are passed to SAM’s decoder to generate segmentation mask. Knowing
    how the decoder uses these embeddings will make it easier to understand the encoding.
    You can read more about SAM’s decoding procedure from my other article “[How does
    the Segment-Anything Model’s (SAM’s) decoder work?](https://medium.com/p/0e4ab4732c37)”.
  prefs: []
  type: TYPE_NORMAL
- en: Encode a guidance click
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In SAM, a guidance click can be of a positive kind, telling the model where
    to segment, and a…
  prefs: []
  type: TYPE_NORMAL
