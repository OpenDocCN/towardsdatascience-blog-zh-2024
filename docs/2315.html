<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Graph RAG into Production — Step-by-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Graph RAG into Production — Step-by-Step</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-rag-into-production-step-by-step-3fe71fb4a98e?source=collection_archive---------0-----------------------#2024-09-23">https://towardsdatascience.com/graph-rag-into-production-step-by-step-3fe71fb4a98e?source=collection_archive---------0-----------------------#2024-09-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="fo bh"><figure class="fp fo bh paragraph-image"><img src="../Images/203dbf1037dad17615e4716e4782dc37.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/0*pmxJS7NSZWB_HLvX"/><figcaption class="fs ft fu fv fw fx fy bf b bg z dx">Photo by <a class="af fz" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JJ Ying</a> on <a class="af fz" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="65b5" class="pw-subtitle-paragraph gz gb gc bf b ha hb hc hd he hf hg hh hi hj hk hl hm hn ho cq dx">A GCP native, fully serverless implementation that you will replicate in minutes</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hp hq hr hs ht ab"><div><div class="ab hu"><div><div class="bm" aria-hidden="false"><a href="https://jakobpoerschmann.medium.com/?source=post_page---byline--3fe71fb4a98e--------------------------------" rel="noopener follow"><div class="l hv hw by hx hy"><div class="l ed"><img alt="Jakob Pörschmann" class="l ep by dd de cx" src="../Images/b130445bf9ac471b70070eb4a2dc6b64.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*TBuIfc2jgp3Nra7DtrgVgw.jpeg"/><div class="hz by l dd de em n ia eo"/></div></div></a></div></div><div class="ib ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3fe71fb4a98e--------------------------------" rel="noopener follow"><div class="l ic id by hx ie"><div class="l ed"><img alt="Towards Data Science" class="l ep by br if cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hz by l br if em n ia eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ig ab q"><div class="ab q ih"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ii ij bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ik" data-testid="authorName" href="https://jakobpoerschmann.medium.com/?source=post_page---byline--3fe71fb4a98e--------------------------------" rel="noopener follow">Jakob Pörschmann</a></p></div></div></div><span class="il im" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ii ij dx"><button class="in io ah ai aj ak al am an ao ap aq ar ip iq ir" disabled="">Follow</button></p></div></div></span></div></div><div class="l is"><span class="bf b bg z dx"><div class="ab cn it iu iv"><div class="iw ix ab"><div class="bf b bg z dx ab iy"><span class="iz l is">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ik ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3fe71fb4a98e--------------------------------" rel="noopener follow"><p class="bf b bg z ja jb jc jd je jf jg jh bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="il im" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="ji jj l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz"><div class="h k w ea eb q"><div class="kp l"><div class="ab q kq kr"><div class="pw-multi-vote-icon ed iz ks kt ku"><div class=""><div class="kv kw kx ky kz la lb am lc ld le ku"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lf lg lh li lj lk ll"><p class="bf b dy z dx"><span class="kw">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kv ln lo ab q ee lp lq" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="fp"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lm fp">7</span></p></button></div></div></div><div class="ab q ka kb kc kd ke kf kg kh ki kj kk kl km kn ko"><div class="lr k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ls an ao ap ip lt lu lv" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lw cn"><div class="l ae"><div class="ab cb"><div class="lx ly lz ma mb fq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ls an ao ap ip mc md lq me mf mg mh mi s mj mk ml mm mn mo mp u mq mr ms"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ls an ao ap ip mc md lq me mf mg mh mi s mj mk ml mm mn mo mp u mq mr ms"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ls an ao ap ip mc md lq me mf mg mh mi s mj mk ml mm mn mo mp u mq mr ms"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="0f83" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">After <a class="af fz" rel="noopener" target="_blank" href="/graph-rag-a-conceptual-introduction-41cd0d431375">discussing Graph RAG conceptually</a>, let’s bring it into production. This is how to productionize GraphRAG: completely serverless, fully parallelized to minimize inference and indexing times, and without ever touching a graph database (promise!).</p><p id="9b32" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">In this article, I will introduce you to <a class="af fz" href="https://github.com/jakobap/graphrag-light" rel="noopener ugc nofollow" target="_blank">graphrag-lite</a>, an end-to-end Graph RAG ingestion and query implementation. I published graphrag-lite as an OSS project to make your life easier when deploying graphrag on GCP. Graphrag-lite is Google Cloud-native and ready to use off the shelf. The code is designed in a modular manner, adjustable for your platform of choice.</p><h1 id="fadd" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Recap:</h1><p id="2c06" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">Retrieval Augmented Generation itself does not yet describe any specific architecture or method. It only depicts the augmentation of a given generation task with an arbitrary retrieval method. The original RAG paper (<a class="af fz" href="https://arxiv.org/abs/2005.11401" rel="noopener ugc nofollow" target="_blank">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Lewis et. al.</a>) compares a two-tower embedding approach with bag-of-words retrieval.</p><p id="7c6a" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Modern Q&amp;A systems differentiate between local and global questions. A local (extractive) question on an unstructured sample knowledge base might be “Who won the Nobel Peace Prize in 2023?”. A global (aggregative) question might be “Who are the most recent Nobel prize winners you know about?”. Text2embedding RAG leaves obvious gaps when it comes to global and structured questions. Graph RAG can close these gaps and it does that well! Via an abstraction layer, it learns the semantics of the knowledge graph communities. That builds a more “global” understanding of the indexed dataset. <a class="af fz" rel="noopener" target="_blank" href="/graph-rag-a-conceptual-introduction-41cd0d431375">Here is a conceptual intro to Graph RAG to read up on.</a></p><h1 id="6916" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">The Graph RAG pipeline</h1><p id="ff4d" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">A Graph RAG pipeline will usually follows the following steps:</p><h2 id="881e" class="oq nq gc bf nr or os ot nu ou ov ow nx nc ox oy oz ng pa pb pc nk pd pe pf pg bk">Graph Extraction</h2><p id="2ec8" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">This is the main ingestion step. Your LLM scans every incoming document with a prompt to extract relevant nodes and edges for our knowledge graph. You iterate multiple times over this prompt to assure you catch all relevant pieces of information.</p><h2 id="58a5" class="oq nq gc bf nr or os ot nu ou ov ow nx nc ox oy oz ng pa pb pc nk pd pe pf pg bk">Graph Storage</h2><p id="369d" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">You store the extracted nodes and edges in your data store of choice. Dedicated Graph DBs are one option, but they are often tedious. Graph2nosql is a Python-based interface to store and manage knowledge graphs in Firestore or any other NoSQL DB. I open sourced this project becase I did not find any other comparable, knowledge graph native option on the market.</p><h2 id="7151" class="oq nq gc bf nr or os ot nu ou ov ow nx nc ox oy oz ng pa pb pc nk pd pe pf pg bk">Community detection</h2><p id="fc91" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">Once you store your knowledge graph data you will use a community detection algorithm to identify groups of nodes that are more densely connected within each other than they are to the rest of the graph. In the context of a knowledge graph, the assumption is that dense communities cover common topics.</p><h2 id="d71b" class="oq nq gc bf nr or os ot nu ou ov ow nx nc ox oy oz ng pa pb pc nk pd pe pf pg bk">Community report generation</h2><p id="505e" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">You then instruct your LLM to generate a report for each graph community. These community reports help abstract across single topics to grasp wider, global concepts across your dataset. Community reports are stored along with your knowledge graph. This concludes the ingestion layer of the pipeline.</p><h2 id="1a9a" class="oq nq gc bf nr or os ot nu ou ov ow nx nc ox oy oz ng pa pb pc nk pd pe pf pg bk">Map-Reduce for final context building.</h2><p id="7a3a" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">At query time you follow a map-reduce pattern to generate an intermediate response to the user query for every community report in your knowledge graph. You have the LLM also rate the relevance of each intermediate query response. Finally, you rank the intermediate responses by relevance and select the top n as context for your final response to the user.</p></div></div><div class="fo"><div class="ab cb"><div class="lx ph ly pi lz pj cf pk cg pl ci bh"><figure class="pn po pp pq pr fo ps pt paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="fv fw pm"><img src="../Images/00aa36eaa8a8bf508304c06feae85d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*auY4VcZknMkP7t89hTfaRw.png"/></div></div><figcaption class="fs ft fu fv fw fx fy bf b bg z dx">Graph RAG step-by-step logic — Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="92f6" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Graph Extraction</h1><p id="1ac8" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">In the initial ingestion step, you instruct your LLM to encode your input document(s) as a graph. An extensive prompt instructs your LLM to first identify nodes of given types, and secondly edges between the nodes you identified. Just like with any LLM prompt, there is not one solution for this challenge. Here is the core of my graph extraction prompt, which I based on <a class="af fz" href="https://github.com/microsoft/graphrag" rel="noopener ugc nofollow" target="_blank">Microsoft’s OSS implementation</a>:</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="f907" class="qc nq gc pz b bg qd qe l qf qg">-Goal-<br/>Given a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.<br/><br/>-Steps-<br/>1. Identify all entities. For each identified entity, extract the following information:<br/>- entity_name: Name of the entity, capitalized<br/>- entity_type: One of the following types: [{entity_types}]<br/>- entity_description: Comprehensive description of the entity's attributes and activities<br/>Format each entity as ("entity"{tuple_delimiter}&lt;entity_name&gt;{tuple_delimiter}&lt;entity_type&gt;{tuple_delimiter}&lt;entity_description&gt;<br/><br/>2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.<br/>For each pair of related entities, extract the following information:<br/>- source_entity: name of the source entity, as identified in step 1<br/>- target_entity: name of the target entity, as identified in step 1<br/>- relationship_description: explanation as to why you think the source entity and the target entity are related to each other<br/>- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity<br/> Format each relationship as ("relationship"{tuple_delimiter}&lt;source_entity&gt;{tuple_delimiter}&lt;target_entity&gt;{tuple_delimiter}&lt;relationship_description&gt;{tuple_delimiter}&lt;relationship_strength&gt;)<br/><br/>3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **{record_delimiter}** as the list delimiter.<br/><br/>4. When finished, output {completion_delimiter}<br/><br/>&lt;Multishot Examples&gt;<br/><br/>-Real Data-<br/>######################<br/>Entity_types: {entity_types}<br/>Text: {input_text}<br/>######################<br/>Output:</span></pre><p id="f272" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">The extraction step is responsible for which information will be reflected in your knowledge base. Thus you should use a rather powerful model such as Gemini 1.5 Pro. You can further increase the result robustness, use the multi-turn version Gemini 1.5 Pro, and query the model to improve its results n times. Here is how I implemented the graph extraction loop in graphrag-lite:</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="951b" class="qc nq gc pz b bg qd qe l qf qg">class GraphExtractor:<br/>    def __init__(self, graph_db) -&gt; None:<br/>        self.tuple_delimiter = "&lt;|&gt;"<br/>        self.record_delimiter = "##"<br/>        self.completion_delimiter = "&lt;|COMPLETE|&gt;"<br/>        self.entity_types = ["organization", "person", "geo", "event"]<br/><br/>        self.graph_extraction_system = prompts.GRAPH_EXTRACTION_SYSTEM.format(<br/>            entity_types=", ".join(self.entity_types),<br/>            record_delimiter=self.record_delimiter,<br/>            tuple_delimiter=self.tuple_delimiter,<br/>            completion_delimiter=self.completion_delimiter,<br/>        )<br/><br/>        self.llm = LLMSession(system_message=self.graph_extraction_system,<br/>                              model_name="gemini-1.5-pro-001")<br/><br/>    def __call__(self, text_input: str, max_extr_rounds: int = 5) -&gt; None:<br/><br/>        input_prompt = self._construct_extractor_input(input_text=text_input)<br/><br/>        print("+++++ Init Graph Extraction +++++")<br/><br/>        init_extr_result = self.llm.generate_chat(<br/>            client_query_string=input_prompt, temperature=0, top_p=0)<br/>        print(f"Init result: {init_extr_result}")<br/><br/>        for round_i in range(max_extr_rounds):<br/><br/>            print(f"+++++ Contd. Graph Extraction round {round_i} +++++")<br/><br/>            round_response = self.llm.generate_chat(<br/>                client_query_string=prompts.CONTINUE_PROMPT, temperature=0, top_p=0)<br/>            init_extr_result += round_response or ""<br/><br/>            print(f"Round response: {round_response}")<br/><br/>            if round_i &gt;= max_extr_rounds - 1:<br/>                break<br/><br/>            completion_check = self.llm.generate_chat(<br/>                client_query_string=prompts.LOOP_PROMPT, temperature=0, top_p=0)<br/><br/>            if "YES" not in completion_check:<br/>                print(<br/>                    f"+++++ Complete with completion check after round {round_i} +++++")<br/>                break</span></pre><p id="908d" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">First I make an initial call to the multi-turn model to extract nodes and edges. Second I ask the model to improve the previous extraction results several times.</p><p id="c10f" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">In the graphrag-lite implementation, the extraction model calls are made by the front-end client. If you want to reduce client load you could outsource the extraction queries to a microservice.</p><h1 id="572c" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Graph Storage</h1><p id="e47f" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">Once you extract the nodes and edges from a document you need to store them in an accessible format. Graph Databases are one way to go, but they can also be cumbersome. For your knowledge graph, you might be looking for something a little more lightweight. I thought the same, because I did not find any knowledge graph native library I open sources graph2nosql. <a class="af fz" href="https://github.com/jakobap/graph2nosql" rel="noopener ugc nofollow" target="_blank">Graph2nosql is a simple knowledge graph native Python interface</a>. It helps store and manage your knowledge graph in any NoSQL DB. All that without blowing up your tech stack with a graph db or needing to learn Cypher.</p><p id="c4e0" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Graph2nosql is designed for knowledge graph retrieval with graph rag in mind. The library is designed around three major datatypes: EdgeData, NodeData, and CommunityData. Nodes are identified by an uid. Edges are identified by source and destination node uid and an edge uid. Given that uids can be freely designed, the graph2nosql data model leaves space for any size of the knowledge graph. You can even add text or graph embeddings. That allows embedding-based analytics, edge prediction, and additional text embedding retrieval (thinking hybrid RAG).</p><p id="bcca" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk"><a class="af fz" href="https://github.com/jakobap/graph2nosql" rel="noopener ugc nofollow" target="_blank">Graph2nosql</a> is natively designed around Firestore.</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="55ff" class="qc nq gc pz b bg qd qe l qf qg">@dataclass<br/>class EdgeData:<br/>    source_uid: str<br/>    target_uid: str <br/>    description: str<br/>    edge_uid: str | None = None<br/>    document_id: str | None = None<br/><br/>@dataclass<br/>class NodeData:<br/>    node_uid: str<br/>    node_title: str<br/>    node_type: str<br/>    node_description: str<br/>    node_degree: int<br/>    document_id: str <br/>    community_id: int | None = None # community id based on source document <br/>    edges_to: list[str] = field(default_factory=list)<br/>    edges_from: list[str] = field(default_factory=list)  # in case of directed graph<br/>    embedding: list[float] = field(default_factory=list)  # text embedding representing node e.g. combination of title &amp; description<br/><br/>@dataclass<br/>class CommunityData:<br/>    title: str # title of comm, None if not yet computed<br/>    community_nodes: set[str] = field(default_factory=set) # list of node_uid belonging to community<br/>    summary: str | None = None # description of comm, None if not yet computed<br/>    document_id: str | None = None # identifier for source knowlede base document for this entity<br/>    community_uid: str | None = None # community identifier<br/>    community_embedding: Tuple[float, ...] = field(default_factory=tuple) # text embedding representing community<br/>    rating: int | None = None<br/>    rating_explanation: str | None = None<br/>    findings: list[dict] | None = None</span></pre><p id="ce18" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">To store your graph data via graph2nosql simply run the following code when parsing the results from your extraction step. Here is the graphrag-lite implementation.</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="a771" class="qc nq gc pz b bg qd qe l qf qg">from graph2nosql.graph2nosql.graph2nosql import NoSQLKnowledgeGraph<br/>from graph2nosql.databases.firestore_kg import FirestoreKG<br/>from graph2nosql.datamodel import data_model<br/><br/>fskg = FirestoreKG(<br/>        gcp_project_id=project_id,<br/>        gcp_credential_file=firestore_credential_file,<br/>        firestore_db_id=database_id,<br/>        node_collection_id=node_coll_id,<br/>        edges_collection_id=edges_coll_id,<br/>        community_collection_id=community_coll_id)<br/><br/>node_data = data_model.NodeData(<br/>        node_uid=entity_name,<br/>        node_title=entity_name,<br/>        node_type=entity_type,<br/>        node_description=entity_description,<br/>        document_id=str(source_doc_id),<br/>        node_degree=0)<br/><br/>fskg.add_node(node_uid=entity_name,node_data=node_data)</span></pre><h1 id="d363" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Community detection</h1><p id="0890" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">With all relevant nodes and edges stored in your Graph DB, you can start building the abstraction layer. One way of doing that is finding nodes that describe similar concepts and describe how they are connected semantically. Graph2nosql offers inbuilt community detection, for example, based on Louvain communities.</p><p id="e45d" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Depending on your extraction result quality you will find zero-degree nodes in your knowledge graph. From experience, zero-degree nodes are often duplicates. Graphrag-lite uses graph communities as a major abstraction layer thus you should drop the nodes without any edges. Thus it would make sense to think about another duplicate/merge step and/or a node prediction step based on description and graph embeddings to add edges that might have been missed in the extraction step. In graphrag-lite I currently simply drop all zero-degree nodes.</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="5c83" class="qc nq gc pz b bg qd qe l qf qg"># clean graph off all nodes without any edges<br/>fskg.clean_zerodegree_nodes()<br/><br/># generate communities based on cleaned graph<br/>comms = kg.get_louvain_communities()</span></pre><p id="6302" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Here is the <a class="af fz" href="https://github.com/jakobap/graphrag-light/blob/39d7ed73fe23509951a8c93cc4806499110e1433/graphrag_lite/GraphExtractor.py#L508C1-L509C1" rel="noopener ugc nofollow" target="_blank">graphrag-lite implementation of community detection</a>.</p><h1 id="1f08" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Optimizing throughput latency in LLM applications</h1><p id="a8f9" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">The GraphRAG pipeline mentioned above takes numerous LLM calls per document ingestion and user query. For example, to generate multiple community reports for every newly indexed document, or to generate intermediate responses for multiple communities at query time. If processed concurrently an awful user experience will be the result. Especially at scale users will have to wait minutes to hours to receive a response to their query. Fortunately, if you frame your LLM prompts the right way you can design them as “stateless workers”. The power of stateless processing architectures is twofold. Firstly, they are easy to parallelize. Secondly, they are easy to implement as serverless infastructure. Combined with a parallelized and serverless architecture maximizes your throughput scalability and minimizes your cost for idle cluster setups.</p><p id="83b9" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">In the graphrag-lite architecture I host both the community report generation and the intermediate query generation as serverless Cloud Run microservice workers. These are fed with messages via GCP’s serverless messaging queue PubSub.</p></div></div><div class="fo"><div class="ab cb"><div class="lx ph ly pi lz pj cf pk cg pl ci bh"><figure class="pn po pp pq pr fo ps pt paragraph-image"><div role="button" tabindex="0" class="pu pv ed pw bh px"><div class="fv fw qh"><img src="../Images/c49303b5e6ca48937ad3fc3115f7eb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*7jLVk_KtHlXX_qfj0oAYNw.png"/></div></div><figcaption class="fs ft fu fv fw fx fy bf b bg z dx">graphrag-lite’s serverless and distributed ingestion &amp; query pipeline — Image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6e1a" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Community report generation</h1><p id="4c70" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">After running the community detection you now know multiple sets of community member nodes. Each of these sets represents a semantic topic within your knowledge graph. The community reporting step needs to abstract across these concepts that originated in different documents within your knowledge base. I again built on the Microsoft implementation and added a function call for easily parsable structured output.</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="d64d" class="qc nq gc pz b bg qd qe l qf qg">You are an AI assistant that helps a human analyst to perform general information discovery. Information discovery is the process of identifying and assessing relevant information associated with certain entities (e.g., organizations and individuals) within a network.<br/><br/># Goal<br/>Write a comprehensive report of a community, given a list of entities that belong to the community as well as their relationships and optional associated claims. The report will be used to inform decision-makers about information associated with the community and their potential impact. The content of this report includes an overview of the community's key entities, their legal compliance, technical capabilities, reputation, and noteworthy claims.<br/><br/># Report Structure<br/><br/>The report should include the following sections:<br/><br/>- TITLE: community's name that represents its key entities - title should be short but specific. When possible, include representative named entities in the title.<br/>- SUMMARY: An executive summary of the community's overall structure, how its entities are related to each other, and significant information associated with its entities.<br/>- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by entities within the community.  IMPACT is the scored importance of a community.<br/>- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.<br/>- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules below. Be comprehensive.</span></pre><p id="831c" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">The community report generation also demonstrated the biggest challenge around knowledge graph retrieval. Theoretically, any document could add a new node to every existing community in the graph. In the worst-case scenario, you re-generate every community report in your knowledge base for each new document added. In practice it is crucial to include a detection step that identifies which communities have changed after a document upload, resulting in new report generation for only the adjusted communities.</p><p id="4145" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">As you need to re-generate multiple community reports for every document upload we are also facing significant latency challenges if running these requests concurrently. Thus you should outsource and parallelize this work to asynchronous workers. As mentioned before, graphrag-lite solved this using a serverless architecture. I use PubSub as a message queue to manage work items and ensure processing. Cloud Run comes on top as a compute platform hosting stateless workers calling the LLM. For generation, they use the prompt as shown above.</p><p id="9359" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk"><a class="af fz" href="https://github.com/jakobap/graphrag-light/blob/main/stateless-comm-reporter/main.py" rel="noopener ugc nofollow" target="_blank">Here is the code that runs in the stateless worker for community report generation</a>:</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="81bf" class="qc nq gc pz b bg qd qe l qf qg">def async_generate_comm_report(self, comm_members: set[str]) -&gt; data_model.CommunityData:<br/><br/>        llm = LLMSession(system_message=prompts.COMMUNITY_REPORT_SYSTEM,<br/>                         model_name="gemini-1.5-flash-001")<br/><br/>        response_schema = {<br/>            "type": "object",<br/>            "properties": {<br/>                    "title": {<br/>                        "type": "string"<br/>                    },<br/>                "summary": {<br/>                        "type": "string"<br/>                        },<br/>                "rating": {<br/>                        "type": "int"<br/>                        },<br/>                "rating_explanation": {<br/>                        "type": "string"<br/>                        },<br/>                "findings": {<br/>                        "type": "array",<br/>                        "items": {<br/>                            "type": "object",<br/>                            "properties": {<br/>                                "summary": {<br/>                                    "type": "string"<br/>                                },<br/>                                "explanation": {<br/>                                    "type": "string"<br/>                                }<br/>                            },<br/>                            # Ensure both fields are present in each finding<br/>                            "required": ["summary", "explanation"]<br/>                        }<br/>                        }<br/>            },<br/>            # List required fields at the top level<br/>            "required": ["title", "summary", "rating", "rating_explanation", "findings"]<br/>        }<br/><br/>        comm_report  = llm.generate(client_query_string=prompts.COMMUNITY_REPORT_QUERY.format(<br/>            entities=comm_nodes,<br/>            relationships=comm_edges,<br/>            response_mime_type="application/json",<br/>            response_schema=response_schema<br/>        ))<br/><br/>comm_data = data_model.CommunityData(title=comm_report_dict["title"],                                              summary=comm_report_dict["summary"],                                                rating=comm_report_dict["rating"],             rating_explanation=comm_report_dict["rating_explanation"],               findings=comm_report_dict["findings"],<br/>community_nodes=comm_members)<br/> <br/> return comm_data</span></pre><p id="6422" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">This completes the ingestion pipeline.</p><h1 id="bed5" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Map-step for intermediate responses</h1><p id="0f4c" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">Finally, you reached query time. To generate your final response to the user, you generate a set of intermediate responses (one per community report). Each intermediate response takes the user query and one community report as input. You then rate these intermediate queries by their relevance. Finally, you use the most relevant community reports and additional information such as node descriptions of the relevant member nodes as the final query context. Given a high number of community reports at scale, this again poses a challenge of latency and cost. Similar to previously you should also parallelize the intermediate response generation (map-step) across serverless microservices. In the future, you could significantly improve efficiency by adding a filter layer to pre-determine the relevance of a community report for a user query.</p><p id="7a4f" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk"><a class="af fz" href="https://github.com/jakobap/graphrag-light/blob/main/stateless-context-processor/main.py" rel="noopener ugc nofollow" target="_blank">The map-step microservice looks as follows</a>:</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="22ea" class="qc nq gc pz b bg qd qe l qf qg">def generate_response(client_query: str, community_report: dict):<br/><br/>    llm = LLMSession(<br/>        system_message=MAP_SYSTEM_PROMPT,<br/>        model_name="gemini-1.5-pro-001"<br/>    )<br/><br/>    response_schema = {<br/>        "type": "object",<br/>        "properties": {<br/>            "response": {<br/>                "type": "string",<br/>                "description": "The response to the user question as raw string.",<br/>            },<br/>            "score": {<br/>                "type": "number",<br/>                "description": "The relevance score of the given community report context towards answering the user question [0.0, 10.0]",<br/>            },<br/>        },<br/>        "required": ["response", "score"],<br/>    }<br/><br/>    query_prompt = MAP_QUERY_PROMPT.format(<br/>        context_community_report=community_report, user_question=client_query)<br/><br/>    response = llm.generate(client_query_string=query_prompt,<br/>                 response_schema=response_schema,<br/>                 response_mime_type="application/json")<br/><br/>    return response</span></pre><p id="f899" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">The map-step microservice uses the following prompt:</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="4247" class="qc nq gc pz b bg qd qe l qf qg">---Role---<br/>You are an expert agent answering questions based on context that is organized as a knowledge graph.<br/>You will be provided with exactly one community report extracted from that same knowledge graph.<br/><br/><br/>---Goal---<br/>Generate a response consisting of a list of key points that responds to the user's question, summarizing all relevant information in the given community report.<br/><br/>You should use the data provided in the community description below as the only context for generating the response.<br/>If you don't know the answer or if the input community description does not contain sufficient information to provide an answer respond "The user question cannot be answered based on the given community context.".<br/><br/>Your response should always contain following elements:<br/>- Query based response: A comprehensive and truthful response to the given user query, solely based on the provided context.<br/>- Importance Score: An integer score between 0-10 that indicates how important the point is in answering the user's question. An 'I don't know' type of response should have a score of 0.<br/><br/>The response should be JSON formatted as follows:<br/>{{"response": "Description of point 1 [Data: Reports (report ids)]", "score": score_value}}<br/><br/>---Context Community Report---<br/>{context_community_report}<br/><br/>---User Question---<br/>{user_question}<br/><br/>---JSON Response---<br/>The json response formatted as follows:<br/>{{"response": "Description of point 1 [Data: Reports (report ids)]", "score": score_value}}<br/><br/>response: </span></pre><h1 id="fa3e" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Reduce-step for final user response</h1><p id="1c07" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">For a successful reduce-step, you need to store the intermediate response for access at query time. With graphrag-lite, I use Firestore as a shared state across microservices. After triggering the intermediate response generations, the client also periodically checks for the existence of all expected entries in the shared state. The following code extract from graphrag-lite shows how I submit every community report to the PubSub queue. After, I periodically query the shared state to check whether all intermediate responses have been processed. Finally, the end response towards the user is generated using the top-scoring community reports as context to respond to the user query.</p><pre class="pn po pp pq pr py pz qa bp qb bb bk"><span id="759e" class="qc nq gc pz b bg qd qe l qf qg">class KGraphGlobalQuery:<br/>    def __init__(self) -&gt; None:<br/>        # initialized with info on mq, knowledge graph, shared nosql state<br/>        pass<br/><br/>    @observe()<br/>    def __call__(self, user_query: str) -&gt; str:<br/><br/>        # orchestration method taking natural language user query to produce and return final answer to client<br/>        comm_report_list = self._get_comm_reports()<br/><br/>        # pair user query with existing community reports<br/>        query_msg_list = self._context_builder(<br/>            user_query=user_query, comm_report_list=comm_report_list)<br/><br/>        # send pairs to pubsub queue for work scheduling<br/>        for msg in query_msg_list:<br/>            self._send_to_mq(message=msg)<br/>        print("int response request sent to mq")<br/><br/>        # periodically query shared state to check for processing compeltion &amp; get intermediate responses<br/>        intermediate_response_list = self._check_shared_state(<br/>            user_query=user_query)<br/><br/>        # based on helpfulness build final context<br/>        sorted_final_responses = self._filter_and_sort_responses(intermediate_response_list=intermediate_response_list)<br/><br/>        # get full community reports for the selected communities<br/>        comm_report_list = self._get_communities_reports(sorted_final_responses)<br/><br/>        # generate &amp; return final response based on final context community repors and nodes.<br/>        final_response_system = prompts.GLOBAL_SEARCH_REDUCE_SYSTEM.format(<br/>            response_type="Detailled and wholistic in academic style analysis of the given information in at least 8-10 sentences across 2-3 paragraphs.")<br/><br/>        llm = LLMSession(<br/>            system_message=final_response_system,<br/>            model_name="gemini-1.5-pro-001"<br/>        )<br/><br/>        final_query_string = prompts.GLOBAL_SEARCH_REDUCE_QUERY.format(<br/>            report_data=comm_report_list,<br/>            user_query=user_query<br/>        )<br/>        final_response = llm.generate(client_query_string=final_query_string)<br/>        return final_response</span></pre><p id="51fc" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Once all entries are found the client triggers the final user response generation given the selected community context.</p><h1 id="71d9" class="np nq gc bf nr ns nt hc nu nv nw hf nx ny nz oa ob oc od oe of og oh oi oj ok bk">Final Thoughts</h1><p id="8a43" class="pw-post-body-paragraph mt mu gc mv b ha ol mx my hd om na nb nc on ne nf ng oo ni nj nk op nm nn no fj bk">Graph RAG is a powerful technique every ML Engineer should add to their toolbox. Every Q&amp;A type of application will eventually arrive at the point that purely extractive, “local” queries don’t cut it anymore. With graphrag-lite, you now have a lightweight, cloud-native, and serverless implementation that you can rapidly replicate.</p><p id="915e" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">Despite these strengths, please note that in the current state Graph RAG still consumes significantly more LLM input tokens than in the text2emb RAG. That usually comes with considerably higher latency and cost for queries and document indexing. Nevertheless, after experiencing the improvement in result quality I am convinced that in the right use cases, Graph RAG is worth the time and money.</p><p id="3baa" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">RAG applications will ultimately move in a hybrid direction. Extractive queries can be handled efficiently and correctly by text2emb RAG. Global abstractive queries might need a knowledge graph as an alternative retrieval layer. Finally, both methods underperform with quantitative and analytical queries. Thus a third text2sql retrieval layer would add massive value. To complete the picture, user queries could initially be classified between the three retrieval methods. Like this, every query could be grounded most efficiently with the right amount and depth of information.</p><p id="d73b" class="pw-post-body-paragraph mt mu gc mv b ha mw mx my hd mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no fj bk">I cannot wait to see where else this is going. <strong class="mv gd">Which alternative retrieval methods have you been working with?</strong></p></div></div></div></div>    
</body>
</html>