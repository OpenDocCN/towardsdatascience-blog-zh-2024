- en: Evaluating LLMs in Cypher Statement Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19](https://towardsdatascience.com/evaluating-llms-in-cypher-statement-generation-c570884089b3?source=collection_archive---------2-----------------------#2024-01-19)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step-by-step tutorial for assessing the accuracy of generated Cypher Statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[![Tomaz
    Bratanic](../Images/d5821aa70918fcb3fc1ff0013497b3d5.png)](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    [Tomaz Bratanic](https://bratanic-tomaz.medium.com/?source=post_page---byline--c570884089b3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c570884089b3--------------------------------)
    ·9 min read·Jan 19, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88372b2e2c33a680583b4fc6098d1e1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after large language models (LLMs) became popular, we realized they
    were decent at translating natural language to database queries such as SQL and
    Cypher. To enable the LLM to create tailored queries for your particular database,
    you must provide its schema and, optionally, a few example queries. With this
    information, the LLM can generate database queries based on natural language input.
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs show great potential at translating natural language to database
    queries, they are far from perfect. Therefore, it is essential to understand how
    well they perform using an evaluation process. Luckily, the process of generating
    SQL statements has been researched by academia in studies like the [Spider](https://arxiv.org/abs/1809.08887).
    We will use the following metrics to evaluate the Cypher generation ability of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Jaro-Winkler**: This is a text similarity metric based on edit distance.
    We compare the produced Cypher query to a correct Cypher query, and measure how
    different the strings are, by how much one would have to edit one query to be
    the same as the other query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pass@1:** This score is 1.0 if the produced query returns the same results
    from the database as the correct Cypher would, and 0.0 otherwise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pass@3:** Similar to Pass@1, but instead of generating 1 query, we generate
    3 queries. If any of them produce the same results as the correct query the score
    is 1.0, otherwise 0.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jaccard similarity:** It measures the Jaccard similarity between the response
    returned by the produced Cypher and the correct Cypher’s response. This metric
    is used in mind to capture examples where the model may return almost correct
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code is available on [GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb)
    as was developed in collaboration with [Adam Schill Collberg](https://medium.com/u/8b125c49d121?source=post_page---user_mention--c570884089b3--------------------------------).
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, the focus is on evaluating responses from the database and
    not the actual Cypher statement itself. One reason is that a Cypher statement
    can be written multiple ways to retrieve identical information. We don’t care
    which syntax the LLM prefers; we only care that it produces correct responses.
    Additionally, we don’t have a strong preference for how the LLM names the column
    in responses, and therefore, we don’t want to evaluate its column naming abilities,
    etc…
  prefs: []
  type: TYPE_NORMAL
- en: Test dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The test dataset consists of question and relevant Cypher statement pairs.
  prefs: []
  type: TYPE_NORMAL
- en: You can use an LLM to generate suggestions for the testing dataset. However,
    you need to manually validate the examples as the LLMs make mistakes and aren’t
    100% reliable. If they were, we wouldn’t need to test them anyway. As we are evaluating
    based on database results and not the Cypher statements themselves, we need to
    have a running database with relevant information that we can use. In this blog
    post, we will use the [recommendations project in Neo4j Sandbox](http://ndbox.neo4j.com/?usecase=recommendations).
    The recommendations project uses the [MovieLens dataset](https://grouplens.org/datasets/movielens/),
    which contains movies, actors, ratings, and more information. The recommendations
    project is also available as readonly access on the demo server, which means you
    don’t have to create a new database instance if you don’t want to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62d91638c35854e38d31dbd85606726e.png)'
  prefs: []
  type: TYPE_IMG
- en: Recommendations project graph schema. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, I used GPT-4 to come up with suggestions for the training dataset,
    and then went through them and corrected them where needed. We will use only 27
    testing pairs. In practice, you probably want to use at least a couple hundred
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Generating Cypher statements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using [LangChain](https://www.langchain.com/) to generate Cypher
    statements. The [Neo4jGraph](https://python.langchain.com/docs/use_cases/graph/graph_cypher_qa)
    object in LangChain establishes the connection to Neo4j and retrieves its schema
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The schema contains node labels, their properties, and the corresponding relationships.
    Next, we will use the LangChain expression language to define a prompt sent to
    the LLM with instructions to translate the natural language to a Cypher statement
    that retrieves relevant information to answer the question. Visit the [official
    documentation](https://python.langchain.com/docs/expression_language/) for more
    details on the LangChain expression language.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you are familiar with conversational LLMs, you can spot the **system** and
    **human** message definitions. As you can observe, we put both the graph schema
    as well as user question into the human message. The exact prompt engineering
    instructions to generate Cypher statements is not a solved problem, which means
    that there could be some improvements made here. Using an evaluation process,
    you could see what works best for the particular LLM. In this example, we are
    using **gpt-4-turbo**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test the Cypher generation with the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that gpt-4-turbo is somewhat decent at translating natural language
    to Cypher statements. Let’s now define the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Running this code took about 5 minutes as we need to generate 81 responses
    to calculate the pass@3 metric.*'
  prefs: []
  type: TYPE_NORMAL
- en: The code is slightly lengthy. However, the gist is quite simple to understand.
    We iterate over all the rows in the data frame that stores the testing examples.
    Next, we generate three Cypher queries for each training example and retrieve
    corresponding data from the database. What follows is then calculating the relevant
    metrics and storing them in lists so that we can evaluate and visualize them.
    We didn’t include the helper functions in the blog post because, in my opinion,
    reviewing each metric code implementation isn’t relevant. All of these functions,
    however, are included in the [notebook](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8aaa45fbcae5ebb94cecbd739f9b8a76.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation of LLM-generated Cypher statements. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation is based on four different metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Jaro-Winkler**: This metric shows a high average of 0.89, indicating that
    the LLMs generated Cypher queries that are very similar to the correct Cypher
    queries on a string level.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pass@1**: The average score here is 0.48, suggesting that nearly half of
    the generated Cypher queries returned the exact same results as the correct query
    when each query is evaluated independently.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pass@3**: With an average of 0.63, this metric indicates an improvement over
    Pass@1\. This suggests that while the LLM may not always get the query right on
    the first attempt, it often comes up with a correct version within three tries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Jaccard Similarity**: The average score of 0.53 is the lowest among the metrics
    but still indicates that more than half of the time, the sets of results from
    the LLM-generated Cypher queries share more than half of their elements with the
    sets from the correct queries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overall, these metrics suggest that LLMs are decent at generating Cypher queries
    that are similar to the correct ones and often produce functionally equivalent
    results, especially when given multiple attempts. However, there is still room
    for improvement, particularly in generating the correct query on the first attempt.
    Additionally there is also room for improvement in the evaluation process. Let’s
    take a look at one example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For the question of which directors never had a movie with a rating below 6.0,
    the LLM did a decent job of getting the results. It used a different approach
    than in the test dataset, but that’s not a problem, as it should get the same
    results. However, we returned both the title and the movie’s rating in the testing
    data. On the other hand, the LLM produced only the titles and not the ratings.
    We can’t blame it, as it simply followed the instructions. Still, you must know
    that the pass@1 score is 0 in this example, while the Jaccard similarity is only
    0.5\. Therefore, you have to very careful how you construct the testing dataset,
    both how you define the prompts as well as the corresponding Cypher statements.
  prefs: []
  type: TYPE_NORMAL
- en: Another characteristic of LLMs is that they are non-deterministic, meaning you
    can get different results on every run. Let’s run the evaluation three times in
    sequence now. This evaluation takes around 15 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88a9e606be5ed7980e371f818c34ad89.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation of LLM-generated Cypher statements. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The bar chart highlights the non-deterministic nature of LLMs. The Jaro-Winkler
    scores are consistently high across all runs, showing minor fluctuations between
    0.88 and 0.89, which indicates stable string similarity of the generated queries.
    However, for Pass@1, there’s a notable variation, with the first run scoring 0.52,
    and subsequent runs showing scores of 0.59 and 0.48\. Pass@3 scores exhibit less
    variance, hovering around 0.56 to 0.63, suggesting that multiple attempts yield
    more consistent correct results.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Through this blog post, we’ve learned that LLMs like GPT-4 have a promising
    capacity for generating Cypher queries, yet the technology isn’t foolproof. The
    evaluation framework presented offers a detailed, quantitative evaluation of LLM
    performance, allowing you to continuously experiment and update prompt engineering
    and other steps needed to generate valid and accurate Cypher statements. Additionally,
    it shows how the non-deterministic nature of LLMs affects the performance from
    one evaluation to another. Therefore, you can expect similar non-deterministic
    behavior in production.
  prefs: []
  type: TYPE_NORMAL
- en: The code is available on [GitHub](https://github.com/tomasonjo/blogs/blob/master/llm/evaluating_cypher.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'F. Maxwell Harper and Joseph A. Konstan. 2015\. The MovieLens Datasets: History
    and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4:
    19:1–19:19\. [https://doi.org/10.1145/2827872](https://doi.org/10.1145/2827872)'
  prefs: []
  type: TYPE_NORMAL
