- en: Transformers Key-Value (KV) Caching Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=collection_archive---------0-----------------------#2024-12-11](https://towardsdatascience.com/transformers-key-value-kv-caching-explained-4d71de62d22d?source=collection_archive---------0-----------------------#2024-12-11)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Speed up your LLM inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michaloleszak.medium.com/?source=post_page---byline--4d71de62d22d--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page---byline--4d71de62d22d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4d71de62d22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4d71de62d22d--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page---byline--4d71de62d22d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4d71de62d22d--------------------------------)
    ·14 min read·Dec 11, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8e43dcc06296939da333306fa6544dd.png)'
  prefs: []
  type: TYPE_IMG
- en: The transformer architecture is arguably one of the most impactful innovations
    in modern deep learning. Proposed in the famous [2017 paper “Attention Is All
    You Need](https://arxiv.org/abs/1706.03762),” it has become the go-to approach
    for most language-related modeling, including all Large Language Models (LLMs),
    such as the [GPT family](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer),
    as well as many computer vision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: As the complexity and size of these models grow, so does the need to optimize
    their inference speed, especially in chat applications where the users expect
    immediate replies. Key-value (KV) caching is a clever trick to do just that —
    let’s see how it works and when to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer architecture overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into KV caching, we will need to take a short detour to the attention
    mechanism used in transformers. Understanding how it works is required to spot
    and appreciate how KV caching optimizes transformer inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will focus on autoregressive models used to generate text. These so-called
    decoder models include the [GPT family](https://platform.openai.com/docs/models),
    [Gemini](https://gemini.google.com/), [Claude](https://www.anthropic.com/claude),
    or [GitHub Copilot](https://github.com/features/copilot). They are trained on
    a simple task: predicting the next token in sequence. During inference, the model
    is provided with some text, and its task is…'
  prefs: []
  type: TYPE_NORMAL
