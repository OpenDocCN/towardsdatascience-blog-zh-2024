- en: Speculative Decoding for Faster Inference with Mixtral-8x7B and Gemma
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08](https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using quantized models for memory-efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    ·7 min read·Mar 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a5b71b90758f6c6a940f19dacf2f8cc.png)'
  prefs: []
  type: TYPE_IMG
- en: A speculating llama — Generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Larger language models typically deliver superior performance but at the cost
    of reduced inference speed. For example, Llama 2 70B significantly outperforms
    Llama 2 7B in downstream tasks, but its inference speed is approximately 10 times
    slower.
  prefs: []
  type: TYPE_NORMAL
- en: Many techniques and adjustments of decoding hyperparameters can speed up inference
    for very large LLMs. Speculative decoding, in particular, can be very effective
    in many use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Speculative decoding uses a small LLM to generate the tokens which are then
    validated, or corrected if needed, by a much better and larger LLM. If the small
    LLM is accurate enough, speculative decoding can dramatically speed up inference.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I first explain how speculative decoding works. Then, I show
    how to run speculative decoding with different pairs of models involving Gemma,
    Mixtral-8x7B, Llama 2, and Pythia, all quantized. I benchmarked the inference
    throughput and memory consumption to highlight what configurations work the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'Speculative Decoding: Draft and Validate with Two LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
