- en: Speculative Decoding for Faster Inference with Mixtral-8x7B and Gemma
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推测性解码：使用Mixtral-8x7B和Gemma加速推理
- en: 原文：[https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08](https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08](https://towardsdatascience.com/speculative-decoding-for-faster-inference-with-mixtral-8x7b-and-gemma-f5b1487f5714?source=collection_archive---------4-----------------------#2024-03-08)
- en: Using quantized models for memory-efficiency
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用量化模型以提高内存效率
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--f5b1487f5714--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    ·7 min read·Mar 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f5b1487f5714--------------------------------)
    ·阅读时间：7分钟·2024年3月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5a5b71b90758f6c6a940f19dacf2f8cc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a5b71b90758f6c6a940f19dacf2f8cc.png)'
- en: A speculating llama — Generated by DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一只推测性思考的骆驼 — 由DALL-E生成
- en: Larger language models typically deliver superior performance but at the cost
    of reduced inference speed. For example, Llama 2 70B significantly outperforms
    Llama 2 7B in downstream tasks, but its inference speed is approximately 10 times
    slower.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的语言模型通常能够提供更强的性能，但以降低推理速度为代价。例如，Llama 2 70B在下游任务中的表现远超Llama 2 7B，但其推理速度大约慢10倍。
- en: Many techniques and adjustments of decoding hyperparameters can speed up inference
    for very large LLMs. Speculative decoding, in particular, can be very effective
    in many use cases.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 许多解码超参数的技巧和调整可以加速非常大规模语言模型（LLM）的推理。尤其是推测性解码，在许多使用场景中都非常有效。
- en: Speculative decoding uses a small LLM to generate the tokens which are then
    validated, or corrected if needed, by a much better and larger LLM. If the small
    LLM is accurate enough, speculative decoding can dramatically speed up inference.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 推测性解码使用一个小型LLM来生成tokens，然后由一个更强大、更大的LLM进行验证，或在必要时进行修正。如果小型LLM足够准确，推测性解码可以显著加快推理速度。
- en: In this article, I first explain how speculative decoding works. Then, I show
    how to run speculative decoding with different pairs of models involving Gemma,
    Mixtral-8x7B, Llama 2, and Pythia, all quantized. I benchmarked the inference
    throughput and memory consumption to highlight what configurations work the best.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我首先解释了推测性解码的工作原理。然后，我展示了如何使用不同的模型对，包括Gemma、Mixtral-8x7B、Llama 2和Pythia，所有模型均为量化版本，来运行推测性解码。我通过基准测试推理吞吐量和内存消耗，以突出显示哪些配置效果最佳。
- en: 'Speculative Decoding: Draft and Validate with Two LLMs'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推测性解码：使用两个LLM进行草稿和验证
