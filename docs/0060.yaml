- en: 'Ablation Testing Neural Networks: The Compensatory Masquerade'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的消融测试：补偿性伪装
- en: 原文：[https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07](https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07](https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07)
- en: Disruptively testing parts of neural networks and other ML architectures to
    make them more robust
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 破坏性地测试神经网络和其他机器学习架构的部分，目的是使其变得更加健壮
- en: '[](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)[![Shafik
    Quoraishee](../Images/439d3502b98af4d994a8fab33b8bb428.png)](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)
    [Shafik Quoraishee](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)[![Shafik
    Quoraishee](../Images/439d3502b98af4d994a8fab33b8bb428.png)](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)
    [Shafik Quoraishee](https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)
    ·8 min read·Jan 7, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------)
    ·8分钟阅读·2024年1月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/17facce6e4c4b29da7fcbbde246cf1b7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17facce6e4c4b29da7fcbbde246cf1b7.png)'
- en: (Image generated by author using DALL-E). Interesting what AI thinks of it’s
    own brain.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者使用DALL-E生成）。有趣的是AI如何看待它自己的大脑。
- en: In a similar fashion to how a person’s intellect can be stress tested, Artificial
    Neural Networks can be subjected to a gamut of tests to evaluate how robust they
    are to different kinds of disruption, by running what’s called controlled Ablation
    Testing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于一个人的智力如何经受压力测试，人工神经网络也可以通过进行所谓的受控消融测试，来评估它们对不同类型干扰的鲁棒性。
- en: 'Before we get into [ablation testing](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)),
    lets talk about a familiar technique in “destructive evolution” that many people
    who study machine learning and artificial intelligence applications might be familiar
    with: [Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics))'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨[消融测试](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence))之前，让我们先谈谈一种许多学习机器学习和人工智能应用的人可能熟悉的“破坏性进化”技术：[正则化](https://en.wikipedia.org/wiki/Regularization_(mathematics))
- en: '**Regularization**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**正则化**'
- en: Regulariztion is a very well known example of ablating, or selectively destroying/deactivating
    parts of a neural network and re-training it to make it an even more powerful
    classifier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是消融的一个非常著名的例子，或者说是选择性地破坏/停用神经网络的部分并重新训练它，使其成为一个更强大的分类器。
- en: Through a process called [Dropout](https://en.wikipedia.org/wiki/Dilution_(neural_networks)),
    neurons can be deactivated in a controlled way, which allow the work of the neural
    network that was previously handled by the now defunct neurons, to be taken up
    by nearby active neurons.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一种叫做[Dropout](https://en.wikipedia.org/wiki/Dilution_(neural_networks))的过程，神经元可以以受控的方式被停用，这使得之前由现在已停用的神经元处理的工作能够被附近的活跃神经元接管。
- en: In nature, the brain actually can undergo similar phenomenon due to the concept
    of neuro-plasticity. If a person suffers brain damage, in some cases nearby neurons
    and brain structures can reorganize to help take up some of the functionality
    of the dead brain tissue.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中，大脑实际上也能由于神经可塑性而经历类似的现象。如果一个人遭受脑损伤，在某些情况下，附近的神经元和大脑结构可以重新组织，帮助接管已死亡脑组织的一些功能。
- en: Or how if someone loses one of their senses, like vision, oftentimes their other
    senses become stronger to make up for their missing capability.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，像是如果一个人失去了某种感官，比如视觉，通常其他感官会变得更强，以弥补其丧失的能力。
- en: This is also known as the [Compensatory Masquerade](https://www.britannica.com/science/compensatory-masquerade).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为[补偿性伪装](https://www.britannica.com/science/compensatory-masquerade)。
- en: '![](../Images/38cb217c01889fc9be7aaf32bad46223.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38cb217c01889fc9be7aaf32bad46223.png)'
- en: A fully connected Neural Network to the left, and randomized dropout version
    on the right. In many cases, these networks may actually perform comparatively
    well (Image by author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧是一个全连接神经网络，右侧是随机丢弃版本。在许多情况下，这些网络实际上可能表现得相对较好（图片由作者提供）
- en: '**Ablation Testing**'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**切除测试**'
- en: While regularization is a technique used in neural networks and other A.I. architectures
    to aide in training a neural network better through artificial “[neuroplasticity](https://en.wikipedia.org/wiki/Neuroplasticity)”,
    sometimes we want to just do a similar procedure on a neural network to see how
    it will behave in the presence of deactivations in terms of accuracy.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然正则化是神经网络和其他人工智能架构中用来通过人工的“[神经可塑性](https://en.wikipedia.org/wiki/Neuroplasticity)”来帮助神经网络更好地训练的技术，但有时我们希望仅仅对神经网络做类似的处理，看看它在准确性方面，在去激活的情况下会有什么表现。
- en: 'We might do this for several other reasons:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会出于几个其他原因这么做：
- en: '**Identifying Critical Parts of a Neural Network:** Some parts of a neural
    network may do more important work than other parts of a neural network. In order
    to optimize the resource usage and the training time of the network, we can selectively
    ablate “weaker learners”'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别神经网络的关键部分：** 神经网络的某些部分可能比其他部分承担更为重要的工作。为了优化网络的资源使用和训练时间，我们可以选择性地去除“较弱的学习者”。'
- en: '**Reducing Complexity of the Neural Network:** Sometimes neural networks can
    get quite large, especially in the case of Deep MLPs ([multi layer perceptrons](https://en.wikipedia.org/wiki/Multilayer_perceptron)).
    This can make it difficult to map their behavior from input to output. By selectively
    shutting of parts of the network, we can potentially identify regions of excessive
    complexity and remove redundancy — simplifying our architecture.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少神经网络的复杂度：** 有时神经网络可能变得非常庞大，尤其是在深度多层感知器（[多层感知器](https://en.wikipedia.org/wiki/Multilayer_perceptron)）的情况下。这可能使得从输入到输出映射它们的行为变得困难。通过选择性地关闭网络的某些部分，我们可以潜在地识别出过度复杂的区域，并去除冗余——简化我们的架构。'
- en: '**Fault Tolerance:** In a realtime system, parts of a system can fail. The
    same applies for parts of a neural network, and thus the systems that depend on their output as we.
    We can turn to ablation studies to determine if destroying certain parts of the
    neural network, will cause the predictive or generative power of the system to
    suffer.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容错性：** 在实时系统中，系统的某些部分可能会失败。神经网络的部分也是如此，因此依赖其输出的系统也会受到影响。我们可以借助切除研究来确定，如果摧毁神经网络的某些部分，是否会导致系统的预测或生成能力受损。'
- en: '**Types of Ablation Tests**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**切除测试的类型**'
- en: 'There are actually many different kinds of ablation tests, and here we are
    going to talk about 3 specific kinds:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上有很多不同种类的切除测试，下面我们将讨论三种特定类型的测试：
- en: Neuronal Ablation
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元切除
- en: Functional Ablation
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能性切除
- en: Input Ablation
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入切除
- en: A quick note that ablation tests will have different effects depending on the
    network you are testing against and the data itself. An ablation test might demonstrate
    weakness in 1 part of the network for a specific data set, and may demonstrate
    weakness in another part of the neural network for a different ablation test.
    That is why that in a truly robust ablation testing system, you will need a wide
    variety of tests to get an accurate picture of the ANN’s ([Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network))
    weak points.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 快速说明，切除测试的效果会根据你测试的网络和数据本身而有所不同。一个切除测试可能会显示在特定数据集上网络某一部分的弱点，而在另一个切除测试中可能会显示神经网络的另一部分的弱点。这就是为什么在一个真正强健的切除测试系统中，你需要进行多种测试来准确地了解人工神经网络（[人工神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)）的弱点。
- en: '**Neuronal Ablation**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经元切除**'
- en: This is the first kind of ablation test we are going to run, and it’s the simplest
    to see the effects of and extend. We will simply remove varying percentages of
    neurons from our neural network
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要进行的第一种切除测试，它最简单且容易观察其效果并扩展。我们将简单地从神经网络中去除不同比例的神经元。
- en: For our experiment we have a simple ANN set up to test the accuracy of random
    character prediction agains using our old friend the [MNIST data set](https://en.wikipedia.org/wiki/MNIST_database).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们设置了一个简单的ANN来测试随机字符预测的准确性，使用我们老朋友的[MNIST数据集](https://en.wikipedia.org/wiki/MNIST_database)。
- en: '![](../Images/193196488e4f29b9aee031723542889b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/193196488e4f29b9aee031723542889b.png)'
- en: A snapshot of digit data from the MNIST data set (Image by author)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自MNIST数据集的数字数据快照（作者提供）
- en: Here is the code I wrote as a simple ANN test harness to test digit classification
    accuracy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我写的代码，作为一个简单的ANN测试工具，测试数字分类的准确性。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: So if we run the above code we see the following result of deactivating increasing
    percentages of our 128 node MLP.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我们运行上述代码，我们会看到如下结果：逐步停用我们128节点MLP的一定比例。
- en: The results are fairly interesting in this simple example, where as you can
    see dropping 80% of the neurons barely effects the accuracy, which means that
    removing excess neurons is certainly an optimization we could consider in building
    this network.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的示例中，结果相当有趣，正如你所看到的，丢弃80%的神经元几乎不影响准确性，这意味着去除多余的神经元肯定是我们在构建这个网络时可以考虑的一种优化方法。
- en: '![](../Images/8804627f95e4e423a2bdd894aef0979b.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8804627f95e4e423a2bdd894aef0979b.png)'
- en: Graph generated for dropout ablation test (Image by author)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为dropout切除测试生成的图表（作者提供）
- en: '**Functional Ablation**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**功能性切除**'
- en: For functional ablation, we change the activation functions of the neurons to
    different curves, with different amounts of non-linearity. The last function we
    use is a straight line, completely destroying the non-linear characteristic of
    the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于功能性切除，我们将神经元的激活函数更改为不同的曲线，具有不同程度的非线性。我们使用的最后一个函数是一条直线，完全破坏了模型的非线性特征。
- en: 'Because non-linear models are by definition more complex than linear models,
    and the purpose of activation functions is to induce nonlinear effects on the
    classification, a line of reasoning one could make is that:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因为非线性模型在定义上比线性模型更复杂，而激活函数的目的是在分类上引入非线性效应，所以可以得出这样的推理：
- en: '*“If we can get away with using linear functions instead of non-linear functions,
    and still have a good classification, then maybe we can simply our architecture
    and lower its cost”*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*“如果我们可以通过使用线性函数代替非线性函数，并且仍然能得到很好的分类结果，那么或许我们可以简化我们的架构，降低其成本”*'
- en: '**Note:** You’ll notice in addition to regularization, certain kinds of ablation
    testing, like functional ablation has similarities to [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization).
    They are similar, but ablation testing refers more to changing parts of the neural
    network architecture (e.g. neurons, layers, etc), where as hyperparameter tuning
    refers to changing structural parameters of the model. Both have the goal of optimization.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 除了正则化之外，你会注意到，某些类型的切除测试，如功能性切除，与[超参数调优](https://en.wikipedia.org/wiki/Hyperparameter_optimization)有相似之处。它们是相似的，但切除测试更多是指改变神经网络架构的部分（例如神经元、层等），而超参数调优则是指改变模型的结构参数。两者的目标都是优化。'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When we run the above code we get the following accuracies vs activation function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行上述代码时，我们得到以下激活函数与准确度的比较。
- en: '![](../Images/a428a18957cb11484ff2823a55c3a840.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a428a18957cb11484ff2823a55c3a840.png)'
- en: Graph generated for functional ablation test (Image by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为功能性切除测试生成的图表（作者提供）
- en: So it indeed it looks like non-linearity of some kind is important to the classification,
    with “[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))” and [hyperbolic
    tangent](https://en.wikipedia.org/wiki/Hyperbolic_functions) non-linearity being
    the most effective. This makes sense, because it’s well known that digit classification
    is best framed as a non-linear task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，确实看起来某种形式的非线性对分类非常重要，其中“[ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))”和[双曲正切](https://en.wikipedia.org/wiki/Hyperbolic_functions)的非线性效果最为有效。这是有道理的，因为众所周知，数字分类最好作为一个非线性任务来处理。
- en: '**Feature Ablation**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征切除**'
- en: We can also remove features from the classification and see how that effects
    the accuracy of our predictor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从分类中移除特征，看看这对预测器的准确性有何影响。
- en: Normally *prior* to doing a machine learning or data science project, we typically
    do exploratory data analysis (EDA) and feature selection to determine what features
    could be important to our classification problem.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在做机器学习或数据科学项目之前，我们通常会进行探索性数据分析（EDA）和特征选择，以确定哪些特征可能对我们的分类问题很重要。
- en: But sometimes interesting effects can be observed, especially with the ever
    mysterious neural networks, by removing features as part of an ablation study
    and seeing the effect on classification. Using the following code, we can remove
    columns of pixels from our letters in groups of 4 columns.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是有时我们可以观察到有趣的效果，特别是在处理神秘的神经网络时，通过在消融研究中移除特征并观察其对分类的影响。使用以下代码，我们可以按4列一组地从字母中移除像素列。
- en: Obviously, there are several ways to ablate the features, by distorting the
    characters in different ways besides in columns. But we can start with this simple
    example and observe the effects.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，消融特征有多种方式，除了按列扭曲字符之外，还可以通过其他方式。但我们可以从这个简单的例子开始并观察其效果。
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After we run the above feature ablation code, we see:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述特征消融代码后，我们看到：
- en: '![](../Images/07a74f18d9fe0131f2c7bfc8f29ebea7.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a74f18d9fe0131f2c7bfc8f29ebea7.png)'
- en: Graph generated for 4-column input feature ablation test (Image by author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为4列输入特征消融测试生成的图表（图片来自作者）
- en: Interestingly, there’s a slight dip in accuracy when we remove columns 8 to
    to 12, and a rise again after that. That suggests that on average, the more “sensitive”
    character geometry lies in those center columns, but the other columns especially
    close to the beginning and end could potentially be removed for an optimization
    effect.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当我们移除第8到第12列时，准确度略有下降，然后再次上升。这表明平均而言，更“敏感”的字符几何形状位于这些中心列中，但其他列，特别是接近开始和结束的列，可能会被移除以优化效果。
- en: Here’s the same test against removing 7 columns at a time, along with the columns.
    Visualizing the actual distorted character data allows us to make a lot more sense
    of the result, as we see that the reason that removing the first few columns makes
    a smaller different is because they are mostly just padding!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对每次移除7列的相同测试，以及相应的列。通过可视化实际的扭曲字符数据，我们可以更好地理解结果，因为我们看到移除前几列对结果的影响较小，这是因为它们大多只是填充！
- en: '![](../Images/5fe4ea84cf30abc94770488eff6c5111.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fe4ea84cf30abc94770488eff6c5111.png)'
- en: Graph generated for result of 4 column pixel removal (Image by author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为4列像素移除结果生成的图表（图片来自作者）
- en: Another interesting example of an ablation study would be testing against different
    sorts of noise profiles. Here below is code I wrote to progressively noise an
    image using the above ANN model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的消融研究例子是测试不同类型的噪声配置文件。下面是我写的代码，用于使用上述ANN模型渐进地为图像加噪声。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve created an ablation study for the robustness of the network in the presence
    of an increasing strength [Gaussian Noise](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.).
    Notice the expected and marked decreasing prediction accuracy as the noise level
    increases.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为网络在增加强度的[高斯噪声](https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed.)环境中的稳健性创建了一个消融研究。请注意，随着噪声水平的增加，预测准确度预期并显著下降。
- en: '![](../Images/0f3ccd17fd6e9b8bc4ad14b5df6b461d.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f3ccd17fd6e9b8bc4ad14b5df6b461d.png)'
- en: Graph generated for result of progressive noising (Image by author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为渐进噪声结果生成的图表（图片来自作者）
- en: Situations like this let us know that we may have to increase the power and
    complexity of our neural network to compensate. Also remember that ablation studies
    can be done in combination which each other, in the presence of different types
    of noise combined with different types of distortion.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样的情况让我们知道，可能需要增加神经网络的能力和复杂性来进行补偿。还要记住，消融研究可以相互结合进行，在不同类型的噪声与不同类型的失真结合的情况下进行。
- en: '**Conclusions**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Ablation studies can be very important to optimizing and testing a neural network.
    We demonstrated a small example here in this post, but there are an innumerable
    number of ways to run these studies on different and more complex network architectures.
    If you have any thoughts, would love some feedback and also perhaps even put them
    in your own article. Thank you for reading!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究对优化和测试神经网络非常重要。我们在这篇文章中演示了一个小例子，但有无数种方法可以在不同和更复杂的网络架构上运行这些研究。如果你有任何想法，非常希望得到反馈，甚至可以将其纳入你的文章中。感谢阅读！
