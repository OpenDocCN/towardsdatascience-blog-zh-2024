["```py\nimport sys\nimport os\nimport torch\nimport intel_extension_for_pytorch as ipex\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# PART 1: Model and tokenizer loading using transformers\ntokenizer = AutoTokenizer.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\n\n# PART 2: Use IPEX to optimize the model\n#dtype = torch.float # use for full precision FP32\ndtype = torch.bfloat16 # use for half precision inference\nmodel = ipex.llm.optimize(model, dtype=dtype)\n\n# PART 3: Create a hugging face inference pipeline and generate results\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nst = time.time()\nresults = pipe(\"A fisherman at sea...\",  max_length=250)\nend = time.time()\ngeneration_latency = end-st\n\nprint('generation latency: ', generation_latency)\nprint(results[0]['generated_text'])\n```", "```py\nimport torch\nimport intel_extension_for_pytorch as ipex\nfrom intel_extension_for_pytorch.quantization import prepare\nimport transformers\n\n# PART 1: Load model and tokenizer from Hugging Face + Load SmoothQuant config mapping\ntokenizer = AutoTokenizer.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\nqconfig = ipex.quantization.get_smooth_quant_qconfig_mapping()\n\n# PART 2: Configure calibration\n# prepare your calibration dataset samples\ncalib_dataset = DataLoader({Your dataloader parameters})\nexample_inputs = # provide a sample input from your calib_dataset\ncalibration_model = ipex.llm.optimize(\n  model.eval(),\n  quantization_config=qconfig,\n)\nprepared_model = prepare(\n  calibration_model.eval(), qconfig, example_inputs=example_inputs\n)\nwith torch.no_grad():\n  for calib_samples in enumerate(calib_dataset):\n    prepared_model(calib_samples)\nprepared_model.save_qconf_summary(qconf_summary=qconfig_summary_file_path)\n\n# PART 3: Model Quantization using SmoothQuant\nmodel = ipex.llm.optimize(\n  model.eval(),\n  quantization_config=qconfig,\n  qconfig_summary_file=qconfig_summary_file_path,\n)\n\n# generation inference loop\nwith torch.inference_mode():\n    model.generate({your generate parameters})\n```", "```py\n # requirements\n#intel-extension-for-pytorch==2.2\n#transformers==4.35.2\n#torch==2.2.0 \n\nimport torch\nimport intel_extension_for_pytorch as ipex\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# PART 1: Model and tokenizer loading\ntokenizer = AutoTokenizer.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\nmodel = AutoModelForCausalLM.from_pretrained(\"Intel/neural-chat-7b-v3-3\")\n\n# PART 2: Preparation of quantization config\nqconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(\n  weight_dtype=torch.qint8, # or torch.quint4x2\n  lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8\n)\ncheckpoint = None # optionally load int4 or int8 checkpoint\n\n# PART 3: Model optimization and quantization\nmodel = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint)\n\ninputs = tokenizer(\"I love learning to code...\", return_tensors=\"pt\").input_ids\n\n# PART 4: Inference output generation\nwith torch.inference_mode():\n    tokens = model.generate(\n        inputs,\n        max_new_tokens=64,\n)\n\nprint(tokenizer.decode(tokens[0], skip_special_tokens=True))\n```"]