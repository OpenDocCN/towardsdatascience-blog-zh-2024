<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Do not over-think about ‘outliers’, use a student-t distribution instead</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Do not over-think about ‘outliers’, use a student-t distribution instead</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-not-over-think-about-outliers-use-a-student-t-distribution-instead-b6c584b91d5c?source=collection_archive---------0-----------------------#2024-03-30">https://towardsdatascience.com/do-not-over-think-about-outliers-use-a-student-t-distribution-instead-b6c584b91d5c?source=collection_archive---------0-----------------------#2024-03-30</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="0f73" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">A Bayesian approach using R and Brms</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@damanrique?source=post_page---byline--b6c584b91d5c--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Daniel Manrique-Castano" class="l ep by dd de cx" src="../Images/06f857ae6e82688113f1089c7f03be88.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*9lYBpTGF3RXmvDod"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b6c584b91d5c--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@damanrique?source=post_page---byline--b6c584b91d5c--------------------------------" rel="noopener follow">Daniel Manrique-Castano</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b6c584b91d5c--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn lg lh ab q ee li lj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count le lf">7</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ih lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="4136" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk nj"><span class="l nk nl nm bo nn no np nq nr ed">F</span>or many researchers, outliers are rogue waves that can dramatically alter the course of the analysis or “confound” some expected effects. I prefer to use the term “extreme observations” and leave the term outlier for observations that are not truly part of the population being studied. For example, in my field (brain ischemia research), an outlier is an animal that does not have ischemia (when it should have), while extreme observations are animals with small or large ischemias that are very different from the others.</p><p id="c9d7" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Traditional (frequentist) statistical models are built on the strong foundation of Gaussian distributions. This has a significant limitation: an inherent assumption that all data points will cluster around a central mean in a predictable pattern (based on the central limit theorem). This may be true in Plato’s world of ideas, but we, scientists in the biomedical field, are aware it's challenging to rely on this assumption given the limited sampling (number of animals) we have available to make observations.</p><p id="fa0f" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Gaussian distributions are very sensitive to extreme observations, and their use leads scientists to believe that eliminating extreme observations is the best way to get “clearer” or “cleaner” results (whatever that means). As I once commented in an article as reviewer 2, “The problem is not the extreme observations that may “hide” your effects, but the fact that you are using a statistical model that (I believe) is inappropriate for your purposes”.</p><blockquote class="ns nt nu"><p id="a926" class="mn mo nv mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">It should be noted that no statistical model is the “right” or “appropriate” one, but we can estimate that, given the data, there are certain statistical models that are more likely to generate the observed data (generative models) than others.</p></blockquote><p id="d063" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Fortunately, nothing forces us to be bound by the assumptions of the Gaussian models, right? We have other options, such as the <strong class="mp fs">Student’s t-distribution</strong> (<a class="af nw" href="#ref-ahsanullah2014" rel="noopener ugc nofollow"><em class="nv">1</em></a>). I see it as a more adaptable vessel to navigate the turbulent seas of real-world biomedical data. The Student’s t-distribution provides a robust alternative to acknowledge that our data may be populated by extreme observations that are normal biological responses that we can expect in any context. There may be patients or animals that don’t respond or overreact to treatment, and it is valuable that our modeling approach recognizes these responses as part of the spectrum. Therefore, this tutorial explores the modeling strategies using Student’s t-distributions through the lens of the <code class="cx nx ny nz oa b"><strong class="mp fs">brms</strong></code> package for R (<a class="af nw" href="#ref-brms" rel="noopener ugc nofollow"><em class="nv">2</em></a>)—a powerful ally for Bayesian modeling</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="6e1e" class="oj ok fr bf ol om on gr oo op oq gu or os ot ou ov ow ox oy oz pa pb pc pd pe bk">What’s behind a student’s t-distribution?</h1><p id="1e49" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">A <a class="af nw" href="https://mathworld.wolfram.com/Studentst-Distribution.html" rel="noopener ugc nofollow" target="_blank">Student’s t-distribution</a> is nothing more than a Gaussian distribution with heavier tails. In other words, we can say that the Gaussian distribution is a special case of the Student’s t-distribution. The Gaussian distribution is defined by the mean (μ) and the standard deviation (σ). The Student t distribution, on the other hand, adds an additional parameter, the degrees of freedom (df), which controls the “thickness” of the distribution. This parameter assigns greater probability to events further from the mean. This feature is particularly useful for small sample sizes, such as in biomedicine, where the assumption of normality is questionable. Note that as the degrees of freedom increase, the Student t-distribution approaches the Gaussian distribution. We can visualize this using density plots:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="87d4" class="ps ok fr oa b bg pt pu l pv pw"># Load necessary libraries<br/>library(ggplot2)<br/><br/># Set seed for reproducibility<br/>set.seed(123)<br/><br/># Define the distributions<br/>x &lt;- seq(-4, 4, length.out = 200)<br/>y_gaussian &lt;- dnorm(x)<br/>y_t3 &lt;- dt(x, df = 3)<br/>y_t10 &lt;- dt(x, df = 10)<br/>y_t30 &lt;- dt(x, df = 30)<br/><br/># Create a data frame for plotting<br/>df &lt;- data.frame(x, y_gaussian, y_t3, y_t10, y_t30)<br/><br/># Plot the distributions<br/>ggplot(df, aes(x)) +<br/>  geom_line(aes(y = y_gaussian, color = "Gaussian")) +<br/>  geom_line(aes(y = y_t3, color = "t, df=3")) +<br/>  geom_line(aes(y = y_t10, color = "t, df=10")) +<br/>  geom_line(aes(y = y_t30, color = "t, df=30")) +<br/>  labs(title = "Comparison of Gaussian and Student t-Distributions",<br/>       x = "Value",<br/>       y = "Density") +<br/>  scale_color_manual(values = c("Gaussian" = "blue", "t, df=3" = "red", "t, df=10" = "green", "t, df=30" = "purple")) +<br/>  theme_classic()</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div role="button" tabindex="0" class="qb qc ed qd bh qe"><div class="px py pz"><img src="../Images/b033f2cc8f3e1543b3b8427615f45731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpYqOYJoqafQ-NOIBUZPhA.png"/></div></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 1: Comparison of Gaussian and Student t-Distributions with different degrees of freedom.</figcaption></figure><p id="12bc" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Note in <a class="af nw" href="#fig-Fig1" rel="noopener ugc nofollow">Figure 1</a> that the hill around the mean gets smaller as the degrees of freedom decrease as a result of the probability mass going to the tails, which are thicker. This property is what gives the Student’s t-distribution a reduced sensitivity to outliers. For more details on this matter, you can check <a class="af nw" href="https://online.stat.psu.edu/stat414/lesson/26/26.4" rel="noopener ugc nofollow" target="_blank">this</a> blog.</p><h1 id="edc2" class="oj ok fr bf ol om ql gr oo op qm gu or os qn ou ov ow qo oy oz pa qp pc pd pe bk">Load the required packages</h1><p id="9e19" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">We load the required libraries:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="b885" class="ps ok fr oa b bg pt pu l pv pw">library(ggplot2)<br/>library(brms)<br/>library(ggdist)<br/>library(easystats)<br/>library(dplyr)<br/>library(tibble)<br/>library(ghibli)</span></pre><h1 id="466b" class="oj ok fr bf ol om ql gr oo op qm gu or os qn ou ov ow qo oy oz pa qp pc pd pe bk">Exploratory data visualization</h1><p id="0757" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">So, let’s skip data simulations and get serious. We’ll work with real data I have acquired from mice performing the rotarod test.</p><p id="781a" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">First, we load the dataset into our environment and set the corresponding factor levels. The dataset contains IDs for the animals, a groping variable (Genotype), an indicator for two different days on which the test was performed (day), and different trials for the same day. For this article, we model only one of the trials (Trial3). We will save the other trials for a future article on modeling variation.</p><p id="c123" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">As the data handling implies, our modeling strategy will be based on Genotype and Day as categorical predictors of the distribution of <code class="cx nx ny nz oa b">Trial3</code>.</p><blockquote class="ns nt nu"><p id="f93d" class="mn mo nv mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">In biomedical science, categorical predictors, or grouping factors, are more common than continuous predictors. Scientists in this field like to divide their samples into groups or conditions and apply different treatments.</p></blockquote><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="bf80" class="ps ok fr oa b bg pt pu l pv pw">data &lt;- read.csv("Data/Rotarod.csv")<br/>data$Day &lt;- factor(data$Day, levels = c("1", "2"))<br/>data$Genotype &lt;- factor(data$Genotype, levels = c("WT", "KO"))<br/>head(data)</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py qq"><img src="../Images/33dfae9980cce2de5d4a4692da86d549.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*rhl-Z5tRDKYWKvJgmP4QvQ.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Data frame</figcaption></figure><p id="b8b9" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Let’s have an initial view of the data using <strong class="mp fs">Raincloud plots</strong> as shown by <span class="ie"><span class="ie" aria-hidden="false"><a class="qr if qs" href="https://medium.com/u/3cca5ff4ed5d?source=post_page---user_mention--b6c584b91d5c--------------------------------" rel="noopener" target="_blank">Guilherme A. Franchi, PhD</a></span></span> in <a class="af nw" href="https://medium.com/@amorimfranchi/raincloud-plots-for-clear-precise-and-efficient-data-communication-4c71d0a37c23#:~:text=Raincloud%20plots%20for%20clear%2C%20precise%20and%20efficient%20data%20communication,-Guilherme%20A.&amp;text=Raw%20data%20visualization%20involves%20presenting,quality%20assessment%20of%20your%20data." rel="noopener">this</a> great blog post.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="d677" class="ps ok fr oa b bg pt pu l pv pw">edv &lt;- ggplot(data, aes(x = Day, y = Trial3, fill=Genotype)) +<br/>  scale_fill_ghibli_d("SpiritedMedium", direction = -1) +<br/>  geom_boxplot(width = 0.1,<br/>               outlier.color = "red") +<br/>  xlab('Day') +<br/>  ylab('Time (s)') +<br/>  ggtitle("Rorarod performance") +<br/>  theme_classic(base_size=18, base_family="serif")+<br/>  theme(text = element_text(size=18),<br/>        axis.text.x = element_text(angle=0, hjust=.1, vjust = 0.5, color = "black"),<br/>        axis.text.y = element_text(color = "black"),<br/>        plot.title = element_text(hjust = 0.5),<br/>        plot.subtitle = element_text(hjust = 0.5),<br/>        legend.position="bottom")+<br/>  scale_y_continuous(breaks = seq(0, 100, by=20), <br/>                     limits=c(0,100)) +<br/># Line below adds dot plots from {ggdist} package <br/>  stat_dots(side = "left", <br/>            justification = 1.12,<br/>            binwidth = 1.9) +<br/># Line below adds half-violin from {ggdist} package<br/>  stat_halfeye(adjust = .5, <br/>               width = .6, <br/>               justification = -.2, <br/>               .width = 0, <br/>               point_colour = NA)<br/>edv</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py qt"><img src="../Images/de2b062a13472328ee63bc1febd3e0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*1ZU56ZN4nOFslrj5xgkatA.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 2: Exploratory data visualization.</figcaption></figure><p id="6d01" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk"><a class="af nw" href="#fig-Fig2" rel="noopener ugc nofollow">Figure 2</a> looks different from the original by <span class="ie"><span class="ie" aria-hidden="false"><a class="qr if qs" href="https://medium.com/u/3cca5ff4ed5d?source=post_page---user_mention--b6c584b91d5c--------------------------------" rel="noopener" target="_blank">Guilherme A. Franchi, PhD</a></span></span> because we are plotting two factors instead of one. However, the nature of the plot is the same. Pay attention to the red dots, these are the ones that can be considered extreme observations that tilt the measures of central tendency (especially the mean) toward one direction. We also observe that the variances are different, so modeling also sigma can give better estimates. Our task now is to model the output using the <code class="cx nx ny nz oa b">brms</code> package.</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="7a02" class="oj ok fr bf ol om on gr oo op oq gu or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Fitting statistical models with brms</h1><p id="257d" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Here we fit our model with <code class="cx nx ny nz oa b">Day</code> and <code class="cx nx ny nz oa b">Genotype</code> as interacting categorical predictors for the distribution of <code class="cx nx ny nz oa b">Trial 3</code>. Let’s first fit a typical Gaussian model, which is analogous to an ordinary least squares (OLS) model from the frequentist framework, since we are using the default flat <code class="cx nx ny nz oa b">brms</code> <a class="af nw" href="https://paul-buerkner.github.io/brms/reference/set_prior.html" rel="noopener ugc nofollow" target="_blank">priors</a>. Priors are beyond the scope of this article, but I promise we’ll cover them in a future blog.</p><p id="f011" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Once we have results from the Gaussian model, we can compare them to the large results from the Student’s t model. We then add<code class="cx nx ny nz oa b">sigma</code> to the equation to account for the difference in the variance of the data.</p><h2 id="1810" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Fitting a “typical” (frequentists) model in Gaussian land</h2><p id="9048" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Our Gaussian model is built under the typical (and often incorrect) assumption of homoscedasticity (<a class="af nw" href="#ref-yang2019" rel="noopener ugc nofollow"><em class="nv">3</em></a>). In other words, we assume that all groups have the same (or very similar) variance. I do not recall seeing this as a researcher.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="bcc5" class="ps ok fr oa b bg pt pu l pv pw">Gaussian_Fit1 &lt;- brm(Trial3 ~ Day * Genotype, <br/>           data = data, <br/>           family = gaussian(),<br/>           # seed for reproducibility purposes<br/>           seed = 8807,<br/>           control = list(adapt_delta = 0.99),<br/>           # this is to save the model in my laptop<br/>           file    = "Models/20240222_OutliersStudent-t/Gaussian_Fit1.rds",<br/>           file_refit = "never")<br/><br/># Add loo for model comparison<br/>Gaussian_Fit1 &lt;- <br/>  add_criterion(Gaussian_Fit1, c("loo", "waic", "bayes_R2"))</span></pre><h2 id="db71" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Model diagnostics</h2><p id="15cb" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Before proceeding, it’s a good idea to do some simple model diagnostics to compare the actual observations with the predictions made by our model. We can do this in several ways, but the most common is to plot full densities. We can achieve this using the <code class="cx nx ny nz oa b">pp_check</code> function from <code class="cx nx ny nz oa b">brms</code>.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="33a3" class="ps ok fr oa b bg pt pu l pv pw">set.seed(8807)<br/><br/>pp_check(Gaussian_Fit1, ndraws = 100) +<br/>  labs(title = "Gaussian model") +<br/>  theme_classic()</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rl"><img src="../Images/10b7786f4b7b43cd0f9e8f5563282a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*SADintb1Dwz3mk78lB4RWw.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 3: Diagnostics for the Gaussian model</figcaption></figure><p id="96ce" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk"><a class="af nw" href="#fig-GaussianDiag1" rel="noopener ugc nofollow">Figure 3</a> suggests that our observations (dark blue) are not meaningfully different from the model predictions. Below, I leave you with additional code to check other <code class="cx nx ny nz oa b">pp_check</code> alternatives with their respective graphs.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="1cfd" class="ps ok fr oa b bg pt pu l pv pw">set.seed(88071)<br/><br/>pp_check(Gaussian_Fit1, group = "Genotype", type = "dens_overlay_grouped", ndraws = 100) +<br/>  labs(title = "Density by Genotype") +<br/>    theme_classic()<br/><br/>pp_check(Gaussian_Fit1, type = "stat_grouped", group = "Genotype", stat = "var", binwidth = 3) +<br/>  coord_cartesian(xlim = c(0, 300)) +<br/>  ggtitle("Grouped variance") +<br/>  theme_classic()<br/><br/>pp_check(Gaussian_Fit1, type = "stat", stat = "var", binwidth = 3) +<br/>  coord_cartesian(xlim = c(0, 600)) +<br/>  ggtitle("How well we captured the variace") +<br/>  theme_classic()<br/><br/>pp_check(Gaussian_Fit1, type = "stat", stat = "mean", binwidth = 2) +<br/>  coord_cartesian(xlim = c(0, 50)) +<br/>  ggtitle("How well we captured the mean") +<br/>  theme_classic()</span></pre><h2 id="0700" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Checking the results for the Gaussian distribution</h2><p id="775c" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Now, we use the <code class="cx nx ny nz oa b">describe_posterior</code> function from the <code class="cx nx ny nz oa b">bayestestR</code> package (<a class="af nw" href="#ref-bayestestR" rel="noopener ugc nofollow"><em class="nv">4</em></a>) to see the results:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="093e" class="ps ok fr oa b bg pt pu l pv pw">describe_posterior(Gaussian_Fit1,<br/>                   centrality = "mean",<br/>                   dispersion = TRUE,<br/>                   ci_method = "HDI",<br/>                   test = "rope",<br/>                   )</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div role="button" tabindex="0" class="qb qc ed qd bh qe"><div class="px py rm"><img src="../Images/375718185cf407d1addd0197e82d3368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_EZZw8PFdXO5xKGQIf__g.png"/></div></div></figure><p id="2853" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Let’s focus here on the ‘intercept’, which is the value for WT at 1 DPI, and ‘GenotypeKO’, the estimated difference for KO animals at the same time point. We see that WT animals spend about 37 seconds in the rotarod, while their KO counterparts spend less than a second (0.54) more. As a researcher in this field, I can say that this difference is meaningless and that genotype has no effect on rotarod performance. Even the effect of day, which is 2.9, seems meaningless to me under this model. We can easily visualize these estimates using the wonderful <code class="cx nx ny nz oa b">conditional_effects</code> function from brms.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="2ccf" class="ps ok fr oa b bg pt pu l pv pw"># We create the graph for convex hull<br/>Gaussian_CondEffects &lt;- <br/>  conditional_effects(Gaussian_Fit1)<br/><br/>Gaussian_CondEffects &lt;- plot(Gaussian_CondEffects, <br/>       plot = FALSE)[[3]]<br/><br/>Gaussian_CondEffects + <br/>  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +<br/>  Plot_theme +<br/>  theme(legend.position = "bottom", legend.direction = "horizontal")</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rn"><img src="../Images/e0f0c649cc2a815a598a96279b882d49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*BSYJfFPm9wwKitUh7P8sEg.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 8: Conditional effects for the Gaussian model</figcaption></figure><p id="1af0" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">In <a class="af nw" href="#fig-GaussianEff" rel="noopener ugc nofollow">Figure 8</a> we can see the estimates and uncertainty for the interaction terms. I have customized the plot with a number of ggplot elements, which you can check in the original <a class="af nw" href="https://github.com/daniel-manrique/MediumBlog/blob/main/20240222_OutliersStudent-t.qmd" rel="noopener ugc nofollow" target="_blank">Quarto Notebook</a>. Note the similar uncertainty for both time points, even though the dispersion is larger on day 1 than on day 2. We will address this point in a small snippet at the end of this article.</p><p id="31cb" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Now let’s see how much our understanding changes when we model the same data using a student-t distribution.</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="748f" class="oj ok fr bf ol om on gr oo op oq gu or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Fitting our guest: a model with a student-t distribution</h1><p id="f566" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">It’s time to use the student-t distribution in our `brms` model.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="6b87" class="ps ok fr oa b bg pt pu l pv pw">Student_Fit &lt;- brm(Trial3 ~ Day * Genotype, <br/>           data = data, <br/>           family = student,<br/>           # seed for reproducibility purposes<br/>           seed = 8807,<br/>           control = list(adapt_delta = 0.99),<br/>           # this is to save the model in my laptop<br/>           file    = "Models/20240222_OutliersStudent-t/Student_Fit.rds",<br/>           file_refit = "never")<br/><br/># Add loo for model comparison<br/>Student_Fit &lt;- <br/>  add_criterion(Student_Fit, c("loo", "waic", "bayes_R2"))</span></pre><h2 id="5ebc" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Model diagnostics</h2><p id="9204" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">We plot the model diagnostics as done before:</p><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rl"><img src="../Images/3dcaf9d4af3919790fa3ee9d281e48b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*EhPTg9H_eMsEmuLl7pxB-g.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 9: Model diagnostics for student-t distribution</figcaption></figure><p id="4e66" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk"><a class="af nw" href="#fig-StudentDiag1" rel="noopener ugc nofollow">Figure 9</a> shows that the mean shape and the peak of the observations and the predictions match. It’s important to note that our model seems to predict values below 0. This is an important research issue that we will skip for now. However, it does imply the use of informative priors or distribution families that set a lower bound at 0, such as the <code class="cx nx ny nz oa b">log_normal',</code>hurdle_lognormal’, or `zero_inflated_poisson’, depending on the case. Andrew Heiss (<a class="af nw" href="#ref-heiss2021" rel="noopener ugc nofollow"><em class="nv">5</em></a>) offers a <a class="af nw" href="https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/" rel="noopener ugc nofollow" target="_blank">great example</a> in this regard.</p><h2 id="2714" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Checking the results for the student-t distribution</h2><p id="14b6" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Let’s take a look at the posterior distribution:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="7494" class="ps ok fr oa b bg pt pu l pv pw">describe_posterior(Student_Fit,<br/>                   centrality = "mean",<br/>                   dispersion = TRUE,<br/>                   ci_method = "HDI",<br/>                   test = "rope",<br/>                   )</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div role="button" tabindex="0" class="qb qc ed qd bh qe"><div class="px py ro"><img src="../Images/8ab1ec417192b14fbabdeae270a75572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xSg6CXBe_AzQbwetMad6DA.png"/></div></div></figure><p id="d0bc" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Under this model, we can see that our estimates have changed moderately, I would say. Our estimate for the intercept (WT at 1 day) is reduced by 7 seconds. And why is that? Because the extreme values we discovered at the beginning have less influence on the measures of central tendency of the data. Thus, this is a more accurate measure of the “typical” WT animal on day 1. We also observe a substantial increase in the effect of day, with almost 10 seconds more than our initial Gaussian estimates. Importantly, the effect of our KO genotype appears to be more notorious, increasing about 10 times from 0.52 in our Gaussian model to 5.5 in our student-t model. From my perspective, given the context of these data, the contrasts between the two models are notorious.</p><p id="2cc7" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Let’s see it in graphical terms using <code class="cx nx ny nz oa b">conditional_effects</code>:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="0ed3" class="ps ok fr oa b bg pt pu l pv pw">Student_CondEffects &lt;- <br/>  conditional_effects(Student_Fit)<br/><br/>Student_CondEffects &lt;- plot(Student_CondEffects, <br/>       plot = FALSE)[[3]]<br/><br/>Student_CondEffects + <br/>  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +<br/>  Plot_theme +<br/>  theme(legend.position = "bottom", legend.direction = "horizontal")</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rn"><img src="../Images/ad19138d3dffa2365ad138d3641aed65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XcR1PJyh81WeHnus9Jdelw.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 10: Conditional effects for the Student-t model</figcaption></figure><p id="5c42" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Can we get better estimates? For this particular example, I think we can. From the start, it was easy to notice the difference in the variance of the data, especially when we compare the first and second-day visuals. We improved our estimates using the student-t distribution, and we can improve them further by developing a model for heteroscedasticity that predicts sigma (the residual variance).</p><blockquote class="ns nt nu"><p id="5156" class="mn mo nv mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">In this way, the model does not assume that your residual variance is equal across your grouping variables, but it becomes a response that can be modeled by predictors.</p></blockquote><p id="8788" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">This is the little point we left for the end.</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="fd16" class="oj ok fr bf ol om on gr oo op oq gu or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Predicting sigma using a student-t distribution</h1><p id="fc02" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">We include sigma as a response variable using the<code class="cx nx ny nz oa b">bf</code> function from <code class="cx nx ny nz oa b">brms</code>. In this case, we are going to model this parameter using the same predictors <code class="cx nx ny nz oa b">Day</code> and <code class="cx nx ny nz oa b">Genotype</code>.</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="2cf4" class="ps ok fr oa b bg pt pu l pv pw">Student_Mdl2 &lt;- bf (Trial3 ~ Day * Genotype,<br/>                     sigma ~ Day * Genotype)<br/><br/>Student_Fit2 &lt;- brm(<br/>           formula = Student_Mdl2,<br/>           data = data, <br/>           family = student,<br/>           # seed for reproducibility purposes<br/>           seed = 8807,<br/>           control = list(adapt_delta = 0.99),<br/>           # this is to save the model in my laptop<br/>           file    = "Models/20240222_OutliersStudent-t/Student_Fit2.rds",<br/>           file_refit = "never")<br/><br/># Add loo for model comparison<br/>Student_Fit2 &lt;- <br/>  add_criterion(Student_Fit2, c("loo", "waic", "bayes_R2"))</span></pre><h2 id="9a2c" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Model diagnostics</h2><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rl"><img src="../Images/51c1140d256a0e6f98db9202a94037f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Sbl0oj0gMlwhejzEqP-AjA.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 11: Model diagnostics for student-t distribution with sigma</figcaption></figure><p id="b52e" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk"><a class="af nw" href="#fig-StudentDiag2" rel="noopener ugc nofollow">Figure 11</a> looks good, except for the uncomfortable predictions below 0. For this case, I judge that this does not strongly bias the estimates and their uncertainty. However, this is an aspect I will take into account when doing actual research.</p><h2 id="f434" class="qu ok fr bf ol qv qw qx oo qy qz ra or mw rb rc rd na re rf rg ne rh ri rj rk bk">Checking the results for the student-t distribution with predicted sigma</h2><p id="f87c" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">Now, let’s take a look at the posterior distribution.</p><figure class="pk pl pm pn po qa px py paragraph-image"><div role="button" tabindex="0" class="qb qc ed qd bh qe"><div class="px py rp"><img src="../Images/5d6d22fa261f290155a4af992fc14c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-jrBVBDA_BhiEeYn3jPLAw.png"/></div></div></figure><p id="f945" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">We see more parameters compared to the other two fitted models because the response for sigma is now included as a main effect in the model. Under this scheme, we see that the intercepts are closer to those of the Gaussian model and the effect of genotype (<code class="cx nx ny nz oa b">GenotypeKO</code>) is reduced by half.</p><p id="9770" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">There is one aspect to note, however. In our first Student-t model, the uncertainty for the intercept was 24.1–37.4. On the other hand, in the last model, the uncertainty increases to 24.3–46.1. This means that when we consider the different variances, we are less certain of this (and other) parameters. The same is true for day, for example, which changes from 1.2–18.9 to -5.6–18.1. In this case, we are now less certain that the second day is associated with an increase in time spent on the rotarod.</p><blockquote class="ns nt nu"><p id="a9ef" class="mn mo nv mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Don’t worry, the purpose of statistical modeling is to provide the best possible quantification of the uncertainty in a measurement, and that’s what we’re doing right now. Of course, our uncertainty increases when we have extreme values that are part of our sample and therefore part of our population.</p></blockquote><p id="8316" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">In this example, we see that accounting for the different variances in our data gives us a very different idea of our results.</p><p id="97bd" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Finally, we can see that sigma, plotted on the log scale, varies meaningfully with day and genotype:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="e2c4" class="ps ok fr oa b bg pt pu l pv pw">Student_CondEffects2 &lt;- <br/>  conditional_effects(Student_Fit2)<br/><br/>Student_CondEffects2 &lt;- plot(Student_CondEffects2, <br/>       plot = FALSE)[[3]]<br/><br/>Student_CondEffects2 + <br/>  geom_point(data=data, aes(x = Day, y = Trial3, color = Genotype), inherit.aes=FALSE) +<br/>  Plot_theme +<br/>  theme(legend.position = "bottom", legend.direction = "horizontal")<br/><br/><br/>Student_CondEffects3 &lt;- <br/>  conditional_effects(Student_Fit2, dpar = "sigma")<br/><br/>Student_CondEffects3 &lt;- plot(Student_CondEffects3, <br/>       plot = FALSE)[[3]]<br/><br/>Student_CondEffects3 + <br/>  Plot_theme +<br/>  theme(legend.position = "bottom", legend.direction = "horizontal")</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rn"><img src="../Images/1941426e2013a1539ee1a8da88ac2c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NMA0bg-Q-OgXRmq-ZhiwOg.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 12: Conditional effects for the Student-t model with sigma</figcaption></figure><figure class="pk pl pm pn po qa px py paragraph-image"><div class="px py rn"><img src="../Images/f77b06a4fa8a838c0f16030ac55f20ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UbSChC3KzE2o2vtUXGhvfA.png"/></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 13: Conditional effects for sigma</figcaption></figure><p id="e583" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">What we see in the second graph is sigma, which effectively accounts for the variance in this parameter between days and genotypes. We see a much higher uncertainty at day 1, especially for WT mice, while the parameter is analogous at day 2.</p><p id="e74a" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">We can conclude this article by comparing the three models for out-of-sample predictions.</p></div></div></div><div class="ab cb ob oc od oe" role="separator"><span class="of by bm og oh oi"/><span class="of by bm og oh oi"/><span class="of by bm og oh"/></div><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h1 id="1034" class="oj ok fr bf ol om on gr oo op oq gu or os ot ou ov ow ox oy oz pa pb pc pd pe bk">Model comparison</h1><p id="b0ac" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">We perform model comparisons using the WAIC criteria (<a class="af nw" href="#ref-gelman2013" rel="noopener ugc nofollow"><em class="nv">6</em></a>)for estimating the out-of-sample prediction error. By considering both the log-likelihood of the observed data and the effective number of parameters, it provides a balance between model fit and complexity. Unlike some other criteria, WAIC inherently accounts for the posterior distribution of the parameters rather than relying on point estimates, making it particularly suited to Bayesian analyses.</p><p id="9de5" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Given a data set and a Bayesian model, the WAIC is calculated as:</p><p id="e75e" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">WAIC=−2×(LLPD−<em class="nv">p</em>WAIC​)</p><p id="b349" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">Where: LLPD is the log pointwise predictive density, calculated as the average log-likelihood for each observed data point across the posterior samples. WAIC is the effective number of parameters, computed as the difference between the average of the log-likelihoods and the log-likelihood of the averages across posterior samples.</p><p id="2c72" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">We use the <code class="cx nx ny nz oa b">compare_performance</code> function from the <code class="cx nx ny nz oa b">performance</code> package, part of the <code class="cx nx ny nz oa b">easystats</code> environment (<a class="af nw" href="#ref-bayestestR" rel="noopener ugc nofollow"><em class="nv">4</em></a>, <a class="af nw" href="#ref-performance" rel="noopener ugc nofollow"><em class="nv">7</em></a>, <a class="af nw" href="#ref-makowski2019" rel="noopener ugc nofollow"><em class="nv">8</em></a>).</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="d53b" class="ps ok fr oa b bg pt pu l pv pw">Fit_Comp &lt;- <br/>  compare_performance(<br/>    Gaussian_Fit1, <br/>    Student_Fit, <br/>    Student_Fit2, <br/>    metrics = "all")<br/><br/>Fit_Comp</span></pre><p id="b1dd" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">The output shows that our Student-t model predicting sigma is the least penalized (WAIC = 497) for out-of-sample prediction. Note that there is no estimate for sigma in this model because it was included as a response variable. This table also shows that the student-t model has less residual variance (sigma) than the Gaussian model, which means that the variance is better explained by the predictors. We can visualize the same results as a graph:</p><pre class="pk pl pm pn po pp oa pq bp pr bb bk"><span id="d5b5" class="ps ok fr oa b bg pt pu l pv pw">Fit_Comp_W &lt;- <br/>loo_compare(<br/> Gaussian_Fit1, <br/>    Student_Fit, <br/>    Student_Fit2,  <br/>  criterion = "waic")<br/><br/># Generate WAIC graph<br/>Fit_Comp_WAIC &lt;- <br/>  Fit_Comp_W[, 7:8] %&gt;% <br/>  data.frame() %&gt;% <br/>  rownames_to_column(var = "model_name") %&gt;% <br/>  <br/>ggplot(<br/>  aes(x    = model_name, <br/>      y    = waic, <br/>      ymin = waic - se_waic, <br/>      ymax = waic + se_waic)<br/>  ) +<br/>  geom_pointrange(shape = 21) +<br/>  scale_x_discrete(<br/>    breaks=c("Gaussian_Fit1", <br/>             "Student_Fit", <br/>             "Student_Fit2"), <br/>            <br/>    labels=c("Gaussian_Fit1", <br/>             "Student_Fit", <br/>             "Student_Fit2") <br/>             <br/>    ) +<br/>  coord_flip() +<br/>  labs(x = "", <br/>       y = "WAIC (score)",<br/>       title = "") +<br/>  Plot_theme<br/><br/>Fit_Comp_WAIC</span></pre><figure class="pk pl pm pn po qa px py paragraph-image"><div role="button" tabindex="0" class="qb qc ed qd bh qe"><div class="px py rq"><img src="../Images/71b63f75bbbc0c81ad01b17c21412573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljcEG9rTyqsl5vMMIezU7A.png"/></div></div><figcaption class="qg qh qi px py qj qk bf b bg z dx">Figure 14: Model comparison by WAIC</figcaption></figure><p id="d79e" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk"><a class="af nw" href="#fig-Models_Graph" rel="noopener ugc nofollow">Figure 14</a> shows that our last model is less penalized for out-of-sample prediction.</p><p id="cd44" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">You can find an updated version of this post on my <a class="af nw" href="https://github.com/daniel-manrique/MediumBlog/blob/main/20240222_OutliersStudent-t.qmd" rel="noopener ugc nofollow" target="_blank">GitHub site</a>. Let me know if this journey was useful to you, and if you have any constructive comments to add to this exercise.</p><p id="6130" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">*Unless otherwise noted, all images are generated by the author using R code.</p><h1 id="1a72" class="oj ok fr bf ol om ql gr oo op qm gu or os qn ou ov ow qo oy oz pa qp pc pd pe bk">References</h1><p id="3953" class="pw-post-body-paragraph mn mo fr mp b gp pf mr ms gs pg mu mv mw ph my mz na pi nc nd ne pj ng nh ni fk bk">1.M. Ahsanullah, B. M. G. Kibria, M. Shakil, <em class="nv">Normal and student´s t distributions and their applications</em> (Atlantis Press, 2014; <a class="af nw" href="http://dx.doi.org/10.2991/978-94-6239-061-4" rel="noopener ugc nofollow" target="_blank">http://dx.doi.org/10.2991/978-94-6239-061-4</a>).</p><p id="9d7f" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">2. P.-C. Bürkner, Brms: An r package for bayesian multilevel models using stan. <strong class="mp fs">80</strong> (2017), doi:<a class="af nw" href="https://doi.org/10.18637/jss.v080.i01" rel="noopener ugc nofollow" target="_blank">10.18637/jss.v080.i01</a>.</p><p id="d050" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">3. K. Yang, J. Tu, T. Chen, <a class="af nw" href="https://doi.org/10.1136/gpsych-2019-100148" rel="noopener ugc nofollow" target="_blank">Homoscedasticity: an overlooked critical assumption for linear regression</a>. <em class="nv">General Psychiatry</em>. <strong class="mp fs">32</strong>, e100148 (2019).</p><p id="4a6f" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">4. D. Makowski, M. S. Ben-Shachar, D. Lüdecke, <a class="af nw" href="https://doi.org/10.21105/joss.01541" rel="noopener ugc nofollow" target="_blank">bayestestR: Describing effects and their uncertainty, existence and significance within the bayesian framework.</a> <strong class="mp fs">4</strong>, 1541 (2019).</p><p id="2b59" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">5. A. Heiss, A guide to modeling proportions with bayesian beta and zero-inflated beta regression models (2021), (available at <a class="af nw" href="http://dx.doi.org/10.59350/7p1a4-0tw75" rel="noopener ugc nofollow" target="_blank">http://dx.doi.org/10.59350/7p1a4-0tw75</a>).</p><p id="fd5d" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">6. A. Gelman, J. Hwang, A. Vehtari, <a class="af nw" href="https://doi.org/10.1007/s11222-013-9416-2" rel="noopener ugc nofollow" target="_blank">Understanding predictive information criteria for Bayesian models</a>. <em class="nv">Statistics and Computing</em>. <strong class="mp fs">24</strong>, 997–1016 (2013).</p><p id="6c95" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">7. D. Lüdecke, M. S. Ben-Shachar, I. Patil, P. Waggoner, D. Makowski, <a class="af nw" href="https://doi.org/10.21105/joss.03139" rel="noopener ugc nofollow" target="_blank">Performance: An r package for assessment, comparison and testing of statistical models</a>. <strong class="mp fs">6</strong>, 3139 (2021).</p><p id="57d5" class="pw-post-body-paragraph mn mo fr mp b gp mq mr ms gs mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fk bk">8. D. Makowski, M. Ben-Shachar, D. Lüdecke, <a class="af nw" href="https://doi.org/10.21105/joss.01541" rel="noopener ugc nofollow" target="_blank">bayestestR: Describing effects and their uncertainty, existence and significance within the bayesian framework</a>. <em class="nv">Journal of Open Source Software</em>. <strong class="mp fs">4</strong>, 1541 (2019).</p></div></div></div></div>    
</body>
</html>