- en: 'Random Forest, Explained: A Visual Guide with Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=collection_archive---------0-----------------------#2024-11-07](https://towardsdatascience.com/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c?source=collection_archive---------0-----------------------#2024-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ENSEMBLE LEARNING
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Making tree-mendous predictions with random trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--9f736a6e1b3c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--9f736a6e1b3c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9f736a6e1b3c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9f736a6e1b3c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--9f736a6e1b3c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9f736a6e1b3c--------------------------------)
    ·12 min read·Nov 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----9f736a6e1b3c--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: A fresh look on our favorite upside-down tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----9f736a6e1b3c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[Decision trees](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    are a great starting point in machine learning — they’re clear and make sense.
    But there’s a catch: they often don’t work well when dealing with new data. The
    predictions can be inconsistent and unreliable, which is a real problem when you’re
    trying to build something useful.'
  prefs: []
  type: TYPE_NORMAL
- en: This is where Random Forest comes in. It takes what’s good about decision trees
    and makes them work better by combining multiple trees together. It’s become a
    favorite tool for many data scientists because it’s both effective and practical.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how Random Forest works and why it might be exactly what you need
    for your next project. It’s time to stop getting lost in the trees and see the
    forest for what it really is — your next reliable tool in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a46fad3053f42f475b5f070c27b60998.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Random Forest is an ensemble machine learning model that combines multiple
    decision trees. Each tree in the forest is trained on a random sample of the data
    (bootstrap sampling) and considers only a random subset of features when making
    splits (feature randomization).
  prefs: []
  type: TYPE_NORMAL
- en: For classification tasks, the forest predicts by majority voting among trees,
    while for regression tasks, it averages the predictions. The model’s strength
    comes from its “wisdom of crowds” approach — while individual trees might make
    errors, the collective decision-making process **tends to average out these mistakes**
    and arrive at more reliable predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7c8ea0333ddfcdeb2779b785e896929.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forest is a part of bagging (bootstrap aggregating) algorithm because
    it builds each tree using different random part of data and combines their answers
    together.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this article, we’ll focus on the classic golf dataset as an example
    for classification. While Random Forests can handle both classification and regression
    tasks equally well, we’ll concentrate on the classification part — predicting
    whether someone will play golf based on weather conditions. The concepts we’ll
    explore can be easily adapted to regression problems (like predicting number of
    player) using the same principles.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05586d1dcea17f8a18206b58019181ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‘Overcast (one-hot-encoded into 3 columns)’, ’Temperature’ (in Fahrenheit),
    ‘Humidity’ (in %), ‘Windy’ (Yes/No) and ‘Play’ (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Main Mechanism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s how Random Forest works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap Sampling:** Each tree gets its own unique training set, created
    by randomly sampling from the original data with replacement. This means some
    data points may appear multiple times while others aren’t used.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random Feature Selection:** When making a split, each tree only considers
    a random subset of features (typically square root of total features).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Growing Trees:** Each tree grows using only its bootstrap sample and selected
    features, making splits until it reaches a stopping point (like pure groups or
    minimum sample size).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final Prediction:** All trees vote together for the final prediction. For
    classification, take the majority vote of class predictions; for regression, average
    the predicted values from all trees.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/3e815426c28ac98519c4884d12f43fac.png)'
  prefs: []
  type: TYPE_IMG
- en: A Random Forest Classifier makes predictions by combining results from 100 different
    decision trees, each analyzing features like temperature and outlook conditions.
    The final prediction comes from the most common answer among all trees.
  prefs: []
  type: TYPE_NORMAL
- en: Training Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Random Forest algorithm constructs multiple decision trees and combines
    them. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Bootstrap Sample Creation**'
  prefs: []
  type: TYPE_NORMAL
- en: 1.0\. Set the number of trees (default = 100)
  prefs: []
  type: TYPE_NORMAL
- en: '1.1\. For each tree in the forest:'
  prefs: []
  type: TYPE_NORMAL
- en: a. Create new training set by random sampling original data with replacement
    until reaching original dataset size. This is called **bootstrap sampling**.
  prefs: []
  type: TYPE_NORMAL
- en: b. Mark and set aside non-selected samples as out-of-bag (OOB) samples for later
    error estimation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1671fe794bccb45ec5a642740a8d0d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forest creates different training sets for each tree by randomly picking
    data points from the original training set, with some numbers appearing multiple
    times. The unused data points become test sets for checking each tree’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fd0d49cc5d9614e8b4d96dedd10a6ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice how similar the percentages of OOB above? When doing bootstrap sampling
    of *n* samples, each individual sample has about a 37% chance of never being picked.
    This comes from the probability calculation (1–1/*n*)*ⁿ*, which approaches 1/e
    ≈ 0.368 as *n* gets larger. That’s why each tree ends up using roughly 63% of
    the data for training, with the remaining 37% becoming OOB samples.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Tree Construction**'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Start at root node with complete bootstrap sample
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58dd94d352eb90afbfe2312662b55270.png)'
  prefs: []
  type: TYPE_IMG
- en: When building each decision tree, Random Forest considers a subset of data points
    and creates splits based on questions about their values — sending smaller values
    to the left and larger values to the right to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: a. Calculate initial node impurity using all samples in node
  prefs: []
  type: TYPE_NORMAL
- en: '· Classification: Gini or entropy'
  prefs: []
  type: TYPE_NORMAL
- en: '· Regression: MSE'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49ffa6509e2a0b7efc710bf7c8891e58.png)'
  prefs: []
  type: TYPE_IMG
- en: Random Forest starts by calculating the Gini Impurity of the entire dataset
    (before any splits) using the ratio of YES and NO labels — a measure of how mixed
    the labels are in the current data.
  prefs: []
  type: TYPE_NORMAL
- en: 'b. Select random subset of features from total available features:'
  prefs: []
  type: TYPE_NORMAL
- en: '· Classification: √n_features'
  prefs: []
  type: TYPE_NORMAL
- en: '· Regression: n_features/3'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71393bd15609a6d43354efe5d9f06ac3.png)'
  prefs: []
  type: TYPE_IMG
- en: For each split in a tree, Random Forest randomly picks a subset of weather features
    (here 2 out of 6) to consider, making each tree focus on different aspects of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'c. For each selected feature:'
  prefs: []
  type: TYPE_NORMAL
- en: · Sort data points by feature values
  prefs: []
  type: TYPE_NORMAL
- en: · Identify potential split points (midpoints between consecutive unique feature
    values)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31b2530c2f2eab495683e971a5cc88da.png)'
  prefs: []
  type: TYPE_IMG
- en: For each chosen feature, Random Forest looks at all possible split points in
    the sorted data (like temperature values 66.0, 69.0, 71.0, etc.) to find the best
    way to separate the data into two groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'd. For each potential split point:'
  prefs: []
  type: TYPE_NORMAL
- en: · Divide samples into left and right groups
  prefs: []
  type: TYPE_NORMAL
- en: · Calculate left child impurity using its samples
  prefs: []
  type: TYPE_NORMAL
- en: · Calculate right child impurity using its samples
  prefs: []
  type: TYPE_NORMAL
- en: '· Calculate impurity reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: parent_impurity — (left_weight × left_impurity + right_weight × right_impurity)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a76a1bc476cd60a143540debd4b04b3.png)'
  prefs: []
  type: TYPE_IMG
- en: To find the best split point, Random Forest calculates Gini Impurity for each
    possible split, takes a weighted average based on group sizes, and picks the split
    that gives the biggest reduction in impurity from the parent node.
  prefs: []
  type: TYPE_NORMAL
- en: e. Split the current node data using the feature and split point that gives
    the highest impurity reduction. Then pass data points to the respective child
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e71e8da4672712136d3478ad9d1cd59b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After comparing all possible splits, Random Forest picks the temperature threshold
    of 73.5°F as it gives the largest impurity reduction (0.041), creating two groups:
    one mixed group with temperatures below 73.5°F and one pure group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'f. For each child node, repeat the process (step b-e) until:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Pure node or minimum impurity decrease'
  prefs: []
  type: TYPE_NORMAL
- en: '- Minimum samples threshold'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum depth'
  prefs: []
  type: TYPE_NORMAL
- en: '- Maximum leaf nodes'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e9f1677218a52ede01934cdc731077e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This process continues for each new group (node): randomly select features,
    find the best split point, and divide the data further until each group is pure
    (all YES or all NO) or can’t be split anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3: Tree Construction** Repeat the whole Step 2 for other bootstrap samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a6197d5a37f101b794221d1c2c3383d.png)'
  prefs: []
  type: TYPE_IMG
- en: Each decision tree in the Random Forest splits data in different ways using
    different features and thresholds. This variety helps the forest make better predictions
    than any single tree.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b46220bdf49910aa49e1d13e353fe1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Accessing the internal bootstrap indices directly isn’t possible in the current
    scikit-learn implementation so this gives different trees than the one calculated
    in our previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For prediction, route new samples through all trees and aggregate:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Classification: majority vote'
  prefs: []
  type: TYPE_NORMAL
- en: '- Regression: mean prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e8bd423acc3d8361e5ce82ec3fd542e4.png)'
  prefs: []
  type: TYPE_IMG
- en: When new data comes in, each tree in the Random Forest uses its own decision
    path to make a prediction. The forest combines all these predictions (74 YES vs
    26 NO) and the majority vote becomes the final answer (YES in this case).
  prefs: []
  type: TYPE_NORMAL
- en: Out-of-Bag (OOB) Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember those samples that didn’t get used for training each tree — that leftover
    1/3? Those are your OOB samples. Instead of just ignoring them, Random Forest
    uses them as a convenient validation set for each tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afccd043882db9aba10d7e5fb93558e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Each tree gets tested on its own out-of-bag samples (data not used in its training).
    By averaging these individual OOB accuracy scores (50%, 66.6%, 60%), Random Forest
    provides a built-in way to measure performance without needing a separate test
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building all the trees, we can evaluate the test set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55f4ae7dd4d6a6513400959cda5c3c12.png)'
  prefs: []
  type: TYPE_IMG
- en: By combining multiple diverse decision trees and using majority voting, Random
    Forest achieves a high accuracy of 85.7% — typically better than single decision
    trees or simpler models!
  prefs: []
  type: TYPE_NORMAL
- en: Key Parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key Random Forest parameters (especially in `scikit-learn`) include all
    Decision Tree parameters, plus some unique ones.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forest-specific parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`oob_score` This uses leftover data (out-of-bag samples) to check how well
    the model works. This gives you a way to test your model without setting aside
    separate test data. It’s especially helpful with small datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators` This parameter controls how many trees to build (default is
    100).To find the optimal number of trees, **track the OOB error rate** as you
    add more trees to the forest. The error typically drops quickly at first, then
    levels off. **The point where it stabilizes suggests the optimal number** — adding
    more trees after this gives minimal improvement while increasing computation time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/97234d73657a128e85e5409216f55df5.png)![](../Images/3d6b30f69790966d7d1dc81b1bed0c83.png)'
  prefs: []
  type: TYPE_IMG
- en: In our results, while around 27 trees showed the best score (0.2857), this early
    performance can be unreliable. Between 40–100 trees, the error rates settle around
    0.5000, showing more consistent results. Using more than 100 trees doesn’t help
    and sometimes makes things worse. This suggests that using about 50–60 trees is
    a good choice — it’s stable, efficient, and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: '`bootstrap` This decides whether each tree learns from a random sample of data
    (`True`) or uses all data ( `False`). The default (`True`) helps create different
    kinds of trees, which is key to how Random Forests work. Only consider **setting
    it to** `**False**` **when you have very little data** and can’t afford to skip
    any samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_jobs` This controls how many processor cores to use during training. Setting
    it to `-1` uses all available cores, making training faster but using more memory.
    With big datasets, you might need to use fewer cores to avoid running out of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shared parameters with Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following parameters works the [same way as in Decision Tree](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e).
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum tree depth'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: Minimum samples needed to split a node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: Minimum samples required at leaf node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compared to Decision Tree, here are key differences in parameter importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth` This matters less in Random Forests because combining many trees
    helps prevent overfitting, even with deeper trees. You can usually let trees grow
    deeper to catch complex patterns in your data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`min_samples_split` and `min_samples_leaf` These are less important in Random
    Forests because using many trees naturally helps avoid overfitting. You can usually
    set these to smaller numbers than you would with a single decision tree.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pros & Cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Strong and Reliable:** Random Forests give accurate results and are less
    likely to overfit than single decision trees. By using random sampling and mixing
    up which features each tree considers at each node, they work well across many
    problems without needing much adjustment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature Importance:** The model can tell you which features matter most in
    making predictions by measuring how much each feature helps across all trees.
    This helps you understand what drives your predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Minimal Preprocessing:** Random Forests handle both numerical and categorical
    variables well without much preparation. They work well with missing values and
    outliers, and can find complex relationships in your data automatically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Computational Cost:** Training and using the model takes more time as you
    add more trees or make them deeper. While you can speed up training by using multiple
    processors, it still needs substantial computing power for big datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Limited Interpretability:** While you can see which features are important
    overall, it’s harder to understand exactly why the model made a specific prediction,
    unlike with single decision trees. This can be a problem when you need to explain
    each decision.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction Speed:** To make a prediction, data must go through all trees
    and then combine their answers. This makes Random Forests slower than simpler
    models, which might be an issue for real-time applications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’ve grown to really like Random Forests after seeing how well they work in
    practice. By combining multiple trees and letting each one learn from different
    parts of the data, they consistently make better predictions — of course, more
    than using just one tree alone.
  prefs: []
  type: TYPE_NORMAL
- en: While you do need to adjust some settings like the number of trees, they usually
    perform well even without much fine-tuning. They do need more computing power
    (and sometimes struggle with rare cases in the data) but their reliable performance
    and ease of use make them my go-to choice for many projects. It’s clear why so
    many data scientists feel the same way!
  prefs: []
  type: TYPE_NORMAL
- en: 🌟 Random Forest Classifier Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 🌟 Random Forest Regressor Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a detailed explanation of the [RandomForestClassifier](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
    and [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    and its implementation in scikit-learn, readers can refer to the official documentation,
    which provides comprehensive information on its usage and parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: '𝙎𝙚𝙚 𝙢𝙤𝙧𝙚 𝙀𝙣𝙨𝙚𝙢𝙗𝙡𝙚 𝙇𝙚𝙖𝙧𝙣𝙞𝙣𝙜 𝙝𝙚𝙧𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9f736a6e1b3c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----9f736a6e1b3c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '𝙔𝙤𝙪 𝙢𝙞𝙜𝙝𝙩 𝙖𝙡𝙨𝙤 𝙡𝙞𝙠𝙚:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----9f736a6e1b3c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----9f736a6e1b3c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
