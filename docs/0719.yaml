- en: Demystifying Mixtral of Experts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/demystifying-mixtral-of-experts-498fe3b9bcf4?source=collection_archive---------6-----------------------#2024-03-17](https://towardsdatascience.com/demystifying-mixtral-of-experts-498fe3b9bcf4?source=collection_archive---------6-----------------------#2024-03-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mistral AI’s open-source Mixtral 8x7B model made a lot of waves — here’s what’s
    under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page---byline--498fe3b9bcf4--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page---byline--498fe3b9bcf4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--498fe3b9bcf4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--498fe3b9bcf4--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page---byline--498fe3b9bcf4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--498fe3b9bcf4--------------------------------)
    ·8 min read·Mar 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6605ae5832d4ad7db1465daf19756d34.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with GPT-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixtral 8x7B, Mistral AI’s new sparse Mixtures of Experts LLM, recently made
    a lot of waves, with dramatic headlines such as “Mistral AI Introduces Mixtral
    8x7B: a Sparse Mixture of Experts (SMoE) Language Model [Transforming Machine
    Learning](https://www.marktechpost.com/2024/01/14/mistral-ai-introduces-mixtral-8x7b-a-sparse-mixture-of-experts-smoe-language-model-transforming-machine-learning/)”or
    “Mistral AI’s Mixtral 8x7B surpasses GPT-3.5, [shaking up the AI world](https://dataconomy.com/2023/12/12/mistral-ais-mixtral-8x7b-surpasses-gpt-3-5-shaking-up-the-ai-world/)”'
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI is a French AI startup founded in 2023 by former engineers from Meta
    and Google. The company released Mixtral 8x7B — in what was perhaps the most unceremonious
    release in the history of LLMs — by simply dumping the Torrent magnet link on
    their Twitter account on December 8th, 2023,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f13f3b1000158524267e15af4a59c7a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Twitter](https://publish.twitter.com/?query=https%3A%2F%2Ftwitter.com%2FMistralAI%2Fstatus%2F1733150512395038967&widget=Tweet)'
  prefs: []
  type: TYPE_NORMAL
- en: sparking numerous [memes](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf306820-a61f-48c8-84aa-a83598fe1320_500x341.png)
    about Mistral’s unconventional way to release models.
  prefs: []
  type: TYPE_NORMAL
- en: “[Mixtral of Experts](https://arxiv.org/abs/2401.04088)” (Jiang et al 2024),
    the accompanying research paper, was published about a month later, on January
    8th of this year, on Arxiv. Let’s take a look, and see if the hype is warranted.
  prefs: []
  type: TYPE_NORMAL
- en: '(Spoiler alert: under the hood, there’s not much that’s technically new.)'
  prefs: []
  type: TYPE_NORMAL
