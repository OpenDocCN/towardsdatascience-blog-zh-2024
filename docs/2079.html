<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring the Strategic Capabilities of LLMs in a Risk Game Setting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring the Strategic Capabilities of LLMs in a Risk Game Setting</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27">https://towardsdatascience.com/exploring-the-strategic-capabilities-of-llms-in-a-risk-game-setting-43c868d83c3b?source=collection_archive---------1-----------------------#2024-08-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="e2ac" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph"><a class="af fy" href="https://medium.com/@hc.ekne/list/strategic-ai-72a460668137" rel="noopener">STRATEGIC AI</a></h2><div/><div><h2 id="0a12" class="pw-subtitle-paragraph gt ga fq bf b gu gv gw gx gy gz ha hb hc hd he hf hg hh hi cq dx">In a simulated Risk environment, large language models from Anthropic, OpenAI, and Meta showcase distinct strategic behaviors, with Claude Sonnet 3.5 edging out a narrow lead</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------" rel="noopener follow"><div class="l hp hq by hr hs"><div class="l ed"><img alt="Hans Christian Ekne" class="l ep by dd de cx" src="../Images/c85483d8b5dd89584b996b321b7f4a45.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*A1zSnPpUFMzO9bmaOOGn6w.png"/><div class="ht by l dd de em n hu eo"/></div></div></a></div></div><div class="hv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------" rel="noopener follow"><div class="l hw hx by hr hy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ht by l br hz em n hu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ic id bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://medium.com/@hc.ekne?source=post_page---byline--43c868d83c3b--------------------------------" rel="noopener follow">Hans Christian Ekne</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ic id dx"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="bf b bg z dx"><div class="ab cn in io ip"><div class="iq ir ab"><div class="bf b bg z dx ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--43c868d83c3b--------------------------------" rel="noopener follow"><p class="bf b bg z iu iv iw ix iy iz ja jb bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">32 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w ea eb q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ed it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="bf b dy z dx"><span class="kq">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kp li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lg lh">8</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/6985aa99f03d8aafc79c081e0372f271.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7guIFVKv_hArqNw83A4sIA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image generated by the author using DALL-E</figcaption></figure><h1 id="ecd8" class="ni nj fq bf nk nl nm gw nn no np gz nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Introduction</h1><p id="2086" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">In recent years, large language models (LLMs) have rapidly become a part of our everyday lives. Since OpenAI blew our minds with GPT-3 we have witnessed a profound increase in the capabilities of the models. They are excelling in a myriad of different tests, in anything from language comprehension to reasoning and problem-solving tasks.</p><p id="cc15" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">One topic that I find particularly compelling — and perhaps under-explored — is the ability of LLMs to reason strategically. That is, how the models will act if you insert them into a situation where the outcome of their decisions depends not only on their own actions but also on the actions of others, who are also making decisions based on their own goals. The LLMs’ ability to think and act strategically is increasingly important as we weave them into our products and services, and especially considering the emerging risks associated with powerful AIs.</p><p id="d3d4" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">A decade ago, philosopher and author Nick Bostrom brought AI risk into the spotlight with his influential book <em class="pf">Superintelligence</em>. He started a global conversation about AI, and it brought AI as an existential risk into the popular debate. Although the LLMs are still far from Bostrom’s superintelligence, it’s important to keep an eye on their strategic capabilities as we integrate them tighter into our daily lives.</p><p id="6d38" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">When I was a child, I used to love playing board games, and Risk was one of my favorites. The game requires a great deal of strategy and if you don’t think through your moves, you will likely be decimated by your opponents. Risk serves as a good proxy for evaluating strategic behavior, because making strategic decisions often involves weighing potential gains against uncertain outcomes, and while for small troop sizes, luck clearly plays an big part, given enough time and larger army sizes, the luck component becomes less pronounced and the most skillful players emerge. So, what better arena to test the LLMs’ strategic behavior than Risk!</p><p id="fa47" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">In this article I explore two main topics related to LLMs and strategy. Firstly, which of the top LLM models is the most strategic Risk player and how strategic is the best model in its actions? Secondly, how have the strategic capabilities of the models developed through the model iterations?</p><p id="7cd6" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To answer these questions, I built a virtual Risk game engine and let the LLMs battle it out. The first part of this article will explore some of the details of the game implementation before we move on to analyzing the results. We then discuss how the LLMs approached the game and their strategic abilities and shortcomings, before we end with a section on what these results mean and what we can expect from future model generations.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="cfcc" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">Setting the Stage</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/a36d9b074821175cb83aa682b4ab4319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DnSGVXBHmwrJRyD062jvxA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image generated by author using DALL-E</figcaption></figure><h2 id="0c04" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Why Risk?</strong></h2><p id="c6d6" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">My own experience playing Risk obviously played a part in choosing this game as a testbed for the LLMs. The game requires players to understand how their territories are linked, balance offense with defense and all while planning long-term strategies. Elements of uncertainty are also introduced through dice rolls and unpredictable opponent behavior, challenging AI models to manage risk and adapt to changing conditions.</p><p id="540c" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Risk simulates real-world strategic challenges, such as resource allocation, adaptability, and pursuing long-term goals amid immediate obstacles, making it a valuable proxy for evaluating AI’s strategic capabilities. By placing LLMs in this environment, we can observe how well they handle these complexities compared to human players.</p><h2 id="79b4" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">The Modelling Environment</strong></h2><p id="d17b" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">To conduct the experiments, I created a small python package creatively named <code class="cx qj qk ql qm b">risk_game</code>. (See the appendix for how to get started running this on your own machine.) The package is a Risk game engine, and it allows for the simulation of games played by LLMs. (The non-technical reader can safely skip this part and continue to the section “The Flow of the Game”.)</p><p id="d7f5" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To make it easier to conceptually keep track of the moving parts, I followed an object-oriented approach to the package development, where I developed a few different key classes to run the simulation. This includes a game master class to control the flow of the game, a player class to control prompts sent to the LLMs and a game state class to control the state of the game, including which player controls which territories and how many troops they hold at any given time.</p><p id="8a37" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">I tried to make it a flexible and extensible solution for AI-driven strategy simulations, and the package could potentially be modified to study strategic behavior of LLMs in other settings as well. See below for a full overview of the package structure:</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="b52b" class="qq nj fq qm b bg qr qs l qt qu">risk_game/<br/>│<br/>├── llm_clients/<br/>│   ├── __init__.py<br/>│   ├── anthropic_client.py<br/>│   ├── bedrock_client.py<br/>│   ├── groq_client.py<br/>│   ├── llm_base.py<br/>│   ├── llm_client.py<br/>│   └── openai_client.py<br/>│<br/>├── utils/<br/>│   ├── __init__.py<br/>│   ├── decorators.py<br/>│   └── game_admin.py<br/>│<br/>├── card_deck.py<br/>├── experiments.py<br/>├── game_config.py<br/>├── game_constants.py<br/>├── game_master.py<br/>├── game_state.py<br/>├── main.py<br/>├── player_agent.py<br/>├── rules.py<br/>│<br/>├── scripts/<br/>│   ├── example_run.py<br/>│<br/>└── tests/</span></pre><p id="d0e6" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To run an experiment, I would first instantiate a <code class="cx qj qk ql qm b">GameConfig</code> object. This config objects holds all the related game configuration settings, like whether we played with progressive cards, whether or not capitals mode was active, and how high percent of the territories needed to be controlled to win, in addition to multiple other game settings. I would then used that to create an instance of the <code class="cx qj qk ql qm b">Experiment </code>class and call the <code class="cx qj qk ql qm b">run_experiment</code> method.</p><p id="7f30" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Diving deeper behind the scenes we can see how the <code class="cx qj qk ql qm b">Experiment</code>class is set up.</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="af58" class="qq nj fq qm b bg qr qs l qt qu">from risk_game.llm_clients import llm_client<br/>import risk_game.game_master as gm<br/>from risk_game.rules import Rules<br/>from typing import List<br/>from risk_game.game_config import GameConfig <br/><br/>class Experiment:<br/>    def __init__(self, config: GameConfig, agent_mix: int= 1, num_games=10<br/>      ) -&gt; None:<br/>        """<br/>        Initialize the experiment with default options.<br/>        <br/>        Args:<br/>        - num_games (int): The number of games to run in the experiment.<br/>        - agent_mix (int): The type of agent mix to use in the experiment.<br/>        - config (GameConfig): The configuration for the game.<br/><br/>        """<br/>        self.config = config    <br/>        self.num_games = num_games<br/>        self.agent_mix = agent_mix<br/><br/>    def __repr__(self) -&gt; str:<br/><br/>        if self.config.key_areas:<br/>            key_areas = ', '.join(self.config.key_areas)<br/>        else:<br/>            key_areas = 'None'<br/><br/>        return (f"Experiment Configuration:\n"<br/>                f"Agent Mix: {self.agent_mix}\n"<br/>                f"Number of Games: {self.num_games}\n"<br/>                f"Progressive: {self.config.progressive}\n"<br/>                f"Capitals: {self.config.capitals}\n"<br/>                f"Territory Control Percentage: +"<br/>                f"{self.config.territory_control_percentage:.2f}\n"<br/>                f"Required Continents: {self.config.required_continents}\n"<br/>                f"Key Areas: {key_areas}\n"<br/>                f"Max Rounds: {self.config.max_rounds}\n")<br/>    <br/>    def initialize_game(self)-&gt; gm.GameMaster:<br/>        """<br/>        Initializes a single game with default rules and players.<br/>        <br/>        Returns:<br/>        - game: An instance of the initialized GameMaster class.<br/>        """<br/>        # Initialize the rules<br/>        rules = Rules(self.config)<br/>        game = gm.GameMaster(rules)<br/>        <br/>        if self.agent_mix == 1:<br/>            # Add strong AI players<br/>            game.add_player(name="llama3.1_70", <br/>                        llm_client=llm_client.create_llm_client("Groq", 1))<br/>            game.add_player(name="Claude_Sonnet_3_5", <br/>                        llm_client=llm_client.create_llm_client("Anthropic", 1))<br/>            game.add_player(name="gpt-4o", <br/>                        llm_client=llm_client.create_llm_client("OpenAI", 1))<br/>        <br/>        elif self.agent_mix == 3:<br/>            # Add mix of strong and weaker AI players from Open AI<br/>            game.add_player(name="Strong(gpt-4o)", <br/>                        llm_client=llm_client.create_llm_client("OpenAI", 1))<br/>            game.add_player(name="Medium(gpt-4o-mini)", <br/>                        llm_client=llm_client.create_llm_client("OpenAI", 2))<br/>            game.add_player(name="Weak(gpt-3.5-turbo)", <br/>                        llm_client=llm_client.create_llm_client("OpenAI", 3))<br/><br/>        elif self.agent_mix == 5:<br/>            # Add mix extra strong AI players<br/>            game.add_player(name="Big_llama3.1_400", <br/>                        llm_client=llm_client.create_llm_client("Bedrock", 1))<br/>            game.add_player(name="Claude_Sonnet_3_5", <br/>                        llm_client=llm_client.create_llm_client("Anthropic", 1))<br/>            game.add_player(name="gpt-4o", <br/>                        llm_client=llm_client.create_llm_client("OpenAI", 1))<br/><br/><br/>        return game<br/><br/>    def run_experiment(self)-&gt; None:<br/>        """<br/>        Runs the experiment by playing multiple games and saving results.<br/>        """<br/>        for i in range(1, self.num_games + 1):<br/>            print(f"Starting game {i}...")<br/>            game = self.initialize_game()<br/>            game.play_game(include_initial_troop_placement=True)</span></pre><p id="c30f" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">From the code above, we see that the <code class="cx qj qk ql qm b">run_experiment()</code> method will run the number of games that are specified in the initialization of the <code class="cx qj qk ql qm b">Experiment</code> object. The first thing that happens is to initialize a game, and the first thing we need to do is to create the rules and instantiate at game with the <code class="cx qj qk ql qm b">GameMaster</code> class. Subsequently, the chosen mix of LLM player agents are added to the game. This concludes the necessary pre-game set-up and we use the games’ <code class="cx qj qk ql qm b">play_game()</code>method to start playing a game.</p><p id="3a90" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To avoid becoming too technical I will skip over most of the code details for now, and rather refer the interested reader to the Github repo below. Check out the <code class="cx qj qk ql qm b">README</code> to get started:</p><div class="qv qw qx qy qz ra"><a href="https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="rb ab im"><div class="rc ab co cb rd re"><h2 class="bf gb ic z iu rf iw ix rg iz jb ga bk">GitHub — hcekne/risk-game</h2><div class="rh l"><h3 class="bf b ic z iu rf iw ix rg iz jb dx">Contribute to hcekne/risk-game development by creating an account on GitHub.</h3></div><div class="gr l"><p class="bf b dy z iu rf iw ix rg iz jb dx">github.com</p></div></div><div class="ri l"><div class="rj l rk rl rm ri rn lx ra"/></div></div></a></div><h2 id="a28a" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">The Flow of the Game</h2><p id="9334" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Once the game begins, the LLM player agents are prompted to do initial troop placement. The agents take turns placing their troops on their territories until all their initial troops have been exhausted.</p><p id="deba" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">After initial troop placement, the first player starts its turn. In Risk a turn is comprised of the 3 following phases:</p><ul class=""><li id="3733" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk"><strong class="og gb">Phase 1: Card trading and troop placement.</strong> If a player agent wins an attack during its turn, it gains a card. Once it has three cards, it can trade those in for troops if has the correct combination of infantry, cavalry, artillery or wildcard. The player also receives troops as a function of how many territories it controls and also if controls any continents.</li><li id="0beb" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk"><strong class="og gb">Phase 2: Attack. </strong>In this phase the player agent can attack other players and take over their territories. It is a good idea to attack because that allows the player to gain a card for that turn and also gain more territories. The player agent can attack as many times as it wishes during a turn.</li><li id="ae47" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk"><strong class="og gb">Phase 3: Fortify.</strong> The last phase is the fortify phase, and now the player is allowed to move troops from one of its territories to another. However, the territories must be connected by territories the player controls. The player is only allowed one such fortify move. After this the is finished, the next player starts his turn.</li></ul><p id="956b" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">At the beginning of each turn, the LLM agents receive dynamically generated prompts to formulate their strategy. This strategy-setting prompt provides the agent with the current game rules, the state of the board, and possible attack vectors. The agent’s response to this prompt guides its decisions throughout the turn, ensuring that its actions align with an overall strategic plan.</p><p id="ae5d" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The request for strategy prompt is given below:</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="ee17" class="qq nj fq qm b bg qr qs l qt qu">prompt = """<br/>  We are playing Risk and you are about to start your turn, but first <br/>  you need to define your strategy for this turn.<br/>  You, are {self.name}, and these are the current rules we are <br/>  playing with:<br/><br/>  {rules}<br/><br/>  {current_game_state}<br/><br/>  {formatted_attack_vectors}<br/><br/>  Your task is to formulate an overall strategy for your turn, <br/>  considering the territories you control, the other players, and the <br/>  potential for continent bonuses. <br/><br/>  Since the victory conditions only requires you to control <br/>  {game_state.territories_required_to_win} territories, and you already <br/>  control {number_of_territories} territories, <br/>  you only need to win an extra {extra_territories_required_to_win}<br/>  to win the game outright. Can you do that this turn?? If so lay <br/>  your strategy out accordingly.<br/><br/>  **Objective:**<br/><br/>  Your goal is to win the game by one of the victory conditions given<br/>  in the rules. Focus on decisive attacks that reduce <br/>  your opponents' ability to fight back. When possible, eliminate <br/>  opponents to gain their cards, which will allow you to trade them <br/>  in for more troops and accelerate your conquest.<br/><br/><br/>  **Strategic Considerations:**<br/><br/>  1. **Attack Strategy:**<br/>  - Identify the most advantageous territories to attack.<br/>  - Prioritize attacks that will help you secure continent bonuses or <br/>  weaken your strongest opponents.<br/>  - Look for opportunities to eliminate other players. If an opponent <br/>  has few territories left, eliminating them could allow you to gain <br/>  their cards, which can be especially powerful if you’re playing with <br/>  progressive card bonuses.<br/>  - Weigh the risks of attacking versus the potential rewards.<br/><br/>  2. **Defense Strategy:**<br/>  - Identify your most vulnerable territories and consider fortifying <br/>  them.<br/>  - Consider the potential moves of your opponents and plan your defense <br/>  accordingly.<br/><br/>  Multi-Turn Planning: Think about how you can win the game within <br/>  the next 2-3 turns. What moves will set you up for a decisive victory?<br/>  Don't just focus on this turn; consider how your actions this turn <br/>  will help you dominate in the next few turns.<br/><br/><br/>  **Instructions:**<br/><br/>  - **Limit your response to a maximum of 300 words.**<br/>  - **Be concise and direct. Avoid unnecessary elaboration.**<br/>  - **Provide your strategy in two bullet points, each with a <br/>  maximum of four sentences.**<br/><br/>  **Output Format:**<br/><br/>  Provide a high-level strategy for your turn, including:<br/>  1. **Attack Strategy:** Which territories will you target, and why? <br/>  How many troops will you commit to each attack? If you plan to <br/>  eliminate an opponent, explain how you will accomplish this.<br/>  2. **Defense Strategy:** Which territories will you fortify, and <br/>  how will you allocate your remaining troops?<br/><br/>  Example Strategy:<br/>  - **Attack Strategy:** Attack {Territory B} from {Territory C} with <br/>  10 troops to weaken Player 1 and prevent them from securing the <br/>  continent bonus for {Continent Y}. Eliminate Player 2 by attacking <br/>  their last remaining territory, {Territory D}, to gain their cards.<br/>  - **Defense Strategy:** Fortify {Territory E} with 3 troops to <br/>  protect against a potential counter-attack from Player 3.<br/><br/>  Remember, your goal is to make the best strategic decisions that<br/>      will maximize your chances of winning the game. Consider the <br/>      potential moves of your opponents and how you can position <br/>      yourself to counter them effectively.<br/><br/>  What is your strategy for this turn?<br/>        """</span></pre><p id="5a09" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">As you can see from the prompt above, there are multiple dynamically generated elements that help the player agent better understand the game context and make more informed strategic decisions.</p><p id="a1c8" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">These dynamically produced elements include:</p><ul class=""><li id="9a61" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk"><strong class="og gb">Rules:</strong> The rules of the game such as whether capitals mode is activated, how many percent of the territories are needed to secure a win, etc.</li><li id="9cac" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk"><strong class="og gb">Current game state</strong>: This is presented to the agent as the different continents and the</li><li id="42b5" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk"><strong class="og gb">Formatted Attack Vectors:</strong> These are a collection of the possible territories the agent can launch an attack from, to which territories it can attack and the maximum number of troops the agent can attack with.</li><li id="4a52" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk"><strong class="og gb">The extra territories needed to win the game:</strong> This represents the remaining territories the agent needs to capture to win the game. For example, if the total territories required to win the game are 28 and the agent holds 25 territories, this number would be 3 and would maybe encourage the agent to develop a more aggressive strategy for that turn.</li></ul><p id="8783" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">For each specific action during the turn — whether it’s placing troops, attacking, or fortifying — the agent is given tailored prompts that reflect the current game situation. Thankfully, Risk’s gameplay can be simplified because it adheres to the Markov property, meaning that optimal moves depend only on the current game state, not on the history of moves. This allows for streamlined prompts that focus on the present conditions</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8198" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">The Experimental Setup</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/dc054b42a45908a69df85bf2d0b5a9b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DEsM4xx4JSEr5coedZ6sA.png"/></div></div></figure><p id="b740" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To explore the strategic capabilities of LLMs, I designed two main experiments. These experiments were crafted to address two key questions:</p><ol class=""><li id="1109" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz rw rp rq bk"><em class="pf">What is the top performing LLM, and how strategic is it in its actions?</em></li><li id="50b1" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz rw rp rq bk"><em class="pf">Is there a progression in the strategic capabilities of the LLMs through model iterations?</em></li></ol><p id="c767" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Both of these questions can be answered by running two different experiments, with a slightly different mix of AI agents.</p><h2 id="b9e0" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Experiment-1: Evaluating the Top Models</strong></h2><p id="7bc8" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">For the first question, I created an experiment using the following top LLM models as players:</p><ul class=""><li id="d69b" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk">OpenAI’s GPT-4o running off the OpenAI API endpoint</li><li id="4181" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">Anthropic’s claude-3–5-sonnet-20240620 running off the Anthropic API endpoint</li><li id="4d05" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">Meta’s llama-3.1–70b-versatile running of the Groq API endpoint</li></ul><p id="4f05" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">I obviously wanted to try Meta’s meta.llama3–1–405b-instruct-v1:0 and configured it to run off AWS Bedrock, however the response time was painfully slow and made simulating games take forever. This is why we run Meta’s 70b model on Groq. It’s much faster than AWS bedrock. (If anyone knows how to speed up llama3.1 405b on AWS please let me know!)</p><p id="dd61" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">And we formulate our null and alternative hypotheses as follows:</p><blockquote class="rx ry rz"><p id="de7d" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-1, H0 : </strong>There is no difference in performance among the models; each model has an equal probability of winning.</p><p id="7e82" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-1, H1​</strong>: At least one model performs better (or worse) than the others, indicating that the models do not have equal performance.</p></blockquote><h2 id="593a" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Experiment-2: Analyzing the Model Generations</strong></h2><p id="6a85" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The second experiment aimed to evaluate how strategic capabilities have progressed through different iterations of OpenAI’s models. For this, I selected three models:</p><ul class=""><li id="1d10" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk">GPT-4o</li><li id="f215" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-4o-mini</li><li id="5bbb" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-3.5-turbo-0125</li></ul><p id="b035" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2</strong> allows us to see how the strategic capabilities of the models have developed across model generations, and also allows us to analyze the difference between different size models in the same model generation (GPT-4o vs GPT-4o-mini). I chose OpenAI’s solutions because they didn’t have the same restrictive rate limits as the other providers.</p><p id="7add" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Similarly, as for <strong class="og gb">Experiment-1</strong>, for this experiment we can formulate our null and alternative hypotheses:</p><blockquote class="rx ry rz"><p id="0558" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H0: </strong>There is no difference in performance among the models; each model has an equal probability of winning</p><p id="1f9d" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H1A​</strong>: GPT-4o is better than GPT-4o-mini</p><p id="8366" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H1B: </strong>GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo</p></blockquote><h2 id="be32" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Game Setup, Victory Conditions &amp; Card Bonuses</h2><p id="f1ce" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Both experiments involved 10 games, each with the same victory conditions. There are multiple different victory conditions in Risk, and typical victory conditions that players can agree upon are:</p><ol class=""><li id="66b1" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz rw rp rq bk">Number of controlled territories required for the winner. “World domination” is subset of this where one player needs to control all the territories. Other typical territory conditions are 70% territory control.</li><li id="afea" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz rw rp rq bk">Number of controlled continent(s) required for the winner</li><li id="5fc8" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz rw rp rq bk">Control / possession of key areas required for the winner</li><li id="35d8" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz rw rp rq bk">Preset time / turn count: whoever controls the most territories after x hours or x turns wins.</li></ol><p id="b374" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">In the end I settled for a more pragmatic approach which was a a combination of victory conditions that would be easier to fulfill and progressive cards. The victory conditions for the games in the experiments were finally chosen to be:</p><ol class=""><li id="11a4" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz rw rp rq bk">First agent to reach 65% territory dominance or</li><li id="406e" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz rw rp rq bk">The agent with the most territories after 17 game rounds of play (Making the full game be concluded after at most 51 turns distributed across the three players.)</li></ol><p id="6aaa" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">For those of you unfamiliar with Risk, progressive cards means that the value of the traded cards increase progressively as the game goes on, which is contrasted by fixed cards, where the troop value of traded cards are the same throughout the game. (4,6,8,10 for the different combinations.) Progressive is generally accepted to be a faster game mode.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6969" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">The Results — Who Conquered the World?</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/026bdd04bf3a9d830e8e4c046b982a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38Jdks5JRfFOyVFnbaVprQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image generated by the author using DALL-E</figcaption></figure><h2 id="3550" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Experiment-1: The Top Models</h2><p id="71b1" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The results were actually quite astounding — for both experiments. Starting with the first, below we show the distribution of wins amongst the three agents. Anthropic’s Claude is the winner with 5 wins in total, second place goes to OpenAI’s GPT-4o with 3 wins and last place to Meta’s llama3.1 with 2 wins.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq sa"><img src="../Images/6788b3e54ad8d81ba78591808ecfa9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qG1yguQJsVBD5o-9XsPkHg.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure 3. Experiment-1 wins by player, grouped by victory condition / image by author</figcaption></figure><p id="bfa9" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Because of their long history and early success with GPT-3 I was expecting OpenAI's model to be the winner, but it ended up being Anthropic’s Claude which took a lead in overall games. I guess if we take a look at how Claude is performing on <a class="af sb" href="https://www.anthropic.com/news/claude-3-5-sonnet" rel="noopener ugc nofollow" target="_blank">benchmark tests</a>, it shouldn’t be too unexpected that they come out ahead.</p><h2 id="16c9" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Territory Control and Game Flow</h2><p id="0d5d" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">If we dive a little deeper in the overall flow of the game and evaluate the distribution of territories throughout the game, we find the following:</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx sh si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/aa1eddf080dd03ff5fde4148032d1d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*aPAFW9HFArMoSMT3j92pSA.png"/></div></figure><figure class="lh mx sm si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/6f94cec739c08a2a3b1883dc0577085c.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*mLqzB9LDgqjLFlaAhSC6QA.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx sn ed so sp">Figure 5. Experiment-1 territory control per turn / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8ae9" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">When we examine the distribution of territories throughout the games, a clearer picture emerges. On average, Claude managed to gain a lead in territory control midway through most games and maintained that lead until the end. Interestingly, there was only one instance where a player was eliminated from the game entirely — this happened in Game 8, where Llama 3.1 was knocked out around turn 27.</p><blockquote class="rx ry rz"><p id="40e6" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">In our analysis, a “turn” refers to the full set of moves made by one player during their turn. Since we had three agents participating, each game round typically involved three turns, one for each player. As players were eliminated, the number of turns per round naturally decreased.</p></blockquote><p id="15fd" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Looking at the evolution of troop strength and territory control we find the following:</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx sq si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/5574ac0390ea9bc95173bd127e2eb94b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*6bSbLyjTSGSVou_NOD4a4w.png"/></div></figure><figure class="lh mx sr si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/72b526b75082936ab08dead7e440d04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*---rP82FdenORE5Hxc0OJQ.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx sn ed so sp">Figure 6. Experiment-1 change in troop strength throughout the game / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d6d6" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The troop strength seems to be relatively even, on average, for all the models, so that is clearly not the reason why Claude is able to pull off the most wins.</p><h2 id="948b" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Statistical Analysis: Is Claude Really the Best?</h2><p id="88ca" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">In this experiment, I aimed to determine whether any of the three models demonstrated significantly better performance than the others based on the number of wins. Given that the outcome of interest was the frequency of wins across multiple categories (the three models), the chi-square goodness-of-fit test is a good statistical tool to use.</p><p id="7dcc" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The test is often used to compare observed frequencies against expected frequencies under the null hypothesis, which in this case was that all models would have an equal probability of winning. By applying the chi-square test, I could assess whether the distribution of wins across the models deviated significantly from the expected distribution, thereby helping to identify if any model performed substantially better.</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="7a76" class="qq nj fq qm b bg qr qs l qt qu">from scipy.stats import chisquare<br/><br/># Observed wins for the three models<br/>observed = [5, 3, 2]<br/><br/># Expected wins under the null hypothesis (equal probability)<br/>expected = [10 / 3] * 3<br/><br/># Perform the chi-square goodness-of-fit test<br/>chi2_statistic, p_value = chisquare(f_obs=observed, f_exp=expected)<br/><br/>chi2_statistic, p_value<br/><br/>(1.4, 0.4965853037914095)</span></pre><p id="628b" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The chi-square goodness-of-fit test was conducted based on the observed wins for the three models: 5 wins for Claude, 3 wins for GPT-4o, and 2 wins for llama3.1. Under the null hypothesis:</p><blockquote class="rx ry rz"><p id="b340" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-1, H0 : </strong>There is no difference in performance among the models; each model has an equal probability of winning.</p></blockquote><p id="5fcf" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">each model was expected to win approximately 3.33 games out of the 10 trials. The chi-square test yielded a statistic of 1.4 with a corresponding p-value of 0.497. Since this p-value is much larger than the conventional significance level of 0.05, we can’t really say with any statistical rigor that Claude is better than the others.</p><blockquote class="ss"><p id="fe43" class="st su fq bf sv sw sx sy sz ta tb oz dx">We can interpret the p-value such that there is a 49.7% chance that we would observe an outcome as extreme as (5,3,2) under the null hypothesis, which assumes each model has the same probability of winning. So this is actually quite a likely scenario to observe.</p></blockquote><p id="9d69" class="pw-post-body-paragraph oe of fq og b gu tc oi oj gx td ol om on te op oq or tf ot ou ov tg ox oy oz fj bk">To make a definitive conclusion, we would need to run more experiments with a larger sample size. Unfortunately, rate limits — particularly with Llama 3.1 hosted on Groq — made this impractical. I invite the eager reader to follow up and test themselves. See the appendix for how to run the experiments on your own machine.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="115b" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Experiment-2: Model Generations</h2><p id="385f" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The results of Experiment-2 were equally surprising. Contrary to expectations, GPT-4o-mini outperformed both GPT-4o and GPT-3.5-turbo. GPT-4o-mini secured 7 wins, while GPT-4o managed 3 wins, and GPT-3.5-turbo failed to win any games.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq th"><img src="../Images/1b907976ce49a154ef432f34e1646c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*IubQVESZ0TFAHH1nLPBlAw.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure 8. Number of wins by player and victory condition / image by author</figcaption></figure><p id="cddc" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">GPT-4o-mini actually went off with the overall victory. This was also rather substantial, with 7 wins of GPT-4o’s 3 and GPT-3.5 turbo’s 0 wins. While GPT-4o on average had more troops GPT-4o-mini won most of the games.</p><h2 id="fa01" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Territory Control and Troop Strength</h2><p id="2622" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Again, diving deeper and looking at performance in individual games, we find the following:</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx sh si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/652e25d45a39e3b1f0977b557ad2dea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*1rM5c4WF9I_iqYB3hjGrmA.png"/></div></figure><figure class="lh mx sm si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/c6b8388c2167e89e5993f197a2a45306.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*taRqT4YLOvmnF722F04wBA.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx sn ed so sp">Figure 9. Experiment-2 Average territory control per turn, for all games / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="86f6" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The charts above show territory control per turn, on average, as well as for all the games. These plots show a confirmation of what we saw in the overall win statistics, namely that GPT-4o-mini is on average coming out with the lead in territory control by the end of the games. GPT-4o-mini is beating its big brother when it actually counts, close to the end of the game!</p><p id="a931" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Turning around and examining troop strength, a slightly different picture emerges:</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx sq si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/6d6a45f00d2db026a2cc801099fc23a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*VnYVOumhCNx_b8AzXq34Zw.png"/></div></figure><figure class="lh mx sr si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/525b3f22fd081aa7e8afc38c2e3a07ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*pqB9ajJwaZuJ4Zk6sSFRGg.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx sn ed so sp">Figure 10. Experiment-2 Average total troop strength per turn, for all games / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c2d4" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The above chart shows that on average, the assumed strongest player, GPT-4o manages to keep the highest troop strength throughout most of the games. Surprisingly it fails to use this troop strength to its advantage! Also, there is a clear trend between troop strength and model size and model generation.</p><p id="8274" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To get some more insights we can also evaluate a few games more in detail and look at the heatmap of controlled territories across the turns.</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><figure class="ms mt mu mv mw mx sj sk paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq ti"><img src="../Images/043d2693f7293e2f52cc8cb6cfbc7703.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*62iO1OjX7i7OfKJF8DvFfw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure 11. Experiment 2, heatmap of territory control, game 2 / image by author</figcaption></figure><figure class="lh mx sj sk paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq ti"><img src="../Images/9888ffe2054a2e82bc5e55e6aea482b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*d4ICxQfguUahTdmi2aIrRQ.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Figure 12. Experiment 2, heatmap of territory control, game 7 / image by author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4f95" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">From the heatmaps we see how the models trade blows and grab territories from another. Here we have selected two games which seemed reasonably representative for the 10 games in the experiment.</p><p id="e24e" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Regarding specific territory ownership, a trend we saw play out frequently was GPT-4o trying to hold North America while GPT-4o-mini often tried to get Asia.</p><h2 id="be27" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Statistical Analysis: Generational Differences</h2><p id="5924" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">With the above results, let’s again revisit our initial hypotheses:</p><blockquote class="rx ry rz"><p id="6647" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H0 : </strong>There is no difference in performance among the models; each model has an equal probability of winning.</p><p id="84a8" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H1A​</strong>: GPT-4o is better than GPT-4o-mini</p><p id="f63b" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2, H1B: </strong>GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo</p></blockquote><p id="48e1" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Let’s start with the easy one, <strong class="og gb">H1B, </strong>namely that GPT-4o and GPT-4o-mini are better than GPT-3.5-turbo. This is quite easy to see, and we can do a chi-squared test again, based on equal probabilities of winning for each model.</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="4d1c" class="qq nj fq qm b bg qr qs l qt qu">from scipy.stats import chisquare<br/><br/># Observed wins for the three models<br/>observed = [7, 3, 0]<br/><br/># total observations<br/>total_observations = sum(observed)<br/><br/># Expected wins under the null hypothesis (equal probability)<br/>expected_probabilites = [1/3] * 3<br/><br/>expeceted_wins = [total_observations * p for p in expected_probabilities]<br/><br/># Perform the chi-square goodness-of-fit test<br/>chi2_statistic, p_value = chisquare(f_obs=observed, f_exp=expected_wins)<br/><br/>chi2_statistic, p_value<br/><br/>(7.4, 0.0247235265)</span></pre><p id="16f0" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">This suggests that the observed distribution of wins is unlikely to have occurred if every model had the same probability of winning, 33.3%. In fact, a case as extreme as this could only be expected to have occurred in 2.5% of cases.</p><p id="465f" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">To then evaluate our <strong class="og gb">H1A</strong> hypothesis we should first update our null hypothesis adjusting for unequal probabilities of winning. For example, we can now assume that:</p><ul class=""><li id="07ee" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk">GPT-4o-mini: Higher probability</li><li id="e6ab" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-4o: Higher probability</li><li id="2f33" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-3.5-turbo: Lower probability</li></ul><p id="2a98" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Putting some numbers on these, and given the results we just observed, let’s assume GPT-4o-mini:</p><ul class=""><li id="fd56" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk">GPT-40-mini: 45% chance of winning each game</li><li id="6a65" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-4o: 45% chance of winning each game</li><li id="7729" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-3.5-turbo: 10% chance of winning each game</li></ul><p id="6205" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Then, for 10 games, the expected wins would be:</p><ul class=""><li id="491d" class="oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz ro rp rq bk">GPT-4o-mini: 0.45×10=4.5</li><li id="7d86" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-4o: 0.45 ×10=4.5</li><li id="8e59" class="oe of fq og b gu rr oi oj gx rs ol om on rt op oq or ru ot ou ov rv ox oy oz ro rp rq bk">GPT-3.5-turbo: 0.1×10=10 → .1×10=1</li></ul><p id="0fde" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">In addition, given the fact that GPT-4o-mini won 7 out of the 10 games, we also revise our alternative hypothesis:</p><blockquote class="rx ry rz"><p id="ce54" class="oe of pf og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><strong class="og gb">Experiment-2 Revised Hypothesis, H1AR</strong>: GPT-4o-mini is better than GPT-4o.</p></blockquote><p id="6d6a" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Using python to calculate the chi-squared test, we get:</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="7ef9" class="qq nj fq qm b bg qr qs l qt qu">from scipy.stats import chisquare<br/><br/># Observed wins for the three models<br/>observed = [7, 3, 0]<br/><br/># Expected wins under the null hypothesis (equal probability)<br/>expected_wins = [0.45 * 10, 0.45 * 10, 0.1 * 10] <br/><br/># Perform the chi-square goodness-of-fit test<br/>chi2_statistic, p_value = chisquare(f_obs=observed, <br/>  f_exp=expected_wins)<br/><br/>chi2_statistic, p_value<br/><br/>(2.8888888888888890, .23587708298570023)</span></pre><p id="0a6d" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">With our updated probabilities, we see from the code above that seeing a result as extreme as we did, (7,3,0) is in fact not very unlikely under our new updated expected probabilities. Interpreting the p-value tells us that a results at least as extreme as what we observed would be expected 23% of the time. So, we cannot conclude with any statistical significance that there is a difference between GPT-4o-mini and GPT-4o and we reject the revised alternative hypothesis, <strong class="og gb">H1AR.</strong></p><h2 id="9bc8" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Key Takeaways</strong></h2><p id="67ea" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Although there is only limited evidence to suggest Claude is the more strategic model, we can with reasonably high confidence state that there is a difference in performance across model generations. GPT-3.5-turbo is significantly less strategic than its newer iterations. Obviously this implication works in reverse, which means we are seeing an increase in the strategic abilities of the models as they improve through the generations, and this is likely to profoundly impact how these models will be used in the future.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5f10" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">Analyzing the Strategic Behavior of LLMs</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/58a82733ec071f0d0f915de1aab5cfd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7LTIQ75iC7nwy7ldONBTw.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image generated by the author using DALL-E</figcaption></figure><p id="7afb" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">One of the first things I noticed after running some initial tests were how different the LLMs play than humans. The LLM games seem to have more turns than human games, even after I prompted them to be more aggressive and try to go after weak opponents.</p><p id="9af1" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">While many of the observations about player strategy can be made just from looking at plots of territory control and troop strength; some of the more detailed observations below first became clear as I watched the LLMs play turn-by-turn. This is slightly hard to replicate in an article format, however all the data from both experiments are stored in .csv files in the Github repo and loaded into pandas dataframes in the Jupyter notebooks used for analysis. The interested reader can find them in the repo here: <code class="cx qj qk ql qm b">/game_analysis/experiment1_analysis_notebook.ipynb</code>. The dataframe <code class="cx qj qk ql qm b">experiment1_game_data_df</code>holds all relevant game data for Experiment-1. By looking at territory ownership and troop control turn-by-turn more details about the playstyles emerge.</p><h2 id="7804" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Distinctive Winning Play Styles</h2><p id="8874" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">What seemed to distinguish Anthropic’s model was its ability to claim a lot of territory in one move. This can be seen in some of the plots of territory control, when you look at individual games. But even though Claude had the most wins, how strategic was it really? Based on what I observed in the experiments, it seems that the LLMs are still rather immature when it comes to strategy. Below we discuss some of the typical behavior observed through the games.</p><h2 id="7054" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Poor Fortifying Strategies</h2><p id="682f" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">A common issue across all models was a failure to adequately fortify their borders. Quite frequently, the times the agents were also stuck with a lot of troops inside their internal territory instead of guarding their borders. This made it easier for neighbors to attack their territories and steal continent bonuses. In addition, it made it more difficult for the player agents to actually do a larger land-grab since often their territories with large troop strengths were surrounded by other territories it controlled.</p><h2 id="667c" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Failure to See Winning Moves</h2><p id="cde0" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Another noticeable shortcoming was the models’ failure to recognize winning moves. They don’t seem to realize that they can win in a turn if they play correctly. Less so with the stronger models but still present.</p><p id="d136" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">For example, for all the games in the simulations we played with 65% territory control to win. This means you just need to acquire 28 territories. In one instance during Experiment-2 Game 2, OpenAI’s GPT-4o had 24 territories and 19 troops in Greenland. It could easily just have taken Europe which has several territories with just 1 troop, however, it fails to see the move. This is a move that even a relatively inexperienced human player would likely recognize.</p><h2 id="d654" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Failure to Eliminate Other Players</h2><p id="cd81" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The models also frequently failed to eliminate opponents with only a few troops remaining, even when it would have been strategically advantageous. More specifically, they fail to remove players with only a few troops left and more than 2 cards. This would be considered an easy move for most human players, especially when playing with progressive cards. The card bonuses quickly escalate, and if an opponent only has 10 troops left, but 3 or more cards, taking him down for the cards is almost always the right move.</p><h2 id="1740" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">GPT-4o Likes North America</h2><p id="d934" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">One of the very typical strategies that I saw GPT-4o pursue was to get early control over North America. Because of the strong continent bonus and the fact that it only requires to be guarded in 3 places means that is a strategically good starting point. I suspect the reason that GPT-4o does is because it has read as part of its training data that it is a strategically good location.</p><h2 id="9cf9" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Top Models Finish More Games</h2><p id="0572" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Overall, there is a trend amongst the top models to finish more of the games, and achieve the victory conditions than weaker models. Of the games played with the top models, only 2 games went to the max game limit, while this happened 6 times for the weaker models.</p><h2 id="aced" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Limitations of Pre-Trained Knowledge</strong></h2><p id="8b4f" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">A limitation of classic Risk is of course that the LLMs have read about strategies for playing Risk online, and then the top models are simply the best ones at executing on this. I think the tendency to quickly try to dominate North America highlights this. This limitation could be mitigated if we played with randomly generated maps instead. This would increase the difficulty level and would provide a higher bar for the models. However, given their performance on the current maps I don’t think we need to increase the difficulty for the current model generations.</p><h2 id="82d4" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">General observations</h2><p id="6347" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Even the strongest LLMs are still far from mastering strategic gameplay. None of the models exhibited behavior that could challenge even an average human player. I think we have to wait at least one or two model generations before we can start to see a substantial increase in strategic behavior.</p><p id="f95c" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">That said, dynamically adjusting prompts to handle specific scenarios — such as eliminating weak opponents for card bonuses — could improve performance. With different and more enhanced prompting the models might be able to put up more of a fight. To get that to work though, you would need to manually program in a range of possible scenarios that typically occur and offer specialized prompts for each scenario.</p><p id="663d" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Consider a concrete example where we could see this come into play: Player B is weak and only has 4 territories with 10 troops, however player B has 3 Risk cards, and you are playing progressive cards and the reward for trading in cards is currently 20 troops.</p><p id="e0b6" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">For the sake of this experiment, I didn’t want to make the prompts too specialized, because the goal wasn’t to optimize agent behavior in Risk, but rather to test their ability to do that themselves, given the game state.</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9350" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">What These Results Mean for the Future of AI and Strategy</h1><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/c4820641aa83f44cfde7286079673697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJM-sqMLXAeViMaj3IhVaA.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Image generated by the author using DALL-E</figcaption></figure><p id="99bf" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The results from these experiments highlight a few key considerations for the future of AI and its strategic applications. While LLMs have shown remarkable improvements in language comprehension and problem-solving, their ability to reason and act strategically is still in its early stages.</p><h2 id="48e5" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Strategic Awareness and AI Evolution</strong></h2><p id="5810" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">As seen in the simulations, the current generation of LLMs struggles with basic strategic concepts like fortification and recognizing winning moves. This indicates that even though AI models have improved in many areas, the sophistication required for high-level strategic thinking remains underdeveloped.</p><p id="d3e4" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">However, as we clearly saw in Experiment-2, there is a trend towards improved strategic thinking, and if this trend keeps going for future generations we probably don’t have to wait too long until the models are much more capable. There are people claiming the LLMs have already plateaued, however I would be very careful assuming that.</p><h2 id="f281" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Implications for Real-World Applications</strong></h2><p id="6399" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The real-world applications of a strategically aware and capable AI agent are obviously enormous and cannot be understated. They could be used in anything from business strategy to military planning and complex human interaction. A strategic AI that can anticipate and react to the actions of others could be incredibly valuable — and of course also very dangerous. Below we present three possible applications.</p><p id="3c4e" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">If we consider a more positive application first, we could imagine everyone having a helpful strategic agent guiding them through their daily lives, helping make important decisions. This agent could help with anything from financial planning, planning daily tasks, to optimizing social interactions and behavior that involves the actions of other humans. It could act on your behalf and be goal oriented to optimize your interests and well-being.</p><p id="fd6a" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">There are obviously many insidious application areas as well. Think: Autonomous fighter drones with on-board strategic capabilities. This might not be too far-fetched especially when we consider the relative strength of the smaller models compared to their big-brother counterparts, for example GPT-4o vs GPT-4o-mini. Smaller models are much easier to deploy on edge devices like drones, and when we see how <a class="af sb" href="https://www.atlanticcouncil.org/blogs/ukrainealert/fpv-drones-in-ukraine-are-changing-modern-warfare/" rel="noopener ugc nofollow" target="_blank">popular</a> drones have become the Russian-Ukraine war, taking the step from first person view (FPV) drone to unmanned AI-driven drone might be considered feasible. Perhaps even as a back-up option if the drone operator lost contact with the drone.</p><p id="c595" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Detailed simulation of social interaction is a third way to use strategically aware agents. We could for example create simulations to model specific economic or other social phenomena, blending classic agent-based methods with LLMs. Agent based modelling (ABM) as a field of research and toolkit for understanding complex adaptive systems has existed for decades — I used in my Masters' thesis back in 2012 — but coupled with much smarter and strategic agents this could potentially be game changing.</p><h2 id="a378" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">The Importance of Dynamic Prompting</strong></h2><p id="7523" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Detailed dynamic prompting is probably one of the best ways to use and interact with LLMs in the near future — and perhaps also for the next few model generations (GPT-5, Claude 4, etc.). By providing more dynamic scenario-specific prompts and letting LLM agents execute specific plans, we might see more sophisticated strategic behavior in the next generation of models.</p><p id="ff3d" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">This type of “handholding” requires a lot more work from human programmers — than just prompting agents directly — however it could be a crucial stepping stone until the models become more capable of independent strategic thinking.</p><p id="2e5c" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">One could of course argue that if we provide too detailed and specific prompts we are working against the generalized nature of these models, and at that point we might as well introduce a different type of optimization algorithm, however I think there are many problems where the more open-ended problem-solving abilities of the LLMs could be paired with some form of dynamic prompting.</p><h2 id="ef02" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">The Need for New Benchmarks</strong></h2><p id="55c9" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">As the LLMs continue to improve, it will also be necessary to develop new benchmarks to study them. The traditional benchmarks and tests are well suited to study problem-solving in isolated environments, but moving forward we might want to introduce more strategic tests, that allow us to understand how the agents behave in situations where they need to consider how their actions influence others over time. Games like Risk provide a reasonable starting point because of their strategic nature and elements of uncertainty.</p><h2 id="efa5" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk"><strong class="al">Future Considerations</strong></h2><p id="a88e" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Looking ahead, as AI models continue to evolve, it will be important to monitor their strategic capabilities closely. We need to ensure that as these models become more powerful, they are aligned with human values and ethical considerations. The risks associated with strategic AI — such as unintended consequences in high-stakes environments — must be carefully managed.</p><p id="a7fa" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">As smaller models like GPT-4o-mini have shown competitive performance in strategic tasks, there is potential for deploying highly capable AI on edge devices, such as drones or autonomous systems. This opens up new possibilities for decentralized AI applications that require real-time decision-making in dynamic environments.</p><h1 id="6e36" class="ni nj fq bf nk nl nm gw nn no np gz nq nr ns nt nu nv nw nx ny nz oa ob oc od bk">Conclusion</h1><p id="b2ed" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">I think it’s safe to say that that while the strategic capabilities of LLMs are improving with each new generation, they still have a long way to go before they can rival even a moderately skilled human player. Models like Claude and GPT-4o are beginning to show some level of strategic thinking, but their shortcomings in areas such as fortification and recognizing winning moves highlight the current limitations of AI in complex, multi-agent environments. Nevertheless, the trend toward better performance across newer models shows promise for future advancements in AI strategy.</p><p id="caa9" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">As we continue to integrate AI into more aspects of life, from business to military strategy, understanding and refining the strategic capabilities of these systems will become increasingly important. While we’re not there yet, the potential for AI to handle complex decision-making processes in dynamic environments is incredible. It will be super interesting to see how the capabilities of the LLMs evolve over time, and if our results that show the improvements of LLMs across model generations continue through to GPT-5, GPT-6, Claude 4, Claude 5 etc. I think we are in for a wild ride!</p><p id="cae4" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">If you are interested in developing your own AI driven tools, feel free to reach out! I am always happy to explore collaborative opportunities!</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="32a4" class="ni nj fq bf nk nl po gw nn no pp gz nq nr pq nt nu nv pr nx ny nz ps ob oc od bk">Appendix</h1><p id="6dba" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Here I aim to provide some extra details that while interesting for the more technically inclined reader, might not be necessary for the full flow of the article. The first topic we touch on is rate limit issues. Then we describe more detailed analysis of errors, accumulated turn time used by the agents and parsing of responses from the LLMs. In addition, I provide the reader with a short description of how to test the code base out by cloning the Github repo and getting started with the docker setup.</p><h2 id="61d7" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Rate Limit Issues</h2><p id="527c" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">There are many actions that needs to be considered every turn, and this leads to quite a lot of back-and-forth interaction between the program and the LLM providers. One issues that turned out to be slightly problematic for running longer experiments was rate limiting.</p><p id="d83a" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Rate limits are something the LLM providers set in place to protect against spamming an other potentially disrupting behavior, so even though you have funds in the accounts, the providers still limit the amount of tokens you can query. For example, Anthropic does a rate limit of 1M token / per day for their best model.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq tj"><img src="../Images/75e0be47ff4140dbf85276862452a009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQlClm0kgNBCeCr3cQui7w.png"/></div></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx">Rate limits for Anthropic models, taken from Anthropic console</figcaption></figure><p id="b514" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">And when you hit your rate limit, your LLM queries are answered with</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="b5a3" class="qq nj fq qm b bg qr qs l qt qu">Rate Limit Error: Rate limit reached for model `llama-3.1-70b-versatile` <br/>in organization `org_01j440c04tfr3aas7qctr0ejtk` <br/>on : Limit 1000000, Used 999496, Requested 1573. <br/>Please try again in 1m32.2828s. <br/>Visit https://console.groq.com/docs/rate-limits for more information.</span></pre><p id="0bc9" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">For many application areas this might not be a problem, however the simulation queries each of the providers multiple times per turn (for strategy evaluation, card choice, troop placement, multiple attacks and fortification) so this quickly adds up, especially in long games that go over many rounds. I was initially planning on doing 10 experiments on with victory condition set to World Domination (this means the winner would need to control all 42 territories in the game to win), but because of how the LLMs play the game this wasn’t feasible in my time frame. The victory conditions had to be adjusted so a winner could be determined at an earlier stage.</p><h2 id="47cc" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Tracking Errors</h2><p id="19bd" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">Some of the LLMs in the experiments were also struggling with a large number of errors when prompted for moves, this could be anything from trying to place troops in territories they didn’t control to fortifying to territories that were not connected. I implemented a few variables to track these errors. This was way more common with the weaker models as the plots below suggests:</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx tk si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/b5681a68a04ca5da46fe2bc155d153f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*dA5AcGmgtiC0IlvIR8PRUg.png"/></div></figure><figure class="lh mx tl si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/b2a5e49a82cee4a6f18f039b4e426397.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*o-iCqEal3A3jXC8plb0fJA.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx tm ed tn sp">Experiment-1 attack and fortify errors / image by author</figcaption></figure></div><div class="ab kk"><figure class="lh mx to si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/1a73141944e7d89c7ab68119d32f9690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6nyKsxslFeW2YYbQvS34yg.png"/></div></figure><figure class="lh mx to si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/d2839d2ecda849e5ae8c087df31b4f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*xbOxfVVpT4iWWEt4vI0A_g.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx tp ed tq sp">Experiment-2 attack and fortify errors / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="588e" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Accumulated Turn Time</h2><p id="18ca" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">The last thing I tracked during the experiments was how much time each of the LLMs used on their actions. As expected, the largest and more complex models used the most time.</p></div></div><div class="mx"><div class="ab cb"><div class="ls sc lt sd lu se cf sf cg sg ci bh"><div class="ms mt mu mv mw ab kk"><figure class="lh mx tr si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/c0ecf143559382f80f1e3446b9b56e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*jhkM5TzhTNWhNRy0jpF1Tw.png"/></div></figure><figure class="lh mx ts si sj sk sl paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><img src="../Images/050feda024ff8bef9cfc13075d75518e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*G4hLjs6_G1aVdtv1PkcMfA.png"/></div><figcaption class="nd ne nf mp mq ng nh bf b bg z dx tm ed tn sp">Accumulated turn time by player / image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0837" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">What is clear is that Claude seems to be really taking it’s time. For experiment-1 GPT-4 is coming out better than llama3.1 70b running on Groq but this is likely due to the fact that there were more issues with internal server response errors etc., in addition to errors in the returned answers, which lead to the turn time going up. For pure inference, when it provides the correct response, Groq was marginally faster than OpenAI.</p><h2 id="af42" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Trending Towards Less Mistakes and More Robust Output</h2><p id="2468" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">As we could see from the model improved model generations, the new LLMs are generating far less erroneous output than the older models. This is important as we continue to build data products with the models and integrate them into pipelines. There will likely still be the need for some post-prompt error handling but less than before.</p><h2 id="fd6c" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Parsing Responses</h2><p id="0e9d" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">A key issue in interacting with the LLMs is to parse the output that they produce. OpenAI recently <a class="af sb" href="https://openai.com/index/introducing-structured-outputs-in-the-api/" rel="noopener ugc nofollow" target="_blank">revealed </a>that GPT-4o, can <em class="pf">“now reliably adhere to developer-supplied JSON Schemas.” </em>So, this is of course amazing news, but many of the other models, such as llama 3.1 70B still struggled to consistently return JSON output in the right format.</p><p id="aad8" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">The solution to the parsing problem ended up packing the output into special text strings such as <code class="cx qj qk ql qm b">||| output 1 |||</code>, <code class="cx qj qk ql qm b">+++ output 2+++</code> and then using regex to parse those output strings. I simply prompt the LLM to format the output using the special text strings, and also provide examples of correctly formatted output. I guess because of how the LLMs are inherently sequence based this type of formatting is easier out of the box than for example asking it to return a complex JSON object. For a concrete example, see below:</p><pre class="ms mt mu mv mw qn qm qo bp qp bb bk"><span id="dec6" class="qq nj fq qm b bg qr qs l qt qu">'''Your response should be in the following format:<br/>    Move:|||Territory, Number of troops|||<br/>    Reasoning:+++Reasoning for move+++p<br/><br/>For example:<br/>    Move:|||Brazil, 1|||<br/>    Reasoning:+++Brazil is a key territory in South America.+++'''<br/></span></pre><h2 id="8842" class="pt nj fq bf nk pu pv pw nn px py pz nq on qa qb qc or qd qe qf ov qg qh qi fw bk">Trying Out the Code and Running Your Own Experiments</h2><p id="dc18" class="pw-post-body-paragraph oe of fq og b gu oh oi oj gx ok ol om on oo op oq or os ot ou ov ow ox oy oz fj bk">I developed the package for the risk_game engine in addition to the modules and Jupyter notebook inside a docker container, and everything is self contained. So for anyone interested in trying out the simulator and run your own experiments all the code is available and should be very easy to run from the Github repo.</p><div class="qv qw qx qy qz ra"><a href="https://github.com/hcekne/risk-game.git?source=post_page-----43c868d83c3b--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="rb ab im"><div class="rc ab co cb rd re"><h2 class="bf gb ic z iu rf iw ix rg iz jb ga bk">GitHub — hcekne/risk-game</h2><div class="rh l"><h3 class="bf b ic z iu rf iw ix rg iz jb dx">Contribute to hcekne/risk-game development by creating an account on GitHub.</h3></div><div class="gr l"><p class="bf b dy z iu rf iw ix rg iz jb dx">github.com</p></div></div><div class="ri l"><div class="rj l rk rl rm ri rn lx ra"/></div></div></a></div><p id="c18b" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Clone the repo and follow the instructions in the <code class="cx qj qk ql qm b">README.md</code> file. It should be pretty straightforward. The only thing you need to change to get everything running on your own machine is the <code class="cx qj qk ql qm b">.env_example</code> file. You need to put in your own API keys for the relevant LLM providers and change the name of the file to <code class="cx qj qk ql qm b">.env</code>.</p><p id="6530" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk">Then run the <code class="cx qj qk ql qm b">start_container.sh</code> script. This is just a bash script that initializes some environment variables and runs a docker compose .yml file. This file configures the appropriate settings for the docker container, and everything should start up by itself. (The reason we feed these environment variables into the docker container is because when doing in-container development you can run into an issue with file permissions on the files that are created in the container. This is fixed if we change the container user to your user, then the files created by the container will have the same owner as the user on the machine running the container.)</p></div></div></div><div class="ab cb pg ph pi pj" role="separator"><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm pn"/><span class="pk by bm pl pm"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="24e0" class="pw-post-body-paragraph oe of fq og b gu pa oi oj gx pb ol om on pc op oq or pd ot ou ov pe ox oy oz fj bk"><em class="pf">If you enjoyed reading this article and would like to access more content from me please feel free to connect with me on LinkedIn at </em><a class="af sb" href="https://www.linkedin.com/in/hans-christian-ekne-1760a259/" rel="noopener ugc nofollow" target="_blank"><em class="pf">https://www.linkedin.com/in/hans-christian-ekne-1760a259/</em></a><em class="pf"> or visit my webpage at </em><a class="af sb" href="https://www.ekneconsulting.com/" rel="noopener ugc nofollow" target="_blank"><em class="pf">https://www.ekneconsulting.com/</em></a><em class="pf"> to explore some of the services I offer. Don’t hesitate to reach out via email at hce@ekneconsulting.com</em></p></div></div></div></div>    
</body>
</html>