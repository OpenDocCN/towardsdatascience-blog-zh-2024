- en: From Vision Transformers to Masked Autoencoders in 5 Minutes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28](https://towardsdatascience.com/from-vision-transformers-to-masked-autoencoders-in-5-minutes-cfd2fa1664ac?source=collection_archive---------3-----------------------#2024-06-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Straightforward Guide on How NLP Tasks Generalize to Computer Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Essam
    Wisam](../Images/6320ce88ba2e5d56d70ce3e0f97ceb1d.png)](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    [Essam Wisam](https://essamwissam.medium.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cfd2fa1664ac--------------------------------)
    ·7 min read·Jun 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Nearly all natural language processing tasks, from language modeling and masked
    word prediction to translation and question-answering, were revolutionized with
    the debut of the transformer architecture in 2017\. It should come as no surprise,
    that within just 2–3 years, transformers were also employed in computer vision
    tasks where they also showed outstanding results. In this story, we explore two
    fundamental architectures that enabled transformers to break into the world of
    computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: · [The Vision Transformer](#c206)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Key Idea](#c302)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Operation](#98e9)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Hybrid Architecture](#ae5c)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Loss of Structure](#af38)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Results](#6ed9)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Self-supervised Learning by Masking](#7607)
  prefs: []
  type: TYPE_NORMAL
- en: · [Masked Autoencoder Vision Transformer](#e126)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Key Idea](#2db8)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Architecture](#3393)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Final Remark and Example](#f78c)
  prefs: []
  type: TYPE_NORMAL
- en: The Vision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/f02bd048ad2f7f140d55716a7d214624.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale”'
  prefs: []
  type: TYPE_NORMAL
- en: Key Idea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The vision transformer is simply meant to generalize the [standard transformer](https://medium.com/@essamwissam/a-systematic-explanation-of-transformers-db82e039b913)
    architecture to process and learn from image input. There is a key idea about
    the architecture that the authors were transparent enough to highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: “Inspired by the Transformer scaling successes in NLP, we experiment with applying
    a standard Transformer directly to images, with the fewest possible modifications.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Operation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s valid to take “*fewest possible modifications”* quite literally because
    they pretty much make zero modifications. What they actuall modify is input structure:'
  prefs: []
  type: TYPE_NORMAL
- en: In NLP, the transformer encoder takes a ***sequence of one-hot vectors*** (or
    equivalently token indices) that ***represent the input sentence/paragraph***
    and returns a sequence of contextual embedding vectors that could be used for
    a further tasks (e.g., classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To generalize the CV, the vision transformer takes a ***sequence of patch vectors***
    that represent the ***input image*** and returns a sequence of contextual embedding
    vectors that could be used for a further tasks (e.g., classification)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In particular, suppose the input images have dimensions (n,n,3) to pass this
    as an input to the transformer, what the vision transformer does is:'
  prefs: []
  type: TYPE_NORMAL
- en: Divides it into k² patches for some k (e.g., k=3) as in the figure above.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now each patch will be (n/k,n/k,3) the next step is to flatten each patch into
    a vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The patch vector will be of dimensionality 3*(n/k)*(n/k). For example, if the
    image is (900,900,3) and we use k=3 then a patch vector will have dimensionality
    300*300*3 representing the pixel values in the flattened patch. In the paper,
    authors use k=16\. Hence, the paper’s name “An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale” instead of feeding a one-hot vector representing
    the word they represent a vector pixels representing a patch of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The rest of the operations remains as in the original transformer encoder:**'
  prefs: []
  type: TYPE_NORMAL
- en: These patch vectors pass by a trainable embedding layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positional embeddings are added to each vector to maintain a sense of spatial
    information in the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is *num_patches* encoder representations (one for each patch) which
    could be used for classification on the patch or image level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More often (and as in the paper), a CLS token is prepended the representation
    corresponding to that is used to make a prediction over the whole image (similar
    to BERT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**How about the transformer decoder?**'
  prefs: []
  type: TYPE_NORMAL
- en: Well, remember it’s just like the transformer encoder; the difference is that
    it uses masked self-attention instead of self-attention (but the same input signature
    remains). In any case, you should expect to seldom use a decoder-only transformer
    architecture because simply predicting the next patch may not a task of great
    interest.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authors also mentions that it’s possible to start with a CNN feature map instead
    of the image itself to form a hybrid architecture (CNN feeding output to vision
    transformer). In this case, we think of the input as a generic (n,n,p) feature
    map and a patch vector will have dimensions (n/k)*(n/k)*p.
  prefs: []
  type: TYPE_NORMAL
- en: Loss of Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may cross your mind that this architecture shouldn’t be so good because it
    treated the image as a linear structure when it isn’t. The author try to depict
    that this is intentional by mentioning
  prefs: []
  type: TYPE_NORMAL
- en: “The two-dimensional neighborhood structure is used very sparingly…position
    embeddings at initialization time carry no information about the 2D positions
    of the patches and all spatial relations between the patches have to be learned
    from scratch”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will see that the transformer is able to learn this as evidenced by its good
    performance in their experiments and more importantly the architecture in the
    next paper.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main verdict from the results is that vision transformers tend to not outperform
    CNN-based models for small datasets but approach or outperofrm CNN-based models
    for larger datasets and either way require significantly less compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e699867cbb8ab95893959a4509a990f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table from Paper: “An Image is Worth 16x16 Words: Transformers for Image Recognition
    at Scale”.'
  prefs: []
  type: TYPE_NORMAL
- en: Here we see that for the JFT-300M dataset (which has 300M images), the ViT models
    pre-trained on the dataset outperform ResNet-based baselines while taking substantially
    less computational resources to pre-train. As can be seen the larget vision transformer
    they used (ViT-Huge with 632M parameters and k=16) used about 25% of the compute
    used for the ResNet based model and still outperformed it. The performance doesn’t
    even downgrade that much with ViT-Large using only <6.8% of the compute.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, others also expose results where the ResNet performed significantly
    better when trained on ImageNet-1K which has just 1.3M images.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised Learning by Masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Authors performed a preliminary exploration on masked patch prediction for self-supervision,
    mimicking the masked language modeling task used in BERT (i.e., masking out patches
    and attempting to predict them).
  prefs: []
  type: TYPE_NORMAL
- en: “We employ the masked patch prediction objective for preliminary self-supervision
    experiments. To do so we corrupt 50% of patch embeddings by either replacing their
    embeddings with a learnable [mask] embedding (80%), a random other patch embedding
    (10%) or just keeping them as is (10%).”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: With self-supervised pre-training, their smaller ViT-Base/16 model achieves
    79.9% accuracy on ImageNet, a significant improvement of 2% to training from scratch.
    But still 4% behind supervised pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Masked Autoencoder Vision Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d7cf73cf4ebc0d818ec829d32a024a11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image from Paper: Masked Autoencoders Are Scalable Vision Learners'
  prefs: []
  type: TYPE_NORMAL
- en: Key Idea
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we have seen from the vision transformer paper, the gains from pretraining
    by masking patches in input images were not as significant as in ordinary NLP
    where masked pretraining can lead to state-of-the-art results in some fine-tuning
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes a vision transformer architecture involving an encoder and
    a decoder that when pretrained with masking results in significant improvements
    over the base vision transformer model (as much as 6% improvement compared to
    training a base size vision transformer in a supervised fashion).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/027d64201111d63dce657b96d4ad9c3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image from Paper: Masked Autoencoders Are Scalable Vision Learners'
  prefs: []
  type: TYPE_NORMAL
- en: This is some sample (input, output, true labels). It’s an autoencoder in the
    sense that it tried to reconstruct the input while filling the missing patches.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Their **encoder** is simply the ordinary vision transformer encoder we explained
    earlier. In training and inference, it takes only the “observed” patches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meanwhile, their **decoder** is also simply the ordinary vision transformer
    encoder but it takes:'
  prefs: []
  type: TYPE_NORMAL
- en: Masked token vectors for the missing patches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoder output vectors for the known patches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So for an image [ [ A, B, X], [C, X, X], [X, D, E]] where X denotes a missing
    patch, the decoder will take the sequence of patch vectors [Enc(A), Enc(B), Vec(X),
    Vec(X), Vec(X), Enc(D), Enc(E)]. Enc returns the encoder output vector given the
    patch vector and X is a vector to represent missing token.
  prefs: []
  type: TYPE_NORMAL
- en: The **last layer** in the decoder is a linear layer that maps the contextual
    embeddings (produced by the vision transformer encoder in the decoder) to a vector
    of length equal to the patch size. The loss function is mean squared error which
    squares the difference between the original patch vector and the predicted one
    by this layer. In the loss function, we only look at the decoder predictions due
    to masked tokens and ignore the ones corresponding the present ones (i.e., Dec(A),.
    Dec(B), Dec(C), etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Final Remark and Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It may be surprising that the authors suggest masking about 75% of the patches
    in the images; BERT would mask only about 15% of the words. They justify like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: Images,are natural signals with heavy spatial redundancy — e.g., a missing patch
    can be recovered from neighboring patches with little high-level understanding
    of parts, objects, and scenes. To overcome this difference and encourage learning
    useful features, we mask a very high portion of random patches.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Want to try it out yourself? Checkout this [demo notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb)
    by NielsRogge.
  prefs: []
  type: TYPE_NORMAL
- en: This is all for this story. We went through a journey to understand how fundamental
    transformer models generalize to the computer vision world. Hope you have found
    it clear, insighful and worth your time.
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Dosovitskiy, A. *et al.* (2021) *An image is worth 16x16 words: Transformers
    for image recognition at scale*, *arXiv.org*. Available at: [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)
    (Accessed: 28 June 2024).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] He, K. *et al.* (2021) *Masked autoencoders are scalable vision learners*,
    *arXiv.org*. Available at: [https://arxiv.org/abs/2111.06377](https://arxiv.org/abs/2111.06377)
    (Accessed: 28 June 2024).'
  prefs: []
  type: TYPE_NORMAL
