- en: Understanding “You Only Cache Once”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04](https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This blog post will go in detail on the “You Only Cache Once: Decoder-Decoder
    Architectures for Language Models” Paper and its findings'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    ·9 min read·Jun 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d9aa62c1a8d8240cbcec5fc97454d24.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — generated by Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: As the Large Language Model (LLM) space becomes more mature, there are increasing
    efforts to take the current performance and make it more cost-effective. This
    has been done by creating custom hardware for them to run on (ie Language Processing
    Units by Groq), by optimizing the low level software that they interact with (think
    Apple’s MLX Library or NVIDIA’s CUDA Library), and by becoming more deliberate
    with the calculations the high-level software does.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “You Only Cache Once: Decoder-Decoder Architectures for Language Models”
    [paper](https://arxiv.org/pdf/2405.05254) presents a new architecture for LLMs
    that improves performance by using memory-efficient architecture. They call this
    YOCO.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Key-Value (KV) Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the changes made here, we first need to discuss the Key-Value
    Cache. Inside of the transformer we have 3 vectors that are critical for attention
    to work — key, value, and query. From a high level, attention is how we pass along
    critical information about the previous tokens to the current token so that it
    can predict the next token. In the example of self-attention with one head, we
    multiply the query vector on the current token with the key vectors from the previous
    tokens and then normalize the resulting matrix (the resulting matrix we call the
    attention pattern). We now multiply the value vectors with the attention pattern
    to get the updates to each token. This data is then added to the current tokens
    embedding so that it now has the context to determine what comes next.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b4d7d5e25ed59a10edd593366d51e5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 1 from “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
  prefs: []
  type: TYPE_NORMAL
- en: We create the attention pattern for every single new token we create, so while
    the queries tend to change, the keys and the values are constant. Consequently,
    the current architectures try to reduce compute time by caching the key and value
    vectors as they are generated by each successive round of attention. This cache
    is called the Key-Value Cache.
  prefs: []
  type: TYPE_NORMAL
- en: While architectures like encoder-only and encoder-decoder transformer models
    have had success, the authors posit that the autoregression shown above, and the
    speed it allows its models, is the reason why decoder-only models are the most
    commonly used today.
  prefs: []
  type: TYPE_NORMAL
- en: YOCO Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the YOCO architecture, we have to start out by understanding how
    it sets out its layers.
  prefs: []
  type: TYPE_NORMAL
- en: For one half of the layer, we use one type of attention to generate the vectors
    needed to fill the KV Cache. Once it crosses into the second half, it will use
    the KV Cache exclusively for the key and value vectors respectively, now generating
    the output token embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e1dd7de0c60a491693f301dc62184e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: This new architecture requires two types of attention — efficient self-attention
    and cross-attention. We’ll go into each below.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Self-Attention and Self-Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Efficient Self-Attention (ESA) is designed to achieve a constant inference memory.
    Put differently we want the cache complexity to rely not on the input length but
    on the number of layers in our block. In the below equation, the authors abstracted
    ESA, but the remainder of the self-decoder is consistent as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5135c5b3a0cf20520cfe699214e12fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the equation step by step. X^l is our token embedding and
    Y^l is an intermediary variable used to generate the next token embedding X^l+1\.
    In the equation, ESA is Efficient Self-Attention, LN is the layer normalization
    function — which here was always Root Mean Square Norm (`RMSNorm` ), and finally
    `SwiGLU`. `SwiGLU` is defined by the below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0421b154f2da20890c1306a1ca2d1672.png)'
  prefs: []
  type: TYPE_IMG
- en: SwiGLU Definition [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: Here `swish = x*sigmoid (Wg * x)`, where Wg is a trainable parameter. We then
    find the element-wise product (Hadamard Product) between that result and X*W1
    before then multiplying that whole product by W2\. The goal with `SwiGLU` is to
    get an activation function that will conditionally pass through different amounts
    of information through the layer to the next token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada8d1166540185146fe5cf619465808.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Element-Wise Product (Hadamard product) from [“Hadamard product (matrices)”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
  prefs: []
  type: TYPE_NORMAL
- en: Now that we see how the self-decoder works, let’s go into the two ways the authors
    considered implementing ESA.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Retention ESA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, they considered what is called Gated Retention. Retention and self-attention
    are admittedly very similar, with the authors of the “Retentive Network: A Successor
    to Transformer for Large Language Models” paper saying that the key difference
    lies in the activation function — retention removes softmax allowing for a recurrent
    formulation. They use this recurrent formulation along with the parallelizability
    to drive memory efficiencies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To dive into the mathematical details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9be5e43e0572690fdb654cdff80e702.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 4 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: We have our typical matrices of Q, K, and V — each of which are multiplied by
    the learnable weights associated with each matrix. We then find the Hadamard product
    between the weighted matrices and the scalar Θ. The goal in using Θ is to create
    exponential decay, while we then use the D matrix to help with casual masking
    (stopping future tokens from interacting with current tokens) and activation.
  prefs: []
  type: TYPE_NORMAL
- en: Gated Retention is distinct from retention via the γ value. Here the matrix
    Wγ is used to allow our ESA to be data-driven.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding Window ESA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sliding Window ESA introduces the idea of limiting how many tokens the attention
    window should pay attention to. While in regular self-attention all previous tokens
    are attended to in some way (even if their value is 0), in sliding window ESA,
    we choose some constant value C that limits the size of these matrices. This means
    that during inference time the KV cache can be reduced to a constant complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'To again dive into the math:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a6e35a9246a498b806dd8b08d6cd899.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 8 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: We have our matrices being scaled by their corresponding weights. Next, we compute
    the head similar to how multi-head attention is computed, where B acts both as
    a causal map and also to make sure only the tokens C back are attended to.
  prefs: []
  type: TYPE_NORMAL
- en: Whether using sliding window or gated retention, the goal of the first half
    of the layer is to generate the KV cache which will then be used in the second
    half to generate the output tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Now we will see exactly how the global KV cache helps speed up inference.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-Attention and the Cross-Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once moving to the second half of the layer, we first create the global KV cache.
    The cache is made up of K-hat and V-hat, which we create by running a layer normalization
    function on the tokens we get out of the first half of the layer and then multiply
    these by their corresponding weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5adb7dc38e3407262f5210015618eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 2 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: 'With the global KV cache created, we now leverage a different decoder and attention
    to generate the next tokens. To dive into the math below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d53bbce94bfb87c6e511e9e1fbe6457c.png)'
  prefs: []
  type: TYPE_IMG
- en: Equations 3 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: We generate our query matrix by taking the token embedding and running the same
    normalization and then matrix multiplication on this as we did on K-hat and V-hat,
    the difference being we run this on every token that comes through, not just on
    the ones from the end of the first half of the layer. We then run cross attention
    on the three matrices, and use normalization and `SwiGLU` from before to determine
    what the next token should be. This X^l+1 is the token that is then predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Cross attention is very similar to self-attention, the twist here is that cross-attention
    leverages embeddings from different corpuses.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s begin by analyzing the memory complexity between Transformers and YOCOs.
    For the Transformer, we have to keep in memory the weights for the input sequence
    (N) as well as the weights for each layer (L) and then do so for every hidden
    dimension (D). This means we are storing memory on the order of L * N * D.
  prefs: []
  type: TYPE_NORMAL
- en: By comparison, the split nature of YOCO means that we have 2 situations to analyze
    to find out the big O memory complexity. When we run through the first half of
    the layer, we are doing efficient self-attention, which we know wants a constant
    cache size (either by sliding window attention or gated retention). This makes
    its big O dependent on the weights for each layer (L) and the number of hidden
    dimensions in the first half of the layer(D). The second half uses cross-attention
    which keeps in memory the weights for the input sequence (N), but then uses the
    constant global cache, making it not change from the big O memory analysis point
    of view. Thus, the only other dependent piece is the number of hidden dimensions
    in the second half of the layer(D), which we will say is effectively the same.
    Thus, we are storing memory on the order of L * D + N * D = (N + L) * D
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c2735538a64932e3077697dc8c71297.png)'
  prefs: []
  type: TYPE_IMG
- en: Table 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: The authors note that when the input size is significantly bigger than the number
    of layers, the big O calculation approximates O(N), which is why they call their
    model You Only Cache Once.
  prefs: []
  type: TYPE_NORMAL
- en: Inference Advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During inference, we have two major stages: prefilling (sometimes called initiation)
    and then generation (sometimes call decoding). During prefilling, we are taking
    the prompt in and create all of the necessary computations to generate the first
    output. This can start with loading model weights into the GPU memory and then
    end with the first token being output. Once that first output is created, the
    autoregressive nature of transformers means that the lion-share of the calculations
    needed to create the entire response has already been completed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/efbf2cbbba3c73349078ecbb253256f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the prefilling stage, both the transfomer and YOCO model will
    load in the weights to GPU memory in the same time, but YOCO has two major advantages
    after that. First, because YOCO’s self-decoder can run in parallel, it can run
    significantly faster than the regular self-attention without parallelization.
    Second, as only the first half generates the global KV cache, only half of the
    layer needs to run during prefilling, significantly reducing the number of computations.
    Both of these result in YOCO’s prefilling stage being much faster than a transformers
    (roughly 30x so!)
  prefs: []
  type: TYPE_NORMAL
- en: During the generation stage, we do not have to have as many changes of GPU memory
    with YOCO as we would with a transformer for the reasons shown above. This is
    a major contributor to the throughput that YOCO can achieve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db9330aab237f4621f1a0c1ef7926b47.png)'
  prefs: []
  type: TYPE_IMG
- en: Part of Figure 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  prefs: []
  type: TYPE_NORMAL
- en: All of these metrics highlight that the architecture change alone can introduce
    significant efficiencies for these models.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With new architectures, there comes a bit of a dilemma. After having spent billions
    of dollars training models with older architectures, companies rightfully wonder
    if it is worth spending billions more on a newer architecture that may itself
    be outmoded soon.
  prefs: []
  type: TYPE_NORMAL
- en: One possible solution to this dilemma is transfer learning. The idea here is
    to put noise into the trained model and then use the output given to then backpropagate
    on the new model. The idea here is that you don’t need to worry about generating
    huge amounts of novel data and potentially the number of epochs you have to train
    for is also significantly reduced. This idea has not been perfected yet, so it
    remains to be seen the role it will play in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, as businesses become more invested in these architectures the
    potential for newer architectures that improve cost will only increase. Time will
    tell how quickly the industry moves to adopt them.
  prefs: []
  type: TYPE_NORMAL
- en: For those who are building apps that allow for a seamless transition between
    models, you can look at the major strives made in throughput and latency by YOCO
    and have hope that the major bottlenecks your app is having may soon be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: It’s an exciting time to be building.
  prefs: []
  type: TYPE_NORMAL
- en: '*With special thanks to* [*Christopher Taylor*](https://www.linkedin.com/in/christopher-taylor-1a539a31)
    *for his feedback on this blog post.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] Sun, Y., et al. [“You Only Cache Once: Decoder-Decoder Architectures for
    Language Models”](https://arxiv.org/pdf/2405.05254) (2024), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Sun, Y., et al. [“Retentive Network: A Successor to Transformer for Large
    Language Models”](https://arxiv.org/pdf/2307.08621) (2023), arXiv'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Wikimedia Foundation, et al. [“Hadamard product (matrices)”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    (2024), Wikipedia'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Sanderson, G. et al., [“Attention in transformers, visually explained |
    Chapter 6, Deep Learning”](https://youtu.be/eMlx5fFNoYc?feature=shared) (2024),
    YouTube'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] A. Vaswani, et al., “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
  prefs: []
  type: TYPE_NORMAL
