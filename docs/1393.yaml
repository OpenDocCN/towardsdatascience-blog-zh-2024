- en: Understanding “You Only Cache Once”
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解“你只需缓存一次”
- en: 原文：[https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04](https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04](https://towardsdatascience.com/understanding-you-only-cache-once-89bf29f21c1d?source=collection_archive---------4-----------------------#2024-06-04)
- en: 'This blog post will go in detail on the “You Only Cache Once: Decoder-Decoder
    Architectures for Language Models” Paper and its findings'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文将详细探讨《你只需缓存一次：面向语言模型的解码器-解码器架构》论文及其发现。
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--89bf29f21c1d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    ·9 min read·Jun 4, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--89bf29f21c1d--------------------------------)
    ·阅读时间9分钟·2024年6月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4d9aa62c1a8d8240cbcec5fc97454d24.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d9aa62c1a8d8240cbcec5fc97454d24.png)'
- en: Image by Author — generated by Stable Diffusion
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图片 — 由Stable Diffusion生成
- en: As the Large Language Model (LLM) space becomes more mature, there are increasing
    efforts to take the current performance and make it more cost-effective. This
    has been done by creating custom hardware for them to run on (ie Language Processing
    Units by Groq), by optimizing the low level software that they interact with (think
    Apple’s MLX Library or NVIDIA’s CUDA Library), and by becoming more deliberate
    with the calculations the high-level software does.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLM）领域的逐渐成熟，越来越多的努力致力于提升当前性能并使其更具成本效益。这一目标的实现方式包括为其创建定制硬件（例如Groq的语言处理单元），优化它们所交互的低层软件（例如苹果的MLX库或NVIDIA的CUDA库），以及在高层软件的计算过程中更加精细化。
- en: 'The “You Only Cache Once: Decoder-Decoder Architectures for Language Models”
    [paper](https://arxiv.org/pdf/2405.05254) presents a new architecture for LLMs
    that improves performance by using memory-efficient architecture. They call this
    YOCO.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 《你只需缓存一次：面向语言模型的解码器-解码器架构》[论文](https://arxiv.org/pdf/2405.05254)提出了一种新的大语言模型架构，通过使用内存高效的架构来提高性能。他们称之为YOCO。
- en: Let’s dive in!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下！
- en: Key-Value (KV) Cache
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 键值（KV）缓存
- en: To understand the changes made here, we first need to discuss the Key-Value
    Cache. Inside of the transformer we have 3 vectors that are critical for attention
    to work — key, value, and query. From a high level, attention is how we pass along
    critical information about the previous tokens to the current token so that it
    can predict the next token. In the example of self-attention with one head, we
    multiply the query vector on the current token with the key vectors from the previous
    tokens and then normalize the resulting matrix (the resulting matrix we call the
    attention pattern). We now multiply the value vectors with the attention pattern
    to get the updates to each token. This data is then added to the current tokens
    embedding so that it now has the context to determine what comes next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这里所做的变化，我们首先需要讨论键值缓存。在变换器（transformer）中，我们有三个对注意力机制至关重要的向量——键（key）、值（value）和查询（query）。从高层次看，注意力机制是我们如何将关于前一个标记的关键信息传递给当前标记，以便它可以预测下一个标记。在单头自注意力的示例中，我们将当前标记的查询向量与前一个标记的键向量相乘，然后对结果矩阵进行归一化（我们称这个结果矩阵为注意力模式）。接着，我们将值向量与注意力模式相乘，以获得对每个标记的更新。然后，这些数据被加到当前标记的嵌入中，从而使其拥有了判断下一个标记所需的上下文。
- en: '![](../Images/5b4d7d5e25ed59a10edd593366d51e5c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b4d7d5e25ed59a10edd593366d51e5c.png)'
- en: Equation 1 from “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 方程1来自[“Attention Is All You Need”](https://arxiv.org/pdf/1706.03762.pdf)
- en: We create the attention pattern for every single new token we create, so while
    the queries tend to change, the keys and the values are constant. Consequently,
    the current architectures try to reduce compute time by caching the key and value
    vectors as they are generated by each successive round of attention. This cache
    is called the Key-Value Cache.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每一个新生成的token创建注意力模式，因此虽然查询会变化，键和值是恒定的。因此，当前的架构尝试通过缓存每次生成的键和值向量来减少计算时间。这种缓存称为键值缓存（Key-Value
    Cache）。
- en: While architectures like encoder-only and encoder-decoder transformer models
    have had success, the authors posit that the autoregression shown above, and the
    speed it allows its models, is the reason why decoder-only models are the most
    commonly used today.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像编码器单一模型和编码器-解码器模型这样的架构已经取得了一定成功，但作者认为上面所示的自回归机制及其带来的速度，是为什么如今解码器单一模型最常用的原因。
- en: YOCO Architecture
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YOCO架构
- en: To understand the YOCO architecture, we have to start out by understanding how
    it sets out its layers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解YOCO架构，我们必须从理解它如何设置其层开始。
- en: For one half of the layer, we use one type of attention to generate the vectors
    needed to fill the KV Cache. Once it crosses into the second half, it will use
    the KV Cache exclusively for the key and value vectors respectively, now generating
    the output token embeddings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一层的前半部分，我们使用一种类型的注意力机制来生成填充KV缓存所需的向量。一旦进入第二部分，它将分别使用KV缓存中的键和值向量，生成输出的token嵌入。
- en: '![](../Images/5e1dd7de0c60a491693f301dc62184e1.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1dd7de0c60a491693f301dc62184e1.png)'
- en: Figure 2 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2[来自论文](https://arxiv.org/pdf/2405.05254)
- en: This new architecture requires two types of attention — efficient self-attention
    and cross-attention. We’ll go into each below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新架构需要两种类型的注意力机制——高效自注意力和交叉注意力。我们将分别介绍这两种机制。
- en: Efficient Self-Attention and Self-Decoder
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效自注意力和自解码器
- en: Efficient Self-Attention (ESA) is designed to achieve a constant inference memory.
    Put differently we want the cache complexity to rely not on the input length but
    on the number of layers in our block. In the below equation, the authors abstracted
    ESA, but the remainder of the self-decoder is consistent as shown below.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 高效自注意力（ESA）旨在实现恒定的推理内存。换句话说，我们希望缓存的复杂度不依赖于输入长度，而是依赖于我们模块中的层数。在下面的方程中，作者抽象出了ESA，但其余的自解码器部分保持一致，如下所示。
- en: '![](../Images/a5135c5b3a0cf20520cfe699214e12fd.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5135c5b3a0cf20520cfe699214e12fd.png)'
- en: Equation 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 方程1[来自论文](https://arxiv.org/pdf/2405.05254)
- en: 'Let’s go through the equation step by step. X^l is our token embedding and
    Y^l is an intermediary variable used to generate the next token embedding X^l+1\.
    In the equation, ESA is Efficient Self-Attention, LN is the layer normalization
    function — which here was always Root Mean Square Norm (`RMSNorm` ), and finally
    `SwiGLU`. `SwiGLU` is defined by the below:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分析这个方程。X^l是我们的token嵌入，Y^l是用于生成下一个token嵌入X^l+1的中间变量。在方程中，ESA表示高效自注意力，LN是层归一化函数——这里始终使用的是根均方根归一化（`RMSNorm`），最后是`SwiGLU`。`SwiGLU`定义如下：
- en: '![](../Images/0421b154f2da20890c1306a1ca2d1672.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0421b154f2da20890c1306a1ca2d1672.png)'
- en: SwiGLU Definition [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: SwiGLU定义[来自论文](https://arxiv.org/pdf/2405.05254)
- en: Here `swish = x*sigmoid (Wg * x)`, where Wg is a trainable parameter. We then
    find the element-wise product (Hadamard Product) between that result and X*W1
    before then multiplying that whole product by W2\. The goal with `SwiGLU` is to
    get an activation function that will conditionally pass through different amounts
    of information through the layer to the next token.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里`swish = x*sigmoid (Wg * x)`，其中Wg是一个可训练的参数。然后，我们找到这个结果与X*W1之间的元素按位乘积（Hadamard乘积），再将整个乘积与W2相乘。`SwiGLU`的目标是获得一个激活函数，它能有条件地传递不同数量的信息，通过层传递到下一个token。
- en: '![](../Images/ada8d1166540185146fe5cf619465808.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ada8d1166540185146fe5cf619465808.png)'
- en: Example of Element-Wise Product (Hadamard product) from [“Hadamard product (matrices)”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[“Hadamard乘积（矩阵）”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))的元素按位乘积（Hadamard乘积）示例
- en: Now that we see how the self-decoder works, let’s go into the two ways the authors
    considered implementing ESA.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了自解码器是如何工作的，让我们来看一下作者考虑实现 ESA 的两种方式。
- en: Gated Retention ESA
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 门控保持 ESA
- en: 'First, they considered what is called Gated Retention. Retention and self-attention
    are admittedly very similar, with the authors of the “Retentive Network: A Successor
    to Transformer for Large Language Models” paper saying that the key difference
    lies in the activation function — retention removes softmax allowing for a recurrent
    formulation. They use this recurrent formulation along with the parallelizability
    to drive memory efficiencies.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，他们考虑了所谓的门控保持（Gated Retention）。保持和自注意力的确非常相似，"Retentive Network: A Successor
    to Transformer for Large Language Models" 论文的作者指出，主要区别在于激活函数——保持移除了 softmax，从而实现了递归公式。他们使用这种递归公式及其并行性来提升内存效率。'
- en: 'To dive into the mathematical details:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深入数学细节：
- en: '![](../Images/b9be5e43e0572690fdb654cdff80e702.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9be5e43e0572690fdb654cdff80e702.png)'
- en: Equation 4 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: We have our typical matrices of Q, K, and V — each of which are multiplied by
    the learnable weights associated with each matrix. We then find the Hadamard product
    between the weighted matrices and the scalar Θ. The goal in using Θ is to create
    exponential decay, while we then use the D matrix to help with casual masking
    (stopping future tokens from interacting with current tokens) and activation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有典型的 Q、K 和 V 矩阵——每个矩阵都与其相应的可学习权重相乘。然后我们计算加权矩阵与标量 Θ 的 Hadamard 乘积。使用 Θ 的目标是创建指数衰减，然后我们使用
    D 矩阵来帮助进行因果掩蔽（防止未来的标记与当前标记交互）和激活。
- en: Gated Retention is distinct from retention via the γ value. Here the matrix
    Wγ is used to allow our ESA to be data-driven.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 门控保持与通过 γ 值实现的保持是不同的。在这里，矩阵 Wγ 用来使我们的 ESA 数据驱动。
- en: Sliding Window ESA
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滑动窗口 ESA
- en: Sliding Window ESA introduces the idea of limiting how many tokens the attention
    window should pay attention to. While in regular self-attention all previous tokens
    are attended to in some way (even if their value is 0), in sliding window ESA,
    we choose some constant value C that limits the size of these matrices. This means
    that during inference time the KV cache can be reduced to a constant complexity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口 ESA 引入了限制注意力窗口应关注多少标记的概念。在常规自注意力中，所有先前的标记都会以某种方式被关注（即使它们的值为 0），而在滑动窗口 ESA
    中，我们选择一个常数值 C 来限制这些矩阵的大小。这意味着在推理时，KV 缓存的复杂度可以保持恒定。
- en: 'To again dive into the math:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次深入探讨数学内容：
- en: '![](../Images/5a6e35a9246a498b806dd8b08d6cd899.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a6e35a9246a498b806dd8b08d6cd899.png)'
- en: Equation 8 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 8 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: We have our matrices being scaled by their corresponding weights. Next, we compute
    the head similar to how multi-head attention is computed, where B acts both as
    a causal map and also to make sure only the tokens C back are attended to.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将矩阵按其相应的权重进行缩放。接下来，我们计算注意力头，类似于多头注意力的计算方式，其中 B 既充当因果映射，又确保只有 C 后面的标记被关注。
- en: Whether using sliding window or gated retention, the goal of the first half
    of the layer is to generate the KV cache which will then be used in the second
    half to generate the output tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是使用滑动窗口还是门控保持，层的前半部分目标都是生成 KV 缓存，然后在后半部分使用该缓存来生成输出标记。
- en: Now we will see exactly how the global KV cache helps speed up inference.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将具体看看全局 KV 缓存如何加速推理过程。
- en: Cross-Attention and the Cross-Decoder
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉注意力和交叉解码器
- en: Once moving to the second half of the layer, we first create the global KV cache.
    The cache is made up of K-hat and V-hat, which we create by running a layer normalization
    function on the tokens we get out of the first half of the layer and then multiply
    these by their corresponding weight matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入层的后半部分，我们首先创建全局 KV 缓存。该缓存由 K-hat 和 V-hat 组成，我们通过对来自层前半部分的标记进行层归一化处理，然后将其与相应的权重矩阵相乘来创建这些值。
- en: '![](../Images/a5adb7dc38e3407262f5210015618eaf.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5adb7dc38e3407262f5210015618eaf.png)'
- en: Equation 2 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: 'With the global KV cache created, we now leverage a different decoder and attention
    to generate the next tokens. To dive into the math below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了全局 KV 缓存后，我们利用不同的解码器和注意力来生成下一个标记。接下来让我们深入探讨下面的数学内容：
- en: '![](../Images/d53bbce94bfb87c6e511e9e1fbe6457c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d53bbce94bfb87c6e511e9e1fbe6457c.png)'
- en: Equations 3 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 3 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: We generate our query matrix by taking the token embedding and running the same
    normalization and then matrix multiplication on this as we did on K-hat and V-hat,
    the difference being we run this on every token that comes through, not just on
    the ones from the end of the first half of the layer. We then run cross attention
    on the three matrices, and use normalization and `SwiGLU` from before to determine
    what the next token should be. This X^l+1 is the token that is then predicted.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过获取标记嵌入并执行相同的归一化操作，然后进行矩阵乘法，生成查询矩阵，这与之前在 K-hat 和 V-hat 上执行的操作相同，唯一的区别是我们对每个通过的标记都执行此操作，而不仅仅是对来自第一半层末尾的标记。然后我们在三个矩阵上运行交叉注意力，使用之前的归一化和
    `SwiGLU` 来确定下一个标记应该是什么。这个 X^l+1 就是接下来被预测的标记。
- en: Cross attention is very similar to self-attention, the twist here is that cross-attention
    leverages embeddings from different corpuses.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉注意力非常类似于自注意力，区别在于交叉注意力利用来自不同语料库的嵌入。
- en: Memory Advantages
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存优势
- en: Let’s begin by analyzing the memory complexity between Transformers and YOCOs.
    For the Transformer, we have to keep in memory the weights for the input sequence
    (N) as well as the weights for each layer (L) and then do so for every hidden
    dimension (D). This means we are storing memory on the order of L * N * D.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分析 Transformers 和 YOCOs 之间的内存复杂度开始。对于 Transformer，我们必须在内存中保留输入序列的权重（N），以及每一层的权重（L），然后对于每个隐藏维度（D）执行相同的操作。这意味着我们存储的内存量是
    L * N * D。
- en: By comparison, the split nature of YOCO means that we have 2 situations to analyze
    to find out the big O memory complexity. When we run through the first half of
    the layer, we are doing efficient self-attention, which we know wants a constant
    cache size (either by sliding window attention or gated retention). This makes
    its big O dependent on the weights for each layer (L) and the number of hidden
    dimensions in the first half of the layer(D). The second half uses cross-attention
    which keeps in memory the weights for the input sequence (N), but then uses the
    constant global cache, making it not change from the big O memory analysis point
    of view. Thus, the only other dependent piece is the number of hidden dimensions
    in the second half of the layer(D), which we will say is effectively the same.
    Thus, we are storing memory on the order of L * D + N * D = (N + L) * D
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，YOCO 的拆分特性意味着我们有两种情况需要分析，以找出大 O 内存复杂度。当我们运行到第一半层时，我们正在执行高效的自注意力机制，而我们知道自注意力需要一个常数缓存大小（无论是通过滑动窗口注意力还是门控保持）。这使得它的大
    O 依赖于每层的权重（L）和第一半层的隐藏维度数（D）。第二半层使用交叉注意力，它在内存中保持输入序列的权重（N），但随后使用常数全局缓存，因此从大 O 内存分析的角度来看，它不会改变。因此，唯一的其他依赖项是第二半层的隐藏维度数（D），我们可以认为它们是相同的。因此，我们存储的内存量为
    L * D + N * D = (N + L) * D
- en: '![](../Images/5c2735538a64932e3077697dc8c71297.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c2735538a64932e3077697dc8c71297.png)'
- en: Table 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: The authors note that when the input size is significantly bigger than the number
    of layers, the big O calculation approximates O(N), which is why they call their
    model You Only Cache Once.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，当输入大小显著大于层数时，大 O 计算近似为 O(N)，这也是他们称其模型为“You Only Cache Once”的原因。
- en: Inference Advantages
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理优势
- en: 'During inference, we have two major stages: prefilling (sometimes called initiation)
    and then generation (sometimes call decoding). During prefilling, we are taking
    the prompt in and create all of the necessary computations to generate the first
    output. This can start with loading model weights into the GPU memory and then
    end with the first token being output. Once that first output is created, the
    autoregressive nature of transformers means that the lion-share of the calculations
    needed to create the entire response has already been completed.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，我们有两个主要阶段：预填充（有时称为初始化）和生成（有时称为解码）。在预填充阶段，我们将提示输入并创建生成第一个输出所需的所有计算。这可以从将模型权重加载到
    GPU 内存中开始，然后以第一个标记的输出结束。一旦第一个输出创建完成，Transformer 的自回归特性意味着创建整个响应所需的大部分计算已经完成。
- en: '![](../Images/efbf2cbbba3c73349078ecbb253256f3.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efbf2cbbba3c73349078ecbb253256f3.png)'
- en: Figure 3 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 [来自论文](https://arxiv.org/pdf/2405.05254)
- en: Starting with the prefilling stage, both the transfomer and YOCO model will
    load in the weights to GPU memory in the same time, but YOCO has two major advantages
    after that. First, because YOCO’s self-decoder can run in parallel, it can run
    significantly faster than the regular self-attention without parallelization.
    Second, as only the first half generates the global KV cache, only half of the
    layer needs to run during prefilling, significantly reducing the number of computations.
    Both of these result in YOCO’s prefilling stage being much faster than a transformers
    (roughly 30x so!)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从预填充阶段开始，transformer和YOCO模型将同时加载权重到GPU内存中，但在此之后，YOCO有两个主要优势。首先，由于YOCO的自解码器可以并行运行，因此它的运行速度显著快于没有并行化的常规自注意力机制。其次，由于只有前半部分生成全局KV缓存，在预填充过程中只有一半的层需要运行，这大大减少了计算量。这两个因素使得YOCO的预填充阶段比transformer快得多（大约快30倍！）
- en: During the generation stage, we do not have to have as many changes of GPU memory
    with YOCO as we would with a transformer for the reasons shown above. This is
    a major contributor to the throughput that YOCO can achieve.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成阶段，由于上述原因，我们使用YOCO时不需要像使用transformer时那样频繁更改GPU内存。这是YOCO能够实现高吞吐量的一个重要原因。
- en: '![](../Images/db9330aab237f4621f1a0c1ef7926b47.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db9330aab237f4621f1a0c1ef7926b47.png)'
- en: Part of Figure 1 [from the paper](https://arxiv.org/pdf/2405.05254)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图1的部分[来自论文](https://arxiv.org/pdf/2405.05254)
- en: All of these metrics highlight that the architecture change alone can introduce
    significant efficiencies for these models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些指标都表明，单单架构的变化就能为这些模型带来显著的效率提升。
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: With new architectures, there comes a bit of a dilemma. After having spent billions
    of dollars training models with older architectures, companies rightfully wonder
    if it is worth spending billions more on a newer architecture that may itself
    be outmoded soon.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 随着新架构的出现，难免会面临一些困境。在已经花费数十亿美元训练旧架构的模型之后，公司理所当然地会质疑是否值得再花费数十亿资金来采用可能很快就会过时的新架构。
- en: One possible solution to this dilemma is transfer learning. The idea here is
    to put noise into the trained model and then use the output given to then backpropagate
    on the new model. The idea here is that you don’t need to worry about generating
    huge amounts of novel data and potentially the number of epochs you have to train
    for is also significantly reduced. This idea has not been perfected yet, so it
    remains to be seen the role it will play in the future.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一困境的一个可能方案是迁移学习。这里的想法是将噪声引入训练过的模型，然后利用给定的输出进行反向传播到新的模型。这里的想法是，你无需担心生成大量新的数据，并且你需要训练的epoch次数可能会显著减少。这个想法尚未完善，因此未来它将扮演怎样的角色还有待观察。
- en: Nevertheless, as businesses become more invested in these architectures the
    potential for newer architectures that improve cost will only increase. Time will
    tell how quickly the industry moves to adopt them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，随着企业对这些架构的投入不断增加，改善成本的更新架构的潜力也将日益增加。时间会告诉我们，行业采纳这些架构的速度有多快。
- en: For those who are building apps that allow for a seamless transition between
    models, you can look at the major strives made in throughput and latency by YOCO
    and have hope that the major bottlenecks your app is having may soon be resolved.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些正在构建能够实现模型之间无缝转换的应用程序的开发者，你可以参考YOCO在吞吐量和延迟方面所取得的重要进展，并希望你的应用程序面临的主要瓶颈很快会得到解决。
- en: It’s an exciting time to be building.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是构建的激动人心的时刻。
- en: '*With special thanks to* [*Christopher Taylor*](https://www.linkedin.com/in/christopher-taylor-1a539a31)
    *for his feedback on this blog post.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别感谢* [*Christopher Taylor*](https://www.linkedin.com/in/christopher-taylor-1a539a31)
    *对本文的反馈。*'
- en: '[1] Sun, Y., et al. [“You Only Cache Once: Decoder-Decoder Architectures for
    Language Models”](https://arxiv.org/pdf/2405.05254) (2024), arXiv'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sun, Y., et al. [“You Only Cache Once: Decoder-Decoder Architectures for
    Language Models”](https://arxiv.org/pdf/2405.05254) (2024), arXiv'
- en: '[2] Sun, Y., et al. [“Retentive Network: A Successor to Transformer for Large
    Language Models”](https://arxiv.org/pdf/2307.08621) (2023), arXiv'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sun, Y., et al. [“Retentive Network: A Successor to Transformer for Large
    Language Models”](https://arxiv.org/pdf/2307.08621) (2023), arXiv'
- en: '[3] Wikimedia Foundation, et al. [“Hadamard product (matrices)”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    (2024), Wikipedia'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wikimedia Foundation, et al. [“Hadamard product (matrices)”](https://en.wikipedia.org/wiki/Hadamard_product_(matrices))
    (2024), Wikipedia'
- en: '[4] Sanderson, G. et al., [“Attention in transformers, visually explained |
    Chapter 6, Deep Learning”](https://youtu.be/eMlx5fFNoYc?feature=shared) (2024),
    YouTube'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Sanderson, G. 等人, [“变压器中的注意力机制，视觉化解释 | 第6章，深度学习”](https://youtu.be/eMlx5fFNoYc?feature=shared)
    (2024), YouTube'
- en: '[5] A. Vaswani, et al., “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] A. Vaswani 等人, “[Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)”
    (2017), arXiv'
