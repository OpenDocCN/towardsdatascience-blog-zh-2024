- en: Time Series Forecasting with TensorFlow and Visualization Techniques to Perform
    Predictions Beyond the Validation Period
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-forecasting-with-tensorflow-neural-networks-and-visualization-techniques-06ad14fd082b?source=collection_archive---------11-----------------------#2024-02-24](https://towardsdatascience.com/time-series-forecasting-with-tensorflow-neural-networks-and-visualization-techniques-06ad14fd082b?source=collection_archive---------11-----------------------#2024-02-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to Extend Your Predictions Beyond Validation Period
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@paulamaranon?source=post_page---byline--06ad14fd082b--------------------------------)[![Paula
    Maranon](../Images/40b163c740105e0d7506eea3335aa268.png)](https://medium.com/@paulamaranon?source=post_page---byline--06ad14fd082b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--06ad14fd082b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--06ad14fd082b--------------------------------)
    [Paula Maranon](https://medium.com/@paulamaranon?source=post_page---byline--06ad14fd082b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--06ad14fd082b--------------------------------)
    ·8 min read·Feb 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/947426607d6525714baa868ba36c92e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’ll guide you through the process of building time series
    models using TensorFlow, a powerful framework for constructing and training neural
    networks. I’ll show you a variety of neural network architectures for time series
    forecasting, ranging from simple models like SimpleRNN to more complex ones such
    as LSTM. Additionally, I’ll present advanced visualization techniques to I’ve
    used to make and visualize predictions beyond the validation period.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I’ve used the following libraries: TensorFlow with Keras for building neural
    networks, Matplotlib for visualization, NumPy for numerical operations, and Scikit-Learn
    for data preprocessing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data preparation is fundamental for the success of any machine learning model.
    In this section, I will perform several steps to prepare the data for training
    and validation.
  prefs: []
  type: TYPE_NORMAL
- en: Separating Data and Time Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step is to separate the time steps from the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: '**For Short Time Series Data (data stored in an array):** we can create an
    array of time steps using ‘np.arange()’:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**For Larger Datasets Stored in Files (e.g., CSV Files):** we can read the
    data and corresponding time steps from the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Splitting Data into Training and Validation Sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After obtaining the time steps and data, we split them into training and validation
    sets to train and evaluate the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Generating windowed datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After that, I’ve created a function to generate windowed datasets for both training
    and validation. Each window consists of a fixed number of data points. For instance,
    with a window size of 4, I use the last four data points in each window to predict
    the next one.
  prefs: []
  type: TYPE_NORMAL
- en: The windowed dataset produceses two-dimensional batches of windows on the data
    (batch size and timestamps).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After following these steps the data is properly prepared for ingestion into
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras provides many time series models that can be used for time-series forecasting.
    I’ll briefly describe some of the models I’ve used, starting with simpler structures
    and gradually increasing in complexity. It’s important to note that these structures
    are just examples. The number of units and the number of layers need to be fine-tuned
    according to the dataset being used.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Recurrent Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RNNs are neural networks used for processing sequences of data while retaining
    information from earlier time steps. However, they may fail in remembering information
    over long sequences.
  prefs: []
  type: TYPE_NORMAL
- en: In each time step, different batches of input data are fed into the RNN cell.
    The output of the RNN cell at each time step dependS not only on the current input
    batch but also on the previous state of the cell, which captures information from
    earlier time steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bab7657e545017b31a99ad53b7947bd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from the author
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a simple example of an RNN model with two recurrent layers and a final
    output layer. When using RNNs, the input data needs to be shaped because RNNs
    typically expect input data in a 3D tensor. [1]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Long Short-Term Memory networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LSTMs networks are a type of recurrent neural network known for their ability
    to retain information over multiple time steps. LSTMs achieve this by incorporating
    a memory cell that passes information from one cell to another and from one time
    step to another within the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4eb866a9f2085b0f0ded5f9d7eec462.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, LSTMs can be bidirectional, allowing them to analyze input data
    not only in the forward direction but also in reverse.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s an example of an LSMT model with two bidirectional LSTM layers. In my
    experience, using multiple layers in LSTMs tends to outperform single-layer models
    by capturing both low-level and high-level features. [2]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Adjusting the Learning Rate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These architectures need to be adapted according to each dataset. However, what
    works for me is to adjust the learning rate first and then experimenting with
    other number of layers and of units.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look to the code for adjusting the learning rate and the number of layers
    in this article: [Optimizing Neural Network Performance through Learning Rate
    Tuning and Hidden Layer Unit Selection](https://medium.com/@paulamaranon/optimizing-neural-network-performance-through-learning-rate-tuning-and-hidden-layer-unit-selection-ed6acc00f9a6).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Important Note: A very useful piece of advice I wish I had known a long time
    ago: apart from experimenting with different numbers of layers and units, I advise
    you to always adjust the weights while training the model. For more detailed information
    and coding, check out my article titled [Achieving Reproducibility in Neural Network
    Predictions](https://medium.com/@paulamaranon/achieving-reproducibility-in-neural-network-predictions-71bf6bfbadf6).'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the model on the validation dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To be able to visualize the performance of the model and the predictions, let’s
    compile and train the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In time series forecasting, it’s common to use the Mean Squared Error (MSE)
    or the Mean Absolute Error (MAE) to validate the performance of the models. Unlike
    MSE, MAE does not square the errors but instead uses their absolute values. This
    approach does not overly penalize large errors, making it suitable for scenarios
    where all errors should be treated equally.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing training and validation loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now create two plots showing the Loss and MAE curves over epochs for both
    training and validation data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an example of the plots which can be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d084eaafc808a0e2290818a0ae08743c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions on the Validation Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that I’ve trained the model and validated its performance using MSE metric,
    it’s time to apply the model to the validation dataset to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Below is the code to make predictions on the validation dataset. Additionally,
    I’ve included how to print the predictions beyond the validation period along
    the actual data for each time step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You can also visualize the predictions beyond the validation period in a plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is an example of an image from the code above, based on a dummy dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/601fa0cb259e516bb75d316fdbfe1ed1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Visualizing time series data and model predictions is basic for understanding
    the performance of your model. In this article I solved a problem I faced several
    times which was how to perform and visualize time series predictions beyond the
    validation period, which may be useful for checking the model on actual data afterward.
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1]SimpleRNN:[https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN#call_arguments](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]LSTM:[https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)'
  prefs: []
  type: TYPE_NORMAL
