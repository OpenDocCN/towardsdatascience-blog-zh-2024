<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Multimodal Large Language Models & Apple’s MM1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Multimodal Large Language Models & Apple’s MM1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13">https://towardsdatascience.com/multimodal-large-language-models-apples-mm1-c1e94d87a161?source=collection_archive---------0-----------------------#2024-04-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2e43" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post will go into the architecture and findings behind Apple’s “MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training” paper</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Gunton" class="l ep by dd de cx" src="../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F8sHS2ai6w95qbGIZ9qM_g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mgunton7?source=post_page---byline--c1e94d87a161--------------------------------" rel="noopener follow">Matthew Gunton</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c1e94d87a161--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/4b89777cfb3161fa6ce57a05fdb4277c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYT-apLhDvn2HV1xT_mlrQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the Author Generated by DALL-E</figcaption></figure><p id="6b39" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Abstraction is one of the most critical concepts in Computer Science, with some of the most powerful implications. From a simplistic point of view, abstraction is the ability to take something and apply it to multiple distinct situations. For example, if you create a way to successfully sort apples based on their size in a factory, your solution could be abstracted to also sort oranges or peaches in the same way. Thus, through abstraction a very powerful solution is able to radically impact multiple parts of the world.</p><p id="dd98" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">While Large Language Models are exceptional at reasoning when given text as an input, recently we have been able to abstract their input so that they can reason with images and sounds.</p><p id="bf99" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The below blog post goes into architectural ablations in Apple’s MM1 paper and their research findings when building a Multimodal Large Language Model (MLLM).</p></div></div></div><div class="ab cb nx ny nz oa" role="separator"><span class="ob by bm oc od oe"/><span class="ob by bm oc od oe"/><span class="ob by bm oc od"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="967f" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Abstracting LLM Input</h1><p id="d511" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">The architecture behind Large Language Models can be traced back to the 2017 paper “Attention is All You Need” where the Transformer Architecture was introduced.</p><p id="432c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This paper showed how you could transform human language into tokens that a neural network would then process (in that paper’s case, process into a different language).</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div class="mi mj pg"><img src="../Images/8537829bb3d08c52694fbb294fe530d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*YXsb3cWfnLBrmfc0QJUDFQ.png"/></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 1 from <a class="af ph" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a></figcaption></figure><p id="efd3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As you can see from the image, we have a transformation occurring early on where we take the input and convert it into tokens (the embedding section). However, there is no inherent reason why only text data can be mapped to tokens. Consequently, the field began trying to map other kinds of data to tokens.</p><h1 id="a1ff" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">MM1 Architecture Base</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pn"><img src="../Images/effb93869b2820023bfd10695c290fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHnNubUAuM_kxaAZtqAs-A.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Part of Figure 3 from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="7c9c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Apple’s model had 3 key components: a visual transformer (ViT) image encoder, Vision-Language Connector, and a Large Language Model. Assuming you already have a good idea of what a LLM is and how it works, let’s dive into the image encoder and VL connector.</p><h1 id="959a" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">Image Encoders &amp; Visual Connectors</h1><p id="a97c" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">While from an abstracted view we can imagine text and images as simply different kinds of inputs, to make this work we need to accept that we may have to treat them differently to get them into token form. At this time, we have 2 different systems that help us transform the image into tokens the LLM can reason with: image encoder and connector.</p><p id="c0b6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First, the image encoder is responsible for taking our image and converting it into the token representation that our transformer model can understand.</p><p id="1e86" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Second, a connector is the piece that takes data from the vision encoder and transforms it into the data that is passed directly to the large language model. Given the image encoder returns tokens, you may wonder why we need the connector at all. The idea appears to be that image encoders give too much information in their tokens, so to reduce costs while optimizing reasoning, we want to be selective with what we pass through.</p><p id="557b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The below image shows the data flow we’re working with here.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj po"><img src="../Images/d88a4a539cf61aeecfdaea062e1ce7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_5QFHq0kE--0-5mFfNlYg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2 from <a class="af ph" href="https://arxiv.org/pdf/2312.06742.pdf" rel="noopener ugc nofollow" target="_blank">“Honeybee: Locality-enhanced Projector for Multimodal LLM”</a></figcaption></figure><h1 id="a718" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">Ablations</h1><p id="7f7a" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">An ablation study in Machine Learning revolves around removing and modifying certain parts of a model to see how they contribute to overall performance. Apple’s research centered around different ways of training the Image Encoder, different projectors for the VL Connector, and different pre-training data.</p><p id="2760" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s dive into the major findings.</p></div></div></div><div class="ab cb nx ny nz oa" role="separator"><span class="ob by bm oc od oe"/><span class="ob by bm oc od oe"/><span class="ob by bm oc od"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4d19" class="of og fq bf oh oi oj gq ok ol om gt on oo op oq or os ot ou ov ow ox oy oz pa bk">Image Encoder Ablations</h1><p id="5ddd" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">For the Image Encoder, they varied between CLIP and AIM models, Image resolution size, and the dataset the models were trained on. The below chart shows you the results for each ablation.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pp"><img src="../Images/72efc54bb2d9ecd0a6ef4975725f6171.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CV6sw1Wz_WkllsngXH_AeA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 1 from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="739b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s go through the major pieces above and explain what they are.</p><p id="57e8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">CLIP</strong> stands for Contrastive Language Image Pre-training and is meant to help your model learn visual concepts by providing names to the things that are meant to be seen as text. As the image below shows, this pairs images with text encodings so that the model will eventually connect the vision tokens (represented in the below image as I, with the text tokens T). This method is called contrastive training.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pq"><img src="../Images/2ee9a0733e16658d9ed133ce1a2ddda0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVMOSuyzK9zd8RXIXF08tA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 1 from <a class="af ph" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">“Learning Transferable Visual Models From Natural Language Supervision”</a></figcaption></figure><p id="ba91" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">AIM</strong> stands for Autoregressive Image Model, and it is trained via a reconstructive loss optimization algorithm. The goal here is to see if the transformer can recreate (reconstruct) the image that it is given.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/2df9b1a9690d422713ce7d372da6f989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Edlr8aeEBl-YAUbRymZsA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2 from <a class="af ph" href="https://arxiv.org/pdf/2401.08541.pdf" rel="noopener ugc nofollow" target="_blank">“Scalable Pre-training of Large Autoregressive Image Models”</a></figcaption></figure><p id="fef0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Image Resolution </strong>here refers to the number of pixels that is fed into the transformer. For example, a 378 x 378 image resolution means we will pass in a matrix of that size and then convert it into embeddings that the model will then be trained on. Training Data<strong class="nd fr"> </strong>was split between the (DFN-2B), (DFN-5B), (DFN-5B + VeCap) and (ImageText-400M).</p><p id="dc21" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The authors found that image resolution was of highest importance, followed by model size and then the training data contents. Specifically, they saw that the better the image resolution, the better the model tended to perform for both zero-shot and few-shot prompting. As more compute is needed to train and run models with higher image resolution requirements, this suggests that for Vision Transformers, compute will remain of paramount importance.</p><h1 id="63cc" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">VL Connection Ablations</h1><p id="4148" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">For the VL Connector, they tested using 64 or 144 tokens for the image, tested using 224, 336, and 378 for the image resolution, and chose between a few architectures. I’ll briefly go over the architectures below.</p><p id="7db5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Average Pooling </strong>is exactly what it sounds like, taking the average of all of the tokens, and then doing a linear projection of this average so that the grid was 8x8 or 12x12.</p><p id="7188" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Attention Pooling</strong> makes the assumption that image tokens should be treated as samples from a fundamentally different population set than the text tokens. Here we adjust how many tokens are fed in for each image, in the paper referred to as k learnable queries. The researchers only considered k of either 64 or 144.</p><p id="95b4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Convolutional Mapping</strong> is a a method from Honeybee that uses a ResNet to dynamically decide how many tokens to pass through to the LLM from the image. This is actualized in the C-Abstractor module.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ps"><img src="../Images/4848e1619cc1b54fc72ebf7f388afa54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mo_DqnRAWyZ9JciEeyTiOg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 4 from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="119b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As you can see from the above, the different architectures actually had very little impact. As one might guess, the higher resolution images and the more tokens passed through increased performance among all of the connectors but not dramatically so.</p><p id="ced7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This finding suggests we either haven’t found a significantly better way to connect the image encoder to the LLM, or that this area is simply not where great models will differentiate themselves.</p><h1 id="0708" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">Pre-Training Data Ablations</h1><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/a7eba075bb1027a4aff1a184921b0107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LHy0gDJHLwOGJlrAuPldqg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 2 from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="2fd3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here, the authors played with 4 different kinds of data: captioned images, synthetically captioned images, interleaved image-text data, and text-only data. They found 4 lessons, each with a graph to summarize the performance changes.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pu"><img src="../Images/817a2d9f60801091eb142e89d99b205a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yY5TSeGnkZIWq_UtU27RDw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 5a from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="39b6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">First</strong>, interleaving data helps with few-shot and text-only performance, while captioned data helps with zero-shot performance. The researchers varied how much interleaving they did, with the graph below showing the results. As you can see, few-shot prompts performed noticeably better on models trained with interleaved data than the models trained with all or nothing.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/4ac3c1384e53bdf08a5a445bbb9a4fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jWwQVNLV3jj9CqOUVTf7XQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 5b from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="28b9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Second</strong>, Text-only data helps with few-shot reasoning. Text-only in this context means that the training data includes image examples and text-only examples. This was done to ensure that the model understands human language as well as images. Comparing the caption-only to caption-with-text shows a marked improvement for all but the 0-shot reasoning, however, interleaved-only performs better than interleaved-plus-text for all but the TextCore test.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/edf1887d66e248908c526bf475ee57b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pppRVVlzxvSl0jd4Eg-pRg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 5c from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="9977" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Third</strong>, if you get the mixture right between image and text you can get really strong performance. The above graph shows different ratios of interleaved + captioned data to text-only data. As the goal is to have a multi-modal model, they never tested the performance if you do not have any image data. The authors here point out that the 91/9 ratio produced the most consistently good results.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/212adb561e42c3bbe4a6878cf7b81c6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2geBTE_22guRRe4FhouXcQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 5d from <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">the paper</a></figcaption></figure><p id="2069" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Fourth</strong>, synthetic data helps with few-shot learning. VeCap stands for Visual-enriched Caption, which is a way of creating captions so that they are sure to describe key visual pieces of the image. For the reverse, imagine a caption that may explain the meaning behind a photo but doesn’t explain any of the elements in the photo. You would typically do this if your data-scraper found images with poor alt-text data.</p><p id="0862" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The authors here concluded that VeCap gives a “non-trivial” boost in few-shot reasoning, but has a relatively small increase in quality. This raises questions about the cost-effectiveness of VeCap.</p><h1 id="4e12" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">Results</h1><p id="d815" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">Using the results from their ablations, the authors created a Transformer in two-forms: Mixture-of-Expert and regular. Both models had an encoder with a 378 x 378 image, pre-trained with DFN-5B dataset only. They had a mix of 45% captioned data, 45% interleaved data, and 10% text-only data (approximating the 91:9 ratio of image to text data). The VL Connector had 144 tokens and they chose a C Abstractor, though they point out that this was a somewhat arbitrary choice. For the LLM itself, they created a 3B, 7B, and 30B parameter model (with the MoE model only going up to 7B). The graph below shows how the these models performed.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj px"><img src="../Images/2e8811a3e70ecaa26b03e606a7725e08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGHmQOBh-QIqA3kRgk_GWg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Table 4 from<a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank"> the paper</a></figcaption></figure><p id="22d0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interestingly, the 30B parameter model performs on par with other models which have billions more parameters than it (LLaVA-NeXT-34B, etc.), suggesting that there may be some quantum relationship between parameter size and performance here.</p><h1 id="c859" class="of og fq bf oh oi pi gq ok ol pj gt on oo pk oq or os pl ou ov ow pm oy oz pa bk">Closing Thoughts</h1><p id="ab67" class="pw-post-body-paragraph nb nc fq nd b go pb nf ng gr pc ni nj nk pd nm nn no pe nq nr ns pf nu nv nw fj bk">Multi-modal LLMs are an incredibly exciting part of the field. As we find better ways to transmit different data types into tokens, we may unlock even greater applications for these transformers. As we look to the future, it is not unreasonable now to consider how other senses could be inputed outside of a text description, such as sound, smell, or even touch. Data quality is likely to only become more valuable.</p><p id="c702" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As the authors concluded that the different language connectors don’t make a major difference, it will be interesting to see if this means research should focus on the image encoder, or rather if we simply haven’t found a true breakthrough way to use the VL connector.</p><p id="fcec" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Outside of this specific paper, one of the big questions that arises is how these MLLMs will perform outside of benchmarks. As LLMs have proliferated, one common criticism revolves around the use of benchmarks to compare them. Often times these benchmarks use a consistent dataset to compare, allowing one model to do better simply by overfitting, even if unintentionally. Using methodologies like ELO, the chess rating algorithm, in the <a class="af ph" href="https://chat.lmsys.org/" rel="noopener ugc nofollow" target="_blank">LLM Arena from lmsys</a> may give a better true comparison of model performance.</p><p id="6ae9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In closing, as more inputs are able to be connected to LLMs, one can expect that the number of applications they can be applied to will increase. Only time will tell how useful we can make this technology.</p></div></div></div><div class="ab cb nx ny nz oa" role="separator"><span class="ob by bm oc od oe"/><span class="ob by bm oc od oe"/><span class="ob by bm oc od"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fb16" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[1] McKinzie, B., et al. <a class="af ph" href="https://arxiv.org/pdf/2403.09611.pdf" rel="noopener ugc nofollow" target="_blank">“MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training” (2024)</a>, arXiv</p><p id="a8f6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2] Cha, J., et al. <a class="af ph" href="https://arxiv.org/pdf/2312.06742.pdf" rel="noopener ugc nofollow" target="_blank">“Honeybee: Locality-enhanced Projector for Multimodal LLM”</a> (2023), arXiv</p><p id="5b6c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[3] Antoniadis, P., et al. <a class="af ph" href="https://www.baeldung.com/cs/ml-ablation-study" rel="noopener ugc nofollow" target="_blank">“Machine Learning: What Is Ablation Study?”</a> (2024), arXiv</p><p id="301f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[4] Radford, A., et al. <a class="af ph" href="https://arxiv.org/pdf/2103.00020.pdf" rel="noopener ugc nofollow" target="_blank">“Learning Transferable Visual Models From Natural Language Supervision”</a> (2021), arXiv</p><p id="967d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[5] El-Nouby, Al., et al. <a class="af ph" href="https://arxiv.org/pdf/2401.08541.pdf" rel="noopener ugc nofollow" target="_blank">“Scalable Pre-training of Large Autoregressive Image Models”</a> (2024), arXiv</p><p id="7a49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[6] Vaswani, A., et al., “<a class="af ph" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a>” (2017), arXiv</p><p id="76d5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[7] Lai, Z., et al., <a class="af ph" href="https://arxiv.org/abs/2310.07699" rel="noopener ugc nofollow" target="_blank">“VeCLIP: Improving CLIP Training via Visual-enriched Captions”</a> (2023), arXiv</p></div></div></div></div>    
</body>
</html>