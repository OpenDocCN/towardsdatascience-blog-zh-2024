<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deploying Large Language Models: vLLM and Quantization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deploying Large Language Models: vLLM and Quantization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05">https://towardsdatascience.com/deploying-large-language-models-vllm-and-quantizationstep-by-step-guide-on-how-to-accelerate-becfe17396a2?source=collection_archive---------1-----------------------#2024-04-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4e0c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Step-by-step guide on how to accelerate large language models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ayoola Olafenwa" class="l ep by dd de cx" src="../Images/86914a74cbf83e711887fab896e318a9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*PxmZHGM2czHVUMvRtJiFoA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://olafenwaayoola.medium.com/?source=post_page---byline--becfe17396a2--------------------------------" rel="noopener follow">Ayoola Olafenwa</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--becfe17396a2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/399881e6f19fe28227d9982dd335ab20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jP16K0_0obXIGFxzPO0UIQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><a class="af nc" href="https://unsplash.com/photos/a-computer-chip-with-the-letter-a-on-top-of-it-eGGFZ5X2LnA" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><h2 id="4927" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Deployment of Large Language Models (LLMs)</h2><p id="d9dc" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">We live in an amazing time of Large Language Models like ChatGPT, GPT-4, and Claude that can perform multiple amazing tasks. In practically every field, ranging from education, healthcare to arts and business, Large Language Models are being used to facilitate efficiency in delivering services. Over the past year, many brilliant open-source Large Language Models, such as Llama, Mistral, Falcon, and Gemma, have been released. These open-source LLMs are available for everyone to use, but deploying them can be very challenging as they can be very slow and require a lot of GPU compute power to run for real-time deployment. Different tools and approaches have been created to simplify the deployment of Large Language Models.</p><p id="7293" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">Many deployment tools have been created for serving LLMs with faster inference, such as vLLM, c2translate, TensorRT-LLM, and llama.cpp. Quantization techniques are also used to optimize GPUs for loading very large Language Models. In this article, I will explain how to deploy Large Language Models with vLLM and quantization.</p><h2 id="fbd6" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="al">Latency and Throughput</strong></h2><p id="6dbe" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Some of the major factors that affect the speed performance of a Large Language Model are GPU hardware requirements and model size. The larger the size of the model, the more GPU compute power is required to run it. Common benchmark metrics used in measuring the speed performance of a Large Language Model are <strong class="od fr"><em class="oz">Latency</em></strong> and <strong class="od fr"><em class="oz">Throughput</em></strong>.</p><p id="c39b" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Latency:</strong> This is the time required for a Large Language Model to generate a response. It is usually measured in seconds or milliseconds.</p><p id="1a10" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Throughput:</strong> This is the number of tokens generated per second or millisecond from a Large Language Model.</p><h2 id="872f" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Install Required Packages</h2><p id="f94e" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Below are the two required packages for running a Large Language Model: Hugging Face <strong class="od fr"><em class="oz">transformers </em></strong>and <strong class="od fr"><em class="oz">accelerate</em></strong>.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="fb17" class="pe ne fq pb b bg pf pg l ph pi">pip3 install transformers<br/>pip3 install accelerate</span></pre><h2 id="b2f8" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">What is Phi-2?</h2><p id="05af" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr"><em class="oz">Phi-2</em></strong> is a state-of-the-art foundation model from Microsoft with 2.7 billion parameters. It was pre-trained with a variety of data sources, ranging from code to textbooks. Learn more about <strong class="od fr"><em class="oz">Phi-2</em></strong> from <a class="af nc" href="https://huggingface.co/microsoft/phi-2" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h2 id="63e0" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Benchmarking LLM Latency and Throughput with Hugging Face Transformers</h2><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><h2 id="c222" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Generated Output</h2><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="7d4c" class="pe ne fq pb b bg pf pg l ph pi">Latency: 2.739394464492798 seconds<br/>Throughput: 32.36171766303386 tokens/second<br/>Generate a python code that accepts a list of numbers and returns the sum. [1, 2, 3, 4, 5]<br/>A: def sum_list(numbers):<br/>    total = 0<br/>    for num in numbers:<br/>        total += num<br/>    return total<br/><br/>print(sum_list([1, 2, 3, 4, 5]))</span></pre><p id="b8da" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Step By Step Code Breakdown</strong></p><p id="acf8" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 6–10: </strong>Loaded <strong class="od fr"><em class="oz">Phi-2 </em></strong>model and tokenized the prompt “<strong class="od fr"><em class="oz">Generate a python code that accepts a list of numbers and returns the sum.</em></strong>”</p><p id="ce48" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 12- 18:</strong> Generated a response from the model and obtained the <strong class="od fr"><em class="oz">latency</em></strong> by calculating the time required to generate the response.</p><p id="195d" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 21–23: </strong>Obtained the total length of tokens in the response generated, divided it by the <strong class="od fr"><em class="oz">latency</em></strong> and calculated the <strong class="od fr"><em class="oz">throughput</em></strong>.</p><p id="93b3" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">This model was run on an A1000 (16GB GPU), and it achieves a <strong class="od fr"><em class="oz">latency</em></strong> of <strong class="od fr"><em class="oz">2.7 seconds</em></strong> and a throughput of <strong class="od fr"><em class="oz">32 tokens/second.</em></strong></p><h1 id="85ef" class="pm ne fq bf nf pn po gq nj pp pq gt nn pr ps pt pu pv pw px py pz qa qb qc qd bk">Deployment of A Large Language Model with vLLM</h1><p id="ec2f" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">vLLM is an open source LLM library for serving Large Language Models at low <strong class="od fr"><em class="oz">latency</em></strong> and high <strong class="od fr"><em class="oz">throughput</em></strong>.</p><h2 id="0aab" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">How vLLM works</h2><p id="c4d0" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">The transformer is the building block of Large Language Models. The transformer network uses a mechanism called the <strong class="od fr"><em class="oz">attention mechanism</em></strong>, which is used by the network to study and understand the context of words. The <strong class="od fr"><em class="oz">attention mechanism</em></strong> is made up of a bunch of mathematical calculations of matrices known as attention keys and values. The memory used by the interaction of these attention keys and values affects the speed of the model. vLLM introduced a new attention mechanism called <strong class="od fr"><em class="oz">PagedAttention</em></strong> that efficiently manages the allocation of memory for the transformer’s attention keys and values during the generation of tokens. The memory efficiency of vLLM has proven very useful in running Large Language Models at low latency and high throughput.</p><p id="ee45" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">This is a high-level explanation of how vLLM works. To learn more in-depth technical details, visit the vLLM documentation.</p><div class="qe qf qg qh qi qj"><a href="https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">GitHub | Documentation | Paper</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">blog.vllm.ai</p></div></div><div class="qs l"><div class="qt l qu qv qw qs qx lr qj"/></div></div></a></div><p id="732c" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Install vLLM</strong></p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="7365" class="pe ne fq pb b bg pf pg l ph pi">pip3 install vllm==0.3.3</span></pre><h2 id="e30c" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="al">Run Phi-2 with vLLM</strong></h2><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><h2 id="7d2b" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Generated Output</h2><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="bdd4" class="pe ne fq pb b bg pf pg l ph pi">Latency: 1.218436622619629seconds<br/>Throughput: 63.15334836428132tokens/second<br/> [1, 2, 3, 4, 5]<br/>A: def sum_list(numbers):<br/>    total = 0<br/>    for num in numbers:<br/>        total += num<br/>    return total<br/><br/>numbers = [1, 2, 3, 4, 5]<br/>print(sum_list(numbers))</span></pre><p id="f34c" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Step By Step Code Breakdown</strong></p><p id="2f76" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 1–3:</strong> Imported required packages from vLLM for running <strong class="od fr"><em class="oz">Phi-2</em></strong>.</p><p id="4afe" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 5–8:</strong> Loaded <strong class="od fr"><em class="oz">Phi-2</em></strong> with vLLM, defined the prompt and set important parameters for running the model.</p><p id="4ede" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 10–16: </strong>Generated the model’s response using <strong class="od fr"><em class="oz">llm.generate </em></strong>and computed the <strong class="od fr"><em class="oz">latency</em></strong>.</p><p id="77ae" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 19–21:</strong> Obtained the length of total tokens generated from the response, divided the length of tokens by the latency to get the <strong class="od fr"><em class="oz">throughput</em></strong>.</p><p id="4d34" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 23–24: </strong>Obtained the generated text.</p><p id="fc15" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">I ran <strong class="od fr"><em class="oz">Phi-2</em></strong> with vLLM on the same prompt, <strong class="od fr"><em class="oz">“Generate a python code that accepts a list of numbers and returns the sum.”</em></strong> On the same GPU, an A1000 (16GB GPU), vLLM produces a <strong class="od fr"><em class="oz">latency</em></strong> of <strong class="od fr"><em class="oz">1.2 seconds</em></strong> and a <strong class="od fr"><em class="oz">throughput</em></strong> of <strong class="od fr"><em class="oz">63 tokens/second</em></strong>, compared to Hugging Face transformers’ <strong class="od fr"><em class="oz">latency</em></strong> of <strong class="od fr"><em class="oz">2.85 seconds</em></strong> and a <strong class="od fr"><em class="oz">throughput</em></strong> of <strong class="od fr"><em class="oz">32 tokens/second.</em></strong> Running a Large Language Model with vLLM produces the same accurate result as using Hugging Face, with much lower latency and higher throughput.</p><p id="500d" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Note:</strong> The metrics (latency and throughput) I obtained for vLLM are estimated benchmarks for vLLM performance. The model generation speed depends on many factors, such as the length of the input prompt and the size of the GPU. According to the official vLLM report, running an LLM model on a powerful GPU like the A100 in a production setting with vLLM achieves <strong class="od fr">24x higher throughput</strong> than Hugging Face Transformers.</p><h2 id="6eb0" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Benchmarking Latency and Throughput in Real Time</h2><p id="f8a0" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">The way I calculated the latency and throughput for running Phi-2 is experimental, and I did this to explain how vLLM accelerates a Large Language Model’s performance. In the real-world use case of LLMs, such as a chat-based system where the model outputs a token as it is generated, measuring the latency and throughput is more complex.</p><p id="d9f5" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">A chat-based system is based on streaming output tokens. Some of the major factors that affect the LLM metrics are <strong class="od fr"><em class="oz">Time to First Token</em></strong> (the time required for a model to generate the first token), <strong class="od fr"><em class="oz">Time Per Output Token</em></strong> (the time spent per output token generated), <strong class="od fr"><em class="oz">the input sequence length, the expected output, the total expected output tokens</em></strong>, and <strong class="od fr"><em class="oz">the model size</em></strong>. In a chat-based system, the latency is usually a combination of <strong class="od fr"><em class="oz">Time to First Token</em></strong> and <strong class="od fr"><em class="oz">Time Per Output Token</em></strong> multiplied by the total expected output tokens.</p><p id="4cc6" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">The longer the input sequence length passed into a model, the slower the response. Some of the approaches used in running LLMs in real-time involve batching users’ input requests or prompts to perform inference on the requests concurrently, which helps in improving the throughput. Generally, using a powerful GPU and serving LLMs with efficient tools like vLLM improves both the latency and throughput in real-time.</p><p id="c800" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Run the vLLM deployment on Google Colab</strong></p><div class="qe qf qg qh qi qj"><a href="https://colab.research.google.com/drive/171tVs8nndleyYHoFr1PVm6YRvYg6kBCx?usp=sharing&amp;source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">Google Colaboratory</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">Edit description</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">colab.research.google.com</p></div></div><div class="qs l"><div class="qy l qu qv qw qs qx lr qj"/></div></div></a></div><h1 id="b5e8" class="pm ne fq bf nf pn po gq nj pp pq gt nn pr ps pt pu pv pw px py pz qa qb qc qd bk">Quantization of Large Language Models</h1><p id="64e6" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Quantization is the conversion of a machine learning model from a higher precision to a lower precision by shrinking the model’s weights into smaller bits, usually <strong class="od fr"><em class="oz">8-bit</em></strong> or <strong class="od fr"><em class="oz">4-bit</em></strong>. Deployment tools like vLLM are very useful for inference serving of Large Language Models at very low latency and high throughput. We are able to run <strong class="od fr"><em class="oz">Phi-2</em></strong> with Hugging Face and vLLM conveniently on the T4 GPU on Google Colab because it is a smaller LLM with <strong class="od fr"><em class="oz">2.7 billion parameters</em></strong>. For example, a 7-billion-parameter model like <strong class="od fr"><em class="oz">Mistral 7B</em></strong> cannot be run on Colab with either Hugging Face or vLLM. Quantization is best for managing GPU hardware requirements for Large Language Models. When GPU availability is limited and we need to run a very large Language Model, quantization is the best approach to load LLMs on constrained devices.</p><h2 id="5a57" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">BitsandBytes</h2><p id="0aa4" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">It is a python library built with custom quantization functions for shrinking model’s weights into lower bits(<strong class="od fr"><em class="oz">8-bit</em></strong> and <strong class="od fr"><em class="oz">4-bit</em></strong>).</p><p id="ce80" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Install BitsandBytes</strong></p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="6156" class="pe ne fq pb b bg pf pg l ph pi">pip3 install bitsandbytes</span></pre><h2 id="10cd" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Quantization of Mistral 7B Model</h2><p id="3d88" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr"><em class="oz">Mistral 7B</em></strong>, a 7-billion-parameter model from MistralAI, is one of the best state-of-the-art open-source Large Language Models. I will go through a step-by-step process of running <strong class="od fr"><em class="oz">Mistral 7B</em></strong> with different quantization techniques that can be run on the T4 GPU on Google Colab.</p><p id="67f4" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Quantization with 8bit Precision</strong>: This is the conversion of a machine learning model’s weight into 8-bit precision. <strong class="od fr"><em class="oz">BitsandBytes</em></strong> has been integrated with Hugging Face transformers to load a language model using the same Hugging Face code, but with minor modifications for quantization.</p><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><p id="313d" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 1: </strong>Imported the needed packages for running model, including the <strong class="od fr"><em class="oz">BitsandBytesConfig</em></strong> library.</p><p id="5083" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 3–4: </strong>Defined the quantization config and set the parameter <strong class="od fr"><em class="oz">load_in_8bit</em></strong> to true for loading the model’s weights in <strong class="od fr"><em class="oz">8-bit </em></strong>precision.</p><p id="766f" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 7–9: </strong>Passed the quantization config into the function for loading the model, set the parameter <strong class="od fr"><em class="oz">device_map</em></strong> for <strong class="od fr"><em class="oz">bitsandbytes</em></strong> to automatically allocate appropriate GPU memory for loading the model. Finally loaded the tokenizer weights.</p><p id="2b4e" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Quantization with 4bit Precision</strong>: This is the conversion of a machine learning model’s weight into <strong class="od fr"><em class="oz">4-bi</em></strong>t precision.</p><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><p id="2b48" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">The code for loading <strong class="od fr"><em class="oz">Mistral 7B</em></strong> in 4-bit precision is similar to that of <strong class="od fr"><em class="oz">8-bit</em></strong> precision except for a few changes:</p><ul class=""><li id="64ff" class="ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot qz ra rb bk">changed <strong class="od fr"><em class="oz">load_in_8bit </em></strong>to <strong class="od fr"><em class="oz">load_in_4bit</em></strong>.</li><li id="15bd" class="ob oc fq od b go rc of og gr rd oi oj no re ol om ns rf oo op nw rg or os ot qz ra rb bk">A new parameter <strong class="od fr"><em class="oz">bnb_4bit_compute_dtype</em></strong> is introduced into the <strong class="od fr"><em class="oz">BitsandBytesConfig</em></strong> to perform the model’s computation in <strong class="od fr"><em class="oz">bfloat16</em></strong>. <strong class="od fr"><em class="oz">bfloat16</em></strong> is computation data type for loading model’s weights for faster inference. It can be used with both <strong class="od fr">4-bit</strong> and <strong class="od fr"><em class="oz">8-bit </em></strong>precisions<strong class="od fr"><em class="oz">. </em></strong>If it is in <strong class="od fr"><em class="oz">8-bit</em></strong> you just need to change the parameter from <strong class="od fr"><em class="oz">bnb_4bit_compute_dtype </em></strong>to<strong class="od fr"><em class="oz"> bnb_8bit_compute_dtype.</em></strong></li></ul><h2 id="807a" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">NF4(4-bit Normal Float) and <strong class="al">Double Quantization</strong></h2><p id="9a80" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr">NF4 (4-bit Normal Float)</strong> from QLoRA is an optimal quantization approach that yields better results than the standard 4-bit quantization. It is integrated with double quantization, where quantization occurs twice; quantized weights from the first stage of quantization are passed into the next stage of quantization, yielding optimal float range values for the model’s weights. According to the report from the QLoRA paper, <strong class="od fr"><em class="oz">NF4 with double quantization</em></strong> does not suffer from a drop in accuracy performance. Read more in-depth technical details about NF4 and Double Quantization from the QLoRA paper:</p><div class="qe qf qg qh qi qj"><a href="https://arxiv.org/abs/2305.14314?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">QLoRA: Efficient Finetuning of Quantized LLMs</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model…</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">arxiv.org</p></div></div><div class="qs l"><div class="rh l qu qv qw qs qx lr qj"/></div></div></a></div><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><p id="90dd" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 4–9:</strong> Extra parameters were set the <strong class="od fr"><em class="oz">BitsandBytesConfig:</em></strong></p><ul class=""><li id="c275" class="ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot qz ra rb bk"><strong class="od fr"><em class="oz">load_4bit:</em></strong> loading model in 4-bit precision is set to true.</li><li id="a45a" class="ob oc fq od b go rc of og gr rd oi oj no re ol om ns rf oo op nw rg or os ot qz ra rb bk"><strong class="od fr"><em class="oz">bnb_4bit_quant_type:</em></strong> The type of quantization is set to nf4.</li><li id="7060" class="ob oc fq od b go rc of og gr rd oi oj no re ol om ns rf oo op nw rg or os ot qz ra rb bk"><strong class="od fr"><em class="oz">bnb_4bit_use_double_quant:</em></strong> Double quantization is set to True.</li><li id="6a02" class="ob oc fq od b go rc of og gr rd oi oj no re ol om ns rf oo op nw rg or os ot qz ra rb bk"><strong class="od fr"><em class="oz">bnb_4_bit_compute_dtype:</em></strong> <strong class="od fr"><em class="oz">bfloat16</em></strong> computation data type is used for faster inference.</li></ul><p id="15ce" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Line 11–13: </strong>Loaded the model’s weights and tokenizer.</p><p id="e1f2" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Full Code for Model Quantization</strong></p><figure class="mm mn mo mp mq mr"><div class="pj io l ed"><div class="pk pl l"/></div></figure><h2 id="ddab" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Generated Output</h2><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="1615" class="pe ne fq pb b bg pf pg l ph pi">&lt;s&gt; [INST] What is Natural Language Processing? [/INST] Natural Language Processing (NLP) is a subfield of artificial intelligence (AI) and<br/>computer science that deals with the interaction between computers and human language. Its main objective is to read, decipher, <br/>understand, and make sense of the human language in a valuable way. It can be used for various tasks such as speech recognition, <br/>text-to-speech synthesis, sentiment analysis, machine translation, part-of-speech tagging, name entity recognition, <br/>summarization, and question-answering systems. NLP technology allows machines to recognize, understand,<br/> and respond to human language in a more natural and intuitive way, making interactions more accessible and efficient.&lt;/s&gt;</span></pre><p id="52a5" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">Quantization is a very good approach for optimizing the running of very Large Language Models on smaller GPUs and can be applied to any model, such as Llama 70B, Falcon 40B, and mpt-30b. According to reports from the <a class="af nc" href="https://arxiv.org/abs/2208.07339" rel="noopener ugc nofollow" target="_blank">LLM.int8 paper</a>, very Large Language Models suffer less from accuracy drops when quantized compared to smaller ones. Quantization is best applied to very Large Language Models and does not work well for smaller models because of the loss in accuracy performance.</p><p id="1b41" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Run Mixtral 7B Quantization on Google Colab</strong></p><div class="qe qf qg qh qi qj"><a href="https://colab.research.google.com/drive/1aYPlWaHC4iy6DLFjR291iEMxFvgeM1ia?usp=sharing&amp;source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">Google Colaboratory</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">Edit description</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">colab.research.google.com</p></div></div><div class="qs l"><div class="ri l qu qv qw qs qx lr qj"/></div></div></a></div><h2 id="8785" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Conclusion</h2><p id="e356" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">In this article, I provided a step-by-step approach to measuring the speed performance of a Large Language Model, explained how vLLM works, and how it can be used to improve the latency and throughput of a Large Language Model. Finally, I explained quantization and how it is used to load Large Language Models on small-scale GPUs.</p><p id="aa27" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">Reach to me via:</strong></p><p id="0d49" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">Email: <a class="af nc" href="https://mail.google.com/mail/u/0/#inbox" rel="noopener ugc nofollow" target="_blank">olafenwaayoola@gmail.com</a></p><p id="5728" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk">Linkedin: <a class="af nc" href="https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ayoola-olafenwa-003b901a9/</a></p><p id="73dd" class="pw-post-body-paragraph ob oc fq od b go ou of og gr ov oi oj no ow ol om ns ox oo op nw oy or os ot fj bk"><strong class="od fr">References</strong></p><div class="qe qf qg qh qi qj"><a href="https://blog.vllm.ai/2023/06/20/vllm.html?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">GitHub | Documentation | Paper</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">blog.vllm.ai</p></div></div><div class="qs l"><div class="rj l qu qv qw qs qx lr qj"/></div></div></a></div><div class="qe qf qg qh qi qj"><a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">We're on a journey to advance and democratize artificial intelligence through open source and open science.</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">huggingface.co</p></div></div><div class="qs l"><div class="rk l qu qv qw qs qx lr qj"/></div></div></a></div><div class="qe qf qg qh qi qj"><a href="https://www.baseten.co/blog/understanding-performance-benchmarks-for-llm-inference/?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">Understanding performance benchmarks for LLM inference</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">This guide helps you interpret LLM performance metrics to make direct comparisons on latency, throughput, and cost.</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">www.baseten.co</p></div></div><div class="qs l"><div class="rl l qu qv qw qs qx lr qj"/></div></div></a></div><div class="qe qf qg qh qi qj"><a href="https://www.tensorops.ai/post/what-are-quantized-llms?source=post_page-----becfe17396a2--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qk ab ig"><div class="ql ab co cb qm qn"><h2 class="bf fr hw z io qo iq ir qp it iv fp bk">What are Quantized LLMs?</h2><div class="qq l"><h3 class="bf b hw z io qo iq ir qp it iv dx">Discover the power of quantized LLMs! Learn how model quantization reduces size, enables efficient hardware usage, and…</h3></div><div class="qr l"><p class="bf b dy z io qo iq ir qp it iv dx">www.tensorops.ai</p></div></div><div class="qs l"><div class="rm l qu qv qw qs qx lr qj"/></div></div></a></div></div></div></div></div>    
</body>
</html>