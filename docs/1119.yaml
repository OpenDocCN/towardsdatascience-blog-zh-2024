- en: Reducing the Size of Docker Images Serving Large Language Models (part 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reducing-the-size-of-docker-images-serving-llm-models-b70ee66e5a76?source=collection_archive---------6-----------------------#2024-05-03](https://towardsdatascience.com/reducing-the-size-of-docker-images-serving-llm-models-b70ee66e5a76?source=collection_archive---------6-----------------------#2024-05-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Have you encountered a problem where a 1 GB transformer-based model increases
    even up to 8 GB when deployed using Docker containerization?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://czuk.medium.com/?source=post_page---byline--b70ee66e5a76--------------------------------)[![Michał
    Marcińczuk, Ph.D.](../Images/74fb7b0099084be3f7a35a149471ffbd.png)](https://czuk.medium.com/?source=post_page---byline--b70ee66e5a76--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b70ee66e5a76--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b70ee66e5a76--------------------------------)
    [Michał Marcińczuk, Ph.D.](https://czuk.medium.com/?source=post_page---byline--b70ee66e5a76--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b70ee66e5a76--------------------------------)
    ·6 min read·May 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/817ed50bee602682eb9c094396535e05.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dominik Lückmann](https://unsplash.com/@exdigy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer-based models like a **BERT**, **RoBERTa**, or **T5** provide a state-of-the-art
    solution for many custom problems in natural language processing. A common way
    to deliver the models on production is to build a Docker image that provides an
    API to the model. The image encapsulates the required dependencies, the model
    itself, and the code to process the input data with the model. Compared to the
    large generative models (GenAI), these models are relatively small, **from 0.5
    to 2 GB**. Nevertheless, when you follow the straightforward way to deploy the
    model as a Docker image, you may be surprised by the size of the image, which
    **can reach 8 GB**. Have you wondered why the target image is so large and if
    there is a way to reduce its size? In this story, I will discuss why the docker
    image might be so large and how to reduce its size.
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples of the Python scripts and Docker files used in the story are also
    available on this repo [1]:'
  prefs: []
  type: TYPE_NORMAL
