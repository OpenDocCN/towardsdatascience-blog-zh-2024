<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into Transformers by Hand ✍︎</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into Transformers by Hand ✍︎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12">https://towardsdatascience.com/deep-dive-into-transformers-by-hand-%EF%B8%8E-68b8be4bd813?source=collection_archive---------0-----------------------#2024-04-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4dc1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the details behind the power of transformers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--68b8be4bd813--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--68b8be4bd813--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">8</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="01c3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There has been a new development in our neighborhood.</p><p id="d9cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A ‘Robo-Truck,’ as my son likes to call it, has made its new home on our street.</p><p id="2d97" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is a Tesla Cyber Truck and I have tried to explain that name to my son many times but he insists on calling it Robo-Truck. Now every time I look at Robo-Truck and hear that name, it reminds me of the movie Transformers where robots could transform to and from cars.</p><p id="0e65" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And isn’t it strange that Transformers as we know them today could very well be on their way to powering these Robo-Trucks? It’s almost a full circle moment. But where am I going with all these?</p><p id="a923" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Well, I am heading to the destination — Transformers. Not the robot car ones but the neural network ones. And you are invited!</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/51e6a28c10ef051412c2b9cfbb2838d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4bAZ8RgZIH6MA114yAmqg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author (Our Transformer — ‘Robtimus Prime’. Colors as mandated by my artist son.)</figcaption></figure><h2 id="8d8e" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">What are Transformers?</h2><p id="7ea7" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Transformers are essentially neural networks. Neural networks that specialize in learning context from the data.</p><p id="8d17" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But what makes them special is the presence of mechanisms that eliminate the need for <strong class="ml fr">labeled datasets </strong>and<strong class="ml fr"> convolution or recurrence</strong> in the network.</p><h2 id="77d3" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">What are these special mechanisms?</h2><p id="d908" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">There are many. But the two mechanisms that are truly the force behind the transformers are attention weighting and feed-forward networks (FFN).</p><h2 id="8c40" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">What is attention-weighting?</h2><p id="84cd" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Attention-weighting is a technique by which the model learns which part of the incoming sequence needs to be focused on. Think of it as the ‘Eye of Sauron’ scanning everything at all times and throwing light on the parts that are relevant.</p><blockquote class="oy oz pa"><p id="44ac" class="mj mk pb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Fun-fact: Apparently, the researchers had almost named the Transformer model ‘Attention-Net’, given Attention is such a crucial part of it.</p></blockquote><h2 id="5d7e" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">What is FFN?</strong></h2><p id="d3af" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">In the context of transformers, FFN is essentially a regular multilayer perceptron acting on a batch of independent data vectors. Combined with attention, it produces the correct ‘position-dimension’ combination.</p><h1 id="8f3c" class="pc nz fq bf oa pd pe gq oe pf pg gt oi ph pi pj pk pl pm pn po pp pq pr ps pt bk"><strong class="al">How do Attention and FFN work?</strong></h1><p id="089a" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">So, without further ado, let’s dive into how <strong class="ml fr">attention-weighting</strong> and <strong class="ml fr">FFN</strong> make transformers so powerful.</p><p id="9a42" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This discussion is based on Prof. Tom Yeh’s wonderful AI by Hand Series on <a class="af pu" href="https://lnkd.in/g39jcD7j" rel="noopener ugc nofollow" target="_blank">Transformers</a> . (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned LinkedIn posts, which I have edited with his permission.)</p><p id="bee8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So here we go:</p><p id="1a14" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The key ideas here : <strong class="ml fr">attention weighting and feed-forward network (FFN)</strong>.</p><p id="e4c4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Keeping those in mind, suppose we are given:</p><ul class=""><li id="ae7b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pv pw px bk">5 input features from a previous block (A 3x5 matrix here, where X1, X2, X3, X4 and X5 are the features and each of the three rows denote their characteristics respectively.)</li></ul><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng py"><img src="../Images/8fbc7712aec4f5834dac58f2c4724322.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*RPnrl4AuoQYjweFTYhEHBA.png"/></div></figure><p id="3e91" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] <strong class="ml fr">Obtain attention weight matrix A</strong></p><p id="5c7d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The first step in the process is to obtain the <strong class="ml fr">attention weight matrix A</strong>. This is the part where the self-attention mechanism comes to play. What it is trying to do is find the most relevant parts in this input sequence.</p><p id="5718" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We do it by feeding the input features into the query-key (QK) module. For simplicity, the details of the QK module are not included here.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pz"><img src="../Images/98c0a5d3d06cbc56f250062940a59e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYNNNiaZac_ZNGFVUn4aag.gif"/></div></div></figure><p id="1634" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] <strong class="ml fr">Attention Weighting</strong></p><p id="2b70" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once we have the <strong class="ml fr">attention weight matrix A (5x5)</strong>, we multiply the input features (3x5) with it to obtain the <strong class="ml fr">attention-weighted features Z</strong>.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qa"><img src="../Images/88977d9819fb55ceb71256aa238c4cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_VmXxp6iPkwVEdhFwExkg.gif"/></div></div></figure><p id="9660" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The important part here is that the features here are combined <strong class="ml fr">based on their positions</strong> P1, P2 and P3 i.e. <strong class="ml fr">horizontally</strong>.</p><p id="be1b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To break it down further, consider this calculation performed row-wise:</p><p id="8990" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P1 X A1 = Z1 → Position [1,1] = 11</p><p id="4ffb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P1 X A2 = Z2 → Position [1,2] = 6</p><p id="607f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P1 X A3 = Z3 → Position [1,3] = 7</p><p id="2407" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P1 X A4 = Z4 → Position [1,4] = 7</p><p id="4907" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P1 X A5 = Z5 → Positon [1,5] = 5</p><p id="5bb6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">.</p><p id="fd49" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">.</p><p id="656e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">.</p><p id="4aa0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P2 X A4 = Z4 → Position [2,4] = 3</p><p id="8765" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P3 X A5 = Z5 →Position [3,5] = 1</p><p id="e29f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As an example:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qb"><img src="../Images/16388ba68af2bae868e9845f6b719465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X6fqG-iOlHNv5JOF-TWxeA.png"/></div></div></figure><p id="4986" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It seems a little tedious in the beginning but follow the multiplication row-wise and the result should be pretty straight-forward.</p><p id="77bf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cool thing is the way our attention-weight matrix<strong class="ml fr"> A</strong> is arranged, the new features <strong class="ml fr">Z</strong> turn out to be the combinations of <strong class="ml fr">X </strong>as below<strong class="ml fr"> </strong>:</p><p id="ecef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Z1 = X1 + X2</p><p id="7c53" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Z2 = X2 + X3</p><p id="c1dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Z3 = X3 + X4</p><p id="33b5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Z4 = X4 + X5</p><p id="0b9d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Z5 = X5 + X1</p><p id="d3fb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">(Hint : Look at the positions of 0s and 1s in matrix <strong class="ml fr">A</strong>).</p><p id="2452" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3] <strong class="ml fr">FFN : First Layer</strong></p><p id="18cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next step is to feed the attention-weighted features into the feed-forward neural network.</p><p id="fdb9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, the difference here lies in <strong class="ml fr">combining the values across dimensions</strong> as opposed to positions in the previous step. It is done as below:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qc"><img src="../Images/ed26d0b60d3f60310db8a988a7a4f437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMJqa8DPZ3LcnWtdccBKQQ.gif"/></div></div></figure><p id="ff8d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What this does is that it looks at the data from the other direction.</p><p id="e16d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">- In the attention step, we combined our input on the basis of the original features to obtain new features.</strong></p><p id="9dbb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">- In this FFN step, we consider their characteristics i.e. combine features vertically to obtain our new matrix.</strong></p><blockquote class="oy oz pa"><p id="5776" class="mj mk pb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Eg: P1(1,1) * Z1(1,1)</p><p id="2d99" class="mj mk pb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">+ P2(1,2) * Z1 (2,1)</p><p id="ca0f" class="mj mk pb ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">+ P3 (1,3) * Z1(3,1) + b(1) = 11, where b is bias.</p></blockquote><p id="55fe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once again element-wise row operations to the rescue. Notice that here the number of dimensions of the new matrix is increased to 4 here.</p><p id="f1ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] <strong class="ml fr">ReLU</strong></p><p id="4358" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our favorite step : ReLU, where the negative values obtained in the previous matrix are returned as zero and the positive value remain unchanged.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qd"><img src="../Images/2dc50d4a90df572bd357073a42b871a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmroND2LsW91TrYXNh2UGQ.gif"/></div></div></figure><p id="61ae" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5] <strong class="ml fr">FFN : Second Layer</strong></p><p id="22c9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally we pass it through the second layer where the dimensionality of the resultant matrix is reduced from 4 back to 3.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qe"><img src="../Images/ca42007aef10d51a48d23fb8a90ee9d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0CE0MMXVIuuu0qPYybrjA.gif"/></div></div></figure><p id="8a1f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The output here is ready to be fed to the next block (see its similarity to the original matrix) and the entire process is repeated from the beginning.</p><p id="3c6e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The two key things to remember here are:</strong></p><ol class=""><li id="7c8b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qf pw px bk"><strong class="ml fr">The attention layer combines across positions (horizontally).</strong></li><li id="ff7c" class="mj mk fq ml b go qg mn mo gr qh mq mr ms qi mu mv mw qj my mz na qk nc nd ne qf pw px bk"><strong class="ml fr">The feed-forward layer combines across dimensions (vertically).</strong></li></ol><p id="58a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And this is the secret sauce behind the power of the transformers — the ability to analyze data from different directions.</p><p id="2fd2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To summarize the ideas above, here are the key points:</p><ol class=""><li id="536b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qf pw px bk">The transformer architecture can be perceived as a combination of the attention layer and the feed-forward layer.</li><li id="0ca1" class="mj mk fq ml b go qg mn mo gr qh mq mr ms qi mu mv mw qj my mz na qk nc nd ne qf pw px bk">The <strong class="ml fr">attention layer combines the features</strong> to produce a new feature. E.g. think of combining two robots Robo-Truck and Optimus Prime to get a new robot Robtimus Prime.</li><li id="76a4" class="mj mk fq ml b go qg mn mo gr qh mq mr ms qi mu mv mw qj my mz na qk nc nd ne qf pw px bk">The <strong class="ml fr">feed-forward (FFN) layer combines the parts or the characteristics</strong> of the a feature to produce new parts/characteristics. E.g. wheels of Robo-Truck and Ion-laser of Optimus Prime could produce a wheeled-laser.</li></ol><h1 id="a6bc" class="pc nz fq bf oa pd pe gq oe pf pg gt oi ph pi pj pk pl pm pn po pp pq pr ps pt bk">The ever powerful Transformers</h1><p id="6843" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Neural networks have existed for quite some time now. Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN) had been reigning supreme but things took quite an eventful turn once Transformers were introduced in the year 2017. And since then, the field of AI has grown at an exponential rate — with new models, new benchmarks, new learnings coming in every single day. And only time will tell if this phenomenal idea will one day lead the way for something even bigger — a real ‘Transformer’.</p><p id="1038" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But for now it would not be wrong to say that an idea can really <em class="pb">transform</em> how we live!</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/860b393c7b33bd9c4edfb8330c924678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pckQhR-RyKrnS28F98TIBQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author</figcaption></figure><p id="ae07" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P.S. If you would like to work through this exercise on your own, here is the blank template for your use.</p><p id="9f12" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pu" href="https://drive.google.com/file/d/1F08laMdmwQ2vxYIqewOghS1eknaprgxe/view?usp=drive_link" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="5153" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now go have some fun and create your own <strong class="ml fr">Robtimus Prime</strong>!</p></div></div></div></div>    
</body>
</html>