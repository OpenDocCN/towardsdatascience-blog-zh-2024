<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Simple Regularization for Your GANs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Simple Regularization for Your GANs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29">https://towardsdatascience.com/a-simple-regularization-for-your-gans-12ea2cd168e?source=collection_archive---------15-----------------------#2024-07-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a845" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to capture data distributions effectively with GANs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shashank Sharma" class="l ep by dd de cx" src="../Images/61adaaf21b57375419eaa5341c322368.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*KJ8mzlVJL9LY3B2Cn2-ebA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@shashank879?source=post_page---byline--12ea2cd168e--------------------------------" rel="noopener follow">Shashank Sharma</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--12ea2cd168e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="3dba" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In 2018, I had the privilege of orally presenting my paper at the AAAI conference. A common feedback was that the insights were clearer in the presentation than in the paper. Although some time has passed since then, I believe there’s still value in sharing the core insights and intuitions.</p><p id="ac99" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper addressed a significant problem of reliably capturing modes in a dataset with Generative Adversarial Networks (GANs). This article is formulated around my intuitions of GANs and derives the proposed approach from those intuitions. Finally, I present a copy-paste solution for those who want to try it out. If you are familiar with GANs, feel free to skip to the next section.</p><blockquote class="ne nf ng"><p id="9d98" class="mi mj nh mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Paper: [Sharma, S. and Namboodiri, V., 2018, April. No modes left behind: Capturing the data distribution effectively using gans. In <em class="fq">Proceedings of the AAAI Conference on Artificial Intelligence</em>]<em class="fq"> </em>(<a class="af ni" href="https://arxiv.org/abs/1802.00771" rel="noopener ugc nofollow" target="_blank">paper</a>, <a class="af ni" href="https://github.com/shashank879/logan" rel="noopener ugc nofollow" target="_blank">github</a>)</p></blockquote><p id="fb3d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">A quick intro to Generative Adversarial Networks</em></strong></p><p id="ef21" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">GANs are used to learn Generators for a given distribution. This means that if we are given a dataset of images, say of birds, we have to learn a function that generates images that look like birds. The Generator function is usually deterministic, so it relies on a random number as input for stochasticity to produce a variety of images. Thus, the function takes a <em class="nh">n</em>-dimensional number as input and outputs an image. The input number <em class="nh">z</em> is typically, low-dimensional and randomly sampled from a uniform or a normal distribution. This distribution is called the latent distribution <em class="nh">Pz</em>.</p><p id="b315" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We refer to the space of “all possible” images as the data space <em class="nh">X, </em>the set of bird images as real <em class="nh">R</em>, and their distribution as <em class="nh">Pr</em>. The Generator at optimality, maps each value of <em class="nh">z</em> to some image that has a high likelihood of belonging to <em class="nh">R</em>.</p><p id="494a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">GANs solve this problem using two learned functions: a Generator (<em class="nh">G)</em> and a Discriminator (<em class="nh">D)</em>. <em class="nh">G</em> takes the number <em class="nh">z</em> as input to produce a sample from data space, <em class="nh">x = G(z)</em>. At any point, we call the set of all images generated by <em class="nh">G</em> as fake <em class="nh">F</em>, and their distribution <em class="nh">Pg</em>. The Discriminator takes a sample <em class="nh">x</em> from the data space and outputs a scalar <em class="nh">D(x),</em> predicting its probability of belonging to the real or fake distribution.</p><p id="b6a4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Initially, neither <em class="nh">G</em> nor <em class="nh">D</em> is well-trained. We sample some random numbers at each training step and pass them through <em class="nh">G </em>to get some fake samples. Similarly, we take an equal number of random samples from the real subset. <em class="nh">D</em> is trained to output 0 for fake, and 1 for real samples via cross-entropic loss. <em class="nh">G</em> is trained to fool <em class="nh">D</em> such that the output of <em class="nh">D(G(z)) </em>becomes 1. In other words, increase the probability of generating samples that score high (produce more), and decrease it for those that score low. The gradients flow from the loss function through <em class="nh">D</em> and then through <em class="nh">G</em>. Please refer to the original GAN paper for the loss equations.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/09f1ea274890b28f6499cb33aa5c0adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lhUbkA9yYIh2mOp-CWueDQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 1.] *Image taken from the presentation “Generative Adversarial Networks” at NIPS Workshop on Perturbation, Optimization, and Statistics, Montreal, 2014. [Note: We refer to Pd as Pr in this article].</figcaption></figure><p id="1b26" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The above figure illustrates how a GAN learns for a 1-dimensional space <em class="nh">X</em>. The black dotted line represents the real distribution, which we refer to as <em class="nh">Pr</em>. The green line represents the fake samples' distribution <em class="nh">Pg</em>. The blue dotted line represents the Discriminator output <em class="nh">D(x)</em> for a data sample. In the beginning, neither <em class="nh">D</em> nor <em class="nh">G</em> performs correctly. First, <em class="nh">D</em> is updated to correctly classify real and fake samples. Next, <em class="nh">G</em> is updated to follow the local gradients of the Discriminator values for the generated samples <em class="nh">D(G(z))</em>, making <em class="nh">Pg</em> come closer to <em class="nh">Pr</em>. In other words, <em class="nh">G</em> slightly improves each sample based on <em class="nh">D</em>’s feedback. The last illustration shows the final equilibrium state.</p><p id="9fc6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This can be thought of as a frequentist approach. If <em class="nh">G</em> produces more samples from a mode than what occurs in <em class="nh">Pr</em>, even though the sample might look flawless, <em class="nh">D</em> begins to classify them as fake, discouraging <em class="nh">G</em> from generating such samples. Conversely, when <em class="nh">G</em> produces fewer samples, <em class="nh">D</em> begins to classify them a real, encouraging <em class="nh">G</em> to generate more of them. This continues till the frequency of generation of an element matches the frequency of its occurrence in <em class="nh">Pr.</em> Or, the element is equally likely in <em class="nh">Pg</em> and <em class="nh">Pr</em>. When the distributions exactly match, <em class="nh">D</em> outputs 0.5 at all points, indicating it cannot distinguish between real and fake samples. Then the loss reaches a minimum, and neither <em class="nh">G</em> nor <em class="nh">D</em> can improve further; this state is called the Nash equilibrium.</p><p id="52f7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Later Wasserstein GANs modified this objective a bit. <em class="nh">D</em> is trained to increase for real samples and decrease for fake unboundedly. They refer to it as a Critic. Rather than computing a frequency-based loss, they modified <em class="nh">G</em>'s objective to move <em class="nh">Pg</em> in the direction that improves <em class="nh">D(G(z))</em> directly. Please refer to the original paper for the equilibrium guarantee and other details of the method.</p><p id="4bdd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In my experience with GANs, I’ve found it more productive to view them not as competition between <em class="nh">G</em> and <em class="nh">D</em>, but a cooperative interaction. The Discriminator's objective is to establish a gradient of ‘realness’ between <em class="nh">Pg</em> and <em class="nh">Pr,</em> like a soft boundary. <em class="nh">G</em> then uses this feedback to move <em class="nh">Pg</em> closer to <em class="nh">Pr</em>. The smoother this boundary, the easier it is for <em class="nh">G</em> to improve. Viewing the GAN setup as competitive is disadvantageous because the loss of either networks, <em class="nh">D</em> or <em class="nh">G</em>, means failure of the final objective. However, the perspective of a joint objective aligns directly with the desired behavior.</p><p id="be8e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">The problem of mode loss</em></strong></p><p id="c12a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">A frequently occurring problem in GANs is the losing of minor modes by the Generator. <em class="nh">G</em> can receive feedback by <em class="nh">D</em> only for the samples it generates. If <em class="nh">G</em> misses a mode because it initially aims for the larger modes, it never improves for it. <em class="nh">G</em> only improves at a mode as long as it produces samples ‘nearby’ that mode. Technically speaking, the Generator follows the local gradients from the Discriminator to shift the modes in <em class="nh">Pg</em> to match those of <em class="nh">Pr</em>. Once <em class="nh">G</em> loses the local gradients to a minor mode, it never faces a penalty for not generating samples from that mode. It is a problem with real-world datasets, which are usually sparse, and many minor modes occur.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk oc"><img src="../Images/96b908831c603de92228004f47cdcf9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EjjJCj-M5doeesjXuw24g.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 2.] In the illustration, the numbers indicate the D(x) contours’ values, and the dashed boundary indicates the fake distribution F. The arrows indicate the gradients (orthogonal to the contours) experienced by G. There are two modes, major M1 and minor M2. Although, the Discriminator has marked M2 as real; Still, since the Generator distribution does not receive the gradients that lead to M2, it is missed.</figcaption></figure><p id="30b1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This can be seen in the differential equation that is used to compute the gradients. Given the loss function, gradients for learning <em class="nh">G</em> are computed as:</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk od"><img src="../Images/e81588ac922291fdf7073b6074b1658d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W85aVx-LIDffFAqOqHVehQ.png"/></div></div></figure><p id="aa2f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The middle term relies on seeing an improvement in <em class="nh">D(G(z))</em> wrt the data sample <em class="nh">G(z)</em> for the generated samples.</p><p id="7a0e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">Our Method</em></strong></p><p id="b721" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In our paper, we proposed a reliable approach to solving this problem. We test it with generated toy datasets and a real-world image dataset with a massive single mode. We also test the quality of learned representations by evaluating the CIFAR score and qualitative analysis using the CelebA face dataset.</p><p id="e712" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The following sections explain the underlying intuitions behind our approach.</p><p id="0c95" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">The inverted Generator or Encoder</em></strong></p><p id="c769" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let’s explore the opposite problem; given a dataset of images, we need to learn a mapping from the image to the latent distribution. Let’s assume the latent distribution is a 10-dimensional Uniform[0, 1] distribution. Thus, we construct a GAN where G is a function that takes images as input and outputs a 10-dimensional number with values in the range [0, 1]. <em class="nh">D</em> takes numbers from this space and outputs their “realness,” which indicates how likely it is to come from the Uniform distribution.</p><p id="59bc" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this scenario, the Generator is called an Encoder <em class="nh">(E)</em>. This can be because it learns to compress information. But is it useful?</p><p id="b5f9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We can visualize the Encoder’s task as assigning 10 floating numbers in the range [0, 1] to each image. This effectively places all the given images along a line of length 1, repeated for 10 different lines. Since we specify the Real distribution as Uniform, at equilibrium, the Encoder will match this distribution. Or, all the images will be uniformly spread along these 10 lines.</p><p id="492c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Assuming <em class="nh">E</em> has a finite capacity, meaning it cannot memorize all the patterns in the features and it is regularized such that there is continuity in outputs for inputs. Meaning, that the weights are finite and outputs cannot abruptly change for small changes in inputs. It will cause <em class="nh">E</em> to bring images with similar features into meaningful groups that can help it complete the task with these constraints. Thus, placing semantically closer images together in the feature space. While the features might be entangled, they yield meaningful representations.</p><p id="63b0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Now let’s look at the problem of mode loss from this perspective. We chose a Uniform distribution as <em class="nh">Pr</em>. Since it is a unimodal distribution, there is no weaker mode to lose. If <em class="nh">E</em> misses a region within the mode, it experiences gradients at the edge of <em class="nh">Pg</em> towards this region. If the Discriminator is regularized, its output will gradually change at the boundary of the missed region. Technically, <em class="nh">D</em> is differentiable wrt <em class="nh">X</em> at the boundary of this region. Then, <em class="nh">E</em> will follow the increasing <em class="nh">D</em> values to improve. Any region missed by <em class="nh">E</em> will eventually be captured. Thus, there can be no problem of mode loss in this case!</p><p id="8300" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Since the entire region is connected, the Encoder will experience corrective gradients for any differences between <em class="nh">Pg</em> and <em class="nh">Pr</em>. There will only be a global optimum, and the network won’t get stuck in a local optimum. Thus, given enough capacity, an Encoder can perfectly encode any data distribution to a unimodal distribution. We show this for a uniform distribution here via an illustration.</p><p id="7c44" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">From here onwards, we refer to the distribution of images as <em class="nh">Pr</em>, the latent distribution as <em class="nh">Pz</em>. The image samples will be denoted as <em class="nh">x</em> and the latent samples as <em class="nh">z</em>. The Generator takes <em class="nh">z</em> as input to produce images <em class="nh">G(z),</em> and the Encoder takes <em class="nh">x</em> as input to yield latent representations <em class="nh">E(x)</em>.</p></div></div><div class="nr"><div class="ab cb"><div class="ll oe lm of ln og cf oh cg oi ci bh"><div class="nm nn no np nq ab ke"><figure class="le nr oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><img src="../Images/d7e8a63f87f4fd90dd9029bd2ac1e2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*p3ERG_4ohJUz2gNukqvDZA.jpeg"/></div></figure><figure class="le nr oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><img src="../Images/b2f9d7128fd1433511dc1af3be08b2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*GzjQ1MvA17o_r5rm9-ntvg.jpeg"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx oo ed op oq">[Fig 3.] Mode loss in Encoder with a uniform latent distribution and with a distribution with disconnected modes.</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b2e5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">BIGAN (Combined training of Encoder &amp; Generator)</em></strong></p><p id="cc79" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">BIGAN was introduced by Donahue et al. in 2017. It simultaneously trains a Generator (<em class="nh">G</em>) and an Encoder (<em class="nh">E</em>) with a shared Discriminator (<em class="nh">D</em>). While the Encoder and Generator operate the same as before, the Discriminator takes both, <em class="nh">x</em> and <em class="nh">z</em>, as input and produces a scalar output.</p><p id="2054" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The objective for <em class="nh">D</em> is to assign 1 to the tuples <em class="nh">(x, E(x))</em> and assign 0 to <em class="nh">(G(z), z)</em>. Thus, it tries to establish a boundary between the distributions of <em class="nh">(x, E(x))</em> and <em class="nh">(G(z), z)</em>. The Generator traverses this boundary gradient upwards to generate more samples labeled as 1 by the Discriminator, and the Encoder cascades down this boundary similarly. The objective of <em class="nh">D</em> here is to help the distributions of <em class="nh">(x, E(x))</em> and <em class="nh">(G(z), z)</em> merge.</p><p id="83ff" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So what is the significance of these distributions merging? This can happen only when the distribution of <em class="nh">G(z)</em> matches the data distribution <em class="nh">Pr</em>, and the distribution of <em class="nh">E(x)</em> matches the latent distribution <em class="nh">Pz</em>. Thus, each latent variable maps to an image, and each image is mapped to a latent variable. Another inherent important feature is that this mapping is reversible, ie. <em class="nh">G(E(x))=x</em> and <em class="nh">E(G(z))=z</em>. Please refer to the original paper for more details.</p><p id="9103" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Let’s visualize what it looks like — the Discriminator functions in the joint space of <em class="nh">x</em> and <em class="nh">z</em>. The illustration below shows the starting and equilibrium states of <em class="nh">G</em> and <em class="nh">E,</em> for a <em class="nh">1-</em>dimensional<em class="nh"> X</em> and a <em class="nh">1-</em>dimensional<em class="nh"> Z</em>. <em class="nh">Pz</em> is a uniform distribution and <em class="nh">Pr </em>is a sparse distribution with 5 point modes. Consequently, modes of <em class="nh">Pr</em> (<em class="nh">{x1, x2, x3, x4, x5}</em>)<em class="nh"> </em>appear as ‘spots,’ while the latent variable's distribution appears continuous. The green points represent the <em class="nh">(G(z), z)</em> tuples and the yellow points represent the <em class="nh">(x, E(x))</em> tuples. Modifying <em class="nh">E</em> moves the yellow spots along the <em class="nh">Z-axis,</em> and modifying <em class="nh">G</em> moves the green points along the <em class="nh">X-axis</em>. Thus, for the distributions to match, <em class="nh">E</em> has to spread the yellow points along the <em class="nh">Z-axis</em> to approximate a uniform distribution. And, <em class="nh">G</em> must move the green points horizontally to resemble the distribution of given data, <em class="nh">Pr</em>.</p></div></div><div class="nr"><div class="ab cb"><div class="ll oe lm of ln og cf oh cg oi ci bh"><div class="nm nn no np nq ab ke"><figure class="le nr oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><img src="../Images/f8d3be67ac9edfb91b3b26f37f1dc57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*5uTdWf1U3294IBJAYX_VOg.jpeg"/></div></figure><figure class="le nr oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><img src="../Images/12cfbc3c95830043bbc11d7f77053aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*back7pRO8RIopdewZh5uDQ.jpeg"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx oo ed op oq">[Fig 4.] In the beginning, G maps all values of z to a random x, and E maps all values of x, {x1,x2,x3,x4,x5}, to a random z. At equilibrium, yellow points are spread uniformly along the Z-axis. And the green points align against the possible modes in Pr. The real samples (x, E(x)) are shown ‘concentrated’ because the Encoder's limited capacity cannot spread the point mode. The generations (G(z), z) are shown ‘stringy’ because z is sampled from a continuous distribution.</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk oc"><img src="../Images/98ba74ff29c38d71ed9962bae75e7730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x2z98XYlr_mDk9M4Q9RoeQ.jpeg"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 5.] Notice that had the Generator and Encoder been trained separately using separate Discriminators, this would also have been a valid configuration. This meets the criterion of matching the distributions but does not allow the invertibility of G and E. This is NOT the objective of BIGAN.</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk oc"><img src="../Images/e686ca5f075b04376a2279d9ddd19fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZV359NBllCHRIyset3qMw.jpeg"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 6.] If the data modes are not points but slightly spread, the Encoder can spread them with limited capacity against a uniform distribution. The Generator with limited capacity is continuous; thus, there are some values of z for which G can output intermodal values of the data.</figcaption></figure><p id="4f88" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It’s important to note that <em class="nh">G</em> and <em class="nh">E</em> do not directly interact with each other, but only via <em class="nh">D</em>. As a result, their objectives or loss functions are independent of the other's performance. For example, the Encoder's objective is to make the distribution of <em class="nh">E(x)</em> match <em class="nh">Pz</em> regardless of how <em class="nh">G</em> is performing. This is because in matching the tuples <em class="nh">(x, E(x))</em> with <em class="nh">(G(z), z)</em>, the Encoder has control over <em class="nh">E(x)</em> only, and <em class="nh">E(x)</em> has to match <em class="nh">Pz</em> regardless of <em class="nh">G(z)</em> matching <em class="nh">Pr</em>. The same argument goes for the Generator. Thus, the Encoder will still perform perfectly for a unimodal distribution.</p><p id="7d60" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">What does the problem of mode loss look like in BIGANs?</em></strong></p><p id="e513" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If the Generator loses the gradients to the weaker modes, they can still be lost, even if they are well Encoded.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk oc"><img src="../Images/00a301c7c01841d53ede934e64059353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NQp3QvrYZvCTRF577TUNQ.jpeg"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 7.] A collapsed Generator that outputs x3 for all values of z.</figcaption></figure><p id="8775" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the illustration above, <em class="nh">G</em> has collapsed to the mode <em class="nh">x3</em>. <em class="nh">G</em> experiences the gradients along the <em class="nh">X-axis</em> to the nearby modes <em class="nh">x2</em> and <em class="nh">x4,</em> shown with blue arrows. However, the distant modes <em class="nh">x1</em> and <em class="nh">x5</em> may get neglected and left behind.</p><p id="d1b1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">Finally, our solution!</em></strong></p><p id="b121" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">An idea was proposed to stabilize Wasserstein GANs by Gulrajani et al. in the paper ‘Improved Training of Wasserstein GANs’. Since the Discriminator in WGANs is unbounded, the loss can spike if it is not regularized. This can be seen in the loss equation via expansion using the chain rule again.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk or"><img src="../Images/2487b0cedc533a62c228076c750bd49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkMTipHJNDTbDcdg8i8QCA.png"/></div></div></figure><p id="f302" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Here the term <em class="nh">∂D/∂G</em> should always be finite or, <em class="nh">D(x)</em> should be differentiable everywhere wrt <em class="nh">x</em>. The original method placed a bound on the weights to achieve this. However, Gulrajani et al. suggested placing a penalty on the gradients directly via an additional loss for the Discriminator. For this, points were randomly sampled between the real and generated samples from the current batch. And the magnitude of the gradients, <em class="nh">∂D/∂x</em>, at those points was forced to be 1 via a mean squared loss.</p><p id="2c53" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The message to take away was that modeling the Discriminator landscape directly is also a viable solution. Inspired by the technique to directly model the landscape of the Discriminator, we can use something similar. Let’s have a look at <em class="nh">Fig 7</em> again.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk oc"><img src="../Images/359ea2cd16720070cbc966ec91005ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwpJrv5rwqY2ql0Dl-c6iQ.jpeg"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 8.] Here the generated points {g1, g2, g3, g4, g5} were supposed to reach the marked modes but failed because of missing gradients.</figcaption></figure><p id="eb76" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The points <em class="nh">{g1, g2, g3, g4, g5}</em> are the generations <em class="nh">G(z)</em> for the encodings <em class="nh">E(x)</em> of the data points in <em class="nh">{x1, x2, x3, x4, x5}</em> respectively or, <em class="nh">gi = G(E(xi))</em>. These are the reconstructions of the points <em class="nh">xi</em>. We need to model gradients <em class="nh">∂D/∂x</em> such that the points <em class="nh">gi </em>start moving towards their respective target points <em class="nh">xi</em>.</p><p id="3b25" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To do this, we sample some points uniformly along the line segments connecting <em class="nh">xi</em> to their reconstructions <em class="nh">gi</em>. We then force the gradients <em class="nh">∂D/∂x</em> at all those points to be unity and directed towards <em class="nh">xi</em> via a mean squared error. We call this pair-wise gradient penalty, and it is added as an additional loss for the Discriminator.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk os"><img src="../Images/4cf992bc3ad4058976fa95771299007d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZ2Gk3Sq83vdjpSn-LXf0A.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">The first term in the loss is the unit vector pointing in the right direction, and the second term is the gradient of the discriminator wrt x at the sampled point. [Note: gi are referred to as x-hat here.]</figcaption></figure><p id="a9ef" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">One might consider using the mean squared error between <em class="nh">xi</em> and its reconstruction <em class="nh">gi</em> as an additional loss term for the Generator, aiming for a similar effect. However, we found it difficult to balance the reconstruction loss with the adversarial loss for the Generator. This is because the adversarial and reconstruction losses are completely different in behavior and scale, making it difficult to find a constant weight that balances them effectively across datasets, toy and real. In contrast, the gradient penalty does not constrain <em class="nh">D(x)</em> directly but only <em class="nh">∂D/∂x</em>; thus, it is not a directly competing objective for the adversarial loss and only has a regularizing effect. We found a single constant (<strong class="mk fr">λ</strong>=1) to work in all cases.</p><p id="d328" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">Does it work?</em></strong></p><p id="7f08" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We train simple networks like DCGAN and MLPs with different losses. We use toy datasets to visualize the solution better and use an image dataset with a heavy central mode to check mode loss.</p><p id="fc85" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">A. Toy Dataset<br/></strong>We synthesize (2-dim <em class="nh">X</em> and 1-dim <em class="nh">Z</em>) datasets with multiple sparse modes using a mixture of Normal distributions. These modes are arranged in circles and girds. It can be seen that the default BIGAN easily misses modes, but our method captured all modes in all cases.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ot"><img src="../Images/be4a97655b354a732af0d37a23ea12e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02gTg3QVo9DXT8PnOqvUqw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 9.] Results from training a BIGAN network using the original method and our proposal on a toy dataset with sparse modes. The first column shows results from the original GAN and the second from our proposal.</figcaption></figure><p id="49b5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">B. Heavy central mode<br/></strong>We extracted snapshots at regular intervals from footage of a traffic intersection (ref. [5]). The background remains static, and there is very little activity at certain times at certain locations in the frame. The dataset has a huge mode as the background only, without vehicles. While the original GAN and WGAN fail consistently at the task, our method shows significant learning.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ou"><img src="../Images/8d1804e00fef4e95b26d9b7f9ecb9fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY-0frUUk1YMkroFLluEhA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 10.] Generations and Reconstructions from the original GAN. Notice it collapses to the most frequent sample.</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ov"><img src="../Images/b077956a40426e862f44fe9289a909dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43QmnhK955buiFf5N82X_A.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 11.] Generations and Reconstructions from our method. The generator can capture the minor modes.</figcaption></figure><p id="a242" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">C. Latent interpolations<br/></strong>We also tested our method with the CelebA face dataset and found that the model learned minor features that occurred only in some frames like hats, glasses, extreme face angles, etc. Please refer to the paper for the complete results.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ow"><img src="../Images/92eb174a03a9493a2161a7cb80c67eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_zl45FJ1Hb5aqikY8fG6Q.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">[Fig 12.] Generations from interpolations in the latent space.</figcaption></figure><p id="25d8" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">Try it out</em></strong></p><p id="1da6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">For those using a BIGAN or any other method where <em class="nh">E</em> and <em class="nh">G</em> are invertible, feel free to try it out. Just add the output of the following function to the Discriminator loss. The approach should work for all network architectures. As for others using traditional GANs, BIGANs could be a valuable consideration.</p><pre class="nm nn no np nq ox oy oz bp pa bb bk"><span id="1fc3" class="pb pc fq oy b bg pd pe l pf pg">def gradient_penalty(x, z, x_hat, discriminator):<br/>    """<br/>    Computes the pair-wise gradient penalty loss for a BIGAN.<br/>    <br/>    Args:<br/>        x: Samples from the real data.<br/>        z: Samples from encoded latent distribution (= Enc(x)).<br/>        x_hat: The reconstruction of the real samples (= G(E(x)))<br/>        discriminator: The discriminator model with signature (x,z).<br/>    Returns:<br/>        gp_loss: Computed per example loss.<br/>    """<br/>    # Assuming only 1st dimension is the batch dimension.<br/>    num_batch_dims = 1<br/>    epsilon = tf.reshape(tf.random.uniform(shape=x.shape[:num_batch_dims]), x.shape[:num_batch_dims] + [1] * (len(x.shape) - num_batch_dims))<br/>    # Compute interpolations.<br/>    x_inter = (epsilon * x) + ((1. - epsilon) * x_hat)<br/>    x_inter = tf.stop_gradient(x_inter)<br/>    z = tf.stop_gradient(z)<br/>    with tf.GradientTape(watch_accessed_variables=False) as tape:<br/>        tape.watch(x_inter)<br/>        # Compute discriminator values for the interpolations.<br/>        d_inter = discriminator(x_inter, z)<br/>    # Compute gradients at the interpolations.<br/>    d_inter_grads = tape.gradient(d_inter, x_inter)<br/>    # Compute the unit vector in the direction (x - x_hat).<br/>    delta = x - x_hat<br/>    unit_delta = delta / tf.norm(delta, axis=-1, keepdims=True)<br/>    # Compute loss as the mse between gradients and the unit vector.<br/>    return tf.reduce_mean((d_inter_grads - unit_delta)**2, -1)</span></pre><p id="baca" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">Conclusion</em></strong></p><p id="4e8c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If the Encoder and Discriminator have enough capacity, the Encoder can map any distribution to a unimodal latent distribution accurately. When this is achieved (and the Generator and Encoder are invertible), the Generator can also learn the real distribution perfectly via pair-wise gradient penalty. The penalty effectively regularizes the Discriminator, eliminating the need to balance the three networks. The method benefits from increasing the capacity of any one of the networks independently.</p><p id="6982" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">I hope this helps people get insights into GANs and maybe help with mode loss :)</p><p id="4bb4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr"><em class="nh">References</em></strong></p><p id="b1ce" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="nh">[Note: Unless otherwise noted, all images are by the author]</em></p><p id="7275" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">[1] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y., 2014. Generative adversarial nets. <em class="nh">Advances in neural information processing systems</em>, <em class="nh">27</em>.<br/>[2] Arjovsky, M., Chintala, S. and Bottou, L., 2017, July. Wasserstein generative adversarial networks. In International conference on machine learning (pp. 214–223). PMLR.<br/>[3] Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V. and Courville, A.C., 2017. Improved training of wasserstein gans. <em class="nh">Advances in neural information processing systems</em>, <em class="nh">30</em>.<br/>[4] Donahue, J., Krähenbühl, P. and Darrell, T., 2017. Adversarial Feature Learning. In: 5th International Conference on Learning Representations (ICLR), Toulon, France, 24–26 April 2017.<br/>[5] (<em class="nh">Traffic dataset</em>): Varadarajan, J. and Odobez, J.M., 2009, September. Topic models for scene analysis and abnormality detection. In <em class="nh">2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops</em> (pp. 1338–1345). IEEE.</p></div></div></div></div>    
</body>
</html>