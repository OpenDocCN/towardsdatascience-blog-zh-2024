<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering Sensor Fusion: LiDAR Obstacle Detection with KITTI Data — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering Sensor Fusion: LiDAR Obstacle Detection with KITTI Data — Part 1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25">https://towardsdatascience.com/sensor-fusion-kitti-lidar-based-obstacle-detection-part-1-9c5f4bc8d497?source=collection_archive---------4-----------------------#2024-12-25</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ed22" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to use Lidar data for obstacle detection with unsupervised learning</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Erol Çıtak" class="l ep by dd de cx" src="../Images/621dc247f6fdbf5e25b74fdc07d25f7b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*U9z9_D3Q1533vFxW9DsoCQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@eroltak?source=post_page---byline--9c5f4bc8d497--------------------------------" rel="noopener follow">Erol Çıtak</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9c5f4bc8d497--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 25, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="la kj lb lc ab q ee" aria-label="responses" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="la ee ko"><path fill-rule="evenodd" d="M18.472 20.272c-1.194 0-2.335-.355-3.39-1.052a8 8 0 0 1-.676-.499 8.7 8.7 0 0 1-2.284.307C7.638 19.028 4 15.668 4 11.54c.001-4.148 3.64-7.508 8.123-7.508 4.475 0 8.118 3.36 8.118 7.49 0 1.977-.816 3.826-2.307 5.231.022.163.058.336.114.528.184.662.515 1.325.985 1.958.144.202.165.461.055.672a.66.66 0 0 1-.588.365h-.029z" clip-rule="evenodd"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="ld k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al le an ao ap id lf lg lh" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep li cn"><div class="l ae"><div class="ab cb"><div class="lj lk ll lm ln lo ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al le an ao ap id lp lq lr ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al le an ao ap id lp lq lr ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al le an ao ap id lp lq lr ls lt lu lv lw s lx ly lz ma mb mc md u me mf mg"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="220e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><strong class="mj fr">Sensor fusion, multi-modal perception, autonomous vehicles</strong> — if these keywords pique your interest, this Medium blog is for you. Join me as I explore the fascinating world of LiDAR and color image-based environment understanding, showcasing how these technologies are combined to enhance obstacle detection and decision-making for autonomous vehicles. This blog and the following series dive into practical implementations and theoretical insights, offering an engaging read for all curious eyes.</p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="115e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In this Medium blog series, we will examine the KITTI 3D Object Detection dataset [1][3] in three distinct parts. In the first article, which is this one, we will be talking about the KITTI Velodyne Lidar sensor and single-mode obstacle detection with this sensor only. In the second article of the series, we will be working on detection studies on color images with a uni-modal approach. In the last article of the series, we will work on multi-modal object detection, which can be called sensor fusion. During that process, both the Lidar and the Color Image sensors come into play to work together.</p><p id="e577" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">One last note before we get into the topic! I promise that I will provide all the theoretical information about each subtopic at a basic level throughout this series :) However, I will also be leaving very high-quality references for each subtopic without forgetting those who want more in-depth information.</p><h1 id="6471" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Introduction</h1><p id="a386" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">KITTI or KITTI Vision Benchmark Suite is a project created in collaboration with Karlsruhe Institute of Technology and Toyota Research Institute. We can say that it is a platform that includes many different test scenarios, including 2D/3D object detection, multi-object tracking, semantic segmentation, and so forth.</p><p id="a4b0" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For 3D object detection, which is the subject of this article series, there are 7481 training and 7518 test data from different sensors, which are Velodyne Lidar Sensor and Stereo Color Image Sensors.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on oo"><img src="../Images/093310bd95e8a099953e0ee4c0850ee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T9ckW3mjIxbOTMtuDPP7VQ.jpeg"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample image for 3D Object Detection [3] (Image Taken from <a class="af pf" href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" rel="noopener ugc nofollow" target="_blank">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d</a>)</figcaption></figure><p id="9ca0" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In this blog post, we will perform obstacle detection using Velodyne Lidar point clouds. In this context, reading point clouds, visualization, and segmentation with the help of unsupervised machine learning algorithms will be the main topics. In addition to these, we will talk a lot about camera calibration and its internal and external parameters, the RANSAC algorithm for vehicle path detection, and basic evaluation metrics to measure the performance of the outputs that we will need while performing these steps.</p><blockquote class="pg ph pi"><p id="b9b7" class="mh mi pj mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Also, I will be using Python language throughout this series, but don’t worry, I will share with you the information about the virtual environment I use. This way, you can quickly get your own environment up and running. Please check the Github repo to get the <strong class="mj fr">requirements.txt</strong> file.</p></blockquote><h2 id="6be8" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk"><strong class="al">Problem Definition</strong></h2><p id="aadb" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">The main goal of this blog post is to detect obstacles in the environment detected by the sensor using the unsupervised learning method on point clouds obtained with the Veloydne Lidar in the KITTI dataset.</p><p id="f439" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Within this scope, I am sharing an example Lidar point cloud image below to visualize the problem. If we analyze the following sample point cloud, we can easily recognize some cars at the left bottom or some other objects on the road.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on qb"><img src="../Images/31b4f75729eac8a14edf8da70fe4eb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIr5op8vsADH8U4wtIi2AQ.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample Lidar point cloud [3] ( from KITTI dataset)</figcaption></figure><p id="ef23" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">To make it more visible, let me draw some arrows and boxes to show them. In the following image, red arrows indicate cars, orange arrows stand for pedestrians, and red boxes are drawn for street lambs.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on qc"><img src="../Images/a5609b489f7b3e2806d3169d93368ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJSmTkXHWC7YfgbmB8EGpg.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample Lidar point cloud [3] ( from KITTI dataset)</figcaption></figure><p id="35be" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Then, you may wonder and ask this question <strong class="mj fr"><em class="pj">“Wouldn’t we also say that there are other objects around, perhaps walls or trees?” </em></strong>The answer is YES! The proof of my answer can be obtained from the color image corresponding to this point cloud. As can be seen from the image below, there are people, a car, street lights, and trees on the scene.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on qd"><img src="../Images/71fa96542279146571ee9bc73e0227d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*5foIyWJbRyjfBaFW_BqTZA.png"/></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample color image [3] ( from KITTI dataset)</figcaption></figure><p id="00fc" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">After this visual analysis, we come to a subject that careful readers will immediately notice. While the Lidar point cloud provides a 360-degree view of the scene, color image only provides a limited wide perception of the scene. The following blog will be taking only this colored image into consideration for object detection and the last one will try to fuse Lidar point cloud and color image sensors to handle the problem <strong class="mj fr"><em class="pj">(I hope they will be available soon!)</em></strong></p><h2 id="04ec" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Sensor Setup</h2><p id="424c" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Then let’s talk about the sensors and their installations and so on. The KITTI 3D object detection dataset was collected using a specially modified Volkswagen Passat B6. Data recording was handled by an eight-core i7 computer with a RAID system, running Ubuntu Linux alongside a real-time database for efficient data management.</p><p id="d1d1" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The following sensors were used for data collection:</p><ul class=""><li id="c66e" class="mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc qe qf qg bk"><strong class="mj fr">Inertial Navigation System (GPS/IMU):</strong> OXTS RT 3003</li><li id="270b" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk"><strong class="mj fr">Lidar Sensor:</strong> Velodyne HDL-64E</li><li id="e4c6" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk"><strong class="mj fr">Grayscale Cameras:</strong> Two Point Grey Flea 2 (FL2–14S3M-C), each with 1.4 Megapixels</li><li id="a538" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk"><strong class="mj fr">Color Cameras:</strong> Two Point Grey Flea 2 (FL2–14S3C-C), each with 1.4 Megapixels</li><li id="3a0b" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk"><strong class="mj fr">Varifocal Lenses:</strong> Four Edmund Optics NT59–917 (4–8 mm)</li></ul><p id="0db1" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The visualization of the aforementioned setup is presented in the following figure.</p></div></div><div class="ou"><div class="ab cb"><div class="lj qm lk qn ll qo cf qp cg qq ci bh"><figure class="op oq or os ot ou qs qt paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on qr"><img src="../Images/9a76a762e1083e1a660dde606ddf0009.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*J7Mu6drn_781Qp_KyYVNvA.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">KITTI dataset setup visualization [3] ( Image Taken from KITTI)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8bf4" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The Velodyne Lidar sensor and the Color cameras are installed on top of the car but their height from the ground and their coordinates are different than each other. No worries! As promised, we will go step by step. It means that, before getting the core of the algorithm of this blog post, we need to revisit the camera calibration topic first!</p><h2 id="baf1" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Camera Calibration</h2><p id="78f5" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Cameras, or sensors in a broader sense, provide perceptual outputs of the surrounding environment in different ways. In this concept, let’s take an RGB camera, it could be your webcam or maybe a professional digital compact camera. It projects 3D points in the world onto a 2D image plane using two sets of parameters; the intrinsic and extrinsic parameters.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on qu"><img src="../Images/609246ec696c69c18964161d7ba54c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*TZm3TXySEjKHd75cZ5Slcg.png"/></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">Projection of 3D points in the world to the 2D image plane ( Image taken from: <a class="af pf" href="https://de.mathworks.com/help/vision/ug/camera-calibration.html" rel="noopener ugc nofollow" target="_blank">https://de.mathworks.com/help/vision/ug/camera-calibration.html</a>)</figcaption></figure><p id="69b6" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">While the extrinsic parameters are about the location and the orientation of the camera in the world frame domain, the intrinsic parameters map the camera coordinates to the pixel coordinates in the image frame.</p><p id="2b79" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In this concept, the camera extrinsic parameters can be represented as a matrix like T = [R | t ] where R stands for the rotation matrix, which is 3x3 and t stands for the translation vector, which is 3x1. As a result, the T matrix is a 3x4 matrix that takes a point in the world and maps it to the ‘camera coordinate’ domain.</p><p id="68cf" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">On the other hand, the camera's intrinsic parameters can be represented as a 3x3 matrix. The corresponding matrix, K, can be given as follows. While fx and fy represent the focal length of the camera, cx and cy stand for principal points, and s indicates the skewness of the pixel.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on qv"><img src="../Images/4a9d8c128ffe6502e63edba0f1a8f298.png" data-original-src="https://miro.medium.com/v2/resize:fit:156/format:webp/1*TFix7qc3QKMm7NOh8vTPFA.png"/></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The camera’s intrinsic parameters</figcaption></figure><p id="56b9" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">As a result, any 3D point can be projectable to the 2D image plane via following complete camera matrix.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on qw"><img src="../Images/dc45b8f23bdd6444edd26494941da738.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RpLdAM4IT3DSrznUMznMLQ.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The complete camera matrix to project a 3D world point into the image plane</figcaption></figure><p id="f8c3" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">I know that camera calibration seems a little bit complicated especially if you encounter it for the first time. But I have searched for some really good references for you. Also, I will be talking about the applied camera calibration operations for our problem in the following sections.</p><p id="590e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">References for the camera calibration topic:</p><p id="7ec9" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><em class="pj">— Carnegie Mellon University, </em><a class="af pf" href="https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf" rel="noopener ugc nofollow" target="_blank"><em class="pj">https://www.cs.cmu.edu/~16385/s17/Slides/11.1_Camera_matrix.pdf</em></a></p><p id="9aac" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><em class="pj">— Columbia University, </em><a class="af pf" href="https://www.youtube.com/watch?v=GUbWsXU1mac" rel="noopener ugc nofollow" target="_blank"><em class="pj">https://www.youtube.com/watch?v=GUbWsXU1mac</em></a></p><p id="d3eb" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><em class="pj">— Camera Calibration Medium Post, </em><a class="af pf" href="https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789" rel="noopener"><em class="pj">https://yagmurcigdemaktas.medium.com/visual-perception-camera-calibration-9108f8be789</em></a></p><h2 id="b3b9" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Dataset Understanding</h2><p id="7764" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">After a couple of terminologies and the required basic theory, now we are able to get into the problem.</p><p id="7346" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">First of all, I highly suggest you download the dataset from here [2] for the following ones;</p><ul class=""><li id="ae80" class="mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc qe qf qg bk">Left Color Images (size is 12GB)</li><li id="8231" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk">Velodyne Point Cloud (size is 29GB)</li><li id="0cc2" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk">Camera Calibration Matrices of the Object Dataset (size is negligible)</li><li id="8635" class="mh mi fq mj b go qh ml mm gr qi mo mp mq qj ms mt mu qk mw mx my ql na nb nc qe qf qg bk">Training Labels (size is negligible)</li></ul><p id="89f9" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The data that we are going to analyze is the ground truth (G.T.)label files. G.T. files are presented in ‘.txt’ format and each object is labeled with 15 different fields. No worries, I prepared a detailed G.T. file read function in my Github repo as follows.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="d996" class="rb nm fq qy b bg rc rd l re rf">def parse_label_file(label_file_path):<br/>    """<br/>    KITTI 3D Object Detection Label Fields:<br/><br/>    Each line in the label file corresponds to one object in the scene and contains 15 fields:<br/><br/>    1. Type (string):<br/>    - The type of object (e.g., Car, Van, Truck, Pedestrian, Cyclist, etc.).<br/>    - "DontCare" indicates regions to ignore during training.<br/><br/>    2. Truncated (float):<br/>    - Value between 0 and 1 indicating how truncated the object is.<br/>    - 0: Fully visible, 1: Completely truncated (partially outside the image).<br/><br/>    3. Occluded (integer):<br/>    - Level of occlusion:<br/>        0: Fully visible.<br/>        1: Partly occluded.<br/>        2: Largely occluded.<br/>        3: Fully occluded (annotated based on prior knowledge).<br/><br/>    4. Alpha (float):<br/>    - Observation angle of the object in the image plane, ranging from [-π, π].<br/>    - Encodes the orientation of the object relative to the camera plane.<br/><br/>    5. Bounding Box (4 floats):<br/>    - (xmin, ymin, xmax, ymax) in pixels.<br/>    - Defines the 2D bounding box in the image plane.<br/><br/>    6. Dimensions (3 floats):<br/>    - (height, width, length) in meters.<br/>    - Dimensions of the object in the 3D world.<br/><br/>    7. Location (3 floats):<br/>    - (x, y, z) in meters.<br/>    - 3D coordinates of the object center in the camera coordinate system:<br/>        - x: Right, y: Down, z: Forward.<br/><br/>    8. Rotation_y (float):<br/>    - Rotation around the Y-axis in camera coordinates, ranging from [-π, π].<br/>    - Defines the orientation of the object in 3D space.<br/><br/>    9. Score (float) [optional]:<br/>    - Confidence score for detections (used for results, not training).<br/><br/>    Example Line:<br/>    Car 0.00 0 -1.82 587.00 156.40 615.00 189.50 1.48 1.60 3.69 1.84 1.47 8.41 -1.56<br/><br/>Notes:<br/>    - "DontCare" objects: Regions ignored during training and evaluation. Their bounding boxes can overlap with actual objects.<br/>    - Camera coordinates: All 3D values are given relative to the camera coordinate system, with the camera at the origin.<br/>    """<br/></span></pre><p id="ed63" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The color images are presented as files in the folder and they can be read easily, which means without any further operations. As a result of this operation, it can be that <strong class="mj fr"><em class="pj"># of training and testing images: 7481 / 7518</em></strong></p><p id="1c58" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The next data that we will be taking into consideration is the calibration files for each scene. As I did before, I prepared another function to parse calibration files as follows.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="68dd" class="rb nm fq qy b bg rc rd l re rf">def parse_calib_file(calib_file_path):<br/>    """<br/>        Parses a calibration file to extract and organize key transformation matrices.<br/>        <br/>        The calibration file contains the following data:<br/>        - P0, P1, P2, P3: 3x4 projection matrices for the respective cameras.<br/>        - R0: 3x3 rectification matrix for aligning data points across sensors.<br/>        - Tr_velo_to_cam: 3x4 transformation matrix from the LiDAR frame to the camera frame.<br/>        - Tr_imu_to_velo: 3x4 transformation matrix from the IMU frame to the LiDAR frame.<br/><br/>        Parameters:<br/>        calib_file_path (str): Path to the calibration file.<br/><br/>        Returns:<br/>        dict: A dictionary where each key corresponds to a calibration parameter <br/>            (e.g., 'P0', 'R0') and its value is the associated 3x4 NumPy matrix.<br/>        <br/>        Process:<br/>        1. Reads the calibration file line by line.<br/>        2. Maps each line to its corresponding key ('P0', 'P1', etc.).<br/>        3. Extracts numerical elements, converts them to a NumPy 3x4 matrix, <br/>        and stores them in a dictionary.<br/><br/>        Example:<br/>        Input file line for 'P0':<br/>        P0: 1.0 0.0 0.0 0.0  0.0 1.0 0.0 0.0  0.0 0.0 1.0 0.0<br/>        Output dictionary:<br/>        {<br/>            'P0': [[1.0, 0.0, 0.0, 0.0],<br/>                [0.0, 1.0, 0.0, 0.0],<br/>                [0.0, 0.0, 1.0, 0.0]]<br/>        }<br/>    """</span></pre><p id="9af8" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The final data is the Velodyne point cloud and they are presented in ‘.bin’ format. In this format, each point cloud line consists of the location of x, y, and z plus the reflectivity score. As before, the corresponding parse function is as follows.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="afc0" class="rb nm fq qy b bg rc rd l re rf">def read_velodyne_bin(file_path):<br/>    """<br/>    Reads a KITTI Velodyne .bin file and returns the point cloud data as a numpy array.<br/><br/>    :param file_path: Path to the .bin file<br/>    :return: Numpy array of shape (N, 4) where N is the number of points,<br/>             and each point has (x, y, z, reflectivity)<br/>                 <br/>    ### For KITTI's Velodyne LiDAR point cloud, the coordinate system used is forward-right-up (FRU).<br/>    KITTI Coordinate System (FRU):<br/>        X-axis (Forward): Points in the positive X direction move forward from the sensor.<br/>        Y-axis (Right): Points in the positive Y direction move to the right of the sensor.<br/>        Z-axis (Up): Points in the positive Z direction move upward from the sensor.<br/><br/>    <br/>    ### Units: All coordinates are in meters (m). A point (10, 5, 2) means:<br/><br/>        It is 10 meters forward.<br/>        5 meters to the right.<br/>        2 meters above the sensor origin.<br/>        Reflectivity: The fourth value in KITTI’s .bin files represents the reflectivity or intensity of the LiDAR laser at that point. It is unrelated to the coordinate system but adds extra context for certain tasks like segmentation or object detection.<br/><br/>        Velodyne Sensor Placement:<br/><br/>        The LiDAR sensor is mounted on a vehicle at a specific height and offset relative to the car's reference frame.<br/>        The point cloud captures objects relative to the sensor’s position.<br/><br/>    """</span></pre><p id="813c" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">At the end of this section, all the required files will be loaded and ready to be used.</p><p id="7386" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For the sample scene, which was presented at the top of this post in the ‘Problem Definition’ section, there are 122794 points in the point cloud.</p><p id="e501" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But since that amount of information could be hard to analyze for some systems in terms of CPU or GPU power, we may want to reduce the number of points in the cloud. To make it possible we can use the “Voxel Downsampling” operation, which is similar to the “Pooling” operation in deep neural networks. Roughly it divides the complete point cloud into a grid of equally sized voxels and chooses a single point from each voxel.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="64e9" class="rb nm fq qy b bg rc rd l re rf">print(f"Points before downsampling: {len(sample_point_cloud.points)} ")<br/>sample_point_cloud = sample_point_cloud.voxel_down_sample(voxel_size=0.2)<br/>print(f"Points after downsampling: {len(sample_point_cloud.points)}")</span></pre><p id="e0ef" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The output of this downsampling looks like this;</p><p id="005f" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Points before downsampling: 122794 <br/>Points after downsampling: 33122</p><p id="a596" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But it shouldn’t be forgotten that reducing the number of points may cause to loss of some information as might be expected. Also, the voxel grid size is a hyper-parameter that we can choose is another crucial thing. Smaller sizes return a high number of points or vice versa.</p><p id="69f7" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But, before getting into the road segmentation by RANSAC, let's quickly re-visit the Voxel Downsampling operation together.</p><h2 id="fb33" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Voxel Downsampling</h2><p id="dcaa" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Voxel Downsampling is a technique to create a downsampled point cloud. It highly helps to reduce some noise and not-required points. It also reduces the required computational power in light of the selected voxel grid size hyperparameter. The visualization of this operation can be given as follows.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on rg"><img src="../Images/ecc3d606cb1364f65fb83effc4485c82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*AuzuTx9takmLmMpGs9uJGA.png"/></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The illustration of Voxel Downsampling (Image taken from <a class="af pf" href="https://www.mdpi.com/2076-3417/14/8/3160" rel="noopener ugc nofollow" target="_blank">https://www.mdpi.com/2076-3417/14/8/3160</a>)</figcaption></figure><p id="479e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Besides that, the steps of this algorithm can be presented as follows.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rh"><img src="../Images/f6eebbf332cd7806759832551cba58c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLebTGa4lsaVSllgu-sFag.png"/></div></div></figure><p id="ac5b" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">To apply this function, we will be using the “open3d” library with a single line;</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="ca2b" class="rb nm fq qy b bg rc rd l re rf">sample_point_cloud = sample_point_cloud.voxel_down_sample(voxel_size=0.2)</span></pre><p id="b99e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In the above single-line code, it can be observed that the voxel size is chosen as 0.2</p><h2 id="64ae" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">RANSAC</h2><p id="a402" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">The next step will be segmenting the largest plane, which is the road for our problem. RANSAC, Random Sample Consensus, is an iterative algorithm and works by randomly sampling a subset of the data points to hypothesize a model and then evaluating its fit to the entire dataset. It aims to find the model that best explains the inliers while ignoring the outliers.</p><p id="2794" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">While the algorithm is highly robust to the extreme outliers, it requires to sample of <em class="pj">n</em> points at the beginning (n=2 for a 2D line or 3 for a 3D plane). Then evaluates the performance of the mathematical equation with respect to it. Then it means;</p><p id="dac3" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">— the chosen points at the beginning are so crucial</p><p id="c01e" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">— the number of iterations to find the best values is so crucial</p><p id="15d4" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">— it may require some computation power, especially for large datasets</p><p id="5139" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But it’s a kind of de-facto operation for many different cases. So first let's visualize the RANSAC to find a 2D line then let me present the key steps of this algorithm.</p><figure class="op oq or os ot ou om on paragraph-image"><div class="om on ri"><img src="../Images/bdb851b12132c171ecc93373df50273f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*Og8LqFss_DcXjmKV.gif"/></div></figure><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rj"><img src="../Images/e9ccf091fbab0098c74cd8a79051d028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrUguJg_lwALPzmqw2q-Jg.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The key steps and working flow of the RANSAC algorithm</figcaption></figure><p id="e31b" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">After reviewing the concept of RANSAC, it is time to apply the algorithm on the point cloud to determine the largest plane, which is a road, for our problem.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="ce7e" class="rb nm fq qy b bg rc rd l re rf"># 3. RANSAC Segmentation to identify the largest plane<br/>plane_model, inliers = sample_point_cloud.segment_plane(distance_threshold=0.3, ransac_n=3, num_iterations=150)<br/><br/>## Identify inlier points -&gt; road<br/>inlier_cloud = sample_point_cloud.select_by_index(inliers)<br/>inlier_cloud.paint_uniform_color([0, 1, 1]) # R, G, B format<br/><br/>## Identify outlier points -&gt; objects on the road<br/>outlier_cloud = sample_point_cloud.select_by_index(inliers, invert=True)<br/>outlier_cloud.paint_uniform_color([1, 0, 0]) # R, G, B format</span></pre><p id="0e80" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The output of this process will show the outside of the road in red and the road will be colored in a mixture of Green and Blue.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rk"><img src="../Images/ed299ba9ce94bff26bece9e1afb7b4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HzbfN-lUab8gVRajU8PDCQ.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The output of the RANSAC algorithm (Image taken from KITTI dataset [3])</figcaption></figure><h2 id="ccdc" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">DBSCAN — a density-based clustering non-parametric algorithm</h2><p id="6875" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">At this stage, the detection of objects outside the road will be performed using the segmented version of the road with RANSAC.</p><p id="236a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In this context, we will be using unsupervised learning algorithms. However, the question that may come to mind here is <strong class="mj fr">“Can’t a detection be made using supervised learning algorithms?” </strong>The answer is very short and clear: Yes! However, since we want to introduce the problem and get a quick result with this blog post, we will continue with DBSCAN, which is a segmentation algorithm in the unsupervised learning domain. If you would like to see the results with a supervised learning-based object detection algorithm on point clouds, please indicate this in the comments.</p><p id="04ba" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Anyway, let’s try to answer these three questions: What is DBSCAN and how does it work? What are the hyper-parameters to consider? How do we apply it to this problem?</p><p id="9dc6" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">DBSCAN also known as a density-based clustering non-parametric algorithm, is an unsupervised clustering algorithm. Even if there are some other unsupervised clustering algorithms, maybe one of the most popular ones is K-Means, DBSCAN is capable of clustering the objects in arbitrary shape while K-Means asumes the shape of the object is spherical. Moreover, probably the most important feature of DBSCAN is that it does not require the number of clusters to be defined/estimated in advance, as in the K-Means algorithm. If you would like to analyze some really good visualizations for some specific problems like “2Moons”, you can visit here: <a class="af pf" href="https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference" rel="noopener ugc nofollow" target="_blank"><em class="pj">https://www.kaggle.com/code/ahmedmohameddawoud/dbscan-vs-k-means-visualizing-the-difference</em></a></p><p id="b7eb" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">DBSCAN works like our eyes. It means it takes the densities of different groups in the data and then makes a decision for clustering. It has two different hyper-parameters: “Epsilon” and “MinimumPoints”. Initially, DBSCAN identifies <em class="pj">core points</em>, which are points with at least a minimum number of neighbors (<em class="pj">minPts</em>) within a specified radius (<em class="pj">epsilon</em>). Clusters are then formed by expanding from these core points, connecting all reachable points within the density criteria. Points that cannot be connected to any cluster are classified as noise. To get in-depth information about this algorithm like ‘Core Point’, ‘Border Point’ and ‘Noise Point’ please visit there: <em class="pj">Josh Starmer, </em><a class="af pf" href="https://www.youtube.com/watch?v=RDZUdRSDOok&amp;t=61s" rel="noopener ugc nofollow" target="_blank"><em class="pj">https://www.youtube.com/watch?v=RDZUdRSDOok&amp;t=61s</em></a></p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rl"><img src="../Images/06993b7230ebdbcb4ca4979cf6964103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DThbuTIUcBnbvY5LpznIA.jpeg"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample clustering result of the DBSCAN algorithm</figcaption></figure><p id="a9ef" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For our problem, while we can use DBSCAN from the SKLearn library, let's use the open3d as follows.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="2e2a" class="rb nm fq qy b bg rc rd l re rf"># 4. Clustering using DBSCAN -&gt; To further segment objects on the road<br/>with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:<br/>    labels = np.array(outlier_cloud.cluster_dbscan(eps=0.45, min_points=10, print_progress=True))</span></pre><p id="e6da" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">As we can see, ‘epsilon’ was chosen as 0.45, and ‘MinPts’ was chosen as 10. A quick comment about these. Since they are hyper-parameters, there are no best “numbers” out there. Unfortunately, it’s a matter of trying and measuring success. But no worries! After you read the last chapter of this blog post, “Evaluation Metrics”, you will be able to measure your algorithm’s performance in total. Then it means you can apply GridSearch <em class="pj">( ref: </em><a class="af pf" href="https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/" rel="noopener ugc nofollow" target="_blank"><em class="pj">https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/</em></a><em class="pj">) </em>to find the best hyper-param pairs!</p><p id="9c6c" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Yep, then let me visualize the output of DBCAN for our point cloud then let's move to the next step!</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rk"><img src="../Images/49c147bb85328c988cd4578517547d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GpBs15b_8TZLCBjYatNnog.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The output of the DBSCAN clustering algorithm (Image taken from KITTI dataset [3])</figcaption></figure><p id="af0a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">To recall, we can see that some of the objects that I first showed and marked by hand are separate and in different colors here! This shows that these objects belong to different clusters (as it should be).</p><h2 id="9dc2" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">G.T. Labels and Their Calibration Process</h2><p id="f496" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Now it’s time to analyze G.T. labels and Calibration files of the KITTI 3D Object Detection benchmark. In the previous section, I shared some tips about them like how to read, how to parse, etc.</p><p id="760b" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But now I want to mention the relation between the G.T. object and the Calibration matrices. First of all, let me share a figure of the G.T. file and the Calibration file side by side.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rm"><img src="../Images/04e7940cc9b9d1ad687b7f4b4bb56eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wgr3djXfEw8AO0GbXIP38w.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample training label file in .txt format</figcaption></figure><p id="f321" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">As we discussed before, the last element of the training label refers to the rotation of the object around the y-axis. The three numbers before the rotation element (1.84, 1.47, and 8.41) stand for the 3D location of the object’s centroid in the camera coordinate system.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rn"><img src="../Images/4ca6a2d76848a8253ad7dbc32e718d6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6u3ll7__X-ncSwDoRH2bQ.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">A sample calibration file in .txt format</figcaption></figure><p id="dd38" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">On the calibration file side; <em class="pj">P0, P1, P2</em>, and <em class="pj">P3</em> are the camera projection matrices for their corresponding cameras. In this blog post, as we indicated before, we are using the ‘Left Color Images’ which is equal to <em class="pj">P2</em>. Also, <em class="pj">R0_rect </em>is a rectification matrix for aligning stereo images. As can be understood from their names, <em class="pj">Tr_velo_to_cam </em>and <em class="pj">Tr_imu_to_velo </em>are transformation matrices that will be used to provide the transition between different coordinate systems. For example, <em class="pj">Tr_velo_to_cam</em> is a transformation matrix converting Velodyne coordinates to the unrectified camera coordinate system.</p><p id="e97a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">After this explanation, I really paid attention to which matrix or which label in the which coordinate system, now we can mention the transformation of G.T. object coordinates to the Velodyne coordinate system easily. It’s a good point to both understand the use of matrices between coordinate systems and evaluate our predicted bounding boxes and G.T. object bounding boxes.</p><p id="9cb0" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">The first thing that we will be doing is computing the G.T. object bounding box in 3D. To do so, you can reach out to the following function in the repo.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="23d3" class="rb nm fq qy b bg rc rd l re rf">def compute_box_3d(obj, Tr_cam_to_velo):<br/>    """<br/>    Compute the 8 corners of a 3D bounding box in Velodyne coordinates.<br/>    Args:<br/>        obj (dict): Object parameters (dimensions, location, rotation_y).<br/>        Tr_cam_to_velo (np.ndarray): Camera to Velodyne transformation matrix.<br/>    Returns:<br/>        np.ndarray: Array of shape (8, 3) with the 3D box corners.<br/>    """</span></pre><p id="6e88" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Given an object’s dimensions (<code class="cx ro rp rq qy b">height</code>, <code class="cx ro rp rq qy b">width</code>, <code class="cx ro rp rq qy b">length</code>) and position (<code class="cx ro rp rq qy b">x, y, z</code>) in the camera coordinate system, this function first rotates the bounding box based on its orientation (<code class="cx ro rp rq qy b">rotation_y</code>) and then computes the corners of the box in 3D space.</p><p id="0cad" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">This computation is based on the transformation that uses a matrix that is capable of transferring any point from the camera coordinate system to the Velodyne coordinate system. But, wait? We don’t have the camera to Velodyne matrix, do we? Yes, we need to calculate it first by taking the inverse of the <em class="pj">Tr_velo_to_cam </em>matrix, which is presented in the calibration files.</p><p id="304a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">No worries, all this workflow is presented by these functions.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="5019" class="rb nm fq qy b bg rc rd l re rf">def transform_points(points, transformation):<br/>    """<br/>    Apply a transformation matrix to 3D points.<br/>    Args:<br/>        points (np.ndarray): Nx3 array of 3D points.<br/>        transformation (np.ndarray): 4x4 transformation matrix.<br/>    Returns:<br/>        np.ndarray: Transformed Nx3 points.<br/>    """</span></pre><pre class="rr qx qy qz bp ra bb bk"><span id="ae04" class="rb nm fq qy b bg rc rd l re rf">def inverse_rigid_trans(Tr):<br/>    """<br/>    Inverse a rigid body transform matrix (3x4 as [R|t]) to [R'|-R't; 0|1].<br/>    Args:<br/>        Tr (np.ndarray): 4x4 transformation matrix.<br/>    Returns:<br/>        np.ndarray: Inverted 4x4 transformation matrix.<br/>    """</span></pre><p id="4713" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In the end, we can easily see the G.T. objects and project them into the Velodyne point cloud coordinate system. Now let's visualize the output and then jump into the evaluation section!</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rs"><img src="../Images/fbaed431d6042557ca876c4ad2910dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nnclCaITZwf3ghwa9i0GsQ.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">The projected G.T. object bounding boxes (Image taken from KITTI dataset [3])</figcaption></figure><p id="0929" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">(I know the green bounding boxes can be a little hard to see, so I added arrows next to them in black.)</p><h2 id="a8df" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Evaluation Metrics</h2><p id="9fd5" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Now we have the predicted bounding boxes by our pipeline and G.T. object boxes! Then let's calculate some metrics to evaluate our pipeline. In order to perform the hyperparameter optimization that we talked about earlier, we must be able to continuously monitor our performance for each parameter group.</p><p id="aa82" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">But before getting into the evaluation metric I need to mention two things. First of all, KITTI has different evaluation criteria for different objects. For example, while a 50% match between the labels produced for pedestrians and G.T. is sufficient, it is 70% for vehicles. Another issue is that while the pipeline we created performs object detection in a 360-degree environment, the KITTI G.T. labels only include the label values ​​of the objects in the viewing angle of the color cameras. Consequently, we can detect more bounding boxes than presented in G.T. label files. So what to do? Based on the concepts I will talk about here, you can reach the final result by carefully analyzing KITTI’s evaluation criteria. But for now, I will not do a more detailed analysis in this section for the continuation posts of this Medium blog post series.</p><p id="523a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">To evaluate the predicted bounding boxes and G.T. bounding boxes, we will be using the TP, FP, and FN metrics.</p><p id="a0ad" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">TP represents the predicted boxes that match with G.T. boxes, FP stands for the predicted boxes that do NOT match with any G.T. boxes, and FN is the condition that there are no corresponding predicted bounding boxes for G.T. bounding boxes.</p><p id="44f3" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">In this context, of course, we need to find a tool to measure how a predicted bounding box and a G.T. bounding box match. The name of our tool is IOU, intersected over union.</p><p id="935f" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">You can easily reach out to the IOU and evaluation functions as follows.</p><pre class="op oq or os ot qx qy qz bp ra bb bk"><span id="659b" class="rb nm fq qy b bg rc rd l re rf">def compute_iou(box1, box2):<br/>    """<br/>    Calculate the Intersection over Union (IoU) between two bounding boxes.<br/>    :param box1: open3d.cpu.pybind.geometry.AxisAlignedBoundingBox object for the first box<br/>    :param box2: open3d.cpu.pybind.geometry.AxisAlignedBoundingBox object for the second box<br/>    :return: IoU value (float)<br/>    """</span></pre><pre class="rr qx qy qz bp ra bb bk"><span id="b817" class="rb nm fq qy b bg rc rd l re rf"># Function to evaluate metrics (TP, FP, FN)<br/>def evaluate_metrics(ground_truth_boxes, predicted_boxes, iou_threshold=0.5):<br/>    """<br/>    Evaluate True Positives (TP), False Positives (FP), and False Negatives (FN).<br/>    :param ground_truth_boxes: List of AxisAlignedBoundingBox objects for ground truth<br/>    :param predicted_boxes: List of AxisAlignedBoundingBox objects for predictions<br/>    :param iou_threshold: IoU threshold for a match<br/>    :return: TP, FP, FN counts<br/>    """<br/></span></pre><p id="5e5a" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Let me finalize this section by giving predicted bounding boxes (RED) and G.T. bounding boxes (GREEN) over the point cloud.</p><figure class="op oq or os ot ou om on paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="om on rk"><img src="../Images/35cd707ba8068c92b4b7499567027efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bE9qU-mslvKufym5A6Pggw.png"/></div></div><figcaption class="pa pb pc om on pd pe bf b bg z dx">Predicted bounding boxes and G.T. bounding boxes are presented together on the point cloud (Image taken from KITTI dataset [3])</figcaption></figure><h2 id="950c" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Conclusion</h2><p id="2398" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">Yeah, it’s a little bit long, but we are about to finish it. First, we have learned a couple of things about the KITTI 3D Object Detection Benchmark and some terminology about different topics, like camera coordinate systems and unsupervised learning, etc.</p><p id="6ad5" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">Now interested readers can extend this study by adding a grid search to find the best hyper-param elements. For example, the number of minimum points in segmentation, or maybe the # of iteration RANSAC or the voxel grid size in Voxel Downsampling operation, all could be possible improvement points.</p><h2 id="33fd" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">What’s next?</h2><p id="96e7" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">The next part will be investigating object detection on ONLY Left Color Camera frames. This is another fundamental step of this series cause we will be fusing the Lidar Point Cloud and Color Camera frames in the last part of this blog series. Then we will be able to make a conclusion and answer this question: <em class="pj">“Does Sensor Fusion reduce the uncertainty and improve the performance in KITTI 3D Object Detection Benchmark?”</em></p><blockquote class="pg ph pi"><p id="4278" class="mh mi pj mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><strong class="mj fr">Any comments, error fixes, or improvements are welcome!</strong></p><p id="ebc5" class="mh mi pj mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><strong class="mj fr"><em class="fq">Thank you all and I wish you healthy days.</em></strong></p></blockquote><p id="dc07" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">********************************************************************************************************************************************************</p><p id="b7d9" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><strong class="mj fr"><em class="pj">Github link</em></strong>: <a class="af pf" href="https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main" rel="noopener ugc nofollow" target="_blank">https://github.com/ErolCitak/KITTI-Sensor-Fusion/tree/main/lidar_based_obstacle_detection</a></p><p id="6375" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk"><strong class="mj fr">References</strong></p><p id="22bf" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">[1] — <a class="af pf" href="https://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank">https://www.cvlibs.net/datasets/kitti/</a></p><p id="dac9" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">[2] — <a class="af pf" href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" rel="noopener ugc nofollow" target="_blank">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d</a></p><p id="ccbc" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">[3] — Geiger, Andreas, et al. “Vision meets robotics: The kitti dataset.” <em class="pj">The International Journal of Robotics Research</em> 32.11 (2013): 1231–1237.</p><h2 id="8a6d" class="pk nm fq bf nn pl pm pn nq po pp pq nt mq pr ps pt mu pu pv pw my px py pz qa bk">Disclaimer</h2><p id="1e0b" class="pw-post-body-paragraph mh mi fq mj b go oh ml mm gr oi mo mp mq oj ms mt mu ok mw mx my ol na nb nc fj bk">The images used in this blog series are taken from the KITTI dataset for education and research purposes. If you want to use it for similar purposes, you must go to the relevant website, approve the intended use there, and use the citations defined by the benchmark creators as follows.</p><p id="0015" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For the <strong class="mj fr">stereo 2012</strong>, <strong class="mj fr">flow 2012</strong>, <strong class="mj fr">odometry</strong>, <strong class="mj fr">object detection,</strong> or <strong class="mj fr">tracking benchmarks</strong>, please cite:<br/>@inproceedings{<a class="af pf" href="https://www.cvlibs.net/publications/Geiger2012CVPR.pdf" rel="noopener ugc nofollow" target="_blank">Geiger2012CVPR</a>,<br/> author = {<a class="af pf" href="https://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank">Andreas Geiger</a> and <a class="af pf" href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" rel="noopener ugc nofollow" target="_blank">Philip Lenz</a> and <a class="af pf" href="http://ttic.uchicago.edu/~rurtasun" rel="noopener ugc nofollow" target="_blank">Raquel Urtasun</a>},<br/> title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},<br/> booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},<br/> year = {2012}<br/>}</p><p id="5eec" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For the <strong class="mj fr">raw dataset</strong>, please cite:<br/>@article{<a class="af pf" href="https://www.cvlibs.net/publications/Geiger2013IJRR.pdf" rel="noopener ugc nofollow" target="_blank">Geiger2013IJRR</a>,<br/> author = {<a class="af pf" href="https://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank">Andreas Geiger</a> and <a class="af pf" href="http://www.mrt.kit.edu/mitarbeiter_lenz.php" rel="noopener ugc nofollow" target="_blank">Philip Lenz</a> and <a class="af pf" href="http://www.mrt.kit.edu/mitarbeiter_stiller.php" rel="noopener ugc nofollow" target="_blank">Christoph Stiller</a> and <a class="af pf" href="http://ttic.uchicago.edu/~rurtasun" rel="noopener ugc nofollow" target="_blank">Raquel Urtasun</a>},<br/> title = {Vision meets Robotics: The KITTI Dataset},<br/> journal = {International Journal of Robotics Research (IJRR)},<br/> year = {2013}<br/>}</p><p id="dee7" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For the <strong class="mj fr">road benchmark</strong>, please cite:<br/>@inproceedings{<a class="af pf" href="https://www.cvlibs.net/publications/Fritsch2013ITSC.pdf" rel="noopener ugc nofollow" target="_blank">Fritsch2013ITSC</a>,<br/> author = {Jannik Fritsch and Tobias Kuehnl and <a class="af pf" href="https://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank">Andreas Geiger</a>},<br/> title = {A New Performance Measure and Evaluation Benchmark for Road Detection Algorithms},<br/> booktitle = {International Conference on Intelligent Transportation Systems (ITSC)},<br/> year = {2013}<br/>}</p><p id="e133" class="pw-post-body-paragraph mh mi fq mj b go mk ml mm gr mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc fj bk">For the <strong class="mj fr">stereo 2015</strong>, <strong class="mj fr">flow 2015,</strong> and <strong class="mj fr">scene flow 2015 benchmarks</strong>, please cite:<br/>@inproceedings{<a class="af pf" href="https://www.cvlibs.net/publications/Menze2015CVPR.pdf" rel="noopener ugc nofollow" target="_blank">Menze2015CVPR</a>,<br/> author = {<a class="af pf" href="http://www.ipi.uni-hannover.de/tmm.html" rel="noopener ugc nofollow" target="_blank">Moritz Menze</a> and <a class="af pf" href="https://www.cvlibs.net/" rel="noopener ugc nofollow" target="_blank">Andreas Geiger</a>},<br/> title = {Object Scene Flow for Autonomous Vehicles},<br/> booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},<br/> year = {2015}<br/>}</p></div></div></div></div>    
</body>
</html>