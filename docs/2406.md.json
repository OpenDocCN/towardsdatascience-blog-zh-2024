["```py\ndef initialize_Q(self):\n    # Initialize the Q-table as a dictionary\n    Q = {}\n    for alpha in range(self.user_capacity + 1):\n        for beta in range(self.user_capacity + 1 - alpha):\n        state = (alpha, beta)\n        Q[state] = {}\n        max_action = self.user_capacity - (alpha + beta)\n        for action in range(max_action + 1):\n            Q[state][action] = np.random.uniform(0, 1)  # Small random values\n    return Q\n```", "```py\ndef update_Q(self, state, action, reward, next_state):\n        # Update the Q-table based on the state, action, reward, and next state\n        best_next_action = max(self.Q[next_state], key=self.Q[next_state].get)\n\n        # reward + gamma * Q[next_state][best_next_action]\n        td_target = reward + self.gamma * self.Q[next_state][best_next_action]\n\n        # td_target - Q[state][action]\n        td_error = td_target - self.Q[state][action]\n\n        # Q[state][action] = Q[state][action] + alpha * td_error\n        self.Q[state][action] += self.alpha * td_error\n```", "```py\nalpha, beta = state\ninit_inv = alpha + beta\ndemand = np.random.poisson(self.poisson_lambda)\n```", "```py\nnew_alpha = max(0, init_inv - demand)\nholding_cost = -new_alpha * self.holding_cost\nstockout_cost = 0\n\nif demand > init_inv:\n\n    stockout_cost = -(demand - init_inv) * self.stockout_cost\n\nreward = holding_cost + stockout_cost\nnext_state = (new_alpha, action)\n```", "```py\ndef choose_action(self, state):\n\n        # Epsilon-greedy action selection\n    if np.random.rand() < self.epsilon:\n\n          return np.random.choice(self.user_capacity - (state[0] + state[1]) + 1)\n\n    else:\n\n        return max(self.Q[state], key=self.Q[state].get)\n```", "```py\ndef train(self):\n\n        self.Q = self.initialize_Q()  # Reinitialize Q-table for each training run\n\n        for episode in range(self.episodes):\n            alpha_0 = random.randint(0, self.user_capacity)\n            beta_0 = random.randint(0, self.user_capacity - alpha_0)\n            state = (alpha_0, beta_0)\n            #total_reward = 0\n            self.batch = []  # Reset the batch at the start of each episode\n            action_taken = 0\n            while action_taken < self.max_actions_per_episode:\n                action = self.choose_action(state)\n                next_state, reward = self.simulate_transition_and_reward(state, action)\n                self.batch.append((state, action, reward, next_state))  # Collect experience\n                state = next_state\n                action_taken += 1\n\n            self.update_Q(self.batch)  # Update Q-table using the batch\n```", "```py\n# Example usage:\nuser_capacity = 10\npoisson_lambda = 4\nholding_cost = 8\nstockout_cost = 10\ngamma = 0.9\nalpha = 0.1\nepsilon = 0.1\nepisodes = 1000\nmax_actions_per_episode = 1000\n```", "```py\n# Define the Class\nql = QLearningInventory(user_capacity, poisson_lambda, holding_cost, stockout_cost, gamma, \n                        alpha, epsilon, episodes, max_actions_per_episode)\n\n# Train Agent\nql.train()\n\n# Get the Optimal Policy\noptimal_policy = ql.get_optimal_policy()\n```", "```py\n# Create a simple policy\ndef order_up_to_policy(state, user_capacity, target_level):\n    alpha, beta = state\n    max_possible_order = user_capacity - (alpha + beta)\n    desired_order = max(0, target_level - (alpha + beta))\n    return min(max_possible_order, desired_order)\n```", "```py\ndef test_policy(self, policy, episodes):\n        \"\"\"\n        Test a given policy on the environment and calculate the total reward.\n\n        Args:\n            policy (dict): A dictionary mapping states to actions.\n            episodes (int): The number of episodes to simulate.\n\n        Returns:\n            float: The total reward accumulated over all episodes.\n        \"\"\"\n        total_reward = 0\n        alpha_0 = random.randint(0, self.user_capacity)\n        beta_0 = random.randint(0, self.user_capacity - alpha_0)\n        state = (alpha_0, beta_0)  # Initialize the state\n\n        for _ in range(episodes):\n\n            action = policy.get(state, 0)\n            next_state, reward = self.simulate_transition_and_reward(state, action)\n            total_reward += reward\n            state = next_state\n\n        return total_reward\n```"]