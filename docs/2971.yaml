- en: 'Predicted Probability, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ¦‚ç‡è§£é‡Šï¼šå¸¦æœ‰ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—ï¼Œé€‚åˆåˆå­¦è€…
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)
- en: MODEL EVALUATION & OPTIMIZATION
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
- en: 7 basic classifiers reveal their prediction confidence math
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7ç§åŸºæœ¬åˆ†ç±»å™¨æ­ç¤ºå…¶é¢„æµ‹ç½®ä¿¡åº¦çš„æ•°å­¦åŸç†
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    Â·17 min readÂ·Dec 10, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    Â·17åˆ†é’Ÿé˜…è¯»Â·2024å¹´12æœˆ10æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Classification models donâ€™t just tell you what they think the answer is â€” they
    also tell you **how sure** they are about that answer. This certainty is shown
    as a probability score. A high score means the model is very confident, while
    a low score means itâ€™s uncertain about its prediction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†ç±»æ¨¡å‹ä¸ä»…å‘Šè¯‰ä½ å®ƒä»¬è®¤ä¸ºç­”æ¡ˆæ˜¯ä»€ä¹ˆâ€”â€”å®ƒä»¬è¿˜å‘Šè¯‰ä½ **å®ƒä»¬å¯¹è¿™ä¸ªç­”æ¡ˆçš„ç¡®å®šç¨‹åº¦**ã€‚è¿™ç§ç¡®å®šæ€§é€šè¿‡æ¦‚ç‡åˆ†æ•°æ˜¾ç¤ºå‡ºæ¥ã€‚é«˜åˆ†æ„å‘³ç€æ¨¡å‹éå¸¸è‡ªä¿¡ï¼Œè€Œä½åˆ†åˆ™æ„å‘³ç€å®ƒå¯¹é¢„æµ‹çš„ç»“æœä¸ç¡®å®šã€‚
- en: Every classification model calculates these probability scores differently.
    Simple models and complex ones each have their own specific methods to determine
    the likelihood of each possible outcome.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªåˆ†ç±»æ¨¡å‹è®¡ç®—è¿™äº›æ¦‚ç‡åˆ†æ•°çš„æ–¹å¼ä¸åŒã€‚ç®€å•çš„æ¨¡å‹å’Œå¤æ‚çš„æ¨¡å‹å„è‡ªæœ‰è‡ªå·±çš„æ–¹æ³•æ¥ç¡®å®šæ¯ç§å¯èƒ½ç»“æœçš„æ¦‚ç‡ã€‚
- en: Weâ€™re going to explore seven basic classification models and visually break
    down how each one figures out its probability scores. No need for a crystal ball
    â€” weâ€™ll make these probability calculations crystal clear!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¢è®¨ä¸ƒç§åŸºæœ¬çš„åˆ†ç±»æ¨¡å‹ï¼Œå¹¶ç›´è§‚åœ°åˆ†ææ¯ç§æ¨¡å‹æ˜¯å¦‚ä½•è®¡ç®—å…¶æ¦‚ç‡åˆ†æ•°çš„ã€‚æ— éœ€æ°´æ™¶çƒâ€”â€”æˆ‘ä»¬å°†è®©è¿™äº›æ¦‚ç‡è®¡ç®—ä¸€ç›®äº†ç„¶ï¼
- en: '![](../Images/b265adbcd86fb1261e938e663049e715.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b265adbcd86fb1261e938e663049e715.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è§†è§‰æ•ˆæœï¼šä½œè€…ä½¿ç”¨Canva Proåˆ›å»ºï¼Œå·²é’ˆå¯¹ç§»åŠ¨è®¾å¤‡è¿›è¡Œä¼˜åŒ–ï¼›åœ¨æ¡Œé¢è®¾å¤‡ä¸Šå¯èƒ½ä¼šæ˜¾å¾—è¿‡å¤§ã€‚
- en: Definition
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: Predicted probability (or â€œclass probabilityâ€) is a number from 0 to 1 (or 0%
    to 100%) that shows how confident a model is about its answer. If the number is
    1, the model is completely sure about its answer. If itâ€™s 0.5, the model is basically
    guessing â€” itâ€™s like flipping a coin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ¦‚ç‡ï¼ˆæˆ–ç§°â€œç±»åˆ«æ¦‚ç‡â€ï¼‰æ˜¯ä¸€ä¸ªä»0åˆ°1ï¼ˆæˆ–0%åˆ°100%ï¼‰çš„æ•°å€¼ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹å…¶ç­”æ¡ˆçš„ä¿¡å¿ƒæ°´å¹³ã€‚å¦‚æœè¯¥æ•°å€¼ä¸º1ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹å…¶ç­”æ¡ˆéå¸¸ç¡®å®šã€‚å¦‚æœä¸º0.5ï¼Œæ¨¡å‹åŸºæœ¬ä¸Šæ˜¯åœ¨çŒœæµ‹â€”â€”å°±åƒæŠ›ç¡¬å¸ä¸€æ ·ã€‚
- en: Components of a Probability Score
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚ç‡åˆ†æ•°çš„ç»„æˆéƒ¨åˆ†
- en: 'When a model has to choose between two classes (called binary classification),
    three main rules apply:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ¨¡å‹éœ€è¦åœ¨ä¸¤ä¸ªç±»åˆ«ä¹‹é—´åšå‡ºé€‰æ‹©æ—¶ï¼ˆç§°ä¸ºäºŒåˆ†ç±»ï¼‰ï¼Œæœ‰ä¸‰æ¡ä¸»è¦è§„åˆ™é€‚ç”¨ï¼š
- en: The predicted probability must be between 0 and 1
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢„æµ‹æ¦‚ç‡å¿…é¡»ä»‹äº0å’Œ1ä¹‹é—´
- en: The chances of both options happening must add up to 1
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªé€‰é¡¹å‘ç”Ÿçš„æ¦‚ç‡æ€»å’Œå¿…é¡»ç­‰äº1
- en: A higher probability means the model is more sure about its choice
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¾ƒé«˜çš„æ¦‚ç‡æ„å‘³ç€æ¨¡å‹å¯¹å…¶é€‰æ‹©æ›´æœ‰ä¿¡å¿ƒ
- en: '![](../Images/36a6cda6b007e30dbfb6888351b20b6f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36a6cda6b007e30dbfb6888351b20b6f.png)'
- en: For binary classification, when we talk about predicted probability, we usually
    mean the probability of the positive class. A higher probability means the model
    thinks the positive class is more likely, while a lower probability means it thinks
    the negative class is more likely.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œå½“æˆ‘ä»¬è°ˆè®ºé¢„æµ‹æ¦‚ç‡æ—¶ï¼Œé€šå¸¸æ˜¯æŒ‡æ­£ç±»çš„æ¦‚ç‡ã€‚æ›´é«˜çš„æ¦‚ç‡æ„å‘³ç€æ¨¡å‹è®¤ä¸ºæ­£ç±»æ›´æœ‰å¯èƒ½å‘ç”Ÿï¼Œè€Œè¾ƒä½çš„æ¦‚ç‡åˆ™æ„å‘³ç€æ¨¡å‹è®¤ä¸ºè´Ÿç±»æ›´æœ‰å¯èƒ½ã€‚
- en: '![](../Images/dac411b12238417f189b5c813e5b3b67.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dac411b12238417f189b5c813e5b3b67.png)'
- en: To make sure these rules are followed, models use mathematical functions to
    convert their calculations into proper probabilities. Each type of model might
    use different functions, which affects how they express their confidence levels.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºç¡®ä¿è¿™äº›è§„åˆ™å¾—åˆ°éµå®ˆï¼Œæ¨¡å‹ä½¿ç”¨æ•°å­¦å‡½æ•°å°†å…¶è®¡ç®—ç»“æœè½¬æ¢ä¸ºé€‚å½“çš„æ¦‚ç‡ã€‚æ¯ç§ç±»å‹çš„æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„å‡½æ•°ï¼Œè¿™ä¼šå½±å“å®ƒä»¬è¡¨è¾¾ç½®ä¿¡åº¦çš„æ–¹å¼ã€‚
- en: Prediction vs. Probability
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é¢„æµ‹ä¸æ¦‚ç‡
- en: In classification, a model picks the class it thinks will most likely happen
    â€” the one with the highest probability score. But two different models might pick
    the same class while being more or less confident about it. Their predicted probability
    scores tell us how sure each model is, even when they make the same choice.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæ¨¡å‹ä¼šé€‰æ‹©å®ƒè®¤ä¸ºæœ€æœ‰å¯èƒ½å‘ç”Ÿçš„ç±»åˆ«â€”â€”å³å…·æœ‰æœ€é«˜æ¦‚ç‡åˆ†æ•°çš„ç±»åˆ«ã€‚ä½†ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹å¯èƒ½ä¼šé€‰æ‹©ç›¸åŒçš„ç±»åˆ«ï¼ŒåŒæ—¶å®ƒä»¬å¯¹è¯¥ç±»åˆ«çš„ä¿¡å¿ƒç¨‹åº¦å¯èƒ½æœ‰æ‰€ä¸åŒã€‚å®ƒä»¬çš„é¢„æµ‹æ¦‚ç‡åˆ†æ•°å‘Šè¯‰æˆ‘ä»¬æ¯ä¸ªæ¨¡å‹æœ‰å¤šç¡®å®šï¼Œå³ä½¿å®ƒä»¬åšå‡ºäº†ç›¸åŒçš„é€‰æ‹©ã€‚
- en: '![](../Images/003cdef1e79d01f669621bce97b57d07.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/003cdef1e79d01f669621bce97b57d07.png)'
- en: 'These different probability scores tell us something important: even when models
    pick the same class, they might understand the data differently.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¸åŒçš„æ¦‚ç‡åˆ†æ•°å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªé‡è¦çš„äº‹å®ï¼šå³ä½¿æ¨¡å‹é€‰æ‹©äº†ç›¸åŒçš„ç±»åˆ«ï¼Œå®ƒä»¬å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼ç†è§£æ•°æ®ã€‚
- en: One model might be very sure about its choice, while another might be less confident
    â€” even though they made the same prediction.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ¨¡å‹å¯èƒ½å¯¹å…¶é€‰æ‹©éå¸¸ç¡®å®šï¼Œè€Œå¦ä¸€ä¸ªæ¨¡å‹å¯èƒ½ä¿¡å¿ƒè¾ƒå¼±â€”â€”å°½ç®¡å®ƒä»¬åšå‡ºäº†ç›¸åŒçš„é¢„æµ‹ã€‚
- en: ğŸ“Š Dataset Used
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- en: 'To understand how predicted probability is calculated, weâ€™ll continue with
    [the same dataset used in my previous articles on Classification Algorithms](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c).
    Our goal remains: predicting if someone will play golf based on the weather.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£é¢„æµ‹æ¦‚ç‡æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Œæˆ‘ä»¬å°†ç»§ç»­ä½¿ç”¨[æˆ‘ä¹‹å‰å…³äºåˆ†ç±»ç®—æ³•çš„æ–‡ç« ä¸­ä½¿ç”¨çš„ç›¸åŒæ•°æ®é›†](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)ã€‚æˆ‘ä»¬çš„ç›®æ ‡ä»ç„¶æ˜¯ï¼šæ ¹æ®å¤©æ°”é¢„æµ‹æŸäººæ˜¯å¦ä¼šæ‰“é«˜å°”å¤«ã€‚
- en: '![](../Images/1560cdcd385ca4877365575c6c84f8b3.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1560cdcd385ca4877365575c6c84f8b3.png)'
- en: 'Columns: â€˜Overcast (one-hot-encoded into 3 columns)â€™, â€™Temperatureâ€™ (in Fahrenheit),
    â€˜Humidityâ€™ (in %), â€˜Windyâ€™ (Yes/No) and â€˜Playâ€™ (Yes/No, target feature)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—ï¼šâ€˜Overcastï¼ˆé€šè¿‡ä¸‰åˆ—è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼‰â€™ï¼Œâ€˜Temperatureâ€™ï¼ˆæ¸©åº¦ï¼Œå•ä½ä¸ºåæ°åº¦ï¼‰ï¼Œâ€˜Humidityâ€™ï¼ˆæ¹¿åº¦ï¼Œå•ä½ä¸º%ï¼‰ï¼Œâ€˜Windyâ€™ï¼ˆé£ï¼ŒYes/Noï¼‰ä»¥åŠâ€˜Playâ€™ï¼ˆæ˜¯å¦æ‰“çƒï¼Œç›®æ ‡ç‰¹å¾ï¼‰
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As some algorithms might need standardized values, we will also do [standard
    scaling](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    to the numerical features and [one-hot encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    to the categorical features, including the target feature:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæŸäº›ç®—æ³•å¯èƒ½éœ€è¦æ ‡å‡†åŒ–çš„æ•°å€¼ï¼Œæˆ‘ä»¬è¿˜å°†å¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œ[æ ‡å‡†åŒ–ç¼©æ”¾](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)ï¼Œå¹¶å¯¹åˆ†ç±»ç‰¹å¾ï¼ŒåŒ…æ‹¬ç›®æ ‡ç‰¹å¾ï¼Œè¿›è¡Œ[ç‹¬çƒ­ç¼–ç ](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)ï¼š
- en: '![](../Images/0b25c11e730a5be1d37aee6342ef4b31.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b25c11e730a5be1d37aee6342ef4b31.png)'
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, letâ€™s see how each of the following 7 classification algorithms calculates
    these probabilities:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»¥ä¸‹7ç§åˆ†ç±»ç®—æ³•æ˜¯å¦‚ä½•è®¡ç®—è¿™äº›æ¦‚ç‡çš„ï¼š
- en: '![](../Images/85322ccb3ef67016db657d6a2a3c02a2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85322ccb3ef67016db657d6a2a3c02a2.png)'
- en: Dummy Classifier Probabilities
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å“‘å·´åˆ†ç±»å™¨æ¦‚ç‡
- en: '[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Dummy Classifier Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
    [## å“‘å·´åˆ†ç±»å™¨è§£æï¼šå¸¦ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—ï¼Œé€‚åˆåˆå­¦è€…'
- en: Setting the Bar in Machine Learning with Simple Baseline Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡ç®€å•çš„åŸºå‡†æ¨¡å‹è®¾å®šæœºå™¨å­¦ä¹ çš„æ ‡å‡†
- en: towardsdatascience.com](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
- en: 'A Dummy Classifier is a prediction model that doesnâ€™t learn patterns from data.
    Instead, it follows basic rules like: picking the most common outcome, making
    random predictions based on how often each outcome appeared in training, always
    picking one answer, or randomly choosing between options with equal chance. The
    Dummy Classifier ignores all input features and just follows these rules.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è™šæ‹Ÿåˆ†ç±»å™¨æ˜¯ä¸€ç§ä¸ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼çš„é¢„æµ‹æ¨¡å‹ã€‚ç›¸åï¼Œå®ƒéµå¾ªä¸€äº›åŸºæœ¬è§„åˆ™ï¼Œä¾‹å¦‚ï¼šé€‰æ‹©æœ€å¸¸è§çš„ç»“æœï¼ŒåŸºäºæ¯ä¸ªç»“æœåœ¨è®­ç»ƒä¸­å‡ºç°çš„é¢‘ç‡åšéšæœºé¢„æµ‹ï¼Œæ€»æ˜¯é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆï¼Œæˆ–åœ¨ç­‰æ¦‚ç‡çš„é€‰é¡¹ä¸­éšæœºé€‰æ‹©ã€‚è™šæ‹Ÿåˆ†ç±»å™¨å¿½ç•¥æ‰€æœ‰è¾“å…¥ç‰¹å¾ï¼Œåªéµå¾ªè¿™äº›è§„åˆ™ã€‚
- en: When this model finishes training, all it remembers is a few numbers showing
    either how often each outcome happened or the constant values it was told to use.
    It doesnâ€™t learn anything about how features relate to outcomes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½çš„åªæ˜¯ä¸€äº›æ•°å­—ï¼Œæ˜¾ç¤ºæ¯ä¸ªç»“æœå‘ç”Ÿçš„é¢‘ç‡ï¼Œæˆ–å®ƒè¢«å‘ŠçŸ¥ä½¿ç”¨çš„å¸¸æ•°å€¼ã€‚å®ƒä¸ä¼šå­¦ä¹ ç‰¹å¾ä¸ç»“æœä¹‹é—´çš„å…³ç³»ã€‚
- en: '![](../Images/6dd5f768865682ac6099b8f627cd9342.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dd5f768865682ac6099b8f627cd9342.png)'
- en: For calculating predicted probability in binary classification, the Dummy Classifier
    uses the most basic approach possible. Since it only remembered how often each
    outcome appeared in the training data, it uses these same numbers as probability
    scores for every prediction â€” either 0 or 1.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè™šæ‹Ÿåˆ†ç±»å™¨ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹æ¦‚ç‡ã€‚ç”±äºå®ƒä»…è®°ä½äº†æ¯ä¸ªç»“æœåœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå®ƒå°±å°†è¿™äº›ç›¸åŒçš„æ•°å­—ä½œä¸ºæ¯æ¬¡é¢„æµ‹çš„æ¦‚ç‡åˆ†æ•°â€”â€”è¦ä¹ˆæ˜¯0ï¼Œè¦ä¹ˆæ˜¯1ã€‚
- en: '![](../Images/7b157b5391af9509372206c4069d9ea0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b157b5391af9509372206c4069d9ea0.png)'
- en: These probability scores stay exactly the same for all new data, because the
    model doesnâ€™t look at or react to any features of the new data itâ€™s trying to
    predict.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¦‚ç‡åˆ†æ•°å¯¹äºæ‰€æœ‰æ–°æ•°æ®æ¥è¯´å®Œå…¨ç›¸åŒï¼Œå› ä¸ºè¯¥æ¨¡å‹å¹¶ä¸ä¼šæŸ¥çœ‹æˆ–å“åº”ä»»ä½•æ–°æ•°æ®çš„ç‰¹å¾ã€‚
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: k-Nearest Neighbors (KNN) Probabilities
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-æœ€è¿‘é‚»ï¼ˆKNNï¼‰æ¦‚ç‡
- en: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
    [## K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples
    for Beginners'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
    [## K æœ€è¿‘é‚»åˆ†ç±»å™¨è¯¦è§£ï¼šé¢å‘åˆå­¦è€…çš„å¯è§†åŒ–æŒ‡å—å’Œä»£ç ç¤ºä¾‹'
- en: The friendly neighbor approach to machine learning
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‹é‚»æ–¹æ³•åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨
- en: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
- en: K-Nearest Neighbors (kNN) is a prediction model that takes a different approach
    â€” instead of learning rules, it keeps all training examples in memory. When it
    needs to make a prediction about new data, it measures how similar this data is
    to every stored example, finds the k most similar ones (where k is a number we
    choose), and makes its decision based on those neighbors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: K-æœ€è¿‘é‚»ï¼ˆkNNï¼‰æ˜¯ä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œå®ƒé‡‡ç”¨äº†ä¸åŒçš„æ–¹æ³•â€”â€”å®ƒä¸æ˜¯å­¦ä¹ è§„åˆ™ï¼Œè€Œæ˜¯å°†æ‰€æœ‰è®­ç»ƒç¤ºä¾‹å­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚å½“å®ƒéœ€è¦å¯¹æ–°æ•°æ®åšå‡ºé¢„æµ‹æ—¶ï¼Œå®ƒä¼šè¡¡é‡è¿™äº›æ•°æ®ä¸æ¯ä¸ªå­˜å‚¨çš„ç¤ºä¾‹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„kä¸ªï¼ˆkæ˜¯æˆ‘ä»¬é€‰æ‹©çš„æ•°å­—ï¼‰ï¼Œå¹¶åŸºäºè¿™äº›é‚»å±…åšå‡ºå†³ç­–ã€‚
- en: When this model finishes training, all it has stored is the complete training
    dataset, the value of k we chose, and a method for measuring how similar two data
    points are (by default using Euclidean distance).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒæ‰€å­˜å‚¨çš„åªæœ‰å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†ã€æˆ‘ä»¬é€‰æ‹©çš„kå€¼ä»¥åŠä¸€ç§è¡¡é‡ä¸¤ä¸ªæ•°æ®ç‚¹ç›¸ä¼¼åº¦çš„æ–¹æ³•ï¼ˆé»˜è®¤ä½¿ç”¨æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰ã€‚
- en: '![](../Images/226337b949ff12aa339c2625a91f47df.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/226337b949ff12aa339c2625a91f47df.png)'
- en: For calculating predicted probability, kNN looks at those k most similar examples
    and counts how many belong to each class. The probability score is simply the
    number of neighbors belonging to a class divided by k.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼ŒkNNä¼šæŸ¥çœ‹é‚£äº›æœ€ç›¸ä¼¼çš„kä¸ªæ ·æœ¬ï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ã€‚æ¦‚ç‡åˆ†æ•°å°±æ˜¯å±äºæŸä¸€ç±»åˆ«çš„é‚»å±…æ•°é‡é™¤ä»¥kã€‚
- en: '![](../Images/d6411613689379bcd1ad1878bbb7005b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6411613689379bcd1ad1878bbb7005b.png)'
- en: Since kNN calculates probability scores by division, it can only give certain
    specific values based on k (say, for k=5, the only possible probability scores
    are 0/5 (0%), 1/5 (20%), 2/5 (40%), 3/5 (60%), 4/5 (80%), and 5/5 (100%)). This
    means kNN canâ€™t give as many different confidence levels as other models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºkNNé€šè¿‡é™¤æ³•è®¡ç®—æ¦‚ç‡åˆ†æ•°ï¼Œå®ƒåªèƒ½æ ¹æ®kç»™å‡ºç‰¹å®šçš„å€¼ï¼ˆä¾‹å¦‚ï¼Œå¯¹äºk=5ï¼Œå”¯ä¸€å¯èƒ½çš„æ¦‚ç‡åˆ†æ•°æ˜¯0/5ï¼ˆ0%ï¼‰ã€1/5ï¼ˆ20%ï¼‰ã€2/5ï¼ˆ40%ï¼‰ã€3/5ï¼ˆ60%ï¼‰ã€4/5ï¼ˆ80%ï¼‰å’Œ5/5ï¼ˆ100%ï¼‰ï¼‰ã€‚è¿™æ„å‘³ç€kNNæ— æ³•åƒå…¶ä»–æ¨¡å‹é‚£æ ·ç»™å‡ºæ›´å¤šçš„ç½®ä¿¡åº¦çº§åˆ«ã€‚
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Naive Bayes Probabilities
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯æ¦‚ç‡
- en: '[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
    [## Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
    [## ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯è§£é‡Šï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹'
- en: Unlocking predictive power through Yes/No probability
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€šè¿‡æ˜¯/å¦æ¦‚ç‡è§£é”é¢„æµ‹èƒ½åŠ›
- en: towardsdatascience.com](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
- en: 'Naive Bayes is a prediction model that uses probability math with a â€œnaiveâ€
    rule: it assumes each feature affects the outcome independently. There are different
    types of Naive Bayes: Gaussian Naive Bayes works with continuous values, while
    Bernoulli Naive Bayes works with binary features. As our dataset has many 0â€“1
    features, weâ€™ll focus on the Bernoulli one here.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§é¢„æµ‹æ¨¡å‹ï¼Œä½¿ç”¨æ¦‚ç‡æ•°å­¦å’Œâ€œæœ´ç´ â€è§„åˆ™ï¼šå®ƒå‡è®¾æ¯ä¸ªç‰¹å¾ç‹¬ç«‹åœ°å½±å“ç»“æœã€‚æœ´ç´ è´å¶æ–¯æœ‰ä¸åŒçš„ç±»å‹ï¼šé«˜æ–¯æœ´ç´ è´å¶æ–¯é€‚ç”¨äºè¿ç»­å€¼ï¼Œè€Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯é€‚ç”¨äºäºŒå…ƒç‰¹å¾ã€‚ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†æœ‰è®¸å¤š
    0 å’Œ 1 ç‰¹å¾ï¼Œä¸‹é¢æˆ‘ä»¬å°†é‡ç‚¹è®²è§£ä¼¯åŠªåˆ©æ¨¡å‹ã€‚
- en: 'When this model finishes training, it remembers probability values: one value
    for how often the positive class occurs, and for each feature, values showing
    how likely different feature values appear when we have a positive outcome.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆæ—¶ï¼Œå®ƒä¼šè®°ä½æ¦‚ç‡å€¼ï¼šä¸€ä¸ªå€¼è¡¨ç¤ºæ­£ç±»å‘ç”Ÿçš„é¢‘ç‡ï¼Œå¯¹äºæ¯ä¸ªç‰¹å¾ï¼Œå€¼è¡¨ç¤ºåœ¨æ­£ç±»ç»“æœä¸‹ä¸åŒç‰¹å¾å€¼å‡ºç°çš„å¯èƒ½æ€§ã€‚
- en: '![](../Images/287032349f3374bb29ca0948ea437c0e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/287032349f3374bb29ca0948ea437c0e.png)'
- en: 'For calculating predicted probability, Naive Bayes multiplies several probabilities
    together: the chance of each class occurring, and the chance of seeing each feature
    value within that class. These multiplied probabilities are then normalized so
    they sum to 1, giving us the final probability scores.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼Œæœ´ç´ è´å¶æ–¯å°†å¤šä¸ªæ¦‚ç‡ç›¸ä¹˜ï¼šæ¯ä¸ªç±»åˆ«å‘ç”Ÿçš„æ¦‚ç‡ï¼Œä»¥åŠåœ¨è¯¥ç±»åˆ«ä¸­è§‚å¯Ÿåˆ°æ¯ä¸ªç‰¹å¾å€¼çš„æ¦‚ç‡ã€‚è¿™äº›ä¹˜ç§¯çš„æ¦‚ç‡éšåä¼šè¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿å…¶å’Œä¸º 1ï¼Œä»è€Œå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡å¾—åˆ†ã€‚
- en: '![](../Images/64912066b621b73326a6d1ba4026bd6c.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64912066b621b73326a6d1ba4026bd6c.png)'
- en: Since Naive Bayes uses probability math, its probability scores naturally fall
    between 0 and 1\. However, when certain features strongly point to one class over
    another, the model can give probability scores very close to 0 or 1, showing itâ€™s
    very confident about its prediction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæœ´ç´ è´å¶æ–¯ä½¿ç”¨æ¦‚ç‡æ•°å­¦ï¼Œå®ƒçš„æ¦‚ç‡å¾—åˆ†è‡ªç„¶è½åœ¨ 0 å’Œ 1 ä¹‹é—´ã€‚ç„¶è€Œï¼Œå½“æŸäº›ç‰¹å¾å¼ºçƒˆæŒ‡å‘æŸä¸€ç±»åˆ«æ—¶ï¼Œæ¨¡å‹å¯èƒ½ä¼šç»™å‡ºéå¸¸æ¥è¿‘ 0 æˆ– 1 çš„æ¦‚ç‡å¾—åˆ†ï¼Œè¡¨æ˜å®ƒå¯¹é¢„æµ‹éå¸¸æœ‰ä¿¡å¿ƒã€‚
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Decision Tree Probabilities
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ¦‚ç‡
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
    [## å†³ç­–æ ‘åˆ†ç±»å™¨è§£é‡Šï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹'
- en: A fresh look on our favorite upside-down tree
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€å–œæ¬¢çš„å€’ç«‹æ ‘çš„æ–°è§†è§’
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
- en: A Decision Tree Classifier works by creating a series of yes/no questions about
    the input data. It builds these questions one at a time, always choosing the most
    useful question that best separates the data into groups. It keeps asking questions
    until it reaches a final answer at the end of a branch.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘åˆ†ç±»å™¨é€šè¿‡é’ˆå¯¹è¾“å…¥æ•°æ®åˆ›å»ºä¸€ç³»åˆ—æ˜¯/å¦é—®é¢˜æ¥å·¥ä½œã€‚å®ƒé€ä¸€æ„å»ºè¿™äº›é—®é¢˜ï¼Œæ€»æ˜¯é€‰æ‹©æœ€æœ‰ç”¨çš„é—®é¢˜ï¼Œèƒ½å¤Ÿæœ€å¥½åœ°å°†æ•°æ®åˆ†æˆä¸åŒçš„ç»„ã€‚å®ƒä¼šä¸æ–­æé—®ï¼Œç›´åˆ°åˆ°è¾¾æ¯ä¸ªåˆ†æ”¯çš„æœ€ç»ˆç­”æ¡ˆã€‚
- en: When this model finishes training, it has created a tree where each point represents
    a question about the data. Each branch shows which way to go based on the answer,
    and at the end of each branch is information about how often each class appeared
    in the training data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆæ—¶ï¼Œå®ƒä¼šåˆ›å»ºä¸€æ£µæ ‘ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå…³äºæ•°æ®çš„é—®é¢˜ã€‚æ¯ä¸ªåˆ†æ”¯è¡¨ç¤ºæ ¹æ®ç­”æ¡ˆåº”é‡‡å–çš„è·¯å¾„ï¼Œè€Œæ¯ä¸ªåˆ†æ”¯çš„æœ«ç«¯åˆ™æ˜¾ç¤ºè¯¥ç±»åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°çš„é¢‘ç‡ã€‚
- en: '![](../Images/9287974aef17afe32b318ba1e212cf2f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9287974aef17afe32b318ba1e212cf2f.png)'
- en: For calculating predicted probability, the Decision Tree follows all its questions
    for new data until it reaches the end of a branch. The probability score is based
    on how many training examples of each class ended up at that same branch during
    training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œå†³ç­–æ ‘ä¼šæ ¹æ®æ–°æ•°æ®ä¾æ¬¡å›ç­”æ‰€æœ‰é—®é¢˜ï¼Œç›´åˆ°åˆ°è¾¾æŸä¸€åˆ†æ”¯çš„æœ«ç«¯ã€‚æ¦‚ç‡åˆ†æ•°åŸºäºæ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒæœŸé—´æœ‰å¤šå°‘è®­ç»ƒæ ·æœ¬æœ€ç»ˆåˆ°è¾¾è¯¥åˆ†æ”¯ã€‚
- en: '![](../Images/e39929dd06dd01db3fe0375771fc6a53.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e39929dd06dd01db3fe0375771fc6a53.png)'
- en: Since Decision Tree probability scores come from counting training examples
    at each branch endpoint, they can only be certain values that were seen during
    training. This means the model can only give probability scores that match the
    patterns it found while learning, which limits how precise its confidence levels
    can be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå†³ç­–æ ‘çš„æ¦‚ç‡åˆ†æ•°æ¥æºäºè®¡æ•°æ¯ä¸ªåˆ†æ”¯ç«¯ç‚¹çš„è®­ç»ƒæ ·æœ¬ï¼Œå› æ­¤å®ƒä»¬åªèƒ½æ˜¯è®­ç»ƒä¸­è§è¿‡çš„ç‰¹å®šå€¼ã€‚è¿™æ„å‘³ç€æ¨¡å‹åªèƒ½ç»™å‡ºä¸å®ƒåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å‘ç°çš„æ¨¡å¼åŒ¹é…çš„æ¦‚ç‡åˆ†æ•°ï¼Œè¿™é™åˆ¶äº†å®ƒçš„ç½®ä¿¡åº¦æ°´å¹³çš„ç²¾ç¡®åº¦ã€‚
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Logistic Regression Probabilities
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’æ¦‚ç‡
- en: '[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
    [## Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
    [## é€»è¾‘å›å½’è§£æï¼šåˆå­¦è€…çš„è§†è§‰æŒ‡å—ä¸ä»£ç ç¤ºä¾‹'
- en: Finding the perfect weights to fit the data in
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°æœ€é€‚åˆæ•°æ®çš„å®Œç¾æƒé‡
- en: towardsdatascience.com](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
- en: A Logistic Regression model, despite its name, predicts between two classes
    using a mathematical equation. For each feature in the input data, it learns how
    important that feature is by giving it a number (weight). It also learns one extra
    number (bias) that helps make better predictions. To turn these numbers into a
    predicted probability, it uses the sigmoid function that keeps the final answer
    between 0 and 1.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡åç§°ä¸ºé€»è¾‘å›å½’ï¼Œé€»è¾‘å›å½’æ¨¡å‹é€šè¿‡ä¸€ä¸ªæ•°å­¦æ–¹ç¨‹æ¥é¢„æµ‹ä¸¤ä¸ªç±»åˆ«ä¹‹é—´çš„æ¦‚ç‡ã€‚å¯¹äºè¾“å…¥æ•°æ®ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œå®ƒé€šè¿‡èµ‹äºˆç‰¹å¾ä¸€ä¸ªæ•°å­—ï¼ˆæƒé‡ï¼‰æ¥å­¦ä¹ è¯¥ç‰¹å¾çš„é‡è¦æ€§ã€‚å®ƒè¿˜ä¼šå­¦ä¹ ä¸€ä¸ªé¢å¤–çš„æ•°å­—ï¼ˆåå·®ï¼‰ï¼Œä»¥å¸®åŠ©åšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚ä¸ºäº†å°†è¿™äº›æ•°å­—è½¬åŒ–ä¸ºé¢„æµ‹æ¦‚ç‡ï¼Œå®ƒä½¿ç”¨äº†ä¸€ä¸ªsigmoidå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æœ€ç»ˆç­”æ¡ˆä¿æŒåœ¨0å’Œ1ä¹‹é—´ã€‚
- en: When this model finishes training, all it remembers is these weights â€” one number
    for each feature, plus the bias number. These numbers are all it needs to make
    predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¯¥æ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒæ‰€è®°ä½çš„åªæ˜¯è¿™äº›æƒé‡â€”â€”æ¯ä¸ªç‰¹å¾çš„ä¸€ä¸ªæ•°å­—ï¼ŒåŠ ä¸Šåå·®å€¼ã€‚è¿™äº›æ•°å­—æ˜¯å®ƒè¿›è¡Œé¢„æµ‹æ‰€éœ€è¦çš„å…¨éƒ¨å†…å®¹ã€‚
- en: '![](../Images/74dcf333c4ede161645adab7e74ed268.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74dcf333c4ede161645adab7e74ed268.png)'
- en: For calculating predicted probability in binary classification, Logistic Regression
    first multiplies each feature value by its weight and adds them all together,
    plus the bias. This sum could be any number, so the model uses the sigmoid function
    to convert it into a probability between 0 and 1.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒåˆ†ç±»ä¸­è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œé€»è¾‘å›å½’é¦–å…ˆå°†æ¯ä¸ªç‰¹å¾å€¼ä¸å…¶æƒé‡ç›¸ä¹˜ï¼Œç„¶åå°†æ‰€æœ‰ç»“æœç›¸åŠ ï¼Œå†åŠ ä¸Šåå·®ã€‚è¿™äº›å’Œå¯èƒ½æ˜¯ä»»ä½•æ•°å­—ï¼Œå› æ­¤æ¨¡å‹ä½¿ç”¨sigmoidå‡½æ•°å°†å…¶è½¬æ¢ä¸º0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ã€‚
- en: '![](../Images/3d83f16c8d25348a9fea8afc8104ffbf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d83f16c8d25348a9fea8afc8104ffbf.png)'
- en: Unlike other models that can only give certain specific probability scores,
    Logistic Regression can give any probability between 0 and 1\. The further the
    input data is from the point where the model switches from one class to another
    (the decision boundary), the closer the probability gets to either 0 or 1\. Data
    points near this switching point get probabilities closer to 0.5, showing the
    model is less confident about these predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åªèƒ½ç»™å‡ºç‰¹å®šæ¦‚ç‡åˆ†æ•°çš„å…¶ä»–æ¨¡å‹ä¸åŒï¼Œé€»è¾‘å›å½’å¯ä»¥ç»™å‡ºä»‹äº0å’Œ1ä¹‹é—´çš„ä»»ä½•æ¦‚ç‡ã€‚è¾“å…¥æ•°æ®è·ç¦»æ¨¡å‹ä»ä¸€ä¸ªç±»åˆ«åˆ‡æ¢åˆ°å¦ä¸€ä¸ªç±»åˆ«çš„ç‚¹ï¼ˆå†³ç­–è¾¹ç•Œï¼‰è¶Šè¿œï¼Œæ¦‚ç‡å°±è¶Šæ¥è¿‘0æˆ–1ã€‚æ¥è¿‘è¿™ä¸ªåˆ‡æ¢ç‚¹çš„æ•°æ®ç‚¹çš„æ¦‚ç‡æ¥è¿‘0.5ï¼Œæ˜¾ç¤ºæ¨¡å‹å¯¹è¿™äº›é¢„æµ‹çš„ä¿¡å¿ƒè¾ƒä½ã€‚
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Support Vector Machine (SVM) Probabilities
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰æ¦‚ç‡
- en: '[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
    [## Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
    [## æ”¯æŒå‘é‡åˆ†ç±»å™¨è§£æï¼šä½¿ç”¨è¿·ä½ 2Dæ•°æ®é›†çš„è§†è§‰æŒ‡å—'
- en: Finding the best â€œlineâ€ to separate the classes? Yeah, sure...
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°æœ€ä½³çš„â€œçº¿â€æ¥åŒºåˆ†ç±»åˆ«ï¼Ÿå½“ç„¶...
- en: towardsdatascience.com](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)'
- en: A Support Vector Machine (SVM) Classifier works by finding the best boundary
    line (or surface) that separates different classes. It focuses on the points closest
    to this boundary (called support vectors). While the basic SVM finds straight
    boundary lines, it can also create curved boundaries using mathematical functions
    called kernels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰åˆ†ç±»å™¨é€šè¿‡å¯»æ‰¾æœ€ä½³è¾¹ç•Œçº¿ï¼ˆæˆ–é¢ï¼‰æ¥åŒºåˆ†ä¸åŒçš„ç±»åˆ«ã€‚å®ƒå…³æ³¨é‚£äº›ç¦»è¾¹ç•Œæœ€è¿‘çš„ç‚¹ï¼ˆå³æ”¯æŒå‘é‡ï¼‰ã€‚è™½ç„¶åŸºæœ¬çš„SVMæ‰¾åˆ°çš„æ˜¯ç›´çº¿è¾¹ç•Œï¼Œä½†å®ƒä¹Ÿå¯ä»¥é€šè¿‡ä½¿ç”¨å«åšæ ¸å‡½æ•°çš„æ•°å­¦å‡½æ•°æ¥åˆ›å»ºå¼¯æ›²çš„è¾¹ç•Œã€‚
- en: 'When this model finishes training, it remembers three things: the important
    points near the boundary (support vectors), how much each point matters (weights),
    and any settings for curved boundaries (kernel parameters). Together, these define
    where and how the boundary separates the classes.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½äº†ä¸‰ä»¶äº‹ï¼šè¾¹ç•Œé™„è¿‘çš„é‡è¦ç‚¹ï¼ˆæ”¯æŒå‘é‡ï¼‰ï¼Œæ¯ä¸ªç‚¹çš„é‡è¦æ€§ï¼ˆæƒé‡ï¼‰ï¼Œä»¥åŠä»»ä½•å…³äºå¼¯æ›²è¾¹ç•Œçš„è®¾ç½®ï¼ˆæ ¸å‡½æ•°å‚æ•°ï¼‰ã€‚è¿™äº›å…±åŒå®šä¹‰äº†è¾¹ç•Œå¦‚ä½•ä»¥åŠåœ¨å“ªé‡Œåˆ†ç¦»å„ä¸ªç±»åˆ«ã€‚
- en: '![](../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png)'
- en: For calculating predicted probability in binary classification, SVM needs an
    extra step because it wasnâ€™t designed to give probability scores. It uses a method
    called Platt Scaling, which adds a Logistic Regression layer to convert distances
    from the boundary into probabilities. These distances go through the sigmoid function
    to get final probability scores.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒåˆ†ç±»ä¸­è®¡ç®—é¢„æµ‹æ¦‚ç‡æ—¶ï¼Œæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰éœ€è¦é¢å¤–çš„æ­¥éª¤ï¼Œå› ä¸ºå®ƒæœ€åˆå¹¶ä¸æ˜¯ä¸ºäº†æä¾›æ¦‚ç‡åˆ†æ•°è€Œè®¾è®¡çš„ã€‚å®ƒä½¿ç”¨ä¸€ç§å«åšPlatt Scalingçš„æ–¹æ³•ï¼Œé€šè¿‡æ·»åŠ ä¸€ä¸ªé€»è¾‘å›å½’å±‚ï¼Œå°†è·ç¦»è¾¹ç•Œçš„è·ç¦»è½¬æ¢ä¸ºæ¦‚ç‡ã€‚è¿™äº›è·ç¦»ç»è¿‡sigmoidå‡½æ•°å¤„ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ¦‚ç‡åˆ†æ•°ã€‚
- en: '![](../Images/b5e58a416fce00dca108a284ebcdeb9a.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b5e58a416fce00dca108a284ebcdeb9a.png)'
- en: Since SVM calculates probabilities this indirect way, the scores show how far
    points are from the boundary rather than true confidence levels. Points far from
    the boundary get probability scores closer to 0 or 1, while points near the boundary
    get scores closer to 0.5\. This means the probability scores are more about location
    relative to the boundary than the modelâ€™s actual confidence in its predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºSVMæ˜¯ä»¥è¿™ç§é—´æ¥çš„æ–¹å¼è®¡ç®—æ¦‚ç‡ï¼Œå¾—åˆ†æ˜¾ç¤ºçš„æ˜¯ç‚¹ç¦»è¾¹ç•Œçš„è·ç¦»ï¼Œè€Œä¸æ˜¯æ¨¡å‹çš„çœŸå®ç½®ä¿¡åº¦ã€‚ç¦»è¾¹ç•Œè¾ƒè¿œçš„ç‚¹ä¼šå¾—åˆ°æ¥è¿‘0æˆ–1çš„æ¦‚ç‡åˆ†æ•°ï¼Œè€Œé è¿‘è¾¹ç•Œçš„ç‚¹åˆ™ä¼šå¾—åˆ°æ¥è¿‘0.5çš„åˆ†æ•°ã€‚è¿™æ„å‘³ç€æ¦‚ç‡åˆ†æ•°æ›´å¤šåœ°åæ˜ äº†ç‚¹ç›¸å¯¹äºè¾¹ç•Œçš„ä½ç½®ï¼Œè€Œéæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„å®é™…ä¿¡å¿ƒã€‚
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Multilayer Perceptron Probabilities
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥æœºæ¦‚ç‡
- en: '[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
    [## Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
    [## å¤šå±‚æ„ŸçŸ¥æœºè¯¦è§£ï¼šå¸¦æœ‰è¿·ä½ 2Dæ•°æ®é›†çš„å¯è§†åŒ–æŒ‡å—'
- en: Dissecting the math (with visuals) of a tiny neural network
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‰–æä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œçš„æ•°å­¦åŸç†ï¼ˆé™„å¸¦å›¾ç¤ºï¼‰
- en: towardsdatascience.com](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)'
- en: A Multi-Layer Perceptron (MLP) Classifier is a type of neural network that processes
    data through several layers of connected nodes (neurons). Each neuron calculates
    a weighted total of its inputs, transforms this number using a function (like
    ReLU), and sends the result to the next layer. For binary classification, the
    last layer uses the sigmoid function to give an output between 0 and 1.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åˆ†ç±»å™¨æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å‡ ä¸ªå±‚çº§çš„è¿æ¥èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰å¤„ç†æ•°æ®ã€‚æ¯ä¸ªç¥ç»å…ƒè®¡ç®—å…¶è¾“å…¥çš„åŠ æƒæ€»å’Œï¼Œä½¿ç”¨ä¸€ä¸ªå‡½æ•°ï¼ˆå¦‚ReLUï¼‰å¯¹è¿™ä¸ªæ•°å€¼è¿›è¡Œè½¬æ¢ï¼Œç„¶åå°†ç»“æœä¼ é€’åˆ°ä¸‹ä¸€å±‚ã€‚å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œæœ€åä¸€å±‚ä½¿ç”¨sigmoidå‡½æ•°è¾“å‡ºä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ã€‚
- en: 'When this model finishes training, it remembers two main things: the connection
    strengths (weights and biases) between neurons in neighboring layers, and how
    the network is structured (how many layers and neurons are in each layer).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è¿™ä¸ªæ¨¡å‹å®Œæˆè®­ç»ƒæ—¶ï¼Œå®ƒè®°ä½äº†ä¸¤ä»¶ä¸»è¦çš„äº‹ï¼šç›¸é‚»å±‚ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥å¼ºåº¦ï¼ˆæƒé‡å’Œåç½®ï¼‰ï¼Œä»¥åŠç½‘ç»œçš„ç»“æ„ï¼ˆæ¯ä¸€å±‚æœ‰å¤šå°‘å±‚å’Œç¥ç»å…ƒï¼‰ã€‚
- en: '![](../Images/ff2e6b7f72a10d9cf94c129377be153e.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff2e6b7f72a10d9cf94c129377be153e.png)'
- en: For calculating predicted probability in binary classification, the MLP moves
    data through its layers, with each layer creating more complex combinations of
    information from the previous layer. The final layer produces a number that the
    sigmoid function converts into a probability between 0 and 1.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œ**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰é€šè¿‡å…¶å±‚æ¬¡å¤„ç†æ•°æ®ï¼Œæ¯ä¸€å±‚éƒ½æ ¹æ®å‰ä¸€å±‚çš„ä¿¡æ¯ç»„åˆå‡ºæ›´å¤æ‚çš„ç‰¹å¾ã€‚æœ€ç»ˆä¸€å±‚ç”Ÿæˆä¸€ä¸ªæ•°å­—ï¼Œé€šè¿‡**Sigmoidå‡½æ•°**å°†å…¶è½¬æ¢ä¸º0åˆ°1ä¹‹é—´çš„æ¦‚ç‡ã€‚
- en: '![](../Images/70865a4ff395a3529685a4435992f71f.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70865a4ff395a3529685a4435992f71f.png)'
- en: The MLP can find more complex patterns in data than many other models because
    it combines features in advanced ways. The final probability score shows how confident
    the network is â€” scores close to 0 or 1 mean the network is very confident about
    its prediction, while scores near 0.5 indicate itâ€™s uncertain.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰èƒ½å¤Ÿæ¯”è®¸å¤šå…¶ä»–æ¨¡å‹æ‰¾åˆ°æ›´å¤æ‚çš„æ•°æ®æ¨¡å¼ï¼Œå› ä¸ºå®ƒä»¥æ›´å…ˆè¿›çš„æ–¹å¼ç»„åˆç‰¹å¾ã€‚æœ€ç»ˆçš„æ¦‚ç‡åˆ†æ•°æ˜¾ç¤ºäº†ç½‘ç»œçš„ç½®ä¿¡åº¦â€”â€”æ¥è¿‘0æˆ–1çš„åˆ†æ•°æ„å‘³ç€ç½‘ç»œå¯¹å…¶é¢„æµ‹éå¸¸æœ‰ä¿¡å¿ƒï¼Œè€Œæ¥è¿‘0.5çš„åˆ†æ•°åˆ™è¡¨ç¤ºç½‘ç»œçš„ä¸ç¡®å®šæ€§è¾ƒå¤§ã€‚'
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Model Comparison
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹æ¯”è¾ƒ
- en: 'To summarize, hereâ€™s how each classifier calculates predicted probabilities:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œä»¥ä¸‹æ˜¯æ¯ä¸ªåˆ†ç±»å™¨è®¡ç®—é¢„æµ‹æ¦‚ç‡çš„æ–¹å¼ï¼š
- en: '**Dummy Classifier**: Uses the same probability scores for all predictions,
    based only on how often each class appeared in training. Ignores all input features.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è™šæ‹Ÿåˆ†ç±»å™¨**ï¼šå¯¹æ‰€æœ‰é¢„æµ‹ä½¿ç”¨ç›¸åŒçš„æ¦‚ç‡åˆ†æ•°ï¼Œè¿™äº›åˆ†æ•°ä»…åŸºäºæ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå¿½ç•¥æ‰€æœ‰è¾“å…¥ç‰¹å¾ã€‚'
- en: '**K-Nearest Neighbors**: The probability score is the fraction of similar neighbors
    belonging to each class. Can only give specific fractions based on k (like 3/5
    or 7/10).'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Kè¿‘é‚»**ï¼šæ¦‚ç‡åˆ†æ•°æ˜¯å±äºæ¯ä¸ªç±»åˆ«çš„ç›¸ä¼¼é‚»å±…çš„æ¯”ä¾‹ã€‚åªèƒ½ç»™å‡ºåŸºäºkçš„ç‰¹å®šæ¯”ä¾‹ï¼ˆä¾‹å¦‚3/5æˆ–7/10ï¼‰ã€‚'
- en: '**Naive Bayes**: Multiplies together the initial class probability and probabilities
    of seeing each feature value, then adjusts the results to add up to 1\. Probability
    scores show how likely features are to appear in each class.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æœ´ç´ è´å¶æ–¯**ï¼šå°†åˆå§‹çš„ç±»æ¦‚ç‡ä¸æ¯ä¸ªç‰¹å¾å€¼çš„æ¦‚ç‡ç›¸ä¹˜ï¼Œç„¶åè°ƒæ•´ç»“æœä½¿å…¶æ€»å’Œä¸º1ã€‚æ¦‚ç‡åˆ†æ•°è¡¨ç¤ºç‰¹å¾åœ¨æ¯ä¸ªç±»ä¸­å‡ºç°çš„å¯èƒ½æ€§ã€‚'
- en: '**Decision Tree**: Gives probability scores based on how often each class appeared
    in the final branches. Can only use probability values that it saw during training.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å†³ç­–æ ‘**ï¼šæ ¹æ®æ¯ä¸ªç±»åˆ«åœ¨æœ€ç»ˆåˆ†æ”¯ä¸­å‡ºç°çš„é¢‘ç‡æ¥ç»™å‡ºæ¦‚ç‡åˆ†æ•°ã€‚åªèƒ½ä½¿ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çœ‹åˆ°çš„æ¦‚ç‡å€¼ã€‚'
- en: '**Logistic Regression**: Uses the sigmoid function to convert weighted feature
    combinations into probability scores. Can give any probability between 0 and 1,
    changing smoothly based on distance from the decision boundary.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é€»è¾‘å›å½’**ï¼šä½¿ç”¨**Sigmoidå‡½æ•°**å°†åŠ æƒçš„ç‰¹å¾ç»„åˆè½¬æ¢ä¸ºæ¦‚ç‡åˆ†æ•°ã€‚å¯ä»¥ç»™å‡ºä»‹äº0å’Œ1ä¹‹é—´çš„ä»»æ„æ¦‚ç‡ï¼Œä¸”éšç€è·ç¦»å†³ç­–è¾¹ç•Œçš„å˜åŒ–å¹³æ»‘å˜åŒ–ã€‚'
- en: '**Support Vector Machine**: Needs an extra step (Platt Scaling) to create probability
    scores, using the sigmoid function to convert distances from the boundary. These
    distances determine how confident the model is.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ”¯æŒå‘é‡æœº**ï¼šéœ€è¦é¢å¤–çš„æ­¥éª¤ï¼ˆPlattç¼©æ”¾ï¼‰æ¥åˆ›å»ºæ¦‚ç‡åˆ†æ•°ï¼Œä½¿ç”¨**Sigmoidå‡½æ•°**å°†è·ç¦»è¾¹ç•Œçš„å€¼è½¬æ¢ä¸ºæ¦‚ç‡ã€‚è¿™äº›è·ç¦»å†³å®šäº†æ¨¡å‹çš„ç½®ä¿¡åº¦ã€‚'
- en: '**Multi-Layer Perceptron**: Processes data through multiple layers of transformations,
    ending with the sigmoid function. Creates probability scores from complex feature
    combinations, giving any value between 0 and 1.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å¤šå±‚æ„ŸçŸ¥æœº**ï¼ˆMLPï¼‰ï¼šé€šè¿‡å¤šä¸ªå±‚æ¬¡çš„è½¬æ¢å¤„ç†æ•°æ®ï¼Œæœ€åé€šè¿‡**Sigmoidå‡½æ•°**è¾“å‡ºæ¦‚ç‡åˆ†æ•°ã€‚é€šè¿‡å¤æ‚çš„ç‰¹å¾ç»„åˆåˆ›å»ºæ¦‚ç‡åˆ†æ•°ï¼Œç»™å‡º0åˆ°1ä¹‹é—´çš„ä»»æ„å€¼ã€‚'
- en: Final Remark
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„è¯´æ˜
- en: 'Looking at how each model calculates its predicted probability shows us something
    important: each model has its own way of showing how confident it is. Some models
    like the Dummy Classifier and Decision Tree can only use certain probability scores
    based on their training data. Others like Logistic Regression and Neural Networks
    can give any probability between 0 and 1, letting them be more precise about their
    uncertainty.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è§‚å¯Ÿæ¯ä¸ªæ¨¡å‹å¦‚ä½•è®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼Œèƒ½æ­ç¤ºä¸€ä¸ªé‡è¦çš„ä¿¡æ¯ï¼šæ¯ä¸ªæ¨¡å‹éƒ½æœ‰è‡ªå·±è¡¨è¾¾ç½®ä¿¡åº¦çš„æ–¹å¼ã€‚æœ‰äº›æ¨¡å‹ï¼Œå¦‚**è™šæ‹Ÿåˆ†ç±»å™¨**å’Œ**å†³ç­–æ ‘**ï¼Œåªèƒ½ä½¿ç”¨åŸºäºå…¶è®­ç»ƒæ•°æ®çš„æŸäº›æ¦‚ç‡åˆ†æ•°ã€‚è€Œåƒ**é€»è¾‘å›å½’**å’Œ**ç¥ç»ç½‘ç»œ**è¿™æ ·çš„æ¨¡å‹å¯ä»¥ç»™å‡ºä»‹äº0å’Œ1ä¹‹é—´çš„ä»»æ„æ¦‚ç‡ï¼Œä½¿å¾—å®ƒä»¬åœ¨è¡¨è¾¾ä¸ç¡®å®šæ€§æ—¶æ›´ä¸ºç²¾ç¡®ã€‚
- en: 'Hereâ€™s whatâ€™s interesting: even though all these models give us numbers between
    0 and 1, these numbers mean different things for each model. Some get their scores
    by simple counting, others by measuring distance from a boundary, and some through
    complex calculations with features. This means a 70% probability from one model
    tells us something completely different than a 70% from another model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸ªæœ‰è¶£çš„åœ°æ–¹ï¼šå°½ç®¡æ‰€æœ‰è¿™äº›æ¨¡å‹éƒ½ç»™å‡ºä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å€¼ï¼Œä½†è¿™äº›æ•°å€¼å¯¹äºæ¯ä¸ªæ¨¡å‹æ¥è¯´å«ä¹‰ä¸åŒã€‚æœ‰äº›æ¨¡å‹é€šè¿‡ç®€å•è®¡æ•°å¾—åˆ°åˆ†æ•°ï¼Œæœ‰äº›åˆ™é€šè¿‡æµ‹é‡ä¸è¾¹ç•Œçš„è·ç¦»æ¥è®¡ç®—ï¼Œè¿˜æœ‰ä¸€äº›é€šè¿‡å¤æ‚çš„ç‰¹å¾è®¡ç®—å¾—åˆ°ç»“æœã€‚è¿™æ„å‘³ç€ä¸€ä¸ªæ¨¡å‹ç»™å‡ºçš„70%æ¦‚ç‡ï¼Œå’Œå¦ä¸€ä¸ªæ¨¡å‹çš„70%æ¦‚ç‡å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯å®Œå…¨ä¸åŒçš„ä¿¡æ¯ã€‚
- en: When picking a model to use, look beyond just accuracy. Think about whether
    the way it calculates predicted probability makes sense for your specific needs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é€‰æ‹©æ¨¡å‹æ—¶ï¼Œä¸è¦åªçœ‹å‡†ç¡®ç‡ã€‚æ€è€ƒä¸€ä¸‹å®ƒè®¡ç®—é¢„æµ‹æ¦‚ç‡çš„æ–¹å¼æ˜¯å¦é€‚åˆä½ çš„å…·ä½“éœ€æ±‚ã€‚
- en: ğŸŒŸ Predicted Probability Code Summarized
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒŸ é¢„æµ‹æ¦‚ç‡ä»£ç æ€»ç»“
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Technical Environment
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŠ€æœ¯ç¯å¢ƒ
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä½¿ç”¨çš„æ˜¯ Python 3.7 å’Œ scikit-learn 1.5ã€‚è™½ç„¶è®¨è®ºçš„æ¦‚å¿µå…·æœ‰å¹¿æ³›çš„é€‚ç”¨æ€§ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œæœ‰æ‰€ä¸åŒã€‚
- en: About the Illustrations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…³äºæ’å›¾
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›å»ºï¼Œå¹¶èå…¥äº†æ¥è‡ª Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™€ğ™«ğ™–ğ™¡ğ™ªğ™–ğ™©ğ™ğ™¤ğ™£ & ğ™Šğ™¥ğ™©ğ™ğ™¢ğ™ğ™¯ğ™–ğ™©ğ™ğ™¤ğ™£ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ™ˆğ™¤ğ™™ğ™šğ™¡ ğ™€ğ™«ğ™–ğ™¡ğ™ªğ™–ğ™©ğ™ğ™¤ğ™£ & ğ™Šğ™¥ğ™©ğ™ğ™¢ğ™ğ™¯ğ™–ğ™©ğ™ğ™¤ğ™£ ğ™¢ğ™šğ™©ğ™ğ™¤ğ™™ğ™¨ ğ™ğ™šğ™§ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Model Evaluation & Optimization
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹è¯„ä¼°ä¸ä¼˜åŒ–
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3
    ä¸ªæ•…äº‹![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Classification Algorithms
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç®—æ³•
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8
    ä¸ªæ•…äº‹![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
- en: Ensemble Learning
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4
    ä¸ªæ•…äº‹![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
