- en: 'Predicted Probability, Explained: A Visual Guide with Code Examples for Beginners'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10](https://towardsdatascience.com/predicted-probability-explained-a-visual-guide-with-code-examples-for-beginners-7c34e8994ec2?source=collection_archive---------3-----------------------#2024-12-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: MODEL EVALUATION & OPTIMIZATION
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7 basic classifiers reveal their prediction confidence math
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--7c34e8994ec2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7c34e8994ec2--------------------------------)
    ¬∑17 min read¬∑Dec 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Classification models don‚Äôt just tell you what they think the answer is ‚Äî they
    also tell you **how sure** they are about that answer. This certainty is shown
    as a probability score. A high score means the model is very confident, while
    a low score means it‚Äôs uncertain about its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Every classification model calculates these probability scores differently.
    Simple models and complex ones each have their own specific methods to determine
    the likelihood of each possible outcome.
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôre going to explore seven basic classification models and visually break
    down how each one figures out its probability scores. No need for a crystal ball
    ‚Äî we‚Äôll make these probability calculations crystal clear!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b265adbcd86fb1261e938e663049e715.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Predicted probability (or ‚Äúclass probability‚Äù) is a number from 0 to 1 (or 0%
    to 100%) that shows how confident a model is about its answer. If the number is
    1, the model is completely sure about its answer. If it‚Äôs 0.5, the model is basically
    guessing ‚Äî it‚Äôs like flipping a coin.
  prefs: []
  type: TYPE_NORMAL
- en: Components of a Probability Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a model has to choose between two classes (called binary classification),
    three main rules apply:'
  prefs: []
  type: TYPE_NORMAL
- en: The predicted probability must be between 0 and 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The chances of both options happening must add up to 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A higher probability means the model is more sure about its choice
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/36a6cda6b007e30dbfb6888351b20b6f.png)'
  prefs: []
  type: TYPE_IMG
- en: For binary classification, when we talk about predicted probability, we usually
    mean the probability of the positive class. A higher probability means the model
    thinks the positive class is more likely, while a lower probability means it thinks
    the negative class is more likely.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dac411b12238417f189b5c813e5b3b67.png)'
  prefs: []
  type: TYPE_IMG
- en: To make sure these rules are followed, models use mathematical functions to
    convert their calculations into proper probabilities. Each type of model might
    use different functions, which affects how they express their confidence levels.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction vs. Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classification, a model picks the class it thinks will most likely happen
    ‚Äî the one with the highest probability score. But two different models might pick
    the same class while being more or less confident about it. Their predicted probability
    scores tell us how sure each model is, even when they make the same choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/003cdef1e79d01f669621bce97b57d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These different probability scores tell us something important: even when models
    pick the same class, they might understand the data differently.'
  prefs: []
  type: TYPE_NORMAL
- en: One model might be very sure about its choice, while another might be less confident
    ‚Äî even though they made the same prediction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: üìä Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how predicted probability is calculated, we‚Äôll continue with
    [the same dataset used in my previous articles on Classification Algorithms](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c).
    Our goal remains: predicting if someone will play golf based on the weather.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1560cdcd385ca4877365575c6c84f8b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: ‚ÄòOvercast (one-hot-encoded into 3 columns)‚Äô, ‚ÄôTemperature‚Äô (in Fahrenheit),
    ‚ÄòHumidity‚Äô (in %), ‚ÄòWindy‚Äô (Yes/No) and ‚ÄòPlay‚Äô (Yes/No, target feature)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As some algorithms might need standardized values, we will also do [standard
    scaling](https://medium.com/towards-data-science/scaling-numerical-data-explained-a-visual-guide-with-code-examples-for-beginners-11676cdb45cb)
    to the numerical features and [one-hot encoding](/encoding-categorical-data-explained-a-visual-guide-with-code-example-for-beginners-b169ac4193ae)
    to the categorical features, including the target feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b25c11e730a5be1d37aee6342ef4b31.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let‚Äôs see how each of the following 7 classification algorithms calculates
    these probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85322ccb3ef67016db657d6a2a3c02a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Dummy Classifier Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Dummy Classifier Explained: A Visual Guide with Code Examples for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Setting the Bar in Machine Learning with Simple Baseline Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'A Dummy Classifier is a prediction model that doesn‚Äôt learn patterns from data.
    Instead, it follows basic rules like: picking the most common outcome, making
    random predictions based on how often each outcome appeared in training, always
    picking one answer, or randomly choosing between options with equal chance. The
    Dummy Classifier ignores all input features and just follows these rules.'
  prefs: []
  type: TYPE_NORMAL
- en: When this model finishes training, all it remembers is a few numbers showing
    either how often each outcome happened or the constant values it was told to use.
    It doesn‚Äôt learn anything about how features relate to outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dd5f768865682ac6099b8f627cd9342.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability in binary classification, the Dummy Classifier
    uses the most basic approach possible. Since it only remembered how often each
    outcome appeared in the training data, it uses these same numbers as probability
    scores for every prediction ‚Äî either 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b157b5391af9509372206c4069d9ea0.png)'
  prefs: []
  type: TYPE_IMG
- en: These probability scores stay exactly the same for all new data, because the
    model doesn‚Äôt look at or react to any features of the new data it‚Äôs trying to
    predict.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: k-Nearest Neighbors (KNN) Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
    [## K Nearest Neighbor Classifier, Explained: A Visual Guide with Code Examples
    for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: The friendly neighbor approach to machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: K-Nearest Neighbors (kNN) is a prediction model that takes a different approach
    ‚Äî instead of learning rules, it keeps all training examples in memory. When it
    needs to make a prediction about new data, it measures how similar this data is
    to every stored example, finds the k most similar ones (where k is a number we
    choose), and makes its decision based on those neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: When this model finishes training, all it has stored is the complete training
    dataset, the value of k we chose, and a method for measuring how similar two data
    points are (by default using Euclidean distance).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/226337b949ff12aa339c2625a91f47df.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability, kNN looks at those k most similar examples
    and counts how many belong to each class. The probability score is simply the
    number of neighbors belonging to a class divided by k.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6411613689379bcd1ad1878bbb7005b.png)'
  prefs: []
  type: TYPE_IMG
- en: Since kNN calculates probability scores by division, it can only give certain
    specific values based on k (say, for k=5, the only possible probability scores
    are 0/5 (0%), 1/5 (20%), 2/5 (40%), 3/5 (60%), 4/5 (80%), and 5/5 (100%)). This
    means kNN can‚Äôt give as many different confidence levels as other models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Naive Bayes Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
    [## Bernoulli Naive Bayes, Explained: A Visual Guide with Code Examples for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking predictive power through Yes/No probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Naive Bayes is a prediction model that uses probability math with a ‚Äúnaive‚Äù
    rule: it assumes each feature affects the outcome independently. There are different
    types of Naive Bayes: Gaussian Naive Bayes works with continuous values, while
    Bernoulli Naive Bayes works with binary features. As our dataset has many 0‚Äì1
    features, we‚Äôll focus on the Bernoulli one here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When this model finishes training, it remembers probability values: one value
    for how often the positive class occurs, and for each feature, values showing
    how likely different feature values appear when we have a positive outcome.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/287032349f3374bb29ca0948ea437c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For calculating predicted probability, Naive Bayes multiplies several probabilities
    together: the chance of each class occurring, and the chance of seeing each feature
    value within that class. These multiplied probabilities are then normalized so
    they sum to 1, giving us the final probability scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64912066b621b73326a6d1ba4026bd6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Since Naive Bayes uses probability math, its probability scores naturally fall
    between 0 and 1\. However, when certain features strongly point to one class over
    another, the model can give probability scores very close to 0 or 1, showing it‚Äôs
    very confident about its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Decision Tree Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
    [## Decision Tree Classifier, Explained: A Visual Guide with Code Examples for
    Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: A fresh look on our favorite upside-down tree
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A Decision Tree Classifier works by creating a series of yes/no questions about
    the input data. It builds these questions one at a time, always choosing the most
    useful question that best separates the data into groups. It keeps asking questions
    until it reaches a final answer at the end of a branch.
  prefs: []
  type: TYPE_NORMAL
- en: When this model finishes training, it has created a tree where each point represents
    a question about the data. Each branch shows which way to go based on the answer,
    and at the end of each branch is information about how often each class appeared
    in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9287974aef17afe32b318ba1e212cf2f.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability, the Decision Tree follows all its questions
    for new data until it reaches the end of a branch. The probability score is based
    on how many training examples of each class ended up at that same branch during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e39929dd06dd01db3fe0375771fc6a53.png)'
  prefs: []
  type: TYPE_IMG
- en: Since Decision Tree probability scores come from counting training examples
    at each branch endpoint, they can only be certain values that were seen during
    training. This means the model can only give probability scores that match the
    patterns it found while learning, which limits how precise its confidence levels
    can be.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Logistic Regression Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
    [## Logistic Regression, Explained: A Visual Guide with Code Examples for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the perfect weights to fit the data in
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A Logistic Regression model, despite its name, predicts between two classes
    using a mathematical equation. For each feature in the input data, it learns how
    important that feature is by giving it a number (weight). It also learns one extra
    number (bias) that helps make better predictions. To turn these numbers into a
    predicted probability, it uses the sigmoid function that keeps the final answer
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: When this model finishes training, all it remembers is these weights ‚Äî one number
    for each feature, plus the bias number. These numbers are all it needs to make
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74dcf333c4ede161645adab7e74ed268.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability in binary classification, Logistic Regression
    first multiplies each feature value by its weight and adds them all together,
    plus the bias. This sum could be any number, so the model uses the sigmoid function
    to convert it into a probability between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d83f16c8d25348a9fea8afc8104ffbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Unlike other models that can only give certain specific probability scores,
    Logistic Regression can give any probability between 0 and 1\. The further the
    input data is from the point where the model switches from one class to another
    (the decision boundary), the closer the probability gets to either 0 or 1\. Data
    points near this switching point get probabilities closer to 0.5, showing the
    model is less confident about these predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Support Vector Machine (SVM) Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
    [## Support Vector Classifier, Explained: A Visual Guide with Mini 2D Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best ‚Äúline‚Äù to separate the classes? Yeah, sure...
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A Support Vector Machine (SVM) Classifier works by finding the best boundary
    line (or surface) that separates different classes. It focuses on the points closest
    to this boundary (called support vectors). While the basic SVM finds straight
    boundary lines, it can also create curved boundaries using mathematical functions
    called kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this model finishes training, it remembers three things: the important
    points near the boundary (support vectors), how much each point matters (weights),
    and any settings for curved boundaries (kernel parameters). Together, these define
    where and how the boundary separates the classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/625ea19ad5b9567b0d3ad6c8fa7d6800.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability in binary classification, SVM needs an
    extra step because it wasn‚Äôt designed to give probability scores. It uses a method
    called Platt Scaling, which adds a Logistic Regression layer to convert distances
    from the boundary into probabilities. These distances go through the sigmoid function
    to get final probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5e58a416fce00dca108a284ebcdeb9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Since SVM calculates probabilities this indirect way, the scores show how far
    points are from the boundary rather than true confidence levels. Points far from
    the boundary get probability scores closer to 0 or 1, while points near the boundary
    get scores closer to 0.5\. This means the probability scores are more about location
    relative to the boundary than the model‚Äôs actual confidence in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Multilayer Perceptron Probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
    [## Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Dissecting the math (with visuals) of a tiny neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=post_page-----7c34e8994ec2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A Multi-Layer Perceptron (MLP) Classifier is a type of neural network that processes
    data through several layers of connected nodes (neurons). Each neuron calculates
    a weighted total of its inputs, transforms this number using a function (like
    ReLU), and sends the result to the next layer. For binary classification, the
    last layer uses the sigmoid function to give an output between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'When this model finishes training, it remembers two main things: the connection
    strengths (weights and biases) between neurons in neighboring layers, and how
    the network is structured (how many layers and neurons are in each layer).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff2e6b7f72a10d9cf94c129377be153e.png)'
  prefs: []
  type: TYPE_IMG
- en: For calculating predicted probability in binary classification, the MLP moves
    data through its layers, with each layer creating more complex combinations of
    information from the previous layer. The final layer produces a number that the
    sigmoid function converts into a probability between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70865a4ff395a3529685a4435992f71f.png)'
  prefs: []
  type: TYPE_IMG
- en: The MLP can find more complex patterns in data than many other models because
    it combines features in advanced ways. The final probability score shows how confident
    the network is ‚Äî scores close to 0 or 1 mean the network is very confident about
    its prediction, while scores near 0.5 indicate it‚Äôs uncertain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Model Comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To summarize, here‚Äôs how each classifier calculates predicted probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dummy Classifier**: Uses the same probability scores for all predictions,
    based only on how often each class appeared in training. Ignores all input features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors**: The probability score is the fraction of similar neighbors
    belonging to each class. Can only give specific fractions based on k (like 3/5
    or 7/10).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Naive Bayes**: Multiplies together the initial class probability and probabilities
    of seeing each feature value, then adjusts the results to add up to 1\. Probability
    scores show how likely features are to appear in each class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Decision Tree**: Gives probability scores based on how often each class appeared
    in the final branches. Can only use probability values that it saw during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Logistic Regression**: Uses the sigmoid function to convert weighted feature
    combinations into probability scores. Can give any probability between 0 and 1,
    changing smoothly based on distance from the decision boundary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Support Vector Machine**: Needs an extra step (Platt Scaling) to create probability
    scores, using the sigmoid function to convert distances from the boundary. These
    distances determine how confident the model is.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Multi-Layer Perceptron**: Processes data through multiple layers of transformations,
    ending with the sigmoid function. Creates probability scores from complex feature
    combinations, giving any value between 0 and 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Final Remark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Looking at how each model calculates its predicted probability shows us something
    important: each model has its own way of showing how confident it is. Some models
    like the Dummy Classifier and Decision Tree can only use certain probability scores
    based on their training data. Others like Logistic Regression and Neural Networks
    can give any probability between 0 and 1, letting them be more precise about their
    uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs what‚Äôs interesting: even though all these models give us numbers between
    0 and 1, these numbers mean different things for each model. Some get their scores
    by simple counting, others by measuring distance from a boundary, and some through
    complex calculations with features. This means a 70% probability from one model
    tells us something completely different than a 70% from another model.'
  prefs: []
  type: TYPE_NORMAL
- en: When picking a model to use, look beyond just accuracy. Think about whether
    the way it calculates predicted probability makes sense for your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: üåü Predicted Probability Code Summarized
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Technical Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5\. While the concepts discussed
    are generally applicable, specific code implementations may vary slightly with
    different versions.
  prefs: []
  type: TYPE_NORMAL
- en: About the Illustrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unless otherwise noted, all images are created by the author, incorporating
    licensed design elements from Canva Pro.
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôéùôöùôö ùô¢ùô§ùôßùôö ùôàùô§ùôôùôöùô° ùôÄùô´ùôñùô°ùô™ùôñùô©ùôûùô§ùô£ & ùôäùô•ùô©ùôûùô¢ùôûùôØùôñùô©ùôûùô§ùô£ ùô¢ùôöùô©ùôùùô§ùôôùô® ùôùùôöùôßùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Model Evaluation & Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/model-evaluation-optimization-331287896864?source=post_page-----7c34e8994ec2--------------------------------)3
    stories![](../Images/18fa82b1435fa7d5571ee54ae93a6c62.png)![](../Images/c95e89d05d1de700c631c342cd008de0.png)![](../Images/30e20e1a8ba3ced1e77644b706acd18d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----7c34e8994ec2--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----7c34e8994ec2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----7c34e8994ec2--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
