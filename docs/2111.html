<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Causal Machine Learning for Customer Retention: a Practical Guide with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Causal Machine Learning for Customer Retention: a Practical Guide with Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/causal-machine-learning-for-customer-retention-a-practical-guide-with-python-6bd959b25741?source=collection_archive---------1-----------------------#2024-08-30">https://towardsdatascience.com/causal-machine-learning-for-customer-retention-a-practical-guide-with-python-6bd959b25741?source=collection_archive---------1-----------------------#2024-08-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/4444080567c58b7b2c0d88100cc89dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q2FU73bCgv-ur168"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Photo by <a class="af gi" href="https://unsplash.com/@purzlbaum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Claudio Schwarz</a> on <a class="af gi" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div/><div><h2 id="fbbf" class="pw-subtitle-paragraph hi gk gl bf b hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx cq dx">An accessible guide to leveraging causal machine learning for optimizing client retention strategies</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hy hz ia ib ic ab"><div><div class="ab id"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@arthur.cruiziat?source=post_page---byline--6bd959b25741--------------------------------" rel="noopener follow"><div class="l ie if by ig ih"><div class="l ed"><img alt="Arthur Cruiziat" class="l ep by dd de cx" src="../Images/32ae05f184523057a5a2e81184f9bc67.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*D4IbTJM0gq2Sv7LxYoa-yg.jpeg"/><div class="ii by l dd de em n ij eo"/></div></div></a></div></div><div class="ik ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6bd959b25741--------------------------------" rel="noopener follow"><div class="l il im by ig in"><div class="l ed"><img alt="Towards Data Science" class="l ep by br io cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ii by l br io em n ij eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ip ab q"><div class="ab q iq"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ir is bk"><a class="af ag ah ai aj ak al am an ao ap aq ar it" data-testid="authorName" href="https://medium.com/@arthur.cruiziat?source=post_page---byline--6bd959b25741--------------------------------" rel="noopener follow">Arthur Cruiziat</a></p></div></div></div><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ir is dx"><button class="iw ix ah ai aj ak al am an ao ap aq ar iy iz ja" disabled="">Follow</button></p></div></div></span></div></div><div class="l jb"><span class="bf b bg z dx"><div class="ab cn jc jd je"><div class="jf jg ab"><div class="bf b bg z dx ab jh"><span class="ji l jb">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar it ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6bd959b25741--------------------------------" rel="noopener follow"><p class="bf b bg z jj jk jl jm jn jo jp jq bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="iu iv" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">20 min read</span><div class="jr js l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="h k w ea eb q"><div class="ky l"><div class="ab q kz la"><div class="pw-multi-vote-icon ed ji lb lc ld"><div class=""><div class="le lf lg lh li lj lk am ll lm ln ld"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l lo lp lq lr ls lt lu"><p class="bf b dy z dx"><span class="lf">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao le lx ly ab q ee lz ma" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lw"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lv lw">2</span></p></button></div></div></div><div class="ab q kj kk kl km kn ko kp kq kr ks kt ku kv kw kx"><div class="mb k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mc an ao ap iy md me mf" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mg cn"><div class="l ae"><div class="ab cb"><div class="mh mi mj mk ml gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mc an ao ap iy mm mn ma mo mp mq mr ms s mt mu mv mw mx my mz u na nb nc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="6754" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Details of this series</h1><p id="7563" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">This article is the second in a series on uplift modeling and causal machine learning. The idea is to dive deep into these methodologies both from a business and a technical perspective.</p><p id="7a39" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Before jumping into this one, I highly recommend reading the previous episode which explains what uplift modeling is and how it can help your company in general.</p><p id="6396" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Link can be found below.</p><div class="pa pb pc pd pe pf"><a rel="noopener follow" target="_blank" href="/from-insights-to-impact-leveraging-data-science-to-maximize-customer-value-9c9af354e192?source=post_page-----6bd959b25741--------------------------------"><div class="pg ab jb"><div class="ph ab co cb pi pj"><h2 class="bf gm ir z jj pk jl jm pl jo jq gk bk">From insights to impact: leveraging data science to maximize customer value</h2><div class="pm l"><h3 class="bf b ir z jj pk jl jm pl jo jq dx">Uplift modeling: how causal machine learning transforms customer relationships and revenue</h3></div><div class="pn l"><p class="bf b dy z jj pk jl jm pl jo jq dx">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt gb pf"/></div></div></a></div><h1 id="82d9" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="0909" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Picture this: you’ve been a client of a bank for a couple years. However, for a month or two, you’ve been considering leaving because their application has become too complicated. Suddenly, an employee of the bank calls you. He asks about your experience and ends up quickly explaining to you how to use the app. In the meantime, your daughter, who’s a client of the same bank also thinks about leaving them because of their trading fees; she thinks they’re too expensive. While about to unsubscribe, out of the blue, she receives a voucher allowing her to trade for free for a month! How is that even possible?</p><p id="0b06" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In my previous article, I introduced the mysterious technique behind this level of personalisation: uplift modeling. When traditional approaches usually predict an outcome — e.g. the probability of churn of a customer— , uplift modeling predicts the potential result of an action taken on a customer. The likelihood of a customer staying if called or if offered a voucher, for example!</p><p id="9e2d" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This approach allows us to target the right customers — as we’ll be removing customers who wouldn’t react positively to our approach — but also to increase our chance of success by tailoring our approach to each customer. Thanks to uplift modeling, not only do we focus our resources toward the right population, we also maximise their impact!</p><p id="5f2c" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Sounds interesting, wouldn’t you agree? Well this is your lucky day as in this article we’ll dive deep into the implementation of this approach by solving a concrete example: improving our retention. We’ll go through every step, from defining our precise use case to evaluating our models results. Our goal today is to provide you with the right knowledge and tools to be able to apply this technique within your own organisation, adapted to your own data and use case, of course.</p><h1 id="3030" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Here’s what we’ll cover:</h1><ol class=""><li id="a76a" class="nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou pu pv pw bk">We’ll start by<strong class="ob gm"> clearly defining our use case.</strong> What is churn? Who do we target? What actions will we set up to try and retain our clients with?</li><li id="4043" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Then, <strong class="ob gm">we’ll look into getting the right data for the job.</strong> What data do we need to implement uplift modeling and how to get it?</li><li id="72e1" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">After that, we’ll look into the actual modeling,<strong class="ob gm"> </strong>focusing on u<strong class="ob gm">nderstanding the various models behind uplift modeling</strong>.</li><li id="487d" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Then, we’ll apply our newly acquired knowledge to a <strong class="ob gm">first case with a single retention action: an email campaign.</strong></li><li id="de38" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Finally, we’ll deep dive into a <strong class="ob gm">more complicated implementation with</strong> <strong class="ob gm">many treatments, approaching user-level personalisation</strong></li></ol><h1 id="7542" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Our use case: improving customer retention</h1><p id="feae" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Before we can apply uplift modeling to improve customer retention, we need to clearly define the context. What constitutes “churn” in our business context? Do we want to target specific users? If yes, why? Which actions do we plan on setting up to retain them? Do we have budget constraints? Let’s try answering these questions.</p><h2 id="daa1" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Defining Churn</h2><p id="7f69" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">This is our first step. By precisely and quantitatively defining churn, we’ll be able to define retention and understand where we stand, how it has evolved and, if needed, take action. The churn definition you’ll choose will 100% depend on your business model and sector. Here are some factors to consider:</p><ul class=""><li id="1ca3" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">If you’re in a transaction-based company, you can look at transaction frequency, or transaction volumes evolution. You could also look at the time since the last transaction occured or a drop in account activity.</li><li id="c03a" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">If you’re in a subscription based company, it can be as simple as looking at users who have unsubscribed, or subscribed users who have stopped using the product.</li></ul><p id="011e" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you’re working in a transaction based tech company, churn could be defined as “<em class="qu">customer who has not done a transaction in 90 days”</em>, whereas if you’re working for a mobile app you may prefer to define it as <em class="qu">“customer who has not logged in in 30 days</em>”. Both the time frame and the nature of churn has to be defined beforehand as flagging churned user will be our first step.</p><p id="de81" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The complexity of your definition will depend on your company’s specificities as well as the number of metrics you want to consider. However, the idea is to set up definitions that provide thresholds that are easy to understand and that enable us identify churners.</p><h2 id="46c7" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Churn Prediction Window</h2><p id="4f06" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now that we know what churn is, we need to define exactly what we want to avoid. What I mean is, do we want to prevent customers from churning within the next 15 days or 30 days? Based on the answer here, you’ll have to organise your data in a specific manner, and define different retention actions. I would recommend not to be too optimistic here for 2 reasons:</p><ul class=""><li id="b0cd" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">The longer the time horizon the harder it is for a model to have good performances.</li><li id="012d" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">The longer we wait after the treatment, the harder it will be to capture its effect.</li></ul><p id="75ac" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">So let’s be reasonable here. If our definition of churn encompasses a 30-day timeframe, let’s go with a 30 days horizon and let’s try to limit churn within the next 30 days.</p><blockquote class="qv"><p id="f4f9" class="qw qx gl bf qy qz ra rb rc rd re ou dx">The idea is that our timeframe must give us enough time to implement our retention strategies and observe their impact on user behavior, while maintaining our models’ performances.</p></blockquote><h2 id="8c6f" class="qc ne gl bf nf qd rf qf ni qg rg qi nl oi rh qk ql om ri qn qo oq rj qq qr qs bk">Selecting Target Users [Optional]</h2><p id="bc2d" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Another question we need to answer is: are we targeting a specific population with our retention actions? Multiple reasons could motivate such an idea.</p><ul class=""><li id="7750" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">We noticed an increase in churn in a specific segment.</li><li id="4dfb" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We want to target highly valuable customers to maximize our ROI with those actions.</li><li id="88ea" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We want to target new customers to ensure a durable activation.</li><li id="067e" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We want to target customers that are likely to churn soon.</li></ul><p id="8371" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Depending on your own use case, you may want to select only a subset of your customers.</p><p id="47df" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In our case, we’ll choose to target clients with a higher probability of churn, so that we target customers that need us most.</p><h2 id="1ad9" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Defining retention Actions</h2><p id="93e8" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Finally, we have to select the actual retention actions we want to use on our clients. This is not an easy one, and working alongside your business stakeholders here is probably a good idea. In our case, we’ll select 4 different actions:</p><ol class=""><li id="ad01" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou pu pv pw bk">Personalized email</li><li id="27d8" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">In-app notifications highlighting new features or opportunities</li><li id="c96d" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Directly calling our customer</li><li id="9847" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Special offers or discounts — <em class="qu">another uplift model could help us identify the best voucher amount, should we explore that next?</em></li></ol><p id="6c81" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Our uplift model will help us determine which of these actions (if any) is most likely to be effective for each individual user.</p><p id="1061" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We’re ready! We defined churn, picked a prediction window, and selected the actions we want to retain our customers with. Now, the fun part begins, let’s gather some data and build a causal machine learning model!</p><h1 id="fc9b" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Data gathering: the foundation of our uplift model</h1><p id="64e4" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Building an effective uplift model requires a good dataset combining both existing user information with experimental data.</p><h2 id="ebfd" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Leveraging existing user data</h2><p id="b754" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">First, let’s look at our available data. Tech companies usually have access to a lot of those! In our case, we need customer level data such as:</p><ol class=""><li id="dd07" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou pu pv pw bk">Customer information (like age, geography, gender, acquisition channel etc.)</li><li id="ab3a" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Product specifics (creation or subscription date, subscription tier etc.)</li><li id="68df" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Transactions information ( frequency of transactions, average transaction value, total spend, types of products/services purchased, time since last transaction etc.)</li><li id="b7ef" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Engagement (e.g., login frequency, time spent on platform, feature usage statistics, etc.)</li></ol><p id="72bc" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can look at this data raw, but what brings even more value is to understand how it evolves over time. It enables us to identify behavioral patterns that will likely improve our models’ performances. Lucky for us, it’s quite simple to do, we just have to look at our data from a different perspective; here are a few transformations that can help:</p><ul class=""><li id="3c1b" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">Taking moving averages (7, 30 days…) of our main usage metrics — transactions for instance.</li><li id="ac45" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Looking at the percentage changes over time.</li><li id="7886" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Aggregating our data at different time scales such as daily, weekly etc.</li><li id="2f81" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Or even adding seasonality indicators such as the day of week or week of year.</li></ul><p id="f7c0" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">These features bring “dynamic information” that could be valuable when it comes to detect future changes! Understanding more precisely which features we should select is beyond the scope of this article, however those approaches are best practices when it comes to work with temporal data.</p><blockquote class="qv"><p id="7e95" class="qw qx gl bf qy qz ra rb rc rd re ou dx">Remember, our goal is to create a comprehensive user profile that evolves over time. This temporal data will serve as the foundation of our uplift model, <strong class="al">enabling us to predict not who might churn, but who is most likely to respond positively to our retention efforts.</strong></p></blockquote><h2 id="0bd9" class="qc ne gl bf nf qd rf qf ni qg rg qi nl oi rh qk ql om ri qn qo oq rj qq qr qs bk">Gathering Experimental Data for Uplift Modeling</h2><p id="3150" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The second part of our data gathering journey is about collecting data related to our retention actions. Now, uplift modeling does not require experimental data. If you have historical data because of past events — you may already have sent emails to customers or offered vouchers — you can leverage those. However, the more recent and unbiased your data is, the better your results will be. Debiasing observational or non randomized data requires extra steps that we will not discuss here.</p><p id="cecc" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">So what exactly do we need? Well, we need to have an idea of the impact of the actions you plan to take. We need to set up a randomized experiment where we test these actions. A lot of extremely good articles already discuss how to set those up, and I will not dive into it here. I just want to add that the better the setup, and the bigger the training set, the better it is us!</p><p id="2b85" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">After the experiment, we’ll obviously analyse the results. And while those are not helping us directly in our quest, it will provide us with additional understanding of the expected impact of our treatments as well as a good effect baseline we’ll try to outperform with our models. Not to bore you too much with definitions and acronyms, but the result of a randomized experiment is called “Average treatment effect” or ATE. On our side, we’re looking to estimate the <strong class="ob gm">Conditional Average Treatment Effect</strong> (CATE), also known as <strong class="ob gm">Individual Treatment Effect</strong> (ITE).</p><blockquote class="rk rl rm"><p id="f7d6" class="nz oa qu ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="gl">While experimental data is ideal, uplift modeling can still provide insights with observational data if an experiment isn’t feasible. If not randomized, several techniques exists to debias our dataset, such as propensity score matching. The key is to have a rich dataset that captures user characteristics, behaviors, and outcomes in relation to our retention efforts.</em></p></blockquote><h2 id="5660" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Generating synthetic data</h2><p id="d36b" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">For the purpose of this example, we’ll be generating synthetic data using the <strong class="ob gm">causalml package from Uber</strong>. Uber has communicated a lot on uplift modeling and even created an easy to use and well documented Python package.</p><p id="d7a2" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here’s how we can generate our synthetic data if you’re curious about it.</p><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="e217" class="rw ne gl rt b bg rx ry l rz sa">import pandas as pd<br/>from causalml.dataset import make_uplift_classification<br/><br/># Dictionary specifying the number of features that will have a positive effect on retention for each treatment<br/>n_uplift_increase_dict = {<br/>    "email_campaign": 2,<br/>    "in_app_notification": 3,<br/>    "call_campaign": 3,<br/>    "voucher": 4<br/>}<br/><br/># Dictionary specifying the number of features that will have a negative effect on retention for each treatment<br/>n_uplift_decrease_dict = {<br/>    "email_campaign": 1,<br/>    "in_app_notification": 1,<br/>    "call_campaign": 2,<br/>    "voucher": 1<br/>}<br/><br/># Dictionary specifying the magnitude of positive effect on retention for each treatment<br/>delta_uplift_increase_dict = {<br/>    "email_campaign": 0.05,  # Email campaign increases retention by 5 percentage points<br/>    "in_app_notification": 0.03,  # In-app notifications have a smaller but still positive effect<br/>    "call_campaign": 0.08,  # Direct calls have a strong positive effect<br/>    "voucher": 0.10  # Vouchers have the strongest positive effect<br/>}<br/><br/># Dictionary specifying the magnitude of negative effect on retention for each treatment<br/>delta_uplift_decrease_dict = {<br/>    "email_campaign": 0.02,  # Email campaign might slightly decrease retention for some customers<br/>    "in_app_notification": 0.01,  # In-app notifications have minimal negative effect<br/>    "call_campaign": 0.03,  # Calls might annoy some customers more<br/>    "voucher": 0.02  # Vouchers might make some customers think the product is overpriced<br/>}<br/><br/># Dictionary specifying the number of mixed features (combination of informative and positive uplift) for each treatment<br/>n_uplift_increase_mix_informative_dict = {<br/>    "email_campaign": 1,<br/>    "in_app_notification": 2,<br/>    "call_campaign": 1,<br/>    "voucher": 2<br/>}<br/><br/># Dictionary specifying the number of mixed features (combination of informative and negative uplift) for each treatment<br/>n_uplift_decrease_mix_informative_dict = {<br/>    "email_campaign": 1,<br/>    "in_app_notification": 1,<br/>    "call_campaign": 1,<br/>    "voucher": 1<br/>}<br/><br/>positive_class_proportion = 0.7  # Baseline retention rate<br/><br/># Generate the dataset<br/>df, feature_names = make_uplift_classification(<br/>    n_samples=20000,  # Increased sample size for more robust results<br/>    treatment_name=['email_campaign', 'in_app_notification', 'call_campaign', 'voucher'],<br/>    y_name='retention',<br/>    n_classification_features=20,  # Increased number of features<br/>    n_classification_informative=10,<br/>    n_uplift_increase_dict=n_uplift_increase_dict,<br/>    n_uplift_decrease_dict=n_uplift_decrease_dict,<br/>    delta_uplift_increase_dict=delta_uplift_increase_dict,<br/>    delta_uplift_decrease_dict=delta_uplift_decrease_dict,<br/>    n_uplift_increase_mix_informative_dict=n_uplift_increase_mix_informative_dict,<br/>    n_uplift_decrease_mix_informative_dict=n_uplift_decrease_mix_informative_dict,<br/>    positive_class_proportion=positive_class_proportion,<br/>    random_seed=42<br/>)<br/><br/>#Encoding treatments variables<br/>encoding_dict = {<br/>    'call_campaign': 3,<br/>    'email_campaign': 1,<br/>    'voucher': 4,<br/>    'in_app_notification':2,<br/>    'control': 0<br/>}<br/><br/># Create a new column with encoded values<br/>df['treatment_group_numeric'] = df['treatment_group_key'].map(encoding_dict)</span></pre><p id="b2be" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Ouf final data should be organized like this:</p><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sb"><img src="../Images/8940c6f65a748c68ab51d0be26a39b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57dbBOPOklWom0TW3M4eog.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">dataset description</figcaption></figure><p id="5b4a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In a “real life use case”, this data would be aggregated at time level, for instance this would be for each user a daily or weekly aggregation of data gathered before we reached out to them.</p><ul class=""><li id="18c5" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">X_1 to X_n would be our user level features</li><li id="6a25" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">T would be the actual treatment (1 or 0, treatment or control, treatment 1, treatment 2, control depending on your use case)</li><li id="f6f0" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">And Y is the actual outcome: did the user stay or not?</li></ul><h2 id="eecb" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Data preparation</h2><p id="cffb" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In our case, in order to analyse both our use cases, we need further preparation. Let’s create 2 distinct datasets — training and a testing set — for each use case:</p><ul class=""><li id="3738" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">First use case: a single treatment case, where we’ll focus on a single retention strategy: sending email to our customers.</li><li id="12d3" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Second use case: a multi treatment case, where we’ll compare the effectiveness of different treatments and most importantly find the best one for each customer.</li></ul><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="f18f" class="rw ne gl rt b bg rx ry l rz sa">from sklearn.model_selection import train_test_split<br/><br/>def prepare_data(df, feature_names, y_name, test_size=0.3, random_state=42):<br/>    """<br/>    Prepare data for uplift modeling, including splitting into train and test sets,<br/>    and creating mono-treatment subsets.<br/>    """<br/>    # Create binary treatment column<br/>    df['treatment_col'] = np.where(df['treatment_group_key'] == 'control', 0, 1)<br/>    <br/>    # Split data into train and test sets<br/>    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state)<br/>    <br/>    # Create mono-treatment subsets<br/>    df_train_mono = df_train[df_train['treatment_group_key'].isin(['email_campaign', 'control'])]<br/>    df_test_mono = df_test[df_test['treatment_group_key'].isin(['email_campaign', 'control'])]<br/>    <br/>    # Prepare features, treatment, and target variables for full dataset<br/>    X_train = df_train[feature_names].values<br/>    X_test = df_test[feature_names].values<br/>    treatment_train = df_train['treatment_group_key'].values<br/>    treatment_test = df_test['treatment_group_key'].values<br/>    y_train = df_train[y_name].values<br/>    y_test = df_test[y_name].values<br/>    <br/>    # Prepare features, treatment, and target variables for mono-treatment dataset<br/>    X_train_mono = df_train_mono[feature_names].values<br/>    X_test_mono = df_test_mono[feature_names].values<br/>    treatment_train_mono = df_train_mono['treatment_group_key'].values<br/>    treatment_test_mono = df_test_mono['treatment_group_key'].values<br/>    y_train_mono = df_train_mono[y_name].values<br/>    y_test_mono = df_test_mono[y_name].values<br/>    <br/>    return {<br/>        'df_train': df_train, 'df_test': df_test,<br/>        'df_train_mono': df_train_mono, 'df_test_mono': df_test_mono,<br/>        'X_train': X_train, 'X_test': X_test,<br/>        'X_train_mono': X_train_mono, 'X_test_mono': X_test_mono,<br/>        'treatment_train': treatment_train, 'treatment_test': treatment_test,<br/>        'treatment_train_mono': treatment_train_mono, 'treatment_test_mono': treatment_test_mono,<br/>        'y_train': y_train, 'y_test': y_test,<br/>        'y_train_mono': y_train_mono, 'y_test_mono': y_test_mono<br/>    }<br/><br/># Usage<br/>data = prepare_data(df, feature_names, y_name)<br/><br/># Print shapes for verification<br/>print(f"Full test set shape: {data['df_test'].shape}")<br/>print(f"Mono-treatment test set shape: {data['df_test_mono'].shape}")<br/><br/># Access prepared data<br/>df_train, df_test = data['df_train'], data['df_test']<br/>df_train_mono, df_test_mono = data['df_train_mono'], data['df_test_mono']<br/>X_train, y_train = data['X_train'], data['y_train']<br/>X_test, y_test = data['X_test'], data['y_test']<br/>X_train_mono, y_train_mono = data['X_train_mono'], data['y_train_mono']<br/>X_test_mono, y_test_mono = data['X_test_mono'], data['y_test_mono']<br/>treatment_train, treatment_test = data['treatment_train'], data['treatment_test']<br/>treatment_train_mono, treatment_test_mono = data['treatment_train_mono'], data['treatment_test_mono']</span></pre><p id="1b7b" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now that our data is ready, let’s go through a bit of theory and investigate the different approaches available to us!</p><h1 id="987d" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Understanding uplift modeling approaches</h1><p id="05d8" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">As we now know, uplift modeling uses machine learning algorithms to estimate th<strong class="ob gm">e heterogeneous treatment effect</strong> of an intervention on a population. This modelling approach focuses on the <strong class="ob gm">Conditional Average Treatment Effect</strong> (CATE), which quantifies the expected difference in outcome with and without the intervention for our customers.</p><p id="9575" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here are the main models we can use to estimate it:</p><h2 id="1c88" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Direct uplift modeling</h2><ul class=""><li id="10be" class="nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou qt pv pw bk">This approach is the simplest one. We simply use a specific algorithm, such as an uplift decision tree, which loss function is optimized to solve this problem. <strong class="ob gm">These models are designed to maximize the difference in outcomes between treated and untreated groups within the same model.</strong></li><li id="fdde" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We’ll be using an <strong class="ob gm">Uplift Random ForestClassifier</strong> as an example of this.</li></ul><h2 id="9194" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Meta-learners</h2><ul class=""><li id="a61e" class="nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou qt pv pw bk">Meta-learners use known machine learning models to estimate the CATE. They can combine multiple models used in different ways, or be trained on the predictions of other models.</li><li id="3b89" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">While many exist, we’ll focus on two types : the <strong class="ob gm">S-Learner and the T-Learner</strong></li></ul><p id="c327" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s quickly understand what those are!</p><h2 id="6e55" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">1. S-Learner (Single-Model)</h2><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sb"><img src="../Images/408c1ab77023700f4a98eab8514625c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Snh1NhrwG7awqRkzLA1jsA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">S Learner — source causalml documentation</figcaption></figure><p id="ac54" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The S-Learner is the simplest meta-learner of all. Why? Because it only consists of using a traditional machine learning model that includes the treatment feature as input. While simple to implement, it may struggle if the importance of the treatment variable is low.</p><h2 id="59bf" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">2. T-Learner (Two-Model)</h2><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sb"><img src="../Images/ec3f45f29794d0b392b3229a0d4e9a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5exwJgpS0FLkm-LB12vx6Q.png"/></div></div></figure><p id="8a58" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">“The T-Learner tries to solve the problem of discarding the treatment entirely by forcing the learner to first split on it. Instead of using a single model, we will use one model per treatment variable.</p><p id="d3ee" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the binary case, there are only two models that we need to estimate (hence the name T)” <em class="qu">Source [3]</em></p><blockquote class="rk rl rm"><p id="2bec" class="nz oa qu ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="gl">Each of these approaches has its pros and cons. How well they work will depend on your data and what you’re trying to achieve.</em></p></blockquote><p id="3d9f" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this article we’ll try out all three: an Uplift Random Forest Classifier, a S-Learner, and a T-Learner, and compare their performances when it comes to improving our company’s retention.</p><h1 id="25bd" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Single treatment uplift model implementation with causal ML</h1><h2 id="9733" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Model Training</h2><p id="c411" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now let’s train our models. We’ll start with our direct uplift model, the uplift random forest classifier. Then we’ll train our meta models using an XGBoost regressor. Two things to note here:</p><ul class=""><li id="69de" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">The algorithm choice behind your meta-models will obviously impact the final model performances, thus you may want to select it carefully.</li><li id="a388" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Yes, we’re selecting regressors as meta models rather than classifiers, mainly because they provide more flexibility, outputting a precise effect.</li></ul><p id="1d45" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here are the different steps you’ll find in the below code:</p><ul class=""><li id="3cb4" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">We initialize our result dataframe</li><li id="da5c" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Then we train each model on our training set</li><li id="e838" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Finally we predict our treatment effects on the test sets before saving the results</li></ul><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="593d" class="rw ne gl rt b bg rx ry l rz sa">from causalml.inference.meta import BaseSRegressor, BaseTRegressor<br/>from causalml.inference.tree import UpliftRandomForestClassifier<br/>from xgboost import XGBRegressor<br/><br/>#save results in a df<br/>df_results_mono = df_test_mono.copy()<br/><br/># Initialize and train a randomForest Classifier<br/>rfc = UpliftRandomForestClassifier(control_name='control')<br/>rfc.fit(X_train_mono, treatment_train_mono, y_train_mono)<br/><br/># Initialize and train S-Learner<br/>learner_s = BaseSRegressor(<br/>    learner=XGBRegressor(<br/>        n_estimators=100,<br/>        max_depth=3,<br/>        learning_rate=0.1,<br/>        random_state=42<br/>    ),<br/>    control_name='control'<br/>)<br/><br/>learner_s.fit(X_train_mono, treatment_train_mono, y_train_mono)<br/><br/># Initialize and train T-Learner<br/>learner_t = BaseTRegressor(<br/>    learner=XGBRegressor(<br/>        n_estimators=100,<br/>        max_depth=3,<br/>        learning_rate=0.1,<br/>        random_state=42<br/>    ),<br/>    control_name='control'<br/>)<br/><br/>learner_t.fit(X_train_mono, treatment_train_mono, y_train_mono)<br/><br/># Predict treatment effects<br/>df_results_mono[["mono_S_learner"]] = learner_s.predict(X=X_test_mono)<br/>df_results_mono[["mono_T_learner"]] = learner_t.predict(X=X_test_mono)<br/>df_results_mono["random_forest_learner"] = rfc.predict(X_test_mono)<br/><br/>display(df_results_mono[["mono_S_learner", "mono_T_learner", "random_forest_learner"]].mean())<br/><br/>df_mono_results_plot = df_results_mono[["mono_S_learner","mono_T_learner", "random_forest_learner","retention","treatment_col"]].copy()</span></pre><p id="a31b" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Note that we’re still using causalml here, and that the API is extremely simple to use, very close to a sklearn-like implementation.</p><h2 id="358d" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Model evaluation</h2><p id="a9f6" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">How to evaluate and compare our models’ performances? That is a great question! As we’re predicting something we do not know — <em class="qu">we don’t know the effect of our treatment on our customers as each customer either received the treatment or was in the control group</em>. <strong class="ob gm">We cannot use classic evaluation metrics.</strong> Hopefully, there are other ways:</p><p id="e16f" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob gm">The Gain curve</strong>: The gain curve offers an easy way to visualise our model’s performance. The idea behind gain is simple:</p><ul class=""><li id="ea25" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">We compute the estimated effect of each of our customers, order them from the biggest effect to the lesser.</li><li id="c890" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">From here, we move point by point. At each point, we calculate the average treatment effect meaning, both the average effect — for control and treatment — and we take the difference.</li><li id="ab3f" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We do that for both our models ordering and a random ordering, simulating random selection, and compare both curves!</li></ul><p id="2b02" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">It helps us understand which improvement our model would have brought versus a random selection.</p><p id="60ff" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob gm">The AAUC score</strong>: the AAUC score is very close to the actual gain curve as it measures the Area under the curve of the gain curve of our model, enabling us to compare it with the one of the random model. It summarizes the gain curve in an easy to compare number.</p><p id="d13a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the following code, we calculate these metrics</p><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="8226" class="rw ne gl rt b bg rx ry l rz sa">from causalml.metrics import plot_gain<br/>from causalml.metrics import auuc_score<br/><br/>#AAUC score<br/>aauc_normalized = auuc_score(df_mono_results_plot, outcome_col='retention', treatment_col='treatment_col', normalize=True, tmle=False)<br/>print(f"AAUC Score Normalized: {aauc_normalized}")<br/><br/># Plot Gain Curve<br/>plot_gain(df_mono_results_plot, outcome_col='retention', treatment_col='treatment_col')<br/>plt.title('Gain Curve - T-Learner')<br/>plt.show()</span></pre><p id="606e" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here are the results we got. Higher scores are better of course.</p><ol class=""><li id="4d31" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou pu pv pw bk">T-Learner: ~6.4 (best performer)</li><li id="1b17" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">S-Learner: ~6.3 (very close second)</li><li id="be6a" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Random Forest: ~5.7 (good, but not as good as the others)</li><li id="1d5e" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Random targeting: ~0.5 (baseline)</li></ol><p id="f0e4" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">What do these results mean?</p><ul class=""><li id="8e24" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">Well, all our models are performing way better than random targeting. This is reassuring. They’re about 12 times more effective! We’ll understand what it means in terms of impact just after.</li><li id="bea2" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">We also understand from these AAUC score that, while all models are performing quite well, the T-Leaner is the best performer</li></ul><p id="f13a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now let’s take a look at the gain curve.</p><h2 id="13ac" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Gain Curve</h2><p id="17b7" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">How to read a gain curve:</p><ul class=""><li id="8cf1" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk"><strong class="ob gm">X-Axis (Population)</strong>: This represents the size of the population you’re targeting, starting from the most responsive individuals (on the left) to the least responsive (on the right).</li><li id="9c1b" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk"><strong class="ob gm">Y-Axis (Gain)</strong>: This shows the cumulative gain, which is the improvement in your outcome (e.g., increased retention).</li></ul><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sb"><img src="../Images/a3e9e18fe560758b686e9a4cca91aded.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sM-5utWzoenu_m64a-qZDg.png"/></div></div></figure><h2 id="79f6" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Gain curve Interpretation</h2><p id="b97c" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The gain curve shows us the benefit — in our initial unit hence “people retained” — of targeting the population using our uplif model or randomly targeting.</p><ul class=""><li id="d2fa" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">In that case it seems that if we reach out to the whole population with our emails, we would retain approximately 100 additional users. This is our baseline scenario. Note that every curve ends by this result which is expected considering our gain definition.</li><li id="6566" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">So how to interpret this? Well, looking at the curve we can say that using our model, <strong class="ob gm">by reaching out to only 50% of the population, we can save 600 additional users!</strong> Six times more than by reaching out to everyone. How is that possible? By targeting only users that are likely to react positively to our outreach, while ignoring those who would leverage this email to actually churn for instance.</li></ul><p id="a56e" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><em class="qu">It is time for a small disclaimer: we’re using synthetic data here, our results are extremely unlikely in the real world, but it is good to illustrate.</em></p><p id="12de" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this case, our models enable us to do more with less. This is a good example on how we can optimize our resources using uplift modeling and targeting a lower share of the population, hence limiting the operation costs, to obtain a good share of the results. A kind of Pareto effect if you’d like.</p><p id="0d60" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">But let’s head over to the really cool stuff : how can we personalize our approach to every customer.</p><h1 id="d9bf" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Multi treatment model: let’s move to Personalization</h1><p id="1f6e" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s now restart our analysis, considering all our retention strategies described above:</p><ol class=""><li id="5932" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou pu pv pw bk">Email campaign</li><li id="4a58" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Call campaign</li><li id="3137" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">In-app notification</li><li id="73cc" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Vouchers</li></ol><p id="017a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In order to achieve this, we need experimentation results of either a multi-treatment experimentation of all those actions, or to aggregate the results of multiple experimentation. the better the experimental data, the better predictive output we’ll get. However, setting up such experiments can take time and resources.</p><p id="d40c" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s use our previously generated data, keeping in mind that obtaining this data in the first place is probably the biggest challenge of this approach!</p><h2 id="0420" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Model Training</h2><p id="d5a2" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Let’s start by training our models. We’ll keep the same model type as before, a Random Forest, S-Learner, and T-Learner.</p><p id="266c" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">However, these models will now learn to differentiate between the effects of our four distinct treatments.</p><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="8668" class="rw ne gl rt b bg rx ry l rz sa">#save results in a df<br/>df_results_multi = df_test.copy()<br/><br/># Define treatment actions<br/>actions = ['call_campaign', 'email_campaign', 'in_app_notification', 'voucher']<br/><br/># Initialize and train Uplift Random Forest Classifier<br/>rfc = UpliftRandomForestClassifier(<br/>    n_estimators=100,<br/>    max_depth=5,<br/>    min_samples_leaf=50,<br/>    min_samples_treatment=10,<br/>    n_reg=10,<br/>    control_name='control',<br/>    random_state=42<br/>)<br/>rfc.fit(X_train , treatment_train, y_train)<br/><br/># Initialize and train S-Learner<br/>learner_s = BaseSRegressor(<br/>    learner=XGBRegressor(<br/>        n_estimators=100,<br/>        max_depth=3,<br/>        learning_rate=0.1,<br/>        random_state=42<br/>    ),<br/>    control_name='control'<br/>)<br/><br/>learner_s.fit(X_train , treatment_train, y_train)<br/><br/># Initialize and train T-Learner<br/>learner_t = BaseTRegressor(<br/>    learner=XGBRegressor(<br/>        n_estimators=100,<br/>        max_depth=3,<br/>        learning_rate=0.1,<br/>        random_state=42<br/>    ),<br/>    control_name='control'<br/>)<br/><br/>learner_t.fit(X_train , treatment_train, y_train)</span></pre><h2 id="9ca1" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Predictions</h2><p id="69f5" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now that our models are trained, let’s generate our predictions for each treatment. For each user, we’ll get the uplift of each treatment. This will enable us to choose the most effective treatment by user, if any treatment has a positive uplift. Otherwise, we just won’t reach out to this person!</p><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="1219" class="rw ne gl rt b bg rx ry l rz sa">def predict_multi(df, learner, learner_name, X_test):<br/>    """<br/>    Predict treatment effects for multiple treatments and determine the best treatment.<br/>    """<br/><br/>    # Predict treatment effects<br/>    cols = [f'{learner_name}_learner_{action}' for action in actions]<br/>    df[cols] = learner.predict(X=X_test)<br/>    <br/>    # Determine the best treatment effect<br/>    df[f'{learner_name}_learner_effect'] = df[cols].max(axis=1)<br/>    <br/>    # Determine the best treatment<br/>    df[f"{learner_name}_best_treatment"] = df[cols].idxmax(axis=1)<br/>    df.loc[df[f'{learner_name}_learner_effect'] &lt; 0, f"{learner_name}_best_treatment"] = "control"<br/>    <br/>    return df<br/><br/># Apply predictions for each model<br/>df_results_multi = predict_multi(df_results_multi, rfc, 'rf', X_test)<br/>df_results_multi = predict_multi(df_results_multi, learner_s, 's', X_test)<br/>df_results_multi = predict_multi(df_results_multi, learner_t, 't', X_test)</span></pre><p id="7d6b" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Here is the kind of data we’ll obtain from this, for each model:</p><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sc"><img src="../Images/fd2ce4069ee2507a9add63ae305b450a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frd9CWt7vpMmyKyqcOWidg.png"/></div></div></figure><p id="3e12" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We’ll be able, for each model, to pick the best treatment for each user!</p><h2 id="7b7e" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Model evaluation</h2><p id="9e55" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Now let’s look at our approach evaluation. As we have multiple treatments, it is slightly different:</p><ul class=""><li id="1e87" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">For each user we select the best treatment.</li><li id="bfd6" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Then we order our user based on their best treatment effect</li><li id="3696" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">And look at what really happened : either the user really stayed or left.</li></ul><p id="c0bc" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Following this rationale, we easily understand how we can outperform random targeting by only targeting a small share of our whole population.</p><p id="266b" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">From here, we’re able to plot our gain curve and compute our AAUC. Easy right? The code below does exactly that, still leveraging causalML.</p><pre class="rn ro rp rq rr rs rt ru bp rv bb bk"><span id="6715" class="rw ne gl rt b bg rx ry l rz sa">#AAUC score<br/>aauc_normalized = auuc_score(df_t_learner_plot_multi, outcome_col='retention', treatment_col='treatment_col', normalize=True, tmle=False)<br/>aauc_non_normalize = auuc_score(df_t_learner_plot_multi, outcome_col='retention', treatment_col='treatment_col', normalize=False, tmle=False)<br/>print(f"AAUC Score Normalized: {aauc_normalized}")<br/>print(f"AAUC Score: {aauc_non_normalize}")<br/><br/># Plot Gain Curve<br/>plot_gain(df_t_learner_plot_multi, outcome_col='retention', treatment_col='treatment_col')<br/>plt.title('Gain Curve - T-Learner')<br/>plt.show()</span></pre><h2 id="4de6" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Results interpretation</h2><ol class=""><li id="8c19" class="nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou pu pv pw bk">T-Learner: ~1.45 (best performer)</li><li id="da1e" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">S-Learner: ~1.42 (very close second)</li><li id="0287" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Random Forest: ~1.20 (good, but not as good as the others)</li><li id="2be7" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Random targeting: ~0.52 (baseline)</li></ol><p id="f71a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">What this means:</p><ul class=""><li id="e72b" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">Once again, all our models outperform random targeting, and once again the T-Learner is the best performer</li><li id="30c8" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">However we note that the difference is lower than in our first case. Different reasons could explain that, one being the actual set-up. We’re considering a bigger population here, which we did not consider in our first experiment. It also could mean that our models do not perform as well when it comes to multi-treatment and we would need to iterate and try to improve their performance.</li></ul><p id="61ec" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">But let’s look at our gain curve to understand better our performance.</p><figure class="rn ro rp rq rr fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp sb"><img src="../Images/eefc212c1cd99fe14ac130ba467534db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31sEn3RHsSjKDW0RR6KoNg.png"/></div></div></figure><h2 id="7b63" class="qc ne gl bf nf qd qe qf ni qg qh qi nl oi qj qk ql om qm qn qo oq qp qq qr qs bk">Interpretation of the Multi-Treatment Gain Curve</h2><ol class=""><li id="3edb" class="nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou pu pv pw bk">As we can see, if we were to target 100% of our population — 30,000 users — we would retain an additional 850 users (approximately)</li><li id="0566" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">however, using our models, we are able to retain 1,600 users while only contacting 33% of the total population</li><li id="8a27" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou pu pv pw bk">Finally, we notice that past 40% of the population all curves start to decrease indicating that there is no value contacting those customers.</li></ol><p id="6ed3" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We made it. We successfully built a model that enables us to personalize effectively our retention actions to maximize our ROI. Based on this model, our company decided to put this model to production and saved millions not wasting resources reaching out to everyone, but also focusing the right type of effort on the right customer!</p><p id="9f71" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Putting such a model to production is another challenge in itself because we need to ensure its performance in the long term, and keep retraining it when possible. The framework to do that would be to:</p><ul class=""><li id="dcaa" class="nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou qt pv pw bk">Generate inference with your model on 80% of your target population</li><li id="7ca0" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Keep 10% of your target population intact : Control</li><li id="be87" class="nz oa gl ob b hj px od oe hm py og oh oi pz ok ol om qa oo op oq qb os ot ou qt pv pw bk">Keep an additional 10% of your population to keep experimenting to train your model for the next time period (month/quarter/year depending on your capabilities)</li></ul><p id="492a" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We might look into this later on!</p><h1 id="08f8" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusion</h1><p id="745c" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">If you made it this far, thank you! I hope this was interesting and that you learned how to create an uplift model and how to evaluate its performance.</p><p id="a86b" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If I did a good job, you may now know that uplift models are an incredible tool to understand and that it can lead to great, direct and measurable impact. You also may have understood that uplift models enable us to target the right population with the right treatment, but require a strong and exploitable experimental data to be trained on. Getting this data up to date is often the big challenge of such projects. It is applicable on historical/observational data, one would need to add specific cleaning and treating steps to ensure that the data is unbiased.</p><p id="f67f" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">So what’s next? While we’re deep-diving in the world of causal machine learning, I want to make sure you are heard. So if you want to look into specific topics that you think you could apply in your own company and would like to learn more about it, let me know, I’ll do my best. Let’s keep all learning from each other! Until next time, happy modeling!</p><h1 id="3a72" class="nd ne gl bf nf ng nh hl ni nj nk ho nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Source</h1><p id="5d3d" class="pw-post-body-paragraph nz oa gl ob b hj oc od oe hm of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><em class="qu">Unless otherwise noted, all images are by the author</em></p><p id="d7b3" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[1] <a class="af gi" href="https://en.wikipedia.org/wiki/Uplift_modelling" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Uplift_modelling</a></p><p id="fcd1" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[2] <a class="af gi" href="https://causalml.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">https://causalml.readthedocs.io/en/latest/index.html</a></p><p id="a2ce" class="pw-post-body-paragraph nz oa gl ob b hj ov od oe hm ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[3] <a class="af gi" href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html" rel="noopener ugc nofollow" target="_blank">https://matheusfacure.github.io/python-causality-handbook/landing-page.html</a></p></div></div></div></div>    
</body>
</html>