# ä½¿ç”¨æœ¬åœ°æ–‡æ¡£è¿è¡ŒSTORM AIç ”ç©¶ç³»ç»Ÿ

> åŸæ–‡ï¼š[https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28](https://towardsdatascience.com/running-the-storm-ai-research-system-with-your-local-documents-e413ea2ae064?source=collection_archive---------3-----------------------#2024-10-28)

## ä½¿ç”¨AIè¾…åŠ©ç ”ç©¶FEMAç¾éš¾å“åº”æ–‡æ¡£

[](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[![Matthew Harris](../Images/4fa3264bb8a028633cd8d37093c16214.png)](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------)[![æ•°æ®ç§‘å­¦å‰æ²¿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------) [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--e413ea2ae064--------------------------------)

Â·å‘å¸ƒäº[æ•°æ®ç§‘å­¦å‰æ²¿](https://towardsdatascience.com/?source=post_page---byline--e413ea2ae064--------------------------------) Â·16åˆ†é’Ÿé˜…è¯»Â·2024å¹´10æœˆ28æ—¥

--

![](../Images/451f325bbb6fd2a154c72081210f8eb8.png)

STORMé€šè¿‡è§†è§’å¼•å¯¼çš„é—®é¢˜æé—®åœ¨æ¨¡æ‹Ÿå¯¹è¯ä¸­ç ”ç©¶ä¸»é¢˜ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)

TL;DR

*LLMä»£ç†çš„ä½¿ç”¨åœ¨å¤„ç†å¤šæ­¥éª¤ã€é•¿ä¸Šä¸‹æ–‡çš„ç ”ç©¶ä»»åŠ¡æ—¶å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œå› ä¸ºä¼ ç»Ÿçš„RAGç›´æ¥æç¤ºæ–¹æ³•æœ‰æ—¶ä¼šé‡åˆ°å›°éš¾ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ç”±æ–¯å¦ç¦å¤§å­¦å¼€å‘çš„ä¸€ç§æ–°å‹ä¸”æœ‰å‰æ™¯çš„æŠ€æœ¯* ***S****ynthesis of* ***T****opic* ***O****utlines through* ***R****etrieval and* ***M****ulti-perspective Question Asking (*[*STORM*](https://arxiv.org/abs/2402.14207)*), å®ƒä½¿ç”¨LLMä»£ç†æ¨¡æ‹Ÿâ€œè§†è§’å¼•å¯¼çš„å¯¹è¯â€ä»¥å®ç°å¤æ‚çš„ç ”ç©¶ç›®æ ‡ï¼Œå¹¶ç”Ÿæˆä¸°å¯Œçš„ç ”ç©¶æ–‡ç« ï¼Œä¾›äººä»¬åœ¨å†™ä½œå‰ç ”ç©¶ä½¿ç”¨ã€‚STORMæœ€åˆæ˜¯ä¸ºäº†ä»ç½‘ç»œèµ„æºä¸­æ”¶é›†ä¿¡æ¯è€Œå¼€å‘çš„ï¼Œä½†ä¹Ÿæ”¯æŒæœç´¢æœ¬åœ°æ–‡æ¡£å‘é‡å­˜å‚¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•å®ç°STORMä»¥æ”¯æŒåŸºäºAIçš„æœ¬åœ°PDFç ”ç©¶ï¼Œä½¿ç”¨ç¾å›½FEMAç¾éš¾å‡†å¤‡å’Œæ´åŠ©æ–‡æ¡£ã€‚*

çœ‹åˆ°åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡ŒçŸ¥è¯†æ£€ç´¢åœ¨ç›¸å¯¹è¾ƒçŸ­çš„æ—¶é—´å†…å–å¾—çš„è¿›å±•ï¼ŒçœŸæ˜¯ä»¤äººæƒŠå¹ã€‚è‡ªä»2020å¹´å‘å¸ƒçš„[é¦–æ¬¡å…³äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®ºæ–‡](https://arxiv.org/abs/2005.11401)ä»¥æ¥ï¼Œæˆ‘ä»¬è§è¯äº†è¿™ä¸ªç”Ÿæ€ç³»ç»Ÿçš„å‘å±•ï¼Œç°å·²æ¶µç›–äº†[ä¸€ç³»åˆ—å¯ç”¨çš„æŠ€æœ¯](https://arxiv.org/html/2312.10997v5#S2)ã€‚å…¶ä¸­æ›´å…ˆè¿›çš„æŠ€æœ¯ä¹‹ä¸€æ˜¯ä»£ç†RAGï¼ŒLLMä»£ç†é€šè¿‡è¿­ä»£å’Œä¼˜åŒ–æ–‡æ¡£æ£€ç´¢æ¥è§£å†³æ›´å¤æ‚çš„ç ”ç©¶ä»»åŠ¡ã€‚è¿™ç±»ä¼¼äºäººç±»è¿›è¡Œç ”ç©¶çš„æ–¹å¼ï¼Œé€šè¿‡æ¢ç´¢å¤šç§ä¸åŒçš„æœç´¢æŸ¥è¯¢æ¥å»ºç«‹æ›´æ¸…æ™°çš„èƒŒæ™¯ï¼Œæœ‰æ—¶è¿˜ä¼šä¸å…¶ä»–äººè®¨è®ºè¿™ä¸€ä¸»é¢˜ï¼Œå¹¶å°†æ‰€æœ‰ä¿¡æ¯æ•´åˆæˆæœ€ç»ˆçš„ç»“æœã€‚è€Œå•å›åˆRAGï¼Œå³ä¾¿é‡‡ç”¨äº†æŸ¥è¯¢æ‰©å±•å’Œé‡æ–°æ’åºç­‰æŠ€æœ¯ï¼Œåœ¨åº”å¯¹åƒè¿™æ ·çš„å¤æ‚å¤šè·³ç ”ç©¶ä»»åŠ¡æ—¶ï¼Œä»ç„¶å­˜åœ¨å›°éš¾ã€‚

ä½¿ç”¨ä»£ç†æ¡†æ¶ï¼ˆå¦‚[Autogen](https://microsoft.github.io/autogen/0.2/)ã€[CrewAI](https://www.crewai.com)å’Œ[LangGraph](https://www.langchain.com/langgraph)ï¼‰ä»¥åŠç‰¹å®šçš„AIç ”ç©¶åŠ©æ‰‹ï¼ˆå¦‚[GPT Researcher](https://github.com/assafelovic/gpt-researcher)ï¼‰è¿›è¡ŒçŸ¥è¯†æ£€ç´¢çš„æ¨¡å¼æœ‰å¾ˆå¤šã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨æ–¯å¦ç¦å¤§å­¦å¼€å‘çš„ä¸€ä¸ªç”±LLMé©±åŠ¨çš„ç ”ç©¶å†™ä½œç³»ç»Ÿï¼Œåä¸º**S**ynthesis of **T**opic **O**utlines through **R**etrieval and **M**ulti-perspective Question Askingï¼ˆ[STORM](https://arxiv.org/abs/2402.14207)ï¼‰ã€‚

# STORM AIç ”ç©¶å†™ä½œç³»ç»Ÿ

STORMåº”ç”¨äº†ä¸€ç§å·§å¦™çš„æŠ€æœ¯ï¼Œå…¶ä¸­LLMä»£ç†æ¨¡æ‹Ÿâ€œè§†è§’å¼•å¯¼å¯¹è¯â€ä»¥è¾¾æˆç ”ç©¶ç›®æ ‡ï¼Œå¹¶æ‰©å±•äº†â€œåŸºäºå¤§çº²çš„RAGâ€ï¼Œä»¥ç”Ÿæˆæ›´ä¸°å¯Œçš„æ–‡ç« å†…å®¹ã€‚

è¯¥ç³»ç»Ÿé…ç½®ä¸ºç”Ÿæˆç±»ä¼¼ç»´åŸºç™¾ç§‘é£æ ¼çš„æ–‡ç« ï¼Œå¹¶åœ¨ä¸€ç»„10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…ä¸­è¿›è¡Œäº†æµ‹è¯•ã€‚

![](../Images/0e52360332e7847eb9aa2237a73652ac.png)

10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…å¯¹STORMåœ¨å®é™…ä½¿ç”¨ä¸­çš„æ„ŸçŸ¥æœ‰ç”¨æ€§çš„è°ƒæŸ¥ç»“æœã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)ã€‚

æ€»ä½“åå“ç§¯æï¼Œ70%çš„ç¼–è¾‘è®¤ä¸ºè¯¥å·¥å…·åœ¨ä»–ä»¬çš„*å†™ä½œå‰*é˜¶æ®µç ”ç©¶ä¸€ä¸ªä¸»é¢˜æ—¶ä¼šéå¸¸æœ‰ç”¨ã€‚å¸Œæœ›æœªæ¥çš„è°ƒæŸ¥èƒ½å¤ŸåŒ…æ‹¬è¶…è¿‡10åç¼–è¾‘ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä½œè€…ä¹Ÿé€šè¿‡ä½¿ç”¨FreshWikiï¼ˆä¸€ç»„è¿‘æœŸé«˜è´¨é‡çš„ç»´åŸºç™¾ç§‘æ–‡ç« æ•°æ®é›†ï¼‰å¯¹ä¼ ç»Ÿæ–‡ç« ç”Ÿæˆæ–¹æ³•è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå…¶ä¸­STORMè¢«å‘ç°ä¼˜äºä»¥å¾€çš„æ–¹æ³•ã€‚

![](../Images/4076dab0bb937a746d51e0eb93b03985.png)

ç”±10åç»éªŒä¸°å¯Œçš„ç»´åŸºç™¾ç§‘ç¼–è¾‘è€…å¯¹STORMå’Œ*oRAG*ç”Ÿæˆçš„20å¯¹æ–‡ç« è¿›è¡Œäººå·¥è¯„ä¼°ã€‚æ¯å¯¹æ–‡ç« ç”±ä¸¤åç»´åŸºç™¾ç§‘ç¼–è¾‘è€…è¯„ä¼°ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)ã€‚

STORMæ˜¯[å¼€æºçš„](https://github.com/stanford-oval/storm/tree/main)ï¼Œå¹¶ä½œä¸ºä¸€ä¸ª[PythonåŒ…](https://pypi.org/project/knowledge-storm/)æä¾›ï¼Œå¦å¤–è¿˜æ”¯æŒä½¿ç”¨å¦‚[LangGraph](https://langchain-ai.github.io/langgraph/tutorials/storm/storm/)ç­‰æ¡†æ¶çš„å®ç°ã€‚æœ€è¿‘ï¼ŒSTORMå·²è¢«å¢å¼ºä»¥æ”¯æŒä¸€ç§åä¸º[Co-STORM](https://www.arxiv.org/abs/2408.15232)çš„äººç±»-AIåä½œçŸ¥è¯†ç­–åˆ’æ–¹æ³•ï¼Œå°†äººç±»ç½®äºAIè¾…åŠ©ç ”ç©¶ç¯è·¯çš„ä¸­å¿ƒã€‚

å°½ç®¡å®ƒåœ¨è‡ªåŠ¨åŒ–å’Œäººå·¥è¯„ä¼°ä¸­éƒ½æ˜¾è‘—ä¼˜äºåŸºå‡†æ–¹æ³•ï¼Œä½†ä½œè€…ä¹Ÿæ‰¿è®¤å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚ç›®å‰å®ƒè¿˜ä¸æ˜¯å¤šæ¨¡æ€çš„ï¼Œç”Ÿæˆçš„å†…å®¹ä¹Ÿæœªè¾¾åˆ°ç»éªŒä¸°å¯Œçš„äººå·¥è´¨é‡â€”â€”æˆ‘è§‰å¾—å®ƒç›®å‰è¿˜ä¸é€‚åˆè¿™ä¸€ç‚¹ï¼Œæ›´å¤šçš„æ˜¯é’ˆå¯¹*å†™ä½œå‰*çš„ç ”ç©¶ï¼Œè€Œéæœ€ç»ˆæ–‡ç« â€”â€”å¦å¤–ï¼Œå‚è€ƒæ–‡çŒ®æ–¹é¢ä¹Ÿå­˜åœ¨ä¸€äº›éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›çš„ç»†èŠ‚ã€‚è¯è™½å¦‚æ­¤ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªæ·±å…¥çš„ç ”ç©¶ä»»åŠ¡ï¼Œå€¼å¾—ä¸€è¯•ã€‚

ä½ å¯ä»¥åœ¨[ç½‘ä¸Š](https://storm.genie.stanford.edu/)è¯•ç”¨STORMâ€”â€”å®ƒéå¸¸æœ‰è¶£ï¼â€”â€”å¹¶é…ç½®ä¸ºä½¿ç”¨ç½‘ç»œä¿¡æ¯è¿›è¡Œç ”ç©¶ã€‚

# **ä½†å¦‚æœç”¨è‡ªå·±çš„æ•°æ®è¿è¡ŒSTORMå‘¢ï¼Ÿ**

è®¸å¤šç»„ç»‡å¸Œæœ›å°†AIç ”ç©¶å·¥å…·ä¸ä»–ä»¬è‡ªå·±çš„å†…éƒ¨æ•°æ®ç»“åˆä½¿ç”¨ã€‚STORMçš„ä½œè€…ä»¬åšå¾—å¾ˆå¥½ï¼Œè®°å½•äº†å¦‚ä½•å°†STORMä¸ä¸åŒçš„LLMæä¾›è€…ä»¥åŠæœ¬åœ°å‘é‡æ•°æ®åº“ç»“åˆä½¿ç”¨ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åœ¨è‡ªå·±çš„æ–‡æ¡£ä¸Šè¿è¡ŒSTORMã€‚

é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬æ¥è¯•è¯•å§ï¼

# è®¾ç½®ä¸ä»£ç 

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/dividor/storm-with-local-docs)æ‰¾åˆ°æœ¬æ–‡çš„ä»£ç ï¼ŒåŒ…å«äº†ç¯å¢ƒè®¾ç½®è¯´æ˜ä»¥åŠå¦‚ä½•æ”¶é›†ä¸€äº›ç¤ºä¾‹æ–‡æ¡£æ¥è¿›è¡Œæ¼”ç¤ºã€‚

# FEMAç¾éš¾å‡†å¤‡å’Œæ´åŠ©æ–‡æ¡£

æˆ‘ä»¬å°†ä½¿ç”¨34ä»½ç”±ç¾å›½è”é‚¦åº”æ€¥ç®¡ç†å±€ï¼ˆ[FEMA](https://www.fema.gov)ï¼‰åˆ›å»ºçš„PDFæ–‡æ¡£ï¼Œå¸®åŠ©äººä»¬ä¸ºç¾éš¾åšå‡†å¤‡å¹¶è¿›è¡Œå“åº”ã€‚è¿™äº›æ–‡æ¡£å¯èƒ½é€šå¸¸ä¸æ˜¯äººä»¬ç”¨æ¥æ’°å†™æ·±å…¥ç ”ç©¶æ–‡ç« çš„å†…å®¹ï¼Œä½†æˆ‘å¾ˆæƒ³çœ‹çœ‹AIå¦‚ä½•å¸®åŠ©äººä»¬ä¸ºç¾éš¾åšå¥½å‡†å¤‡ã€‚

â€¦â€¦æˆ‘å·²ç»ç¼–å†™äº†å¤„ç†FEMAæŠ¥å‘Šçš„ä»£ç ï¼Œè¿™äº›ä»£ç æ¥è‡ªä¹‹å‰çš„ä¸€äº›åšå®¢æ–‡ç« ï¼Œå·²ç»åŒ…å«åœ¨ä¸Šé¢é“¾æ¥çš„ä»£ç åº“ä¸­ã€‚ğŸ˜Š

# è§£æä¸åˆ‡åˆ†

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ–‡æ¡£ï¼Œå°±éœ€è¦å°†å…¶æ‹†åˆ†æˆæ›´å°çš„æ–‡æ¡£ï¼Œä»¥ä¾¿STORMèƒ½å¤Ÿåœ¨è¯­æ–™åº“ä¸­æŸ¥æ‰¾ç‰¹å®šçš„ä¸»é¢˜ã€‚ç”±äºSTORMæœ€åˆæ—¨åœ¨ç”Ÿæˆç±»ä¼¼ç»´åŸºç™¾ç§‘çš„æ–‡ç« ï¼Œæˆ‘é€‰æ‹©å°è¯•ä¸¤ç§æ–¹æ³•ï¼šï¼ˆiï¼‰ç®€å•åœ°é€šè¿‡[LangChainçš„PyPDFLoader](https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/)æŒ‰é¡µå°†æ–‡æ¡£æ‹†åˆ†ä¸ºå­æ–‡æ¡£ï¼Œä»è€Œç²—ç•¥æ¨¡æ‹Ÿä¸€ä¸ªåŒ…å«å¤šä¸ªå­ä¸»é¢˜çš„ç»´åŸºç™¾ç§‘é¡µé¢ã€‚è®¸å¤šFEMAçš„PDFæ˜¯å•é¡µæ–‡æ¡£ï¼Œçœ‹èµ·æ¥ä¸ç»´åŸºç™¾ç§‘æ–‡ç« ç›¸å·®ä¸å¤§ï¼›ï¼ˆiiï¼‰è¿›ä¸€æ­¥å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºæ›´å°çš„éƒ¨åˆ†ï¼Œæ›´å¯èƒ½æ¶µç›–ä¸€ä¸ªç¦»æ•£çš„å­ä¸»é¢˜ã€‚

è¿™äº›å½“ç„¶æ˜¯*éå¸¸*åŸºç¡€çš„è§£ææ–¹æ³•ï¼Œä½†æˆ‘æƒ³çœ‹çœ‹è¿™ä¸¤ç§æŠ€æœ¯åœ¨ç»“æœä¸Šçš„å·®å¼‚ã€‚ä»»ä½•å¯¹STORMåœ¨æœ¬åœ°æ–‡æ¡£ä¸­çš„ä¸¥è‚ƒä½¿ç”¨éƒ½åº”è¯¥æŠ•å…¥æ‰€æœ‰å¸¸è§çš„é…å¯¹ä¼˜åŒ–å·¥ä½œã€‚

```py
def parse_pdfs():
    """
    Parses all PDF files in the specified directory and loads their content.

    This function iterates through all files in the directory specified by PDF_DIR,
    checks if they have a .pdf extension, and loads their content using PyPDFLoader.
    The loaded content from each PDF is appended to a list which is then returned.

    Returns:
        list: A list containing the content of all loaded PDF documents.
    """
    docs = []
    pdfs = os.listdir(PDF_DIR)
    print(f"We have {len(pdfs)} pdfs")
    for pdf_file in pdfs:
        if not pdf_file.endswith(".pdf"):
            continue
        print(f"Loading PDF: {pdf_file}")
        file_path = f"{PDF_DIR}/{pdf_file}"
        loader = PyPDFLoader(file_path)
        docs = docs + loader.load()
        print(f"Loaded {len(docs)} documents")

    return docs

docs = parse_pdfs()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
chunks = text_splitter.split_documents(docs)
```

# å…ƒæ•°æ®å¢å¼º

[STORMçš„ç¤ºä¾‹æ–‡æ¡£](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)è¦æ±‚æ–‡æ¡£å…·æœ‰å…ƒæ•°æ®å­—æ®µâ€˜URLâ€™ï¼Œâ€˜titleâ€™ï¼Œå’Œâ€˜descriptionâ€™ï¼Œå…¶ä¸­â€˜URLâ€™åº”è¯¥æ˜¯å”¯ä¸€çš„ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨æ‹†åˆ†PDFæ–‡æ¡£ï¼Œå› æ­¤æ²¡æœ‰å•ç‹¬é¡µé¢å’Œå—çš„æ ‡é¢˜å’Œæè¿°ï¼Œå› æ­¤æˆ‘é€‰æ‹©ä½¿ç”¨ç®€å•çš„LLMè°ƒç”¨æ¥ç”Ÿæˆè¿™äº›å†…å®¹ã€‚

å¯¹äºURLsï¼Œæˆ‘ä»¬æœ‰æ¯ä¸ªPDFé¡µé¢çš„é“¾æ¥ï¼Œä½†å¯¹äºé¡µé¢å†…çš„å—ï¼Œå¤æ‚çš„çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿå¯ä»¥é€šè¿‡å¸ƒå±€æ£€æµ‹æ¨¡å‹ç”Ÿæˆå…ƒæ•°æ®ï¼Œè¿™æ ·æ–‡æœ¬å—åŒºåŸŸå°±å¯ä»¥åœ¨ç›¸åº”çš„PDFä¸­é«˜äº®æ˜¾ç¤ºï¼Œä½†å¯¹äºè¿™ä¸ªæ¼”ç¤ºï¼Œæˆ‘åªæ˜¯ç®€å•åœ°åœ¨URLä¸­æ·»åŠ äº†ä¸€ä¸ªâ€˜_idâ€™æŸ¥è¯¢å‚æ•°ï¼Œè¿™ä¸ªå‚æ•°ä¸ä¼šåšä»»ä½•äº‹æƒ…ï¼Œåªæ˜¯ç¡®ä¿å®ƒä»¬å¯¹äºä¸åŒçš„å—æ˜¯å”¯ä¸€çš„ã€‚

```py
def summarize_text(text, prompt):
    """
    Generate a summary of some text based on the user's prompt

    Args:

    text (str) - the text to analyze
    prompt (str) - prompt instruction on how to summarize the text, eg 'generate a title'

    Returns:

    summary (text) - LLM-generated summary

    """
    messages = [
        (
            "system",
            "You are an assistant that gives very brief single sentence description of text.",
        ),
        ("human", f"{prompt} :: \n\n {text}"),
    ]
    ai_msg = llm.invoke(messages)
    summary = ai_msg.content
    return summary

def enrich_metadata(docs):
    """
    Uses an LLM to populate 'title' and 'description' for text chunks

    Args:

    docs (list) - list of LangChain documents

    Returns:

    docs (list) - list of LangChain documents with metadata fields populated

    """
    new_docs = []
    for doc in docs:

        # pdf name is last part of doc.metadata['source']
        pdf_name = doc.metadata["source"].split("/")[-1]

        # Find row in df where pdf_name is in URL
        row = df[df["Document"].str.contains(pdf_name)]
        page = doc.metadata["page"] + 1
        url = f"{row['Document'].values[0]}?id={str(uuid4())}#page={page}"

        # We'll use an LLM to generate a summary and title of the text, used by STORM
        # This is just for the demo, proper application would have better metadata
        summary = summarize_text(doc.page_content, prompt="Please describe this text:")
        title = summarize_text(
            doc.page_content, prompt="Please generate a 5 word title for this text:"
        )

        doc.metadata["description"] = summary
        doc.metadata["title"] = title
        doc.metadata["url"] = url
        doc.metadata["content"] = doc.page_content

        # print(json.dumps(doc.metadata, indent=2))
        new_docs.append(doc)

    print(f"There are {len(docs)} docs")

    return new_docs

docs = enrich_metadata(docs)
chunks = enrich_metadata(chunks)
```

# æ„å»ºå‘é‡æ•°æ®åº“

STORMå·²ç»æ”¯æŒ[Qdrantå‘é‡å­˜å‚¨](https://www.google.com/url?sa=t&rct=j&opi=89978449&url=https%3A%2F%2Fqdrant.tech%2F&ved=2ahUKEwiHuK-_766JAxXutokEHbnUMhwQFnoECAgQAQ&usg=AOvVaw1SKthNlGkmNDis3BK1WPSq)ã€‚æˆ‘å–œæ¬¢åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ä½¿ç”¨åƒLangChainå’ŒLlama Indexè¿™æ ·çš„æ¡†æ¶ï¼Œè¿™æ ·å¯ä»¥åœ¨å°†æ¥æ›´å®¹æ˜“åœ°æ›´æ¢æä¾›å•†ï¼Œå› æ­¤æˆ‘é€‰æ‹©ä½¿ç”¨LangChainæ„å»ºä¸€ä¸ª[æŒä¹…åŒ–åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿçš„æœ¬åœ°Qdrantå‘é‡æ•°æ®åº“](https://python.langchain.com/docs/integrations/vectorstores/qdrant/#local-mode)ï¼Œè€Œä¸æ˜¯STORMçš„è‡ªåŠ¨å‘é‡æ•°æ®åº“ç®¡ç†ã€‚æˆ‘è®¤ä¸ºè¿™æä¾›äº†æ›´å¤šçš„æ§åˆ¶ï¼Œå¹¶ä¸”å¯¹äºé‚£äº›å·²ç»æœ‰å¡«å……æ–‡æ¡£å‘é‡å­˜å‚¨çš„ç®¡é“çš„äººæ¥è¯´æ›´å…·å¯è¯†åˆ«æ€§ã€‚

```py
def build_vector_store(doc_type, docs):
    """
    Givena  list of LangChain docs, will embed and create a file-system Qdrant vector database.
    The folder includes doc_type in its name to avoid overwriting.

    Args:

    doc_type (str) - String to indicate level of document split, eg 'pages',
                     'chunks'. Used to name the database save folder
    docs (list) - List of langchain documents to embed and store in vector database

    Returns:

    Nothing returned by function, but db saved to f"{DB_DIR}_{doc_type}".

    """

    print(f"There are {len(docs)} docs")

    save_dir = f"{DB_DIR}_{doc_type}"

    print(f"Saving vectors to directory {save_dir}")

    client = QdrantClient(path=save_dir)

    client.create_collection(
        collection_name=DB_COLLECTION_NAME,
        vectors_config=VectorParams(size=num_vectors, distance=Distance.COSINE),
    )

    vector_store = QdrantVectorStore(
        client=client,
        collection_name=DB_COLLECTION_NAME,
        embedding=embeddings,
    )

    uuids = [str(uuid4()) for _ in range(len(docs))]

    vector_store.add_documents(documents=docs, ids=uuids)

build_vector_store("pages", docs)
build_vector_store("chunks", docs)
```

# è¿è¡ŒSTORM

STORMçš„ä»£ç åº“æœ‰[ä¸€äº›å¾ˆæ£’çš„ç¤ºä¾‹](https://github.com/stanford-oval/storm/blob/main/examples/storm_examples/README.md)ï¼ŒåŒ…æ‹¬ä¸åŒçš„æœç´¢å¼•æ“å’ŒLLMçš„ä½¿ç”¨ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨Qdrantå‘é‡å­˜å‚¨ã€‚æˆ‘å†³å®šç»“åˆè¿™äº›ç¤ºä¾‹ä¸­çš„å„ç§åŠŸèƒ½ï¼Œå¹¶æ·»åŠ ä¸€äº›é¢å¤–çš„åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

1.  å¢åŠ äº†ä¸OpenAIæˆ–Ollamaä¸€èµ·è¿è¡Œçš„åŠŸèƒ½

1.  å¢åŠ äº†æ”¯æŒä¼ å…¥å‘é‡æ•°æ®åº“ç›®å½•çš„åŠŸèƒ½

1.  å¢åŠ äº†ä¸€ä¸ªåŠŸèƒ½æ¥è§£æå‚è€ƒæ–‡çŒ®å…ƒæ•°æ®æ–‡ä»¶ï¼Œå°†å‚è€ƒæ–‡çŒ®æ·»åŠ åˆ°ç”Ÿæˆçš„ç²¾ç‚¼æ–‡ç« ä¸­ã€‚STORMå°†è¿™äº›å‚è€ƒæ–‡çŒ®ç”Ÿæˆåœ¨ä¸€ä¸ªJSONæ–‡ä»¶ä¸­ï¼Œä½†æ²¡æœ‰è‡ªåŠ¨å°†å®ƒä»¬æ·»åŠ åˆ°è¾“å‡ºæ–‡ç« ä¸­ã€‚æˆ‘ä¸ç¡®å®šè¿™æ˜¯ä¸æ˜¯ç”±äºæˆ‘é”™è¿‡äº†æŸä¸ªè®¾ç½®ï¼Œä½†å‚è€ƒæ–‡çŒ®å¯¹äºè¯„ä¼°ä»»ä½•AIç ”ç©¶æŠ€æœ¯éƒ½è‡³å…³é‡è¦ï¼Œå› æ­¤æˆ‘å¢åŠ äº†è¿™ä¸ªè‡ªå®šä¹‰åå¤„ç†æ­¥éª¤ã€‚

1.  æœ€åï¼Œæˆ‘æ³¨æ„åˆ°å¼€æ”¾æ¨¡å‹åœ¨æ¨¡æ¿å’Œè§’è‰²è®¾å®šæ–¹é¢æä¾›äº†æ›´å¤šçš„æŒ‡å¯¼ï¼Œå› ä¸ºå®ƒä»¬æ‰§è¡ŒæŒ‡ä»¤çš„å‡†ç¡®åº¦ä¸å¦‚å•†ä¸šæ¨¡å‹ã€‚æˆ‘å–œæ¬¢è¿™äº›æ§åˆ¶çš„é€æ˜æ€§ï¼Œå¹¶å°†å…¶ä¿ç•™åœ¨OpenAIä¸­ï¼Œä»¥ä¾¿å°†æ¥åœ¨å·¥ä½œä¸­è¿›è¡Œè°ƒæ•´ã€‚

è¿™é‡Œæ˜¯æ‰€æœ‰å†…å®¹ï¼ˆæŸ¥çœ‹[repo notebook](https://github.com/dividor/storm-with-local-docs/blob/main/storm-local-docs.ipynb)ä»¥è·å–å®Œæ•´ä»£ç ï¼‰â€¦

```py
def set_instructions(runner):
    """
    Adjusts templates and personas for the STORM AI Research algorithm.

    Args:

    runner - STORM runner object

    Returns:

    runner - STORM runner object with extra prompting

    """

    # Open LMs are generally weaker in following output format.
    # One way for mitigation is to add one-shot example to the prompt to exemplify the desired output format.
    # For example, we can add the following examples to the two prompts used in StormPersonaGenerator.
    # Note that the example should be an object of dspy.Example with fields matching the InputField
    # and OutputField in the prompt (i.e., dspy.Signature).
    find_related_topic_example = Example(
        topic="Knowledge Curation",
        related_topics="https://en.wikipedia.org/wiki/Knowledge_management\n"
        "https://en.wikipedia.org/wiki/Information_science\n"
        "https://en.wikipedia.org/wiki/Library_science\n",
    )
    gen_persona_example = Example(
        topic="Knowledge Curation",
        examples="Title: Knowledge management\n"
        "Table of Contents: History\nResearch\n  Dimensions\n  Strategies\n  Motivations\nKM technologies"
        "\nKnowledge barriers\nKnowledge retention\nKnowledge audit\nKnowledge protection\n"
        "  Knowledge protection methods\n    Formal methods\n    Informal methods\n"
        "  Balancing knowledge protection and knowledge sharing\n  Knowledge protection risks",
        personas="1\. Historian of Knowledge Systems: This editor will focus on the history and evolution of knowledge curation. They will provide context on how knowledge curation has changed over time and its impact on modern practices.\n"
        "2\. Information Science Professional: With insights from 'Information science', this editor will explore the foundational theories, definitions, and philosophy that underpin knowledge curation\n"
        "3\. Digital Librarian: This editor will delve into the specifics of how digital libraries operate, including software, metadata, digital preservation.\n"
        "4\. Technical expert: This editor will focus on the technical aspects of knowledge curation, such as common features of content management systems.\n"
        "5\. Museum Curator: The museum curator will contribute expertise on the curation of physical items and the transition of these practices into the digital realm.",
    )
    runner.storm_knowledge_curation_module.persona_generator.create_writer_with_persona.find_related_topic.demos = [
        find_related_topic_example
    ]
    runner.storm_knowledge_curation_module.persona_generator.create_writer_with_persona.gen_persona.demos = [
        gen_persona_example
    ]

    # A trade-off of adding one-shot example is that it will increase the input length of the prompt. Also, some
    # examples may be very long (e.g., an example for writing a section based on the given information), which may
    # confuse the model. For these cases, you can create a pseudo-example that is short and easy to understand to steer
    # the model's output format.
    # For example, we can add the following pseudo-examples to the prompt used in WritePageOutlineFromConv and
    # ConvToSection.
    write_page_outline_example = Example(
        topic="Example Topic",
        conv="Wikipedia Writer: ...\nExpert: ...\nWikipedia Writer: ...\nExpert: ...",
        old_outline="# Section 1\n## Subsection 1\n## Subsection 2\n"
        "# Section 2\n## Subsection 1\n## Subsection 2\n"
        "# Section 3",
        outline="# New Section 1\n## New Subsection 1\n## New Subsection 2\n"
        "# New Section 2\n"
        "# New Section 3\n## New Subsection 1\n## New Subsection 2\n## New Subsection 3",
    )
    runner.storm_outline_generation_module.write_outline.write_page_outline.demos = [
        write_page_outline_example
    ]
    write_section_example = Example(
        info="[1]\nInformation in document 1\n[2]\nInformation in document 2\n[3]\nInformation in document 3",
        topic="Example Topic",
        section="Example Section",
        output="# Example Topic\n## Subsection 1\n"
        "This is an example sentence [1]. This is another example sentence [2][3].\n"
        "## Subsection 2\nThis is one more example sentence [1].",
    )
    runner.storm_article_generation.section_gen.write_section.demos = [
        write_section_example
    ]

    return runner

def latest_dir(parent_folder):
    """
    Find the most recent folder (by modified date) in the specified parent folder.

    Args:
        parent_folder (str): The path to the parent folder where the search for the most recent folder will be conducted. Defaults to f"{DATA_DIR}/storm_output".

    Returns:
        str: The path to the most recently modified folder within the parent folder.
    """
    # Find most recent folder (by modified date) in DATA_DIR/storm_data
    # TODO, find out how exactly storm passes back its output directory to avoid this hack
    folders = [f.path for f in os.scandir(parent_folder) if f.is_dir()]
    folder = max(folders, key=os.path.getmtime)

    return folder

def generate_footnotes(folder):
    """
    Generates footnotes from a JSON file containing URL information.

    Args:
        folder (str): The directory path where the 'url_to_info.json' file is located.

    Returns:
        str: A formatted string containing footnotes with URLs and their corresponding titles.
    """

    file = f"{folder}/url_to_info.json"

    with open(file) as f:
        data = json.load(f)

    refs = {}
    for rec in data["url_to_unified_index"]:
        val = data["url_to_unified_index"][rec]
        title = data["url_to_info"][rec]["title"].replace('"', "")
        refs[val] = f"- {val} [{title}]({rec})"

    keys = list(refs.keys())
    keys.sort()

    footer = ""
    for key in keys:
        footer += f"{refs[key]}\n"

    return footer, refs

def generate_markdown_article(output_dir):
    """
    Generates a markdown article by reading a text file, appending footnotes, 
    and saving the result as a markdown file.

    The function performs the following steps:
    1\. Retrieves the latest directory using the `latest_dir` function.
    2\. Generates footnotes for the article using the `generate_footnotes` function.
    3\. Reads the content of a text file named 'storm_gen_article_polished.txt' 
       located in the latest directory.
    4\. Appends the generated footnotes to the end of the article content.
    5\. Writes the modified content to a new markdown file named 
       STORM_OUTPUT_MARKDOWN_ARTICLE in the same directory.

    Args:

    output_dir (str) - The directory where the STORM output is stored.

    """

    folder = latest_dir(output_dir)
    footnotes, refs = generate_footnotes(folder)

    with open(f"{folder}/storm_gen_article_polished.txt") as f:
        text = f.read()

    # Update text references like [10] to link to URLs
    for ref in refs:
        print(f"Ref: {ref}, Ref_num: {refs[ref]}")
        url = refs[ref].split("(")[1].split(")")[0]
        text = text.replace(f"[{ref}]", f"\[[{ref}]({url})\]")

    text += f"\n\n## References\n\n{footnotes}"

    with open(f"{folder}/{STORM_OUTPUT_MARKDOWN_ARTICLE}", "w") as f:
        f.write(text)

def run_storm(topic, model_type, db_dir):
    """
    This function runs the STORM AI Research algorithm using data
    in a QDrant local database.

    Args:

    topic (str) - The research topic to generate the article for
    model_type (str) - One of 'openai' and 'ollama' to control LLM used
    db_dir (str) - Directory where the QDrant vector database is

    """
    if model_type not in ["openai", "ollama"]:
        print("Unsupported model_type")
        sys.exit()

    # Clear lock so can be read
    if os.path.exists(f"{db_dir}/.lock"):
        print(f"Removing lock file {db_dir}/.lock")
        os.remove(f"{db_dir}/.lock")

    print(f"Loading Qdrant vector store from {db_dir}")

    engine_lm_configs = STORMWikiLMConfigs()

    if model_type == "openai":

        print("Using OpenAI models")

        # Initialize the language model configurations
        openai_kwargs = {
            "api_key": os.getenv("OPENAI_API_KEY"),
            "temperature": 1.0,
            "top_p": 0.9,
        }

        ModelClass = (
            OpenAIModel
            if os.getenv("OPENAI_API_TYPE") == "openai"
            else AzureOpenAIModel
        )
        # If you are using Azure service, make sure the model name matches your own deployed model name.
        # The default name here is only used for demonstration and may not match your case.
        gpt_35_model_name = (
            "gpt-4o-mini"
            if os.getenv("OPENAI_API_TYPE") == "openai"
            else "gpt-35-turbo"
        )
        gpt_4_model_name = "gpt-4o"
        if os.getenv("OPENAI_API_TYPE") == "azure":
            openai_kwargs["api_base"] = os.getenv("AZURE_API_BASE")
            openai_kwargs["api_version"] = os.getenv("AZURE_API_VERSION")

        # STORM is a LM system so different components can be powered by different models.
        # For a good balance between cost and quality, you can choose a cheaper/faster model for conv_simulator_lm
        # which is used to split queries, synthesize answers in the conversation. We recommend using stronger models
        # for outline_gen_lm which is responsible for organizing the collected information, and article_gen_lm
        # which is responsible for generating sections with citations.
        conv_simulator_lm = ModelClass(
            model=gpt_35_model_name, max_tokens=10000, **openai_kwargs
        )
        question_asker_lm = ModelClass(
            model=gpt_35_model_name, max_tokens=10000, **openai_kwargs
        )
        outline_gen_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs
        )
        article_gen_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs
        )
        article_polish_lm = ModelClass(
            model=gpt_4_model_name, max_tokens=10000, **openai_kwargs
        )

    elif model_type == "ollama":

        print("Using Ollama models")

        ollama_kwargs = {
            # "model": "llama3.2:3b",
            "model": "llama3.1:latest",
            # "model": "qwen2.5:14b",
            "port": "11434",
            "url": "http://localhost",
            "stop": (
                "\n\n---",
            ),  # dspy uses "\n\n---" to separate examples. Open models sometimes generate this.
        }

        conv_simulator_lm = OllamaClient(max_tokens=500, **ollama_kwargs)
        question_asker_lm = OllamaClient(max_tokens=500, **ollama_kwargs)
        outline_gen_lm = OllamaClient(max_tokens=400, **ollama_kwargs)
        article_gen_lm = OllamaClient(max_tokens=700, **ollama_kwargs)
        article_polish_lm = OllamaClient(max_tokens=4000, **ollama_kwargs)

    engine_lm_configs.set_conv_simulator_lm(conv_simulator_lm)
    engine_lm_configs.set_question_asker_lm(question_asker_lm)
    engine_lm_configs.set_outline_gen_lm(outline_gen_lm)
    engine_lm_configs.set_article_gen_lm(article_gen_lm)
    engine_lm_configs.set_article_polish_lm(article_polish_lm)

    max_conv_turn = 4
    max_perspective = 3
    search_top_k = 10
    max_thread_num = 1
    device = "cpu"
    vector_db_mode = "offline"

    do_research = True
    do_generate_outline = True
    do_generate_article = True
    do_polish_article = True

    # Initialize the engine arguments
    output_dir=f"{STORM_OUTPUT_DIR}/{db_dir.split('db_')[1]}"
    print(f"Output directory: {output_dir}")

    engine_args = STORMWikiRunnerArguments(
        output_dir=output_dir,
        max_conv_turn=max_conv_turn,
        max_perspective=max_perspective,
        search_top_k=search_top_k,
        max_thread_num=max_thread_num,
    )

    # Setup VectorRM to retrieve information from your own data
    rm = VectorRM(
        collection_name=DB_COLLECTION_NAME,
        embedding_model=EMBEDDING_MODEL,
        device=device,
        k=search_top_k,
    )

    # initialize the vector store, either online (store the db on Qdrant server) or offline (store the db locally):
    if vector_db_mode == "offline":
        rm.init_offline_vector_db(vector_store_path=db_dir)

    # Initialize the STORM Wiki Runner
    runner = STORMWikiRunner(engine_args, engine_lm_configs, rm)

    # Set instructions for the STORM AI Research algorithm
    runner = set_instructions(runner)

    # run the pipeline
    runner.run(
        topic=topic,
        do_research=do_research,
        do_generate_outline=do_generate_outline,
        do_generate_article=do_generate_article,
        do_polish_article=do_polish_article,
    )
    runner.post_run()
    runner.summary()

    generate_markdown_article(output_dir)
```

æˆ‘ä»¬å‡†å¤‡å¥½è¿è¡ŒSTORMäº†ï¼

å¯¹äºç ”ç©¶è¯¾é¢˜ï¼Œæˆ‘é€‰æ‹©äº†ä¸€ä¸ªå…¸å‹çš„RAGç³»ç»Ÿéš¾ä»¥å›ç­”çš„ä¸»é¢˜ï¼Œä¸”åœ¨PDFæ•°æ®ä¸­æ²¡æœ‰å¾ˆå¥½è¦†ç›–çš„å†…å®¹ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å½’å› æ•ˆæœå¦‚ä½•â€¦â€¦

â€œ***æ¯”è¾ƒä¸åŒç±»å‹ç¾å®³çš„è´¢åŠ¡å½±å“åŠå…¶å¯¹ç¤¾åŒºçš„å½±å“***â€

åœ¨ä¸¤ä¸ªæ•°æ®åº“ä¸Šè¿è¡Œæ­¤æ“ä½œâ€¦â€¦

```py
query = "Compare the financial impact of different types of disasters and how those impact communities"

for doc_type in ["pages", "chunks"]:
    db_dir = f"{DB_DIR}_{doc_type}"
    run_storm(query=query, model_type="openai", db_dir=db_dir)
```

ä½¿ç”¨OpenAIæ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹åœ¨æˆ‘çš„Macbook Pro M2ï¼ˆ16GBå†…å­˜ï¼‰ä¸Šå¤§çº¦èŠ±è´¹äº†6åˆ†é’Ÿã€‚æˆ‘éœ€è¦æŒ‡å‡ºçš„æ˜¯ï¼Œå…¶ä»–ä¸€äº›ç®€å•æŸ¥è¯¢ï¼Œå°¤å…¶æ˜¯åº•å±‚æ–‡æ¡£ä¸­æœ‰æ›´å¤šæ”¯æŒå†…å®¹çš„æŸ¥è¯¢ï¼Œè¦å¿«å¾—å¤šï¼ˆæœ‰äº›æƒ…å†µä¸‹ä¸åˆ°30ç§’ï¼‰ã€‚

# STORMç»“æœ

STORMç”Ÿæˆäº†ä¸€ç»„è¾“å‡ºæ–‡ä»¶â€¦â€¦

![](../Images/9352963a2332997ba080d259b2f9151b.png)

ç”±STORMç”Ÿæˆçš„æ–‡ä»¶ï¼Œå…¶ä¸­ä¸€ä¸ªMarkdownæ–‡ä»¶ç»“åˆäº†æ¶¦è‰²åçš„æ–‡ç« å’Œå‚è€ƒè„šæ³¨ã€‚

å®¡è§†**conversation_log.json**å’Œ**llm_call_history.json**æ–‡ä»¶ï¼ŒæŸ¥çœ‹ä»¥è§†è§’å¼•å¯¼çš„å¯¹è¯ç»„ä»¶ï¼Œååˆ†æœ‰è¶£ã€‚

å¯¹äºæˆ‘ä»¬çš„ç ”ç©¶è¯¾é¢˜â€¦â€¦

â€œ***æ¯”è¾ƒä¸åŒç±»å‹ç¾å®³çš„è´¢åŠ¡å½±å“åŠå…¶å¯¹ç¤¾åŒºçš„å½±å“***â€

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« â€¦â€¦

+   [STORMç”Ÿæˆçš„æ–‡ç«  â€” ä½¿ç”¨æŒ‰é¡µé¢æ‹†åˆ†çš„æ–‡æœ¬](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)

+   [STORMç”Ÿæˆçš„æ–‡ç«  â€” ä½¿ç”¨é€’å½’æ–‡æœ¬æ‹†åˆ†å™¨è¿›ä¸€æ­¥æ‹†åˆ†çš„æ–‡æœ¬](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)

**ä¸€äº›å¿«é€Ÿè§‚å¯Ÿ**

è¿™ä¸ªæ¼”ç¤ºæ²¡æœ‰è¿›è¡Œæ­£å¼çš„è¯„ä¼° â€” è¿™å¯èƒ½æ¯”å•è·³RAGç³»ç»Ÿ[æ›´ä¸ºå¤æ‚](https://arxiv.org/abs/2401.15391) â€” ä½†è¿™é‡Œæœ‰ä¸€äº›å¯èƒ½æœ‰ç”¨çš„ä¸»è§‚è§‚å¯Ÿâ€¦â€¦

1.  æŒ‰é¡µé¢æˆ–è¾ƒå°å—æ‹†åˆ†çš„è§£æä¼šç”Ÿæˆåˆç†çš„é¢„è¯»æŠ¥å‘Šï¼Œäººç±»å¯ä»¥ç”¨æ¥ç ”ç©¶ä¸ç¾å®³è´¢åŠ¡å½±å“ç›¸å…³çš„é¢†åŸŸ

1.  ä¸¤ç§é…å¯¹æ–¹æ³•éƒ½åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­æä¾›äº†å¼•ç”¨ï¼Œä½†ä½¿ç”¨æ›´å°çš„æ–‡æœ¬å—ä¼¼ä¹äº§ç”Ÿäº†æ›´å°‘çš„å¼•ç”¨ã€‚è¯·å‚è§ä¸Šè¿°ä¸¤ç¯‡æ–‡ç« ä¸­çš„æ€»ç»“éƒ¨åˆ†ã€‚æ›´å¤šçš„å¼•ç”¨å¯ä»¥ä¸ºåˆ†ææä¾›æ›´åšå®çš„åŸºç¡€ï¼

1.  æŒ‰è¾ƒå°å—æ‹†åˆ†çš„è§£ææœ‰æ—¶ä¼šäº§ç”Ÿä¸ç›¸å…³çš„å¼•ç”¨ï¼Œè¿™æ˜¯STORMè®ºæ–‡ä¸­æåˆ°çš„å¼•ç”¨é—®é¢˜ä¹‹ä¸€ã€‚è¯·å‚è§[æ€»ç»“éƒ¨åˆ†](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/chunks/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)ä¸­æºâ€˜10â€™çš„å¼•ç”¨ï¼Œè¯¥å¼•ç”¨ä¸å‚è€ƒå¥å­ä¸ç¬¦ã€‚

1.  æ€»ä½“æ¥è¯´ï¼Œæ­£å¦‚æˆ‘é¢„æœŸçš„é‚£æ ·ï¼ŒåŸºäº Wiki æ–‡ç« å¼€å‘çš„ç®—æ³•ï¼ŒæŒ‰ PDF åˆ†å‰²æ–‡æœ¬ä¼¼ä¹èƒ½ç”Ÿæˆä¸€ç¯‡[æ›´å…·å‡èšåŠ›å’ŒåŸºç¡€æ€§çš„æ–‡ç« ](https://github.com/dividor/storm-with-local-docs/blob/main/data/storm_output/pages/Compare_the_financial_impact_of_different_types_of_disasters_and_how_those_impact_communities/storm_gen_article_polished.md)ï¼ˆå¯¹æˆ‘æ¥è¯´ï¼ï¼‰

å°½ç®¡è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜åœ¨åŸºç¡€æ–‡æ¡£ä¸­å¹¶æ²¡æœ‰è¢«æ·±å…¥æ¢è®¨ï¼Œä½†ç”Ÿæˆçš„æŠ¥å‘Šä¸ºè¿›ä¸€æ­¥çš„äººç±»åˆ†ææä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚

# æœªæ¥çš„å·¥ä½œ

æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ²¡æœ‰è®¨è®º[Co-Storm](https://www.arxiv.org/abs/2408.15232)ï¼Œå®ƒå°†äººç±»å¼•å…¥è¿‡ç¨‹ã€‚è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªéå¸¸é€‚åˆ AI é©±åŠ¨ç ”ç©¶çš„æ–¹å‘ï¼Œæˆ‘æ­£åœ¨ç ”ç©¶è¿™ä¸ªæ–¹å‘ã€‚

æœªæ¥çš„å·¥ä½œè¿˜å¯ä»¥è€ƒè™‘å°†ç³»ç»Ÿæç¤ºå’Œäººç‰©è§’è‰²è°ƒæ•´åˆ°å…·ä½“çš„å•†ä¸šæ¡ˆä¾‹ä¸­ã€‚ç›®å‰ï¼Œè¿™äº›æç¤ºæ˜¯ä¸ºç±»ä¼¼ç»´åŸºç™¾ç§‘çš„è¿‡ç¨‹è®¾è®¡çš„â€¦â€¦

![](../Images/41fbcc46e187420f6725280d4e787d88.png)

STORM ç³»ç»Ÿæç¤ºï¼Œå±•ç¤ºäº†å¦‚ä½•å¼ºè°ƒåˆ›å»ºç»´åŸºç™¾ç§‘é£æ ¼çš„æ–‡ç« ã€‚[æ¥æº](https://arxiv.org/abs/2402.14207)

å¦ä¸€ä¸ªå¯èƒ½çš„æ–¹å‘æ˜¯å°† STORM çš„è¿æ¥å™¨æ‰©å±•åˆ° Qdrant ä»¥å¤–çš„ç³»ç»Ÿï¼Œä¾‹å¦‚ï¼Œæ”¯æŒå…¶ä»–å‘é‡å­˜å‚¨ï¼Œæˆ–è€…æ›´å¥½çš„æ˜¯ï¼Œæ”¯æŒ Langchain å’Œ Llama index å‘é‡å­˜å‚¨çš„é€šç”¨æ”¯æŒã€‚ä½œè€…é¼“åŠ±è¿™ä¸€ç±»çš„å·¥ä½œï¼Œæ¶‰åŠ[è¿™ä¸ªæ–‡ä»¶](https://github.com/stanford-oval/storm/blob/main/knowledge_storm/rm.py)çš„ PR å¯èƒ½æ˜¯æˆ‘æœªæ¥çš„å·¥ä½œæ–¹å‘ã€‚

åœ¨æ²¡æœ‰äº’è”ç½‘è¿æ¥çš„æƒ…å†µä¸‹è¿è¡Œ STORM å°†æ˜¯ä¸€ä»¶ä»¤äººå…´å¥‹çš„äº‹æƒ…ï¼Œå› ä¸ºå®ƒä¸ºç°åœºçš„ AI åŠ©æ‰‹å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚æ­£å¦‚ä»æ¼”ç¤ºä»£ç ä¸­å¯ä»¥çœ‹åˆ°çš„ï¼Œæˆ‘å¢åŠ äº†è¿è¡Œ STORM çš„èƒ½åŠ›ï¼Œä½¿ç”¨ Ollama æœ¬åœ°æ‰˜ç®¡çš„æ¨¡å‹ï¼Œä½†ç”±äºä»¤ç‰Œååç‡è¿‡ä½ï¼ŒLLM ä»£ç†è®¨è®ºé˜¶æ®µæ²¡æœ‰å®Œæˆï¼Œå› æ­¤ç³»ç»Ÿåœ¨æˆ‘ä½¿ç”¨å°å‹é‡åŒ–æ¨¡å‹çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šæ²¡æœ‰å®Œæˆä»»åŠ¡ã€‚è¿™æˆ–è®¸æ˜¯æœªæ¥åšå®¢æ–‡ç« çš„ä¸€ä¸ªè¯é¢˜ï¼

æœ€åï¼Œå°½ç®¡[åœ¨çº¿ç”¨æˆ·ç•Œé¢éå¸¸å‹å¥½](https://storm.genie.stanford.edu)ï¼Œä½†[éšä»£ç ä»“åº“é™„å¸¦çš„æ¼”ç¤ºç•Œé¢](https://github.com/stanford-oval/storm/tree/main/frontend/demo_light)éå¸¸åŸºç¡€ï¼Œæ— æ³•ç”¨äºç”Ÿäº§ç¯å¢ƒã€‚ä¹Ÿè®¸æ–¯å¦ç¦å›¢é˜Ÿå°†å‘å¸ƒæ›´å…ˆè¿›çš„ç•Œé¢â€”â€”å¯èƒ½å·²ç»æœ‰äº†ï¼Ÿâ€”â€”å¦‚æœæ²¡æœ‰çš„è¯ï¼Œä»ç„¶éœ€è¦åœ¨è¿™æ–¹é¢è¿›è¡Œæ”¹è¿›ã€‚

# ç»“è®º

è¿™æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ¼”ç¤ºï¼Œæ—¨åœ¨å¸®åŠ©äººä»¬å¼€å§‹ä½¿ç”¨ STORM å¤„ç†è‡ªå·±çš„æ–‡æ¡£ã€‚æˆ‘æ²¡æœ‰è¿›è¡Œç³»ç»Ÿçš„è¯„ä¼°ï¼Œå¦‚æœåœ¨å®é™…ç¯å¢ƒä¸­ä½¿ç”¨ STORMï¼Œæ˜¾ç„¶éœ€è¦è¿›è¡Œè¿™é¡¹å·¥ä½œã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è¿˜æ˜¯å¯¹ STORM èƒ½å¤Ÿç”Ÿæˆç›¸å¯¹ç»†è‡´çš„ç ”ç©¶ä¸»é¢˜å’Œç”Ÿæˆæœ‰è‰¯å¥½å¼•ç”¨çš„é¢„å†™ä½œç ”ç©¶å†…å®¹æ„Ÿåˆ°å°è±¡æ·±åˆ»ï¼Œè¿™å°†å¸®åŠ©æˆ‘è¿›è¡Œè‡ªå·±çš„ç ”ç©¶ã€‚

# å‚è€ƒæ–‡çŒ®

[çŸ¥è¯†å¯†é›†å‹ NLP ä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ](https://arxiv.org/abs/2005.11401)ï¼ŒLewis ç­‰äººï¼Œ2020å¹´

[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šç»¼è¿°](https://arxiv.org/html/2312.10997v5#S2)ï¼ŒYunfan ç­‰äººï¼Œ2024å¹´

[ä»é›¶å¼€å§‹åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ååŠ©æ’°å†™ç±»ä¼¼ Wikipedia çš„æ–‡ç« ](https://arxiv.org/abs/2402.14207)ï¼ŒShao ç­‰ï¼Œ2024

[è¿›å…¥æœªçŸ¥çš„æœªçŸ¥ï¼šé€šè¿‡å‚ä¸è¯­è¨€æ¨¡å‹ä»£ç†å¯¹è¯è¿›è¡Œç§¯æçš„äººç±»å­¦ä¹ ](https://www.arxiv.org/abs/2408.15232)ï¼ŒJiang ç­‰ï¼Œ2024

[MultiHop-RAGï¼šå¤šè·³æŸ¥è¯¢çš„æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†æµ‹è¯•](https://arxiv.org/abs/2401.15391)ï¼ŒTang ç­‰ï¼Œ2024

ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/dividor/storm-with-local-docs)æ‰¾åˆ°æœ¬æ–‡çš„ä»£ç 

***å¦‚æœå–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·ç‚¹èµï¼Œå¦‚æœä½ èƒ½å…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸é«˜å…´ï¼ä½ å¯ä»¥åœ¨*** [***è¿™é‡Œ***](/@astrobagel) ***æ‰¾åˆ°æ›´å¤šæ–‡ç« ***ï¼Œ***æˆ–è€…åœ¨*** [***LinkedIn***](https://www.linkedin.com/in/matthew-harris-4018865/) ***ä¸Šè”ç³»æˆ‘ã€‚***
