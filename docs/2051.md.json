["```py\n# ************CONVOLUTIONAL NEURAL NETWORK-THREE CLASSES DETECTION**************************\n# REFEREE\n# WHITE TEAM (white_away)\n# YELLOW TEAM (yellow_home)\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\n\n#******************************Data transformation********************************************\n# Training and Validation Datasets\ndata_dir = 'D:/PYTHON/teams_sample_dataset'\n\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\nval_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n#********************************CNN Model Architecture**************************************\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(512, 3)  #Three Classes\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 18 * 18)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)  \n        return x\n\n#********************************CNN TRAINING**********************************************\n\n# Model-loss function-optimizer\nmodel = CNNModel()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n#*********************************Training*************************************************\nnum_epochs = 10\ntrain_losses, val_losses = [], []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        labels = labels.type(torch.LongTensor)  \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    train_losses.append(running_loss / len(train_loader))\n\n    model.eval()\n    val_loss = 0.0\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            labels = labels.type(torch.LongTensor)  \n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, preds = torch.max(outputs, 1)  \n            all_labels.extend(labels.tolist())\n            all_preds.extend(preds.tolist())\n\n#********************************METRICS & PERFORMANCE************************************\n\n    val_losses.append(val_loss / len(val_loader))\n    val_accuracy = accuracy_score(all_labels, all_preds)\n    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)\n    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)\n    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n          f\"Loss: {train_losses[-1]:.4f}, \"\n          f\"Val Loss: {val_losses[-1]:.4f}, \"\n          f\"Val Acc: {val_accuracy:.2%}, \"\n          f\"Val Precision: {val_precision:.4f}, \"\n          f\"Val Recall: {val_recall:.4f}, \"\n          f\"Val F1 Score: {val_f1:.4f}\")\n\n#*******************************SHOW METRICS & PERFORMANCE**********************************\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.legend()\nplt.show()\n\n# SAVE THE MODEL FOR THE GH_CV_track_teams CODE\ntorch.save(model.state_dict(), 'D:/PYTHON/hockey_team_classifier.pth')\n```", "```py\n #******************************Sharding the dataset**************************\n\ndef shard_dataset(dataset, num_shards):\n    indices = list(range(len(dataset)))\n    np.random.shuffle(indices)\n    shards = []\n    shard_size = len(dataset) // num_shards\n    for i in range(num_shards):\n        shard_indices = indices[i * shard_size : (i + 1) * shard_size]\n        shards.append(Subset(dataset, shard_indices))\n    return shards\n\n#******************************Overlapping Slices***************************\ndef create_overlapping_slices(shard, slice_size, overlap):\n    indices = list(shard.indices)\n    slices = []\n    step = slice_size - overlap\n    for start in range(0, len(indices) - slice_size + 1, step):\n        slice_indices = indices[start:start + slice_size]\n        slices.append(Subset(shard.dataset, slice_indices))\n    return slices\n```", "```py\n #**************************Applying Sharding and Slicing*******************\n\nnum_shards = 4  \nslice_size = len(full_train_dataset) // num_shards // 2\noverlap = slice_size // 2\nshards = shard_dataset(full_train_dataset, num_shards)\n\n#************************Overlapping slices for each shard*****************\nall_slices = []\nfor shard in shards:\n    slices = create_overlapping_slices(shard, slice_size, overlap)\n    all_slices.extend(slices)\n```", "```py\n #**************************+*Isolate datapoints******************************\ndef isolate_data_for_unlearning(slice, data_points_to_remove):\n    new_indices = [i for i in slice.indices if i not in data_points_to_remove]\n    return Subset(slice.dataset, new_indices)\n\n#*****Identify the indices of the images we want to rectify/erasure**********\ndef get_indices_to_remove(dataset, image_names_to_remove):\n    indices_to_remove = [] #list is empty\n    image_to_index = {img_path: idx for idx, (img_path, _) in enumerate(dataset.imgs)}\n    for image_name in image_names_to_remove:\n        if image_name in image_to_index:\n            indices_to_remove.append(image_to_index[image_name])\n    return indices_to_remove\n\n#*************************Specify and remove images***************************\nimages_to_remove = []\nindices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)\nupdated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]\n```", "```py\n import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, Subset\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport matplotlib.pyplot as plt\n\n#******************************Data transformation********************************************\n# Training and Validation Datasets\ndata_dir = 'D:/PYTHON/teams_sample_dataset'\n\ntransform = transforms.Compose([\n    transforms.Resize((150, 150)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load data\nfull_train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=transform)\nval_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=transform)\n\n#******************************Sharding the dataset**************************\n\ndef shard_dataset(dataset, num_shards):\n    indices = list(range(len(dataset)))\n    np.random.shuffle(indices)\n    shards = []\n    shard_size = len(dataset) // num_shards\n    for i in range(num_shards):\n        shard_indices = indices[i * shard_size : (i + 1) * shard_size]\n        shards.append(Subset(dataset, shard_indices))\n    return shards\n\n#******************************Overlapping Slices***************************\ndef create_overlapping_slices(shard, slice_size, overlap):\n    indices = list(shard.indices)\n    slices = []\n    step = slice_size - overlap\n    for start in range(0, len(indices) - slice_size + 1, step):\n        slice_indices = indices[start:start + slice_size]\n        slices.append(Subset(shard.dataset, slice_indices))\n    return slices\n\n#**************************Applying Sharding and Slicing*******************\n\nnum_shards = 4  \nslice_size = len(full_train_dataset) // num_shards // 2\noverlap = slice_size // 2\nshards = shard_dataset(full_train_dataset, num_shards)\n\n#************************Overlapping slices for each shard*****************\nall_slices = []\nfor shard in shards:\n    slices = create_overlapping_slices(shard, slice_size, overlap)\n    all_slices.extend(slices)\n\n#**************************+*Isolate datapoints******************************\ndef isolate_data_for_unlearning(slice, data_points_to_remove):\n    new_indices = [i for i in slice.indices if i not in data_points_to_remove]\n    return Subset(slice.dataset, new_indices)\n\n#*****Identify the indices of the images we want to rectify/erasure**********\ndef get_indices_to_remove(dataset, image_names_to_remove):\n    indices_to_remove = []\n    image_to_index = {img_path: idx for idx, (img_path, _) in enumerate(dataset.imgs)}\n    for image_name in image_names_to_remove:\n        if image_name in image_to_index:\n            indices_to_remove.append(image_to_index[image_name])\n    return indices_to_remove\n\n#*************************Specify and remove images***************************\nimages_to_remove = []\nindices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)\nupdated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]\n\n#********************************CNN Model Architecture**************************************\n\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128 * 18 * 18, 512)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(512, 3)  # Output three classes\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = x.view(-1, 128 * 18 * 18)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n#********************************CNN TRAINING**********************************************\n\n# Model-loss function-optimizer\nmodel = CNNModel()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n#*********************************Training*************************************************\nnum_epochs = 10\ntrain_losses, val_losses = [], []\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for slice in updated_slices:\n        train_loader = DataLoader(slice, batch_size=32, shuffle=True)\n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            labels = labels.type(torch.LongTensor)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n    train_losses.append(running_loss / (len(updated_slices)))\n\n    model.eval()\n    val_loss = 0.0\n    all_labels = []\n    all_preds = []\n    with torch.no_grad():\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n        for inputs, labels in val_loader:\n            outputs = model(inputs)\n            labels = labels.type(torch.LongTensor)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            all_labels.extend(labels.tolist())\n            all_preds.extend(preds.tolist())\n\n#********************************METRICS & PERFORMANCE************************************\n\n    val_losses.append(val_loss / len(val_loader))\n    val_accuracy = accuracy_score(all_labels, all_preds)\n    val_precision = precision_score(all_labels, all_preds, average='macro', zero_division=1)\n    val_recall = recall_score(all_labels, all_preds, average='macro', zero_division=1)\n    val_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=1)\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n          f\"Loss: {train_losses[-1]:.4f}, \"\n          f\"Val Loss: {val_losses[-1]:.4f}, \"\n          f\"Val Acc: {val_accuracy:.2%}, \"\n          f\"Val Precision: {val_precision:.4f}, \"\n          f\"Val Recall: {val_recall:.4f}, \"\n          f\"Val F1 Score: {val_f1:.4f}\")\n\n#*******************************SHOW METRICS & PERFORMANCE**********************************\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.legend()\nplt.show()\n\n# SAVE THE MODEL\ntorch.save(model.state_dict(), 'hockey_team_classifier_SISA.pth') \n```", "```py\n#*************************Specify and remove images***************************\nimages_to_remove = [\"Away_image03.JPG\", \"Away_image04.JPG\", \"Away_image05.JPG\"]\nindices_to_remove = get_indices_to_remove(full_train_dataset, images_to_remove)\nupdated_slices = [isolate_data_for_unlearning(slice, indices_to_remove) for slice in all_slices]\n```"]