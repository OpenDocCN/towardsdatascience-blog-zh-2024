- en: How to Build a Generative Search Engine for Your Local Files Using Llama 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用Llama 3构建本地文件的生成式搜索引擎
- en: 原文：[https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08](https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08](https://towardsdatascience.com/how-to-build-a-generative-search-engine-for-your-local-files-using-llama-3-399551786965?source=collection_archive---------0-----------------------#2024-06-08)
- en: Use Qdrant, NVidia NIM API, or Llama 3 8B locally for your local GenAI assistant
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Qdrant、NVIDIA NIM API或Llama 3 8B在本地构建您的本地GenAI助手
- en: '[](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)[![Nikola
    Milosevic (Data Warrior)](../Images/ebea6501c00030561a59a4a12ab7a79a.png)](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)
    [Nikola Milosevic (Data Warrior)](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)[![Nikola
    Milosevic (Data Warrior)](../Images/ebea6501c00030561a59a4a12ab7a79a.png)](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)
    [Nikola Milosevic (Data Warrior)](https://datawarrior.medium.com/?source=post_page---byline--399551786965--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)
    ·12 min read·Jun 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--399551786965--------------------------------)
    ·阅读时间：12分钟·2024年6月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: On the 23rd of May, I received an email from a person at Nvidia inviting me
    to the [Generative AI Agents Developer Contest by NVIDIA and LangChain](https://www.nvidia.com/en-us/ai-data-science/generative-ai/developer-contest-with-langchain/?ncid=em-anno-571922&DkwibgoBGQo58Ndn2XA_QZk4Oek6oAyhMHrbuo7f2iF8fmTr0phnYJSKrENRDiGu3MOeEry08HydZtz_EiC0eg=&mkt_tok=MTU2LU9GTi03NDIAAAGTQxOVSz583h1Gr6TvRfnNz4TJLyss1TypLIotdWccXzlkMpJ8mAtsKiyWooZ5pnhPM3ALyJdxJh6gpak9YASo8xEIOWv-5FZaaptj4FmiBLsaCVMdI5w).
    My first thought was that it is quite a little time, and given we had a baby recently
    and my parents were supposed to come, I would not have time to participate. But
    then second thoughts came, and I decided that I could code something and submit
    it. I thought about what I could make for a few days, and one idea stuck with
    me — an Open-Source Generative Search Engine that lets you interact with local
    files. Microsoft Copilot already provides something like this, but I thought I
    could make an open-source version, for fun, and share a bit of learnings that
    I gathered during the quick coding of the system.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 5月23日，我收到了一封来自Nvidia的邮件，邀请我参加[NVIDIA和LangChain的生成式AI代理开发者大赛](https://www.nvidia.com/en-us/ai-data-science/generative-ai/developer-contest-with-langchain/?ncid=em-anno-571922&DkwibgoBGQo58Ndn2XA_QZk4Oek6oAyhMHrbuo7f2iF8fmTr0phnYJSKrENRDiGu3MOeEry08HydZtz_EiC0eg=&mkt_tok=MTU2LU9GTi03NDIAAAGTQxOVSz583h1Gr6TvRfnNz4TJLyss1TypLIotdWccXzlkMpJ8mAtsKiyWooZ5pnhPM3ALyJdxJh6gpak9YASo8xEIOWv-5FZaaptj4FmiBLsaCVMdI5w)。我第一反应是时间太紧，考虑到我们最近有了孩子，而且我的父母也正好要来，我应该没时间参与。但接着我又有了第二个想法，我决定可以编写一些代码并提交。我思考了几天应该做什么，最终有一个想法深深打动了我——一个开源生成式搜索引擎，允许你与本地文件进行交互。微软Copilot已经提供了类似的功能，但我想我可以做一个开源版本，出于兴趣，并分享我在快速编码系统过程中收获的一些经验。
- en: System Design
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统设计
- en: 'In order to build a local generative search engine or assistant, we would need
    several components:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个本地生成式搜索引擎或助手，我们需要几个组件：
- en: An index with the content of the local files, with an information retrieval
    engine to retrieve the most relevant documents for a given query/question.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含本地文件内容的索引，以及一个信息检索引擎，用于根据给定的查询/问题检索最相关的文档。
- en: A language model to use selected content from local documents and generate a
    summarized answer
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个语言模型，用于从本地文档中选择内容并生成总结性回答
- en: A user interface
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用户界面
- en: How the components interact is presented in a diagram below.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了各个组件如何交互。
- en: '![](../Images/93ac4331f5c89da60084b967ac949115.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93ac4331f5c89da60084b967ac949115.png)'
- en: System design and architecture. Qdrant is used for vector store, while Streamlit
    is for the user interface. Llama 3 is either used via Nvidia NIM API (70B version)
    or is downloaded via HuggingFace (8B version). Document chunking is done using
    Langchain. Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 系统设计与架构。Qdrant用于向量存储，而Streamlit用于用户界面。Llama 3通过Nvidia NIM API（70B版本）或通过HuggingFace下载（8B版本）使用。文档切块使用Langchain完成。图像由作者提供。
- en: First, we need to index our local files into the index that can be queried for
    the content of the local files. Then, when the user asks a question, we would
    use the created index, with some of the asymmetric paragraph or document embeddings
    to retrieve the most relevant documents that may contain the answer. The content
    of these documents and the question are passed to the deployed large language
    model, which would use the content of given documents to generate answers. In
    the instruction prompt, we would ask a large language model to also return references
    to the used document. Ultimately, everything will be visualized to the user on
    the user interface.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将本地文件编入可以查询本地文件内容的索引。然后，当用户提问时，我们将使用创建的索引，以及一些非对称的段落或文档嵌入来检索可能包含答案的最相关文档。这些文档的内容和问题将传递给部署的大型语言模型，模型将使用这些文档内容生成答案。在指令提示中，我们会要求大型语言模型还返回使用文档的参考资料。最终，所有内容将通过用户界面展示给用户。
- en: Now, let’s have a look in more detail at each of the components.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看看每个组件。
- en: Semantic Index
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义索引
- en: We are building a semantic index that will provide us with the most relevant
    documents based on the similarity of the file's content and a given query. To
    create such an index we will use Qdrant as a vector store. Interestingly, a [Qdrant
    client library](https://github.com/qdrant/qdrant-client) does not require a full
    installation of [Qdrant server](https://qdrant.tech/) and can do a similarity
    of documents that fit in working memory (RAM). Therefore, all we need to do is
    to pip install Qdrant client.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在构建一个语义索引，它将根据文件内容的相似性和给定查询提供最相关的文档。为了创建这样的索引，我们将使用Qdrant作为向量存储。值得注意的是，[Qdrant客户端库](https://github.com/qdrant/qdrant-client)不需要完整安装[Qdrant服务器](https://qdrant.tech/)，并且可以进行文档相似性匹配，只要文档能够适应工作内存（RAM）。因此，我们所需要做的就是通过pip安装Qdrant客户端。
- en: 'We can initialize Qdrant in the following way (note that the hf parameter is
    later defined due to the story flow, but with Qdrant client you already need to
    define which vectorization method and metric is being used):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式初始化Qdrant（请注意，hf参数稍后根据故事流程定义，但在使用Qdrant客户端时，您已经需要定义所使用的向量化方法和度量标准）：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In order to create a vector index, we will have to embed the documents on the
    hard drive. For embeddings, we will have to select the right embedding method
    and the right vector comparison metric. Several paragraph, sentence, or word embedding
    methods can be used, with varied results. The main issue with creating vector
    search, based on the documents, is the problem of asymmetric search. Asymmetric
    search problems are common to information retrieval and happen when one has short
    queries and long documents. Word or sentence embeddings are often fine-tuned to
    provide similarity scores based on documents of similar size (sentences, or paragraphs).
    Once that is not the case, the proper information retrieval may fail.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建一个向量索引，我们需要将文档嵌入硬盘中。对于嵌入，我们需要选择合适的嵌入方法和向量比较度量。有几种段落、句子或单词的嵌入方法可以使用，结果各不相同。基于文档创建向量搜索的主要问题是非对称搜索问题。非对称搜索问题在信息检索中很常见，当查询较短而文档较长时就会发生。单词或句子嵌入通常会进行微调，以根据相似大小的文档（句子或段落）提供相似度评分。一旦情况发生变化，正确的信息检索可能会失败。
- en: However, we can find an embedding methodology that would work well on asymmetric
    search problems. For example, models fine-tuned on the MSMARCO dataset usually
    work well. MSMARCO dataset is based on Bing Search queries and documents and has
    been released by Microsoft. Therefore, it is ideal for the problem we are dealing
    with.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以找到一种适用于非对称搜索问题的嵌入方法。例如，在MSMARCO数据集上微调的模型通常效果不错。MSMARCO数据集基于Bing搜索查询和文档，由微软发布。因此，它非常适合我们正在处理的问题。
- en: 'For this particular implementation, I have selected an already fine-tuned model,
    called:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的实现，我选择了一个已经微调过的模型，名为：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This model is based on BERT and it was fine-tuned using dot product as a similarity
    metric. We have already initialized qdrant client to use dot product as a similarity
    metric in line (note this model has dimension of 768):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型基于BERT，并使用点积作为相似度度量进行了微调。我们已经初始化了qdrant客户端，以便在线使用点积作为相似度度量（请注意该模型的维度为768）：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We could use other metrics, such as cosine similarity, however, given this
    model is fine-tuned using dot product, we will get the best performance using
    this metric. On top of that, thinking geometrically: Cosine similarity focuses
    solely on the difference in angles, whereas the dot product takes into account
    both angle and magnitude. By normalizing data to have uniform magnitudes, the
    two measures become equivalent. In situations where ignoring magnitude is beneficial,
    cosine similarity is useful. However, the dot product is a more suitable similarity
    measure if the magnitude is significant.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用其他度量标准，如余弦相似度，然而，鉴于该模型是使用点积微调的，我们使用该度量标准能获得最佳性能。此外，从几何角度来看：余弦相似度仅关注角度差异，而点积则同时考虑了角度和幅度。通过将数据归一化为具有相同幅度，两个度量标准变得等效。在忽略幅度有利的情况下，余弦相似度是有用的。然而，如果幅度重要，点积是更合适的相似度度量。
- en: 'The code for initializing the MSMarco model is (in case you have available
    GPU, use it. by all means):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化MSMarco模型的代码如下（如果您有可用的GPU，请使用它。无论如何）：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next problem: we need to deal with is that BERT-like models have limited
    context size, due to the quadratic memory requirements of transformer models.
    In the case of many BERT-like models, this context size is set to 512 tokens.
    There are two options: (1) we can base our answer only on the first 512 tokens
    and ignore the rest of the document, or (2) create an index, where one document
    will be split into multiple chunks and stored in the index as chunks. In the first
    case, we would lose a lot of important information, and therefore, we picked the
    second variant. To chunk documents, we can use a prebuilt chunker from LangChain:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题是：我们需要处理的是BERT类模型的上下文大小受限，由于变换器模型的二次内存需求。在许多BERT类模型中，这个上下文大小被设置为512个标记。有两个选项：（1）我们可以仅根据前512个标记生成答案，忽略文档的其余部分，或（2）创建一个索引，其中一个文档会被拆分成多个块，并作为块存储在索引中。在第一种情况下，我们将失去大量重要信息，因此我们选择了第二种方案。为了拆分文档，我们可以使用LangChain中的预构建拆分器：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the provided part of the code, we chunk text into the size of 500 tokens,
    with a window of 50 overlapping tokens. This way we keep a bit of context on the
    places where chunks end or begin. In the rest of the code, we create metadata
    with the document path on the user’s hard disk and add these chunks with metadata
    to the index.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的代码部分中，我们将文本拆分为500个标记的块，每个块有50个重叠的标记。这样，我们在块的结束或开始处保留了一些上下文。在代码的其余部分，我们创建了带有文档路径的元数据，并将这些块和元数据添加到索引中。
- en: However, before we add the content of the files to the index, we need to read
    it. Even before we read files, we need to get all the files we need to index.
    For the sake of simplicity, in this project, the user can define a folder that
    he/she would like to index. The indexer retrieves all the files from that folder
    and its subfolder in a recursive manner and indexes files that are supported (we
    will look at how to support PDF, Word, PPT, and TXT).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将文件内容添加到索引之前，需要先读取它。甚至在读取文件之前，我们需要获取所有需要索引的文件。为了简化，在此项目中，用户可以定义他/她希望索引的文件夹。索引器将递归地从该文件夹及其子文件夹中检索所有文件，并索引支持的文件（我们将探讨如何支持PDF、Word、PPT和TXT文件）。
- en: 'We can retrieve all the files in a given folder and its subfolder in a recursive
    way:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以递归方式检索给定文件夹及其子文件夹中的所有文件：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once all the files are retrieved in the list, we can read the content of files
    containing text. In this tool, for start, we will support MS Word documents (with
    extension “.docx”), PDF documents, MS PowerPoint presentations (with extension
    “.pptx”), and plain text files (with extension “.txt”).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有文件都被检索到列表中，我们就可以读取包含文本的文件内容。在这个工具中，初始支持MS Word文档（扩展名为“.docx”）、PDF文档、MS PowerPoint演示文稿（扩展名为“.pptx”）和纯文本文件（扩展名为“.txt”）。
- en: 'In order to read MS Word documents, we can use the docx-python library. The
    function reading documents into a string variable would look something like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了读取MS Word文档，我们可以使用docx-python库。将文档读取为字符串变量的函数大致如下所示：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'A similar thing can be done with MS PowerPoint files. For this, we will need
    to download and install the pptx-python library and write a function like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MS PowerPoint文件，也可以做类似的操作。为此，我们需要下载并安装pptx-python库，并编写一个类似这样的函数：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Reading text files is pretty simple:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读文本文件非常简单：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For PDF files we will in this case use the PyPDF2 library:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于PDF文件，在这种情况下我们将使用PyPDF2库：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, the whole indexing function would look something like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个索引函数大致如下所示：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we stated, we use TokenTextSplitter from LangChain to create chunks of 500
    tokens with 50 token overlap. Now, when we have created an index, we can create
    a web service for querying it and generating answers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，我们使用LangChain中的TokenTextSplitter将文本分割成每个包含500个令牌并有50个令牌重叠的片段。现在，当我们创建了索引后，可以创建一个Web服务来查询索引并生成答案。
- en: Generative Search API
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式搜索API。
- en: We will create a web service using FastAPI to host our generative search engine.
    The API will access the Qdrant client with the indexed data we created in the
    previous section, perform a search using a vector similarity metric, use the top
    chunks to generate an answer with the Llama 3 model, and finally provide the answer
    back to the user.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用FastAPI创建一个Web服务来托管我们的生成式搜索引擎。该API将访问我们在上一节中创建的索引数据，使用向量相似度度量进行搜索，使用最相关的片段与Llama
    3模型生成答案，并最终将答案返回给用户。
- en: 'In order to initialize and import libraries for the generative search component,
    we can use the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了初始化和导入生成式搜索组件的库，我们可以使用以下代码：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As previously mentioned, we are using FastAPI to create the API interface. We
    will utilize the qdrant_client library to access the indexed data we created and
    leverage the langchain_qdrant library for additional support. For embeddings and
    loading Llama 3 models locally, we will use the PyTorch and Transformers libraries.
    Additionally, we will make calls to the NVIDIA NIM API using the OpenAI library,
    with the API keys stored in the environment_var (for both Nvidia and HuggingFace)
    file we created.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用FastAPI来创建API接口。我们将利用qdrant_client库访问我们创建的索引数据，并使用langchain_qdrant库提供额外支持。对于嵌入和本地加载Llama
    3模型，我们将使用PyTorch和Transformers库。此外，我们还将使用OpenAI库调用NVIDIA NIM API，API密钥存储在我们创建的environment_var（包括Nvidia和HuggingFace）文件中。
- en: We create class Item, derived from BaseModel in Pydantic to pass as parameters
    to request functions. It will have one field, called query.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个Item类，继承自Pydantic中的BaseModel，用于作为请求函数的参数传递。它将包含一个字段，名为query。
- en: Now, we can start initializing our machine-learning models
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始初始化我们的机器学习模型。
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the first few lines, we load weights for the BERT-based model fine-tuned
    on MSMARCO data that we have also used to index our documents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几行代码中，我们加载了针对MSMARCO数据进行微调的基于BERT的模型权重，这个模型也用于索引我们的文档。
- en: Then, we check whether nvidia_key is provided, and if it is, we use the OpenAI
    library to call NVIDIA NIM API. When we use NVIDIA NIM API, we can use a big version
    of the Llama 3 instruct model, with 70B parameters. In case nvidia_key is not
    provided, we will load Llama 3 locally. However, locally, at least for most consumer
    electronics, it would not be possible to load the 70B parameters model. Therefore,
    we will either load the Llama 3 8B parameter model or the Llama 3 8B parameters
    model that has been additionally quantized. With quantization, we save space and
    enable model execution on less RAM. For example, Llama 3 8B usually needs about
    14GB of GPU RAM, while Llama 3 8B quantized would be able to run on 6GB of GPU
    RAM. Therefore, we load either a full or quantized model depending on a parameter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查是否提供了nvidia_key，如果提供了，我们使用OpenAI库调用NVIDIA NIM API。当我们使用NVIDIA NIM API时，可以使用一个大版本的Llama
    3指令模型，拥有70B参数。如果未提供nvidia_key，我们将本地加载Llama 3。然而，在本地环境下，至少对于大多数消费电子产品，无法加载70B参数模型。因此，我们将加载Llama
    3 8B参数模型或已量化的Llama 3 8B参数模型。通过量化，我们节省了空间，并使模型能够在较少的RAM上运行。例如，Llama 3 8B通常需要大约14GB的GPU
    RAM，而量化后的Llama 3 8B则可以在6GB的GPU RAM上运行。因此，我们将根据参数加载完整模型或量化模型。
- en: We can now initialize the Qdrant client
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以初始化Qdrant客户端。
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Also, FastAPI and create a first mock GET function
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，使用FastAPI并创建第一个模拟GET函数。
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This function would return JSON in format {“message”:”Hello World”}
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将返回JSON格式的数据 {“message”:”Hello World”}
- en: However, for this API to be functional, we will create two functions, one that
    performs only semantic search, while the other would perform search and then put
    the top 10 chunks as a context and generate an answer, referencing documents it
    used.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了使这个API能够正常工作，我们将创建两个功能，一个仅执行语义搜索，另一个则执行搜索并将前10个片段作为上下文生成答案，并引用它使用的文档。
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Both functions are POST methods, and we use our Item class to pass the query
    via JSON body. The first method returns the 10 most similar document chunks, with
    the path, and assigns document ID from 0–9\. Therefore, it just performs the plain
    semantic search using dot product as similarity metric (this was defined during
    indexing in Qdrant — remember line containing distance=Distance.DOT).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 两个功能都是POST方法，我们使用我们的Item类通过JSON体传递查询。第一个方法返回10个最相似的文档片段，包含路径，并分配文档ID从0到9。因此，它仅执行使用点积作为相似度度量的简单语义搜索（这一点在Qdrant的索引过程中定义——记住那一行包含distance=Distance.DOT）。
- en: 'The second function, called ask_localai is slightly more complex. It contains
    a search mechanism from the first method (therefore it may be easier to go through
    code there to understand semantic search), but adds a generative part. It creates
    a prompt for Llama 3, containing instructions in a system prompt message saying:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个功能叫做ask_localai，稍微复杂一些。它包含来自第一个方法的搜索机制（因此可以更容易地通过代码理解语义搜索），但增加了生成部分。它为Llama
    3生成了一个提示，包含一个系统提示消息，内容如下：
- en: Answer the user’s question using the documents given in the context. In the
    context are documents that should contain an answer. Please always reference the
    document ID (in square brackets, for example [0],[1]) of the document that was
    used to make a claim. Use as many citations and documents as it is necessary to
    answer a question.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用上下文中提供的文档回答用户的问题。在上下文中是应该包含答案的文档。请始终引用用于做出声明的文档ID（用方括号表示，例如[0]、[1]）。根据需要使用足够多的引用和文档来回答问题。
- en: The user’s message contains a list of documents structured as an ID (0–9) followed
    by the document chunk on the next line. To maintain the mapping between IDs and
    document paths, we create a list called list_res, which includes the ID, path,
    and content. The user prompt ends with the word “Question” followed by the user’s
    query.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的消息包含一个文档列表，结构为ID（0–9），后跟文档片段在下一行。为了保持ID与文档路径之间的映射，我们创建了一个名为list_res的列表，其中包含ID、路径和内容。用户提示以“Question”一词结束，后面跟着用户的查询。
- en: The response contains context and generated answer. However, the answer is again
    generated by either the Llama 3 70B model (using NVIDIA NIM API), local Llama
    3 8B, or local Llama 3 8B quantized depending on the passed parameters.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 响应包含上下文和生成的答案。然而，答案是由Llama 3 70B模型（使用NVIDIA NIM API）、本地Llama 3 8B或本地量化的Llama
    3 8B生成的，这取决于传递的参数。
- en: 'The API can be started from a separate file containing the following lines
    of code (given, that our generative component is in a file called api.py, as the
    first argument in Uvicorn maps to the file name):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: API可以从一个包含以下代码行的单独文件中启动（假设我们的生成组件位于名为api.py的文件中，Uvicorn的第一个参数映射到文件名）：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Simple User Interface
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单的用户界面
- en: The final component of our local generative search engine is the user interface.
    We will build a simple user interface using [Streamlit](https://streamlit.io/),
    which will include an input bar, a search button, a section for displaying the
    generated answer, and a list of referenced documents that can be opened or downloaded.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的本地生成搜索引擎的最终组件是用户界面。我们将使用[Streamlit](https://streamlit.io/)构建一个简单的用户界面，包含一个输入框、一个搜索按钮、一个显示生成答案的区域，以及一个可以打开或下载的参考文档列表。
- en: 'The whole code for the user interface in Streamlit has less than 45 lines of
    code (44 to be exact):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit中用户界面的全部代码不到45行（准确来说是44行）：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It will all end up looking like this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最终效果将是这样的：
- en: '![](../Images/a81808986db73b743ff6767665018ad7.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a81808986db73b743ff6767665018ad7.png)'
- en: An example of an answered question in the built user interface. Screenshot by
    author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面中已回答问题的示例。截图由作者提供。
- en: Availability
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用性
- en: The entire code for the described project is available on GitHub, at [https://github.com/nikolamilosevic86/local-genAI-search](https://github.com/nikolamilosevic86/local-genAI-search).
    In the past, I have worked on several generative search projects, on which there
    have also been some publications. You can have a look at [https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html](https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html)
    or [https://arxiv.org/abs/2402.18589](https://arxiv.org/abs/2402.18589).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的完整代码可以在GitHub上找到，网址为[https://github.com/nikolamilosevic86/local-genAI-search](https://github.com/nikolamilosevic86/local-genAI-search)。过去，我曾参与多个生成式搜索项目，并且也有一些相关的出版物。你可以查看[https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html](https://www.thinkmind.org/library/INTERNET/INTERNET_2024/internet_2024_1_10_48001.html)或[https://arxiv.org/abs/2402.18589](https://arxiv.org/abs/2402.18589)。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article showed how one can leverage generative AI with semantic search
    using Qdrant. It is generally a Retrieval-Augmented Generation (RAG) pipeline
    over local files with instructions to reference claims to the local documents.
    The whole code is about 300 lines long, and we have even added complexity by giving
    a choice to the user between 3 different Llama 3 models. For this use case, both
    8B and 70B parameter models work quite well.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了如何利用生成式AI结合Qdrant进行语义搜索。它通常是一个检索增强生成（RAG）管道，作用于本地文件，并提供引用本地文档声明的指令。整个代码大约有300行，我们甚至增加了复杂性，给用户提供了在3个不同的Llama
    3模型之间选择的选项。对于这个使用案例，8B和70B参数模型都能很好地工作。
- en: I wanted to explain the steps I did, in case this can be helpful for someone
    in the future. However, if you want to use this particular tool, the easiest way
    to do so is by just getting it from [GitHub](https://github.com/nikolamilosevic86/local-genAI-search),
    it is all open source!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我想解释一下我所做的步骤，希望这能对将来某些人有所帮助。不过，如果你想使用这个特定的工具，最简单的方式就是直接从[GitHub](https://github.com/nikolamilosevic86/local-genAI-search)获取，它是完全开源的！
