- en: A Bird’s-Eye View on the Evolution of Language Models for Text Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型在文本生成中的演变概述
- en: 原文：[https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10](https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10](https://towardsdatascience.com/a-birds-eye-view-on-the-evolution-of-language-models-for-text-generation-9b6b3fcb96a4?source=collection_archive---------7-----------------------#2024-07-10)
- en: '[](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[![Aleksandr
    Perevalov](../Images/3b2512ac381b1dc2e72ae79d7e14285a.png)](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)
    [Aleksandr Perevalov](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[![Aleksandr
    Perevalov](../Images/3b2512ac381b1dc2e72ae79d7e14285a.png)](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)
    [Aleksandr Perevalov](https://perevalov.medium.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)
    ·8 min read·Jul 10, 2024
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9b6b3fcb96a4--------------------------------)
    ·阅读时间8分钟·2024年7月10日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In this article I would like to share my notes on how language models (LMs)
    have been developing during the last decades. This text may serve a a gentle introduction
    and help to understand the conceptual points of LMs throughout their history.
    It’s worth mentioning that I don’t dive very deep into the implementation details
    and math behind it, however, the level of description is enough to understand
    LMs’ evolution properly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我想分享一些关于语言模型（LM）在过去几十年发展过程中的笔记。本文可以作为一个温和的入门介绍，帮助理解语言模型历史中的概念要点。值得一提的是，我并没有深入探讨实现细节和背后的数学原理，但本文的描述层次足以让人正确理解语言模型的演变。
- en: What is Language Modeling?
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是语言建模？
- en: Generally speaking, Language Modeling is a process of formalizing a language,
    in particular — natural language, in order to make it machine-readable and process
    it in various ways. Hence, it is not only about generating language, but also
    about language representation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，语言建模是将语言，特别是自然语言，形式化的过程，目的是使其机器可读并能够以各种方式处理。因此，这不仅仅是生成语言的问题，还是关于语言表示的问题。
- en: The most popular association with “language modeling”, thanks to GenAI, is tightly
    connected with the text generation process. This is why my article considers the
    evolution of the language models from the text generation point of view.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成式人工智能（GenAI）的普及，“语言建模”这一概念通常与文本生成过程紧密相关。因此，本文从文本生成的角度，探讨语言模型的演变过程。
- en: N-gram Language Models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-gram语言模型
- en: Although the foundation of n-gram LMs was created in the middle of 20th century,
    the widespread of such models has started in 1980s and 1990s.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管n-gram语言模型的基础在20世纪中期就已经奠定，但这类模型的广泛应用始于1980年代和1990年代。
- en: 'The n-gram LMs make use of the *Markov assumption*, which claims, in the context
    of LMs, that in the probability of a next word in a sequence depends only on the
    previous word(s). Therefore, the probability approximation of a word given its
    context with an n-gram LM can be formalized as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram语言模型利用了*马尔科夫假设*，该假设在语言模型的背景下指出，下一个词的概率仅依赖于前一个词。因此，利用n-gram语言模型对给定上下文的词的概率进行近似，可以形式化为以下方式：
- en: '![](../Images/a8655e4c191b867b3b0ed8009aaadbb9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8655e4c191b867b3b0ed8009aaadbb9.png)'
- en: Probability of a next word given a sequence of previous ones can be approximated
    by the probability of a next word given the N previous ones (e.g. N=2 — bi-gram
    LM) (image by author)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一系列前面的词，预测下一个词的概率可以通过给定N个前面词的概率来近似（例如，N=2 — 二元语言模型）。（图片来源：作者）
- en: 'where *t* is the number of words in the whole sequence and *N* is the size
    of the context (uni-gram (1), bi-gram (2), etc.). Now, the question is how to
    estimate those n-gram probabilities? The simplest approach is to use n-gram counts
    (to be calculated on a large text corpora in an “unsupervised” way):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t*是整个序列中的单词数，*N*是上下文的大小（单字模型（1），双字模型（2）等）。现在，问题是如何估计这些n-gram概率？最简单的方法是使用n-gram计数（可以通过“无监督”的方式在大文本语料库中计算）：
- en: '![](../Images/903414b12eb3d1f80e92ca41646a6bda.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/903414b12eb3d1f80e92ca41646a6bda.png)'
- en: 'The probability of a next word given the N previous ones: numerator — how many
    times the resulting sequence occurs in the data, denominator — how many times
    the sequence of previous words occurs in the data (image by author)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定前N个单词的下一个单词的概率：分子——结果序列在数据中出现的次数，分母——前一个单词序列在数据中出现的次数（图片来自作者）
- en: 'Obviously, the probability estimation from the equation above may appear to
    be naive. What if the numerator or even denominator values will be zero? This
    is why more advanced probability estimations include smoothing or backoff (e.g.,
    [add-k smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), [stupid backoff](https://aclanthology.org/D07-1090.pdf),
    [Kneser-Ney smoothing](https://ieeexplore.ieee.org/document/479394)). We won’t
    explore those methods here, however, conceptually the probability estimation approach
    doesn’t change with any smoothing or backoff method. The high-level representation
    of an n-gram LM is shown below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，以上方程中的概率估计可能显得过于简单。如果分子甚至分母的值为零怎么办？这就是为什么更高级的概率估计方法包括平滑或回退（例如，[加法平滑](https://en.wikipedia.org/wiki/Additive_smoothing)，[愚蠢回退](https://aclanthology.org/D07-1090.pdf)，[Kneser-Ney平滑](https://ieeexplore.ieee.org/document/479394)）。不过我们在这里不会深入探讨这些方法，然而从概念上讲，概率估计方法在任何平滑或回退方法下都不会改变。n-gram语言模型的高级表示如下所示：
- en: '![](../Images/e2479016e74d7eb7fb2bdc037d78f29f.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2479016e74d7eb7fb2bdc037d78f29f.png)'
- en: The high-level representation of an n-gram LM (image by author)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram语言模型的高级表示（图片来自作者）
- en: 'Having the counts calculated, how do we generate text from such LM? Essentially,
    the answer to this question applies to all LMs to be considered below. The process
    of selecting the next word given the probability distribution fron an LM is called
    *sampling*. Here are couple *sampling strategies* applicable to the n-gram LMs:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 得到计数之后，我们如何从这样的语言模型中生成文本呢？实际上，这个问题的答案适用于下面要讨论的所有语言模型。从语言模型中根据概率分布选择下一个单词的过程叫做*采样*。以下是适用于n-gram语言模型的几种*采样策略*：
- en: '*greedy sampling* — select the word with the highest probability;'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*贪婪采样*——选择具有最高概率的单词；'
- en: '*random sampling*— select the random word following the probability'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机采样*——根据概率选择下一个随机单词'
- en: distribution.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分布。
- en: Feedforward Neural Network Language Models
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈神经网络语言模型
- en: 'Despite smoothing and backoff, the probability estimation of the n-gram LMs
    is still intuitively too simple to model natural language. A game-changing approach
    of [Yoshua Bengio et al. (2000)](https://papers.nips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf)
    was very simple yet innovative: *what if instead of n-gram counts we will use
    neural networks to estimate word probabilities*? Although the paper claims that
    recurrent neural networks (RNNs) can be also used for this task, main content
    focuses on a feedforward neural network (FFNN) architecture.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有平滑和回退，n-gram语言模型的概率估计仍然直观上过于简单，无法建模自然语言。Yoshua Bengio等人（2000年）提出的一个具有改变游戏规则的创新方法非常简单，却很有创新性：*如果我们用神经网络来估计单词概率，而不是使用n-gram计数，会怎么样*？尽管论文声称递归神经网络（RNN）也可以用于这个任务，但主要内容集中在前馈神经网络（FFNN）架构上。
- en: 'The FFNN architecture proposed by Bengio is a simple multi-class classifier
    (the number of classes is the size of vocabulary *V*). The training process is
    based on the task of predicting a missing word *w* in the sequence of the context
    words *c*: P (*w|c*), where |*c*| is the *context window size*. The FFNN architecture
    proposed by Bengio et al. is shown below:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Bengio提出的FFNN架构是一个简单的多类分类器（类别数量为词汇表的大小*V*）。训练过程基于预测上下文单词*c*中的缺失单词*w*的任务：P (*w|c*)，其中|*c*|是*上下文窗口大小*。Bengio等人提出的FFNN架构如下所示：
- en: '![](../Images/20d54637d01b39090efa0804621bab9e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20d54637d01b39090efa0804621bab9e.png)'
- en: FFNN architecture for next word probability estimation (image by author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: FFNN架构用于下一个单词概率估计（图片来自作者）
- en: Such FFNN-based LMs can be trained on a large text corpora in an self-supervised
    manner (i.e., no explicitly labeled dataset is required).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的基于 FFNN 的语言模型可以在大规模文本语料库上以自监督的方式进行训练（即不需要显式标注的数据集）。
- en: 'What about sampling? In addition to the greedy and random strategies, there
    are two more that can be applied to NN-based LMs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 那么采样呢？除了贪婪和随机策略外，还有两种可以应用于基于神经网络的语言模型的策略：
- en: '*top-k sampling* — the same as greedy, but made within a renormalized set of
    top-k'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*top-k 采样* —— 与贪婪相同，但在经过重新归一化的 top-k 集合中进行'
- en: words (softmax is recalculated on top-k words),
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 词汇（softmax 在 top-k 词汇上重新计算），
- en: '*nucleus* *sampling*— the same as top-k, but instead of k as a number use percentage.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*nucleus* *采样* —— 与 top-k 相同，但使用百分比而不是 k 作为数字。'
- en: Recurrent Neural Network Language Models
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络语言模型
- en: To this point we were working with the assumption that the probability of the
    next word depends only on the previous one(s). We also considered a fixed context
    or n-gram size to estimate the probability. *What if the connections between words
    are also important to consider*? *What if we want to consider the whole sequence
    of preceding words to predict the next one*? This can be perfectly modeled by
    RNNs!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直假设下一个词的概率仅依赖于前一个（或多个）词。我们还考虑了固定的上下文或 n-gram 大小来估计概率。*如果词与词之间的连接也很重要呢*？*如果我们想要考虑整个前置词序列来预测下一个词呢*？这可以通过
    RNN 完美建模！
- en: 'Naturally, RNNs’ advantage is that they are able to capture dependencies of
    the whole word sequence while adding the hidden layer output from the previous
    step (*t-1*) to the input from the current step (*t*):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，RNN 的优势在于它们能够捕捉整个词序列的依赖关系，同时将来自前一步（*t-1*）的隐藏层输出添加到当前步（*t*）的输入中：
- en: '![](../Images/53aa8cc7d715ab41c6979e641f14d80d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53aa8cc7d715ab41c6979e641f14d80d.png)'
- en: Hidden layer output computation at step t for a simple RNN (image by author)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 简单 RNN 在步骤 t 处的隐藏层输出计算（图片来自作者）
- en: where *h* — hidden layer output, *g(x)* — activation function, *U* and *W* —
    weight matrices.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *h* — 隐藏层输出，*g(x)* — 激活函数，*U* 和 *W* — 权重矩阵。
- en: 'The RNNs are also trained following the self-supervised setting on a large
    text corpora to predict the next word given a sequence. The text generation is
    then performed via so-called *autoregressive generation* process, which is also
    called *causal language modeling generation*. The autoregressive generation with
    an RNN is demonstrated below:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 还在大规模文本语料库上按照自监督的设置进行训练，以预测给定序列的下一个词。然后通过所谓的 *自回归生成* 过程进行文本生成，这也被称为 *因果语言建模生成*。下面演示了使用
    RNN 进行的自回归生成：
- en: '![](../Images/05e5eb97d1523a8ebe931b96c16f7489.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05e5eb97d1523a8ebe931b96c16f7489.png)'
- en: Autoregressive text generation with a RNN and other examples listed (image by
    author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RNN 进行自回归文本生成和其他列出的示例（图片来自作者）
- en: In practice, canonical RNNs are rarely used for the LM tasks. Instead, there
    are improved RNN architectures such as stacked and [bidirectional](https://ieeexplore.ieee.org/document/650093),
    [long short-term memory (LSTM)](https://www.bioinf.jku.at/publications/older/2604.pdf)
    and its variations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，经典的 RNN 很少用于语言模型任务。相反，有改进的 RNN 架构，例如堆叠和 [双向](https://ieeexplore.ieee.org/document/650093)，[长短期记忆（LSTM）](https://www.bioinf.jku.at/publications/older/2604.pdf)
    及其变体。
- en: One of the most remarkable RNN architectures was proposed by [Sutskever et al.
    (2014)](https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
    —the *encoder-decoder* (or seq2seq) LSTM-based architecture. Instead of simple
    autoregressive generation, seq2seq model encodes an input sequence to an intermediate
    representation — context vector — and then uses autoregressive generation to decode
    it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[Sutskever 等人（2014）](https://proceedings.neurips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)
    提出了最著名的 RNN 架构之一 —— *编码器-解码器*（或 seq2seq）基于 LSTM 的架构。与简单的自回归生成不同，seq2seq 模型将输入序列编码为中间表示
    —— 上下文向量 —— 然后使用自回归生成解码它。'
- en: However, the initial seq2seq architecture had a major bottleneck — the *encoder
    narrows down the whole input sequence to the only one representation — context
    vector.* To remove this bottleneck, Bahdanau et al. (2014) introduces the attention
    mechanism, that (1) produces an individual context vector for every decoder hidden
    state (2) based on weighted encoder hidden states. Hence, the *intuition behind
    the attention mechanism is that every input word impacts every output word and
    the intensity of this impact varies*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最初的seq2seq架构存在一个主要瓶颈——*编码器将整个输入序列压缩为一个表示——上下文向量*。为了消除这个瓶颈，Bahdanau等人（2014年）提出了注意力机制，它（1）为每个解码器隐藏状态生成一个单独的上下文向量（2）基于加权的编码器隐藏状态。因此，*注意力机制的直觉是每个输入词都会影响每个输出词，而且这种影响的强度是变化的*。
- en: It is worth mentioning that RNN-based models are used for learning language
    representations. In particular, the most well known models are [ELMo](https://aclanthology.org/N18-1202.pdf)
    (2018) and [ULMFiT](https://aclanthology.org/P18-1031.pdf) (2018).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，基于RNN的模型被用于学习语言表示。特别是，最著名的模型包括[ELMo](https://aclanthology.org/N18-1202.pdf)（2018年）和[ULMFiT](https://aclanthology.org/P18-1031.pdf)（2018年）。
- en: 'Evaluation: Perplexity'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估：困惑度
- en: While considering LMs without applying them to a particular task (e.g. machine
    translation) there is one universal measure that may give us insights on how good
    is our LM is. This measure is called *Perplexity*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有将语言模型应用于特定任务（例如机器翻译）的情况下，有一个通用的衡量标准可以帮助我们了解语言模型的表现。这一标准叫做*困惑度*。
- en: '![](../Images/7df8bd08bbce35a6225e3e6a28ac3318.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7df8bd08bbce35a6225e3e6a28ac3318.png)'
- en: Perplexity formula (image by author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度公式（图片由作者提供）
- en: where *p* — probability distribution of the words, *N* — is the total number
    of words in the sequence, *wi* — represents the *i*-th word. Since Perplexity
    uses the concept of entropy, the intuition behind it is *how unsure a particular
    model about the predicted sequence*. The lower the perplexity, the less uncertain
    the model is, and thus, the better it is at predicting the sample.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*p* — 词的概率分布，*N* — 序列中的总词数，*wi* — 表示第*i*个词。由于困惑度使用了熵的概念，其直觉是*模型对预测序列的不确定程度*。困惑度越低，模型的预测不确定性越小，因而模型的预测能力越强。
- en: Transformer Language Models
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer语言模型
- en: The modern state-of-the-art LMs make use of the attention mechanism, introduced
    in the previous paragraph, and, in particular, [*self-attention*](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf),
    which is an integral part of the [*transformer* architecture](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现代最先进的语言模型（LMs）利用了前一段提到的注意力机制，特别是[*自注意力机制*](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)，它是[*Transformer架构*](https://papers.nips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)的一个重要组成部分。
- en: The transformer LMs have a significant advantage over the RNN LMs in terms of
    computation efficiency due to their ability to parallelize computations. In RNNs,
    sequences are processed one step at a time, this makes RNNs slower, especially
    for long sequences. In contrast, transformer models use a self-attention mechanism
    that allows them to process all positions in the sequence simultaneously. Below
    is a high-level representation of a transformer model with an *LM head*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer语言模型（LMs）在计算效率方面相较于RNN语言模型（RNN LMs）具有显著优势，因为它们能够并行化计算。在RNN中，序列是逐步处理的，这使得RNN的速度较慢，尤其是处理长序列时。相比之下，Transformer模型使用自注意力机制，能够同时处理序列中的所有位置。下面是带有*LM头*的Transformer模型的高层表示。
- en: '![](../Images/515e67db399b59feb3e17df0a4456a70.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/515e67db399b59feb3e17df0a4456a70.png)'
- en: A simplified architecture of a transformer LM (image by author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 简化版的Transformer语言模型架构（图片由作者提供）
- en: To represent the input token, transformers add token and position embeddings
    together. The last hidden state of the last transformer layer is typically used
    to produce the next word probabilities via the LM head. The transformer LMs are
    *pre-trained* following the self-supervised paradigm. When considering the decoder
    or encoder-decoder models, the pre-training task is to predict the next word in
    a sequence, similarly to the previous LMs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示输入标记，Transformer将标记和位置嵌入加在一起。通常，最后一个Transformer层的最后隐藏状态用于通过LM头生成下一个词的概率。Transformer语言模型是按照自监督范式进行*预训练*的。当考虑解码器或编码-解码器模型时，预训练任务是预测序列中的下一个词，类似于之前的语言模型。
- en: 'It is worth mentioning that *the most advances in the language modeling since
    the inception of transformers (2017) are lying in the two major directions*: (1)
    [model size scaling](https://huggingface.co/blog/large-language-models) and (2)
    [instruction fine-tuning](https://arxiv.org/abs/2308.10792) including [reinforcement
    learning with human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，*自从transformer（2017年）的诞生以来，语言建模的最大进展体现在两个主要方向*： (1) [模型规模扩展](https://huggingface.co/blog/large-language-models)
    和 (2) [指令微调](https://arxiv.org/abs/2308.10792)，包括[基于人类反馈的强化学习](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)。
- en: 'Evaluation: Instruction Benchmarks'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估：指令基准
- en: The instruction-tuned LMs are considered as general problem-solvers. Therefore,
    Perplexity might not be the best quality measure since it calculates the quality
    of such models *implicitly*. The *explicit* way of evaluating intruction-tuned
    LMs is based on on instruction benchmarks,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 经过指令微调的语言模型被认为是通用问题解决者。因此，困惑度可能不是最好的质量度量标准，因为它*隐含地*计算了这类模型的质量。评估经过指令微调的语言模型的*显式*方法是基于指令基准，
- en: such as [Massive Multitask Language Understanding (MMLU)](https://arxiv.org/abs/2009.03300),
    [HumanEval for code](https://arxiv.org/abs/2107.03374), [Mathematical Problem
    Solving (MATH)](https://arxiv.org/abs/2103.03874), and others.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 例如[大规模多任务语言理解（MMLU）](https://arxiv.org/abs/2009.03300)、[代码的HumanEval](https://arxiv.org/abs/2107.03374)、[数学问题求解（MATH）](https://arxiv.org/abs/2103.03874)等。
- en: Summary
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We considered here the evolution of language models in the context of text generation
    that covers at least last three decades. Despite not diving deeply into the details,
    it is clear how language models have been developing since the 1990s.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里考虑了语言模型在文本生成中的演变，涵盖了至少过去三十年的发展。尽管没有深入探讨细节，但可以清楚地看到自1990年代以来，语言模型的发展历程。
- en: The n-gram language models approximated the next word probability using the
    n-gram counts and smoothing methods applied to it. To improve this approach, feedforward
    neural network architectures were proposed to approximate the word probability.
    While both n-gram and FFNN models considered only a fixed number of context and
    ignored the connections between the words in an input sentence, RNN LMs filled
    this gap by naturally considering connections between the words and the whole
    sequence of input tokens. Finally, the transformer LMs demonstrated better computation
    efficiency over RNNs as well as utilized self-attention mechanism for producing
    more contextualized representations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: n-gram语言模型通过使用n-gram计数和应用平滑方法来近似下一个单词的概率。为了改进这种方法，提出了前馈神经网络架构来近似单词的概率。尽管n-gram模型和FFNN模型仅考虑了固定数量的上下文，并忽略了输入句子中单词之间的联系，但RNN语言模型通过自然地考虑单词之间的联系以及整个输入令牌序列填补了这一空白。最终，transformer语言模型在计算效率上优于RNN，并利用自注意力机制生成了更多上下文化的表示。
- en: Since the invention of the transformer architecture in 2017, the biggest advances
    in language modeling are considered to be the model size scaling and instruction
    fine-tuning including RLHF.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年transformer架构的发明以来，语言建模领域的最大进展被认为是模型规模扩展和指令微调，包括基于人类反馈的强化学习（RLHF）。
- en: References
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: I would like to acknowledge Dan Jurafsky and James H. Martin for their [Speech
    and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) book that was
    the main source of inspiration for this article.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我想感谢Dan Jurafsky和James H. Martin的[《语音与语言处理》](https://web.stanford.edu/~jurafsky/slp3/)一书，这本书是本文的主要灵感来源。
- en: The other references are included as hyperlinks in the text.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参考资料已作为超链接包含在文中。
- en: Postscriptum
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后记
- en: Text me [contact (at) perevalov (dot) com] or visit my [website](https://perevalov.com/)
    if you want to get more knowledge on applying LLMs in real-world industrial use
    cases (e.g. AI Assistants, agent-based systems and many more).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多有关在实际工业用例（如AI助手、基于代理的系统等）中应用LLM的知识，请通过[contact (at) perevalov (dot)
    com]与我联系或访问我的[网站](https://perevalov.com/)。
