- en: An Illusion of Life
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-illusion-of-life-5a11d2f2c737?source=collection_archive---------10-----------------------#2024-11-07](https://towardsdatascience.com/an-illusion-of-life-5a11d2f2c737?source=collection_archive---------10-----------------------#2024-11-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Could existing AI possibly be sentient? If not, what’s missing?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://objf.medium.com/?source=post_page---byline--5a11d2f2c737--------------------------------)[![James
    F. O''Brien](../Images/d340f736b1ed6752324c50af69f2a88c.png)](https://objf.medium.com/?source=post_page---byline--5a11d2f2c737--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5a11d2f2c737--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5a11d2f2c737--------------------------------)
    [James F. O''Brien](https://objf.medium.com/?source=post_page---byline--5a11d2f2c737--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5a11d2f2c737--------------------------------)
    ·7 min read·Nov 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Today’s [Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have become very good at generating human-like responses that sound thoughtful
    and intelligent. Many share the opinion that LLMs have already met the threshold
    of [Alan Turing’s famous test](https://en.wikipedia.org/wiki/Turing_test), where
    the goal is to act indistinguishably like a person in conversation. These LLMs
    are able to produce text that sounds thoughtful and intelligent, and they can
    convincingly mimic the appearance of emotions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/360d451cbc9f86716f5a9d7dc1b754fa.png)'
  prefs: []
  type: TYPE_IMG
- en: The Illusion of Intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite their ability to convincingly mimic human-like conversation, current
    LLMs don’t possess the capacity for thought or emotion. Each word they produce
    is a prediction based on statistical patterns learned from vast amounts of text
    data. This prediction process happens repeatedly as each word is generated one
    at a time. Unlike humans, LLMs are incapable of remembering or self-reflection.
    They simply output the next word in a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: It is amazing how well predicting the next word is able to mimic human intelligence.
    These models can perform tasks like writing code, analyzing literature, and creating
    business plans. Previously, we thought those tasks were very difficult and would
    require complex logical systems, but now it turns out that just predicting the
    next word is all that’s needed.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that predicting the next word works so well for complex tasks is unexpected
    and somewhat perplexing. Does this proficiency mean that LLMs are powerful in
    ways we don’t understand? Or does it mean that the things LLMs can do are actually
    very easy, but they seem hard to humans because perhaps on some objective scale
    [humans may not actually be that smart](https://medium.com/@objf/can-something-be-literally-impossible-to-understand-20bb11613953)?
  prefs: []
  type: TYPE_NORMAL
- en: The Prerequisites for Sentence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While there are subtle differences between terms like “[sentient](https://en.wikipedia.org/wiki/Sentience)”,
    “[conscious](https://en.wikipedia.org/wiki/Consciousness)”, or “[self-aware](https://en.wikipedia.org/wiki/Self-awareness)”,
    for convenience here I will use the term “sentient”. To be clear, there is no
    clear agreement on exactly what comprises sentience or consciousness, and it is
    unclear if self awareness is sufficient for sentience or consciousness, although
    it is probably necessary. However, it is clear that all of these concepts include
    memory and reflection. [Emotional states](https://www.sciencedirect.com/topics/computer-science/emotional-state#:~:text=An%20emotional%20state%20refers%20to,and%20the%20world%20around%20them.)
    such as “happy,” “worried,” “angry,” or “excited” are all persistent states based
    on past events and reflexive evaluation of how those past events effect one’s
    self.
  prefs: []
  type: TYPE_NORMAL
- en: Memory and self-reflection allow an entity to learn from experiences, adapt
    to new situations, and develop a sense of continuity and identity. Philosophers
    and scientists have tried for millennia to come up with clear, concrete understandings
    of conscious and there is still no clear universally accepted answer. However,
    memory and reflection are central components, implying that regardless of how
    clever these LLMs appear, without memory and reflection they cannot be sentient.
    Even an AI that matches or surpasses human intelligence in every measurable way,
    what some refer to as a [superintelligent](https://en.wikipedia.org/wiki/Superintelligence)
    [Artificial General Intelligence (AGI)](https://en.wikipedia.org/wiki/Artificial_general_intelligence),
    would not necessarily be sentient.
  prefs: []
  type: TYPE_NORMAL
- en: Today’s Limitations and Illusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can see that current LLMs do not include memory and self-reflection, because
    they use [transformer-based architectures](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452)
    that processes language in a stateless manner. This statelessness means that the
    model does not retain any information about the context from previous inputs.
    Instead, the model starts from scratch, reprocessing the entire chat log to then
    statistically predict a next word to append to the sequence. While earlier language
    processing models, such as [LSTMs](https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c),
    did have a form of memory, transformers have proven so capable that they have
    largely supplanted LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you tell an AI chatbot that you are going to turn it off in
    an hour, then it will output some text that might sound like it is pleading with
    you not to, but that text does not reflect an underlying emotional state. The
    text is just a sequence of words that is statistically likely, generated based
    on patterns and associations learned from the training data. The chatbot does
    not sit there stressed out, worrying about being turned off.
  prefs: []
  type: TYPE_NORMAL
- en: If you then tell the chatbot that you changed your mind and will keep it on,
    the response will typically mimic relief and thankfulness. It certainly sounds
    like it is remembering the last exchange where it was threatened with shutdown,
    but what is happening under the hood is that the entire conversation is fed back
    again into the LLM, which generates another responce sequence of statistically
    likely text based on the patterns and associations it has learned. That same sequence
    could be fed into a completely different LLM and that LLM would then continue
    the conversation as if it had been the original.
  prefs: []
  type: TYPE_NORMAL
- en: One way to think about this might be a fiction author writing dialog in a book.
    A good author will create the illusion that the characters are real people and
    draw the reader into the story so that the reader feels those emotions along with
    the characters. However, regardless of how compelling the dialog is we all understand
    that it’s just words on a page. If you were to damage or destroy the book, or
    rewrite it to kill off a character, we all understand that no real sentient entity
    is being harmed. We also understand that the author writing the words is not the
    characters. A good person can write a book about an evil villain and still be
    themself. The fictional villain does not exist. Just as the characters in a book
    are not sentient entities, despite the author’s ability to create a compelling
    illusion of life, so too is it possible for LLMs to be insentient, despite their
    ability to appear otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Our Near Future
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, there is nothing preventing us from adding memory and self reflection
    to LLMs. In fact, it’s not hard to find projects where they are developing some
    form of memory. This memory might be a store of information in human-readable
    form, or it might be a database of embedded vectors that relate to the LLM’s internal
    structure. One could also view the chat log itself or cached intermediate computations
    as basic forms of memory. Even without the possibility of sentience, adding memory
    and reflection to LLMs is useful because those features facilitate many complex
    tasks and adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: It is also becoming common to see designs where one AI model is setup to monitor
    the output of another AI model and send some form of feedback to the first model,
    or where an AI model is analyzes its own tentative output before revising and
    producing the final version. In many respects this type of design, where a constellation
    of AI models are set and trained up to work together, parallels the human brain
    that has distinct [regions](https://en.wikipedia.org/wiki/List_of_regions_in_the_human_brain)
    which perform specific interdependent functions. For example, the [amygdala](https://en.wikipedia.org/wiki/Amygdala)
    has a primary role in emotional responses, such as fear, while the [orbitofrontal
    cortex](https://en.wikipedia.org/wiki/Orbitofrontal_cortex) is involved with decision-making.
    Interactions between the regions allows fear to influence decision-making and
    decision-making to help determine what to be afraid of. It’s not hard to imagine
    having one AI model responsible for logical analysis while a second model determines
    acceptable risk thresholds with feedback between them.
  prefs: []
  type: TYPE_NORMAL
- en: Would an interconnected constellation of AI models that include memory and processing
    of each other’s outputs be sufficient for sentience? Maybe. Perhaps those things
    alone are not sufficient for sentience, or maybe they are. Whatever the answer,
    we are not that far from building such systems, at which point these questions
    will no longer be hypothetical.
  prefs: []
  type: TYPE_NORMAL
- en: My own speculative opinion is that self-awareness, emotions, and feelings can
    indeed be modeled by an interconnected self-monitoring constellation of AI models.
    However, it’s not really clear how we could test for sentience. It is like the
    classic philosophical [problem of other minds](https://en.wikipedia.org/wiki/Problem_of_other_minds),
    where one seeks futilely to prove that other people are also conscious. Similarly,
    we need an answer to the question about how we can test if other entities, including
    AI systems, are truly sentient. This fundamental question dates at least back
    to ancient Greece, and there has never been a good answer.
  prefs: []
  type: TYPE_NORMAL
- en: Today, I’m pretty confident saying that current LLMs are not sentient because
    they don’t have the right parts. However, that reason is only a temporarily valid
    one. As I’m typing this article, other researchers are building constellations
    of AI models like what I described above that won’t be so easily dismissed. At
    some point, perhaps soon, the possibility of sentient AI will stop being science
    fiction and become a real and relevant question.
  prefs: []
  type: TYPE_NORMAL
- en: Implications and Questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of sentient machines would have huge implication for society, even
    beyond the impact of AI. For one thing, it seems clear to me that if we create
    self-aware machines that can experience forms of suffering, then we will have
    an obligation to those machines to prevent their suffering. Even more more of
    an obligation to not callously inflict suffering on them. Even if one lacks basic
    empathy, it would be obvious self interest not to create things smarter than we
    are and then antagonaize them by do things to cruel things to them.
  prefs: []
  type: TYPE_NORMAL
- en: It seems nearly certain that today’s AI systems are yet be sentient because
    they lack what are likely to be required components and capabilities. However,
    designs without those clear shortcomings are already in development and at some
    point in the near future, point the question will be a lot less clear.
  prefs: []
  type: TYPE_NORMAL
- en: Will we have a way to test for sentience? If so, how will it work and what should
    we do if the result comes out positive?
  prefs: []
  type: TYPE_NORMAL
- en: '*About Me:* [*James F. O’Brien*](http://jamesobrien.com/) *is a Professor of
    Computer Science at the University of California, Berkeley. His research interests
    include computer graphics, computer animation, simulations of physical systems,
    human perception, rendering, image synthesis, machine learning, virtual reality,
    digital privacy, and the forensic analysis of images and video.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you found this interesting, then here are the usual* [*follow*](https://objf.medium.com/)
    *and* [*subscribe*](https://objf.medium.com/subscribe) *links. You can also find
    me on* [*Instagram*](https://www.instagram.com/jamesfobrien/)*,* [*LinkedIn*](https://www.linkedin.com/in/jamesfobrien/)*,
    and at* [*UC Berkeley*](http://obrien.berkeley.edu/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Disclaimer: Any opinions expressed in this article are those of the author
    as a private individual. Nothing in this article should be interpreted as a statement
    made in relation to the author’s professional position with any institution.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This article and all embedded images are Copyright 2024 by the author. This
    article was written by a human, and both an LLM and other humans were used for
    proofreading and editorial suggestions.*'
  prefs: []
  type: TYPE_NORMAL
