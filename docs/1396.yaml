- en: Data Disruptions to Elevate Entity Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/data-disruptions-to-elevate-entity-embeddings-b1ddf86a3c95?source=collection_archive---------7-----------------------#2024-06-04](https://towardsdatascience.com/data-disruptions-to-elevate-entity-embeddings-b1ddf86a3c95?source=collection_archive---------7-----------------------#2024-06-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Injecting random values during neural network training can help you get more
    from your categoricals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vla6?source=post_page---byline--b1ddf86a3c95--------------------------------)[![Valerie
    Carey](../Images/9ef394fe5a6a5439521c1905e0195751.png)](https://medium.com/@vla6?source=post_page---byline--b1ddf86a3c95--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b1ddf86a3c95--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b1ddf86a3c95--------------------------------)
    [Valerie Carey](https://medium.com/@vla6?source=post_page---byline--b1ddf86a3c95--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b1ddf86a3c95--------------------------------)
    ·11 min read·Jun 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65a2fea5cd298a5eeaefac2afe531fcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [dylan nolte](https://unsplash.com/@dylan_nolte?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Today I will discuss a **stochastic regularization** method to improve generalizability
    of entity embeddings in neural network models. I use a data generator to randomly
    inject selected input values into data during training, to help a model learn
    how to deal with unseen codes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance improvements are especially dramatic for hierarchical categorical
    features.** Randomization helps models leverage higher-level group information
    to compensate for unseen lower-level codes.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding noise, removing information, or otherwise messing with data, is often
    used to increase model robustness and reduce overfitting [1,2]. Here, it’s used
    to help a model learn what to do with missing categorical information. I examine
    one public test dataset, comparing unmodified data with randomization done two
    ways.
  prefs: []
  type: TYPE_NORMAL
- en: When unseen codes matter, randomly injecting vales helps a model generalize.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using a data generator to shuffles random values so each mini-batch sees different
    scenarios performs better than static data modification.
  prefs: []
  type: TYPE_NORMAL
- en: A caveat is that overfitting can occur when a coding hierarchy is unrelated
    to…
  prefs: []
  type: TYPE_NORMAL
