- en: 'Deep Learning Illustrated, Part 5: Long Short-Term Memory (LSTM)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deep-learning-illustrated-part-5-long-short-term-memory-lstm-d379fbbc9bc6?source=collection_archive---------2-----------------------#2024-06-21](https://towardsdatascience.com/deep-learning-illustrated-part-5-long-short-term-memory-lstm-d379fbbc9bc6?source=collection_archive---------2-----------------------#2024-06-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An illustrated and intuitive guide on the inner workings of an LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page---byline--d379fbbc9bc6--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page---byline--d379fbbc9bc6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d379fbbc9bc6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d379fbbc9bc6--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page---byline--d379fbbc9bc6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d379fbbc9bc6--------------------------------)
    ·9 min read·Jun 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to Part 5 in our illustrated journey through Deep Learning!
  prefs: []
  type: TYPE_NORMAL
- en: '![Shreya Rao](../Images/45d3d481fab74a720c78346bc47e95fd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----d379fbbc9bc6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning, Illustrated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@shreya.rao/list/deep-learning-illustrated-ae6c27de1640?source=post_page-----d379fbbc9bc6--------------------------------)5
    stories![](../Images/9668eeb3fd221bb26c2341a0ec0bfeab.png)![](../Images/1c261ce54b80b877b7737964ba5bf3f2.png)![](../Images/10364c8fdf64c9c6fb8300ce74259d00.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Today we’re going to talk about Long Short-Term Memory (LSTM) networks, which
    are an upgrade to regular Recurrent Neural Networks (RNN) which we discussed in
    the [previous article](https://medium.com/towards-data-science/deep-learning-illustrated-part-4-recurrent-neural-networks-d0121f27bc74).
    We saw that RNNs are used to solve sequence-based problems but struggle with retaining
    information over long distances, leading to short-term memory issues. Here’s where
    LSTMs come in to save the day. They use the same recurrent aspect of RNNs but
    with a twist. So let’s see how they achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: '*Sidenote — this is one of my favorite articles I’ve written, so I can’t wait
    to take you on this journey!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first see what was happening in our RNN previously. We had a neural network
    with an input *x*, one hidden layer that consists of one neuron with the tanh
    activation function, and one output neuron with the sigmoid activation function.
    So the first step of the RNN looks something like this:'
  prefs: []
  type: TYPE_NORMAL
