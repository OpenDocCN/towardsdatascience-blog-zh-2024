["```py\npip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n```", "```py\nimport gc\nimport os\n\nimport torch\nimport wandb\nfrom datasets import load_dataset\nfrom google.colab import userdata\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n)\nfrom trl import ORPOConfig, ORPOTrainer, setup_chat_format\nwb_token = userdata.get('wandb')\nwandb.login(key=wb_token)\n```", "```py\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    attn_implementation = \"flash_attention_2\"\n    torch_dtype = torch.bfloat16\nelse:\n    attn_implementation = \"eager\"\n    torch_dtype = torch.float16\n```", "```py\n# Model\nbase_model = \"meta-llama/Meta-Llama-3-8B\"\nnew_model = \"OrpoLlama-3-8B\"\n\n# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = prepare_model_for_kbit_training(model)\n```", "```py\ndataset_name = \"mlabonne/orpo-dpo-mix-40k\"\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=42).select(range(1000))\n\ndef format_chat_template(row):\n    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= os.cpu_count(),\n)\ndataset = dataset.train_test_split(test_size=0.01)\n```", "```py\norpo_args = ORPOConfig(\n    learning_rate=8e-6,\n    beta=0.1,\n    lr_scheduler_type=\"linear\",\n    max_length=1024,\n    max_prompt_length=512,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    report_to=\"wandb\",\n    output_dir=\"./results/\",\n)\n\ntrainer = ORPOTrainer(\n    model=model,\n    args=orpo_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()\ntrainer.save_model(new_model)\n```", "```py\n# Flush memory\ndel trainer, model\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Merge adapter with base model\nmodel = PeftModel.from_pretrained(model, new_model)\nmodel = model.merge_and_unload()\nmodel.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n```"]