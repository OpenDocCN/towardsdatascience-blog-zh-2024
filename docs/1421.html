<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Applied LLM Quantisation with AWS Sagemaker | Analytics.gov</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Applied LLM Quantisation with AWS Sagemaker | Analytics.gov</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07">https://towardsdatascience.com/applied-llm-quantisation-with-aws-sagemaker-analytics-gov-ab210bd6697d?source=collection_archive---------3-----------------------#2024-06-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f3d2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">Host production-ready LLMs endpoints at twice the speed but one fifth the cost.</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="James Teo" class="l ep by dd de cx" src="../Images/393a3137764cce6bcc760d8bc980e78c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Eveya8xokDqmYbPx5b3l_Q.jpeg"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@james-teo?source=post_page---byline--ab210bd6697d--------------------------------" rel="noopener follow">James Teo</a></p></div></div></div><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="ic id ah ai aj ak al am an ao ap aq ar ie if ig" disabled="">Follow</button></p></div></div></span></div></div><div class="l ih"><span class="bf b bg z dx"><div class="ab cn ii ij ik"><div class="il im ab"><div class="bf b bg z dx ab in"><span class="io l ih">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ab210bd6697d--------------------------------" rel="noopener follow"><p class="bf b bg z ip iq ir is it iu iv iw bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ia ib" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="ix iy l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo"><div class="h k w ea eb q"><div class="ke l"><div class="ab q kf kg"><div class="pw-multi-vote-icon ed io kh ki kj"><div class=""><div class="kk kl km kn ko kp kq am kr ks kt kj"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ku kv kw kx ky kz la"><p class="bf b dy z dx"><span class="kl">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kk lb lc ab q ee ld le" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jp jq jr js jt ju jv jw jx jy jz ka kb kc kd"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap ie li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap ie ls lt le lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/39d0752a052e81ea9d506acbe5077570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SeoofuvBWpXi3CSA_WECUA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author, Generated in Analytics.gov with AWS Sagemaker Jumpstart - Stable Diffusion XL 1.0 (open-source)</figcaption></figure><p id="0968" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">Disclosure: I am a Data Engineer with Singapore’s Government Technology Agency (GovTech) Data Science and Artificial Intelligence Division (DSAID). As one of the key developers working on Analytics.gov, I work with agencies across the entire public sector to develop Data Science and AI/ML capabilities for public good.</em></p><h1 id="191e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Table of Contents</h1><ol class=""><li id="6d14" class="nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx pa pb pc bk"><a class="af pd" href="#270c" rel="noopener ugc nofollow">Preamble</a></li><li id="6953" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#a831" rel="noopener ugc nofollow">Why use open-source models?</a></li><li id="f26f" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#9f25" rel="noopener ugc nofollow">Blockers for Hosting Open-source LLMs</a></li><li id="1ebc" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#1db5" rel="noopener ugc nofollow">What is quantisation and how can it help?</a></li><li id="b950" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#fc28" rel="noopener ugc nofollow">How do AWS Sagemaker Endpoints work?</a></li><li id="aafb" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#4036" rel="noopener ugc nofollow">Hosting a Quantised Model in AG Sagemaker</a></li><li id="9de8" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#87cd" rel="noopener ugc nofollow">Benchmarks</a></li><li id="f236" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><a class="af pd" href="#9809" rel="noopener ugc nofollow">Conclusion</a></li></ol><h1 id="270c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">1. Preamble</h1><p id="c61b" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">If you haven’t read our previous publications, you can peruse them here!</em></p><div class="pj pk pl pm pn po"><a href="https://medium.com/dsaid-govtech/accelerating-machine-learning-and-ai-impact-with-mlops-on-analytics-gov-ada449f216b6?source=post_page-----ab210bd6697d--------------------------------" rel="noopener follow" target="_blank"><div class="pp ab ih"><div class="pq ab co cb pr ps"><h2 class="bf fr hx z ip pt ir is pu iu iw fp bk">Accelerating Machine Learning and AI impact with MLOps on Analytics.gov</h2><div class="pv l"><h3 class="bf b hx z ip pt ir is pu iu iw dx">Introduction to Analytics.gov</h3></div><div class="pw l"><p class="bf b dy z ip pt ir is pu iu iw dx">medium.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc lr po"/></div></div></a></div><div class="pj pk pl pm pn po"><a href="https://medium.com/dsaid-govtech/productionising-llms-and-ml-models-with-analytics-gov-moms-journey-into-ai-solution-deployment-bad4ceb12df2?source=post_page-----ab210bd6697d--------------------------------" rel="noopener follow" target="_blank"><div class="pp ab ih"><div class="pq ab co cb pr ps"><h2 class="bf fr hx z ip pt ir is pu iu iw fp bk">Productionising LLMs and ML Models with Analytics.gov: MOM’s Journey into AI Solution Deployment</h2><div class="pv l"><h3 class="bf b hx z ip pt ir is pu iu iw dx">Shoutout to our co-contributors for this article: MOM Forward Deployed Team (Barry Tng, Ethan Mak, Joel Koo), and…</h3></div><div class="pw l"><p class="bf b dy z ip pt ir is pu iu iw dx">medium.com</p></div></div><div class="px l"><div class="qd l pz qa qb px qc lr po"/></div></div></a></div><p id="caa8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Analytics.gov (AG), developed by GovTech Singapore’s Data Science and Artificial Intelligence Division (DSAID), is a Central Machine Learning Operations (MLOps) platform that productionises ML and AI use cases for the Whole-of-Government (WOG). Hosted on Government Commercial Cloud (GCC) 2.0, it utilises best-practice network and security configurations to provide a safe and secure environment for all data science and AI needs. Through AG, government officers are able to access compute resources, managed AI services and other utilities directly from their government issued laptops without the need for managing or developing new infrastructure, thereby fast-tracking AI/ML initiatives across the whole of government.</p><p id="1a7c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">AG provides custom functionalities to create and manage production-ready inference endpoints for quantised models through the capabilities offered by AWS Sagemaker Endpoints. With just a few lines of code, end users can quickly set up their own private inference endpoints for quantised models, reducing what could have taken days or weeks of work into mere minutes. This substantially lowers the barrier of entry for agencies across the whole of government to leverage the power of GenAI with greater efficiency and cost-effectiveness.</p><p id="12c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this article, we will explore how AG enables government agencies to run LLMs efficiently and cost-effectively. Our goal is to demystify model quantisation, illustrate how we streamlined the process of hosting quantised open-source LLMs in AWS Sagemaker, and provide benchmarks to gauge the gains in performance and cost-efficiency.</p><h1 id="a831" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">2. Why use open-source models?</h1><p id="c552" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">For a brilliant read on Open LLMs, please view Sau Sheong’s publication here! (Note: its a medium member-only story)</em></p><div class="pj pk pl pm pn po"><a href="https://sausheong.com/programming-with-ai-open-llms-28091f77a088?source=post_page-----ab210bd6697d--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pp ab ih"><div class="pq ab co cb pr ps"><h2 class="bf fr hx z ip pt ir is pu iu iw fp bk">Programming with AI — Open LLMs</h2><div class="pv l"><h3 class="bf b hx z ip pt ir is pu iu iw dx">Using Open LLMs in LLM Applications</h3></div><div class="pw l"><p class="bf b dy z ip pt ir is pu iu iw dx">sausheong.com</p></div></div><div class="px l"><div class="qe l pz qa qb px qc lr po"/></div></div></a></div><p id="ddeb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">I highly recommend it, as it sheds light on hosting open-source LLMs as APIs, providing a great complement to this article.</em></p><p id="e17f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Security &amp; Sensitivity</strong></p><p id="2004" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Open-source models can be hosted privately on your own devices or cloud environments, meaning that queries to your model do not get sent to third-party providers. This is particularly crucial with government data, as a large majority of it contains sensitive information.</p><p id="0589" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Controlled Output Generation</strong></p><p id="0f66" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Usage of open-sourced models can be controlled on a more granular level. Closed-sourced models have to be interfaced via exposed commercial APIs which abstracts out complexity but reduces the degree of control over the model. Locally hosted open-sourced models allow for full control over the output generation, this is important as many useful libraries such as <a class="af pd" href="https://lmql.ai/docs/models/openai.html#openai-api-limitations" rel="noopener ugc nofollow" target="_blank">LMQL</a> and <a class="af pd" href="https://github.com/guidance-ai/guidance?tab=readme-ov-file#vertex-ai" rel="noopener ugc nofollow" target="_blank">Guidance</a> work better with locally hosted models.</p><p id="e133" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Variety</strong></p><p id="145b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As of writing, there are over 600k models in HuggingFace, including the models posted by major players such as Meta and Google and individual contributors who publish their own variants. Some variants are fine-tuned for specific purposes/tasks, which can be used out of the box. Users can simply reuse these models instead of fine-tuning their own.</p><p id="1f51" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, <a class="af pd" href="https://huggingface.co/aisingapore/sea-lion-7b" rel="noopener ugc nofollow" target="_blank">AiSingapore’s SEA-LION</a> model is instruct-tuned for the Southeast Asia (SEA) region languages, where its training dataset consists of diverse languages from Malay to Thai. Utilising this model would save the effort in obtaining large amounts of datasets in different languages and computational cost of fine-tuning.</p><h1 id="9f25" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">3. Blockers for Hosting Open-source LLMs</h1><p id="f048" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Language Models come in many shapes and sizes, popular models range from TinyLlama (1.1B) to the upcoming Llama-3 400B+. While Small Language Models (SLM) like TinyLlama works well for smaller and more straightforward use cases, complex use cases usually require the “smarter” Large Language Models (LLM). It goes without saying that all GenAI applications would benefit from having better output quality from the larger LLMs, however with extra size also comes with extra tradeoffs.</p><p id="9070" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To maximise the speed of inference, models have to be fully loaded in GPU memory as any movement between disk and GPU memory or CPU and GPU memory would introduce overheads that can substantially slow down inference speeds.</p><p id="ad6b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">LLMs require massive amounts of memory to host, the bigger the LLM, the more GPU memory is required to host it. Most large models demand multiple GPUs to fully host in memory, making it an extremely resource intensive and expensive task.</p><p id="8862" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Naturally, as the size of the model increases, more computation is required for each inference task. Consequently, the larger the LLMs, the lower the inference speed.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/d20e7536584ab714bc1046d07f70ab2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cd8AokvenK2RFYBwEbOqGg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Transformers BF16 Inference Benchmark by Author</em></figcaption></figure><h2 id="a1cd" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">Just how big are these models?</strong></h2><p id="7dcb" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The size of these LLMs can be estimated with the following formula (Note, this is a naive estimation and model sizes are almost always slightly larger.)</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/90355390f5ae733ad064c1f1a40e2472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6Ui2dRmeeXlF6GYChnu5w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Simplified Formula for Calculating Model Size by Author, Inspired by </em><a class="af pd" href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/" rel="noopener ugc nofollow" target="_blank"><em class="hd">https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/</em></a></figcaption></figure><p id="2824" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using the formula we can estimate the model size for some popular models:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/9ef3247db8e9f4a67f14bbb4dd1be710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g49d8OPo3ulPWrpQd5EJ7w.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Table of Model Sizes for Popular Models by Author</em></figcaption></figure><p id="4be8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">Note: The formula merely estimates the model size, real world GPU requirements will certainly be much larger and are different depending on other factors. (As you will see in the later section on Benchmarks, the actual GPU requirements completely blows these estimates out of the water). “BF16” stands for the number format brain float 16, while “FP16” stands for floating point 16.</em></p><p id="fc84" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The upcoming Meta’s Llama-3 400B+ will be one of the biggest open-source models available when it is released. We can estimate that this beast would be as big as 800 GB. For context, 800 GB would require at least 10 x A100 80GB GPU cards to host even if we naively assume zero hosting overheads.</p><p id="4947" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another popular but more reasonably sized model — Llama-3 70B published at bf16 or 16 bits per weight (bpw) precision, would still require 141.2 GB of GPU memory to host for inference.</p><h2 id="32c8" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">Why is Large GPU Memory Requirements an issue?</strong></h2><p id="7e90" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">As GPUs are in short supply and high demand currently, it’s not easy to find multiple GPU chips for cheap. Hosting LLMs in their raw and unquantised format can thus be a very expensive business that is only available to the privileged few that can afford it. This can be limiting for projects that require the wisdom of LLMs but is not valuable enough to warrant the use of multiple scarce and expensive GPUs.</p><p id="fbbc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Slower inference speeds from larger LLM sizes also results in:</p><ol class=""><li id="d0ba" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Worse user experience due to slow output.</li><li id="dd44" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Reduced total possible throughput that can be extracted by downstream applications. For applications that are heavy on token usage such as text-summarisation or report generation, the reduced throughput can seriously hurt the viability of the application.</li></ol><p id="9855" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Slow inference and expensive costs are debilitating factors for production-grade use cases, hence each GenAI application will need to make the tradeoff between output quality, inference speed and cost.</p><h1 id="1db5" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">4. What is quantisation and how can it help?</h1><h2 id="063f" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">What is Quantisation?</strong></h2><p id="9738" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">For a more rigorous explanation on Quantisation, please refer to to these two fantastic guides: </em><a class="af pd" href="https://www.tensorops.ai/post/what-are-quantized-llms" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.tensorops.ai/post/what-are-quantized-llms</em></a><em class="ny">, </em><a class="af pd" href="https://www.semianalysis.com/p/neural-network-quantization-and-number" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.semianalysis.com/p/neural-network-quantization-and-number</em></a></p><p id="d76f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">For simplicity, the following section will only refers to Post-Training Quantisation (PTQ)</em></p><p id="0494" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In simple terms, in the domain of AI/ML, Quantisation is a technique for reducing the size of a model. Underneath the hood, model weights are stored as numbers. Typically these weights are stored in number formats like floating point 16 (FP16) or brain float 16 (BF16), which as the name suggests, takes 16 bits to store a number.</p><p id="405e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Quantisation reduces the number of bits required to store each number, this allows the storage size of the model to be reduced as less bits are used to store each model weight.</p><p id="da8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, using fewer bits per weight means the precision of the weights is reduced. This is why Quantisation is aptly described by most articles as “reducing the precision of model weights”.</p><p id="9135" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For visual learners here is <strong class="ne fr">π</strong> represented in different precisions:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/89bce72766d2c9c1a3fdddc9d2dd25dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIjP0e4DpVQRg_tv6uI0Fg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Representation of </em><strong class="bf ob">π</strong><em class="hd"> in different precisions by Author</em></figcaption></figure><p id="6676" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can try for yourself using this <a class="af pd" href="https://observablehq.com/@benaubin/floating-point" rel="noopener ugc nofollow" target="_blank"><em class="ny">floating point calculator</em></a><em class="ny">.</em></p><p id="bdeb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note: Modern quantisation methods may use bespoke number formats rather than FP series to quantise models. These can go as low as 1 bit quantisation (Q1).</p><p id="d0f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As seen in the table, the precision of <strong class="ne fr">π</strong> is reduced as the number of bits decreases. This not only affects the number of decimal places, but also in the approximation of the number itself.</p><p id="90fc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, 3.141592502593994 cannot be represented exactly in FP8, so it has to be rounded off to the nearest possible value that FP8 can represent — 3.125, this is also known as Floating Point Error.</p><h2 id="ad49" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">How does it help?</strong></h2><p id="d049" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">As the number of bits per weight decreases, total GPU memory requirement is also reduced. For instance, a FP16 to 8-bit Quantisation (Q8) reduces the amount of bits required to store each number from 16 bits to 8 bits. This reduces the size of the model by 50%.</p><p id="0dbc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To put this in an example, an unquantised FP16 Mistral 7B is estimated to be about 14.48 GB in size, while a Q8 Mistral 7B is only 7.24 GB. A Q4 Mistral 7b will only be a mere 3.62 GB, making it possible to load into some mobile devices.</p><p id="3999" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Not only does reduction in memory reduce the minimum computation requirements to host a model, it also improves inference speeds.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/908427595f63835aba24552068d2ac8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJdJVHQENCwJQ1U_d2_fNg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">7B Model benchmarked in Various Quants by Author</em></figcaption></figure><h2 id="6118" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">What’s the catch?</strong></h2><p id="fd02" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Of course there is no free lunch in this world! Reduction in precision will impact the output quality of the model. Relating to our previous Table on Representation of <strong class="ne fr">π</strong>, a <strong class="ne fr">π</strong> represented in FP16 would probably be accurate enough for passing a math test, but a FP8 <strong class="ne fr">π</strong> will give you an F.</p><p id="2bda" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Luckily most LLMs are not too sensitive to reduction at higher precisions. As a general rule of thumb, 8-bit Quantisation or Q8 models are nearly as good as the raw ones. This is shown in the following benchmarks from “<a class="af pd" href="https://arxiv.org/pdf/2404.14047" rel="noopener ugc nofollow" target="_blank"><em class="ny">How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study</em></a>”.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/7da740454cc39e235647f639308254d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmnuNVoar-ujzC9xYpaP2g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Extracted table of 8-bit Quantised Llama-3 against benchmarks, Source: </em><a class="af pd" href="https://arxiv.org/pdf/2404.14047" rel="noopener ugc nofollow" target="_blank"><em class="hd">https://arxiv.org/pdf/2404.14047</em></a><em class="hd">.</em></figcaption></figure><p id="01cd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In short, this means that you can get a <strong class="ne fr">50% reduction in model size for almost free </strong>just by quantising model weights to Q8.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/44f12ec6af5c6a5efed577e63df93413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9__qRK-4aWHUbQgnSDUzQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Extracted table of 4-bit Quantised Llama-3 against benchmarks, Source: </em><a class="af pd" href="https://arxiv.org/pdf/2404.14047" rel="noopener ugc nofollow" target="_blank"><em class="hd">https://arxiv.org/pdf/2404.14047</em></a><em class="hd">.</em></figcaption></figure><p id="c901" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a 75% reduction in model size, i.e Q4, the model is still decent using the smarter quantisation techniques like AWQ, albeit with visible loss in quality.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/7c0a10cc584a8a836c5f300ca8f506d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRXaRLc6bB32ZWdKmfDAwA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Extracted table of 3-bit Quantised Llama-3 against benchmarks, Source: </em><a class="af pd" href="https://arxiv.org/pdf/2404.14047" rel="noopener ugc nofollow" target="_blank"><em class="hd">https://arxiv.org/pdf/2404.14047</em></a><em class="hd">.</em></figcaption></figure><p id="fc42" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Anything below Q4 and you may run into severe degradation of model output quality.</p><p id="71f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Do note that the effects of quantisation on model quality may vary from model to model. The best way to determine the best quantisation level is really based on your own usage and testing.</p><h2 id="e2b9" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">What Quantisation Framework to choose?</strong></h2><p id="e92c" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">For more rigorous discourse on choosing Quantisation frameworks please see: </em><a class="af pd" href="https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/</em></a><em class="ny"> , </em><a class="af pd" href="https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/</em></a></p><p id="a66f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are many quantisation frameworks available, some of the more popular ones are GGUF, GPTQ, EXL2 and AWQ. The best quantisation framework for you will depend on your use case. The following are my personal recommendations from what I’ve observed in my usage. What’s best for you will depend on your use case and your mileage may vary.</p><p id="45da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">GGUF</strong></p><p id="387e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Created by <a class="af pd" href="https://github.com/ggerganov/llama.cpp" rel="noopener ugc nofollow" target="_blank">Georgi Gerganov</a> with the goal of enabling LLM inference with minimal setup and state-of-the-art performance on any hardware locally or in the cloud, GGUF has become a mainstay for AI/ML enthusiasts looking to host LLMs due to its ease of use.</p><p id="653b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you need to host models on commodity hardware or CPU only systems, then GGUF is the most suitable as it is the only framework that has CPU hosting support. GGUF also allows you to run newer models on older GPUs as well. GGUF is also the most stable framework due to how it packages the model weights as a single file in a unified format. If you need to host a quantised model reliably on any machine i.e. even your laptop, then GGUF is the way to go.</p><p id="e85c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The caveat for GGUF is that it’s older quants (Qx_0) uses more simple methods of quantisation such as round-to-nearest (RTN) quantisation. This may reduce model output quality to some extent, but it’s less affected at higher quantisation levels. Newer quantisation methods in GGUF (Qx_K or IQx_S) are better at preserving model quality at lower quantisation levels.</p><p id="36d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">GPTQ, EXL2 and AWQ</strong></p><p id="c6a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GPTQ, EXL2 and AWQ are specialised for GPU usage, they are all based on the GPTQ format. These frameworks tend to be much faster than GGUF as they are specially optimised for running on GPU. EXL2 allows for mixing quantisation levels within a model. AWQ tends to have the best output quality as it uses even “smarter” quantisation techniques than GPTQ. Both EXL2 and AWQ attempt to reduce degradation at lower quantisation levels. GPTQ tends to be the most supported for downstream inference engines.</p><p id="06e0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In conclusion, choose GGUF for ease of hosting, EXL2 for mixed quantisation levels, AWQ for output quality and GPTQ if your choice of inference engine does not support the rest.</p><h1 id="fc28" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">5. How do AWS Sagemaker Endpoints work?</h1><p id="c560" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Now that we understand what quantisation is, how do we bring it into our users on AG’s AWS Sagemaker so that they will be able to host their own production-ready models inference endpoints for their use case?</p><h2 id="950b" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">What are Sagemaker Endpoints?</strong></h2><p id="35a9" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">AWS Sagemaker Endpoints are the native tools within AWS Sagemaker to host model inference. Its advantages are:</p><ol class=""><li id="41aa" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk"><strong class="ne fr">Easy to configure Auto Scaling</strong>: It only takes a few lines to add auto scaling to existing endpoints.</li><li id="ba48" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><strong class="ne fr">Zero Downtime Updates</strong>: Updates to Sagemaker Endpoints uses BlueGreen Deployment by default.</li><li id="3b5d" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><strong class="ne fr">Flexibility &amp; Customisation</strong>: Sagemaker Endpoints are able to use customised containers.</li><li id="b936" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk"><strong class="ne fr">Access to AWS Services</strong>: Sagemaker Endpoints are able to access AWS services like S3 which can allow for more flexibility in adding additional steps to process inference requests.</li></ol><p id="4d07" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This helps to save time and expertise for users who just want to deploy a model and not think about the engineering work required to manage it on a production scale, turning what could be days/weeks of work into mere minutes.</p><h2 id="bddd" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">How does Sagemaker Endpoints work?</strong></h2><p id="ebd4" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Underneath the hood, Sagemaker Endpoints utilises special inference containers based on the <a class="af pd" href="https://github.com/aws/sagemaker-inference-toolkit" rel="noopener ugc nofollow" target="_blank">Sagemaker-Inference-Toolkit</a> library for hosting model APIs. These containers provide a quick and easy method of running inference without needing to build your own container images and supports many different frameworks from simple scikit-learn models using their scikit-learn container to even complex LLMs (and also their AWQ/GPTQ quantised variants) using the <a class="af pd" href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md" rel="noopener ugc nofollow" target="_blank">TensorRT-LLM</a> container.</p><p id="1ffb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However GGUF and EXL2 quants will still require heavy customised inference frameworks. Thankfully, Sagemaker provides the flexibility to use custom containers and Sagemaker Endpoints make it very simple to do so. There are only a few details to keep in mind to make this work:</p><ol class=""><li id="768c" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Container must listen on port 8080.</li><li id="bce2" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Container must respond to /ping and /invocations</li><li id="2040" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Container will be run with the <em class="ny">‘docker run &lt;image&gt; serve’</em> command, containers are expected to use ENTRYPOINT instead of CMD</li><li id="afed" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Model artifacts are brought into the ‘/opt/ml/model’ direction by specifying the S3 path to a tar.gz containing the model artifacts. This happens right before the runtime of the container.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/8fdb800e571624302a550ec307f56da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EddIZ1CGug1w2wPC"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visual representation of Custom Sagemaker Container Requirements by Author, Inspired by <a class="af pd" href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html" rel="noopener ugc nofollow" target="_blank">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html</a></figcaption></figure><h2 id="6997" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">Customise for an open-source inference engine</strong></h2><p id="9a98" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The above diagram represents a container pre-packed with Sagemaker-Inference-Toolkit. To use our own serving engine, we can simply replace the pre-packed packages with our own custom packages.</p><p id="0905" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For instance, one of the custom containers we curated enables users to host GGUF models through using Abetlen’s <a class="af pd" href="https://github.com/abetlen/llama-cpp-python" rel="noopener ugc nofollow" target="_blank">Llama-cpp-python</a> as the inference engine. This library is open-source and under the permissive MIT license.</p><p id="e9d5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our dockerfile, we only needed to write a few lines of code to conform to sagemaker endpoint requirements:</p><ol class=""><li id="699f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pa pb pc bk">Change listening port to 8080</li><li id="bfb2" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Add routes for /ping and /invocations</li><li id="a4df" class="nc nd fq ne b go pe ng nh gr pf nj nk nl pg nn no np ph nr ns nt pi nv nw nx pa pb pc bk">Run on ENTRYPOINT</li></ol><h1 id="4036" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">6. Hosting a Quantised Model in AG Sagemaker</h1><p id="4489" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Using the custom containers, hosting a quantised LLM in AG’s Sagemaker environment is reduced down to a few lines of code.</p><pre class="mm mn mo mp mq rd re rf bp rg bb bk"><span id="fb89" class="rh oa fq re b bg ri rj l rk rl"># Code will vary depending on how you have curated your own custom container.<br/><br/>from sagemaker.model import Model<br/><br/>endpoint_name = "&lt;Name of endpoint&gt;"<br/>image_uri = "&lt;ECR Image URI to Llama-cpp-python Image&gt;"<br/>model_artifact_location = "&lt;S3 Path to Model Artifacts&gt;"<br/>model = "&lt;Path to model file&gt;"<br/><br/><br/># All other ENV variables defined in documentation<br/>model_endpoint = Model(<br/>  image_uri = image_uri,<br/>  model_data = model_artifact_location,<br/>  role = role,<br/>  env = {<br/>    "MODEL": model_file_path_in_container,<br/>    "N_GPU_LAYERS": "999",<br/>    "INVOCATIONS_ROUTE": "/v1/completions"<br/>  }<br/>)<br/><br/>model_endpoint.deploy(<br/>  initial_instance_count=1,<br/>  instance_type="ml.g4dn.xlarge",<br/>  endpoint_name=endpoint_name<br/>)</span></pre><p id="1849" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">That’s it, short and simple. With this, our users can focus on developing their LLM use cases without being encumbered by the complexity behind the scenes.</p><h1 id="87cd" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">7. Benchmarks</h1><p id="3961" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">The following are some benchmarks for the average tokens generated per second based on single query inference tested 5 times over 30 prompts i.e. each candidate is based on an average of 150 tests. For all tests, we used the CodeLlama model as it is available in many sizes, namely 7, 13, 34 and 70 billion parameters. We tested both quantised and unquantised models with different inference engines, using Transformers as the baseline as it’s typically the normal way for running unquantised models.</p><p id="35a8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following are the specifications for the benchmarking:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/7347e3a11bf2fdb6cbcfc801cedd13b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTO37Ca16BJEpUoJVfJ6xg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Benchmark specifications by Author</em></figcaption></figure><p id="4afa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">Note ExllamaV2 refers to the inference engine, while EXL2 is the quantisation format native to the ExllamaV2, in this case, ExllamaV2 also supports inference for GPTQ. ExllamaV2 will only be benchmarked with Q4_0 as some Q8_0 quants are not found on HuggingFace.</em></p><h2 id="a02b" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">Unquantised via Transformers (Baseline)</strong></h2><p id="e2f9" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><strong class="ne fr">BF16:</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/9cad550d302275d1fb63d80b3eda99ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUgTgDuskJqMRP5yuqF-yA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">Transformers BF16 Inference Benchmark by Author</em></figcaption></figure><p id="64f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All multiples in the following tests are based on using Transformers as a baseline. For instance, the GPTQ 7b Q4_0 model has a “(3.42x)” multiple in the “Tokens per second” column, this means that GPTQ is 3.42 times as fast as the Transformers baseline for the 7b model.</p><h2 id="09e3" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">GGUF via Llama-cpp-python</strong></h2><p id="87dc" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">GGUF can support hosting on older Nvidia T4s from the g4dn instance families, so we added extra tests that optimises for cost using g4dn instance types when possible:</em></p><p id="a878" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q4_0</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/4fedaab4ea800417fe750538a56b471e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4GbcKdfDnQNs160W1L9HQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">GGUF Q4_0 Inference (Minimised Cost) Benchmark by Author</em></figcaption></figure><p id="cf72" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q8_0</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/b85997ee93e5411a97f96efccde3f9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VwGfXYxztgQzb9lPVwqlw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">GGUF Q8_0 Inference (Minimised Cost) Benchmark by Author</em></figcaption></figure><p id="9420" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="ny">Using newer Nvidia A10g from the g5 instance family:</em></p><p id="4c71" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q4_0</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/54d8f6e23d74914cd6672b0751417cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MVvzfrPKWfrPuOsYLokpyw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">GGUF Q4_0 Inference Benchmark by Author</em></figcaption></figure><p id="1c5b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q8_0</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/14d5790a9458f501defae9e52bfcdad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fu3iYmzdH3IjFLY5N2TtDw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">GGUF Q8_0 Inference Benchmark by Author</em></figcaption></figure><p id="3c34" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In every single case, GGUF can run the Models much cheaper or at the same price but significantly faster. For instance, the Q8 13B model is 74% faster than the baseline but at one fifth the cost!</p><h2 id="7f94" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">GPTQ — Via ExllamaV2</strong></h2><p id="fa2e" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk"><em class="ny">ExllamaV2 only supports the newer hosting on the newer Nvidia A10g from the g5 instance family and not the g4dn instance family.</em></p><p id="8b30" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Q4_0</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/9559a0e2f5026a85a7dc9c5bf48cf8f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Ne3zHn_1tVMYC72n67KMQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">GPTQ Q4_0 Inference Benchmark by Author</em></figcaption></figure><p id="babe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GPTQ on ExllamaV2 takes the performance improvements to a whole new level, with <strong class="ne fr">more than</strong> <strong class="ne fr">triple</strong> the speeds from the baseline for every model size quantised in Q4_0.</p><h2 id="bdad" class="qg oa fq bf ob qh qi qj oe qk ql qm oh nl qn qo qp np qq qr qs nt qt qu qv qw bk"><strong class="al">AWS Sagemaker Jumpstart</strong></h2><p id="a8c5" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Natively AWS also provides a service called JumpStart that allows deployment of pretrained models with a few clicks. These AWS Sagemaker containers implement the Sagemaker Inference Toolkit and have various inference engines pre-installed. In this case, it’s using the HuggingFace’s Text Generation Inference (TGI) Framework as the inference engine.</p><p id="694b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">BF16:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/c021db0da5c3b01e942d9a59c822e6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuJ_nesWP4wMlGr0yMeWIg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="hd">AWS Jumpstart TGI BF16 Inference Benchmark by Author</em></figcaption></figure><p id="b296" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice how 13B is faster than 7B. This is because the TGI container is able to utilise more GPU memory to increase the speed of inference. On larger parameter sizes like 34B and 70B, using AWS Sagemaker Jumpstart with TGI containers can even outperform GPTQ on ExllamaV2.</p><h1 id="9809" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">8. Conclusion</h1><p id="4ed0" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Quantisation offers substantial benefits for LLMs as it reduces memory requirements for hosting them. The reduction in memory requirements increases inference speeds and reduces costs. Higher bit quantisation can be achieved with almost zero loss in output quality, substantial gains in speed and reduced cost — essentially a Pareto improvement over using unquantised LLMs.</p><p id="38de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With auxiliary functionalities provided by AG on top of AWS Sagemaker Endpoints, agencies across the entire public sector can easily access capabilities to create and manage production-ready quantised Open LLM APIs. By streamlining the process of deploying quantised large language models, AG significantly lowers the barrier of entry for producing efficient and cost-effective GenAI applications, allowing government agencies to focus on innovating and developing technology for public good.</p><p id="4d1d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Dovetailing with this, AG will continue to further its GenAI endeavours by providing access to closed-source models like Azure OpenAI and VertexAI’s Gemini via secured cross-cloud integration, alongside our existing services with AWS Bedrock. Through robust and comprehensive offerings, AG empowers users to rightsize models for their use cases, resulting in better, faster and cheaper GenAI applications in the public sector.</p><h1 id="6563" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">References</h1><p id="8c73" class="pw-post-body-paragraph nc nd fq ne b go ov ng nh gr ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">[1] Sau Sheong, Programming with AI — Open LLMs (2024), <a class="af pd" href="https://sausheong.com/programming-with-ai-open-llms-28091f77a088" rel="noopener ugc nofollow" target="_blank">https://sausheong.com/programming-with-ai-open-llms-28091f77a088</a></p><p id="1e4c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] S. Stoelinga, Calculating GPU memory for serving LLMs (2023), <a class="af pd" href="https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.substratus.ai/blog/calculating-gpu-memory-for-llm/</em></a></p><p id="1394" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[3] M.C. Neves, What are Quantized LLMs? (2023), <a class="af pd" href="https://www.tensorops.ai/post/what-are-quantized-llms" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.tensorops.ai/post/what-are-quantized-llms</em></a></p><p id="e35f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[4] D. Patel, Neural Network Quantization &amp; Number Formats From First Principles (2024), <a class="af pd" href="https://www.semianalysis.com/p/neural-network-quantization-and-number" rel="noopener ugc nofollow" target="_blank">https://www.semianalysis.com/p/neural-network-quantization-and-number</a></p><p id="f3b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[5] W. Huang, How Good Are Low-bit Quantized LLAMA3 Models? An Empirical Study (2024), <a class="af pd" href="https://arxiv.org/pdf/2404.14047" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.14047</a></p><p id="78bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[6] Oobabooga, A detailed comparison between GPTQ, AWQ, EXL2, q4_K_M, q4_K_S, and load_in_4bit: perplexity, VRAM, speed, model size, and loading time. (N.A.), <a class="af pd" href="https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://oobabooga.github.io/blog/posts/gptq-awq-exl2-llamacpp/</em></a></p><p id="7104" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[7] Sgsdxzy, Guide to choosing quants and engines (2024), <a class="af pd" href="https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://www.reddit.com/r/LocalLLaMA/comments/1anb2fz/guide_to_choosing_quants_and_engines/</em></a></p><p id="1ade" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[8] Amazon Web Services, Use Your Own Inference Code with Hosting Services (N.A.), <a class="af pd" href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html" rel="noopener ugc nofollow" target="_blank">https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html</a></p></div></div></div></div>    
</body>
</html>