<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How Can We Continually Adapt Vision-Language Models?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How Can We Continually Adapt Vision-Language Models?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-can-we-continually-adapt-vision-language-models-3e7bfa19b34e?source=collection_archive---------6-----------------------#2024-08-26">https://towardsdatascience.com/how-can-we-continually-adapt-vision-language-models-3e7bfa19b34e?source=collection_archive---------6-----------------------#2024-08-26</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6614" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring continual learning strategies for CLIP</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://alicjadobrzeniecka.medium.com/?source=post_page---byline--3e7bfa19b34e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alicja Dobrzeniecka" class="l ep by dd de cx" src="../Images/b731eb2bb8fde56e84273af8050b59e4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*loIiVAiu4CpQKkIleHe-0w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3e7bfa19b34e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://alicjadobrzeniecka.medium.com/?source=post_page---byline--3e7bfa19b34e--------------------------------" rel="noopener follow">Alicja Dobrzeniecka</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3e7bfa19b34e--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 26, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/50e5d7820bf844fb40605562495b7cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6a8mdlqXf4RVUupL"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author created in Midjourney</figcaption></figure><p id="fe0f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">There is currently a growing interest in the study and application of Large Language Models. However, these models can only process textual data, which limits their usefulness for some applications. Humans are capable of processing information across multiple modalities, such as written and spoken language, and visual understanding of the reality around us. We would expect models to be capable of similar processing.</p><p id="9a6b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Vision-Language models</strong> can address both textual and visual data, which has a wide range of use cases such as image analysis (e.g. medical images), object recognition and better scene understanding (e.g. for self-driving cars), generating captions for the images, responding to the visual questions, chatting about images, and more…</p><blockquote class="nx"><p id="afd7" class="ny nz fq bf oa ob oc od oe of og nw dx">Unfortunately, multi-modal models face the same challenges as unimodal ones. Once trained, they can become outdated over time as new data samples arrive or the data distribution changes.</p></blockquote><p id="a53b" class="pw-post-body-paragraph nb nc fq nd b go oh nf ng gr oi ni nj nk oj nm nn no ok nq nr ns ol nu nv nw fj bk">In my <a class="af om" href="https://medium.com/towards-data-science/ai-models-have-expiry-date-9a6e2c9c0a9f" rel="noopener">last article</a> I introduced the <strong class="nd fr">Continual Learning (CL)</strong> approach to AI models in general. Continual Learning tries to find ways to continually train models, which may be a more sustainable solution for the future. In this article, I want to explore the possibilities of <strong class="nd fr">applying CL to Vision-Language models (VLMs) </strong>— specifically the Contrastive Language-Image Pretraining (CLIP) model.</p><h2 id="7911" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">But what is CLIP?</h2><p id="acdf" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">Contrastive Language-Image Pretraining (CLIP) was introduced by the OpenAI in 2021 in the<em class="pn"> Learning Transferable Visual Models From Natural Language Supervision</em> paper [1].</p><blockquote class="nx"><p id="6582" class="ny nz fq bf oa ob oc od oe of og nw dx">The goal of the CLIP model is to <strong class="al">understand the relation between text and an image</strong>. If you input it a piece of text it should return the most relevant image in a given set of images for it. Likewise if you put in the model an image it should give you the most fitting text from a set of available texts.</p></blockquote><p id="d74e" class="pw-post-body-paragraph nb nc fq nd b go oh nf ng gr oi ni nj nk oj nm nn no ok nq nr ns ol nu nv nw fj bk">CLIP was trained on a large dataset of text-image pairs. Contrastive learning was used to bring matching text-image pairs closer together in the embedding space and to move non-matching pairs away from each other. This learned shared embedding space is then used during inference to understand the relationship between text and images. If you want to know more about CLIP, I recommend the <a class="af om" href="https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40" rel="noopener">following article</a>, which describes it in detail.</p><h2 id="e6e2" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">Why do we need Continual Learning for Vision-Language models?</h2><p id="29a1" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">Large foundation models can become obsolete over time due to shifts in distribution or the arrival of new data samples. Re-training such models is expensive and time consuming. The authors of the TiC-CLIP paper [7] show that current evaluation practices often fail to capture the difference in performance when considering time-evolving data.</p><p id="7c49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In Figure 1 you can see that if we compare OpenAI models trained before 2020 and OpenCLIP models trained before 2022, although there is not much difference between their robustness on Imagenet (left image), there is a performance gap when compared on retrieval tasks from 2014–2016 and 2021–2022 (right image), indicating that OpenAI models have less zero-shot robustness with time-evolving data [7].</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj po"><img src="../Images/ce1610a9e1bdb4c9f10f04d6a784a443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhP-3G6KX4IL_zQXBFLivw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Fig. 1. Image from the paper TiC-CLIP: Continual Training of Clip Models [7].</figcaption></figure><p id="7ee9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In addition, Continual Learning may be a natural choice for some use cases such as Online Lifelong Learning (OLL) [8] where data comes from continuous and non-stationary data streams and evolves with time.</p><p id="5037" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Finally, as pointed out in [4], CLIP shows remarkable zero-shot capabilities, but for some domains it may struggle to achieve good performance due to insufficient data for some categories during pre-training.</p><h2 id="2324" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">Challenges</h2><p id="4196" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">As some of the current state-of-the-art Vision-Language models require more and more computational time and resources, finding a way to continually adapt them without re-training seems to be crucial. However, there are some challenges in continually adapting such models:</p><ul class=""><li id="d723" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pp pq pr bk"><strong class="nd fr">catastrophic forgetting — </strong>learning new tasks can damage the performance on the old tasks.</li><li id="9193" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk"><strong class="nd fr">losing zero-shot capability — </strong>pre-trained models can display zero-shot behaviour meaning that they can perform a task for which they have not received training data, i.e. classify a class of images without seeing them during training. This ability can be lost when training continually.</li><li id="a7af" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk"><strong class="nd fr">misalignment between text and image representations — </strong>as noted by the authors of [12], during Continual Learning for CLIP, there may be a deterioration in the alignment of the multimodal representation space, which can lead to performance degradation in the long run.</li></ul><h2 id="5133" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">Continual Learning Methods for CLIP</h2><p id="1b0b" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">There is an ongoing research on improving the continual aspect of multi-modal models. Below are some of the existing strategies and use cases:</p><ol class=""><li id="b597" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw px pq pr bk"><strong class="nd fr">Mixture of Experts (MoE)</strong></li></ol><ul class=""><li id="de97" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pp pq pr bk">To continually train the CLIP, the authors of [2] propose<strong class="nd fr"> MoE</strong> approach using task-specific adapters. They build a dynamic extension architecture on top of a frozen CLIP model.</li><li id="d936" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">The idea here is to add new adapters as new tasks are trained. At the same time, the Distribution Discriminative Auto-Selector is trained so that later, during the inference phase, the model can automatically choose whether the test data should go to the MoE adapters or to the pre-trained CLIP for zero-shot detection.</li></ul><p id="3f09" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">2. <strong class="nd fr">CoLeCLIP</strong></p><ul class=""><li id="176c" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pp pq pr bk">The authors of [4] focus on the problem of Continual Learning for Vision-Language models in open domains — where we may have datasets from diverse seen and unseen domains with novel classes.</li><li id="88fc" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">Addressing open domain challenges is particularly important for use cases such as <strong class="nd fr">AI assistants, autonomous driving systems and robotics, </strong>as<strong class="nd fr"> </strong>these models operate in complex and changing environments [4].</li><li id="7022" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk"><strong class="nd fr">CoLeCLIP </strong>is based on CLIP but adjusted for open-domain problems.</li><li id="5bf3" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">In CoLeCLIP an external laernable Parameter-Efficient Fine-Tuning (PEFT) module per task is attached to the frozen text encoder of CLIP to learn the text embeddings of the classes [4].</li></ul><p id="efd0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">3. <strong class="nd fr">Continual Language Learning (CLL)</strong></p><ul class=""><li id="186b" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pp pq pr bk">The authors of [3] noted that current pre-trained Vision-Language models often only support English. At the same time popular methods for creating multilingual models are expensive and require large amounts of data.</li><li id="1aa2" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">In their paper, they propose to extend language capability by using <strong class="nd fr">CLL</strong>, where linguistic knowledge is updated incrementally.</li><li id="366e" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk"><strong class="nd fr">CLL-CLIP</strong> uses an expandable embedding layer to store linguistic differences. It trains only token embeddings and is optimised for learning alignment between images and multilingual text [3].</li><li id="923d" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">The authors also propose a novel approach to ensure that the distribution of all token embeddings is identical during initialisation and later regularised during training. You can see a visualisation of this process in Figure 2 from their paper.</li></ul><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj py"><img src="../Images/532d655751f4c0832b09042cda678768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZ-4YQNLRwu5YPuMQJSljg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Fig. 2. Image from the paper Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning [3].</figcaption></figure><p id="ed13" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">4. <strong class="nd fr">Symmetric Image-Text tuning strategy (SIT)</strong></p><ul class=""><li id="f8ca" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw pp pq pr bk">In [8] the authors observe that there occurs asymetry between text and image during Parameter-Efficient Tuning (PET) for their Online Lifelong Learning scenario which may lead to catastrophic forgetting.</li><li id="e803" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">They propose to use the SIT strategy to mitigate this problem. This approach matches images and class labels within the current batch only during online learning.</li><li id="4528" class="nb nc fq nd b go ps nf ng gr pt ni nj nk pu nm nn no pv nq nr ns pw nu nv nw pp pq pr bk">The goal is to preserve the generalisation ability of CLIP while improving its performance on a specific downstream task or dataset, without introducing asymmetry between the encoders.</li></ul><h2 id="80db" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">Evaluation of the Continual Learning models</h2><p id="17ff" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">The evaluation standards for CL appear to be still a work in progress. Many of the existing benchmarks for evaluating the effectiveness of CL models do not take the time factor into account when constructing data sets. As mentioned by [7], the performance gap may sometimes only become visible when we recreate the time-evolving setup for the test data.</p><p id="ab0a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In addition, many of the existing benchmarks for Vision-Language models focus only on the single-image input, without measuring multi-image understanding, which may be critical in some applications. The authors of [5] develop a benchmark for multi-image evaluation that allows a more fine-grained assessment of the limitations and capabilities of current state-of-the-art models.</p><h2 id="0355" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">Continual Learning does not solve all the problems…</h2><p id="c2a6" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">Visual-Language models like CLIP have their shortcomings. In [6], the authors explored the gap between CLIP’s visual embedding space and purely visual self-supervised learning. They investigated false matches in the embedding space, where images have similar encoding when they should not.</p><blockquote class="nx"><p id="420f" class="ny nz fq bf oa ob oc od oe of og nw dx">From their results it can be concluded that if a pre-trained model has a weakness, it can be propagated when the model is adapted. Learning visual representations remains an open challenge, and vision models may become a bottleneck in multimodal systems, as scaling alone does not solve the built-in limitations of models such as CLIP. [6]</p></blockquote><h2 id="0c59" class="on oo fq bf op oq pz os ot ou qa ow ox nk qb oz pa no qc pc pd ns qd pf pg ph bk"><strong class="al">Conclusion</strong></h2><p id="a760" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">This article explores the opportunities and challenges of applying Continual Learning to Vision-Language models, focusing on the CLIP model. Hopefully this article has given you a first impression of what is possible, and that while Continual Learning seems to be a good direction for the future of AI models, there is still a lot of work to be done to make it fully usable.</p><p id="de13" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">If you have any questions or comments, please feel free to share them in the comments section.</strong></p><p id="2552" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Until next time!</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/1fca5a8264874543865ae3508b2a7c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VxwDuuhkooiDre4k"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by the author generated in Midjourney.</figcaption></figure><h2 id="bce5" class="on oo fq bf op oq or os ot ou ov ow ox nk oy oz pa no pb pc pd ns pe pf pg ph bk">References</h2><p id="4ab4" class="pw-post-body-paragraph nb nc fq nd b go pi nf ng gr pj ni nj nk pk nm nn no pl nq nr ns pm nu nv nw fj bk">[1] Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. In <em class="pn">Proceedings of the 38th International Conference on Machine Learning</em> (pp. 8748–8763). PMLR.</p><p id="9a59" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, &amp; You He. (2024). Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters.</p><p id="e88a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[3] Bang Yang, Yong Dai, Xuxin Cheng, Yaowei Li, Asif Raza, &amp; Yuexian Zou. (2024). Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning.</p><p id="302d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[4] Yukun Li, Guansong Pang, Wei Suo, Chenchen Jing, Yuling Xi, Lingqiao Liu, Hao Chen, Guoqiang Liang, &amp; Peng Wang. (2024). CoLeCLIP: Open-Domain Continual Learning via Joint Task Prompt and Vocabulary Learning.</p><p id="128d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[5] Bingchen Zhao, Yongshuo Zong, Letian Zhang, &amp; Timothy Hospedales. (2024). Benchmarking Multi-Image Understanding in Vision and Language Models: Perception, Knowledge, Reasoning, and Multi-Hop Reasoning.</p><p id="9958" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[6] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, &amp; Saining Xie. (2024). Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs.</p><p id="40f7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[7] Saurabh Garg, Hadi Pour Ansari, Mehrdad Farajtabar, Sachin Mehta, Raviteja Vemulapalli, Oncel Tuzel, Vaishaal Shankar, &amp; Fartash Faghri (2023). TiC-CLIP: Continual Training of CLIP Models. In <em class="pn">NeurIPS Workshop</em>.</p><p id="69a8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[8] Leyuan Wang, Liuyu Xiang, Yujie Wei, Yunlong Wang, &amp; Zhaofeng He. (2024). CLIP model is an Efficient Online Lifelong Learner.</p><p id="c3c7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[9] Vishal Thengane, Salman Khan, Munawar Hayat, &amp; Fahad Khan. (2023). CLIP model is an Efficient Continual Learner.</p><p id="e7b7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[10] Yuxuan Ding, Lingqiao Liu, Chunna Tian, Jingyuan Yang, &amp; Haoxuan Ding. (2022). Don’t Stop Learning: Towards Continual Learning for the CLIP Model.</p><p id="68b7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[11] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinĳa Jain, &amp; Aman Chadha. (2024). Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions.</p><p id="1e27" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[12] Ni, Z., Wei, L., Tang, S., Zhuang, Y., &amp; Tian, Q. (2023). Continual vision-language representation learning with off-diagonal information. In <em class="pn">Proceedings of the 40th International Conference on Machine Learning</em>. JMLR.org.</p></div></div></div></div>    
</body>
</html>