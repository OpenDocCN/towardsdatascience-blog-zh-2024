- en: How to Evaluate Multilingual LLMs in Any Language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09](https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Evaluation of language-specific LLM accuracy on the global Massive Multitask
    Language Understanding (Global-MMLU) benchmark in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[![Dr.
    Leon Eversberg](../Images/56dc3579a29933f7047a9ce60be4697a.png)](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    [Dr. Leon Eversberg](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    ·8 min read·Dec 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7fc65514649f1df9e4413d80ee13bb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Benjamin Kaufmann](https://unsplash.com/@devnull?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'As soon as a new Large Language Model (LLM) is released, the obvious question
    we ask ourselves is this: Is this LLM better than the one I’m currently using?'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are typically evaluated against a large number of benchmarks, most of which
    are in English only.
  prefs: []
  type: TYPE_NORMAL
- en: For multilingual models, it is very rare to find evaluation metrics for every
    specific language that was in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes evaluation metrics are published for the base model and not for the
    model tuned to the instructions. And usually the evaluation is not done on the
    quantization model that we actually use locally.
  prefs: []
  type: TYPE_NORMAL
- en: So it is very unlikely to find comparable evaluation results from several LLMs
    in a specific language other than English.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this article, we will use the Global-MMLU dataset to perform our
    own evaluation using the widely used MMLU benchmark in the language of our choice.
  prefs: []
  type: TYPE_NORMAL
- en: Table Of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
