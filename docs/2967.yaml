- en: How to Evaluate Multilingual LLMs in Any Language
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何评估任何语言中的多语言LLM
- en: 原文：[https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09](https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09](https://towardsdatascience.com/how-to-evaluate-multilingual-llms-with-global-mmlu-ce314aedee8f?source=collection_archive---------7-----------------------#2024-12-09)
- en: Evaluation of language-specific LLM accuracy on the global Massive Multitask
    Language Understanding (Global-MMLU) benchmark in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在全球大规模多任务语言理解（Global-MMLU）基准上，评估特定语言LLM的准确性（使用Python）
- en: '[](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[![Dr.
    Leon Eversberg](../Images/56dc3579a29933f7047a9ce60be4697a.png)](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    [Dr. Leon Eversberg](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[![Dr.
    Leon Eversberg](../Images/56dc3579a29933f7047a9ce60be4697a.png)](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    [Dr. Leon Eversberg](https://medium.com/@leoneversberg?source=post_page---byline--ce314aedee8f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    ·8 min read·Dec 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[《Towards Data Science》](https://towardsdatascience.com/?source=post_page---byline--ce314aedee8f--------------------------------)
    ·8分钟阅读·2024年12月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f7fc65514649f1df9e4413d80ee13bb0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7fc65514649f1df9e4413d80ee13bb0.png)'
- en: Photo by [Benjamin Kaufmann](https://unsplash.com/@devnull?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Benjamin Kaufmann](https://unsplash.com/@devnull?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'As soon as a new Large Language Model (LLM) is released, the obvious question
    we ask ourselves is this: Is this LLM better than the one I’m currently using?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每当一个新的大型语言模型（LLM）发布时，我们自然会问自己一个明显的问题：这个LLM是否比我当前使用的更好？
- en: LLMs are typically evaluated against a large number of benchmarks, most of which
    are in English only.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLM通常会在大量基准测试中进行评估，其中大多数基准仅为英语。
- en: For multilingual models, it is very rare to find evaluation metrics for every
    specific language that was in the training data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多语言模型来说，能够找到每种特定语言的评估指标是非常罕见的，这些语言出现在训练数据中。
- en: Sometimes evaluation metrics are published for the base model and not for the
    model tuned to the instructions. And usually the evaluation is not done on the
    quantization model that we actually use locally.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有时评估指标是针对基础模型发布的，而不是针对经过指令微调的模型。通常，评估也不会在我们实际本地使用的量化模型上进行。
- en: So it is very unlikely to find comparable evaluation results from several LLMs
    in a specific language other than English.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，很难找到在英语以外的特定语言中，来自多个大型语言模型（LLM）的可比较评估结果。
- en: Therefore, in this article, we will use the Global-MMLU dataset to perform our
    own evaluation using the widely used MMLU benchmark in the language of our choice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将使用全球大规模多任务语言理解（Global-MMLU）数据集，并通过我们选择的语言，利用广泛使用的MMLU基准进行评估。
- en: Table Of Contents
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
