<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Training AI Models on CPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Training AI Models on CPU</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01">https://towardsdatascience.com/training-ai-models-on-cpu-3903adc9f388?source=collection_archive---------1-----------------------#2024-09-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a8ec" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Revisiting CPU for ML in an Era of GPU Scarcity</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--3903adc9f388--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3903adc9f388--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/bce1518ce10fce87f67bb138caf736c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Tf0e2-5_s5L2MZGZ"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@quinoal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Quino Al</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d258" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The recent successes in AI are often attributed to the emergence and evolutions of the GPU. The GPU’s architecture, which typically includes thousands of multi-processors, high-speed memory, dedicated tensor cores, and more, is particularly well-suited to meet the intensive demands of AI/ML workloads. Unfortunately, the rapid growth in AI development has led to a surge in the demand for GPUs, making them difficult to obtain. As a result, ML developers are increasingly exploring alternative hardware options for training and running their models. In previous posts, we discussed the possibility of training on dedicated AI ASICs such as <a class="af nc" rel="noopener" target="_blank" href="/tpu-training-6eb84100d138">Google Cloud TPU</a>, <a class="af nc" rel="noopener" target="_blank" href="/training-on-aws-with-habana-gaudi-3126e183048">Haban Gaudi</a>, and <a class="af nc" rel="noopener" target="_blank" href="/a-first-look-at-aws-trainium-1e0605071970">AWS Trainium</a>. While these options offer significant cost-saving opportunities, they do not suit all ML models and can, like the GPU, also suffer from limited availability. In this post we return to the good old-fashioned CPU and revisit its relevance to ML applications. Although CPUs are generally less suited to ML workloads compared to GPUs, they are much easier to acquire. The ability to run (at least some of) our workloads on CPU could have significant implications on development productivity.</p><p id="52cf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851">here</a>) we emphasized the importance of analyzing and optimizing the runtime performance of AI/ML workloads as a means of accelerating development and minimizing costs. While this is crucial regardless of the compute engine used, the profiling tools and optimization techniques can vary greatly between platforms. In this post, we will discuss some of the performance optimization options that pertain to CPU. Our focus will be on <a class="af nc" href="https://www.intel.com/content/www/us/en/products/details/processors/xeon.html" rel="noopener ugc nofollow" target="_blank">Intel® Xeon® CPU</a> processors (with <a class="af nc" href="https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html" rel="noopener ugc nofollow" target="_blank">Intel® AVX-512</a>) and on the PyTorch (version 2.4) framework (although similar techniques can be applied to other CPUs and frameworks, as well). More specifically, we will run our experiments on an <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/c7i/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 c7i</a> instance with an <a class="af nc" href="https://docs.aws.amazon.com/dlami/" rel="noopener ugc nofollow" target="_blank">AWS Deep Learning AMI</a>. Please do not view our choice of Cloud platform, CPU version, ML framework, or any other tool or library we should mention, as an endorsement over their alternatives.</p><p id="064f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our goal will be to demonstrate that although ML development on CPU may not be our first choice, there are ways to “soften the blow” and — in some cases — perhaps even make it a viable alternative.</p><h2 id="d6f1" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Disclaimers</h2><p id="0ca9" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Our intention in this post is to demonstrate just a few of the ML optimization opportunities available on CPU. Contrary to most of the online tutorials on the topic of ML optimization on CPU, we will focus on a training workload rather than an inference workload. There are a number of optimization tools focused specifically on inference that we will not cover (e.g., see <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torchserve_with_ipex.html" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af nc" href="https://pytorch.org/blog/accelerated-cpu-inference/" rel="noopener ugc nofollow" target="_blank">here</a>).</p><p id="764b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Please do not view this post as a replacement of the official documentation on any of the tools or techniques that we mention. Keep in mind that given the rapid pace of AI/ML development, some of the content, libraries, and/or instructions that we mention may become outdated by the time you read this. Please be sure to refer to the most up-to-date documentation available.</p><p id="fc18" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, the impact of the optimizations that we discuss on runtime performance is likely to vary greatly based on the model and the details of the environment (e.g., see the high degree of variance between models on the official PyTorch <a class="af nc" href="http://github.com/pytorch/pytorch/issues/93531#issuecomment-1457373890" rel="noopener ugc nofollow" target="_blank">TouchInductor CPU Inference Performance Dashboard</a>). The comparative performance numbers we will share are specific to the toy model and runtime environment that we will use. Be sure to reevaluate all of the proposed optimizations on your own model and runtime environment.</p><p id="aaca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Lastly, our focus will be solely on throughput performance (as measured in samples per second) — not on training convergence. However, it should be noted that some optimization techniques (e.g., batch size tuning, mixed precision, and more) could have a negative effect on the convergence of certain models. In some cases, this can be overcome through appropriate hyperparameter tuning.</p><h1 id="0b08" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Toy Example — ResNet-50</h1><p id="893e" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We will run our experiments on a simple image classification model with a <a class="af nc" href="https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50" rel="noopener ugc nofollow" target="_blank">ResNet-50</a> backbone (from <a class="af nc" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">Deep Residual Learning for Image Recognition</a>). We will train the model on a fake dataset. The full training script appears in the code block below (loosely based on <a class="af nc" href="https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py" rel="noopener ugc nofollow" target="_blank">this example</a>):</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="9934" class="pv oa fq ps b bg pw px l py pz">import torch<br/>import torchvision<br/>from torch.utils.data import Dataset, DataLoader<br/>import time<br/><br/># A dataset with random images and labels<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>        label = torch.tensor(data=index % 10, dtype=torch.uint8)<br/>        return rand_image, label<br/><br/>train_set = FakeDataset()<br/><br/>batch_size=128<br/>num_workers=0<br/><br/>train_loader = DataLoader(<br/>    dataset=train_set,<br/>    batch_size=batch_size,<br/>    num_workers=num_workers<br/>)<br/><br/>model = torchvision.models.resnet50()<br/>criterion = torch.nn.CrossEntropyLoss()<br/>optimizer = torch.optim.SGD(model.parameters())<br/>model.train()<br/><br/>t0 = time.perf_counter()<br/>summ = 0<br/>count = 0<br/><br/>for idx, (data, target) in enumerate(train_loader):<br/>    optimizer.zero_grad()<br/>    output = model(data)<br/>    loss = criterion(output, target)<br/>    loss.backward()<br/>    optimizer.step()<br/>    batch_time = time.perf_counter() - t0<br/>    if idx &gt; 10:  # skip first steps<br/>        summ += batch_time<br/>        count += 1<br/>    t0 = time.perf_counter()<br/>    if idx &gt; 100:<br/>        break<br/><br/>print(f'average step time: {summ/count}')<br/>print(f'throughput: {count*batch_size/summ}')</span></pre><p id="4071" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Running this script on a c7i.2xlarge (with 8 vCPUs) and the <a class="af nc" href="https://download.pytorch.org/whl/cpu" rel="noopener ugc nofollow" target="_blank">CPU</a> version of PyTorch 2.4, results in a throughput of 9.12 samples per second. For the sake of comparison, we note that the throughput of the same (unoptimized script) on an <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/g5/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 g5.2xlarge</a> instance (with 1 GPU and 8 vCPUs) is 340 samples per second. Taking into account the <a class="af nc" href="https://aws.amazon.com/ec2/pricing/on-demand/" rel="noopener ugc nofollow" target="_blank">comparative costs</a> of these two instance types ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the time of this writing), we find that training on the GPU instance to give roughly eleven(!!) times better price performance. Based on these results, the preference for using GPUs to train ML models is very well founded. Let’s assess some of the possibilities for reducing this gap.</p><h1 id="5a6d" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">PyTorch Performance Optimizations</h1><p id="eae8" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In this section we will explore some basic methods for increasing the runtime performance of our training workload. Although you may recognize some of these from our <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">post</a> on GPU optimization, it is important to highlight a significant difference between training optimization on CPU and GPU platforms. On GPU platforms much of our effort was dedicated to maximizing the parallelization between (the training data preprocessing on) the CPU and (the model training on) the GPU. On CPU platforms all of the processing occurs on the CPU and our goal will be to allocate its resources most effectively.</p><h2 id="1bb4" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Batch Size</h2><p id="756c" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Increasing the training batch size can potentially increase performance by reducing the frequency of the model parameter updates. (On GPUs it has the added benefit of reducing the overhead of CPU-GPU transactions such as kernel loading). However, while on GPU we aimed for a batch size that would maximize the utilization of the GPU memory, the same strategy might hurt performance on CPU. For reasons beyond the scope of this post, CPU memory is more complicated and the best approach for discovering the most optimal batch size may be through trial and error. Keep in mind that changing the batch size could affect training convergence.</p><p id="869f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The table below summarizes the throughput of our training workload for a few (arbitrary) choices of batch size:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/ca91444a8f8fffe4352661a473bb0085.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*v0JhpJSauBSFFDfFl0DGpw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Training Throughput as Function of Batch Size (by Author)</figcaption></figure><p id="15c0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Contrary to our findings on GPU, on the c7i.2xlarge instance type our model appears to prefer lower batch sizes.</p><h2 id="c777" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Multi-process Data Loading</h2><p id="5e02" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">A common technique on GPUs is to <a class="af nc" href="https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading" rel="noopener ugc nofollow" target="_blank">assign multiple processes</a> to the data loader so as to reduce the likelihood of starvation of the GPU. On GPU platforms, a general rule of thumb is to set the number of workers according to the number of CPU cores. However, on CPU platforms, where the model training uses the same resources as the data loader, this approach could backfire. Once again, the best approach for choosing the optimal number of workers may be trial and error. The table below shows the average throughput for different choices of <em class="qb">num_workers</em>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/e99407110718c6bd1fc86acc56c9d07f.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*TkNWkBuzhWD9XkW4w0QaTw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Training Throughput as Function of the Number of Data Loading Workers (by Author)</figcaption></figure><h2 id="a363" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Mixed Precision</h2><p id="65ec" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Another popular technique is to use lower precision floating point datatypes such as <code class="cx qd qe qf ps b">torch.float16</code> or <code class="cx qd qe qf ps b">torch.bfloat16</code> with the dynamic range of <code class="cx qd qe qf ps b">torch.bfloat16</code> generally considered to be more amiable to ML training. Naturally, reducing the datatype precision can have adverse effects on convergence and should be done carefully. PyTorch comes with <a class="af nc" href="https://pytorch.org/docs/stable/amp.html" rel="noopener ugc nofollow" target="_blank">torch.amp</a>, an automatic mixed precision package for optimizing the use of these datatypes. Intel® AVX-512 includes <a class="af nc" href="https://pytorch.org/blog/empowering-pytorch-on-intel-xeon-scalable-processors-with-bfloat16/" rel="noopener ugc nofollow" target="_blank">support for the bfloat16</a> datatype. The modified training step appears below:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="6b85" class="pv oa fq ps b bg pw px l py pz">for idx, (data, target) in enumerate(train_loader):<br/>    optimizer.zero_grad()<br/>    with torch.amp.autocast('cpu',dtype=torch.bfloat16):<br/>        output = model(data)<br/>        loss = criterion(output, target)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="55a5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The throughput following this optimization is 24.34 samples per second, an increase of 86%!!</p><h2 id="baf3" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Channels Last Memory Format</h2><p id="03bc" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk"><a class="af nc" href="https://pytorch.org/tutorials/intermediate/memory_format_tutorial.html" rel="noopener ugc nofollow" target="_blank">Channels last memory format</a> is a beta-level optimization (at the time of this writing), pertaining primarily to vision models, that supports storing four dimensional (NCHW) tensors in memory such that the channels are the last dimension. This results in all of the data of each pixel being stored together. This optimization pertains primarily to vision models. <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#channels-last" rel="noopener ugc nofollow" target="_blank">Considered to be more “friendly to Intel platforms”</a>, this memory format <a class="af nc" href="https://pytorch.org/blog/accelerating-pytorch-vision-models-with-channels-last-on-cpu/" rel="noopener ugc nofollow" target="_blank">reported</a>ly boosts the performance of a ResNet-50 on an <a class="af nc" href="https://www.intel.com/content/www/us/en/products/details/processors/xeon.html" rel="noopener ugc nofollow" target="_blank">Intel® Xeon® CPU</a>. The adjusted training step appears below:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="580f" class="pv oa fq ps b bg pw px l py pz">for idx, (data, target) in enumerate(train_loader):<br/>    data = data.to(memory_format=torch.channels_last)<br/>    optimizer.zero_grad()<br/>    with torch.amp.autocast('cpu',dtype=torch.bfloat16):<br/>        output = model(data)<br/>        loss = criterion(output, target)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="63f5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resulting throughput is 37.93 samples per second — an additional 56% improvement and a total of 415% compared to our baseline experiment. We are on a role!!</p><h2 id="0b23" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Torch Compilation</h2><p id="1023" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In a <a class="af nc" rel="noopener" target="_blank" href="/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d">previous post</a> we covered the virtues of PyTorch’s support for <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">graph compilation</a> and its potential impact on runtime performance. Contrary to the default eager execution mode in which each operation is run independently (a.k.a., “eagerly”), the <a class="af nc" href="https://pytorch.org/docs/stable/generated/torch.compile.html" rel="noopener ugc nofollow" target="_blank">compile</a> API converts the model into an intermediate computation graph which is then JIT-compiled into low-level machine code in a manner that is optimal for the underlying training engine. The API supports compilation via different backend libraries and with multiple configuration options. Here we will limit our evaluation to the <em class="qb">default </em>(TorchInductor) backend and the <a class="af nc" href="https://github.com/intel/intel-extension-for-pytorch" rel="noopener ugc nofollow" target="_blank"><em class="qb">ipex</em></a> backend from the <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html" rel="noopener ugc nofollow" target="_blank">Intel® Extension for PyTorch</a>, a library with dedicated optimizations for Intel hardware. Please see the <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&amp;version=v2.4.0%2bcpu&amp;os=linux%2fwsl2&amp;package=pip" rel="noopener ugc nofollow" target="_blank">documentation</a> for appropriate installation and usage instructions. The updated model definition appears below:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="64b1" class="pv oa fq ps b bg pw px l py pz">import intel_extension_for_pytorch as ipex<br/><br/>model = torchvision.models.resnet50()<br/>backend='inductor' # optionally change to 'ipex'<br/>model = torch.compile(model, backend=backend)</span></pre><p id="a308" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the case of our toy model, the impact of torch compilation is only apparent when the “channels last” optimization is disabled (an increase of ~27% for each of the backends). When “channels last” is applied, the performance actually drops. As a result, we drop this optimization from our subsequent experiments.</p><h1 id="f9c1" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Memory and Thread Optimizations</h1><p id="5e75" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">There are a number of opportunities for <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#cpu-specific-optimizations" rel="noopener ugc nofollow" target="_blank">optimizing the use of the underlying CPU resources</a>. These include optimizing memory management and thread allocation to the <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#intel-cpu-structure" rel="noopener ugc nofollow" target="_blank">structure</a> of the underlying CPU hardware. Memory management can be improved through the use of <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#switch-memory-allocator" rel="noopener ugc nofollow" target="_blank">advanced memory allocators</a> (such as <a class="af nc" href="https://github.com/jemalloc/jemalloc" rel="noopener ugc nofollow" target="_blank">Jemalloc</a> and <a class="af nc" href="https://google.github.io/tcmalloc/overview.html" rel="noopener ugc nofollow" target="_blank">TCMalloc</a>) and/or reducing memory accesses that are slower (i.e., across <a class="af nc" href="https://en.wikipedia.org/wiki/Non-uniform_memory_access" rel="noopener ugc nofollow" target="_blank">NUMA nodes</a>). Threading allocation can be improved through appropriate <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp" rel="noopener ugc nofollow" target="_blank">configuration of the OpenMP threading library</a> and/or use of <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#utilize-openmp" rel="noopener ugc nofollow" target="_blank">Intel’s Open MP library</a>.</p><p id="cc80" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Generally speaking, these kinds of optimizations require a deep level understanding of the CPU architecture and the features of its supporting SW stack. To simplify matters, PyTorch offers the <a class="af nc" href="https://pytorch.org/tutorials/recipes/xeon_run_cpu.html" rel="noopener ugc nofollow" target="_blank"><em class="qb">torch.backends.xeon.run_cpu</em></a> script for automatically configuring the memory and threading libraries so as to optimize runtime performance. The command below will result in the use of the dedicated memory and threading libraries. We will return to the topic of NUMA nodes when we discuss the option of distributed training.</p><p id="ca53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We verify appropriate installation of <a class="af nc" href="https://google.github.io/tcmalloc/overview.html" rel="noopener ugc nofollow" target="_blank">TCMalloc</a> (<code class="cx qd qe qf ps b">conda install conda-forge::gperftools</code>) and <a class="af nc" href="https://pypi.org/project/intel-openmp/" rel="noopener ugc nofollow" target="_blank">Intel’s Open MP library</a> (<code class="cx qd qe qf ps b">pip install intel-openmp</code>), and run the following command.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="3d73" class="pv oa fq ps b bg pw px l py pz">python -m torch.backends.xeon.run_cpu train.py</span></pre><p id="3a33" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The use of the <a class="af nc" href="https://pytorch.org/tutorials/recipes/xeon_run_cpu.html" rel="noopener ugc nofollow" target="_blank"><em class="qb">run_cpu</em></a> script further boosts our runtime performance to 39.05 samples per second. Note that the <a class="af nc" href="https://pytorch.org/tutorials/recipes/xeon_run_cpu.html" rel="noopener ugc nofollow" target="_blank"><em class="qb">run_cpu</em></a> script includes many controls for further tuning performance. Be sure to check out the documentation in order to maximize its use.</p><h1 id="1794" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">The Intel Extension for PyTorch</h1><p id="55bb" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html" rel="noopener ugc nofollow" target="_blank">Intel® Extension for PyTorch</a> includes additional opportunities for <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa" rel="noopener ugc nofollow" target="_blank">training optimization</a> via its <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/api_doc.html" rel="noopener ugc nofollow" target="_blank">ipex.optimize</a> function. Here we demonstrate its default use. Please see the documentation to learn of its full capabilities.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="1704" class="pv oa fq ps b bg pw px l py pz"> model = torchvision.models.resnet50()<br/> criterion = torch.nn.CrossEntropyLoss()<br/> optimizer = torch.optim.SGD(model.parameters())<br/> model.train()<br/> model, optimizer = ipex.optimize(<br/>    model, <br/>    optimizer=optimizer,<br/>    dtype=torch.bfloat16<br/> )</span></pre><p id="06b3" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Combined with the memory and thread optimizations discussed above, the resultant throughput is 40.73 samples per second. (Note that a similar result is reached when disabling the “channel’s last” configuration.)</p><h1 id="00cd" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Distributed Training on CPU</h1><p id="b987" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk"><a class="af nc" href="https://www.intel.com/content/www/us/en/products/details/processors/xeon.html" rel="noopener ugc nofollow" target="_blank">Intel® Xeon®</a> processors are designed with <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html#non-uniform-memory-access-numa" rel="noopener ugc nofollow" target="_blank">Non-Uniform Memory Access (NUMA)</a> in which the CPU memory is divided into groups, a.k.a., NUMA nodes, and each of the CPU cores is assigned to one node. Although any CPU core can access the memory of any NUMA node, the access to its own node (i.e., its local memory) is much faster. This gives rise to the notion of <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/examples.html#distributed-training" rel="noopener ugc nofollow" target="_blank">distributing training across NUMA nodes</a>, where the CPU cores assigned to each NUMA node act as a single process in a <a class="af nc" href="https://pytorch.org/docs/stable/distributed.html" rel="noopener ugc nofollow" target="_blank">distributed process group</a> and <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#train-a-model-on-cpu-with-pytorch-distributeddataparallel-ddp-functionality" rel="noopener ugc nofollow" target="_blank">data distribution</a> across nodes is managed by <a class="af nc" href="https://github.com/oneapi-src/oneCCL" rel="noopener ugc nofollow" target="_blank">Intel® oneCCL</a>, Intel’s dedicated collective communications library.</p><p id="2e2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can run data distributed training across NUMA nodes easily using the <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html" rel="noopener ugc nofollow" target="_blank"><em class="qb">ipexrun</em></a><em class="qb"> </em>utility. In the following code block (loosely based on <a class="af nc" href="https://github.com/intel/intel-extension-for-pytorch/blob/main/examples/cpu/training/python-scripts/distributed_data_parallel_training.py" rel="noopener ugc nofollow" target="_blank">this example</a>) we adapt our script to run data distributed training (according to usage detailed <a class="af nc" href="https://github.com/intel/torch-ccl?tab=readme-ov-file#usage" rel="noopener ugc nofollow" target="_blank">here</a>):</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="05c9" class="pv oa fq ps b bg pw px l py pz">import os, time<br/>import torch<br/>from torch.utils.data import Dataset, DataLoader<br/>from torch.utils.data.distributed import DistributedSampler<br/>import torch.distributed as dist<br/>import torchvision<br/>import oneccl_bindings_for_pytorch as torch_ccl<br/>import intel_extension_for_pytorch as ipex<br/><br/><br/>os.environ["MASTER_ADDR"] = "127.0.0.1"<br/>os.environ["MASTER_PORT"] = "29500"<br/>os.environ["RANK"] = os.environ.get("PMI_RANK", "0")<br/>os.environ["WORLD_SIZE"] = os.environ.get("PMI_SIZE", "1")<br/>dist.init_process_group(backend="ccl", init_method="env://")<br/>rank = os.environ["RANK"]<br/>world_size = os.environ["WORLD_SIZE"]<br/><br/>batch_size = 128<br/>num_workers = 0<br/><br/># define dataset and dataloader<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>        label = torch.tensor(data=index % 10, dtype=torch.uint8)<br/>        return rand_image, label<br/><br/>train_dataset = FakeDataset()<br/>dist_sampler = DistributedSampler(train_dataset)<br/>train_loader = DataLoader(<br/>    dataset=train_dataset, <br/>    batch_size=batch_size,<br/>    num_workers=num_workers,<br/>    sampler=dist_sampler<br/>)<br/><br/># define model artifacts<br/>model = torchvision.models.resnet50()<br/>criterion = torch.nn.CrossEntropyLoss()<br/>optimizer = torch.optim.SGD(model.parameters())<br/>model.train()<br/>model, optimizer = ipex.optimize(<br/>    model, <br/>    optimizer=optimizer,<br/>    dtype=torch.bfloat16<br/>)<br/><br/># configure DDP<br/>model = torch.nn.parallel.DistributedDataParallel(model)<br/><br/># run training loop<br/><br/># destroy the process group<br/>dist.destroy_process_group()<br/></span></pre><p id="944b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Unfortunately, as of the time of this writing, the <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/c7i/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 c7i</a> instance family does not include a multi-NUMA instance type. To test our distributed training script, we revert back to an <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/c6i/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 c6i.32xlarge</a> instance with 64 vCPUs and 2 NUMA nodes. We verify the <a class="af nc" href="https://github.com/intel/torch-ccl?tab=readme-ov-file#installation" rel="noopener ugc nofollow" target="_blank">installation</a> of <a class="af nc" href="https://github.com/intel/torch-ccl" rel="noopener ugc nofollow" target="_blank">Intel® oneCCL Bindings for PyTorch</a> and run the following command (as documented <a class="af nc" href="https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/training/python-scripts#running-example-scripts" rel="noopener ugc nofollow" target="_blank">here</a>):</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="b9e8" class="pv oa fq ps b bg pw px l py pz">source $(python -c "import oneccl_bindings_for_pytorch as torch_ccl;print(torch_ccl.cwd)")/env/setvars.sh<br/><br/># This example command would utilize all the numa sockets of the processor, taking each socket as a rank.<br/>ipexrun cpu --nnodes 1 --omp_runtime intel train.py </span></pre><p id="d21f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The following table compares the performance results on the <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/c6i/" rel="noopener ugc nofollow" target="_blank">c6i.32xlarge</a> instance with and without distributed training:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/d07c43929a4320f03b6319a28c7cbdd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*4CyPnZmZgO9S1CUvsFZM5w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Distributed Training Across NUMA Nodes (by Author)</figcaption></figure><p id="ba77" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In our experiment, data distribution did <em class="qb">not</em> boost the runtime performance. Please see <a class="af nc" href="https://intel.github.io/intel-extension-for-pytorch/latest/tutorials/performance_tuning/launch_script.html" rel="noopener ugc nofollow" target="_blank"><em class="qb">ipexrun documentation</em></a> for additional performance tuning options.</p><h1 id="27c4" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">CPU Training with Torch/XLA</h1><p id="dc98" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/how-to-accelerate-your-pytorch-training-with-xla-on-aws-3d599bc8f6a9">here</a>) we discussed the <a class="af nc" href="https://github.com/pytorch/xla" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a> library and its use of <a class="af nc" href="https://openxla.org/xla" rel="noopener ugc nofollow" target="_blank">XLA compilation</a> to enable PyTorch based training on <a class="af nc" href="https://github.com/pytorch/xla/blob/master/API_GUIDE.md" rel="noopener ugc nofollow" target="_blank"><em class="qb">XLA devices</em></a><em class="qb"> </em>such as TPU, GPU, <em class="qb">and</em> CPU. Similar to torch compilation, XLA uses graph compilation to generate machine code that is optimized for the target device. With the establishment of the <a class="af nc" href="https://cloud.google.com/blog/products/ai-machine-learning/googles-open-source-momentum-openxla-new-partnerships" rel="noopener ugc nofollow" target="_blank">OpenXLA Project</a>, one of the stated goals was to support high performance across all hardware backends, including CPU (see the CPU RFC <a class="af nc" href="https://docs.google.com/document/d/1ZzMcrjxITJeN2IjjgbzUjHh-4W1YgDUus3j25Dvn9ng/edit#heading=h.w9ztr841aqk8" rel="noopener ugc nofollow" target="_blank">here</a>). The code block below demonstrates the adjustments to our original (unoptimized) script required to train using <a class="af nc" href="https://pytorch.org/xla/release/r2.4/index.html#" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a>:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="8764" class="pv oa fq ps b bg pw px l py pz">import torch<br/>import torchvision<br/>import timeimport torch_xla<br/>import torch_xla.core.xla_model as xm<br/><br/><br/>device = xm.xla_device()<br/><br/>model = torchvision.models.resnet50().to(device)<br/>criterion = torch.nn.CrossEntropyLoss()<br/>optimizer = torch.optim.SGD(model.parameters())<br/>model.train()<br/><br/>for idx, (data, target) in enumerate(train_loader):<br/>    data = data.to(device)<br/>    target = target.to(device)<br/>    optimizer.zero_grad()<br/>    output = model(data)<br/>    loss = criterion(output, target)<br/>    loss.backward()<br/>    optimizer.step()<br/>    xm.mark_step()</span></pre><p id="2338" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Unfortunately, (as of the time of this writing) the XLA results on our toy model seem far inferior to the (unoptimized) results we saw above (— by as much as 7X). We expect this to improve as PyTorch/XLA’s CPU support matures.</p><h1 id="9e84" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Results</h1><p id="bd60" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We summarize the results of a subset of our experiments in the table below. For the sake of comparison, we add the throughput of training our model on <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/g5/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 g5.2xlarge</a> GPU instance following the optimization steps discussed in <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">this post</a>. The <em class="qb">samples per dollar</em> was calculated based on the<a class="af nc" href="https://aws.amazon.com/ec2/pricing/on-demand/" rel="noopener ugc nofollow" target="_blank"> Amazon EC2 On-demand pricing</a> page ($0.357 per hour for a c7i.2xlarge and $1.212 for a g5.2xlarge, as of the time of this writing).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/6e442a5f28ce4f1785d21a11e462bd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkEDtwXx9isLLlAo-IpNKg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Performance Optimization Results (by Author)</figcaption></figure><p id="1c2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although we succeeded in boosting the training performance of our toy model on the CPU instance by a considerable margin (446%), it remains inferior to the (optimized) performance on the GPU instance. Based on our results, training on GPU would be ~6.7 times cheaper. It is likely that with additional performance tuning and/or applying additional optimizations strategies, we could further close the gap. Once again, we emphasize that the comparative performance results we have reached are unique to this model and runtime environment.</p><h2 id="cb86" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Amazon EC2 Spot Instances Discounts</h2><p id="502f" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The increased availability of cloud-based CPU instance types (compared to GPU instance types) may imply greater opportunity for obtaining compute power at discounted rates, e.g., through Spot Instance utilization. <a class="af nc" href="https://aws.amazon.com/ec2/spot/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 Spot Instances</a> are instances from surplus cloud service capacity that are offered for a discount of as much as 90% off the On-Demand pricing. In exchange for the discounted price, AWS maintains the right to preempt the instance with little to no warning. Given the high demand for GPUs, you may find CPU spot instances easier to get ahold of than their GPU counterparts. At the time of this writing, c7i.2xlarge <a class="af nc" href="https://aws.amazon.com/ec2/spot/pricing/" rel="noopener ugc nofollow" target="_blank">Spot Instance price</a> is $0.1291 which would improve our samples per dollar result to 1135.76 and further reduces the gap between the optimized GPU and CPU price performances (to 2.43X).</p><p id="a938" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While the runtime performance results of the optimized CPU training of our toy model (and our chosen environment) were lower than the GPU results, it is likely that the same optimization steps applied to other model architectures (e.g., ones that include components that are not supported by GPU) may result in the CPU performance matching or beating that of the GPU. And even in cases where the performance gap is not bridged, there may very well be cases where the shortage of GPU compute capacity would justify running some of our ML workloads on CPU.</p><h1 id="0675" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Summary</h1><p id="6974" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Given the ubiquity of the CPU, the ability to use them effectively for training and/or running ML workloads could have huge implications on development productivity and on end-product deployment strategy. While the nature of the CPU architecture is less amiable to many ML applications when compared to the GPU, there are many tools and techniques available for boosting its performance — a select few of which we have discussed and demonstrated in this post.</p><p id="7876" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post we focused optimizing training on CPU. Please be sure to check out our many <a class="af nc" href="https://chaimrand.medium.com/" rel="noopener">other posts on medium</a> covering a wide variety of topics pertaining to performance analysis and optimization of machine learning workloads.</p></div></div></div></div>    
</body>
</html>