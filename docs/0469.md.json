["```py\npip install llama-index\n```", "```py\npip uninstall llama-index\npip install llama-index --upgrade --no-cache-dir --force-reinstall\n```", "```py\npip install weaviate-client llama-index-vector-stores-weaviate\n```", "```py\nOPENAI_API_KEY=\"<YOUR_OPENAI_API_KEY>\"\n```", "```py\n# !pip install python-dotenv\nimport os\nfrom dotenv import load_dotenv,find_dotenv\n\nload_dotenv(find_dotenv())\n```", "```py\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core.settings import Settings\n\nSettings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\nSettings.embed_model = OpenAIEmbedding()\n```", "```py\n!mkdir -p 'data'\n!wget '<https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt>' -O 'data/paul_graham_essay.txt'\n```", "```py\nfrom llama_index.core import SimpleDirectoryReader\n\n# Load data\ndocuments = SimpleDirectoryReader(\n        input_files=[\"./data/paul_graham_essay.txt\"]\n).load_data()\n```", "```py\nfrom llama_index.core.node_parser import SimpleNodeParser\n\nnode_parser = SimpleNodeParser.from_defaults(chunk_size=1024)\n\n# Extract nodes from documents\nnodes = node_parser.get_nodes_from_documents(documents)\n```", "```py\nimport weaviate\n\n# Connect to your Weaviate instance\nclient = weaviate.Client(\n    embedded_options=weaviate.embedded.EmbeddedOptions(), \n)\n```", "```py\nfrom llama_index.core import VectorStoreIndex, StorageContext\nfrom llama_index.vector_stores.weaviate import WeaviateVectorStore\n\nindex_name = \"MyExternalContext\"\n\n# Construct vector store\nvector_store = WeaviateVectorStore(\n    weaviate_client = client, \n    index_name = index_name\n)\n\n# Set up the storage for the embeddings\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# Setup the index\n# build VectorStoreIndex that takes care of chunking documents\n# and encoding chunks to embeddings for future retrieval\nindex = VectorStoreIndex(\n    nodes,\n    storage_context = storage_context,\n)\n```", "```py\n# The QueryEngine class is equipped with the generator\n# and facilitates the retrieval and generation steps\nquery_engine = index.as_query_engine()\n```", "```py\n# Run your naive RAG query\nresponse = query_engine.query(\n    \"What happened at Interleaf?\"\n)\n```", "```py\nfrom llama_index.core.node_parser import SentenceWindowNodeParser\n\n# create the sentence window node parser w/ default settings\nnode_parser = SentenceWindowNodeParser.from_defaults(\n    window_size=3,\n    window_metadata_key=\"window\",\n    original_text_metadata_key=\"original_text\",\n)\n```", "```py\nfrom llama_index.core.postprocessor import MetadataReplacementPostProcessor\n\n# The target key defaults to `window` to match the node_parser's default\npostproc = MetadataReplacementPostProcessor(\n    target_metadata_key=\"window\"\n)\n\n...\n\nquery_engine = index.as_query_engine( \n    node_postprocessors = [postproc],\n)\n```", "```py\nquery_engine = index.as_query_engine(\n    ...,\n    vector_store_query_mode=\"hybrid\", \n    alpha=0.5,\n    ...\n)\n```", "```py\n# !pip install torch sentence-transformers\nfrom llama_index.core.postprocessor import SentenceTransformerRerank\n\n# Define reranker model\nrerank = SentenceTransformerRerank(\n    top_n = 2, \n    model = \"BAAI/bge-reranker-base\"\n)\n\n...\n\n# Add reranker to query engine\nquery_engine = index.as_query_engine(\n\t\tsimilarity_top_k = 6,\n\t\t...,\n                node_postprocessors = [rerank],\n\t\t...,\n)\n```"]