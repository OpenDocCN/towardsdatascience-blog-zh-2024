<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Ablation Testing Neural Networks: The Compensatory Masquerade</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Ablation Testing Neural Networks: The Compensatory Masquerade</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07">https://towardsdatascience.com/ablation-testing-neural-networks-the-compensatory-masquerade-ba27d0037a88?source=collection_archive---------5-----------------------#2024-01-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6cbd" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Disruptively testing parts of neural networks and other ML architectures to make them more robust</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shafik Quoraishee" class="l ep by dd de cx" src="../Images/439d3502b98af4d994a8fab33b8bb428.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kHLKrilFxIqISeJWLYwlpA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://squoraishee.medium.com/?source=post_page---byline--ba27d0037a88--------------------------------" rel="noopener follow">Shafik Quoraishee</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ba27d0037a88--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/17facce6e4c4b29da7fcbbde246cf1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEepWfQ5M26K5UnhoZhdYA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">(Image generated by author using DALL-E). Interesting what AI thinks of it’s own brain.</figcaption></figure><p id="28d6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In a similar fashion to how a person’s intellect can be stress tested, Artificial Neural Networks can be subjected to a gamut of tests to evaluate how robust they are to different kinds of disruption, by running what’s called controlled Ablation Testing.</p><p id="9cf7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Before we get into <a class="af nx" href="https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence)" rel="noopener ugc nofollow" target="_blank">ablation testing</a>, lets talk about a familiar technique in “destructive evolution” that many people who study machine learning and artificial intelligence applications might be familiar with: <a class="af nx" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">Regularization</a></p><h2 id="6500" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk"><strong class="al">Regularization</strong></h2><p id="7b43" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">Regulariztion is a very well known example of ablating, or selectively destroying/deactivating parts of a neural network and re-training it to make it an even more powerful classifier.</p><p id="dfbb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Through a process called <a class="af nx" href="https://en.wikipedia.org/wiki/Dilution_(neural_networks)" rel="noopener ugc nofollow" target="_blank">Dropout</a>, neurons can be deactivated in a controlled way, which allow the work of the neural network that was previously handled by the now defunct neurons, to be taken up by nearby active neurons.</p><p id="6a49" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In nature, the brain actually can undergo similar phenomenon due to the concept of neuro-plasticity. If a person suffers brain damage, in some cases nearby neurons and brain structures can reorganize to help take up some of the functionality of the dead brain tissue.</p><p id="6868" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Or how if someone loses one of their senses, like vision, oftentimes their other senses become stronger to make up for their missing capability.</p><p id="7ecb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is also known as the <a class="af nx" href="https://www.britannica.com/science/compensatory-masquerade" rel="noopener ugc nofollow" target="_blank">Compensatory Masquerade</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj oy"><img src="../Images/38cb217c01889fc9be7aaf32bad46223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9-Jb7FMBzzqtF5DObEQxw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A fully connected Neural Network to the left, and randomized dropout version on the right. In many cases, these networks may actually perform comparatively well (Image by author)</figcaption></figure><h2 id="f8c7" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk"><strong class="al">Ablation Testing</strong></h2><p id="c6e3" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">While regularization is a technique used in neural networks and other A.I. architectures to aide in training a neural network better through artificial “<a class="af nx" href="https://en.wikipedia.org/wiki/Neuroplasticity" rel="noopener ugc nofollow" target="_blank">neuroplasticity</a>”, sometimes we want to just do a similar procedure on a neural network to see how it will behave in the presence of deactivations in terms of accuracy.</p><p id="4dca" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We might do this for several other reasons:</p><ul class=""><li id="38fc" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw oz pa pb bk"><strong class="nd fr">Identifying Critical Parts of a Neural Network:</strong> Some parts of a neural network may do more important work than other parts of a neural network. In order to optimize the resource usage and the training time of the network, we can selectively ablate “weaker learners”</li><li id="381a" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw oz pa pb bk"><strong class="nd fr">Reducing Complexity of the Neural Network: </strong>Sometimes neural networks can get quite large, especially in the case of Deep MLPs (<a class="af nx" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">multi layer perceptrons</a>). This can make it difficult to map their behavior from input to output. By selectively shutting of parts of the network, we can potentially identify regions of excessive complexity and remove redundancy — simplifying our architecture.</li><li id="fee9" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw oz pa pb bk"><strong class="nd fr">Fault Tolerance: </strong>In a realtime system, parts of a system can fail. The same applies for parts of a neural network, and thus the systems that depend on their output as we. We can turn to ablation studies to determine if destroying certain parts of the neural network, will cause the predictive or generative power of the system to suffer.</li></ul><h2 id="9747" class="ny nz fq bf oa ob oc od oe of og oh oi nk oj ok ol no om on oo ns op oq or os bk"><strong class="al">Types of Ablation Tests</strong></h2><p id="666f" class="pw-post-body-paragraph nb nc fq nd b go ot nf ng gr ou ni nj nk ov nm nn no ow nq nr ns ox nu nv nw fj bk">There are actually many different kinds of ablation tests, and here we are going to talk about 3 specific kinds:</p><ul class=""><li id="1242" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw oz pa pb bk">Neuronal Ablation</li><li id="a703" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw oz pa pb bk">Functional Ablation</li><li id="142f" class="nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw oz pa pb bk">Input Ablation</li></ul><p id="fd46" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A quick note that ablation tests will have different effects depending on the network you are testing against and the data itself. An ablation test might demonstrate weakness in 1 part of the network for a specific data set, and may demonstrate weakness in another part of the neural network for a different ablation test. That is why that in a truly robust ablation testing system, you will need a wide variety of tests to get an accurate picture of the ANN’s (<a class="af nx" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">Artificial Neural Network</a>) weak points.</p><p id="beff" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Neuronal Ablation</strong></p><p id="9ba5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is the first kind of ablation test we are going to run, and it’s the simplest to see the effects of and extend. We will simply remove varying percentages of neurons from our neural network</p><p id="0804" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For our experiment we have a simple ANN set up to test the accuracy of random character prediction agains using our old friend the <a class="af nx" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">MNIST data set</a>.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ph"><img src="../Images/193196488e4f29b9aee031723542889b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZTy0RHTJ4llH0qx0d1RRvg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">A snapshot of digit data from the MNIST data set (Image by author)</figcaption></figure><p id="dbcb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is the code I wrote as a simple ANN test harness to test digit classification accuracy.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="46b8" class="pm nz fq pj b bg pn po l pp pq">import tensorflow as tf<br/>from tensorflow.keras.datasets import mnist<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Dropout, Flatten<br/>from tensorflow.keras.optimizers import Adam<br/>import matplotlib.pyplot as plt<br/><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0<br/><br/># Create the ANN Model<br/>def create_model(dropout_rate=0.0):<br/>    model = Sequential([<br/>        Flatten(input_shape=(28, 28)),<br/>        Dense(128, activation='relu'),<br/>        Dropout(dropout_rate),<br/>        Dense(10, activation='softmax')<br/>    ])<br/>    model.compile(optimizer=Adam(),<br/>                  loss='sparse_categorical_crossentropy',<br/>                  metrics=['accuracy'])<br/>    return model<br/><br/># Run the ablation study: Dropout percentages of neurons<br/>dropout_rates = [0.0, 0.2, 0.4, 0.6, 0.8]<br/>accuracies = []<br/><br/>for rate in dropout_rates:<br/>    model = create_model(dropout_rate=rate)<br/>    model.fit(x_train, y_train, epochs=5, validation_split=0.2, verbose=0)<br/>    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)<br/>    accuracies.append(accuracy)<br/><br/>plt.plot(dropout_rates, accuracies, marker='o')<br/>plt.title('Accuracy vs Dropout Rate')<br/>plt.xlabel('Dropout Rate')<br/>plt.ylabel('Accuracy')<br/>plt.grid(True)<br/>plt.show()</span></pre><p id="e90b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So if we run the above code we see the following result of deactivating increasing percentages of our 128 node MLP.</p><p id="f1c9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The results are fairly interesting in this simple example, where as you can see dropping 80% of the neurons barely effects the accuracy, which means that removing excess neurons is certainly an optimization we could consider in building this network.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pr"><img src="../Images/8804627f95e4e423a2bdd894aef0979b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hC0ePrJmIjxWHcHyRPsX_g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Graph generated for dropout ablation test (Image by author)</figcaption></figure><p id="3575" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Functional Ablation</strong></p><p id="1479" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For functional ablation, we change the activation functions of the neurons to different curves, with different amounts of non-linearity. The last function we use is a straight line, completely destroying the non-linear characteristic of the model.</p><p id="57b5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Because non-linear models are by definition more complex than linear models, and the purpose of activation functions is to induce nonlinear effects on the classification, a line of reasoning one could make is that:</p><p id="c401" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="ps">“If we can get away with using linear functions instead of non-linear functions, and still have a good classification, then maybe we can simply our architecture and lower its cost”</em></p><p id="7655" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Note:</strong> You’ll notice in addition to regularization, certain kinds of ablation testing, like functional ablation has similarities to <a class="af nx" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">hyperparameter tuning</a>. They are similar, but ablation testing refers more to changing parts of the neural network architecture (e.g. neurons, layers, etc), where as hyperparameter tuning refers to changing structural parameters of the model. Both have the goal of optimization.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="5769" class="pm nz fq pj b bg pn po l pp pq"># Activation function ablation<br/>activation_functions = ['relu', 'sigmoid', 'tanh', 'linear']<br/>activation_ablation_accuracies = []<br/>for activation in activation_functions:<br/>    model = create_model(activation=activation)<br/>    model.fit(x_train, y_train, epochs=5, validation_split=0.2, verbose=0)<br/>    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)<br/>    activation_ablation_accuracies.append(accuracy)</span></pre><p id="a80b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">When we run the above code we get the following accuracies vs activation function.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pt"><img src="../Images/a428a18957cb11484ff2823a55c3a840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*om1NSjx7sgp8Png0hzlZyg.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Graph generated for functional ablation test (Image by author)</figcaption></figure><p id="157e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So it indeed it looks like non-linearity of some kind is important to the classification, with “<a class="af nx" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank">ReLU</a>” and <a class="af nx" href="https://en.wikipedia.org/wiki/Hyperbolic_functions" rel="noopener ugc nofollow" target="_blank">hyperbolic tangent</a> non-linearity being the most effective. This makes sense, because it’s well known that digit classification is best framed as a non-linear task.</p><p id="4a88" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Feature Ablation</strong></p><p id="d825" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We can also remove features from the classification and see how that effects the accuracy of our predictor.</p><p id="9c46" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Normally <em class="ps">prior</em> to doing a machine learning or data science project, we typically do exploratory data analysis (EDA) and feature selection to determine what features could be important to our classification problem.</p><p id="a390" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">But sometimes interesting effects can be observed, especially with the ever mysterious neural networks, by removing features as part of an ablation study and seeing the effect on classification. Using the following code, we can remove columns of pixels from our letters in groups of 4 columns.</p><p id="52d6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Obviously, there are several ways to ablate the features, by distorting the characters in different ways besides in columns. But we can start with this simple example and observe the effects.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="8818" class="pm nz fq pj b bg pn po l pp pq"># Input feature ablation<br/>input_ablation_accuracies = []<br/>for i in range(0, 28, 4):  # Remove columns of pixels groups of 4<br/>    x_train_ablated = np.copy(x_train)<br/>    x_test_ablated = np.copy(x_test)<br/>    x_train_ablated[:, :, i:min(i+4, 28)] = 0<br/>    x_test_ablated[:, :, i:min(i+4, 28)] = 0<br/><br/>    model = create_model()<br/>    model.fit(x_train_ablated, y_train, epochs=5, validation_split=0.2, verbose=0)<br/>    loss, accuracy = model.evaluate(x_test_ablated, y_test, verbose=0)<br/>    input_ablation_accuracies.append(accuracy)</span></pre><p id="86a3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">After we run the above feature ablation code, we see:</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pu"><img src="../Images/07a74f18d9fe0131f2c7bfc8f29ebea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CvIXFZpBiwE_T1mo6qRRDA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Graph generated for 4-column input feature ablation test (Image by author)</figcaption></figure><p id="e21b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interestingly, there’s a slight dip in accuracy when we remove columns 8 to to 12, and a rise again after that. That suggests that on average, the more “sensitive” character geometry lies in those center columns, but the other columns especially close to the beginning and end could potentially be removed for an optimization effect.</p><p id="11a5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here’s the same test against removing 7 columns at a time, along with the columns. Visualizing the actual distorted character data allows us to make a lot more sense of the result, as we see that the reason that removing the first few columns makes a smaller different is because they are mostly just padding!</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pv"><img src="../Images/5fe4ea84cf30abc94770488eff6c5111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4J_cTdn0ksNrGawW89Q2Xw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Graph generated for result of 4 column pixel removal (Image by author)</figcaption></figure><p id="f9cd" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Another interesting example of an ablation study would be testing against different sorts of noise profiles. Here below is code I wrote to progressively noise an image using the above ANN model.</p><pre class="ml mm mn mo mp pi pj pk bp pl bb bk"><span id="715d" class="pm nz fq pj b bg pn po l pp pq"># Ablation study with noise<br/>noise_levels = [0, 0.1, 0.2, 0.3, 0.4, 0.5]<br/>noise_ablation_accuracies = []<br/><br/>plt.figure(figsize=(12, 6))<br/><br/>for i, noise_level in enumerate(noise_levels):<br/>    x_train_noisy = x_train + noise_level * np.random.normal(0, 1, x_train.shape)<br/>    x_test_noisy = x_test + noise_level * np.random.normal(0, 1, x_test.shape)<br/>    x_train_noisy = np.clip(x_train_noisy, 0, 1)<br/>    x_test_noisy = np.clip(x_test_noisy, 0, 1)<br/><br/>    model = create_model()<br/>    model.fit(x_train_noisy, y_train, epochs=5, validation_split=0.2, verbose=0)<br/>    loss, accuracy = model.evaluate(x_test_noisy, y_test, verbose=0)<br/>    noise_ablation_accuracies.append(accuracy)<br/><br/>    # Plot noisy test images<br/>    plt.subplot(2, len(noise_levels), i + 1)<br/>    for j in range(5):  # Display first 5 images<br/>        plt.imshow(x_test_noisy[j], cmap='gray')<br/>        plt.axis('off')<br/>    plt.title(f'Noise Level: {noise_level}')</span></pre><p id="e504" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We’ve created an ablation study for the robustness of the network in the presence of an increasing strength <a class="af nx" href="https://en.wikipedia.org/wiki/Gaussian_noise#:~:text=In%20signal%20processing%20theory%2C%20Gaussian,can%20take%20are%20Gaussian%2Ddistributed." rel="noopener ugc nofollow" target="_blank">Gaussian Noise</a>. Notice the expected and marked decreasing prediction accuracy as the noise level increases.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj pw"><img src="../Images/0f3ccd17fd6e9b8bc4ad14b5df6b461d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JTyhn99tskEO-rk_jYnb2g.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Graph generated for result of progressive noising (Image by author)</figcaption></figure><p id="a759" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Situations like this let us know that we may have to increase the power and complexity of our neural network to compensate. Also remember that ablation studies can be done in combination which each other, in the presence of different types of noise combined with different types of distortion.</p><p id="0a4c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Conclusions</strong></p><p id="1441" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Ablation studies can be very important to optimizing and testing a neural network. We demonstrated a small example here in this post, but there are an innumerable number of ways to run these studies on different and more complex network architectures. If you have any thoughts, would love some feedback and also perhaps even put them in your own article. Thank you for reading!</p></div></div></div></div>    
</body>
</html>