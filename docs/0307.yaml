- en: 'A Data Mesh Implementation: Expediting Value Extraction from ERP/CRM Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-data-mesh-implementation-expediting-value-extraction-from-erp-crm-systems-66ac65644fe2?source=collection_archive---------5-----------------------#2024-02-01](https://towardsdatascience.com/a-data-mesh-implementation-expediting-value-extraction-from-erp-crm-systems-66ac65644fe2?source=collection_archive---------5-----------------------#2024-02-01)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Enabling fast data development from big operational systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@david.rubio_63959?source=post_page---byline--66ac65644fe2--------------------------------)[![David
    Rubio](../Images/6a828bf368bd40aa5b1efda618dffed8.png)](https://medium.com/@david.rubio_63959?source=post_page---byline--66ac65644fe2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--66ac65644fe2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--66ac65644fe2--------------------------------)
    [David Rubio](https://medium.com/@david.rubio_63959?source=post_page---byline--66ac65644fe2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--66ac65644fe2--------------------------------)
    ·7 min read·Feb 1, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0572559b67060803cd5562c1e4b416e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Benjamin Zanatta](https://unsplash.com/@benjaminzanatta?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The challenge when facing the ‘monster’
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a data engineer building analytics from transactional systems such as ERP
    (enterprise resource planning) and CRM (customer relationship management), the
    main challenge lies in navigating the gap between raw operational data and domain
    knowledge. ERP and CRM systems are designed and built to fulfil a broad range
    of business processes and functions. **This generalisation makes their data models
    complex and cryptic and require domain expertise**.
  prefs: []
  type: TYPE_NORMAL
- en: Even harder to manage, a common setup within large organisations is to have
    several instances of these systems with some underlaying processes in charge of
    transmitting data among them, which could lead to duplications, inconsistencies,
    and opacity.
  prefs: []
  type: TYPE_NORMAL
- en: The disconnection between the operational teams immersed in the day-to-day functions
    and those extracting business value from data generated in the operational processes
    still remains a significant friction point.
  prefs: []
  type: TYPE_NORMAL
- en: Searching for data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine being a data engineer/analyst tasked with identifying the top-selling
    products within your company. Your first step might be to locate the orders. Then
    you begin researching database objects and find a couple of views, but there are
    some inconsistencies between them so you do not know which one to use. Additionally,
    it is really hard to identify the owners, one of them has even recently left the
    company. As you do not want to start your development with uncertainty, you decide
    to go for the operational raw data directly. Does it sound familiar?
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Operational Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I used to connect to views in transactional databases or APIs offered by operational
    systems to request the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e7a117965fe474c52087b5a4fe9564a.png)'
  prefs: []
  type: TYPE_IMG
- en: Order snapshots are stored in my own development area (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: To prevent my extractions from impacting performance on the operational side,
    I queried this data regularly and stored it in a persistent staging area (PSA)
    within my data warehouse. This allowed me to execute complex queries and data
    pipelines using these snapshots without consuming any resource from operational
    systems, but could result in unnecessary duplication of data in case I was not
    aware of other teams doing the same extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Operational Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the raw operational data was available, then I needed to deal with the
    next challenge: deciphering all the cryptic objects and properties and dealing
    with the labyrinth of dozens of relationships between them (i.e. General Material
    Data in SAP documented [https://leanx.eu/en/sap/table/mara.html](https://leanx.eu/en/sap/table/mara.html))'
  prefs: []
  type: TYPE_NORMAL
- en: Even though standard objects within ERP or CRM systems are well documented,
    **I needed to deal with numerous custom objects and properties that require domain
    expertise** as these objects cannot be found in the standard data models. Most
    of the time I found myself throwing ‘trial-and-error’ queries in an attempt to
    align keys across operational objects, interpreting the meaning of the properties
    according to their values and checking with operational UI screenshots my assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Operational data management in Data Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Data Mesh implementation improved my experience in these aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge**: I could quickly identify the owners of the exposed data. **The
    distance between the owner and the domain that generated the data is key to expedite
    further analytical development.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discoverability**: A shared data platform provides a catalog of operational
    datasets in the form of source-aligned data products that helped me to understand
    the status and nature of the data exposed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility**: I could easily request access to these data products. As
    this data is stored in the shared data platform and not in the operational systems,
    I did not need to align with operational teams for available windows to run my
    own data extraction without impacting operational performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Source-aligned Data Products
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to the Data Mesh taxonomy, data products built on top of operational
    sources are named Source-aligned Data Products:'
  prefs: []
  type: TYPE_NORMAL
- en: Source domain datasets represent closely the raw data at the point of creation,
    and are not fitted or modelled for a particular consumer — [Zhamak Dehghani](https://twitter.com/zhamakd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Source-aligned data products aim to represent operational sources within a shared
    data platform in a one-to-one relationship with operational entities and they
    should not hold any business logic that could alter any of their properties.
  prefs: []
  type: TYPE_NORMAL
- en: Ownership
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a Data Mesh implementation, **these data products should**
  prefs: []
  type: TYPE_NORMAL
- en: '**strictly** **be owned by the business domain that generates the raw data**.
    The owner is responsible for the quality, reliability, and accessibility of their
    data and data is treated as a product that can be used by the same team and other
    data teams in other parts of the organisation.'
  prefs: []
  type: TYPE_NORMAL
- en: '**This ownership ensures domain knowledge is close to the exposed data**. This
    is critical to enabling the fast development of analytical data products, as any
    clarification needed by other data teams can be handled quickly and effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following this approach, the Sales domain is responsible for publishing a ‘sales_orders’
    data product and making it available in a shared data catalog.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fb3038cd98501d96003c3778206c103.png)'
  prefs: []
  type: TYPE_IMG
- en: Sales Orders DP exposing sales_orders_dataset (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The data pipeline in charge of maintaining the data product could be defined
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2aefb513782736438d769f3d84ae04aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Data pipeline steps (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '**Data extraction**'
  prefs: []
  type: TYPE_NORMAL
- en: The first step to building source-aligned data products is to extract the data
    we want to expose from operational sources. There are a bunch of Data Integration
    tools that offer a UI to simplify the ingestion. Data teams can create a job there
    to extract raw data from operational sources using JDBC connections or APIs. To
    avoid wasting computational work, and whenever possible, only the updated raw
    data since the last extraction should be incrementally added to the data product.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data cleansing**'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have obtained the desired data, the next step involves some curation,
    so consumers do not need to deal with existing inconsistencies in the real sources.
    Although any business logic should not not be implemented when building source-aligned
    data products, basic cleansing and standardisation is allowed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Data update**'
  prefs: []
  type: TYPE_NORMAL
- en: Once extracted operational data is prepared for consumption, the data product’s
    internal dataset is incrementally updated with the latest snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: One of the requirements for a data product is to be **interoperable**. This
    means that we need to expose global identifiers so our data product might be universally
    used in other domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**Metadata update**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data products need to be **understandable**. Producers need to incorporate
    meaningful metadata for the entities and properties contained. This metadata should
    cover these aspects for each property:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Business description: What each property represents for the business. For example,
    “*Business category for the sales order*”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Source system: Establish a mapping with the original property in the operational
    domain. For instance, “*Original Source: ERP | MARA-MTART table BIC/MARACAT property*”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data characteristics: Specific characteristics of the data, such as enumerations
    and options. For example, “*It is an enumeration with these options: Invoice,
    Payment, Complaint*”.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data products also need to be **discoverable**. Producers need to publish them
    in a shared data catalog and indicate how the data is to be consumed by defining
    output port assets that serve as interfaces to which the data is exposed.
  prefs: []
  type: TYPE_NORMAL
- en: And data products must be **observable**. Producers need to deploy a set of
    monitors that can be shown within the catalog. When a potential consumer discovers
    a data product in the catalog, they can quickly understand the health of the data
    contained.
  prefs: []
  type: TYPE_NORMAL
- en: Consumer experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, again, imagine being a data engineer tasked with identifying the top-selling
    products within your company. But this time, imagine that you have access to a
    data catalog that offers data products that represent the truth of each domain
    shaping the business. You simply input ‘orders’ into the data product catalog
    and find the entry published by the Sales data team. And, at a glance, you can
    assess the quality and freshness of the data and read a detailed description of
    its contents.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f282ce8153984c50b5a3703776d02d79.png)'
  prefs: []
  type: TYPE_IMG
- en: Entry for Sales Orders DP within the Data Catalog example (image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: This upgraded experience eliminates the uncertainties of traditional discovery,
    allowing you to start working with the data right away. But what’s more, you know
    who is accountable for the data in case further information is needed. And whenever
    there is an issue with the Sales orders data product, you will receive a notification
    so that you can take actions in advance.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have identified several benefits of enabling operational data through source-aligned
    data products, especially when they are owned by data producers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Curated operational data accessibility:** In large organisations, source-aligned
    data products represent a bridge between operational and analytical planes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collision reduction with operational work**: Operational systems accesses
    are isolated within source-aligned data products pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source of truth**: A common data catalog with a list of curated operational
    business objects reducing duplication and inconsistencies across the organisation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clear data ownership**: Source-aligned data products should be **owned by
    the domain that generates the operational data to ensure domain knowledge** is
    close to the exposed data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on my own experience, this approach works exceptionally well in scenarios
    where large organisations struggle with data inconsistencies across different
    domains and friction when building their own analytics on top of operational data.
    **Data Mesh encourages each domain to build the ‘source of truth’ for the core
    entities they generate** and make them available in a shared catalog allowing
    other teams to access them and create consistent metrics across the whole organisation.
    **This enables analytical data teams to accelerate their work in generating analytics
    that drive real business value.**
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://martinfowler.com/articles/data-monolith-to-mesh.html?source=post_page-----66ac65644fe2--------------------------------)
    [## How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh'
  prefs: []
  type: TYPE_NORMAL
- en: There are problems with the centralized data lake. A future data mesh needs
    domains, self-service platforms, and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: martinfowler.com](https://martinfowler.com/articles/data-monolith-to-mesh.html?source=post_page-----66ac65644fe2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.oreilly.com/library/view/data-mesh/9781492092384/](https://www.oreilly.com/library/view/data-mesh/9781492092384/)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks to my Thoughtworks colleagues Arne (twice!), Pablo, Ayush and Samvardhan
    for taking the time to review the early versions of this article*'
  prefs: []
  type: TYPE_NORMAL
