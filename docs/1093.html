<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>My First Billion (of Rows) in DuckDB</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>My First Billion (of Rows) in DuckDB</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01">https://towardsdatascience.com/my-first-billion-of-rows-in-duckdb-11873e5edbb5?source=collection_archive---------0-----------------------#2024-05-01</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="816b" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">First Impressions of DuckDB handling 450Gb in a real project</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="João Pedro" class="l ep by dd de cx" src="../Images/64a0e14527be213e5fde0a02439fbfa7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*AGYCo0LFRlCcch_Ati7jFA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://joaopedro214.medium.com/?source=post_page---byline--11873e5edbb5--------------------------------" rel="noopener follow">João Pedro</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--11873e5edbb5--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 1, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">11</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/f57054e6187d4d4d8a6e084b95284c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UQCWVypEdkUXVhos"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Duck blueprint. Generated by Copilot Designer.</figcaption></figure><h1 id="7591" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Introduction</h1><p id="d226" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The fields of AI, Data Science, and Data Engineering are progressing at full steam. Every day new tools, new paradigms, and new architectures are created, always trying to solve the problems of the previous ones. In this sea of new opportunities, it’s interesting to know a little about the available tools to solve problems efficiently. And I’m not talking only about the technicalities, but the scope of use, advantages, disadvantages, challenges, and opportunities, something acquired with practice.</p><p id="bc95" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this post, I’ll describe my first experience in DuckDB (the new hyped database for processing huge amounts of data locally on your computer) revisiting an old problem that I faced previously — The processing of Logs of Brazilian Electronic Ballot Boxes to calculate vote-time metrics. As you’ll see through this post, this is a challenging problem that serves as a good benchmark for both performance and user experience assessments.</p><p id="25f1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The idea is that this post can serve as input for you, who want to know a little more about DuckDB, as I will cover both technical aspects, running the problem, and calculating the database performance, and more ‘soft’ aspects, like programming experience and usability.</p></div></div></div><div class="ab cb oz pa pb pc" role="separator"><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf pg"/><span class="pd by bm pe pf"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2263" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="ph">DuckDB is an Open Source Project [</em><a class="af pi" href="https://opensource.org/osd" rel="noopener ugc nofollow" target="_blank"><em class="ph">OSD</em></a><em class="ph">], the author has no affiliation with DuckDB/DuckDB Labs. The data used is available in the </em><a class="af pi" href="https://opendatacommons.org/licenses/odbl/" rel="noopener ugc nofollow" target="_blank"><em class="ph">ODbL</em></a><em class="ph"> License. This project is completely free to carry out. It does not require payment for any services, data access, or other expenses.</em></p><h1 id="5202" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">The Problem</h1><p id="47d4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The problem consists of processing records from Electronic Ballot Boxes’ Logs to obtain statistical metrics about the voting time of Brazilian voters. For example, calculate the average time citizens use to vote, collect fingerprints for identification, and so on. These metrics should be aggregated in several granularity levels: at the country level, state, electoral zone, and electoral section.</p><p id="5325" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In case you don’t know, Brazil has a 100% electronic voting system, where all the 100+ million citizens vote on a single day and the election’s result is computed and released near real-time. Votes are collected by thousands of electronic ballot boxes spread all over the country.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pj"><img src="../Images/c93e2da9a47c8ee0f7722ab4d5e391ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/0*FISZMwhejywRpn4G.jpeg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Electronic ballot box. <a class="af pi" href="https://www.tre-rn.jus.br/comunicacao/noticias/2021/Maio/urna-eletronica-25-anos-100-brasileira-e-admirada-pelo-mundo" rel="noopener ugc nofollow" target="_blank">Image from the Brazillian Superior Electoral Court</a>.</figcaption></figure><blockquote class="pk pl pm"><p id="2ca2" class="ny nz ph oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">An electronic ballot box is a microcomputer for specific use for election<strong class="oa fr">s</strong>, with the following characteristics: resistant, small, light, with energy autonomy, and with security features [<a class="af pi" href="https://international.tse.jus.br/en/electronic-ballot-box/presentation" rel="noopener ugc nofollow" target="_blank">4</a>]. Each can hold up to 500 voters, a number chosen to avoid big queues in the voting locations.</p></blockquote><p id="5b83" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The system is administered by the TSE (Supreme Electoral Court), which shares data about the process in its <a class="af pi" href="https://dadosabertos.tse.jus.br/" rel="noopener ugc nofollow" target="_blank">open data portal</a> [<a class="af pi" href="https://opendatacommons.org/licenses/odbl/" rel="noopener ugc nofollow" target="_blank">ODbL</a> License]. The logs are text files with an exhaustive list of all events in the ballot box.</p><p id="2172" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And that’s where the challenge begins. As the logs register absolutely every single event, it’s possible to calculate an enormous amount of metrics from them; it’s a vibrant information fountain. But what makes them rich, also makes them extremely hard to handle, as the totality of all the country’s records reaches the milestone of 450Gb in TSV files with + 4 billion lines.</p><p id="a789" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Besides the volume, another thing that makes this work a good benchmark, in my opinion, is that the needed transformations to reach our final goal are from all sorts of complexities, from simple (where, group by, order by) to complex SQL operations (like windows functions).</p><h1 id="430f" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">DuckDB</h1><p id="83b6" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">With this relatively high volume of data, one can be willing to evoke traditional Big Data tools, like Apache Spark, and process this data in a cluster with many workers, several gigabytes of RAM, and a dozen CPUs.</p><p id="6d67" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">DuckDB was created to challenge this <em class="ph">status quo</em>.</p><p id="56c5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As its creator defends (<a class="af pi" href="https://youtu.be/GaHWuQ_cBhA" rel="noopener ugc nofollow" target="_blank">in this video</a>), it is a database thought to empower single machines with the ability to process large volumes of data.</p><p id="5e48" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">I.e., instead of looking for complex industry solutions — like PySpark — or cloud-based solutions — like Google BigQuery — one will use a local in-process database with standard SQL to realize the needed transformations.</p><p id="29b6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">So, in a nutshell, DuckDB is an in-process (that runs in the program itself, it has no independent process, resembling SQLite), OLAP (adjusted to analytical loads), that handles data in traditional formats (CSV, parquet), optimized to handle large volumes of data using the power of a single machine (that doesn’t need to be very powerful).</p><h1 id="e3be" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">The Data</h1><p id="3217" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">A ballot box’s log is a single TSV file with a standardized name — XXXXXYYYYZZZZ.csv, composed of the box’s location metadata, with the 5 first digits being the city code, the next 4 the electoral zone (a geographical state’s subdivision), and the last 4 the electoral section (the ballot box itself).</p><p id="b6b4" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">There are almost 500,000 ballot boxes in Brazil, so, almost 500.000 files. The file’s size depends on the number of voters in the section, which varies from 1 to 500. This is what the logs look like:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="edb2" class="pr nd fq po b bg ps pt l pu pv">2022-10-02 09:35:17 INFO 67305985 VOTA Voter was enabled<br/>2022-10-02 09:43:55 INFO 67305985 VOTA Vote confirmed for [Federal Deputy]<br/>2022-10-02 09:48:39 INFO 67305985 VOTA Vote confirmed for [State Deputy]<br/>2022-10-02 09:49:10 INFO 67305985 VOTA Vote confirmed for [Senator]<br/>2022-10-02 09:49:47 INFO 67305985 VOTA Vote confirmed for [Governor]<br/>2022-10-02 09:50:08 INFO 67305985 VOTA Vote confirmed for [President]<br/>2022-10-02 09:50:09 INFO 67305985 VOTA The voter's vote was computed<br/># Literal Translations to English<br/># Events that represent a vote</span></pre><p id="bc54" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">We’re interested in transforming this raw information into statistical metrics about voting time(How much time each voter takes to vote? How many votes are computed each minute?) in several granularity levels (country, state, city) and, to achieve that, we’re going to create an <a class="af pi" href="https://en.wikipedia.org/wiki/OLAP_cube" rel="noopener ugc nofollow" target="_blank">OLAP Cube</a> like that:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="ced9" class="pr nd fq po b bg ps pt l pu pv">| State         | City              | Mean Voting Time (seconds) | Max Votes Computed in 5 Min |<br/>|---------------|-------------------|----------------------------|-----------------------------|<br/>| Null          | Null              | 50                         | 260                         |<br/>| São Paulo     | São Paulo         | 30                         | 300                         |<br/>| São Paulo     | Campinas          | 35                         | 260                         |<br/>| São Paulo     | Null              | 20                         | 260                         |<br/>| Rio de Janeiro| Rio de Janeiro    | 25                         | 360                         |<br/>| Minas Gerais  | Belo Horizonte    | 40                         | 180                         |<br/>| Bahia         | Salvador          | 28                         | 320                         |<br/>| Rio Grande ...| Porto Alegre      | 30                         | 300                         |<br/>| ...           | ...               | ...                        | ...                         |</span></pre><h1 id="a779" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">The Implementation</h1><h2 id="59b3" class="pw nd fq bf ne px py pz nh qa qb qc nk oh qd qe qf ol qg qh qi op qj qk ql qm bk">Setup the environment</h2><p id="5453" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">All that’s needed to run this project is a Python environment with the <a class="af pi" href="https://duckdb.org/docs/guides/python/install.html" rel="noopener ugc nofollow" target="_blank">DuckDB package installed</a>.</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="0ac3" class="pr nd fq po b bg ps pt l pu pv">pip install duckdb</span></pre><h2 id="08b1" class="pw nd fq bf ne px py pz nh qa qb qc nk oh qd qe qf ol qg qh qi op qj qk ql qm bk">Transforming the data</h2><p id="f721" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the following sections, I’ll describe each transformation, its objectives, how DuckDB can perform each one, the advantages, challenges, results, and conclusions.</p><p id="1907" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The processing is divided into 4 steps: Convert TSV files to Parquet; Filter and Clear; Isolate votes and their attributes; and Compute metrics to the OLAP Cube.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/441568ddc6e145b3636bcef42c853242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsYEDbaMY6tyfLNBhZtbRw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Processing Steps. Image by Author.</figcaption></figure><p id="fa61" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Unfortunately, to avoid making this post enormous, I’ll not explain each transformation in detail. But all the code is available on the <a class="af pi" href="https://github.com/jaumpedro214/urna-logs-data-tseng" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>.</p><p id="9d8f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">Converting TSV files to Parquet</strong></p><p id="b96a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A simple and indispensable step for anyone who wants to work with large volumes of data. Doing this on DuckDB is straightforward.</p><p id="b168" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">First, create a DuckDB session:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="15c2" class="pr nd fq po b bg ps pt l pu pv">cursor = duckdb.connect("")</span></pre><p id="6d46" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this example, we instantiate the database connector with an empty string. This is done to indicate that DuckDB should not create its own database file; rather, it should only interact with system files. As mentioned earlier, DuckDB is a database, so it has the functionalities to create tables, views, and so on, which we won’t explore here. We’ll focus solely on using it as a transformation engine.</p><p id="53b3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And define the following query:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="31fe" class="pr nd fq po b bg ps pt l pu pv">query = f"""<br/>    COPY (<br/>        SELECT <br/>            * <br/>        FROM read_csv('/data/logs/2_{state}/*.csv', filename=True)<br/>    ) TO '{state}.parquet' (FORMAT 'parquet');<br/>"""</span></pre><pre class="qo pn po pp bp pq bb bk"><span id="7ca2" class="pr nd fq po b bg ps pt l pu pv">cursor.execute(query)</span></pre><p id="8a4d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And that’s all!</p><p id="c744" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Let’s detail the query:</p><p id="54a3" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The inner expression is just a standard<em class="ph"> SELECT * FROM table</em> query, the only difference is that, instead of referencing a table, DuckDB can reference files directly.</p><p id="eb04" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The result of this query could be imported to a pandas dataframe for further expression, just like this:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="3223" class="pr nd fq po b bg ps pt l pu pv">my_df = cursor.execute(query).df()</span></pre><p id="e763" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Which allows seamless integration between DuckDB and pandas.</p><p id="dfb6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The outer expression is a simple <em class="ph">COPY … TO …</em> , which writes the inner query’s result as a file.</p><p id="89ef" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this first transformation, we can start to see one of the strengths of DuckDB— the ability to interact with files using plain old SQL, without needing to configure anything else. The above query is not different at all from day-to-day operations that we make in standard SGBDs, like PostgreSQL and MySQL, with the only difference being that, instead of manipulating tables, we’re interacting with files.</p><p id="b81d" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Originally, we had <strong class="oa fr">450Gb </strong>of TSV files and, after ~<strong class="oa fr">30min</strong>, we ended up with <strong class="oa fr">97Gb </strong>of Parquet.</p><h2 id="479e" class="pw nd fq bf ne px py pz nh qa qb qc nk oh qd qe qf ol qg qh qi op qj qk ql qm bk">Filter and Clear</h2><p id="2ae0" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As mentioned earlier, the Logs store every event that happens on a ballot box. This first step aims to filter only vote-related events, like ‘<em class="ph">The voter voted for PRESIDENT</em>’, ‘<em class="ph">The Voter had fingerprints collected</em>’, and ‘<em class="ph">The vote was computed</em>’ that happened on the election days (that’s important, as the logs also store training sections and other administrative procedures realized).</p><p id="bdc6" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">A simple query, but with a lot of text and date manipulations:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="ce5a" class="pr nd fq po b bg ps pt l pu pv"><br/>VOTES_DESCRIPTIONS = [<br/>    # VOTES<br/>    "event_description = 'Aguardando digitação do título'",  <br/>    # Awaiting voter's title (Voter Registration ID) input<br/>    "event_description = 'Título digitado pelo mesário'",  <br/>    # Voter's title entered by the poll worker<br/>    "event_description = 'Eleitor foi habilitado'",  <br/>    # Voter has been enabled<br/>    "event_description ILIKE 'Voto confirmado par%'", <br/>    # Vote confirmed for ... could be [PRESIDENT, SENATOR, DEPUTY, ...]<br/>    "event_description = 'O voto do eleitor foi computado'",  <br/>    # Voter's vote has been computed<br/>]<br/><br/>ACCEPTED_DATES = [<br/>    '2022-10-02', '2022-10-30', # Constitutional date of the election filter<br/>    '2022-10-03', '2022-10-31', <br/>]<br/><br/>query = F"""<br/>    SELECT <br/>        *<br/>    FROM (<br/>        SELECT<br/>            event_timestamp,<br/>            event_timestamp::date AS event_date,<br/>            event_type,<br/>            some_id,<br/>            event_system,<br/>            event_description,<br/>            event_id,<br/>                <br/>            REPLACE(SPLIT_PART(filename, '/', 5), '_new.csv', '') AS filename,<br/>            <br/>            -- Metadata from filename<br/>            SUBSTRING( SPLIT_PART(SPLIT_PART(filename, '/', 5), '-', 2),  1, 5 ) AS city_code,<br/>            SUBSTRING( SPLIT_PART(SPLIT_PART(filename, '/', 5), '-', 2),  6, 4 ) AS zone_code,<br/>            SUBSTRING( SPLIT_PART(SPLIT_PART(filename, '/', 5), '-', 2), 10, 4 ) AS section_code,<br/>            REPLACE(SPLIT_PART(filename, '/', 4), '2_', '') AS uf<br/>        FROM<br/>            {DATASET}<br/>        WHERE 1=1<br/>            AND ( {' OR '.join(VOTES_DESCRIPTIONS)} )<br/>    ) _<br/>    WHERE 1=1<br/>    AND event_date IN ({', '.join([F"'{date}'" for date in ACCEPTED_DATES])})<br/>"""</span></pre><p id="1140" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this query, another advantage of DuckDB is highlighted, the ability to read and write partitioned data. Table partitioning is very relevant in the context of Big Data, but is still even more significant in the single-machine paradigm, given that we’re operating the same disk for input and output, i.e., it suffers twice, and every optimization is welcome.</p><p id="64a2" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Originally, we had 97Gb, but after ~30min, we were left with 63Gb of Parquet.</p><h2 id="21a9" class="pw nd fq bf ne px py pz nh qa qb qc nk oh qd qe qf ol qg qh qi op qj qk ql qm bk">Isolate votes and their attributes</h2><p id="5860" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As each vote is composed of several lines, we need to condense all the information in a unique record, to ease the calculations. Here things get complicated, as the query gets complex and, unfortunately, DuckDB could not process all the data in one go.</p><p id="c4fd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">To overcome this issue, I did a loop to process the data incrementally in slices:</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="eb6f" class="pr nd fq po b bg ps pt l pu pv">for state in states:<br/>    for date in ACCEPTED_DATES:<br/>        for zone_group in ZONE_GROUPS:<br/>            query = F"""<br/>                COPY <br/>                {<br/>                    complex_query_goes_here<br/>                    .replace('&lt;uf&gt;', state)<br/>                    .replace('&lt;event_date&gt;', date)<br/>                    .replace('&lt;zone_id_min&gt;', str(zone_group[0]))<br/>                    .replace('&lt;zone_id_max&gt;', str(zone_group[1]))<br/>                } <br/>                TO 'VOTES.parquet' <br/>                (FORMAT 'parquet', PARTITION_BY (event_date, uf, zone_group), OVERWRITE_OR_IGNORE 1);<br/>            """</span></pre><p id="1c28" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The implementation details don’t matter, the interesting part is that we don’t need to change the code too much to build this final table incrementally. As each ‘slice’ processed represents a partition, by setting the parameter OVERWRITE_OR_IGNORE to 1, DuckDB will automatically overwrite any existing data for that partition or ignore it if it already exists.</p><p id="c0cf" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Originally, we had 63GB, after ~1 hour and 20 minutes, we ended up with 15GB of Parquet.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/21c5aa0e745157d524083a297e1ab2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnMZ3oO__KU-ZewwoELaQg.png"/></div></div></figure><h2 id="390e" class="pw nd fq bf ne px py pz nh qa qb qc nk oh qd qe qf ol qg qh qi op qj qk ql qm bk">Compute metrics and build the OLAP Cube</h2><p id="7f9a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">This is a simple step. Now, with each vote represented by a record, all needed is to compute the metrics.</p><pre class="mm mn mo mp mq pn po pp bp pq bb bk"><span id="250b" class="pr nd fq po b bg ps pt l pu pv">query_metrics = F"""<br/>    SELECT<br/>        turno, state,<br/>        zone_code,<br/>       section_code,<br/><br/>        COUNT(*) AS total_votes,<br/>        COUNT( DISTINCT state || zone_code || section_code ) AS total_sections,<br/><br/>        SUM( vote_time ) AS voting_time_sum,<br/>        AVG( vote_time  ) AS average_voting_time,<br/><br/>        MAX( nr_of_votes )   AS total_ballot_items_voted,<br/>        SUM( nr_of_keys_pressed ) AS total_keys_pressed<br/><br/>    FROM<br/>        source<br/>    GROUP BY ROLLUP(turno, state, zone_code, section_code)<br/>"""</span></pre><p id="e6bc" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">As we need to compute the metrics in many levels of granularity, the ideal way to do this is with a GROUP BY + ROLLUP.</p><p id="7656" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In this case, DuckDB stood out significantly: we started with 15 GB and, after 36 seconds, the file was reduced to 88 MB!</p><p id="f1c1" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">This is a blazing fast performance, it grouped more than 200 million rows in 4 different levels of granularity, where the highest level has cardinality=2 and, the lowest, cardinality=~200,000 in less than a minute!</p><h1 id="3a80" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Results</h1><p id="9d94" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The table below summarizes the results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/7425a6bf2d7c1611cba5e2da4e673e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRZbOshwOzcs6A7uNb1Ssw.png"/></div></div></figure><p id="f9ca" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">The total pipeline’s execution time was ~2h30min, executed on WSL with the following specs: ~16GB of DDR4 RAM, an Intel 12th generation Core i7 processor, and a 1TB NVMe SSD.</p><p id="4bbd" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">During the process, I noticed that memory usage was a bottleneck, as DuckDB constantly created temporary files in the disk in a .temp/ directory. Also, I had plenty of problems in running queries with Windows functions: they not only took more time than expected to execute, but also the program randomly crashed several times.</p><p id="ba82" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Despite that, I believe that the performance reached was satisfactory, after all, we’re talking about 1/2Tb of data being processed with complex queries by just one single machine (that’s not so strong, compared with clusters of computers).</p><h1 id="9195" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion</h1><p id="c03e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The fact is that processing data is, sometimes, like refining uranium. We start with an enormous mass of raw material and, through a hard, time-consuming, and costly process (that, sometimes, puts lives at risk), we extract a small portion of the relevant refined information.</p><p id="3b5e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Jokes aside, in my posts, I’ve explored many ways to perform data processing, talking about tools, techniques, data architectures… always looking for the best way of doing things. This kind of knowledge is important, as it helps us choose the right tool for the right job. The goal of this post was exactly to know what kind of job DuckDB solves, and what experience it serves.</p><p id="dfbe" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">And, in general terms, it was a good experience.</p><p id="f526" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Working with this database was very smooth, I didn’t have to configure practically anything, just imported and manipulated the data with plain-old SQL statements. In other words, the tool has an almost zero initial entry barrier for those who already know SQL and a little bit of Python. In my opinion, this was DuckDB’s big victory. It not only empowered my machine with the ability to process 450Gb of data but this was achieved with a low adaptation cost for the environment (and the programmer).</p><p id="f61c" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">In terms of processing speed, considering the complexity of the project, the volume of 450Gb, and the fact that I didn’t optimize the database parameters, 2h30m was a good result. Especially thinking that, without this tool, it would be impossible, or extremely complex, to realize this task on my computer.</p><p id="6545" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">DuckDB is somewhat between Pandas and Spark. For small volumes of data, Pandas can be more attractive in terms of usability, especially for folks with some background in programming, as the package has many built-in transformations that could be tricky to implement in SQL. It also has seamless integration with many other Python packages, including DuckDB. For enormous volumes of data, Spark will probably be a better alternative, with the parallelism, clusters, and all that stuff. So, DuckDB fills a blind spot of medium-to-not-so-large projects, where using pandas would be impossible and Spark, overkill.</p><p id="8210" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">DuckDB extends the limits that a single machine can reach and expands the projects that can be developed locally, bringing speed to the analysis/manipulation of large volumes of data. Without a doubt, it is a powerful tool that I will proudly add to my toolbox.</p><p id="b04f" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Furthermore, I hope this post helped you get a better view of DuckDB. As always, I’m not an expert in any of the subjects addressed in this post, and I strongly recommend further reading, my references are listed below and the code is available on GitHub.</p><p id="8f71" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Thank you for reading! ;)</p><h1 id="794a" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">References</h1><blockquote class="pk pl pm"><p id="3c34" class="ny nz ph oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="fq">All the code is available in </em><a class="af pi" href="https://github.com/jaumpedro214/urna-logs-data-eng" rel="noopener ugc nofollow" target="_blank"><em class="fq">this GitHub repository</em></a><em class="fq">.<br/>Interested in more works like this one? Visit my </em><a class="af pi" href="https://github.com/jaumpedro214/posts" rel="noopener ugc nofollow" target="_blank"><em class="fq">posts repository</em></a><em class="fq">.</em></p></blockquote><p id="166a" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">[1] <em class="ph">2022 Results —Files transmitted for totalization— TSE Open Data Portal</em>. <a class="af pi" href="https://dadosabertos.tse.jus.br/dataset/resultados-2022-arquivos-transmitidos-para-totalizacao" rel="noopener ugc nofollow" target="_blank">Link</a>. [<a class="af pi" href="https://opendatacommons.org/licenses/odbl/" rel="noopener ugc nofollow" target="_blank">ODbL</a>]<br/>[2] Databricks. (2023, June 29). <a class="af pi" href="https://www.youtube.com/watch?v=GaHWuQ_cBhA" rel="noopener ugc nofollow" target="_blank"><em class="ph">Data + AI Summit Keynote, Thursday Part 5 — DuckDB</em></a>. YouTube. <br/>[3]<a class="af pi" href="https://duckdb.org/docs/" rel="noopener ugc nofollow" target="_blank"><em class="ph">DuckDB Official</em> <em class="ph">Documentation</em></a>. DuckDB. <br/>[4]<em class="ph"> </em><a class="af pi" href="https://international.tse.jus.br/en/electronic-ballot-box/presentation" rel="noopener ugc nofollow" target="_blank">The electronic ballot box</a>. <em class="ph">Superior Electoral Court</em>.<br/>[5] Wikipedia contributors. (2023, July 25). <a class="af pi" href="https://en.wikipedia.org/wiki/OLAP_cube" rel="noopener ugc nofollow" target="_blank"><em class="ph">OLAP cube</em></a>. Wikipedia.<br/>[6] Duckdb — GitHub. <a class="af pi" href="https://github.com/duckdb/duckdb/issues/7809" rel="noopener ugc nofollow" target="_blank"><em class="ph">window performance · Issue #7809 · duckdb/duckdb</em></a>.<br/>[7] Gunnarmorling. <em class="ph">GitHub — gunnarmorling/1brc: </em><a class="af pi" href="https://github.com/gunnarmorling/1brc" rel="noopener ugc nofollow" target="_blank"><em class="ph">1️⃣🐝🏎️ The One Billion Row Challenge</em></a><em class="ph"> — A fun exploration of how quickly 1B rows from a text file can be aggregated with Java</em>.</p></div></div></div></div>    
</body>
</html>