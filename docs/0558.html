<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Improving LLM Inference Speeds on CPUs with Model Quantization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Improving LLM Inference Speeds on CPUs with Model Quantization</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29">https://towardsdatascience.com/improving-llm-inference-latency-on-cpus-with-model-quantization-28aefb495657?source=collection_archive---------1-----------------------#2024-02-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/ba70dd27842dfef63454e83252c90ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iS3mFlEoUWT3UT6E0RzK5Q.jpeg"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Image Property of Author — Create with Nightcafe</figcaption></figure><div/><div><h2 id="50f2" class="pw-subtitle-paragraph hh gj gk bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Discover how to significantly improve inference latency on CPUs using quantization techniques for bf16, int8, and int4 precisions</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Eduardo Alvarez" class="l ep by dd de cx" src="../Images/8a51c754fdd3362aa82dee5acd2a68c5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QseVWgYrBJ1R4Ych9FEaQw.png"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://eduand-alvarez.medium.com/?source=post_page---byline--28aefb495657--------------------------------" rel="noopener follow">Eduardo Alvarez</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--28aefb495657--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">2</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="9446" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One of the most significant challenges the AI space faces is the need for computing resources to host large-scale production-grade LLM-based applications. At scale, LLM applications require redundancy, scalability, and reliability, which have historically been only possible on general computing platforms like CPUs. Still, the prevailing narrative today is that CPUs cannot handle LLM inference at latencies comparable with high-end GPUs.</p><p id="0889" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One open-source tool in the ecosystem that can help address inference latency challenges on CPUs is the <a class="af ny" href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html" rel="noopener ugc nofollow" target="_blank">Intel® Extension for PyTorch*</a> (IPEX), which provides up-to-date feature optimizations for an extra performance boost on Intel hardware. IPEX delivers a variety of easy-to-implement optimizations that make use of hardware-level instructions. This tutorial will dive into the theory of model compression and the out-of-the-box model compression techniques IPEX provides. These compression techniques directly impact LLM inference performance on general computing platforms, like Intel 4th and 5th-generation CPUs.</p><h1 id="b1f1" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Inference Latency in Application Development</h1><p id="17ff" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Second only to application safety and security, inference latency is one of the most critical parameters of an AI application in production. Regarding LLM-based applications, latency or throughput is often measured in tokens/second. As illustrated in the simplified inference processing sequence below, tokens are processed by the language model and then de-tokenized into natural language.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pa"><img src="../Images/b2d4ae537a79c711aded3a1a056f8f26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIf08bgcZG0K9mw3Yh2c5g.gif"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">GIF 1. of inference processing sequence — Image by Author</figcaption></figure><p id="19dd" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Interpreting inference this way can sometimes lead us astray because we analyze this component of AI applications in abstraction of the traditional production software paradigm. Yes, AI apps have their nuances, but at the end of the day, we are still talking about transactions per unit of time. If we start to think about inference as a transaction, like any other, from an application design point of view, the problem becomes less complex. For example, let’s say we have a chat application that has the following requirements:</p><ul class=""><li id="870f" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pg ph pi bk">Average of <strong class="ne gl">300 user sessions</strong> per hour</li><li id="fa9f" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Average of <strong class="ne gl">5 transactions</strong> (LLM inference requests) per user per session</li><li id="1155" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Average <strong class="ne gl">100 tokens</strong> generated per transaction</li><li id="392b" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Each session has an average of <strong class="ne gl">10,000ms (10s) overhead</strong> for user authentication, guardrailing, network latency, and pre/post-processing.</li><li id="5334" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Users take an average of <strong class="ne gl">30,000ms (30s) to respond</strong> when actively engaged with the chatbot.</li><li id="48be" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">The average total active <strong class="ne gl">session time goal is 3</strong> minutes or less.</li></ul><p id="938f" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below, you can see that with some simple napkin math, we can get some approximate calculations for the required latency of our LLM inference engine.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp po"><img src="../Images/e81ec3c2fcdaa2f85ed293707e29dd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zp9qM6qBFosRLCQG5-de9Q.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Figure 1. A simple equation to calculate the required transaction and token latency based on various application requirements. — Image by Author</figcaption></figure><p id="280b" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Achieving required latency thresholds in production is a challenge, especially if you need to do it without incurring additional compute infrastructure costs. In the remainder of this article, we will explore one way that we can significantly improve inference latency through model compression.</p><h1 id="5248" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Model Compression</h1><p id="1fa6" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Model compression is a loaded term because it addresses a variety of techniques, such as model quantization, distillation, pruning, and more. At their core, the chief aim of these techniques is to reduce the computational complexity of neural networks.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pa"><img src="../Images/a063d0f1575e7dbbb20b419bc28230c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jq0mIHkHT0LKAWbZN7ELoQ.gif"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">GIF 2. Illustration of inference processing sequence — Image by Author</figcaption></figure><p id="6010" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The method we will focus on today is model quantization, which involves reducing the byte precision of the weights and, at times, the activations, reducing the computational load of matrix operations and the memory burden of moving around larger, higher precision values. The figure below illustrates the process of quantifying fp32 weights to int8.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp pp"><img src="../Images/82cd0e972f9a9a49ad1b4eff1cc721db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cv0izqxOGSQT6-bZS72bSA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Fig 2. Visual representation of model quantization going from full precision at FP32 down to quarter precision at INT8, theoretically reducing the model complexity by a factor of 4. — Image by Author</figcaption></figure><p id="6748" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is worth mentioning that the reduction of complexity by a factor of 4 that results from quantizing from fp32 (full precision) to int8 (quarter precision) does not result in a 4x latency reduction during inference because inference latency involves more factors beyond just model-centric properties.</p><p id="43eb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Like with many things, there is no one-size-fits-all approach, and in this article, we will explore three of my favorite techniques for quantizing models using IPEX:</p><h2 id="2d4b" class="pq oa gk bf ob pr ps pt oe pu pv pw oh nl px py pz np qa qb qc nt qd qe qf qg bk"><strong class="al">Inference at bf16 or fp32</strong></h2><p id="eade" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">This technique quantizes the weights in the neural network down to a user defined precision. This technique is ideal for smaller models, like the &lt;1B LLMs of the world.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/b8ad85dbaf7f150a475f67a5031c129d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bB3fjRwe5lDPkv5UD3qLGw.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Fig 3. Simple illustration of bf16/fp32, showing FP32 weights in orange and half-precision quantized bf16 weights in green. — Image by Author</figcaption></figure><p id="2774" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The implementation is quite straightforward: using hugging face transformers, a model can be loaded into memory and optimized using the IPEX llm-specific optimization function <code class="cx qi qj qk ql b">ipex.llm.optimize(model, dtype=dtype)</code> by setting <code class="cx qi qj qk ql b">dtype = torch.bfloat16,</code> we can activate the half-prevision inference capability, which improves the inference latency over full-precision (fp32) and stock.</p><pre class="pb pc pd pe pf qm ql qn bp qo bb bk"><span id="d120" class="qp oa gk ql b bg qq qr l qs qt">import sys<br/>import os<br/>import torch<br/>import intel_extension_for_pytorch as ipex<br/>from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline<br/><br/># PART 1: Model and tokenizer loading using transformers<br/>tokenizer = AutoTokenizer.from_pretrained("Intel/neural-chat-7b-v3-3")<br/>model = AutoModelForCausalLM.from_pretrained("Intel/neural-chat-7b-v3-3")<br/><br/># PART 2: Use IPEX to optimize the model<br/>#dtype = torch.float # use for full precision FP32<br/>dtype = torch.bfloat16 # use for half precision inference<br/>model = ipex.llm.optimize(model, dtype=dtype)<br/><br/># PART 3: Create a hugging face inference pipeline and generate results<br/>pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)<br/>st = time.time()<br/>results = pipe("A fisherman at sea...",  max_length=250)<br/>end = time.time()<br/>generation_latency = end-st<br/><br/>print('generation latency: ', generation_latency)<br/>print(results[0]['generated_text'])</span></pre><p id="15f8" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Of the three compression techniques we will explore, this is the easiest to implement (measured by unique lines of code) and offers the smallest net improvement over a non-quantized baseline.</p><h2 id="0cb6" class="pq oa gk bf ob pr ps pt oe pu pv pw oh nl px py pz np qa qb qc nt qd qe qf qg bk"><strong class="al">SmoothQuant (int8)</strong></h2><p id="b0cf" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">This technique addresses the core challenges of quantizing LLMs, which include handling large-magnitude outliers in activation channels across all layers and tokens, a common issue that traditional quantization techniques struggle to manage effectively. This technique employs a joint mathematical transformation on both weights and activations within the model. The transformation strategically reduces the disparity between outlier and non-outlier values for activations, albeit at the cost of increasing this ratio for weights. This adjustment renders the Transformer layers “quantization-friendly,” enabling the successful application of int8 quantization without degrading model quality.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/c723f913c23b0e8577171ad521f992c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pVoW8GbSvsfQ0JwcuBhDTA.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Fig 4. Simple illustration of SmoothQuant showing weights as circles and activations as triangles. The diagram depicts the two main steps: (1) the application of scaler for smoothing and (2) the quantization to int8 — Image by Author</figcaption></figure><p id="7166" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below, you’ll find a simple SmoothQuant implementation — omitting the code for creating the DataLoader, which is a common and well-documented PyTorch principle. SmoothQuant is an accuracy-aware post-training quantization recipe, meaning that by providing a calibration dataset and model you will be able to provide a baseline and limit the language modeling degradation. The calibration model generates a quantization configuration, which is then passed to <code class="cx qi qj qk ql b">ipex.llm.optimize()</code> along with the SmoothQuant mapping. Upon execution, the SmoothQuant is applied, and the model can be tested using the <code class="cx qi qj qk ql b">.generate()</code> method.</p><pre class="pb pc pd pe pf qm ql qn bp qo bb bk"><span id="35cc" class="qp oa gk ql b bg qq qr l qs qt">import torch<br/>import intel_extension_for_pytorch as ipex<br/>from intel_extension_for_pytorch.quantization import prepare<br/>import transformers<br/><br/># PART 1: Load model and tokenizer from Hugging Face + Load SmoothQuant config mapping<br/>tokenizer = AutoTokenizer.from_pretrained("Intel/neural-chat-7b-v3-3")<br/>model = AutoModelForCausalLM.from_pretrained("Intel/neural-chat-7b-v3-3")<br/>qconfig = ipex.quantization.get_smooth_quant_qconfig_mapping()<br/><br/># PART 2: Configure calibration<br/># prepare your calibration dataset samples<br/>calib_dataset = DataLoader({Your dataloader parameters})<br/>example_inputs = # provide a sample input from your calib_dataset<br/>calibration_model = ipex.llm.optimize(<br/>  model.eval(),<br/>  quantization_config=qconfig,<br/>)<br/>prepared_model = prepare(<br/>  calibration_model.eval(), qconfig, example_inputs=example_inputs<br/>)<br/>with torch.no_grad():<br/>  for calib_samples in enumerate(calib_dataset):<br/>    prepared_model(calib_samples)<br/>prepared_model.save_qconf_summary(qconf_summary=qconfig_summary_file_path)<br/><br/># PART 3: Model Quantization using SmoothQuant<br/>model = ipex.llm.optimize(<br/>  model.eval(),<br/>  quantization_config=qconfig,<br/>  qconfig_summary_file=qconfig_summary_file_path,<br/>)<br/><br/># generation inference loop<br/>with torch.inference_mode():<br/>    model.generate({your generate parameters})</span></pre><p id="9f6b" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">SmoothQuant is a powerful model compression technique and helps significantly improve inference latency over full-precision models. Still, it requires a little upfront work to prepare a calibration dataset and model.</p><h2 id="f798" class="pq oa gk bf ob pr ps pt oe pu pv pw oh nl px py pz np qa qb qc nt qd qe qf qg bk"><strong class="al">Weight-Only Quantization (int8 and int4)</strong></h2><p id="553f" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">Compared to traditional int8 quantization applied to both activation and weight, weight-only quantization (WOQ) offers a better balance between performance and accuracy. It is worth noting that int4 WOQ requires dequantizing to bf16/fp16 before computation (Figure 4), which introduces an overhead in compute. A basic WOQ technique, tensor-wise asymmetric Round To Nearest (RTN) quantization, presents challenges and often leads to reduced accuracy (<a class="af ny" href="https://huggingface.co/blog/intel-starcoder-quantization" rel="noopener ugc nofollow" target="_blank">source</a>). However, literature (Zhewei Yao, 2022) suggests that groupwise quantizing the model’s weights helps maintain accuracy. Since the weights are only dequantized for computation, a significant memory advantage remains despite this extra step.</p><figure class="pb pc pd pe pf fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp qh"><img src="../Images/159691607f7555e9159295d849f92e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEmqizumc4X5C0UhtsD1hg.png"/></div></div><figcaption class="gd ge gf fo fp gg gh bf b bg z dx">Fig 5. Simple illustration of weight-only quantization, with pre-quantized weights in orange and the quantized weights in green. Note that this depicts the initial quantization to int4/int8 and dequantization to fp16/bf16 for the computation step. — Image by Author</figcaption></figure><p id="0aee" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The WOQ implementation below showcases the few lines of code required to quantize a model from Hugging Face with this technique. As with the previous implementations, we start by loading a model and tokenizer from Hugging Face. We can use the <code class="cx qi qj qk ql b">get_weight_only_quant_qconfig_mapping()</code> method to configure the WOQ recipe. The recipe is then passed to the <code class="cx qi qj qk ql b">ipex.llm.optimize()</code> function along with the model for optimization and quantization. The quantized model can then be used for inference with the <code class="cx qi qj qk ql b">.generate()</code> method.</p><pre class="pb pc pd pe pf qm ql qn bp qo bb bk"><span id="43a5" class="qp oa gk ql b bg qq qr l qs qt"><br/># requirements<br/>#intel-extension-for-pytorch==2.2<br/>#transformers==4.35.2<br/>#torch==2.2.0 <br/><br/>import torch<br/>import intel_extension_for_pytorch as ipex<br/>from transformers import AutoTokenizer, AutoModelForCausalLM<br/><br/># PART 1: Model and tokenizer loading<br/>tokenizer = AutoTokenizer.from_pretrained("Intel/neural-chat-7b-v3-3")<br/>model = AutoModelForCausalLM.from_pretrained("Intel/neural-chat-7b-v3-3")<br/><br/># PART 2: Preparation of quantization config<br/>qconfig = ipex.quantization.get_weight_only_quant_qconfig_mapping(<br/>  weight_dtype=torch.qint8, # or torch.quint4x2<br/>  lowp_mode=ipex.quantization.WoqLowpMode.NONE, # or FP16, BF16, INT8<br/>)<br/>checkpoint = None # optionally load int4 or int8 checkpoint<br/><br/># PART 3: Model optimization and quantization<br/>model = ipex.llm.optimize(model, quantization_config=qconfig, low_precision_checkpoint=checkpoint)<br/><br/>inputs = tokenizer("I love learning to code...", return_tensors="pt").input_ids<br/><br/># PART 4: Inference output generation<br/>with torch.inference_mode():<br/>    tokens = model.generate(<br/>        inputs,<br/>        max_new_tokens=64,<br/>)<br/><br/>print(tokenizer.decode(tokens[0], skip_special_tokens=True))</span></pre><p id="e597" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, WOQ provides a powerful way to compress models down to a fraction of their original size with limited impact on language modeling capabilities.</p><h1 id="6266" class="nz oa gk bf ob oc od hk oe of og hn oh oi oj ok ol om on oo op oq or os ot ou bk">Conclusion and Discussion</h1><p id="6583" class="pw-post-body-paragraph nc nd gk ne b hi ov ng nh hl ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fj bk">As an engineer at Intel, I’ve worked closely with the IPEX engineering team at Intel. This has afforded me a unique insight into its advantages and development roadmap, making IPEX a preferred tool. However, for developers seeking simplicity without the need to manage an extra dependency, PyTorch offers <a class="af ny" href="https://pytorch.org/docs/stable/quantization.html" rel="noopener ugc nofollow" target="_blank">three quantization recipes</a>: Eager Mode, FX Graph Mode (under maintenance), and PyTorch 2 Export Quantization, providing strong, less specialized alternatives.</p><p id="bebb" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">No matter what technique you choose, model compression techniques will result in some degree of language modeling performance loss, albeit in &lt;1% in many cases. For this reason, it’s essential to evaluate the application’s fault tolerance and establish a baseline for model performance at full (FP32) and/or half-precision (BF16/FP16) before pursuing quantization.</p><p id="7d50" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In applications that leverage some degree of in-context learning, like Retrieval Augmented Generation (RAG), model compression might be an excellent choice. In these cases, the mission-critical knowledge is spoon-fed to the model at the time of inference, so the risk is heavily reduced even with low-fault-tolerant applications.</p><p id="6566" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Quantization is an excellent way to address LLM inference latency concerns without upgrading or expanding compute infrastructure. It is worth exploring regardless of your use case, and IPEX provides a good option to start with just a few lines of code.</p><p id="0d5b" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl">A few exciting things to try would be:</strong></p><ul class=""><li id="e42b" class="nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pg ph pi bk">Test the sample code in this tutorial on the Intel Developer Cloud’s <a class="af ny" href="https://console.idcservice.net/training" rel="noopener ugc nofollow" target="_blank">free Jupyter Environment</a>.</li><li id="5526" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Take an existing model that you’re running on an accelerator at complete precision and test it out on a CPU at int4/int8</li><li id="7583" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk">Explore all three techniques and determine which works best for your use case. Make sure to compare the loss of language modeling performance, not just latency.</li><li id="e2bb" class="nc nd gk ne b hi pj ng nh hl pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx pg ph pi bk"><a class="af ny" href="https://github.com/intel/ai-innovation-bridge/blob/master/workshops/ai-workloads-with-huggingface/6%20-%20Uploading%20and%20Sharing%20Models%20on%20Hugging%20Face%20Hub%20with%20Intel%20Optimizations.ipynb" rel="noopener ugc nofollow" target="_blank">Upload your quantized model to the Hugging Face Model Hub</a>! If you do, let me know — I’d love to check it out!</li></ul><p id="eba3" class="pw-post-body-paragraph nc nd gk ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne gl"><em class="qu">Thank you for reading! Don’t forget to follow </em></strong><a class="af ny" href="https://eduand-alvarez.medium.com/" rel="noopener"><strong class="ne gl"><em class="qu">my profile for more articles</em></strong></a><strong class="ne gl"><em class="qu"> like this!</em></strong></p></div></div></div></div>    
</body>
</html>