<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Exploring Object Detection with R-CNN Models — A Comprehensive Beginner’s Guide (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Exploring Object Detection with R-CNN Models — A Comprehensive Beginner’s Guide (Part 2)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17">https://towardsdatascience.com/exploring-object-detection-with-r-cnn-models-a-comprehensive-beginners-guide-part-2-685bc89775e2?source=collection_archive---------5-----------------------#2024-02-17</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="5214" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">Object detection models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Raghav Bali" class="l ep by dd de cx" src="../Images/49fea68f38f59d0bc39dab484b55684f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6nRZK0-KCmkqu5I3auzK3w.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@Rghv_Bali?source=post_page---byline--685bc89775e2--------------------------------" rel="noopener follow">Raghav Bali</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--685bc89775e2--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn le lf ab q ee lg lh" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="li"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ih ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ih lv lw lh lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ih lv lw lh lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ih lv lw lh lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mm bh"><figure class="mn mo mp mq mr mm bh paragraph-image"><img src="../Images/77bfdeb62b64a50add93691e90b907d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/0*iS8U_4PDeGERdC7A"/><figcaption class="mt mu mv mw mx my mz bf b bg z dx">Photo by <a class="af na" href="https://unsplash.com/@datalore?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">liam siegel</a> on <a class="af na" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="53ee" class="nb nc fr bf nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Object Detection Models</h2><p id="0147" class="pw-post-body-paragraph nz oa fr ob b gp oc od oe gs of og oh nm oi oj ok nq ol om on nu oo op oq or fk bk">Object detection is an involved process which helps in localization and classification of objects in a given image. In <a class="af na" rel="noopener" target="_blank" href="/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78">part 1</a>, we developed an understanding of the basic concepts and the general framework for object detection. In this article, we will briefly cover a number of important object detection models with a focus on understanding their key contributions.</p><p id="0419" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The general object detection framework highlights the fact that there are a few interim steps to perform object detection. Building on the same thought process, researchers have come up with a number of innovative architectures which solve this task of object detection. One of the ways of segregating such models is in the way they tackle the given task. Object detection models which leverage multiple models and/or steps to solve this task as called as multi-stage object detectors. The Region based CNN (RCNN) family of models are a prime example of <strong class="ob fs">multi-stage object detectors</strong>. Subsequently, a number of improvements led to model architectures that solve this task using a single model itself. Such models are called as <strong class="ob fs">single-stage object detectors</strong>. We will cover single-stage models in a subsequent article. For now, let us now have a look under the hood for some of these multi-stage object detectors.</p><h2 id="e21c" class="nb nc fr bf nd ne nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Region Based Convolutional Neural Networks</h2><p id="2b3c" class="pw-post-body-paragraph nz oa fr ob b gp oc od oe gs of og oh nm oi oj ok nq ol om on nu oo op oq or fk bk">Region based Convolutional Neural Networks (R-CNNs) were initially presented by Girshick et. al. in their paper titled “<a class="af na" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank">Rich feature hierarchies for accurate object detection and semantic segmentation</a>” in 2013. R-CNN is a multi-stage object detection models which became the starting point for faster and more sophisticated variants in following years. Let’s get started with this base idea before we understand the improvements achieved through <strong class="ob fs">Fast R-CNN</strong> and <strong class="ob fs">Faster R-CNN</strong> models.</p><p id="9e95" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The R-CNN model is made up of four main components:</p><ul class=""><li id="1e23" class="nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or ox oy oz bk"><strong class="ob fs">Region Proposal</strong>: The extraction of regions of interest is the first and foremost step in this pipeline. The R-CNN model makes use of an algorithm called Selective Search for region proposal. Selective Search is a greedy search algorithm proposed by <a class="af na" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf" rel="noopener ugc nofollow" target="_blank">Uijlings et. al</a>. in 2012. Without going into too many details, selective search makes use of a bottoms-up multi-scale iterative approach to identify ROIs. In every iteration the algorithm groups similar regions until the whole image is a single region. Similarity between regions is calculated based on color, texture, brightness etc. Selective search generates a lot of false positive (background) ROIs but has a high recall. The list of ROIs is passed onto the next step for processing.</li><li id="2f7c" class="nz oa fr ob b gp pa od oe gs pb og oh nm pc oj ok nq pd om on nu pe op oq or ox oy oz bk"><strong class="ob fs">Feature Extraction</strong>: The R-CNN network makes use of pre-trained CNNs such as VGGs or ResNets for extracting features from each of the ROIs identified in the previous step. Before the regions/crops are passed as inputs to the pre-trained network these are reshaped or warped to the required dimensions (each pretrained network requires inputs in specific dimensions only). The pre-trained network is used without the final classification layer. The output of this stage is a long list of tensors, one for each ROI from the previous stage.</li><li id="7dd4" class="nz oa fr ob b gp pa od oe gs pb og oh nm pc oj ok nq pd om on nu pe op oq or ox oy oz bk"><strong class="ob fs">Classification Head</strong>: The original R-CNN paper made use of Support Vector Machines (SVMs) as the classifier to identify the class of object in the ROI. SVM is a traditional supervised algorithm widely used for classification purposes. The output from this step is a classification label for every ROI.</li><li id="728e" class="nz oa fr ob b gp pa od oe gs pb og oh nm pc oj ok nq pd om on nu pe op oq or ox oy oz bk"><strong class="ob fs">Regression Head</strong>: This module takes care of the localization aspect of the object detection task. As discussed in the previous section, bounding boxes can be uniquely identified using 4 coordinates (top-left (x, y) coordinates along with width and height of the box). The regressor outputs these 4 values for every ROI.</li></ul><p id="513b" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">This pipeline is visually depicted in figure 1 for reference. As shown in the figure, the network requires multiple independent forward passes (one of each ROI) using the pretrained network. This is one of the primary reasons which slows down the R-CNN model, both for training as well as inference. The authors of the paper mention that it requires 80+ hours to train the network and an immense amount of disk space. The second bottleneck is the selective search algorithm itself.</p><figure class="mn mo mp mq mr mm mw mx paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mw mx pf"><img src="../Images/bc1576d845aac3e157ff3d86511e95fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_bYL-bCFWVilXe0DEViGmw.png"/></div></div><figcaption class="mt mu mv mw mx my mz bf b bg z dx">Figure 1: Components of the R-CNN model. Region proposal component is based on selective search followed by a pre-trained network such as VGG for feature extraction. Classification head makes use of SVMs and a separate regression head. Source: Author</figcaption></figure><p id="2a01" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The R-CNN model is a good example of how different ideas can be leveraged as building blocks to solve a complex problem. While we will have a detailed hands-on exercise to see object detection in context of transfer learning, in its original setup itself R-CNN makes use of transfer learning.</p><p id="19a7" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The R-CNN model was slow, but it provided a good base for object detection models to come down the line. The computationally expensive and slow feature extraction step was mainly addressed in the <strong class="ob fs">Fast R-CNN</strong> implementation. The <a class="af na" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">Fast R-CNN</a> was presented by Ross Grishick in 2015. This implementation boasts of not just faster training and inference but also improved mAP on <a class="af na" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/" rel="noopener ugc nofollow" target="_blank">PASCAL VOC 2012</a> dataset.</p><p id="76d1" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The key contributions from the <strong class="ob fs">Fast R-CNN</strong> paper can be summarized as follows:</p><ul class=""><li id="9827" class="nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or ox oy oz bk"><strong class="ob fs">Region Proposal</strong>: For the base R-CNN model, we discussed how selective search algorithm is applied on the input image to generate thousands of ROIs upon which a pretrained network works to extract features. The Fast R-CNN changes this step to derive maximum impact. Instead of applying the feature extraction step using the pretrained network thousands of times, the Fast R-CNN network does it only once. In other words, we first process the whole input image through the pretrained network just once. The output features are then used as input for the selective search algorithm for identification of ROIs. This change in order of components reduces the computation requirements and performance bottleneck to a good extent.</li><li id="98c0" class="nz oa fr ob b gp pa od oe gs pb og oh nm pc oj ok nq pd om on nu pe op oq or ox oy oz bk"><strong class="ob fs">ROI Pooling Layer</strong>: The ROIs identified in the previous step can be arbitrary size (as identified by the selective search algorithm). But the fully connected layers after the ROIs have been extracted take only fixed size feature maps as inputs. The ROI pooling layer is thus a fixed size filter (the paper mentions a size of 7x7) which helps transform these arbitrary sized ROIs into fixed size output vectors. This layer works by first dividing the ROI into equal sized sections. It then finds the largest value in each section (similar to Max-Pooling operation). The output is just the max values from each of equal sized sections. The ROI pooling layer speeds up the inference and training times considerably.</li><li id="a13c" class="nz oa fr ob b gp pa od oe gs pb og oh nm pc oj ok nq pd om on nu pe op oq or ox oy oz bk"><strong class="ob fs">Multi-task Loss</strong>: As opposed to two different components (SVM and bounding box regressor) in R-CNN implementation, Faster R-CNN makes use of a multi-headed network. This setup enables the network to be trained jointly for both the tasks using a multi-task loss function. The multi-task loss is a weighted sum of classification and regression losses for object classification and bounding box regression tasks respectively. The loss function is given as:</li></ul><blockquote class="pk"><p id="65c7" class="pl pm fr bf pn po pp pq pr ps pt or dx">Lₘₜ = Lₒ + 𝛾Lᵣ</p></blockquote><p id="a6a3" class="pw-post-body-paragraph nz oa fr ob b gp pu od oe gs pv og oh nm pw oj ok nq px om on nu py op oq or fk bk">where 𝛾 ≥ 1 if the ROI contains an object (objectness score), 0 otherwise. Classification loss is simply a negative log loss while the regression loss used in the original implementation is the smooth L1 loss.</p><p id="2d69" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The original paper details a number of experiments which highlight performance improvements based on various combinations of hyper-parameters and layers fine-tuned in the pre-trained network. The original implementation made use of pretrained VGG-16 as the feature extraction network. A number of faster and improved implementation such as MobileNet, ResNet, etc. have come up since the Fast R-CNN’s original implementation. These networks can also be swapped in place of VGG-16 to improve the performance further.</p><p id="0dff" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk"><strong class="ob fs">Faster R-CNN</strong> is the final member of this family of multi-stage object detectors. This is by far the most complex and fastest variant of them all. While Fast R-CNN improved training and inference times considerably it was still getting penalized due to the selective search algorithm. The Faster R-CNN model presented in 2016 by Ren et. al. in their paper titled “<a class="af na" href="https://papers.nips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>” addresses the regional proposal aspect primarily. This network builds on top of Fast R-CNN network by introducing a novel component called <strong class="ob fs">Region Proposal Network (RPN)</strong>. The overall Faster R-CNN network is depicted in figure 2 for reference.</p><figure class="mn mo mp mq mr mm mw mx paragraph-image"><div role="button" tabindex="0" class="pg ph ed pi bh pj"><div class="mw mx pz"><img src="../Images/317b48c82e252b5db04dbe89c0ba9aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONcbBe4mtiEr6Mq4iMkZ8A.png"/></div></div><figcaption class="mt mu mv mw mx my mz bf b bg z dx">Figure 2: Faster R-CNN is composed of two main components: 1) a Region Proposal Network (RPN) to identify ROIs and 2) a Fast R-CNN like multi-headed network with ROI pooling layer. Source: Author</figcaption></figure><p id="86c0" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">RPN is a fully convolutional network (FCN) that helps in generating ROIs. As shown in figure 3.12, RPN consists of two layers only. The first being a 3x3 convolutional layer with 512 filters followed by two parallel 1x1 convolutional layers (one each for classification and regression respectively). The 3x3 convolutional filter is applied onto the feature map output of the pre-trained network (the input to which is the original image). Please note that the classification layer in RPN is a binary classification layer for determination of objectness score (not the object class). The bounding box regression is performed using 1x1 convolutional filters on anchor boxes. The proposed setup in the paper uses 9 anchor boxes per window, thus the RPN generates 18 objectness scores (2xK) and 36 location coordinates (4xK), where K=9 is the number of anchor boxes. The use of RPN (instead of selective search) improves the training and inference times by orders of magnitudes.</p><p id="9628" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">The Faster R-CNN network is an end-to-end object detection network. Unlike the base R-CNN and Fast R-CNN models which made use of a number of independent components for training, Faster R-CNN can be trained as a whole.</p><p id="1aed" class="pw-post-body-paragraph nz oa fr ob b gp os od oe gs ot og oh nm ou oj ok nq ov om on nu ow op oq or fk bk">This concludes our discussion on the R-CNN family of object detectors. We discussed key contributions to better understand how these networks work.</p></div></div></div></div>    
</body>
</html>