- en: 'Mistral AI vs. Meta: Comparing Top Open-source LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23](https://towardsdatascience.com/mistral-ai-vs-meta-comparing-top-open-source-llms-565c1bc1516e?source=collection_archive---------1-----------------------#2024-01-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comparison between Mistral 7B vs Llama 2 7B and Mixtral 8x7B vs Llama 2 70B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page---byline--565c1bc1516e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--565c1bc1516e--------------------------------)
    ·16 min read·Jan 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*This post was co-authored with Rafael Guedes.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Latest developments in Natural Language Processing and, particularly, in Large
    Language Models (LLMs) are focused on improving model performance, which often
    leads to an increase in model size. As one can expect, the escalation in model
    size also increases computational costs and inference latency, raising barriers
    when it comes to deploying and using LLMs in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI, a European company based in Paris, has been researching how to improve
    model performance and, at the same time, reduce the computational resources needed
    to deploy LLMs for practical use cases. Mistral 7B is the smallest LLM they created
    that brings two novel concepts to the traditional Transformer architecture, Group-Query
    Attention (GQA) and Sliding Window Attention (SWA). These components accelerate
    the inference speed and reduce memory requirements during decoding enabling a
    higher throughput and the ability to handle longer sequences of tokens without
    sacrificing the quality of the responses generated compared to Llama 2 7B in benchmark
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Mistral 7B is not the only model they have developed, they also created Mixtral
    8x7B to compete with larger LLMs like Llama 2 70B. Apart from using GQA and SWA,
    this version also adds a third…
  prefs: []
  type: TYPE_NORMAL
