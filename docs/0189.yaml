- en: 'A Weekend AI Project: Running Speech Recognition and a LLaMA-2 GPT on a Raspberry
    Pi'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-weekend-ai-project-running-speech-recognition-and-a-llama-2-gpt-on-a-raspberry-pi-5298d6edf812?source=collection_archive---------1-----------------------#2024-01-20](https://towardsdatascience.com/a-weekend-ai-project-running-speech-recognition-and-a-llama-2-gpt-on-a-raspberry-pi-5298d6edf812?source=collection_archive---------1-----------------------#2024-01-20)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A fully offline use of Whisper ASR and LLaMA-2 GPT Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--5298d6edf812--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--5298d6edf812--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5298d6edf812--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5298d6edf812--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--5298d6edf812--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5298d6edf812--------------------------------)
    ·10 min read·Jan 20, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa256df4511c3a0516ebc6d3475467d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Raspberry Pi running a LLaMA model, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, nobody will be surprised by running a deep learning model in the cloud.
    But the situation can be much more complicated in the edge or consumer device
    world. There are several reasons for that. First, the use of cloud APIs requires
    devices to always be online. This is not a problem for a web service but can be
    a dealbreaker for the device that needs to be functional without Internet access.
    Second, cloud APIs cost money, and customers likely will not be happy to pay yet
    another subscription fee. Last but not least, after several years, the project
    may be finished, API endpoints will be shut down, and the expensive hardware will
    turn into a brick. Which is naturally not friendly for customers, the ecosystem,
    and the environment. That’s why I am convinced that the end-user hardware should
    be fully functional offline, without extra costs or using the online APIs (well,
    it can be optional but not mandatory).
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will show how to run a LLaMA GPT model and automatic speech
    recognition (ASR) on a Raspberry Pi. That will allow us to ask Raspberry Pi questions
    and get answers. And as promised, all this will work fully offline.
  prefs: []
  type: TYPE_NORMAL
