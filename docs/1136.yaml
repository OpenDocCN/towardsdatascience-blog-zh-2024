- en: How does temperature impact next token prediction in LLMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-does-temperature-impact-next-token-prediction-in-llms-779bd908f2cf?source=collection_archive---------1-----------------------#2024-05-06](https://towardsdatascience.com/how-does-temperature-impact-next-token-prediction-in-llms-779bd908f2cf?source=collection_archive---------1-----------------------#2024-05-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://ankur-m.medium.com/?source=post_page---byline--779bd908f2cf--------------------------------)[![Ankur
    Manikandan](../Images/3b84353a1979a484a46ee443c0a5bfb6.png)](https://ankur-m.medium.com/?source=post_page---byline--779bd908f2cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--779bd908f2cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--779bd908f2cf--------------------------------)
    [Ankur Manikandan](https://ankur-m.medium.com/?source=post_page---byline--779bd908f2cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--779bd908f2cf--------------------------------)
    ·4 min read·May 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**TLDR**'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. At a temperature of 1, the probability values are the same as those derived
    from the standard softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Raising the temperature inflates the probabilities of the less likely tokens,
    thereby broadening the range of potential candidates (or diversity) for the model’s
    next token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Lowering the temperature, on the other hand, makes the probability of the
    most likely token approach 1.0, boosting the model’s confidence. Decreasing the
    temperature effectively eliminates the uncertainty within the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Google colab notebook**](https://colab.research.google.com/drive/1G6XZ_0DsHTZQBppjlgVM-ICdR6yz4g7m?usp=sharing)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction**'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are versatile generative models suited for a wide
    array of tasks. They can produce consistent, repeatable outputs or generate creative
    content by placing unlikely words together. The “temperature” setting allows users
    to fine-tune the model’s output, controlling the degree of predictability.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a hypothetical example to understand the impact of temperature on
    the next token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We asked an LLM to complete the sentence, **“This is a wonderful _____.”**
    Let’s assume the potential candidate tokens are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The logits are passed through a softmax function so that the sum of the values
    is equal to one. Essentially, the softmax function generates probability estimates
    for each token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d581b23a99b01e18aef8ec99d1dabb2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Standard softmax function
  prefs: []
  type: TYPE_NORMAL
- en: Let’s calculate the probability estimates in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ed3c4b8677cc0f1065ceb18f6e73ccc1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **softmax function with temperature** is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0805fb3b6ec66ae8072b0ecacbd3fdd0.png)'
  prefs: []
  type: TYPE_IMG
- en: where (T) is the temperature, (x_i) is the (i)-th component of the input vector
    (logits), and (n) is the number of components in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: At T = 1,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4250d9a5ebcd02c95d879ccd132cc5a3.png)'
  prefs: []
  type: TYPE_IMG
- en: At a temperature of 1, the probability values are the same as those derived
    from the standard softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: At T > 1,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28a0ff2d89cddc0ce05b88819422a3b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Raising the temperature inflates the probabilities of the less likely tokens,
    thereby broadening the range of potential candidates (or diversity) for the model’s
    next token prediction.
  prefs: []
  type: TYPE_NORMAL
- en: At T < 1,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0803db21504f8d41d771b1ed63ea7350.png)'
  prefs: []
  type: TYPE_IMG
- en: Lowering the temperature, on the other hand, makes the probability of the most
    likely token approach 1.0, boosting the model’s confidence. Decreasing the temperature
    effectively eliminates the uncertainty within the model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs leverage the temperature parameter to offer flexibility in their predictions.
    The model behaves predictably at a temperature of 1, closely following the original
    softmax distribution. Increasing the temperature introduces greater diversity,
    amplifying less likely tokens. Conversely, decreasing the temperature makes the
    predictions more focused, increasing the model’s confidence in the most probable
    token by reducing uncertainty. This adaptability allows users to tailor LLM outputs
    to a wide array of tasks, striking a balance between creative exploration and
    deterministic output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
