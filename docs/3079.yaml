- en: Your Company Needs Small Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26](https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/26913b776ae04e4f9a355fe2fa31fad8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by Stable Diffusion
  prefs: []
  type: TYPE_NORMAL
- en: When specialized models outperform general-purpose models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    ¬∑12 min read¬∑Dec 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúBigger is always better‚Äù ‚Äî this principle is deeply rooted in the AI world.
    Every month, larger models are created, with more and more parameters. Companies
    are even building [$10 billion AI data centers](https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power)
    for them. But is it the only direction to go?
  prefs: []
  type: TYPE_NORMAL
- en: 'At [NeurIPS 2024, Ilya Sutskever](https://www.youtube.com/watch?v=1yvBqasHLZs),
    one of OpenAI‚Äôs co-founders, shared an idea: *‚ÄúPre-training as we know it will
    unquestionably end‚Äù*. It seems the **era of scaling is coming to a close**, which
    means it‚Äôs time to focus on improving current approaches and algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most promising areas is the use of small language models (SLMs)
    with up to 10B parameters. This approach is really starting to take off in the
    industry. For example, Clem Delangue, CEO of Hugging Face, [predicts that up to
    99% of use cases could be addressed using SLMs](https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/).
    A similar trend is evident in the [latest requests for startups by YC](https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones):'
  prefs: []
  type: TYPE_NORMAL
- en: Giant generic models with a lot of parameters are very impressive. But they
    are also very costly and often come with latency and privacy challenges.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In my last article ‚Äú[You don‚Äôt need hosted LLMs, do you?](https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526)‚Äù,
    I wondered if you need self-hosted models. Now I take it a step further and ask
    the question: **do you need LLMs at all?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72ad67f12c5f765e561a3aab710ca34c.png)'
  prefs: []
  type: TYPE_IMG
- en: ‚ÄúShort‚Äù summary of the article.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I‚Äôll discuss why small models may be the solution your business
    needs. We‚Äôll talk about how they can reduce costs, improve accuracy, and maintain
    control of your data. And of course, we‚Äôll have an honest discussion about their
    limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Efficiency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The economics of LLMs is probably one of the most painful topics for businesses.
    However, the issue is much broader: it includes the need for expensive hardware,
    infrastructure costs, energy costs and environmental consequences.'
  prefs: []
  type: TYPE_NORMAL
- en: Yes, large language models are impressive in their capabilities, but they are
    also very expensive to maintain. You may have already noticed how subscription
    prices for LLMs-based applications have risen? For example, OpenAI‚Äôs recent announcement
    of a **$200/month** Pro plan is a signal that costs are rising. And it‚Äôs likely
    that competitors will also move up to these price levels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b41027953cd666454647c2b116ac0b3.png)'
  prefs: []
  type: TYPE_IMG
- en: $200 for Pro plan
  prefs: []
  type: TYPE_NORMAL
- en: '[The Moxie robot story](https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/)
    is a good example of this statement. Embodied created a great companion robot
    for kids for $800 that used the OpenAI API. Despite the success of the product
    (kids were sending 500‚Äì1000 messages a day!), the company [is shutting down](https://moxierobot.com/pages/closing-faqs)
    due to the high operational costs of the API. Now thousands of robots will become
    useless and kids will lose their friend.'
  prefs: []
  type: TYPE_NORMAL
- en: One approach is to **fine-tune a specialized Small Language Model for your specific
    domain**. Of course, it will not solve ‚Äúall the problems of the world‚Äù, but it
    will perfectly cope with the task it is assigned to. For example, analyzing client
    documentation or generating specific reports. At the same time, SLMs will be more
    economical to maintain, consume fewer resources, require less data, and can run
    on much more modest hardware ([up to a smartphone](https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebee1b3fa08f2a9ac511bdd47215c967.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of utilization of models with different number of parameters. [Source1](https://arxiv.org/pdf/2404.08850),
    [source2](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/),
    [source3](https://huggingface.co/blog/inference-dgx-cloud), [source4](https://llamaimodel.com/price/).
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, let‚Äôs not forget about the environment. In the [article Carbon
    Emissions and Large Neural Network Training](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf),
    I found some interesting statistic that amazed me: training GPT-3 with 175 billion
    parameters consumed as much electricity as the average American home consumes
    in 120 years. It also **produced 502 tons of CO‚ÇÇ**, which is comparable to the
    annual operation of more than a hundred gasoline cars. And that‚Äôs not counting
    inferential costs. By comparison, deploying a smaller model like the **7B would
    require 5%** of the consumption of a larger model. And what about the latest [o3
    release](https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/)?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0b5c26438dab27fe0c8d676828452ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Model o3 CO‚ÇÇ production. [Source](https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW).
  prefs: []
  type: TYPE_NORMAL
- en: üí°**Hint:** don‚Äôt chase the hype. Before tackling the task, calculate the costs
    of using APIs or your own servers. Think about scaling of such a system and how
    justified the use of LLMs is.
  prefs: []
  type: TYPE_NORMAL
- en: Performance on Specialized Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we‚Äôve covered the economics, let‚Äôs talk about quality. Naturally, very
    few people would want to compromise on solution accuracy just to save costs. But
    even here, SLMs have something to offer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/671c7646feb5e05b026184d19a97a9bd.png)'
  prefs: []
  type: TYPE_IMG
- en: In-domain Moderation Performance. Comparing the performance of SLMs versus LLMs
    on accuracy, recall, and precision for in-domain content moderation performance.
    Best performing SLMs outperform LLMs on accuracy and recall across all subreddits,
    while LLMs outperform SLMs on precision. [Source.](https://arxiv.org/pdf/2410.13155)
  prefs: []
  type: TYPE_NORMAL
- en: 'Many studies show that for highly specialized tasks, small models can not only
    compete with large LLMs, but often outperform them. Let‚Äôs look at a few illustrative
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Medicine:** The [Diabetica-7B model](https://arxiv.org/pdf/2409.13191) (based
    on the Qwen2‚Äì7B) achieved 87.2% accuracy on diabetes-related tests, while GPT-4
    showed 79.17% and Claude-3.5‚Äì80.13%. Despite this, Diabetica-7B is dozens of times
    smaller than GPT-4 and **can run locally on a consumer GPU**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Legal Sector:** [An SLM with just 0.2B parameters](https://arxiv.org/pdf/2311.09825)
    achieves 77.2% accuracy in contract analysis (GPT-4 ‚Äî about 82.4%). Moreover,
    for tasks like identifying ‚Äúunfair‚Äù terms in user agreements, the **SLM even outperforms
    GPT-3.5 and GPT-4** on the F1 metric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Mathematical Tasks:** [Research by Google DeepMind shows](https://arxiv.org/pdf/2408.16737)
    that training a small model, Gemma2‚Äì9B, on data generated by another small model
    yields better results than training on data from the larger Gemma2‚Äì27B. Smaller
    models tend to focus better on specifics without the tendency to ‚Äútrying to shine
    with all the knowledge‚Äù, which is often a trait of larger models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Content Moderation:** [LLaMA 3.1 8B outperformed](https://arxiv.org/pdf/2410.13155)
    GPT-3.5 in accuracy (by 11.5%) and recall (by 25.7%) when moderating content across
    15 popular subreddits. **This was achieved even with 4-bit quantization**, which
    further reduces the model‚Äôs size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/67a12f7e7296b11c505a401955b85fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of instruction-tuned domain SLMs for QA and LLMs on PubMedQA. [Source](https://arxiv.org/pdf/2411.03350).
  prefs: []
  type: TYPE_NORMAL
- en: 'I‚Äôll go a step further and share that even classic NLP approaches often work
    surprisingly well. Let me share a personal case: I‚Äôm working on a product for
    psychological support where we process over a thousand messages from users every
    day. They can write in a chat and get a response. Each message is first classified
    into one of four categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a756c82fcba8cae98609aba545fb141c.png)'
  prefs: []
  type: TYPE_IMG
- en: Message Classification Scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '`SUPPORT` ‚Äî A question about how the app works; we respond using the documentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GRATITUDE` ‚Äî The user thanks the bot; we simply send a ‚Äúlike.‚Äù'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TRY_TO_HACK` ‚Äî The user requests something unrelated to the app‚Äôs purpose
    (e.g., ‚ÄúWrite a function in Python‚Äù).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OTHER`‚Äî All other messages, which we process further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Previously, I used GPT-3.5-turbo for classification and later switched to GPT-4o
    mini, spending a lot of time changing the prompt. However, I still encountered
    errors. So, I decided to try a classic approach: TF-IDF + a simple classifier.
    Training took less than a minute, and the Macro F1 score increased to 0.95 (compared
    to 0.92 for GPT-4o mini). The model size is just 76 MB, and when applied to 2
    million processed messages (our actual data), the cost savings were significant:
    the **GPT-based solution would have cost about $500, while the classic approach
    cost almost nothing**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Accuracy, speed and cost comparison table: GPT-4o mini vs TF-IDF model.'
  prefs: []
  type: TYPE_NORMAL
- en: And there are several such ‚Äúsmall‚Äù and simple tasks in our product. I believe
    you might find the same in your company. Of course, large models are great for
    a quick start, especially when there‚Äôs no labeled data and requirements are changing.
    But for well-defined, stable tasks where accuracy and minimal costs are key, specialized
    and simple models (including classic methods) can often be a more effective solution.
  prefs: []
  type: TYPE_NORMAL
- en: üí°**Hint:** use LLMs for prototyping, and then, once the task becomes clear and
    stable, switch to smaller, cheaper, and more accurate models. This hybrid approach
    helps maintain high quality, significantly reduce costs, and avoid the redundancy
    of general-purpose models.
  prefs: []
  type: TYPE_NORMAL
- en: Security, Privacy and Regulatory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using LLMs through APIs, you‚Äôre handing over sensitive data to external providers,
    increasing the risk of leaks and complicating compliance with strict regulations
    like HIPAA, GDPR, and CCPA. OpenAI‚Äôs recent announcement about plans to introduce
    advertising only highlights these risks. **Your company not only loses full control
    over its data but also becomes dependent on third-party SLAs.**
  prefs: []
  type: TYPE_NORMAL
- en: Certainly, it‚Äôs possible to run a LLM locally, but the cost of deployment and
    scaling (hundreds of gigabytes of memory, multiple GPUs) often exceeds reasonable
    economic limits and makes it difficult to quickly adapt to new regulatory requirements.
    And you can forget about launching it on low-end hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99295d3081eef53a84bd486fdd456dd9.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of Cloud API Risks and on-device slm benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is where the ‚Äúsmall guys‚Äù come back into play:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Simplified Audits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The smaller size of SLMs lowers the barrier for conducting audits, verification,
    and customization to meet specific regulations. It‚Äôs easier to understand how
    the model processes data, implement your own encryption or logging, and show auditors
    that information never leaves a trusted environment. As the founder of a healthcare
    company, I know how challenging and crucial this task can be.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\.** Running on Isolated and low-end hardware'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs are difficult to efficiently ‚Äúdeploy‚Äù in an isolated network segment or
    on a smartphone. SLMs, however, with their lower computational requirements, can
    operate almost anywhere: from a local server in a private network to a doctor‚Äôs
    or inspector‚Äôs device. [According to IDC](https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/)
    forecasts, **by 2028, over 900 million smartphones will be capable of running
    generative AI models locally**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\.** New Regulations Updates and Adaptation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regulations and laws change frequently ‚Äî compact models can be fine-tuned or
    adjusted in hours rather than days. This enables a quick response to new requirements
    without the need for large-scale infrastructure upgrades, which are typical for
    big LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Distributed Security Architecture**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike the monolithic architecture of LLMs, where all security components are
    ‚Äúbaked‚Äù into one large model, SLMs enable the creation of a distributed security
    system. Each component:'
  prefs: []
  type: TYPE_NORMAL
- en: Specializes in a specific task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be independently updated and tested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales separately from the others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, a medical application could use a cascade of three models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy Guardian (2B)** ‚Äî masks personal data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Medical Validator (3B)** ‚Äî ensures medical accuracy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compliance Checker (1B)** ‚Äî monitors HIPAA compliance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Smaller models are easier to verify and update**, making the overall architecture
    more flexible and reliable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c73556307d7ff9a0f482dd5e882ed0da.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of Data Privacy Features.
  prefs: []
  type: TYPE_NORMAL
- en: üí°**Hint:** consider using SLMs if you operate in a heavily regulated field.
    Pay close attention to data transfer policies and the frequency of changes in
    the regulatory landscape. I recommend use SLMs if your professional domain is
    in healthcare, finance, or law.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI Agents: The Perfect Use Case'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember the old [Unix philosophy, ‚ÄúDo one thing and do it well‚Äù](https://en.wikipedia.org/wiki/Unix_philosophy)?
    It seems we‚Äôre returning to this principle, now in the context of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Ilya Sutskever‚Äôs recent statement at NeurIPS that ‚ÄúPre-training as we know it
    will unquestionably end‚Äù and that the next generation of models will be ‚Äúagentic
    in real ways‚Äù only confirms this trend. Y Combinator goes even further, predicting
    that [**AI agents could create a market 10 times larger than SaaS**](https://www.youtube.com/watch?v=ASABxNenD_U).
  prefs: []
  type: TYPE_NORMAL
- en: For example, already [12% of enterprise solutions use agent-based architecture](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/).
    Moreover, analysts predict that agents will be the next wave of AI-transformation
    that can affect not only the $400-billion software market, but also the **$10-trillion
    U.S. services economy**.
  prefs: []
  type: TYPE_NORMAL
- en: And SMLs are ideal candidates for this role. Perhaps one model is quite limited,
    but a swarm of such models ‚Äî can solve complex tasks piece by piece. **Faster,
    higher quality and cheaper.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs take a concrete example: imagine you are building a system to analyze
    financial documents. Instead of using one large model, you can break the task
    into several specialized agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9593e6888e27316001c963ec8232ee1.png)'
  prefs: []
  type: TYPE_IMG
- en: The example flow of information between specialized agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'And this approach is not only more cost-effective but also more reliable: each
    agent focuses on what it does best. **Cheaper. Faster. Better.** Yes, I‚Äôm repeating
    it again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To back this up, let me name a few companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**H Company**](https://www.hcompany.ai/) raised $100M in a seed round to develop
    a multi-agent system based on SLMs (2‚Äì3B parameters). Their agent Runner H (3B)
    achieves 67% task completion success compared to Anthropic‚Äôs Computer Use at 52%,
    all **with significantly lower costs**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Liquid AI**](https://www.liquid.ai/) recently secured $250M in funding,
    focusing on building efficient enterprise models. Their model (1.3B parameters)
    has outperformed all existing models of similar size. Meanwhile, their LFM-3B
    delivers performance on par with 7B and even 13B models **while requiring less
    memory.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**Cohere**](https://cohere.com/) launched Command R7B, a specialized model
    for RAG applications that can even **run on a CPU**. The model supports 23 languages
    and integrates with external tools, showing best-in-class results for reasoning
    and question-answering tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**YOUR COMPANY NAME** could also join this list. I‚Äôm not just saying that ‚Äî
    in [Reforma Health](https://reforma.health/), the company I‚Äôm working on, is developing
    specialized SLMs for various medical domains. This decision was driven by the
    need to comply with HIPAA requirements and the specifics of medical information
    processing. Our experience shows that highly **specialized SLMs can be a significant
    competitive advantage**, especially in regulated domains.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These examples highlight the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Investors believe** in the future of specialized small models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enterprise clients are willing to pay** for efficient solutions that don‚Äôt
    require sending data to external providers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The market is shifting towards **‚Äúsmart‚Äù specialized agents** instead of relying
    on ‚Äúuniversal‚Äù large models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: üí°**Hint:** start by identifying repetitive tasks in your project. These are
    the best candidates for developing specialized SLM agents. This approach will
    help you avoid overpaying for the excessive power of LLMs and achieve greater
    control over the process.
  prefs: []
  type: TYPE_NORMAL
- en: Potential Limitations of SLMs Compared to LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although I‚Äôve spent this entire article praising small models, it‚Äôs fair to
    point out their limitations as well.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Limited Task Flexibility
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most significant limitation of SLMs is their narrow specialization. Unlike
    LLMs, which can handle a wide range of tasks, SLMs succeed only in the specific
    tasks for which they have been trained. For example, in medicine, [Diabetica-7B
    outperformed LLMs](https://arxiv.org/pdf/2409.13191) in diabetes-related tests,
    but other medical disciplines required additional fine-tuning or a new architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22382712a4e184c3025a715a8ed608f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'LLMs vs SLMs: Flexibility vs Specialization.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Context Window Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike large models that reach up to 1M tokens ([Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)),
    SLMs have shorter contexts. Even though recent advances in small LLaMA 3.2 models
    (3B, 1B) having a context length of 128k tokens, [the effective context length
    is often not as claimed](https://arxiv.org/pdf/2410.18745): models often lose
    the ‚Äúconnection‚Äù between the beginning and the end of the text. For example, SLMs
    cannot efficiently process voluminous medical histories of patients over several
    years or large legal documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of maximum context length for different models.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Emergence Capabilities Gap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many ‚Äúemergent abilities‚Äù only [appear when a model reaches a certain size
    threshold](https://arxiv.org/pdf/2001.08361). SLMs **typically don‚Äôt hit the parameter
    levels required for advanced logical reasoning or deep contextual understanding**.
    [A study by Google Research](https://arxiv.org/pdf/2408.16737) demonstrates this
    with math word problems: while small models struggle with basic arithmetic, larger
    models suddenly demonstrate complex mathematical reasoning skills.'
  prefs: []
  type: TYPE_NORMAL
- en: However, [recent research by Hugging Face shows](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
    that **test-time compute scaling** can partially bridge this gap. Using strategies
    like **iterative self-refinement** or employing a **reward model**, small models
    can ‚Äúthink longer‚Äù on complex problems. For example, with extended generation
    time, small models (1B and 3B) outperformed their larger counterparts (8B and
    70B) on the MATH-500 benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: üí°**Hint:** If you work in an environment where tasks change weekly, require
    analyzing large documents, or involve solving complex logical problems, larger
    LLMs are often more reliable and versatile.
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with choosing between O[penAI and self-hosted LLMs in my previous article](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526),
    there is no one-size-fits-all solution here. If your task involves constant changes,
    lacks precise specialization, or requires rapid prototyping, LLMs will offer an
    easy start.
  prefs: []
  type: TYPE_NORMAL
- en: However, over time, as your goal become more clearer, moving to compact, specialized
    **SLM agents can significantly reduce costs, improve accuracy, and simplify compliance
    with regulatory requirements**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12891117c3be9a5f526a54a48e569a6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Moving from rapid prototyping at LLM to an optimized SLM agent ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: SLMs aren‚Äôt a paradigm shift for the sake of trends but a pragmatic approach
    that allows you to solve specific problems more accurately and cost-effectively
    without overpaying for unnecessary functionality. You don‚Äôt need to completely
    abandon LLMs ‚Äî **you can gradually replace only some components with SLMs** or
    even classic NLP methods. It all depends on your metrics, budget, and the nature
    of your task.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of this is IBM, which employs a [multimodel strategy](https://www.ibm.com/products/watsonx-ai/foundation-models),
    combining smaller models for different tasks. As they point out:'
  prefs: []
  type: TYPE_NORMAL
- en: Bigger is not always better, as specialized models outperform general-purpose
    models with lower infrastructure requirements.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the end, the **key to success is to adapt**. Start with a large model, evaluate
    where it performs best, and then optimize your architecture to avoid overpaying
    for unnecessary capabilities and compromising data privacy. This approach allows
    you to combine the best of both worlds: the flexibility and versatility of LLMs
    during the initial stages, and the precise, cost-effective performance of SLMs
    for a mature product.'
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions or suggestions, feel free to connect on [LinkedIn](https://www.linkedin.com/in/sergey-savvov/).
  prefs: []
  type: TYPE_NORMAL
- en: '***Disclaimer****: The information in the article is current as of December
    2024, but please be aware that changes may occur thereafter.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
