- en: Your Company Needs Small Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ çš„å…¬å¸éœ€è¦å°å‹è¯­è¨€æ¨¡å‹
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26](https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26](https://towardsdatascience.com/your-company-needs-small-language-models-d0a223e0b6d9?source=collection_archive---------0-----------------------#2024-12-26)
- en: '![](../Images/26913b776ae04e4f9a355fe2fa31fad8.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26913b776ae04e4f9a355fe2fa31fad8.png)'
- en: Image generated by Stable Diffusion
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±Stable Diffusionç”Ÿæˆçš„å›¾åƒ
- en: When specialized models outperform general-purpose models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å½“ä¸“ç”¨æ¨¡å‹è¶…è¿‡é€šç”¨æ¨¡å‹æ—¶
- en: '[](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Sergei
    Savvov](../Images/a653eaeeec954f1a71e6341b424f009a.png)](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    [Sergei Savvov](https://slgero.medium.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    Â·12 min readÂ·Dec 26, 2024
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0a223e0b6d9--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2024å¹´12æœˆ26æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: â€œBigger is always betterâ€ â€” this principle is deeply rooted in the AI world.
    Every month, larger models are created, with more and more parameters. Companies
    are even building [$10 billion AI data centers](https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power)
    for them. But is it the only direction to go?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: â€œæ›´å¤§æ€»æ˜¯æ›´å¥½â€â€”â€”è¿™ä¸€åŸåˆ™åœ¨AIé¢†åŸŸæ ¹æ·±è’‚å›ºã€‚æ¯ä¸ªæœˆï¼Œæ›´å¤šçš„å‚æ•°å’Œæ›´å¤§çš„æ¨¡å‹è¢«åˆ›é€ å‡ºæ¥ã€‚å…¬å¸ç”šè‡³ä¸ºæ­¤å»ºç«‹äº†[$100äº¿çš„AIæ•°æ®ä¸­å¿ƒ](https://www.datacenterfrontier.com/hyperscale/article/55248311/meta-sees-10b-ai-data-center-in-louisiana-using-combo-of-clean-energy-nuclear-power)ã€‚ä½†æ˜¯ï¼Œè¿™çœŸçš„æ˜¯å”¯ä¸€çš„æ–¹å‘å—ï¼Ÿ
- en: 'At [NeurIPS 2024, Ilya Sutskever](https://www.youtube.com/watch?v=1yvBqasHLZs),
    one of OpenAIâ€™s co-founders, shared an idea: *â€œPre-training as we know it will
    unquestionably endâ€*. It seems the **era of scaling is coming to a close**, which
    means itâ€™s time to focus on improving current approaches and algorithms.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[NeurIPS 2024ä¼šè®®ä¸Šï¼ŒOpenAIçš„è”åˆåˆ›å§‹äººIlya Sutskever](https://www.youtube.com/watch?v=1yvBqasHLZs)åˆ†äº«äº†ä¸€ä¸ªè§‚ç‚¹ï¼šâ€œ*æˆ‘ä»¬æ‰€çŸ¥çš„é¢„è®­ç»ƒå°†æ— å¯é¿å…åœ°ç»“æŸ*â€ã€‚çœ‹èµ·æ¥**è§„æ¨¡åŒ–æ—¶ä»£å³å°†ç»“æŸ**ï¼Œè¿™æ„å‘³ç€ç°åœ¨æ˜¯æ—¶å€™é›†ä¸­ç²¾åŠ›æ”¹è¿›ç°æœ‰çš„æ–¹æ³•å’Œç®—æ³•äº†ã€‚
- en: 'One of the most promising areas is the use of small language models (SLMs)
    with up to 10B parameters. This approach is really starting to take off in the
    industry. For example, Clem Delangue, CEO of Hugging Face, [predicts that up to
    99% of use cases could be addressed using SLMs](https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/).
    A similar trend is evident in the [latest requests for startups by YC](https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æœ‰å‰æ™¯çš„é¢†åŸŸä¹‹ä¸€æ˜¯ä½¿ç”¨æœ€å¤š10Bå‚æ•°çš„å°å‹è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰ã€‚è¿™ç§æ–¹æ³•åœ¨è¡Œä¸šä¸­çœŸæ­£å¼€å§‹èµ·é£ã€‚ä¾‹å¦‚ï¼ŒHugging Faceçš„CEO Clem Delangueï¼Œ[é¢„æµ‹æœ€å¤š99%çš„ä½¿ç”¨æ¡ˆä¾‹å¯ä»¥é€šè¿‡SLMsæ¥è§£å†³](https://www.linkedin.com/posts/clementdelangue_ive-said-it-and-will-say-it-again-1-activity-7112524134395318273-T3z6/)ã€‚YCçš„[æœ€æ–°åˆ›ä¸šè¯·æ±‚](https://www.ycombinator.com/rfs#summer-2024-small-fine-tuned-models-as-an-alternative-to-giant-generic-ones)ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°ç±»ä¼¼çš„è¶‹åŠ¿ï¼š
- en: Giant generic models with a lot of parameters are very impressive. But they
    are also very costly and often come with latency and privacy challenges.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰å¤§é‡å‚æ•°çš„å·¨å‹é€šç”¨æ¨¡å‹éå¸¸ä»¤äººå°è±¡æ·±åˆ»ã€‚ä½†å®ƒä»¬ä¹Ÿéå¸¸æ˜‚è´µï¼Œä¸”å¾€å¾€é¢ä¸´å»¶è¿Ÿå’Œéšç§é—®é¢˜ã€‚
- en: 'In my last article â€œ[You donâ€™t need hosted LLMs, do you?](https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526)â€,
    I wondered if you need self-hosted models. Now I take it a step further and ask
    the question: **do you need LLMs at all?**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä¸Šä¸€ç¯‡æ–‡ç« â€œ[ä½ çœŸçš„éœ€è¦æ‰˜ç®¡çš„LLMå—ï¼Ÿ](https://betterprogramming.pub/you-dont-need-hosted-llms-do-you-1160b2520526)â€ä¸­ï¼Œæˆ‘æ›¾ç»æƒ³è¿‡ä½ æ˜¯å¦éœ€è¦è‡ªæ‰˜ç®¡çš„æ¨¡å‹ã€‚ç°åœ¨æˆ‘æ›´è¿›ä¸€æ­¥ï¼Œæå‡ºä¸€ä¸ªé—®é¢˜ï¼š**ä½ æ ¹æœ¬éœ€è¦LLMå—ï¼Ÿ**
- en: '![](../Images/72ad67f12c5f765e561a3aab710ca34c.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72ad67f12c5f765e561a3aab710ca34c.png)'
- en: â€œShortâ€ summary of the article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ç« çš„â€œç®€çŸ­â€æ€»ç»“ã€‚
- en: In this article, Iâ€™ll discuss why small models may be the solution your business
    needs. Weâ€™ll talk about how they can reduce costs, improve accuracy, and maintain
    control of your data. And of course, weâ€™ll have an honest discussion about their
    limitations.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†è®¨è®ºä¸ºä»€ä¹ˆå°å‹æ¨¡å‹å¯èƒ½æ˜¯æ‚¨ä¼ä¸šæ‰€éœ€çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†æ¢è®¨å®ƒä»¬å¦‚ä½•å‡å°‘æˆæœ¬ã€æé«˜å‡†ç¡®æ€§ï¼Œå¹¶ä¿æŒå¯¹æ‚¨æ•°æ®çš„æ§åˆ¶ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬è¿˜å°†è¯šå®åœ°è®¨è®ºå®ƒä»¬çš„å±€é™æ€§ã€‚
- en: Cost Efficiency
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆæœ¬æ•ˆç›Š
- en: 'The economics of LLMs is probably one of the most painful topics for businesses.
    However, the issue is much broader: it includes the need for expensive hardware,
    infrastructure costs, energy costs and environmental consequences.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„ç»æµå­¦å¯èƒ½æ˜¯ä¼ä¸šæœ€ç—›è‹¦çš„è¯é¢˜ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œè¿™ä¸ªé—®é¢˜è¿œä¸æ­¢å¦‚æ­¤ï¼šå®ƒåŒ…æ‹¬å¯¹æ˜‚è´µç¡¬ä»¶çš„éœ€æ±‚ã€åŸºç¡€è®¾æ–½æˆæœ¬ã€èƒ½æºæˆæœ¬å’Œç¯å¢ƒåæœã€‚
- en: Yes, large language models are impressive in their capabilities, but they are
    also very expensive to maintain. You may have already noticed how subscription
    prices for LLMs-based applications have risen? For example, OpenAIâ€™s recent announcement
    of a **$200/month** Pro plan is a signal that costs are rising. And itâ€™s likely
    that competitors will also move up to these price levels.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¶èƒ½åŠ›ä¸Šä»¤äººå°è±¡æ·±åˆ»ï¼Œä½†å®ƒä»¬çš„ç»´æŠ¤æˆæœ¬ä¹Ÿéå¸¸é«˜ã€‚æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°åŸºäºLLMçš„åº”ç”¨ç¨‹åºè®¢é˜…ä»·æ ¼çš„ä¸Šæ¶¨ï¼Ÿä¾‹å¦‚ï¼ŒOpenAIæœ€è¿‘å®£å¸ƒçš„**æ¯æœˆ$200**ä¸“ä¸šç‰ˆè®¡åˆ’å°±æ˜¯æˆæœ¬ä¸Šå‡çš„ä¿¡å·ã€‚è€Œä¸”ï¼Œå¾ˆå¯èƒ½ç«äº‰å¯¹æ‰‹ä¹Ÿä¼šå°†ä»·æ ¼æé«˜åˆ°è¿™äº›æ°´å¹³ã€‚
- en: '![](../Images/2b41027953cd666454647c2b116ac0b3.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b41027953cd666454647c2b116ac0b3.png)'
- en: $200 for Pro plan
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“ä¸šç‰ˆè®¡åˆ’ï¼Œ$200
- en: '[The Moxie robot story](https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/)
    is a good example of this statement. Embodied created a great companion robot
    for kids for $800 that used the OpenAI API. Despite the success of the product
    (kids were sending 500â€“1000 messages a day!), the company [is shutting down](https://moxierobot.com/pages/closing-faqs)
    due to the high operational costs of the API. Now thousands of robots will become
    useless and kids will lose their friend.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[Moxieæœºå™¨äººæ•…äº‹](https://arstechnica.com/gadgets/2024/12/startup-will-brick-800-emotional-support-robot-for-kids-without-refunds/)æ˜¯è¿™ä¸€è§‚ç‚¹çš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚Embodiedä¸ºå­©å­ä»¬åˆ›é€ äº†ä¸€ä¸ªä¼˜ç§€çš„ä¼´ä¾£æœºå™¨äººï¼Œå”®ä»·ä¸º$800ï¼Œä½¿ç”¨äº†OpenAI
    APIã€‚å°½ç®¡äº§å“å–å¾—äº†æˆåŠŸï¼ˆå­©å­ä»¬æ¯å¤©å‘é€500åˆ°1000æ¡æ¶ˆæ¯ï¼ï¼‰ï¼Œä½†ç”±äºAPIçš„é«˜è¿è¥æˆæœ¬ï¼Œå…¬å¸[æ­£åœ¨å…³é—­](https://moxierobot.com/pages/closing-faqs)ã€‚ç°åœ¨ï¼Œæˆåƒä¸Šä¸‡çš„æœºå™¨äººå°†å˜å¾—æ— ç”¨ï¼Œå­©å­ä»¬å°†å¤±å»ä»–ä»¬çš„æœ‹å‹ã€‚'
- en: One approach is to **fine-tune a specialized Small Language Model for your specific
    domain**. Of course, it will not solve â€œall the problems of the worldâ€, but it
    will perfectly cope with the task it is assigned to. For example, analyzing client
    documentation or generating specific reports. At the same time, SLMs will be more
    economical to maintain, consume fewer resources, require less data, and can run
    on much more modest hardware ([up to a smartphone](https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–¹æ³•æ˜¯**é’ˆå¯¹æ‚¨çš„ç‰¹å®šé¢†åŸŸå¯¹å°å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒ**ã€‚å½“ç„¶ï¼Œå®ƒå¹¶ä¸èƒ½è§£å†³â€œä¸–ç•Œä¸Šæ‰€æœ‰çš„é—®é¢˜â€ï¼Œä½†å®ƒèƒ½å®Œç¾åœ°åº”å¯¹åˆ†é…ç»™å®ƒçš„ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œåˆ†æå®¢æˆ·æ–‡æ¡£æˆ–ç”Ÿæˆç‰¹å®šæŠ¥å‘Šã€‚ä¸æ­¤åŒæ—¶ï¼Œå°å‹è¯­è¨€æ¨¡å‹åœ¨ç»´æŠ¤ä¸Šæ›´å…·ç»æµæ€§ï¼Œæ¶ˆè€—çš„èµ„æºæ›´å°‘ï¼Œæ‰€éœ€çš„æ•°æ®é‡æ›´å°ï¼Œå¹¶ä¸”å¯ä»¥åœ¨æ›´åŠ ç®€æœ´çš„ç¡¬ä»¶ä¸Šè¿è¡Œï¼ˆ[ç”šè‡³æ˜¯æ™ºèƒ½æ‰‹æœº](https://privatellm.app/blog/run-local-gpt-on-ios-complete-guide)ï¼‰ã€‚
- en: '![](../Images/ebee1b3fa08f2a9ac511bdd47215c967.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebee1b3fa08f2a9ac511bdd47215c967.png)'
- en: Comparison of utilization of models with different number of parameters. [Source1](https://arxiv.org/pdf/2404.08850),
    [source2](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/),
    [source3](https://huggingface.co/blog/inference-dgx-cloud), [source4](https://llamaimodel.com/price/).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒå‚æ•°é‡æ¨¡å‹çš„ä½¿ç”¨å¯¹æ¯”ã€‚[æ¥æº1](https://arxiv.org/pdf/2404.08850)ï¼Œ[æ¥æº2](https://adasci.org/how-much-energy-do-llms-consume-unveiling-the-power-behind-ai/)ï¼Œ[æ¥æº3](https://huggingface.co/blog/inference-dgx-cloud)ï¼Œ[æ¥æº4](https://llamaimodel.com/price/)ã€‚
- en: 'And finally, letâ€™s not forget about the environment. In the [article Carbon
    Emissions and Large Neural Network Training](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf),
    I found some interesting statistic that amazed me: training GPT-3 with 175 billion
    parameters consumed as much electricity as the average American home consumes
    in 120 years. It also **produced 502 tons of COâ‚‚**, which is comparable to the
    annual operation of more than a hundred gasoline cars. And thatâ€™s not counting
    inferential costs. By comparison, deploying a smaller model like the **7B would
    require 5%** of the consumption of a larger model. And what about the latest [o3
    release](https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/)?'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä¸èƒ½å¿½è§†ç¯å¢ƒå› ç´ ã€‚åœ¨ [ã€Šç¢³æ’æ”¾ä¸å¤§è§„æ¨¡ç¥ç»ç½‘ç»œè®­ç»ƒã€‹](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)
    æ–‡ç« ä¸­ï¼Œæˆ‘å‘ç°äº†ä¸€äº›è®©æˆ‘æƒŠè®¶çš„ç»Ÿè®¡æ•°æ®ï¼šè®­ç»ƒ GPT-3ï¼ˆæ‹¥æœ‰ 1750 äº¿å‚æ•°ï¼‰æ‰€æ¶ˆè€—çš„ç”µåŠ›ç›¸å½“äºç¾å›½æ™®é€šå®¶åº­ 120 å¹´çš„ç”¨ç”µé‡ã€‚å®ƒè¿˜ **æ’æ”¾äº† 502
    å¨ COâ‚‚**ï¼Œç›¸å½“äºä¸€ç™¾å¤šè¾†æ±½æ²¹è½¦ä¸€å¹´çš„æ’æ”¾é‡ã€‚è€Œè¿™è¿˜ä¸åŒ…æ‹¬æ¨ç†æˆæœ¬ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œéƒ¨ç½²åƒ **7B** è¿™æ ·çš„è¾ƒå°æ¨¡å‹ï¼Œä»…éœ€æ¶ˆè€—å¤§å‹æ¨¡å‹ 5% çš„ç”µåŠ›ã€‚é‚£æœ€æ–°çš„
    [o3 ç‰ˆæœ¬](https://techcrunch.com/2024/12/20/openai-announces-new-o3-model/)å‘¢ï¼Ÿ
- en: '![](../Images/e0b5c26438dab27fe0c8d676828452ac.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0b5c26438dab27fe0c8d676828452ac.png)'
- en: Model o3 COâ‚‚ production. [Source](https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡æ‹Ÿ o3 COâ‚‚ æ’æ”¾ã€‚[æ¥æº](https://www.linkedin.com/posts/bgamazay_openai-has-announced-o3-which-appears-to-activity-7276250095019335680-sVbW).
- en: ğŸ’¡**Hint:** donâ€™t chase the hype. Before tackling the task, calculate the costs
    of using APIs or your own servers. Think about scaling of such a system and how
    justified the use of LLMs is.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡**æç¤ºï¼š** ä¸è¦ç›²ç›®è·Ÿé£ã€‚åœ¨è§£å†³ä»»åŠ¡ä¹‹å‰ï¼Œå…ˆè®¡ç®—ä½¿ç”¨ API æˆ–è‡ªå·±çš„æœåŠ¡å™¨çš„æˆæœ¬ã€‚è€ƒè™‘è¿™æ ·çš„ç³»ç»Ÿæ‰©å±•æ€§ï¼Œä»¥åŠä½¿ç”¨ LLM çš„åˆç†æ€§ã€‚
- en: Performance on Specialized Tasks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸“ä¸šä»»åŠ¡è¡¨ç°
- en: Now that weâ€™ve covered the economics, letâ€™s talk about quality. Naturally, very
    few people would want to compromise on solution accuracy just to save costs. But
    even here, SLMs have something to offer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¢ç„¶æˆ‘ä»¬å·²ç»è®¨è®ºäº†ç»æµæ€§ï¼Œæ¥ä¸‹æ¥è°ˆè°ˆè´¨é‡ã€‚è‡ªç„¶ï¼Œå‡ ä¹æ²¡æœ‰äººæ„¿æ„ä»…ä»…ä¸ºäº†èŠ‚çœæˆæœ¬è€Œåœ¨è§£å†³æ–¹æ¡ˆçš„å‡†ç¡®æ€§ä¸Šåšå‡ºå¦¥åã€‚ä½†å³ä¾¿å¦‚æ­¤ï¼ŒSLM ä¹Ÿæœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚
- en: '![](../Images/671c7646feb5e05b026184d19a97a9bd.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/671c7646feb5e05b026184d19a97a9bd.png)'
- en: In-domain Moderation Performance. Comparing the performance of SLMs versus LLMs
    on accuracy, recall, and precision for in-domain content moderation performance.
    Best performing SLMs outperform LLMs on accuracy and recall across all subreddits,
    while LLMs outperform SLMs on precision. [Source.](https://arxiv.org/pdf/2410.13155)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œä¸šå†…å†…å®¹å®¡æ ¸è¡¨ç°ã€‚æ¯”è¾ƒ SLM å’Œ LLM åœ¨è¡Œä¸šå†…å†…å®¹å®¡æ ¸ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§ã€å¬å›ç‡å’Œç²¾åº¦è¡¨ç°ã€‚åœ¨æ‰€æœ‰å­ç‰ˆå—ä¸­ï¼Œè¡¨ç°æœ€å¥½çš„ SLM åœ¨å‡†ç¡®æ€§å’Œå¬å›ç‡ä¸Šè¶…è¶Šäº†
    LLMï¼Œè€Œ LLM åœ¨ç²¾åº¦ä¸Šåˆ™è¶…è¿‡äº† SLMã€‚[æ¥æº](https://arxiv.org/pdf/2410.13155)
- en: 'Many studies show that for highly specialized tasks, small models can not only
    compete with large LLMs, but often outperform them. Letâ€™s look at a few illustrative
    examples:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºé«˜åº¦ä¸“ä¸šåŒ–çš„ä»»åŠ¡ï¼Œå°å‹æ¨¡å‹ä¸ä»…èƒ½å¤Ÿä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç«äº‰ï¼Œç”šè‡³å¾€å¾€è¶…è¿‡å®ƒä»¬ã€‚è®©æˆ‘ä»¬æ¥çœ‹å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­ï¼š
- en: '**Medicine:** The [Diabetica-7B model](https://arxiv.org/pdf/2409.13191) (based
    on the Qwen2â€“7B) achieved 87.2% accuracy on diabetes-related tests, while GPT-4
    showed 79.17% and Claude-3.5â€“80.13%. Despite this, Diabetica-7B is dozens of times
    smaller than GPT-4 and **can run locally on a consumer GPU**.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŒ»å­¦é¢†åŸŸï¼š** [Diabetica-7B æ¨¡å‹](https://arxiv.org/pdf/2409.13191)ï¼ˆåŸºäº Qwen2â€“7Bï¼‰åœ¨ä¸ç³–å°¿ç—…ç›¸å…³çš„æµ‹è¯•ä¸­è¾¾åˆ°äº†
    87.2% çš„å‡†ç¡®ç‡ï¼Œè€Œ GPT-4 æ˜¾ç¤ºä¸º 79.17%ï¼ŒClaude-3.5 åˆ™ä¸º 80.13%ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒDiabetica-7B çš„ä½“ç§¯æ¯” GPT-4
    å°äº†å‡ åå€ï¼Œè€Œä¸” **å¯ä»¥åœ¨æ¶ˆè´¹è€…çº§ GPU ä¸Šæœ¬åœ°è¿è¡Œ**ã€‚'
- en: '**Legal Sector:** [An SLM with just 0.2B parameters](https://arxiv.org/pdf/2311.09825)
    achieves 77.2% accuracy in contract analysis (GPT-4 â€” about 82.4%). Moreover,
    for tasks like identifying â€œunfairâ€ terms in user agreements, the **SLM even outperforms
    GPT-3.5 and GPT-4** on the F1 metric.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ³•å¾‹é¢†åŸŸï¼š** ä¸€ä¸ªä»…æœ‰ 0.2B å‚æ•°çš„ [SLM](https://arxiv.org/pdf/2311.09825) åœ¨åˆåŒåˆ†æä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°äº†
    77.2%ï¼ˆGPT-4 ä¸ºçº¦ 82.4%ï¼‰ã€‚æ­¤å¤–ï¼Œå¯¹äºè¯†åˆ«ç”¨æˆ·åè®®ä¸­çš„â€œä¸å…¬å¹³â€æ¡æ¬¾ç­‰ä»»åŠ¡ï¼Œ**SLM ç”šè‡³åœ¨ F1 æŒ‡æ ‡ä¸Šè¶…è¶Šäº† GPT-3.5 å’Œ GPT-4**ã€‚'
- en: '**Mathematical Tasks:** [Research by Google DeepMind shows](https://arxiv.org/pdf/2408.16737)
    that training a small model, Gemma2â€“9B, on data generated by another small model
    yields better results than training on data from the larger Gemma2â€“27B. Smaller
    models tend to focus better on specifics without the tendency to â€œtrying to shine
    with all the knowledgeâ€, which is often a trait of larger models.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°å­¦ä»»åŠ¡ï¼š** [Google DeepMind çš„ç ”ç©¶è¡¨æ˜](https://arxiv.org/pdf/2408.16737)ï¼Œè®­ç»ƒä¸€ä¸ªå°å‹æ¨¡å‹
    Gemma2â€“9Bï¼Œä½¿ç”¨å¦ä¸€ä¸ªå°å‹æ¨¡å‹ç”Ÿæˆçš„æ•°æ®ï¼Œæ¯”ä½¿ç”¨æ¥è‡ªæ›´å¤§æ¨¡å‹ Gemma2â€“27B çš„æ•°æ®è®­ç»ƒæ•ˆæœæ›´å¥½ã€‚è¾ƒå°çš„æ¨¡å‹å¾€å¾€èƒ½æ›´å¥½åœ°ä¸“æ³¨äºç»†èŠ‚ï¼Œè€Œä¸ä¼šåƒå¤§å‹æ¨¡å‹é‚£æ ·â€œè¯•å›¾é€šè¿‡æ‰€æœ‰çŸ¥è¯†æ¥æ˜¾ç¤ºè‡ªå·±â€ã€‚'
- en: '**Content Moderation:** [LLaMA 3.1 8B outperformed](https://arxiv.org/pdf/2410.13155)
    GPT-3.5 in accuracy (by 11.5%) and recall (by 25.7%) when moderating content across
    15 popular subreddits. **This was achieved even with 4-bit quantization**, which
    further reduces the modelâ€™s size.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å†…å®¹å®¡æ ¸ï¼š**[LLaMA 3.1 8Bè¶…è¶Šäº†](https://arxiv.org/pdf/2410.13155)GPT-3.5ï¼Œåœ¨å‡†ç¡®æ€§ï¼ˆæé«˜äº†11.5%ï¼‰å’Œå¬å›ç‡ï¼ˆæé«˜äº†25.7%ï¼‰æ–¹é¢è¡¨ç°æ›´å¥½ï¼Œå°¤å…¶æ˜¯åœ¨å®¡æ ¸15ä¸ªçƒ­é—¨å­ç‰ˆå—çš„å†…å®¹æ—¶ã€‚**å³ä½¿åœ¨4ä½é‡åŒ–çš„æƒ…å†µä¸‹**ï¼Œä¹ŸæˆåŠŸå‡å°‘äº†æ¨¡å‹çš„å¤§å°ã€‚'
- en: '![](../Images/67a12f7e7296b11c505a401955b85fa3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67a12f7e7296b11c505a401955b85fa3.png)'
- en: Comparison of instruction-tuned domain SLMs for QA and LLMs on PubMedQA. [Source](https://arxiv.org/pdf/2411.03350).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: é’ˆå¯¹PubMedQAçš„æŒ‡ä»¤è°ƒä¼˜é¢†åŸŸSLMä¸LLMçš„æ¯”è¾ƒã€‚[æ¥æº](https://arxiv.org/pdf/2411.03350)ã€‚
- en: 'Iâ€™ll go a step further and share that even classic NLP approaches often work
    surprisingly well. Let me share a personal case: Iâ€™m working on a product for
    psychological support where we process over a thousand messages from users every
    day. They can write in a chat and get a response. Each message is first classified
    into one of four categories:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æ›´è¿›ä¸€æ­¥ï¼Œåˆ†äº«ä¸€ä¸ªä¸ªäººæ¡ˆä¾‹ï¼šæˆ‘æ­£åœ¨å¼€å‘ä¸€ä¸ªå¿ƒç†æ”¯æŒäº§å“ï¼Œæˆ‘ä»¬æ¯å¤©å¤„ç†æ¥è‡ªç”¨æˆ·çš„è¶…è¿‡ä¸€åƒæ¡æ¶ˆæ¯ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡èŠå¤©å‘é€æ¶ˆæ¯å¹¶è·å¾—å›å¤ã€‚æ¯æ¡æ¶ˆæ¯é¦–å…ˆä¼šè¢«åˆ†ç±»åˆ°ä»¥ä¸‹å››ç±»ä¹‹ä¸€ï¼š
- en: '![](../Images/a756c82fcba8cae98609aba545fb141c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a756c82fcba8cae98609aba545fb141c.png)'
- en: Message Classification Scheme.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶ˆæ¯åˆ†ç±»æ–¹æ¡ˆã€‚
- en: '`SUPPORT` â€” A question about how the app works; we respond using the documentation.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SUPPORT` â€” å…³äºåº”ç”¨ç¨‹åºå¦‚ä½•å·¥ä½œçš„æé—®ï¼›æˆ‘ä»¬é€šè¿‡æ–‡æ¡£è¿›è¡Œå›ç­”ã€‚'
- en: '`GRATITUDE` â€” The user thanks the bot; we simply send a â€œlike.â€'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GRATITUDE` â€” ç”¨æˆ·æ„Ÿè°¢æœºå™¨äººï¼›æˆ‘ä»¬åªéœ€å‘é€ä¸€ä¸ªâ€œç‚¹èµâ€ã€‚'
- en: '`TRY_TO_HACK` â€” The user requests something unrelated to the appâ€™s purpose
    (e.g., â€œWrite a function in Pythonâ€).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TRY_TO_HACK` â€” ç”¨æˆ·è¯·æ±‚çš„å†…å®¹ä¸åº”ç”¨ç¨‹åºçš„ç›®çš„æ— å…³ï¼ˆä¾‹å¦‚ï¼Œâ€œå†™ä¸€ä¸ªPythonå‡½æ•°â€ï¼‰ã€‚'
- en: '`OTHER`â€” All other messages, which we process further.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OTHER` â€” æ‰€æœ‰å…¶ä»–æ¶ˆæ¯ï¼Œæˆ‘ä»¬ä¼šè¿›ä¸€æ­¥å¤„ç†ã€‚'
- en: 'Previously, I used GPT-3.5-turbo for classification and later switched to GPT-4o
    mini, spending a lot of time changing the prompt. However, I still encountered
    errors. So, I decided to try a classic approach: TF-IDF + a simple classifier.
    Training took less than a minute, and the Macro F1 score increased to 0.95 (compared
    to 0.92 for GPT-4o mini). The model size is just 76 MB, and when applied to 2
    million processed messages (our actual data), the cost savings were significant:
    the **GPT-based solution would have cost about $500, while the classic approach
    cost almost nothing**.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰ï¼Œæˆ‘ä½¿ç”¨äº†GPT-3.5-turboè¿›è¡Œåˆ†ç±»ï¼Œåæ¥åˆ‡æ¢åˆ°GPT-4o miniï¼ŒèŠ±è´¹äº†å¤§é‡æ—¶é—´æ›´æ”¹æç¤ºã€‚ä½†æ˜¯ï¼Œæˆ‘ä»ç„¶é‡åˆ°é”™è¯¯ã€‚å› æ­¤ï¼Œæˆ‘å†³å®šå°è¯•ç»å…¸æ–¹æ³•ï¼šTF-IDF
    + ç®€å•åˆ†ç±»å™¨ã€‚è®­ç»ƒæ—¶é—´ä¸åˆ°ä¸€åˆ†é’Ÿï¼Œå®è§‚F1å¾—åˆ†æé«˜åˆ°äº†0.95ï¼ˆç›¸æ¯”ä¹‹ä¸‹ï¼ŒGPT-4o miniä¸º0.92ï¼‰ã€‚æ¨¡å‹å¤§å°ä»…ä¸º76MBï¼Œå½“åº”ç”¨äº200ä¸‡æ¡å¤„ç†è¿‡çš„æ¶ˆæ¯ï¼ˆæˆ‘ä»¬çš„å®é™…æ•°æ®ï¼‰æ—¶ï¼ŒèŠ‚çœçš„æˆæœ¬éå¸¸å¯è§‚ï¼š**åŸºäºGPTçš„è§£å†³æ–¹æ¡ˆå¤§çº¦éœ€è¦500ç¾å…ƒï¼Œè€Œç»å…¸æ–¹æ³•å‡ ä¹ä¸éœ€è¦è´¹ç”¨**ã€‚
- en: '![](../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/299ede5a52ee02c3ce6ef99f7f448dc2.png)'
- en: 'Accuracy, speed and cost comparison table: GPT-4o mini vs TF-IDF model.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®æ€§ã€é€Ÿåº¦å’Œæˆæœ¬æ¯”è¾ƒè¡¨ï¼šGPT-4o miniä¸TF-IDFæ¨¡å‹ã€‚
- en: And there are several such â€œsmallâ€ and simple tasks in our product. I believe
    you might find the same in your company. Of course, large models are great for
    a quick start, especially when thereâ€™s no labeled data and requirements are changing.
    But for well-defined, stable tasks where accuracy and minimal costs are key, specialized
    and simple models (including classic methods) can often be a more effective solution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„äº§å“ä¸­æœ‰è®¸å¤šè¿™æ ·çš„â€œå°â€ä¸”ç®€å•çš„ä»»åŠ¡ã€‚æˆ‘ç›¸ä¿¡ä½ ä»¬çš„å…¬å¸ä¹Ÿå¯èƒ½ä¼šå‘ç°ç±»ä¼¼çš„æƒ…å†µã€‚å½“ç„¶ï¼Œå¤§æ¨¡å‹é€‚åˆå¿«é€Ÿå¯åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æ²¡æœ‰æ ‡æ³¨æ•°æ®ä¸”éœ€æ±‚ä¸æ–­å˜åŒ–çš„æƒ…å†µä¸‹ã€‚ä½†å¯¹äºé‚£äº›å®šä¹‰æ˜ç¡®ã€ç¨³å®šçš„ä»»åŠ¡ï¼Œå‡†ç¡®æ€§å’Œæœ€å°æˆæœ¬æ˜¯å…³é”®ï¼Œä¸“ç”¨ä¸”ç®€å•çš„æ¨¡å‹ï¼ˆåŒ…æ‹¬ç»å…¸æ–¹æ³•ï¼‰å¾€å¾€èƒ½æä¾›æ›´æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚
- en: ğŸ’¡**Hint:** use LLMs for prototyping, and then, once the task becomes clear and
    stable, switch to smaller, cheaper, and more accurate models. This hybrid approach
    helps maintain high quality, significantly reduce costs, and avoid the redundancy
    of general-purpose models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡**æç¤ºï¼š**ä½¿ç”¨LLMè¿›è¡ŒåŸå‹è®¾è®¡ï¼Œå½“ä»»åŠ¡å˜å¾—æ¸…æ™°ä¸”ç¨³å®šæ—¶ï¼Œå†åˆ‡æ¢åˆ°æ›´å°ã€æ›´ä¾¿å®œã€æ›´ç²¾ç¡®çš„æ¨¡å‹ã€‚è¿™ç§æ··åˆæ–¹æ³•æœ‰åŠ©äºä¿æŒé«˜è´¨é‡ï¼Œæ˜¾è‘—é™ä½æˆæœ¬ï¼Œå¹¶é¿å…é€šç”¨æ¨¡å‹çš„å†—ä½™ã€‚
- en: Security, Privacy and Regulatory
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®‰å…¨æ€§ã€éšç§å’Œåˆè§„æ€§
- en: Using LLMs through APIs, youâ€™re handing over sensitive data to external providers,
    increasing the risk of leaks and complicating compliance with strict regulations
    like HIPAA, GDPR, and CCPA. OpenAIâ€™s recent announcement about plans to introduce
    advertising only highlights these risks. **Your company not only loses full control
    over its data but also becomes dependent on third-party SLAs.**
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡APIä½¿ç”¨LLMsï¼Œä½ å®é™…ä¸Šæ˜¯å°†æ•æ„Ÿæ•°æ®äº¤ç»™å¤–éƒ¨æä¾›å•†ï¼Œå¢åŠ äº†æ•°æ®æ³„æ¼çš„é£é™©ï¼Œå¹¶ä½¿å¾—éµå®ˆä¸¥æ ¼æ³•è§„ï¼ˆå¦‚HIPAAã€GDPRå’ŒCCPAï¼‰å˜å¾—æ›´åŠ å¤æ‚ã€‚OpenAIæœ€è¿‘å…³äºå¼•å…¥å¹¿å‘Šçš„å…¬å‘Šè¿›ä¸€æ­¥å‡¸æ˜¾äº†è¿™äº›é£é™©ã€‚**ä½ çš„å…¬å¸ä¸ä»…å¤±å»äº†å¯¹æ•°æ®çš„å®Œå…¨æ§åˆ¶ï¼Œè¿˜å˜å¾—ä¾èµ–äºç¬¬ä¸‰æ–¹çš„æœåŠ¡æ°´å¹³åè®®ï¼ˆSLAï¼‰ã€‚**
- en: Certainly, itâ€™s possible to run a LLM locally, but the cost of deployment and
    scaling (hundreds of gigabytes of memory, multiple GPUs) often exceeds reasonable
    economic limits and makes it difficult to quickly adapt to new regulatory requirements.
    And you can forget about launching it on low-end hardware.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¯ä»¥åœ¨æœ¬åœ°è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä½†å…¶éƒ¨ç½²å’Œæ‰©å±•çš„æˆæœ¬ï¼ˆæ•°ç™¾GBçš„å†…å­˜ã€å¤šä¸ªGPUï¼‰å¾€å¾€è¶…å‡ºåˆç†çš„ç»æµé™åˆ¶ï¼Œå¹¶ä¸”ä½¿å¾—å¿«é€Ÿé€‚åº”æ–°çš„ç›‘ç®¡è¦æ±‚å˜å¾—å›°éš¾ã€‚ä½ å¯ä»¥å¿˜è®°åœ¨ä½ç«¯ç¡¬ä»¶ä¸Šå¯åŠ¨å®ƒã€‚
- en: '![](../Images/99295d3081eef53a84bd486fdd456dd9.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99295d3081eef53a84bd486fdd456dd9.png)'
- en: Comparison of Cloud API Risks and on-device slm benefits.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: äº‘APIé£é™©ä¸è®¾å¤‡ä¸ŠSLMä¼˜åŠ¿æ¯”è¾ƒ
- en: 'And this is where the â€œsmall guysâ€ come back into play:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ­£æ˜¯â€œå°å‹æ¨¡å‹â€é‡æ–°å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ï¼š
- en: 1\. Simplified Audits
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. ç®€åŒ–å®¡è®¡
- en: The smaller size of SLMs lowers the barrier for conducting audits, verification,
    and customization to meet specific regulations. Itâ€™s easier to understand how
    the model processes data, implement your own encryption or logging, and show auditors
    that information never leaves a trusted environment. As the founder of a healthcare
    company, I know how challenging and crucial this task can be.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: SLMè¾ƒå°çš„ä½“ç§¯é™ä½äº†è¿›è¡Œå®¡è®¡ã€éªŒè¯å’Œå®šåˆ¶ä»¥æ»¡è¶³ç‰¹å®šæ³•è§„çš„é—¨æ§›ã€‚æ›´å®¹æ˜“ç†è§£æ¨¡å‹å¦‚ä½•å¤„ç†æ•°æ®ï¼Œå®ç°è‡ªå®šä¹‰åŠ å¯†æˆ–æ—¥å¿—è®°å½•ï¼Œå¹¶å‘å®¡è®¡å‘˜å±•ç¤ºä¿¡æ¯ä»æœªç¦»å¼€è¿‡å—ä¿¡ä»»çš„ç¯å¢ƒã€‚ä½œä¸ºä¸€å®¶åŒ»ç–—å…¬å¸åˆ›å§‹äººï¼Œæˆ‘æ·±çŸ¥è¿™ä¸€ä»»åŠ¡æœ‰å¤šä¹ˆå…·æœ‰æŒ‘æˆ˜æ€§å’Œé‡è¦æ€§ã€‚
- en: '**2\.** Running on Isolated and low-end hardware'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2.** åœ¨éš”ç¦»å’Œä½ç«¯ç¡¬ä»¶ä¸Šè¿è¡Œ'
- en: 'LLMs are difficult to efficiently â€œdeployâ€ in an isolated network segment or
    on a smartphone. SLMs, however, with their lower computational requirements, can
    operate almost anywhere: from a local server in a private network to a doctorâ€™s
    or inspectorâ€™s device. [According to IDC](https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/)
    forecasts, **by 2028, over 900 million smartphones will be capable of running
    generative AI models locally**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨éš”ç¦»çš„ç½‘ç»œæ®µæˆ–æ™ºèƒ½æ‰‹æœºä¸Šé«˜æ•ˆâ€œéƒ¨ç½²â€ååˆ†å›°éš¾ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°å…¶è¾ƒä½çš„è®¡ç®—éœ€æ±‚ï¼Œç®€åŒ–è¯­è¨€æ¨¡å‹ï¼ˆSLMsï¼‰å‡ ä¹å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹è¿è¡Œï¼šä»ç§æœ‰ç½‘ç»œä¸­çš„æœ¬åœ°æœåŠ¡å™¨åˆ°åŒ»ç”Ÿæˆ–æ£€æŸ¥å‘˜çš„è®¾å¤‡ã€‚[æ ¹æ®IDCçš„é¢„æµ‹](https://blogs.idc.com/2024/07/05/the-rise-of-gen-ai-smartphones/)ï¼Œ**åˆ°2028å¹´ï¼Œè¶…è¿‡9äº¿éƒ¨æ™ºèƒ½æ‰‹æœºå°†èƒ½å¤Ÿæœ¬åœ°è¿è¡Œç”Ÿæˆæ€§AIæ¨¡å‹**ã€‚
- en: '**3\.** New Regulations Updates and Adaptation'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3.** æ–°æ³•è§„æ›´æ–°ä¸é€‚åº”'
- en: Regulations and laws change frequently â€” compact models can be fine-tuned or
    adjusted in hours rather than days. This enables a quick response to new requirements
    without the need for large-scale infrastructure upgrades, which are typical for
    big LLMs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ³•è§„å’Œæ³•å¾‹ç»å¸¸å˜åŒ–â€”â€”ç´§å‡‘å‹æ¨¡å‹å¯ä»¥åœ¨æ•°å°æ—¶å†…è¿›è¡Œå¾®è°ƒæˆ–è°ƒæ•´ï¼Œè€Œä¸æ˜¯å‡ å¤©ã€‚è¿™ä½¿å¾—èƒ½å¤Ÿå¿«é€Ÿå“åº”æ–°çš„è¦æ±‚ï¼Œè€Œä¸éœ€è¦è¿›è¡Œå¤§è§„æ¨¡çš„åŸºç¡€è®¾æ–½å‡çº§ï¼Œè¿™æ˜¯å¤§è§„æ¨¡LLMsçš„å¸¸è§ç‰¹å¾ã€‚
- en: '**4\. Distributed Security Architecture**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**4. åˆ†å¸ƒå¼å®‰å…¨æ¶æ„**'
- en: 'Unlike the monolithic architecture of LLMs, where all security components are
    â€œbakedâ€ into one large model, SLMs enable the creation of a distributed security
    system. Each component:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸LLMsçš„å•ä½“æ¶æ„ä¸åŒï¼ŒLLMså°†æ‰€æœ‰å®‰å…¨ç»„ä»¶â€œé›†æˆâ€åˆ°ä¸€ä¸ªå¤§å‹æ¨¡å‹ä¸­ï¼Œè€ŒSLMsåˆ™å…è®¸åˆ›å»ºä¸€ä¸ªåˆ†å¸ƒå¼å®‰å…¨ç³»ç»Ÿã€‚æ¯ä¸ªç»„ä»¶ï¼š
- en: Specializes in a specific task.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸“æ³¨äºç‰¹å®šä»»åŠ¡ã€‚
- en: Can be independently updated and tested.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯ä»¥ç‹¬ç«‹æ›´æ–°å’Œæµ‹è¯•ã€‚
- en: Scales separately from the others.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–ç»„ä»¶åˆ†å¼€æ‰©å±•ã€‚
- en: 'For example, a medical application could use a cascade of three models:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸€ä¸ªåŒ»ç–—åº”ç”¨å¯ä»¥ä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹çš„çº§è”ï¼š
- en: '**Privacy Guardian (2B)** â€” masks personal data.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éšç§å®ˆæŠ¤è€…ï¼ˆ2Bï¼‰**â€”â€”æ©ç›–ä¸ªäººæ•°æ®ã€‚'
- en: '**Medical Validator (3B)** â€” ensures medical accuracy.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åŒ»å­¦éªŒè¯å™¨ï¼ˆ3Bï¼‰**â€”â€”ç¡®ä¿åŒ»å­¦å‡†ç¡®æ€§ã€‚'
- en: '**Compliance Checker (1B)** â€” monitors HIPAA compliance.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**åˆè§„æ£€æŸ¥å™¨ï¼ˆ1Bï¼‰**â€”â€”ç›‘æ§HIPAAåˆè§„æ€§ã€‚'
- en: '**Smaller models are easier to verify and update**, making the overall architecture
    more flexible and reliable.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾ƒå°çš„æ¨¡å‹æ›´å®¹æ˜“éªŒè¯å’Œæ›´æ–°**ï¼Œä½¿æ•´ä½“æ¶æ„æ›´åŠ çµæ´»å’Œå¯é ã€‚'
- en: '![](../Images/c73556307d7ff9a0f482dd5e882ed0da.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c73556307d7ff9a0f482dd5e882ed0da.png)'
- en: Comparison of Data Privacy Features.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®éšç§ç‰¹æ€§æ¯”è¾ƒ
- en: ğŸ’¡**Hint:** consider using SLMs if you operate in a heavily regulated field.
    Pay close attention to data transfer policies and the frequency of changes in
    the regulatory landscape. I recommend use SLMs if your professional domain is
    in healthcare, finance, or law.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡**æç¤ºï¼š**å¦‚æœä½ æ‰€åœ¨çš„é¢†åŸŸå—åˆ°ä¸¥æ ¼ç›‘ç®¡ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨SLMsã€‚å¯†åˆ‡å…³æ³¨æ•°æ®ä¼ è¾“æ”¿ç­–å’Œç›‘ç®¡ç¯å¢ƒå˜åŒ–çš„é¢‘ç‡ã€‚å¦‚æœä½ çš„ä¸“ä¸šé¢†åŸŸæ˜¯åŒ»ç–—ã€é‡‘èæˆ–æ³•å¾‹ï¼Œæˆ‘æ¨èä½¿ç”¨SLMsã€‚
- en: 'AI Agents: The Perfect Use Case'
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AIä»£ç†ï¼šå®Œç¾çš„åº”ç”¨åœºæ™¯
- en: Remember the old [Unix philosophy, â€œDo one thing and do it wellâ€](https://en.wikipedia.org/wiki/Unix_philosophy)?
    It seems weâ€™re returning to this principle, now in the context of AI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜è®°å¾—æ—§çš„[Unixå“²å­¦ï¼šâ€œåšä¸€ä»¶äº‹ï¼Œå¹¶æŠŠå®ƒåšå¾—å¾ˆå¥½â€](https://en.wikipedia.org/wiki/Unix_philosophy)å—ï¼Ÿç°åœ¨ä¼¼ä¹æˆ‘ä»¬åˆå›åˆ°äº†è¿™ä¸€åŸåˆ™ï¼Œåªä¸è¿‡æ˜¯åœ¨äººå·¥æ™ºèƒ½çš„èƒŒæ™¯ä¸‹ã€‚
- en: Ilya Sutskeverâ€™s recent statement at NeurIPS that â€œPre-training as we know it
    will unquestionably endâ€ and that the next generation of models will be â€œagentic
    in real waysâ€ only confirms this trend. Y Combinator goes even further, predicting
    that [**AI agents could create a market 10 times larger than SaaS**](https://www.youtube.com/watch?v=ASABxNenD_U).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Ilya Sutskeveræœ€è¿‘åœ¨NeurIPSä¸Šçš„å£°æ˜â€”â€”â€œæˆ‘ä»¬æ‰€çŸ¥é“çš„é¢„è®­ç»ƒå°†æ¯«æ— ç–‘é—®åœ°ç»“æŸâ€ä»¥åŠä¸‹ä»£æ¨¡å‹å°†â€œä»¥çœŸå®æ–¹å¼å…·å¤‡ä»£ç†èƒ½åŠ›â€â€”â€”åªä¼šè¿›ä¸€æ­¥ç¡®è®¤è¿™ä¸€è¶‹åŠ¿ã€‚Y
    Combinatorç”šè‡³é¢„æµ‹ï¼Œ[**AIä»£ç†å¯èƒ½åˆ›é€ ä¸€ä¸ªæ¯”SaaSå¸‚åœºå¤§10å€çš„å¸‚åœº**](https://www.youtube.com/watch?v=ASABxNenD_U)ã€‚
- en: For example, already [12% of enterprise solutions use agent-based architecture](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/).
    Moreover, analysts predict that agents will be the next wave of AI-transformation
    that can affect not only the $400-billion software market, but also the **$10-trillion
    U.S. services economy**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œç›®å‰å·²æœ‰[12%çš„ä¼ä¸šè§£å†³æ–¹æ¡ˆé‡‡ç”¨åŸºäºä»£ç†çš„æ¶æ„](https://menlovc.com/2024-the-state-of-generative-ai-in-the-enterprise/)ã€‚æ­¤å¤–ï¼Œåˆ†æå¸ˆé¢„æµ‹ï¼Œä»£ç†å°†æˆä¸ºAIè½¬å‹çš„ä¸‹ä¸€ä¸ªæµªæ½®ï¼Œä¸ä»…èƒ½å½±å“4000äº¿ç¾å…ƒçš„è½¯ä»¶å¸‚åœºï¼Œè¿˜èƒ½å½±å“**10ä¸‡äº¿ç¾å…ƒçš„ç¾å›½æœåŠ¡ç»æµ**ã€‚
- en: And SMLs are ideal candidates for this role. Perhaps one model is quite limited,
    but a swarm of such models â€” can solve complex tasks piece by piece. **Faster,
    higher quality and cheaper.**
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è€ŒSMLsæ˜¯è¿™ä¸ªè§’è‰²çš„ç†æƒ³å€™é€‰è€…ã€‚ä¹Ÿè®¸å•ä¸ªæ¨¡å‹éå¸¸æœ‰é™ï¼Œä½†è¿™æ ·çš„å¤šä¸ªæ¨¡å‹â€”â€”å¯ä»¥é€æ­¥è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚**æ›´å¿«ã€æ›´é«˜è´¨é‡ã€æ›´ä¾¿å®œã€‚**
- en: 'Letâ€™s take a concrete example: imagine you are building a system to analyze
    financial documents. Instead of using one large model, you can break the task
    into several specialized agents:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼šå‡è®¾ä½ æ­£åœ¨æ„å»ºä¸€ä¸ªç³»ç»Ÿæ¥åˆ†æé‡‘èæ–‡æ¡£ã€‚ä½ å¯ä»¥å°†ä»»åŠ¡æ‹†åˆ†æˆå¤šä¸ªä¸“é—¨çš„ä»£ç†ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸€ä¸ªå¤§æ¨¡å‹ï¼š
- en: '![](../Images/b9593e6888e27316001c963ec8232ee1.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9593e6888e27316001c963ec8232ee1.png)'
- en: The example flow of information between specialized agents.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸“é—¨ä»£ç†ä¹‹é—´çš„ä¿¡æ¯æµåŠ¨ç¤ºä¾‹ã€‚
- en: 'And this approach is not only more cost-effective but also more reliable: each
    agent focuses on what it does best. **Cheaper. Faster. Better.** Yes, Iâ€™m repeating
    it again.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œè¿™ç§æ–¹æ³•ä¸ä»…æ›´å…·æˆæœ¬æ•ˆç›Šï¼Œè¿˜æ›´å¯é ï¼šæ¯ä¸ªä»£ç†ä¸“æ³¨äºè‡ªå·±æœ€æ“…é•¿çš„é¢†åŸŸã€‚**æ›´ä¾¿å®œã€æ›´å¿«ã€æ›´å¥½ã€‚**æ˜¯çš„ï¼Œæˆ‘å†æ¬¡é‡å¤ã€‚
- en: 'To back this up, let me name a few companies:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¯æŒè¿™ä¸€ç‚¹ï¼Œè®©æˆ‘åˆ—ä¸¾å‡ ä¸ªå…¬å¸ï¼š
- en: '[**H Company**](https://www.hcompany.ai/) raised $100M in a seed round to develop
    a multi-agent system based on SLMs (2â€“3B parameters). Their agent Runner H (3B)
    achieves 67% task completion success compared to Anthropicâ€™s Computer Use at 52%,
    all **with significantly lower costs**.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Hå…¬å¸**](https://www.hcompany.ai/)åœ¨ç§å­è½®èèµ„ä¸­ç­¹é›†äº†1äº¿ç¾å…ƒï¼Œæ—¨åœ¨å¼€å‘åŸºäºSLMsçš„å¤šä»£ç†ç³»ç»Ÿï¼ˆ2-3Bå‚æ•°ï¼‰ã€‚ä»–ä»¬çš„ä»£ç†Runner
    Hï¼ˆ3Bï¼‰å®Œæˆä»»åŠ¡çš„æˆåŠŸç‡ä¸º67%ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼ŒAnthropicçš„Computer Useä¸º52%ï¼Œ**ä¸”æˆæœ¬æ˜¾è‘—æ›´ä½**ã€‚'
- en: '[**Liquid AI**](https://www.liquid.ai/) recently secured $250M in funding,
    focusing on building efficient enterprise models. Their model (1.3B parameters)
    has outperformed all existing models of similar size. Meanwhile, their LFM-3B
    delivers performance on par with 7B and even 13B models **while requiring less
    memory.**'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Liquid AI**](https://www.liquid.ai/)æœ€è¿‘è·å¾—äº†2.5äº¿ç¾å…ƒçš„èèµ„ï¼Œä¸“æ³¨äºæ„å»ºé«˜æ•ˆçš„ä¼ä¸šæ¨¡å‹ã€‚ä»–ä»¬çš„æ¨¡å‹ï¼ˆ1.3Bå‚æ•°ï¼‰è¶…è¶Šäº†æ‰€æœ‰ç±»ä¼¼è§„æ¨¡çš„ç°æœ‰æ¨¡å‹ã€‚åŒæ—¶ï¼Œä»–ä»¬çš„LFM-3Båœ¨æ€§èƒ½ä¸Šä¸7Bç”šè‡³13Bæ¨¡å‹ä¸ç›¸ä¸Šä¸‹ï¼Œ**åŒæ—¶éœ€è¦æ›´å°‘çš„å†…å­˜**ã€‚'
- en: '[**Cohere**](https://cohere.com/) launched Command R7B, a specialized model
    for RAG applications that can even **run on a CPU**. The model supports 23 languages
    and integrates with external tools, showing best-in-class results for reasoning
    and question-answering tasks.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**Cohere**](https://cohere.com/)æ¨å‡ºäº†Command R7Bï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨é’ˆå¯¹RAGåº”ç”¨çš„æ¨¡å‹ï¼Œç”šè‡³å¯ä»¥**åœ¨CPUä¸Šè¿è¡Œ**ã€‚è¯¥æ¨¡å‹æ”¯æŒ23ç§è¯­è¨€ï¼Œå¹¶ä¸å¤–éƒ¨å·¥å…·é›†æˆï¼Œåœ¨æ¨ç†å’Œé—®ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºæœ€é¡¶çº§çš„ç»“æœã€‚'
- en: '**YOUR COMPANY NAME** could also join this list. Iâ€™m not just saying that â€”
    in [Reforma Health](https://reforma.health/), the company Iâ€™m working on, is developing
    specialized SLMs for various medical domains. This decision was driven by the
    need to comply with HIPAA requirements and the specifics of medical information
    processing. Our experience shows that highly **specialized SLMs can be a significant
    competitive advantage**, especially in regulated domains.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è´µå…¬å¸åç§°**ä¹Ÿå¯ä»¥åŠ å…¥è¿™ä¸ªåå•ã€‚æˆ‘ä¸ä»…ä»…æ˜¯è¿™ä¹ˆè¯´çš„â€”â€”åœ¨æˆ‘å·¥ä½œçš„å…¬å¸[Reforma Health](https://reforma.health/)ï¼Œæˆ‘ä»¬æ­£åœ¨å¼€å‘é’ˆå¯¹å„ç§åŒ»ç–—é¢†åŸŸçš„ä¸“ç”¨SLMã€‚åšå‡ºè¿™ä¸€å†³å®šæ˜¯å› ä¸ºéœ€è¦éµå®ˆHIPAAè§„å®šä»¥åŠåŒ»ç–—ä¿¡æ¯å¤„ç†çš„å…·ä½“è¦æ±‚ã€‚æˆ‘ä»¬çš„ç»éªŒè¡¨æ˜ï¼Œé«˜åº¦**ä¸“ç”¨çš„SLMå¯ä»¥æˆä¸ºæ˜¾è‘—çš„ç«äº‰ä¼˜åŠ¿**ï¼Œå°¤å…¶æ˜¯åœ¨å—ç›‘ç®¡çš„é¢†åŸŸã€‚'
- en: 'These examples highlight the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¾‹å­çªæ˜¾äº†ä»¥ä¸‹å‡ ç‚¹ï¼š
- en: '**Investors believe** in the future of specialized small models.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æŠ•èµ„è€…ç›¸ä¿¡**ä¸“ç”¨å°å‹æ¨¡å‹çš„æœªæ¥ã€‚'
- en: '**Enterprise clients are willing to pay** for efficient solutions that donâ€™t
    require sending data to external providers.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼ä¸šå®¢æˆ·æ„¿æ„æ”¯ä»˜**é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸éœ€è¦å°†æ•°æ®å‘é€ç»™å¤–éƒ¨ä¾›åº”å•†ã€‚'
- en: The market is shifting towards **â€œsmartâ€ specialized agents** instead of relying
    on â€œuniversalâ€ large models.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¸‚åœºæ­£åœ¨è½¬å‘**â€œæ™ºèƒ½â€ä¸“ç”¨ä»£ç†**ï¼Œè€Œä¸å†ä¾èµ–äºâ€œé€šç”¨â€å¤§å‹æ¨¡å‹ã€‚
- en: ğŸ’¡**Hint:** start by identifying repetitive tasks in your project. These are
    the best candidates for developing specialized SLM agents. This approach will
    help you avoid overpaying for the excessive power of LLMs and achieve greater
    control over the process.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡**æç¤ºï¼š**ä»è¯†åˆ«é¡¹ç›®ä¸­çš„é‡å¤ä»»åŠ¡å¼€å§‹ã€‚è¿™äº›ä»»åŠ¡æœ€é€‚åˆå¼€å‘ä¸“ç”¨çš„SLMä»£ç†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œä½ å°†é¿å…ä¸ºLLMçš„è¿‡åº¦èƒ½åŠ›æ”¯ä»˜è¿‡å¤šè´¹ç”¨ï¼Œå¹¶èƒ½æ›´å¥½åœ°æŒæ§æµç¨‹ã€‚
- en: Potential Limitations of SLMs Compared to LLMs
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SLMä¸LLMç›¸æ¯”çš„æ½œåœ¨å±€é™æ€§
- en: Although Iâ€™ve spent this entire article praising small models, itâ€™s fair to
    point out their limitations as well.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘åœ¨æ•´ç¯‡æ–‡ç« ä¸­éƒ½åœ¨èµæ‰¬å°å‹æ¨¡å‹ï¼Œä½†å…¬å¹³åœ°è¯´ï¼Œä¹Ÿåº”æŒ‡å‡ºå®ƒä»¬çš„å±€é™æ€§ã€‚
- en: 1\. Limited Task Flexibility
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. ä»»åŠ¡çµæ´»æ€§çš„å±€é™
- en: The most significant limitation of SLMs is their narrow specialization. Unlike
    LLMs, which can handle a wide range of tasks, SLMs succeed only in the specific
    tasks for which they have been trained. For example, in medicine, [Diabetica-7B
    outperformed LLMs](https://arxiv.org/pdf/2409.13191) in diabetes-related tests,
    but other medical disciplines required additional fine-tuning or a new architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: SLMçš„æœ€å¤§é™åˆ¶æ˜¯å…¶ç‹­çª„çš„ä¸“ç”¨æ€§ã€‚ä¸å¯ä»¥å¤„ç†å¹¿æ³›ä»»åŠ¡çš„LLMä¸åŒï¼ŒSLMä»…åœ¨å®ƒä»¬ç»è¿‡è®­ç»ƒçš„ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»å­¦é¢†åŸŸï¼Œ[Diabetica-7Båœ¨ç³–å°¿ç—…ç›¸å…³æµ‹è¯•ä¸­ä¼˜äºLLM](https://arxiv.org/pdf/2409.13191)ï¼Œä½†å…¶ä»–åŒ»å­¦é¢†åŸŸåˆ™éœ€è¦é¢å¤–çš„å¾®è°ƒæˆ–å…¨æ–°çš„æ¶æ„ã€‚
- en: '![](../Images/22382712a4e184c3025a715a8ed608f2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22382712a4e184c3025a715a8ed608f2.png)'
- en: 'LLMs vs SLMs: Flexibility vs Specialization.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: LLMä¸SLMï¼šçµæ´»æ€§ä¸ä¸“ç”¨æ€§ã€‚
- en: 2\. Context Window Limitations
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. ä¸Šä¸‹æ–‡çª—å£çš„é™åˆ¶
- en: 'Unlike large models that reach up to 1M tokens ([Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)),
    SLMs have shorter contexts. Even though recent advances in small LLaMA 3.2 models
    (3B, 1B) having a context length of 128k tokens, [the effective context length
    is often not as claimed](https://arxiv.org/pdf/2410.18745): models often lose
    the â€œconnectionâ€ between the beginning and the end of the text. For example, SLMs
    cannot efficiently process voluminous medical histories of patients over several
    years or large legal documents.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¯ä»¥å¤„ç†é«˜è¾¾1M tokensçš„è¾ƒå¤§æ¨¡å‹ï¼ˆå¦‚[Gemini 2.0](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/)ï¼‰ä¸åŒï¼ŒSLMçš„ä¸Šä¸‹æ–‡é•¿åº¦è¾ƒçŸ­ã€‚å°½ç®¡è¿‘æœŸå°å‹LLaMA
    3.2æ¨¡å‹ï¼ˆ3B, 1Bï¼‰å·²å…·æœ‰128k tokensçš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œ[ä½†æœ‰æ•ˆçš„ä¸Šä¸‹æ–‡é•¿åº¦å¾€å¾€å¹¶éå¦‚å®£ä¼ æ‰€ç§°](https://arxiv.org/pdf/2410.18745)ï¼šæ¨¡å‹é€šå¸¸æ— æ³•ä¿æŒæ–‡æœ¬å¼€å¤´ä¸ç»“å°¾ä¹‹é—´çš„â€œè”ç³»â€ã€‚ä¾‹å¦‚ï¼ŒSLMæ— æ³•é«˜æ•ˆå¤„ç†å¤šå¹´çš„ç—…äººåŒ»ç–—å†å²æˆ–å¤§é‡æ³•å¾‹æ–‡ä»¶ã€‚
- en: '![](../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3bce47a45b59ed7cdc06638f6f63d2d2.png)'
- en: Comparison of maximum context length for different models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦çš„æ¯”è¾ƒã€‚
- en: 3\. Emergence Capabilities Gap
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. æ–°å…´èƒ½åŠ›å·®è·
- en: 'Many â€œemergent abilitiesâ€ only [appear when a model reaches a certain size
    threshold](https://arxiv.org/pdf/2001.08361). SLMs **typically donâ€™t hit the parameter
    levels required for advanced logical reasoning or deep contextual understanding**.
    [A study by Google Research](https://arxiv.org/pdf/2408.16737) demonstrates this
    with math word problems: while small models struggle with basic arithmetic, larger
    models suddenly demonstrate complex mathematical reasoning skills.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: è®¸å¤šâ€œæ–°å…´èƒ½åŠ›â€åªæœ‰åœ¨æ¨¡å‹è¾¾åˆ°æŸä¸ªè§„æ¨¡é—¨æ§›æ—¶æ‰ä¼š[æ˜¾ç°](https://arxiv.org/pdf/2001.08361)ã€‚SLM**é€šå¸¸æ— æ³•è¾¾åˆ°è¿›è¡Œé«˜çº§é€»è¾‘æ¨ç†æˆ–æ·±åº¦ä¸Šä¸‹æ–‡ç†è§£æ‰€éœ€çš„å‚æ•°æ°´å¹³**ã€‚[è°·æ­Œç ”ç©¶çš„ä¸€é¡¹ç ”ç©¶](https://arxiv.org/pdf/2408.16737)é€šè¿‡æ•°å­¦é—®é¢˜å±•ç¤ºäº†è¿™ä¸€ç‚¹ï¼šå°å‹æ¨¡å‹åœ¨åŸºç¡€ç®—æœ¯æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œè€Œè¾ƒå¤§æ¨¡å‹åˆ™èƒ½çªç„¶å±•ç¤ºå‡ºå¤æ‚çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚
- en: However, [recent research by Hugging Face shows](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
    that **test-time compute scaling** can partially bridge this gap. Using strategies
    like **iterative self-refinement** or employing a **reward model**, small models
    can â€œthink longerâ€ on complex problems. For example, with extended generation
    time, small models (1B and 3B) outperformed their larger counterparts (8B and
    70B) on the MATH-500 benchmark.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œ[Hugging Face æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute)
    **æµ‹è¯•æ—¶è®¡ç®—æ‰©å±•**å¯ä»¥éƒ¨åˆ†å¼¥åˆè¿™ä¸€å·®è·ã€‚é€šè¿‡ä½¿ç”¨**è¿­ä»£è‡ªæˆ‘ä¼˜åŒ–**æˆ–é‡‡ç”¨**å¥–åŠ±æ¨¡å‹**ç­‰ç­–ç•¥ï¼Œå°å‹æ¨¡å‹å¯ä»¥åœ¨å¤æ‚é—®é¢˜ä¸Šâ€œæ€è€ƒæ›´é•¿æ—¶é—´â€ã€‚ä¾‹å¦‚ï¼Œåœ¨å»¶é•¿ç”Ÿæˆæ—¶é—´çš„æƒ…å†µä¸‹ï¼Œå°å‹æ¨¡å‹ï¼ˆ1Bå’Œ3Bï¼‰åœ¨MATH-500åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡äº†å®ƒä»¬çš„å¤§å‹æ¨¡å‹ï¼ˆ8Bå’Œ70Bï¼‰ã€‚
- en: ğŸ’¡**Hint:** If you work in an environment where tasks change weekly, require
    analyzing large documents, or involve solving complex logical problems, larger
    LLMs are often more reliable and versatile.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡**æç¤ºï¼š** å¦‚æœæ‚¨åœ¨ä¸€ä¸ªä»»åŠ¡æ¯å‘¨éƒ½åœ¨å˜åŒ–ã€éœ€è¦åˆ†æå¤§é‡æ–‡æ¡£æˆ–æ¶‰åŠè§£å†³å¤æ‚é€»è¾‘é—®é¢˜çš„ç¯å¢ƒä¸­å·¥ä½œï¼Œè¾ƒå¤§çš„LLMé€šå¸¸æ›´å¯é å’Œå¤šåŠŸèƒ½ã€‚
- en: Closing thoughts
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸè¯­
- en: As with choosing between O[penAI and self-hosted LLMs in my previous article](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526),
    there is no one-size-fits-all solution here. If your task involves constant changes,
    lacks precise specialization, or requires rapid prototyping, LLMs will offer an
    easy start.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æˆ‘åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­è®¨è®ºçš„é€‰æ‹©[OpenAIä¸è‡ªæ‰˜ç®¡LLMä¹‹é—´çš„å¹³è¡¡](https://medium.com/better-programming/you-dont-need-hosted-llms-do-you-1160b2520526)ä¸€æ ·ï¼Œè¿™é‡Œä¹Ÿæ²¡æœ‰â€œä¸€åˆ€åˆ‡â€çš„è§£å†³æ–¹æ¡ˆã€‚å¦‚æœæ‚¨çš„ä»»åŠ¡æ¶‰åŠæŒç»­å˜åŒ–ã€ç¼ºä¹ç²¾ç¡®çš„ä¸“ä¸šåŒ–ï¼Œæˆ–éœ€è¦å¿«é€ŸåŸå‹å¼€å‘ï¼ŒLLMå°†æä¾›ä¸€ä¸ªè½»æ¾çš„èµ·ç‚¹ã€‚
- en: However, over time, as your goal become more clearer, moving to compact, specialized
    **SLM agents can significantly reduce costs, improve accuracy, and simplify compliance
    with regulatory requirements**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå½“æ‚¨çš„ç›®æ ‡å˜å¾—æ›´åŠ æ˜ç¡®æ—¶ï¼Œè½¬å‘ç´§å‡‘ã€ä¸“ä¸šåŒ–çš„**SLM ä»£ç†å¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬ã€æé«˜å‡†ç¡®æ€§ï¼Œå¹¶ç®€åŒ–åˆè§„æ€§è¦æ±‚**ã€‚
- en: '![](../Images/12891117c3be9a5f526a54a48e569a6d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12891117c3be9a5f526a54a48e569a6d.png)'
- en: Moving from rapid prototyping at LLM to an optimized SLM agent ecosystem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä»LLMçš„å¿«é€ŸåŸå‹å¼€å‘è½¬å‘ä¼˜åŒ–çš„SLMä»£ç†ç”Ÿæ€ç³»ç»Ÿã€‚
- en: SLMs arenâ€™t a paradigm shift for the sake of trends but a pragmatic approach
    that allows you to solve specific problems more accurately and cost-effectively
    without overpaying for unnecessary functionality. You donâ€™t need to completely
    abandon LLMs â€” **you can gradually replace only some components with SLMs** or
    even classic NLP methods. It all depends on your metrics, budget, and the nature
    of your task.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SLM å¹¶éä¸ºäº†è·Ÿéšæ½®æµè€Œæ”¹å˜èŒƒå¼ï¼Œè€Œæ˜¯ä¸€ç§åŠ¡å®çš„æ–¹æ³•ï¼Œä½¿æ‚¨èƒ½å¤Ÿæ›´å‡†ç¡®ã€ç»æµåœ°è§£å†³ç‰¹å®šé—®é¢˜ï¼Œè€Œæ— éœ€ä¸ºä¸å¿…è¦çš„åŠŸèƒ½æ”¯ä»˜è¿‡å¤šè´¹ç”¨ã€‚æ‚¨ä¸éœ€è¦å®Œå…¨æ”¾å¼ƒLLMâ€”â€”**æ‚¨å¯ä»¥é€æ­¥ç”¨SLM**æˆ–ç”šè‡³ç»å…¸çš„NLPæ–¹æ³•æ›¿æ¢å…¶ä¸­çš„æŸäº›ç»„ä»¶ã€‚æ‰€æœ‰è¿™äº›å–å†³äºæ‚¨çš„æŒ‡æ ‡ã€é¢„ç®—å’Œä»»åŠ¡çš„æ€§è´¨ã€‚
- en: 'A good example of this is IBM, which employs a [multimodel strategy](https://www.ibm.com/products/watsonx-ai/foundation-models),
    combining smaller models for different tasks. As they point out:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯ IBMï¼Œå®ƒé‡‡ç”¨äº†[å¤šæ¨¡å‹ç­–ç•¥](https://www.ibm.com/products/watsonx-ai/foundation-models)ï¼Œå°†è¾ƒå°çš„æ¨¡å‹ç”¨äºä¸åŒçš„ä»»åŠ¡ã€‚æ­£å¦‚ä»–ä»¬æ‰€æŒ‡å‡ºçš„ï¼š
- en: Bigger is not always better, as specialized models outperform general-purpose
    models with lower infrastructure requirements.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ›´å¤§å¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼Œå› ä¸ºä¸“ä¸šåŒ–æ¨¡å‹åœ¨åŸºç¡€è®¾æ–½è¦æ±‚è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œé€šå¸¸è¶…è¿‡é€šç”¨æ¨¡å‹çš„è¡¨ç°ã€‚
- en: 'In the end, the **key to success is to adapt**. Start with a large model, evaluate
    where it performs best, and then optimize your architecture to avoid overpaying
    for unnecessary capabilities and compromising data privacy. This approach allows
    you to combine the best of both worlds: the flexibility and versatility of LLMs
    during the initial stages, and the precise, cost-effective performance of SLMs
    for a mature product.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œ**æˆåŠŸçš„å…³é”®æ˜¯é€‚åº”**ã€‚ä»ä¸€ä¸ªå¤§å‹æ¨¡å‹å¼€å§‹ï¼Œè¯„ä¼°å®ƒåœ¨ä½•å¤„è¡¨ç°æœ€ä½³ï¼Œç„¶åä¼˜åŒ–æ‚¨çš„æ¶æ„ï¼Œä»¥é¿å…ä¸ºä¸å¿…è¦çš„èƒ½åŠ›æ”¯ä»˜è¿‡é«˜è´¹ç”¨ï¼Œå¹¶ç¡®ä¿æ•°æ®éšç§ä¸å—å½±å“ã€‚è¿™ç§æ–¹æ³•ä½¿æ‚¨èƒ½å¤Ÿç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼šåœ¨åˆæœŸé˜¶æ®µï¼ŒLLMçš„çµæ´»æ€§å’Œå¤šåŠŸèƒ½æ€§ï¼Œä»¥åŠåœ¨æˆç†Ÿäº§å“é˜¶æ®µï¼ŒSLMçš„ç²¾ç¡®å’Œæˆæœ¬æ•ˆç›Šã€‚
- en: If you have any questions or suggestions, feel free to connect on [LinkedIn](https://www.linkedin.com/in/sergey-savvov/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶åœ¨[LinkedIn](https://www.linkedin.com/in/sergey-savvov/)ä¸Šä¸æˆ‘è”ç³»ã€‚
- en: '***Disclaimer****: The information in the article is current as of December
    2024, but please be aware that changes may occur thereafter.*'
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***å…è´£å£°æ˜***ï¼šæœ¬æ–‡ä¸­çš„ä¿¡æ¯æˆªè‡³2024å¹´12æœˆï¼Œä½†è¯·æ³¨æ„æ­¤åå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚'
- en: ''
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…æä¾›ã€‚*'
