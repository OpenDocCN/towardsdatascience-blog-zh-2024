- en: The (lesser known) rising application of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-lesser-known-rising-application-of-llms-775834116477?source=collection_archive---------3-----------------------#2024-05-13](https://towardsdatascience.com/the-lesser-known-rising-application-of-llms-775834116477?source=collection_archive---------3-----------------------#2024-05-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vianney.mixtur_39698?source=post_page---byline--775834116477--------------------------------)[![Vianney
    Mixtur](../Images/a97e69c63123e450910f6887b7c0813d.png)](https://medium.com/@vianney.mixtur_39698?source=post_page---byline--775834116477--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--775834116477--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--775834116477--------------------------------)
    [Vianney Mixtur](https://medium.com/@vianney.mixtur_39698?source=post_page---byline--775834116477--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--775834116477--------------------------------)
    ·8 min read·May 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Large Language Models** (LLMs) are often described as **Generative Artificial
    Intelligence** (GenAI) as they indeed have the ability to generate text. The first
    popular application of LLMs were chatbots with ChatGPT leading the way. Then we
    extended their horizon to other tasks such as [**semantic search**](https://vianmixt.notion.site/Practical-Semantic-Search-with-MongoDB-and-OpenAI-451692801b41465fae1bea5f70238279)
    and **retrieval augmented generation**(RAG). Today, I want to talk about a rising
    application for LLM which is **structuring unstructed data** for which I am going
    to show you an example by structuring raw texts into JSON data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using LLMs for **data structuration and extraction** is a very promising application
    with a lot of potential. Here’s why:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved accuracy :** LLMs understand the nuances of human language. This
    allows them to identify key information within messy, unstructured text with greater
    accuracy than traditional rule-based systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation potential** : Extracting information from unstructured data can
    be a time-consuming and laborious task. LLMs can automate this process, freeing
    up human resources for other tasks and allowing for faster analysis of larger
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability and learning capabilities**: LLMs, on the other hand, can be
    continuously fine-tuned and adapted to handle new data sources and information
    types. As they are exposed to more unstructured data, they can learn and improve
    their ability to identify patterns and extract relevant information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business outcome :** A vast amount of valuable information resides within
    unstructured textual data sources like emails, customer reviews, social media
    conversations, and internal documents. However, this data is often difficult to
    analyze. LLMs can unlock this hidden potential by transforming unstructured data
    into a structured format. This allows businesses to leverage powerful analytics
    tools to identify trends, and gain insights. Essentially, by structuring unstructured
    data with LLMs, businesses can transform a liability (unusable data) into an asset
    (valuable insights) that drives better decision-making and improves overall business
    outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recently, I was searching for an open-source recipes dataset for a personal
    project but I could not find any except for [this github repository](https://github.com/ronaldlong46/public-domain-recipes)
    containing the recipes displayed on [publicdomainrecipes.com](https://publicdomainrecipes.com/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04ad1e467b2d1359d7502b35c19f079e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jeff Sheldon](https://unsplash.com/@ugmonk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, I needed a dataset that was more exploitable, i.e something closer
    to **tabular data** or to a **NoSQL document**. That’s how I thought about finding
    a way to transform the raw data into something more suitable to my needs, without
    spending hours, days and weeks doing it manually.
  prefs: []
  type: TYPE_NORMAL
- en: Let me show you how I used the power of Large Language Models to automate the
    process of converting the raw text into structured documents.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The original dataset is a collection of markdown files. Each file representing
    a recipe.
  prefs: []
  type: TYPE_NORMAL
- en: An example of markdown file describing a recipe to make french crêpes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this is not completely unstructured, there are nice tabular
    metadata on top of the file, then there are 4 distincts sections:'
  prefs: []
  type: TYPE_NORMAL
- en: An introduction,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list of ingredients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some tips.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on this observation, [Sebastian Bahr](https://www.linkedin.com/in/sebastianbahr/),
    developed a parser to transform the markdown files into JSON [here](https://github.com/sebastianbahr/RecipeRecommender).
  prefs: []
  type: TYPE_NORMAL
- en: The output of the parser is already more exploitable, besides Sebastian used
    it to [build a recipe recommender chatbot](/build-a-recipe-recommender-chatbot-using-rag-and-hybrid-search-part-i-c4aa07d14dcf).
    However, there are still some drawbacks. The ingredients and directions keys contain
    raw texts that could be better structured.
  prefs: []
  type: TYPE_NORMAL
- en: As-is, some useful information is hidden.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the quantities for the ingredients, the preparation or cooking
    time for each step.
  prefs: []
  type: TYPE_NORMAL
- en: Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the remainder of this article, I’ll show the steps that I undertook to get
    to JSON documents that look like the one below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code to reproduce the tutorial is on GitHub [here](https://github.com/VianneyMI/baker).
  prefs: []
  type: TYPE_NORMAL
- en: I relied on two powerful libraries `[langchain](https://www.langchain.com/)`
    for communicating with LLM providers and `[pydantic](https://docs.pydantic.dev/latest/)`
    to format the output of the LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: First, I defined the two main components of a recipe with the `Ingredient`and
    `Step`classes.
  prefs: []
  type: TYPE_NORMAL
- en: In each class, I defined the relevant attributes and provided a description
    of the field and examples. Those are then fed to the LLMs by `langchain` leading
    to better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Technical Details**'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to not have a model which is too strict here otherwise, the
    pydantic validation of the JSON outputted by the LLM will fail. A good way to
    give some flexibility is too provide default values like `None` or empty lists
    `[]` depending on the targeted output type.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note the `field_validator`on the `quantity`attribute of the `Ingredient` , is
    there to help the engine parse quantities. It was not initially there but by doing
    some trials, I found out that the LLM was often providing quantities as strings
    such as `1/3`or `1/2` .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `used_ingredients`allow to formally link the ingredients to the relevant
    steps of the recipes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model of the output being defined the rest of the process is pretty smooth.
  prefs: []
  type: TYPE_NORMAL
- en: In a `prompt.py`file, I defined a `create_prompt`function to easily generate
    prompts. A “new” prompt is generated for every recipe. All prompts have the same
    basis but the recipe itself is passed as a variable to the base prompt to create
    a new one.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The communication with the LLM logic was defined in the`run`function of the`core.py`file,
    that I won’t show here for brevity.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, I combined all those components in my`demo.ipynb`notebook whose content
    is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: I used [MistralAI](https://mistral.ai/) as a LLM provider, with their `open-mixtral-8x7b`model
    which is a very good open-source alternative to [OpenAI](https://openai.com/).
    `langchain`allows you to easily switch provider given you have created an account
    on the provider’s platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are trying to reproduce the results:'
  prefs: []
  type: TYPE_NORMAL
- en: (#1) — Make sure you have a `MISTRAL_API_KEY`in a .env file or in your OS environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (#2) — Be careful to the path to the data. If you clone my repo, this won’t
    be an issue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the code on the entire dataset cost less than 2€.
  prefs: []
  type: TYPE_NORMAL
- en: The structured dataset resulting from this code can be found [here](https://github.com/VianneyMI/baker/blob/main/data/output/parsed_recipes_all_8x7b.json)
    in my repository.
  prefs: []
  type: TYPE_NORMAL
- en: I am happy with the results but I could still try to iterate on the prompt,
    my field descriptions or the model used to improve them. I might try MistralAI
    newer model, the `open-mixtral-8x22b` or try another LLM provider by simply changing
    2 or 3 lines of code thanks to `langchain` .
  prefs: []
  type: TYPE_NORMAL
- en: When I am ready, I can get back to my original project. Stay tuned if you want
    to know what it was. In the meantime, let me know in the comments what would you
    do with the final dataset ?
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) offer a powerful tool for structuring unstructured
    data. Their ability to understand and interpret human language nuances, automate
    laborious tasks, and adapt to evolving data make them an invaluable resource in
    data analysis. By unlocking the hidden potential within unstructured textual data,
    businesses can transform this data into valuable insights, driving better decision-making
    and business outcomes. The example provided, of transforming raw recipes data
    into a structured format, is just one of the countless possibilities that LLMs
    offer.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to explore and develop these models, we can expect to see many
    more innovative applications in the future. The journey of harnessing the full
    potential of LLMs is just beginning, and the road ahead promises to be an exciting
    one.
  prefs: []
  type: TYPE_NORMAL
