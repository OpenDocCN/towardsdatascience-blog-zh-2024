<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Is ReFT All We Needed?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Is ReFT All We Needed?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21">https://towardsdatascience.com/is-reft-all-we-needed-1ab38e457320?source=collection_archive---------6-----------------------#2024-11-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4981" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Representation Fintuning — Beyond the PEFT Techniques for fine-tuning LLMs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--1ab38e457320--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1ab38e457320--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="715a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Hasn’t everyone started using ReFT yet?</p><p id="dade" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Stanford published the paper <a class="af nf" href="https://arxiv.org/abs/2404.03592" rel="noopener ugc nofollow" target="_blank">ReFT: Representation finetuning for language models</a> in May 2024, which immediately showed its great potential. In July 2024, <a class="af nf" href="https://www.oxen.ai/blog/fine-tuning-llama-3-in-14-minutes-using-reft" rel="noopener ugc nofollow" target="_blank">Oxen.ai presented an experiment</a> finetuning Llama3 (8B) on a single Nvidia A10 GPU within 14 mins, further demonstrating this technique's power.</p><p id="1723" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unlike SOTA PEFT methods, which focus on modifying the model weights or input, the ReFT technique is based on a previously proposed <a class="af nf" href="https://proceedings.mlr.press/v236/geiger24a.html" rel="noopener ugc nofollow" target="_blank">distributed interchange intervention (DII)</a> method. The DII method first projects the embedding from the deep learning model to a lower dimension subspace and then interferes through the subspace for fine-tuning purposes.</p><p id="376a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the following, we’ll first walk the readers through SOTA fine-tuning PEFT algorithms such as LoRA, prompt tuning, and prefix tuning; then we’ll discuss the original DII method to provide a better context for understanding; lastly, we’ll discuss the ReFT technique and present the results from the paper.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh ni"><img src="../Images/4d8eb5635422e951d302e37a6821c5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*67-ihEjX5X6_QI7VNOb-Cw.jpeg"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image source: <a class="af nf" href="https://pxhere.com/en/photo/1377005" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/1377005</a></figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f97a" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">PEFT — Parameter Efficient Finetuning Techniques</h2><p id="bb2a" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Hugging Face has a <a class="af nf" href="https://huggingface.co/blog/peft" rel="noopener ugc nofollow" target="_blank">blog detailing different PEFT techniques</a> for fine-tuning LLMs. Here, we quickly recap these techniques.</p><p id="574b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Proposed in 2021, <strong class="ml fr">LoRA</strong> has become one of the most successful techniques for fine-tuning LLMs and diffusion models (e.g., <a class="af nf" href="https://openreview.net/forum?id=SgODU2mx9T" rel="noopener ugc nofollow" target="_blank">Time-varying LoRA</a>) due to its simplicity and generalization ability. The idea is simple: instead of fine-tuning the original weight parameters for each layer, the LoRA technique adds two low-rank matrices and only finetunes the low-rank matrices. The trainable parameters could be reduced to less than 0.3% during fine-tuning of the whole network, which significantly speeds up the learning process and minimizes the GPU memory.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh ph"><img src="../Images/1a201f9802d82fb7b293865603fc973a.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*Ddk1MMH-dkdiF7E1BytDSg.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">LoRA model update. Image source: <a class="af nf" href="https://arxiv.org/pdf/2106.09685" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2106.09685</a></figcaption></figure><p id="2155" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead of changing the pre-trained model’s inner layers, the <strong class="ml fr">Prompt Tuning </strong>technique proposed to use “<em class="pi">soft prompts</em>,” a learnable task-specific prompt embedding as a prefix. Given mixed-task batch prompts, the model could efficiently perform multi-task prediction without extra task-specific model copy (as against the Model Tuning in the following left sub-figure).</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pj"><img src="../Images/eda3db8789900b021b199fbfcbfdfe3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*h3uzoOz_xlaUHoupDL27Ng.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Prompt tuning vs classical model finetuning. Image source: <a class="af nf" href="https://arxiv.org/pdf/2104.08691" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2104.08691</a></figcaption></figure><p id="5274" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To provide universality for prompt tuning models at scales (e.g., over 10B parameters), <strong class="ml fr">Prefix Tuning (P-Tuning v2) </strong>proposed to prefix trainable prompt embeddings at different layers, which allows learning task-specific information at various scales.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pk"><img src="../Images/5b57852181d21175d8ef208725dd0ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3jrfYxZpzys8DiPh7eeaig.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Multi-scale prompts for P-tuning v2. Image source: <a class="af nf" href="https://arxiv.org/pdf/2110.07602" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2110.07602</a></figcaption></figure><p id="228f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Among all these PEFT techniques, LoRA is the most widely used in fine-tuning LLMs for its robustness and efficiency. A detailed empirical analysis can be found in this <a class="af nf" href="https://arxiv.org/pdf/2304.14999" rel="noopener ugc nofollow" target="_blank">paper</a>.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="ba4c" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Distributed Interchange Intervention (DII)</h2><p id="f75a" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Causal abstraction is a robust artificial intelligence framework that uses the intervention between a causal model (a <strong class="ml fr">high-level</strong> model) and a neural network model (or a <strong class="ml fr">low-level</strong> model) to induce alignment estimation. If there exists an alignment between the two models, we know the underlying mechanisms between the causal model and the NN are the same. The approach of discovering the underlying alignment by intervention is called interchange intervention (II), which is intuitively explained in this <a class="af nf" href="https://www.youtube.com/watch?v=6pwpOOj33aw" rel="noopener ugc nofollow" target="_blank">lecture video</a>.</p><p id="c7d4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, classical causal abstraction uses brute force to search through all possible alignments of model states, which is less optimal. A <strong class="ml fr">Distributed Interchange Intervention (DII)</strong> system first projects high-level and low-level models to sub-spaces through a series of <strong class="ml fr">orthogonal projections</strong> and then produces an intervened model using certain rotation operations. A fascinating intervention experiment on vision models can be found <a class="af nf" href="https://cs231n.stanford.edu/2024/papers/interchange-interventions-on-vision-models.pdf" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="f275" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">More specifically, the DII could be written as the following:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pl"><img src="../Images/640b4e88d605cc53e71eb4bd800c6b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*ZILu43YzIBkOKREW3vrKFw.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Equation source: <a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.03592</a></figcaption></figure><p id="eee3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where R is a low-rank matrix with orthogonal rows, indicating orthogonal projections; <strong class="ml fr">b</strong> and <strong class="ml fr">s</strong> are two different representations encoded by the model from two different inputs; the intervention will happen on the low-rank space, e.g., the space that contains <strong class="ml fr">Rs</strong> and <strong class="ml fr">Rb</strong>; the projection matrix <strong class="ml fr">R</strong> will be further learnt by <strong class="ml fr">distributed alignment search (DAS)</strong>, which optimizes towards “<a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank"><em class="pi">the subspace that would maximize the probability of expected counterfactual output after intervention</em></a>.”</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3760" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">ReFT — Representation Fintuning</h2><p id="36eb" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Thus, the ReFT technique could be seen as the intervention of the model's hidden representation in a lower dimension space, as illustrated below, where \phi is the intervention and directly applied to the hidden representation at layer L and position P:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pm"><img src="../Images/3a64d382d5a269f8b54c65bdb43a7026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*HiujuKZvqZyZ4V0QpRDkNw.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">ReFT intervention at a high level. Image source: <a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.03592</a></figcaption></figure><p id="fe27" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Specifically, the paper further proposes a <strong class="ml fr">Low-rank Linear Subspace Reft (LoReFT)</strong>, which further introduces a learnt projected source:</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh pn"><img src="../Images/4146ee3e4a8c3d618db11b3db0fb2c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*Cv9ZsQyP3dqmYeg07Gzw9w.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Equation source: <a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.03592</a></figcaption></figure><p id="3c67" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where <strong class="ml fr">h</strong> is the hidden representation, (<strong class="ml fr">Rs = Wh + b</strong>) is the learnt protected source, which <em class="pi">edits </em>the representation h in the projected low-dimension space spanned by <strong class="ml fr">R</strong>. Now, we can illustrate the LoReFT in the original deep neural network layer below.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div class="ng nh po"><img src="../Images/7ada2c72f3bc9e51286b05b717c2c7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*OeD8fCw0QlXY2Y4-A4L5YQ.png"/></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image source: <a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.03592</a></figcaption></figure><p id="24aa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When <strong class="ml fr">fine-tuning on an LLM</strong>, the parameters of the LM are kept frozen while only the parameters of the projection <strong class="ml fr">\phi={R, W, b}</strong> are trained.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="50a3" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk"><strong class="al">Experiments</strong></h2><p id="ba9c" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">The original paper shows experiments comparing the LoReFT (and other techniques from the ReFT family) to full fine-tuning (FT), LoRA, Prefix-tuning, etc., on four types of benchmarks: common-sense reasoning, arithmetic reasoning, instruction following, and natural language understanding. We can see that, compared to LoRA, the ReFT techniques further reduce the parameters by at least 90% while achieving higher performance by a large margin.</p><figure class="nj nk nl nm nn no ng nh paragraph-image"><div role="button" tabindex="0" class="np nq ed nr bh ns"><div class="ng nh pp"><img src="../Images/2e85eede1c29e56a1025e6da9700dacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ZKxltCMDCO9nZyVSb3A9g.png"/></div></div><figcaption class="nu nv nw ng nh nx ny bf b bg z dx">Image souce: <a class="af nf" href="https://arxiv.org/pdf/2404.03592" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2404.03592</a></figcaption></figure></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="432a" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">Discussions</h2><p id="54a2" class="pw-post-body-paragraph mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne fj bk">Why is ReFT so fascinating? Firstly, the technique provides convincing results with Llama-family models on various benchmarks outperforming the SOTA fine-tuning methods. Secondly, the technique is deeply rooted in the causal abstraction algorithm, which offers further ground for model interpretation, especially from the hidden representation’s perspective. As mentioned in the original paper, ReFT shows that “<em class="pi">a linear subspace distributed across a set of neurons can achieve generalized control over a vast number of tasks</em>,” which might further open doors for helping us better understand large language models.</p></div></div></div><div class="ab cb nz oa ob oc" role="separator"><span class="od by bm oe of og"/><span class="od by bm oe of og"/><span class="od by bm oe of"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="66df" class="oh oi fq bf oj ok ol om on oo op oq or ms os ot ou mw ov ow ox na oy oz pa pb bk">References</h2><ul class=""><li id="e19a" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne pq pr ps bk">Wu Z, Arora A, Wang Z, Geiger A, Jurafsky D, Manning CD, Potts C. Reft: Representation finetuning for language models. arXiv preprint arXiv:2404.03592. 2024 Apr 4.</li><li id="079a" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. 2021 Jun 17.</li><li id="e03d" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Zhuang Z, Zhang Y, Wang X, Lu J, Wei Y, Zhang Y. Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems 2024.</li><li id="4195" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Liu X, Ji K, Fu Y, Tam WL, Du Z, Yang Z, Tang J. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXiv preprint arXiv:2110.07602. 2021 Oct 14.</li><li id="3caa" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Geiger A, Wu Z, Potts C, Icard T, Goodman N. Finding alignments between interpretable causal variables and distributed neural representations. InCausal Learning and Reasoning 2024 Mar 15 (pp. 160–187). PMLR.</li><li id="35cc" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691. 2021 Apr 18.</li><li id="35a9" class="mj mk fq ml b go pt mn mo gr pu mq mr ms pv mu mv mw pw my mz na px nc nd ne pq pr ps bk">Pu G, Jain A, Yin J, Kaplan R. Empirical analysis of the strengths and weaknesses of PEFT techniques for LLMs. arXiv preprint arXiv:2304.14999. 2023 Apr 28.</li></ul></div></div></div></div>    
</body>
</html>