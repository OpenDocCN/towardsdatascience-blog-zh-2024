- en: Reinforcement Learning from Human Feedback (RLHF) for LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23](https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An ultimate guide to the crucial technique behind Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    ·14 min read·Sep 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f4286c24057147ea4d07672f02cd693.png)'
  prefs: []
  type: TYPE_IMG
- en: Reinforcement Learning from Human Feedback (RLHF) has turned out to be the key
    to unlocking the full potential of today’s large language models (LLMs). There
    is arguably no better evidence for this than OpenAI’s GPT-3 model. It was released
    back in 2020, but it was only its RLHF-trained version dubbed ChatGPT that became
    an overnight sensation, capturing the attention of millions and setting a new
    standard for conversational AI.
  prefs: []
  type: TYPE_NORMAL
- en: Before RLHF, the LLM training process typically consisted of a pre-training
    stage in which the model learned the general structure of the language and a fine-tuning
    stage in which it learned to perform a specific task. By integrating human judgment
    as a third training stage, RLHF ensures that models not only produce coherent
    and useful outputs but also align more closely with human values, preferences,
    and expectations. It achieves this through a feedback loop where human evaluators
    rate or rank the model’s outputs, which is then used to adjust the model’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: This article explores the intricacies of RLHF. We will look at its importance
    for language modeling, analyze its inner workings in detail, and discuss the…
  prefs: []
  type: TYPE_NORMAL
