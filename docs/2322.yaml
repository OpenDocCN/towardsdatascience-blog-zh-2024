- en: Reinforcement Learning from Human Feedback (RLHF) for LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对LLM的**人类反馈强化学习**（RLHF）
- en: 原文：[https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23](https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23](https://towardsdatascience.com/reinforcement-learning-from-human-feedback-rlhf-for-llms-9cd1288c9a50?source=collection_archive---------7-----------------------#2024-09-23)
- en: LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs
- en: An ultimate guide to the crucial technique behind Large Language Models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型背后的关键技术终极指南
- en: '[](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Michał
    Oleszak](../Images/61b32e70cec4ba54612a8ca22e977176.png)](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    [Michał Oleszak](https://michaloleszak.medium.com/?source=post_page---byline--9cd1288c9a50--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    ·14 min read·Sep 23, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9cd1288c9a50--------------------------------)
    ·14分钟阅读·2024年9月23日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7f4286c24057147ea4d07672f02cd693.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f4286c24057147ea4d07672f02cd693.png)'
- en: Reinforcement Learning from Human Feedback (RLHF) has turned out to be the key
    to unlocking the full potential of today’s large language models (LLMs). There
    is arguably no better evidence for this than OpenAI’s GPT-3 model. It was released
    back in 2020, but it was only its RLHF-trained version dubbed ChatGPT that became
    an overnight sensation, capturing the attention of millions and setting a new
    standard for conversational AI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类反馈强化学习**（RLHF）已被证明是解锁当今大型语言模型（LLMs）全部潜力的关键。或许没有比OpenAI的GPT-3模型更好的证据来证明这一点。该模型于2020年发布，但正是经过RLHF训练后的版本——被称为ChatGPT——才一夜之间成名，吸引了数百万人的关注，并为对话型人工智能设立了新标准。'
- en: Before RLHF, the LLM training process typically consisted of a pre-training
    stage in which the model learned the general structure of the language and a fine-tuning
    stage in which it learned to perform a specific task. By integrating human judgment
    as a third training stage, RLHF ensures that models not only produce coherent
    and useful outputs but also align more closely with human values, preferences,
    and expectations. It achieves this through a feedback loop where human evaluators
    rate or rank the model’s outputs, which is then used to adjust the model’s behavior.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF出现之前，LLM的训练过程通常包括一个预训练阶段，在此阶段模型学习语言的一般结构，以及一个微调阶段，在此阶段模型学习执行特定任务。通过将人类判断作为第三个训练阶段，RLHF确保模型不仅能生成连贯且有用的输出，还能更紧密地与人类的价值观、偏好和期望对齐。它通过一个反馈循环实现这一点，在这个循环中，人类评估者对模型的输出进行评分或排名，然后用于调整模型的行为。
- en: This article explores the intricacies of RLHF. We will look at its importance
    for language modeling, analyze its inner workings in detail, and discuss the…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了**RLHF**的复杂性。我们将研究其在语言建模中的重要性，详细分析其内部运作，并讨论...
