- en: 'A Practical Framework for Data Analysis: 6 Essential Principles'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-practical-framework-for-data-analysis-6-essential-principles-9e8c689eaa66?source=collection_archive---------0-----------------------#2024-11-14](https://towardsdatascience.com/a-practical-framework-for-data-analysis-6-essential-principles-9e8c689eaa66?source=collection_archive---------0-----------------------#2024-11-14)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6704646a9526498eb9f4d6a10be7ca51.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Cytonn Photography](https://unsplash.com/@cytonn_photography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: How to uncover insights from data like a pro
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@pararawendy19?source=post_page---byline--9e8c689eaa66--------------------------------)[![Pararawendy
    Indarjo](../Images/afba0cb7f3af9554a187bbc7a3c00e60.png)](https://medium.com/@pararawendy19?source=post_page---byline--9e8c689eaa66--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9e8c689eaa66--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9e8c689eaa66--------------------------------)
    [Pararawendy Indarjo](https://medium.com/@pararawendy19?source=post_page---byline--9e8c689eaa66--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9e8c689eaa66--------------------------------)
    ·11 min read·Nov 14, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Working as a data scientist in the consumer tech industry for the past six years,
    I’ve carried out countless exploratory data analyses (EDA) to uncover insights
    from data, with the ultimate goal of answering business questions and validating
    hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing on this experience, I distilled my key insights into six data analysis
    principles. These principles have consistently proven useful in my day-to-day
    work, and I am delighted to share them with you.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this article, we will discuss these six principles one at
    a time.
  prefs: []
  type: TYPE_NORMAL
- en: Establish a baseline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalize the metrics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MECE grouping
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate granular data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove irrelevant data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply Pareto principle
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Establish a Baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you’re working at an e-commerce company where management wants to identify
    locations with good customers (where “good” can be defined by various metrics
    such as total spending, average order value, or purchase frequency).
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity, assume the company operates in the three biggest cities in
    Indonesia: Jakarta, Bandung, and Surabaya.'
  prefs: []
  type: TYPE_NORMAL
- en: An inexperienced analyst might hastily calculate the number of good customers
    in each city. Let’s say they find something as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93a3a8530c6a84107783237e41b876bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Good Users Distribution (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Note that 60% of good customers are located in Jakarta. Based on this finding,
    they recommend the management to increase marketing spend in Jakarta.
  prefs: []
  type: TYPE_NORMAL
- en: However, we can do better than this!
  prefs: []
  type: TYPE_NORMAL
- en: The problem with this approach is it only tells us which city has the highest
    absolute number of good customers. It fails to consider that the city with the
    most good customers might simply be the city with the largest overall user base.
  prefs: []
  type: TYPE_NORMAL
- en: 'In light of this, we need to compare the good customer distribution against
    a **baseline: distribution of all users**. This baseline helps us sanity check
    whether or not the high number of good customers in Jakarta is actually an interesting
    finding. Because it might be the case that Jakarta just has the highest number
    of all users — hence, it’s rather expected to have the highest number of good
    customers.'
  prefs: []
  type: TYPE_NORMAL
- en: We proceed to retrieve the total user distribution and obtain the following
    results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/705c9aa5d003ef32e5adbdad529be24b.png)'
  prefs: []
  type: TYPE_IMG
- en: All users distribution as Baseline (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The results show that Jakarta accounts for 60% of all users. Note that it validates
    our previous concern: the fact that Jakarta has 60% of high-value customers is
    simply proportional to its user base; so nothing particularly special happening
    in Jakarta.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the following data when we combine both data to get good customers
    ratio by city.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f2593e17ef61b28a4bb471c4863c730.png)'
  prefs: []
  type: TYPE_IMG
- en: Good Users Ratio by City (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Observe Surabaya: it is home to 30 good users while only being the home for
    150 of total users, resulting in 20% good users ratio — the highest amongst cities.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the kind of insight worth acting on. It indicates that Surabaya has
    an above-average propensity for high-value customers — in other words, a user
    in Surabaya is more likely to become a good customer compared to one in Jakarta.
  prefs: []
  type: TYPE_NORMAL
- en: Normalize the Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the following scenario: the business team has just run two different
    thematic product campaigns, and we have been tasked with evaluating and comparing
    their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To that purpose, we calculate the total sales volume of the two campaigns and
    compare them. Let’s say we obtain the following data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19bdc63223b48d2517f64b3f061203d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Campaign total sales (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: From this result, we conclude that Campaign A is superior than Campaign B, because
    450 Mio is larger than 360 Mio.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we overlooked an important aspect: campaign duration. What if it turned
    out that both campaigns had different durations? If this is the case, we need
    to normalize the comparison metrics. Because otherwise, we do not do justice,
    as campaign A may have higher sales simply because it ran longer.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics normalization ensures that we compare metrics apples to apples**,
    allowing for fair comparison. In this case, we can normalize the sales metrics
    by dividing them by the number of days of campaign duration to derive sales per
    day metric.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we got the following results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b830558bad74b5cd38116869d19fc7b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Campaign data with normalized sales data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion has flipped! After normalizing the sales metrics, it’s actually
    Campaign B that performed better. It gathered 12 Mio sales per day, 20% higher
    than Campaign A’s 10 Mio per day.
  prefs: []
  type: TYPE_NORMAL
- en: MECE Grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MECE is a consultant’s favorite framework. MECE is their go-to method to break
    down difficult problems into smaller, more manageable chunks or partitions.
  prefs: []
  type: TYPE_NORMAL
- en: MECE stands for Mutually Exclusive, Collectively Exhaustive. So, there are two
    concepts here. Let’s tackle them one by one. For concept demonstration, imagine
    we wish to study the attribution of user acquisition channels for a specific consumer
    app service. To gain more insight, we separate out the users based on their attribution
    channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose at the first attempt, we breakdown the attribution channels as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Paid social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook ad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organic traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f52c21c0a1bae42b820c3ef5ed5c3ac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Set-diagram of the above grouping: Non-MECE (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutually Exclusive (ME)** means that the breakdown sets must not overlap
    with one another. In other words, there are no analysis units that belong to more
    than one breakdown group. The above breakdown is *not* mutually exclusive, as
    Facebook ads are a subset of paid social media. As a result, all users in the
    Facebook ad group are also members of the Paid social media group.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collectively exhaustive (CE)** means that the breakdown groups must include
    all possible cases/subsets of the universal set. In other words, no analysis unit
    is unattached to any breakdown group. The above breakdown is *not* collectively
    exhaustive because it doesn’t include users acquired through other channels such
    as search engine ads and affiliate networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MECE breakdown version of the above case could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Paid social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engine ads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affiliate networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/71e7ce1c37e89a55b97fdfa9ea0c7023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Set-diagram of the updated grouping: MECE! (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: MECE grouping enables us to break down large, heterogeneous datasets into smaller,
    more homogeneous partitions. This approach facilitates specific data subset optimization,
    root cause analysis, and other analytical tasks.
  prefs: []
  type: TYPE_NORMAL
- en: However, creating MECE breakdowns can be challenging when there are numerous
    subsets, i.e. when the factor variable to be broken down contains many unique
    values. Consider an e-commerce app funnel analysis for understanding user product
    discovery behavior. In an e-commerce app, users can discover products through
    numerous pathways, making the standard MECE grouping complex (search, category,
    banner, let alone the combinations of them).
  prefs: []
  type: TYPE_NORMAL
- en: 'In such circumstances, suppose we’re primarily interested in understanding
    user search behavior. Then it’s practical to create a binary grouping: is_search
    users, in which a user has a value of 1 if he or she has ever used the app’s search
    function. This streamlines MECE breakdown while still supporting the primary analytical
    goal.'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, binary flags offer a straightforward MECE breakdown approach,
    where we focus on the most relevant category as the positive value (such as is_search,
    is_paid_channel, or is_jakarta_user).
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate Granular Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many datasets in industry are granular, which means they are presented at a
    raw-detailed level. Examples include transaction data, payment status logs, in-app
    activity logs, and so on. Such granular data are low-level, containing rich information
    at the expense of high verbosity.
  prefs: []
  type: TYPE_NORMAL
- en: We need to be careful when dealing with granular data because it may hinder
    us from gaining useful insights. Consider the following example of simplified
    transaction data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd42d8c02a2d5d0c8784d867aff96b58.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample granular transaction data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, the table does not appear to contain any interesting findings.
    There are 20 transactions involving different phones, each with a uniform quantity
    of 1\. As a result, we may come to the conclusion that there is no interesting
    pattern, such as which phone is dominant/favored over the others, because they
    all perform identically: all of them are sold in the same quantity.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we can improve the analysis by aggregating at the phone brands level
    and calculating the percentage share of quantity sold for each brand.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a2807011d9d38b8e1c12c7bb38d6062.png)'
  prefs: []
  type: TYPE_IMG
- en: Aggregation process of transaction data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Suddenly, we got non-trivial findings. Samsung phones are the most prevalent,
    accounting for 45% of total sales. It is followed by Apple phones, which account
    for 30% of total sales. Xiaomi is next, with a 15% share. While Realme and Oppo
    are the least purchased, each with a 5% share.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, **aggregation is an effective tool for working with granular
    data**. It helps to transform the low-level representations of granular data into
    higher-level representations, increasing the likelihood of obtaining non-trivial
    findings from our data.
  prefs: []
  type: TYPE_NORMAL
- en: For readers who want to learn more about how aggregation can help uncover interesting
    insights, please see my Medium post below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-powerful-eda-tool-group-by-aggregation-696736c5f3a1?source=post_page-----9e8c689eaa66--------------------------------)
    [## A Powerful EDA Tool: Group-By Aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to use group-by aggregation to uncover insights from your data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-powerful-eda-tool-group-by-aggregation-696736c5f3a1?source=post_page-----9e8c689eaa66--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Remove Irrelevant Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-world data are both messy and dirty. Beyond technical issues such as missing
    values and duplicated entries, there are also issues regarding data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: This is especially true in the consumer app industry. By design, consumer apps
    are used by a huge number of end users. One common characteristic of consumer
    apps is their heavy reliance on promotional strategies. However, there exists
    a particular subset of users who are extremely opportunistic. If they perceive
    a promotional strategy as valuable, they may place so many orders to maximize
    their benefits. This outlier behavior can be harmful to our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a scenario where we’re data analysts at an e-grocery
    platform. We’ve been assigned an interesting project: analyzing the natural reordering
    interval for each product category. In other words, we want to understand: How
    many days do users need to reorder vegetables? How many days typically pass before
    users reorder laundry detergent? What about snacks? Milk? And so on. This information
    will be utilized by the CRM team to send timely order reminders.'
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we examine transaction data from the past 6 months,
    aiming to obtain the median reorder interval for each product category. Suppose
    we got the following results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7b1e2e36dd33ed22c5a73d6943bf7c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Median reorder interval for each product category (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the data, the results are somewhat surprising. The table shows that
    rice has a median reorder interval of 3 days, and cooking oil just 2 days. Laundry
    detergent and dishwashing liquid have median reorder periods of 5 days. On the
    other hand, order frequencies for vegetables, milk, and snacks roughly align with
    our expectations: vegetables are bought weekly, milk and snacks are bought twice
    a month.'
  prefs: []
  type: TYPE_NORMAL
- en: Should we report these findings to the CRM team? Not so fast!
  prefs: []
  type: TYPE_NORMAL
- en: Is it realistic that people buy rice every 3 days or cooking oil every 2 days?
    What kind of consumers would do that?
  prefs: []
  type: TYPE_NORMAL
- en: Upon revisiting the data, we discovered a group of users making transactions
    extremely frequently — even daily. These excessive purchases were concentrated
    in popular non-perishable products, corresponding to the product categories showing
    surprisingly low median reorder intervals in our findings.
  prefs: []
  type: TYPE_NORMAL
- en: We believe these super-frequent users don’t represent our typical target customers.
    Therefore, we excluded them from our analysis and generated updated findings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56fbc0701a5dffac390d4718577925ac.png)'
  prefs: []
  type: TYPE_IMG
- en: Updated median reorder data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Now everything makes sense. The true reorder cadence for rice, cooking oil,
    laundry detergent, and dishwashing liquid had been skewed by these anomalous super-frequent
    users, who were irrelevant to our analysis. After removing these outliers, we
    discovered that people typically reorder rice and cooking oil every 14 days (biweekly),
    while laundry detergent and dishwashing liquid are purchased in monthly basis.
  prefs: []
  type: TYPE_NORMAL
- en: Now we’re confident to share the insights with the CRM team!
  prefs: []
  type: TYPE_NORMAL
- en: The practice of removing irrelevant data from analysis is both common and crucial
    in industry settings. In real-world data, anomalies are frequent, and we need
    to exclude them to prevent our results from being distorted by their extreme behavior,
    which isn’t representative of our typical users’ behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Apply the Pareto Principle
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final principle I’d like to share is how to get the most bang for our buck
    when analyzing data. To this end, we will apply the Pareto principle.
  prefs: []
  type: TYPE_NORMAL
- en: The Pareto principle states that for many outcomes, **roughly 80% of consequences
    come from 20% of causes.**
  prefs: []
  type: TYPE_NORMAL
- en: 'From my industry experience, I’ve observed the Pareto principle manifesting
    in many scenarios: only a small number of products contribute to the majority
    of sales, just a handful of cities host most of the customer base, and so on.
    We can use this principle in data analysis to save time and effort when creating
    insights.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where we’re working at an e-commerce platform operating
    across all tier 1 and tier 2 cities in Indonesia (there are tens of them). We’re
    tasked with analyzing user transaction profiles based on cities, involving metrics
    such as basket size, frequency, products purchased, shipment SLA, and user address
    distance.
  prefs: []
  type: TYPE_NORMAL
- en: 'After a preliminary look at the data, we discovered that 85% of sales volume
    comes from just three cities: Jakarta, Bandung, and Surabaya. Given this fact,
    it makes sense to focus our analysis on these three cities rather than attempting
    to analyze all cities (which would be like boiling the ocean, with diminishing
    returns).'
  prefs: []
  type: TYPE_NORMAL
- en: Using this strategy, we minimized our effort while still meeting the key analysis
    objectives. The insights gained will remain meaningful and relevant because they
    come from the majority of the population. Furthermore, the following business
    recommendations based on the insights will, by definition, have a significant
    impact on the entire population, making them still powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another advantage of applying the Pareto principle is related to establishing
    MECE groupings. In our example, we can categorize the cities into four groups:
    Jakarta, Bandung, Surabaya, and “Others” (combining all remaining cities into
    one group). In this way, the Pareto principle helps streamline our MECE grouping:
    each major contributing city stands alone, while the remaining cities (beyond
    the Pareto threshold) are consolidated into a single group.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for persevering until the last bit of this article!
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we discussed six data analysis principles that can help us discover
    insights more effectively. These principles are derived from my years of industry
    experience and are extremely useful in my EDA exercises. Hopefully, you will find
    these principles useful in your future EDA projects as well.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, thanks for reading, and let’s connect with me on [LinkedIn](https://www.linkedin.com/in/pararawendy-indarjo/)!
    👋
  prefs: []
  type: TYPE_NORMAL
