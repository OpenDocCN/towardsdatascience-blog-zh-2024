<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building an Explainable Reinforcement Learning Framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building an Explainable Reinforcement Learning Framework</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13">https://towardsdatascience.com/building-an-explainable-reinforcement-learning-framework-084ef2d23d01?source=collection_archive---------9-----------------------#2024-03-13</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="fr fs ft fu fv fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp fq"><img src="../Images/7b8e8fae19e2beff62bb005375135544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQXOuxzBdf_29c5ctrFhTw.png"/></div></div></figure><h2 id="6afa" class="gd ge gf bf b dy gg gh gi gj gk gl dx gm" aria-label="kicker paragraph">Explainable Results Through Symbolic Policy Discovery</h2><div/><div><h2 id="3ff0" class="pw-subtitle-paragraph hh go gf bf b hi hj hk hl hm hn ho hp hq hr hs ht hu hv hw cq dx">Symbolic genetic algorithms, action potentials, and equation trees</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hx hy hz ia ib ab"><div><div class="ab ic"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------" rel="noopener follow"><div class="l id ie by if ig"><div class="l ed"><img alt="Dani Lisle" class="l ep by dd de cx" src="../Images/2933bbbca26cf198e7964547a91b2751.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*a4gKCYqEZAlyGqsVKTldOg.jpeg"/><div class="ih by l dd de em n ii eo"/></div></div></a></div></div><div class="ij ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------" rel="noopener follow"><div class="l ik il by if im"><div class="l ed"><img alt="Towards Data Science" class="l ep by br in cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ih by l br in em n ii eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="io ab q"><div class="ab q ip"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b iq ir bk"><a class="af ag ah ai aj ak al am an ao ap aq ar is" data-testid="authorName" href="https://medium.com/@Dani_Lisle?source=post_page---byline--084ef2d23d01--------------------------------" rel="noopener follow">Dani Lisle</a></p></div></div></div><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b iq ir dx"><button class="iv iw ah ai aj ak al am an ao ap aq ar ix iy iz" disabled="">Follow</button></p></div></div></span></div></div><div class="l ja"><span class="bf b bg z dx"><div class="ab cn jb jc jd"><div class="je jf ab"><div class="bf b bg z dx ab jg"><span class="jh l ja">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar is ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--084ef2d23d01--------------------------------" rel="noopener follow"><p class="bf b bg z ji jj jk jl jm jn jo jp bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="it iu" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="jq jr l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 13, 2024</span></div></span></div></span></div></div></div><div class="ab cp js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="h k w ea eb q"><div class="kx l"><div class="ab q ky kz"><div class="pw-multi-vote-icon ed jh la lb lc"><div class=""><div class="ld le lf lg lh li lj am lk ll lm lc"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ln lo lp lq lr ls lt"><p class="bf b dy z dx"><span class="le">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ld lw lx ab q ee ly lz" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lv"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lu lv">1</span></p></button></div></div></div><div class="ab q ki kj kk kl km kn ko kp kq kr ks kt ku kv kw"><div class="ma k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al mb an ao ap ix mc md me" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep mf cn"><div class="l ae"><div class="ab cb"><div class="mg mh mi mj mk gb ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al mb an ao ap ix ml mm lz mn mo mp mq mr s ms mt mu mv mw mx my u mz na nb"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6434" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ve learned to train models that can beat world champions at a games like Chess and Go, with one major limitation: explainability. Many methods exist to create a black-box model that knows how to play a game or system better than any human, but creating a model with a human-readable closed-form strategy is another problem altogether.</p><p id="73dd" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The potential upsides of being better at this problem are plentiful. Strategies that humans can quickly understand don’t stay in a codebase — they enter the scientific literature, and perhaps even popular awareness. They could contribute a reality of augmented cognition between human and computer, and reduce siloing between our knowledge as a species and the knowledge hidden, and effectively encrypted deep in a massive high-dimensional tensors.</p><p id="b400" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But if we had more algorithms to provide us with such explainable results from training, how would we encode them in a human-readable way?</p><p id="ac3d" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">One of the most viable options is the use of differential equations (or difference equations in the discrete case). These equations, characterized by their definition of derivatives, or rates of change of quantities, give us an efficient way to communicate and intuitively understand the dynamics of almost any system. Here’s a famous example that relates the time and space derivatives of heat in a system:</p><figure class="nz oa ob oc od fw fo fp paragraph-image"><div role="button" tabindex="0" class="fx fy ed fz bh ga"><div class="fo fp ny"><img src="../Images/88ee96e7f8c15f63480837e562b8be1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nUWzz9GbYkA4UKWrtxrEkQ.png"/></div></div><figcaption class="oe of og fo fp oh oi bf b bg z dx">Heat Equation in ‘n’ Dimensions (<a class="af oj" href="https://en.wikipedia.org/wiki/Heat_equation" rel="noopener ugc nofollow" target="_blank">Wikipedia: “Heat Equation”</a>)</figcaption></figure><p id="43c3" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In fact, work has been done in the way of algorithms that operate to evolve such equations directly, rather than trying to extract them (as knowledge) from tensors. Last year I authored a paper which detailed a framework for game theoretic simulations using dynamics equation which evolve symbol-wise via genetic algorithms. Another paper by Chen et al. presented work on a symbolic genetic algorithm for discovering partial differential equations which, like the heat equation, describe the dynamics of a physical system. This group was able to mine such equations from generated datasets.</p><p id="53ad" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But consider again the game of Chess. What if our capabilities in the computational learning of these equations were not limited to mere predictive applications? What if we could use these evolutionary techniques to learn optimal strategies for socioeconomic games in the real world?</p><p id="eef6" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a time where new human and human-machine relationships, and complex strategies, are entering play more quickly than ever, computational methods to find intuitive and transferable strategic insight have never been more valuable. The opportunities and potential threats are both compelling and overwhelming.</p><h1 id="391a" class="ok ol gf bf om on oo hk op oq or hn os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Let’s Begin</h1><p id="e958" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk"><em class="pl">All Python code discussed in this article are accessible in my running GitHub repo for the project: </em><a class="af oj" href="https://github.com/dreamchef/abm-dynamics-viz" rel="noopener ugc nofollow" target="_blank"><em class="pl">https://github.com/dreamchef/abm-dynamics-viz</em></a><em class="pl">.</em></p><p id="e90c" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In a recent article I wrote about simulating dynamically-behaving agents in a theoretic game. As much as I’d like to approach such multi-agent games using symbolic evolution, it’s wise to work atomically, expand our scope, and take advantage of some previous work. Behind the achievements of groups like DeepMind in creating models with world-class skill at competitive board games is a sub-discipline of ML: reinforcement learning. In this paradigm, agents have an observation space (variables in their environment which they can measure and use as values), an action space (ways to interact with or move/change in the environment), and a reward system. Over time through experimentation, the reward dynamics allow them to build a strategy, or policy, which maximizes reward.</p><p id="eafd" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can apply our symbolic genetic algorithms to some classic reinforcement learning problems in order to explore and fine tune them. The Gymnasium library provides a collection of games and tasks perfect for reinforcement learning experiments. One such game which I determined to be well-suited to our goals is “Lunar Lander”.</p><figure class="nz oa ob oc od fw fo fp paragraph-image"><div class="fo fp pm"><img src="../Images/1c4f6127527a81d09e1da5ca69df3b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*m6j5j1rgtTB7qK6CjD46eg.gif"/></div><figcaption class="oe of og fo fp oh oi bf b bg z dx">Lunar Lander (Credit: Gymnasium)</figcaption></figure><p id="b5ba" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The game is specified as follows:</p><ul class=""><li id="0beb" class="nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pn po pp bk">Observation space (8): x,y position, x, y velocity, angle, angular velocity, left, right foot touching ground. Continuous.</li><li id="b979" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx pn po pp bk">Action space (4): no engine, bottom, left, right engine firing. Discrete.</li></ul><h1 id="1191" class="ok ol gf bf om on oo hk op oq or hn os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Learning Symbolic Policies for the Lander Task</h1><p id="5096" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">You might have noticed that while the variables like velocity and angle are continuous, action space is discrete. So how do we define a function that takes continuous inputs and outputs, effectively, a classification? In fact this is a well-known problem and the common approach is using an Action Potential function.</p><h2 id="5ed4" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Action Potential Equation</h2><p id="ef91" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">Named after the neurological mechanism, which operates as a threshold, a typical Action Potential function calculates a continuous value from the inputs, and outputs:</p><ul class=""><li id="9659" class="nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pn po pp bk">True output if the continuous value is at or above a threshold.</li><li id="673a" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx pn po pp bk">False is the output is below.</li></ul><p id="b251" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our problem, we actually need to get a discrete output in 4 possible values. We could carefully consider the dynamics of the task in devising this system, but I chose a naive approach, as a semi-adversarial effort to put more pressure on our SGA algorithm to ultimately shine through. It uses the general intuition that near the target probably means we shouldn’t use the side thrusters as much:</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="bd51" class="qp ol gf qm b bg qq qr l qs qt">            <br/>def potential_to_action(potential):<br/><br/>    if abs(potential-0) &lt; 0.5:<br/>        return 0<br/>    <br/>    elif abs(potential-0) &lt; 1:<br/>        return 2<br/><br/>    elif potential &lt; 0:<br/>        return 1<br/>    <br/>    else:<br/>        return 3</span></pre><p id="07f8" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With this figured out, let’s make a roadmap for the rest of our journey. Our main tasks will be:</p><ol class=""><li id="88f0" class="nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qu po pp bk">Evolutionary structure in which families and generations of equations can exist and compete.</li><li id="e1b3" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx qu po pp bk">Data structure to store equations (which facilitates their genetic modification).</li><li id="0477" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx qu po pp bk">Symbolic mutation algorithm— how and what will we mutate?</li><li id="504c" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx qu po pp bk">Selection method — which and how many candidates will we bring to the next round?</li><li id="29b3" class="nc nd gf ne b hi pq ng nh hl pr nj nk nl ps nn no np pt nr ns nt pu nv nw nx qu po pp bk">Evaluation method — how will we measure the fitness of an equation?</li></ol><h2 id="6293" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Evolutionary Structure</h2><p id="4f58" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">We start by writing out the code on a high-level and leaving some of the algorithm implementations for successive steps. This mostly takes the form of an array where we can store the population of equations and a main loop that evolves them for the specified number of generations while calling the mutation, selection/culling, and testing algorithms.</p><p id="1cba" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also define a set of parameters for the evolutionary model including number of generations, and specifying how many mutations to create and select from each parent policy.</p><p id="5e4c" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following code</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="44a1" class="qp ol gf qm b bg qq qr l qs qt">last_gen = [F]<br/><br/>for i in range(GENS):<br/>    next_gen = []<br/><br/>    for policy in last_gen:<br/>        batch = cull(mutants(policy))<br/><br/>        for policy in batch:<br/>            next_gen.append(policy) <br/>    <br/>    last_gen = next_gen</span></pre><p id="86a8" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally it selects the best-performing policies, and validates them using another round of testing (against Lunar Lander simulation rounds):</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="ddac" class="qp ol gf qm b bg qq qr l qs qt">last_gen.sort(key=lambda x: x['score'])<br/><br/>final_cull = last_gen [-30:]<br/><br/>for policy in final_cull:<br/><br/>    policy['score'] = score_policy(policy,ep=7)<br/><br/>final_cull.sort(key=lambda x: x['score'])<br/><br/>print('Final Popluation #:',len(last_gen))<br/><br/>for policy in final_cull:<br/>    print(policy['AP'])<br/>    print(policy['score'])<br/>    print('-'*20)<br/><br/>env.close()</span></pre><h2 id="0e33" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Data Structure to Store Equations</h2><p id="ce04" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">We start by choosing a set of binary and unary operators and operands (from the observation space) which we represent and mutate:</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="af78" class="qp ol gf qm b bg qq qr l qs qt">BIN_OPS = ['mult','add','sub', 'div']<br/>UN_OPS = ['abs','exp','log','sqrt','sin','cos']<br/>OPNDS = ['x','y','dx','dy','angle','dangle','L','R']</span></pre><p id="7992" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we borrow from Chen et al. the idea of encoding equations int the form of trees. This will allow us to iterate through the equations and mutate the symbols as individual objects. Specifically I chose to do using nested arrays the time being. This example encodes <em class="pl">x*y + dx*dy:</em></p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="062d" class="qp ol gf qm b bg qq qr l qs qt">F = {'AP': ['add', <br/>                ['mult','x','y'],<br/>                ['mult','dx','dy']],<br/>        'score': 0<br/>        }</span></pre><p id="5c99" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Each equation includes both the tree defining its form, and a score object which will store its evaluated score in the Lander task.</p><h2 id="dd8a" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Symbolic Mutation Algorithm</h2><p id="f767" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">We could approach the mutation of algorithms in a variety of ways, depending on our desired probability distribution for modifying different symbols in the equations. I used a recursive approach where at each level of the tree, the algorithm randomly chooses a symbol, and in the case of a binary operator, moves down to the next level to choose again.</p><p id="15ad" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following main mutation function accepts a source policy and outputs an array including the unchanged source and mutated policies.</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="db95" class="qp ol gf qm b bg qq qr l qs qt">def mutants(policy, sample=1):<br/>    children = [policy]<br/>    mutation_target = policy<br/><br/>    for i in range(REPL):<br/>        new_policy = copy.deepcopy(policy)<br/>        new_policy['AP'] = mutate_recursive(new_policy['AP'])<br/>        children.append(new_policy)<br/><br/>    return children</span></pre><p id="e0e7" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This helper function contains recursive algorithm:</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="fcee" class="qp ol gf qm b bg qq qr l qs qt">def mutate_recursive(target, probability=MUTATE_P):<br/><br/>    # Recursive case<br/>    if isinstance(target, list):<br/>      random_element = random.choice(range(len(target)))<br/>      target[random_element] = mutate_recursive(target[random_element])<br/>      return target<br/><br/>    # Base cases<br/>    elif(target in BIN_OPS):<br/>      new = random.choice(BIN_OPS)<br/>      return new<br/>    <br/>    elif(target in UN_OPS):<br/>      new = random.choice(UN_OPS)<br/>      return new<br/>    <br/>    elif(target in OPNDS):<br/>      new = random.choice(OPNDS)<br/>      return new</span></pre><h2 id="5520" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Selection Method</h2><p id="375c" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">Selecting the best policies will involve testing them to get a score and then deciding on a way to let them compete and progress to further stages of evolution. Here I used an evolutionary family tree structure in which each successive generation in a family, or batch (e.g. the two on the lower left), contains children with one mutation that differentiates them from the parent.</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="a555" class="qp ol gf qm b bg qq qr l qs qt">                +----------+<br/>                | x + dy^2 |<br/>                +----------+<br/>                     |<br/>          +----------+----------+<br/>          |                     |<br/>     +----v----+           +----v----+<br/>     | y + dy^2|           | x / dy^2|<br/>     +---------+           +---------+<br/>          |                      |<br/>     +----+----+            +----+-----+<br/>     |         |            |          |<br/> +---v--+-+ +--v---+-+   +--v-----+ +--v-----+<br/> |y - dy^2| |y - dy^2|   |x / dx^2| |y - dy^3|<br/> +--------+ +--------+   +--------+ +--------+</span></pre><p id="3b20" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After scoring of the equations, each batch of equations is ranked and the best N are kept in the running, while the rest are discarded:</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="e84b" class="qp ol gf qm b bg qq qr l qs qt">def cull(batch):<br/><br/>    for policy in batch[1:]:<br/>        policy['score'] = score_policy(policy)<br/><br/>    batch.sort(key=lambda x: x['score'], reverse=True)<br/><br/>    return batch[:CULL]</span></pre><h2 id="04b0" class="pv ol gf bf om pw px py op pz qa qb os nl qc qd qe np qf qg qh nt qi qj qk gl bk">Scoring Methods Through Simulation Episodes</h2><p id="1f6c" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">To decide which equations encode the best policies, we use the Gymnasium framework for the Lunar Lander task.</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="139e" class="qp ol gf qm b bg qq qr l qs qt">def score_policy(policy, ep=10, render=False):<br/>    observation = env.reset()[0]  # Reset the environment to start a new episode<br/>    total_reward = 0<br/>    sample = 0<br/><br/>    for episode in range(ep):<br/>        <br/>        while True:<br/>            if render:<br/>                env.render()<br/>                <br/>            values = list(observation)<br/>            values =    {'x': values[0],<br/>            'y': values[1],<br/>            'dx': values[2],<br/>            'dy': values[3],<br/>            'angle': values[4],<br/>            'dangle': values[5],<br/>            'L': values[6],<br/>            'R': values[7]<br/>            }<br/><br/>            potential = policy_compute(policy['AP'], values)<br/>            action = potential_to_action(potential)<br/><br/>            sample += 1<br/><br/>            observation, reward, done, info = env.step(action)[:4]<br/>            total_reward += reward<br/><br/>            if done:  # If the episode is finished<br/>                break<br/>    <br/>    return total_reward/EPISODES</span></pre><p id="5a7c" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The main loop for scoring runs the number of episodes (simulation runs) specified, and in each episode we see the fundamental reinforcement learning paradigm.</p><p id="1c3d" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From a starting observation, the information is used to compute an action via our method, the action interacts with the environment, and the observation for the next step is obtained.</p><p id="e5fe" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since we store the equations as trees, we need a separate method to compute the potential from this form. The following function uses recursion to obtain a result from the encoded equation, given the observation values:</p><pre class="nz oa ob oc od ql qm qn bp qo bb bk"><span id="a85d" class="qp ol gf qm b bg qq qr l qs qt">def policy_compute(policy, values):<br/><br/>    if isinstance(policy, str):<br/>        if policy in values:<br/>            return values[policy]<br/>        else:<br/>            print('ERROR')<br/>        <br/>    elif isinstance(policy, list):<br/>        operation = policy[0]<br/>        branches = policy[1:]<br/><br/>        if operation in BIN_OPS:<br/><br/>            if len(branches) != 2:<br/>                raise ValueError(f"At {policy}, Operation {operation} expects 2 operands, got {len(branches)}")<br/><br/>            operands = [operand for operand in branches]<br/>            <br/>            left = policy_compute(operands[0], values)<br/>            right = policy_compute(operands[1], values)<br/><br/>            if operation == 'add':<br/>                return left + right<br/>            elif operation == 'sub':<br/>                return left - right<br/>            elif operation == 'mult':<br/>                if left is None or right is None:<br/>                    print('ERROR: left:',left,'right:',right)<br/>                return left * right<br/>            elif operation == 'div':<br/>                if right == 0:<br/>                    return 0<br/>                return left / right<br/><br/>        elif operation in UN_OPS:<br/>            if len(branches) != 1:<br/>                raise ValueError(f"Operation {operation} expects 1 operand, got {len(branches)}")<br/><br/>            operand_value = policy_compute(next(iter(branches)), values)<br/>            <br/>            if operation == 'abs':<br/>                return abs(operand_value)<br/>            elif operation == 'exp':<br/>                return math.exp(operand_value)<br/>            elif operation == 'logabs':<br/>                return math.log(abs(operand_value))<br/>            elif operation == 'sin':<br/>                return math.sin(operand_value)<br/>            elif operation == 'cos':<br/>                return math.cos(operand_value)<br/>            elif operation == 'sqrtabs':<br/>                return math.sqrt(abs(operand_value))<br/>        <br/>        else:<br/>            raise ValueError(f"Unknown operation: {operation}")<br/><br/>    else:<br/>        print('ERROR')<br/>        return 0</span></pre><p id="6c71" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code above goes through each level of the tree, checks if the current symbol is an operand or operator, and according either computes the right/left side recursively or returns back in the recursive stack to do the appropriate operator computations.</p><h1 id="f010" class="ok ol gf bf om on oo hk op oq or hn os ot ou ov ow ox oy oz pa pb pc pd pe pf bk">Next Steps</h1><p id="378f" class="pw-post-body-paragraph nc nd gf ne b hi pg ng nh hl ph nj nk nl pi nn no np pj nr ns nt pk nv nw nx fj bk">This concluded the implementation. In the next article in this series, I’ll be explaining the results of training, motivating changes in the experimental regime, and exploring pathways to expand the training framework by improving the mutation and selection algorithms.</p><p id="b464" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the meantime, you can access <a class="af oj" href="https://publish.obsidian.md/danilisle/2024+SIAM+Front+Range+Student+Conference+-+GENETIC+REINFORCEMENT+LEARNING+OF+OPTIMAL+STRATEGY+DYNAMICS+IN+SYMBOLIC+FORM" rel="noopener ugc nofollow" target="_blank">here</a> the slides for a recent talk I gave at the 2024 SIAM Front Range Student Conference at University of Colorado Denver which discussed preliminary training results.</p><p id="3958" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All code for this project is on my repo: <a class="af oj" href="https://github.com/dreamchef/abm-dynamics-viz" rel="noopener ugc nofollow" target="_blank">https://github.com/dreamchef/abm-dynamics-viz</a>. I’d love to hear what else you may find, or your thoughts on my work, in the comments! Feel free to reach out to me on <a class="af oj" href="https://twitter.com/dani_lisle" rel="noopener ugc nofollow" target="_blank">Twitter</a> and <a class="af oj" href="https://www.linkedin.com/in/danilisle/" rel="noopener ugc nofollow" target="_blank">LinkedIn</a> as well.</p><p id="064f" class="pw-post-body-paragraph nc nd gf ne b hi nf ng nh hl ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="pl">All images were created by the author except where otherwise noted.</em></p></div></div></div></div>    
</body>
</html>