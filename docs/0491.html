<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Evaluations with Chat Formats</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Evaluations with Chat Formats</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21">https://towardsdatascience.com/evaluations-with-chat-formats-7604067023c9?source=collection_archive---------6-----------------------#2024-02-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8348" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Applying chat templates to generative LM evaluation tests</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Furman" class="l ep by dd de cx" src="../Images/f7a1b4c6239ede8bb01e50f167931719.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YEXy-kekw391NRd3lVIHnw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@daniel_furman?source=post_page---byline--7604067023c9--------------------------------" rel="noopener follow">Daniel Furman</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7604067023c9--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/a8e714bc5984165c0816edc5f138bd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JirFPycfjMxdIrm5"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@googledeepmind" rel="noopener ugc nofollow" target="_blank">Google DeepMind</a> on <a class="af nc" href="https://unsplash.com/photos/a-close-up-of-a-metal-structure-made-of-wood-and-metal-pyET8SQTc0A" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="nd ne nf"><p id="08ff" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">“<strong class="nj fr">Building solid evals should be the starting point</strong> for any LLM-based system or product (as well as conventional machine learning systems).” — Eugene Yan, <a class="af nc" href="https://eugeneyan.com/writing/llm-patterns/#how-to-apply-evals" rel="noopener ugc nofollow" target="_blank">link</a></p></blockquote><h1 id="3268" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk"><strong class="al">tl;dr</strong></h1><p id="c33c" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">Chat models are typically fine-tuned on datasets formatted with a prompt template. These chat templates are programmed recipes that convert a chat conversation into a single string. At prediction time, it’s standard to match an LLM’s expected chat format — not doing so is oft-noted as causing performance degradations [1]. However, do we in fact see these degradations on evaluation benchmarks?</p><p id="9d34" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk"><strong class="nj fr">NB</strong>: This blog post is intended for readers with basic familiarity with Python programming and neural language modeling.</p><h1 id="3017" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Introduction</h1><p id="7ac8" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">If you’ve built on top of OpenAI’s chat API, the following code will be recognizable. Under the hood, this input is transformed into one tokenizable string via the <a class="af nc" href="https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md#working-with-chat-markup-language-chatml" rel="noopener ugc nofollow" target="_blank">ChatML</a> format:</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="e880" class="pi oe fq pf b bg pj pk l pl pm">from openai import OpenAI<br/>client = OpenAI()<br/><br/>response = client.chat.completions.create(<br/>  model="gpt-3.5-turbo",<br/>  messages=[<br/>    {"role": "system", "content": "You are a helpful assistant."},<br/>    {"role": "user", "content": "Who won the world series in 2020?"},<br/>    {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},<br/>    {"role": "user", "content": "Where was it played?"}<br/>  ]<br/>)</span></pre><pre class="pn pe pf pg bp ph bb bk"><span id="9e93" class="pi oe fq pf b bg pj pk l pl pm">"&lt;|im_start|&gt;system<br/>You are a helpful assistant.<br/>&lt;|im_start|&gt;user<br/>Who won the world series in 2020?&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;assistant<br/>The Los Angeles Dodgers won the World Series in 2020.&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;user<br/>Where was it played?&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;assistant"</span></pre><p id="8eaf" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">It turns out there’s a wide variety of chat templates across the LLM research community. Take an open-source model like <code class="cx po pp pq pf b">Mixtral-8x7B-Instruct-v0.1</code><em class="ni">. </em>It’s format looks wildly different from <code class="cx po pp pq pf b">gpt-3.5-turbo</code> above:</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="787b" class="pi oe fq pf b bg pj pk l pl pm">from transformers import AutoTokenizer<br/>tokenizer = AutoTokenizer.from_pretrained("mistralai/Mixtral-8x7B-Instruct-v0.1")<br/>chat = [<br/>  {"role": "user", "content": "Hello, how are you?"},<br/>  {"role": "assistant", "content": "I'm doing great. How can I help you today?"},<br/>  {"role": "user", "content": "Write me a haiku about coding."},<br/>]<br/>tokenizer.apply_chat_template(chat, tokenize=False)</span></pre><pre class="pn pe pf pg bp ph bb bk"><span id="7b13" class="pi oe fq pf b bg pj pk l pl pm">"&lt;s&gt;[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?&lt;/s&gt; [INST] Write me a haiku about coding. [/INST]"</span></pre><p id="d795" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Why bother with chat templates? Well, it’s strongly advised to match the expected chat template at prediction time (for instance, see the info on “Instruction format” at the <a class="af nc" href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" rel="noopener ugc nofollow" target="_blank">repo</a> for <code class="cx po pp pq pf b">Mixtral-8x7B-Instruct-v0.1</code>). And, with proprietary chat models like <code class="cx po pp pq pf b">gpt-3.5-turbo</code>, chat templates are often applied behind the scenes of an endpoint whether you like it or not!</p><p id="1e2e" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">But how do we know whether chat formatting is indeed improving our performance? Enter LM evals.</p><h1 id="0798" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">LM evals</h1><p id="c35e" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">Evaluations are used to measure an AI/ML model’s performance, and they can take many shapes and sizes. Evals include two core components: a dataset curated for a specific task and associated metric(s) measuring the modeling performance.</p><p id="a4ab" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Generative LM evals carry some additional nuances. For example, different frameworks measure text generation performance in different ways — even varying for the same eval (<a class="af nc" href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" rel="noopener ugc nofollow" target="_blank">reference</a>). When comparing scores across studies, it’s therefore very important to confirm that the results were computed with the same code and config to avoid any errant analysis.</p><p id="ce20" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The superb Instruction-Following Evaluation (<a class="af nc" href="https://arxiv.org/abs/2311.07911" rel="noopener ugc nofollow" target="_blank">IFEval</a>) [2] is used for our testing here. This eval includes 541 prompts that measures a language model’s ability to follow verifiable natural language instructions. Examples of these verifiable instructions include:</p><blockquote class="nd ne nf"><p id="b72b" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">“Write 450 to 500 words”, “your entire output should be in JSON output”, “include a title, and put it into two square brackets such as [[ title ]]”</p></blockquote><p id="531b" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">For a given response and a verifiable instruction, we examine whether the instruction has been followed or not with the following four metrics:</p><blockquote class="nd ne nf"><p id="0536" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">1. <strong class="nj fr">Prompt-level strict-accuracy</strong>: The percentage of prompts that all verifiable instructions in each prompt are followed.</p><p id="c367" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">2. <strong class="nj fr">Inst-level strict-accuracy</strong>: The percentage of verifiable instructions that are followed.</p><p id="eca1" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">3. <strong class="nj fr">Prompt-level loose-accuracy</strong>: Prompt-level accuracy computed with the loose criterion.</p><p id="1c51" class="ng nh ni nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">4. <strong class="nj fr">Inst-level loose-accuracy</strong>: Instruction-level accuracy computed with a loose criterion.</p></blockquote><p id="449b" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The average of these four metrics was computed here (Table 1), primarily to use a single metric that captures the most diverse signal available.</p><p id="4128" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">IFEval is an ideal test for exploring the impacts of chat templates, since the test is specifically designed to measure instruction-following capabilities on chat data. Another interesting line of questioning is whether chat templating positively impacts evals that aren’t as well suited for chat data — a topic left for future research.</p><h1 id="14db" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Chat templates for IFEval</h1><p id="ad3a" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">Eleuther.AI’s <a class="af nc" href="https://github.com/EleutherAI/lm-evaluation-harness" rel="noopener ugc nofollow" target="_blank">lm-eval</a> is the de facto open-source package for LM evaluation. Since chat templating for more models is an oft-requested addition to the library, it was easy to sync up with other developers wanting to work on this feature in the 🤗 model class specifically. At present, development is underway at the <code class="cx po pp pq pf b">add-chat-templating</code> branch (<a class="af nc" href="https://github.com/EleutherAI/lm-evaluation-harness/tree/add-chat-templating" rel="noopener ugc nofollow" target="_blank">link</a>), spurred by issues #1098 (<a class="af nc" href="https://github.com/EleutherAI/lm-evaluation-harness/issues/1098#issuecomment-1947116099" rel="noopener ugc nofollow" target="_blank">link</a>) and #1209 (<a class="af nc" href="https://github.com/EleutherAI/lm-evaluation-harness/issues/1209#issuecomment-1879966071" rel="noopener ugc nofollow" target="_blank">link</a>). When using this branch, we can apply chat formats to an eval as follows:</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="0002" class="pi oe fq pf b bg pj pk l pl pm">!lm_eval --model hf \<br/>    --model_args=pretrained=meta-llama/Llama-2-70b-chat-hf,dtype="bfloat16",parallelize=True,device_map="auto",use_chat_template=True,system_prompt="You are a helpful assistant." \<br/>    --tasks ifeval \<br/>    --batch_size 16 \<br/>    --output_path output/Llama-2-70b-chat-hf \<br/>    --log_samples \<br/>    --num_fewshot 0</span></pre><p id="594f" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The newly introduced triggers <code class="cx po pp pq pf b">use_chat_template</code> and <code class="cx po pp pq pf b">system_prompt</code> appear to the right of <code class="cx po pp pq pf b">model_args</code> and control how the chat template is applied. In the branch’s current experimental form, the code prints the first prompt before and after applying the chat template. Here’s what that looks like for the above code block:</p><pre class="mm mn mo mp mq pe pf pg bp ph bb bk"><span id="439c" class="pi oe fq pf b bg pj pk l pl pm"># First element before prompt formatting...<br/>('Write a 300+ word summary of the wikipedia page "https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.', {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280})<br/><br/># First element after prompt formatting...<br/>('&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nYou are a helpful assistant.\n&lt;&lt;/SYS&gt;&gt;\n\nWrite a 300+ word summary of the wikipedia page "https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*. [/INST]', {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280})</span></pre><p id="2bce" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">The output has taken on the desired chat template!</p><p id="b65c" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">We are now ready to A/B test the influence of chat templates on the IFEval. A handful of popular LLMs were selected for our experiment— each with its own unique chat template. On the larger end we have the 70B parameter <code class="cx po pp pq pf b">Llama-2–70b-chat</code>, two variants of the same 47B parameter model, <code class="cx po pp pq pf b">Mixtral-8x7B-Instruct-v0.1</code> and <code class="cx po pp pq pf b">Nous-Hermes-2-Mixtral-8x7B-DPO</code>, as well as the 34B parameter <code class="cx po pp pq pf b">Nous-Hermes-2-Yi-34B</code>. On the smaller end we have three 7B parameter models: <code class="cx po pp pq pf b">Mistral-Instruct-7B-v0.2</code>, <code class="cx po pp pq pf b">Zephyr-7b-beta</code>, and <code class="cx po pp pq pf b">Starling-LM-7B-alpha</code>. As for the system prompt, a simple “You are a helpful assistant.” was used for compatible models. More details about each of these seven models are included below [3].</p><p id="d32f" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">And, without further delay, our results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pr"><img src="../Images/89bcb4f71233c0c5252237b46374b882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJFe5BEvyteKhwBZCG5lQg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf of">Table 1</strong>: Results from the A/B test on IFEval, sorted by model size descending (<a class="af nc" href="https://docs.google.com/spreadsheets/d/1Tawz9IHH2B-_XWj-JjeVGmu-og60lgSSpMywrGxcj6Q/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">link</a>). See the “Additional Notes” section below for more details, such as links to the run logs. As per reproducibility, the experiments were executed with models in half precision bfloat16, a workstation equipped with 2x H100 80 GB SXM5 chips, and a fork of the lm-eval package at hash <a class="af nc" href="https://github.com/EleutherAI/lm-evaluation-harness/tree/0c0c314c0df4c10f35bf7c17dc80f745f8027e9b" rel="noopener ugc nofollow" target="_blank">0c0c314c0df4c10f35bf7c17dc80f745f8027e9b</a>.</figcaption></figure><p id="8307" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">🔥 Chat templates caused serious shakeup to IFEval scoring! <code class="cx po pp pq pf b">Nous-Hermes-2-Mixtral-8x7B-DPO</code> clocked in as the most performant model tested here, with an average score of ~63%. In contrast, <code class="cx po pp pq pf b">Zephyr-7b-beta</code> was the worst performing model yet had the largest boost from chat templating — a whopping +39%! As a reference, the IFEval paper reported <code class="cx po pp pq pf b">gpt-4</code> (Nov 2023) at an average score of ~81% and <code class="cx po pp pq pf b">PaLM 2S</code>(Aug 2023) at ~51% [2].</p><p id="6382" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">In sum, these results point to a couple key insights:</p><ol class=""><li id="da74" class="ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc ps pt pu bk">Chat templating has a positive impact on instruction-following for open-source LLMs, the extent to which varies by model.</li><li id="fc2a" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc ps pt pu bk">Open-source LLMs are less equipped at following natural language instructions than SOA proprietary models like <code class="cx po pp pq pf b">gpt-4</code>.</li></ol><h1 id="9e1b" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Conclusion</h1><p id="0624" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">Chat templates caused a significant uplift in IFEval scores across the board in our experiment, as proven over a variety of formats and models. However, I don’t necessarily expect these effects to generalize to all LM evals. To further explore the impacts of chat templating on benchmarks, next steps include experimentation with:</p><ul class=""><li id="1ee9" class="ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qa pt pu bk">more instruction-following evals similar to IFEval</li><li id="a0ed" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">general-purpose evals such as those in 🤗’ <a class="af nc" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a></li><li id="9400" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">in-context retrieval evals like “<a class="af nc" href="https://github.com/Arize-ai/LLMTest_NeedleInAHaystack2" rel="noopener ugc nofollow" target="_blank">Needle in a Haystack</a>”</li><li id="1fdd" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">and much, much more!</li></ul><p id="2078" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">Zooming out to a thirty thousand foot level, it’s a great time to research LM evals — for one, because stronger LLMs require a new generation of tests to effectively evaluate them. Whether you create your own or build on top of existing ones, researching evals is an impactful way to contribute to the open science community.</p><h1 id="f3fe" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Citations</h1><p id="ca8f" class="pw-post-body-paragraph ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc fj bk">[1] Matthew Carrigan (2023), <a class="af nc" href="https://huggingface.co/blog/chat-templates" rel="noopener ugc nofollow" target="_blank">Chat Templates: An End to the Silent Performance Killer</a>, Hugging Face.</p><p id="f06e" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">[2] Zhou et al. (2023),<a class="af nc" href="https://arxiv.org/pdf/2311.07911.pdf" rel="noopener ugc nofollow" target="_blank"> Instruction-Following Evaluation for Large Language Models</a>, arXiv.</p><ul class=""><li id="4d78" class="ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qa pt pu bk"><strong class="nj fr">Dataset licensing</strong>: The IFEval dataset used herein is publicly available to all without restriction (<a class="af nc" href="https://github.com/google-research/google-research/tree/master#Apache-2.0-1-ov-file" rel="noopener ugc nofollow" target="_blank">Apache-2.0 license</a>).</li></ul><p id="fac7" class="pw-post-body-paragraph ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc fj bk">[3] Models used here, from largest to smallest (all permissively licensed for research use).</p><ul class=""><li id="17f8" class="ng nh fq nj b go nk nl nm gr nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Llama-2–70b-chat</code> (<a class="af nc" href="http://meta-llama/Llama-2-70b-chat-hf" rel="noopener ugc nofollow" target="_blank">link</a>) — Meta</li><li id="d432" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Mixtral-8x7B-Instruct-v0.1</code> (<a class="af nc" href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1" rel="noopener ugc nofollow" target="_blank">link</a>) — Mistral.AI</li><li id="2b1e" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Nous-Hermes-2-Mixtral-8x7B-DPO</code> (<a class="af nc" href="https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO" rel="noopener ugc nofollow" target="_blank">link</a>) — Nous-Research</li><li id="3e81" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Nous-Hermes-2-Yi-34B</code> (<a class="af nc" href="http://NousResearch/Nous-Hermes-2-Yi-34B" rel="noopener ugc nofollow" target="_blank">link</a>) — Nous-Research</li><li id="065f" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Starling-LM-7B-alpha</code> (<a class="af nc" href="https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha" rel="noopener ugc nofollow" target="_blank">link</a>) — Berkeley NEST</li><li id="c194" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Zephyr-7B-beta</code> (<a class="af nc" href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta" rel="noopener ugc nofollow" target="_blank">link</a>) — Hugging Face</li><li id="5c88" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk"><code class="cx po pp pq pf b">Mistral-7B-Instruct-v0.2</code> (<a class="af nc" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">link</a>) — Mistral.AI</li></ul><h1 id="df9e" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Additional Notes</h1><ul class=""><li id="e773" class="ng nh fq nj b go oz nl nm gr pa no np nq pb ns nt nu pc nw nx ny pd oa ob oc qa pt pu bk">See the notebooks <a class="af nc" href="https://github.com/daniel-furman/evals-with-chat-formats" rel="noopener ugc nofollow" target="_blank">here</a> for the code used to run the experiments.</li><li id="d916" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">To audit the results, see outputs for each run <a class="af nc" href="https://github.com/daniel-furman/evals-with-chat-formats/tree/main/assets/IFEval_results" rel="noopener ugc nofollow" target="_blank">here</a> as well as Zeno logs <a class="af nc" href="https://hub.zenoml.com/project/79b4684d-0f4e-48f4-b739-ba4e0bd63ee8/IFEval-chat-templating-experiments-run-1" rel="noopener ugc nofollow" target="_blank">here</a> and <a class="af nc" href="https://hub.zenoml.com/project/548fcb7a-52cf-4c60-aaf7-13d6b03343fd/IFEval-chat-templating-experiments-run-2" rel="noopener ugc nofollow" target="_blank">here</a> (models were ran in 2 total batches). Note that the Zeno logs don’t yet capture the application of chat templates to the prompts — this is a “to do” item in development backlog.</li><li id="704c" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">For compute, RunPod (<a class="af nc" href="https://www.runpod.io/" rel="noopener ugc nofollow" target="_blank">link</a>) was used for access to workstations with Nvidia GPU chips — in particular, a cluster with 2x H100 80 GB SXM5 chips. In total, the experiment included 14 runs of the IFEval, which accumulated ~6 hrs of cluster uptime.</li><li id="c013" class="ng nh fq nj b go pv nl nm gr pw no np nq px ns nt nu py nw nx ny pz oa ob oc qa pt pu bk">Confidence intervals were taken to estimate statistical uncertainty in our results (the bootstrap resampling method was used). These 95% confidence intervals ranged from roughly +/- 2.75% to 4.25% — small relative to the measured effects of chat templating.</li></ul></div></div></div></div>    
</body>
</html>