- en: Build your Personal Assistant with Agents and Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-your-personal-assistant-with-agents-and-tools-048637ac308e?source=collection_archive---------0-----------------------#2024-11-24](https://towardsdatascience.com/build-your-personal-assistant-with-agents-and-tools-048637ac308e?source=collection_archive---------0-----------------------#2024-11-24)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LLMs alone suffer from not being able to access external or real-time data.
    Learn how to build your personal assistant using LangChain agents and Gemini by
    grounding it in external sources.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benjamin_47408?source=post_page---byline--048637ac308e--------------------------------)[![Benjamin
    Etienne](../Images/cad8bc2d4b900575e76b7cf9debc9eea.png)](https://medium.com/@benjamin_47408?source=post_page---byline--048637ac308e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--048637ac308e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--048637ac308e--------------------------------)
    [Benjamin Etienne](https://medium.com/@benjamin_47408?source=post_page---byline--048637ac308e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--048637ac308e--------------------------------)
    ·14 min read·Nov 24, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Summary:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with LLMs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are Agents, Tools and Chains ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a simple chat without Tools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adding Tools to our chat: The Google way with Function Calling'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adding Tools to our chat : The Langchain way with Agents'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding Memory to our Agent
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a Chain with a Human Validation step
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using search tools
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1\. The problem with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So you have your favorite chatbot, and you use it for your daily job to boost
    your productivity. It can translate text, write nice emails, tell jokes, etc.
    And then comes the day when your colleague comes to you and asks :'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Do you know the current exchange rate between USD and EUR ? I wonder if I
    should sell my EUR…”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You ask your favorite chatbot, and the answer pops :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*What is the problem here ?*'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that you have stumbled on one of the shortcomings of LLMs. Large
    Language Models (LLMs) are powerful at solving many types of problems, such as
    problem solving, text summarization, generation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, they are constrained by the following limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**They are frozen after training, leading to stale knowledge.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They can’t query or modify external data.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same way as we are using search engines every day, reading books and documents
    or querying databases, we would ideally want to provide this knowledge to our
    LLM to make it more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, there is a way to do that: Tools and Agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Foundational models, despite their impressive text and image generation, remain
    constrained by their inability to interact with the outside world. Tools bridge
    this gap, empowering agents to interact with external data and services while
    unlocking a wider range of actions beyond that of the underlying model alone
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*(source : Google Agents whitepaper)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using agents and tools, we could then be able to, from our chat interface:'
  prefs: []
  type: TYPE_NORMAL
- en: retrieve data from our own documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: read / send emails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: interact with internal databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: perform real time Google searches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. What are Agents, Tools and Chains ?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An *agent* is an application which attempts to achieve a goal (or a task) by
    having at its disposal a set of tools and taking decisions based on its observations
    of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good example of an agent could be you, for example: if you need to compute
    a complex mathematical operation (goal), you could use a calculator (tool #1),
    or a programming language (tool #2). Maybe you would choose the calculator to
    do a simple addition, but choose tool #2 for more complex algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agents are therefore made of :'
  prefs: []
  type: TYPE_NORMAL
- en: 'A model : The brain in our agent is the LLM. It will understand the query (the
    goal), and browse through its tools available to select the best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One or more *tools* : These are functions, or APIs, that are responsible for
    performing a specific action (ie: retrieving the current currency rate for USD
    vs EUR, adding numbers, etc.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An orchestration process: this is how the model will behave when asked to solve
    a task. It is a cognitive process that defines how the model will analyze the
    problem, refine inputs, choose a tool, etc. Examples of such processes are ReAct,
    CoT (Chain of Thought), ToT (Tree-of-Thought)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is below a workflow explanation
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85a293f46127b027cf0d29236c2e7d04.png)'
  prefs: []
  type: TYPE_IMG
- en: image by author
  prefs: []
  type: TYPE_NORMAL
- en: '*Chains* are somehow different. Whereas agents can ‘decide’ by themselves what
    to do and which steps to take, chains are just a sequence of predefined steps.
    They can still rely on tools though, meaning that they can include a step in which
    they need to select from available tools. We’ll cover that later.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Creating a simple chat without Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate our point, we will first of all see how our LLM performs as-is,
    without any help.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s install the needed libraries :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And create our very simple chat using Google’s Gemini LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this simple chat and ask a question about the current exchange rate,
    you might probably get a similar answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Not surprising, as we know LLMs do not have access to real-time data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s add a tool for that. Our tool will be little function that calls an API
    to retrieve exchange rate data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we know how our tools works, we would like to tell our chat LLM to use
    this function to answer our question. We will therefore create a mono-tool agent.
    To do that, we have several options which I will list here:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Google’s Gemini chat API with Function Calling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LangChain’s API with Agents and Tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both have their advantages and drawbacks. The purpose of this article is also
    to show you the possibilities and let you decide which one you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Adding Tools to our chat: The Google way with Function Calling'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are basically two ways of creating a tool out of a function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 1st one is a “dictionary” approach where you specify inputs and description
    of the function in the Tool. The imporant parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the function (be explicit)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Description : be verbose here, as a solid and exhaustive description will help
    the LLM select the right tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parameters : this is where you specify your arguments (type and description).
    Again, be verbose in the description of your arguments to help the LLM know how
    to pass value to your function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The 2nd way of adding a tool using Google’s SDK is with a `from_func` instantiation.
    This requires editing our original function to be more explicit, with a docstring,
    etc. Instead of being verbose in the Tool creation, we are being verbose in the
    function creation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is really about creating the tool. For that, we will add our
    FunctionDeclaration to a list to create our Tool object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now pass that to our chat and see if it now can answer our query about
    exchange rates ! Remember, without tools, our chat answered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8206400b049669a30d50ea74965d1d7a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s try Google’s Function calling tool and see if this helps ! First, let’s
    send our query to the chat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The LLM correctly guessed it needed to use the `get_exchange_rate` function,
    and also correctly guessed the 2 parameters were `USD` and `EUR` .
  prefs: []
  type: TYPE_NORMAL
- en: But this is not enough. What we want now is to actually run this function to
    get our results!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see our chat is able to answer our question! It:'
  prefs: []
  type: TYPE_NORMAL
- en: Correctly guessed to function to call, `get_exchange_rate`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Correctly assigned the parameters to call the function `{‘currency_from’: ‘USD’,
    ‘currency_to’: ‘EUR’}`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Got results from the API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And nicely formatted the answer to be human-readable!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now see another way of doing with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Adding Tools to our chat: The Langchain way with Agents'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain is a composable framework to build with LLMs. It is the orchestration
    framework for controllable agentic workflows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to what we did before the “Google” way, we will build tools in the
    Langchain way. Let’s begin with defining our functions. Same as for Google, we
    need to be exhaustive and verbose in the docstrings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to spice things up, I will add another tool which can list tables
    in a BigQuery dataset. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Add once done, we add our functions to our LangChain toolbox !
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'To build our agent, we will use the `AgentExecutor`object from LangChain. This
    object will basically take 3 components, which are the ones we defined earlier
    :'
  prefs: []
  type: TYPE_NORMAL
- en: A LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s first choose our LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we create a prompt to manage the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, we create the `AgentExecutor` and run a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Hmmm. Seems like the agent is missing one argument, or at least asking for
    more information…Let’s reply by giving this information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Well, seems we’re back to square one. The LLM has been told the project id but
    forgot about the question. Our agent seems to be lacking memory to remember previous
    questions and answers. Maybe we should think of…
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Adding Memory to our Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Memory is another concept in Agents, which basically helps the system to remember
    the conversation history and avoid endless loops like above. Think of memory as
    being a notepad where the LLM keeps track of previous questions and answers to
    build context around the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will modify our prompt (instructions) to the model to include memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now rerun our query from the beginning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With an empty chat history, the model still asks for the project id. Pretty
    consistent with what we had before with a memoryless agent. Let’s reply to the
    agent and add the missing information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice how, in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: The `chat history` keeps track of the previous Q&A
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output now returns the list of the tables!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In some use cases however, certain actions might require special attention because
    of their nature (ie deleting an entry in a database, editing information, sending
    an email, etc.). Full automation without control might leads to situations where
    the agent takes wrong decisions and creates damage.
  prefs: []
  type: TYPE_NORMAL
- en: One way to secure our workflows is to add a human-in-the-loop step.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Creating a Chain with a Human Validation step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A chain is somehow different from an agent. Whereas the agent can decide to
    use or not to use tools, a chain is more static. It is a sequence of steps, for
    which we can still include a step where the LLM will choose from a set of tools.
  prefs: []
  type: TYPE_NORMAL
- en: To build chains in LangChain, we use LCEL.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Expression Language, or LCEL, is a declarative way to easily compose
    chains together. Chains in LangChain use the pipe `|` operator to indicate the
    orders in which steps have to be executed, such as `step 1 | step 2 | step 3 etc.`
    The difference with Agents is that Chains will always follow those steps, whereas
    Agents can “decide” by themselves and are autonomous in their decision-making
    process.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we will proceed as follows to build a simple `prompt | llm` chain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Remember how in the previous step we passed an agent to our `RunnableWithMessageHistory`?
    Well, we will do the same here, but...
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Unlike the agent, a chain does not provide the answer unless we tell it to.
    In our case, it stopped at the step where the LLM returns the function that needs
    to be called.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to add an extra step to actually *call* the tool. Let’s add another
    function to call the tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now get the following output, which shows the API has been successfully
    called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now we understood how to chain steps, let’s add our human-in-the-loop step !
    We want this step to check that the LLM has understood our requests and will make
    the right call to an API. If the LLM has misunderstood the request or will use
    the function incorrectly, we can decide to interrupt the process.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add this step to the chain before the function call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You will then be asked to confirm that the LLM understood correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5028f700edfcffd815b18eeb32e9da41.png)'
  prefs: []
  type: TYPE_IMG
- en: This human-in-the-loop step can be very helpful for critical workflows where
    a misinterpretation from the LLM could have dramatic consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Using search tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most convenient tools to retrieve information in real-time are search
    engines . One way to do that is to use `GoogleSerperAPIWrapper` (you will need
    to register to get an API key in order to use it), which provides a nice interface
    to query Google Search and get results quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, LangChain already provides a tool for you, so we won’t have to write
    the function ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s therefore try to ask a question on yesterday’s event (Nov 20th) and see
    if our agent can answer. Our question is about Rafael Nadal’s last official game
    (which he lost to van de Zandschulp).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Without being able to access Google Search, our model is unable to answer because
    this information was not available at the time it was trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now add our Serper tool to our toolbox and see if our model can use Google
    Search to find the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'And rerun our query :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs alone often hit a blocker when it comes to using personal, corporate, private
    or real-data. Indeed, such information is generally not available at training
    time. Agents and tools are a powerful way to augment these models by allowing
    them to interact with systems and APIs, and orchestrate workflows to boost productivity.
  prefs: []
  type: TYPE_NORMAL
