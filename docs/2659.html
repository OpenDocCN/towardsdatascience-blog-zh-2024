<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Minimum Viable MLE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Minimum Viable MLE</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/minimum-viable-mle-306877dd6030?source=collection_archive---------9-----------------------#2024-10-31">https://towardsdatascience.com/minimum-viable-mle-306877dd6030?source=collection_archive---------9-----------------------#2024-10-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="eb7e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Building a minimal production-ready sentiment analysis model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lenixc210?source=post_page---byline--306877dd6030--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lenix Carter" class="l ep by dd de cx" src="../Images/d25c86c00d6b2ee64b70cae8297fd761.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*iCuSet__hNRYxuBMlfgE6w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--306877dd6030--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lenixc210?source=post_page---byline--306877dd6030--------------------------------" rel="noopener follow">Lenix Carter</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--306877dd6030--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/27789732b7a4449b4ff94b4e82876275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vGKPMIjDnhQZxP8b"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@dawson2406?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Stephen Dawson</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="93c1" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">What is a production-ready model?</h1><p id="0896" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">We hear a lot about productionized machine learning, but what does it really mean to have a model that can thrive in real-world applications?There are plenty of things that go into, and contribute, to the efficacy of a machine learning model in production. For the sake of this article we will be focusing on five of them.</p><ul class=""><li id="67fa" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou pa pb pc bk"><strong class="ob fr">Reproducibility</strong></li><li id="dc97" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><strong class="ob fr">Monitoring</strong></li><li id="6432" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><strong class="ob fr">Testing</strong></li><li id="40b5" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><strong class="ob fr">Automation</strong></li><li id="6e58" class="nz oa fq ob b go pd od oe gr pe og oh oi pf ok ol om pg oo op oq ph os ot ou pa pb pc bk"><strong class="ob fr">Version Control</strong></li></ul></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="73e4" class="pq ne fq bf nf pr ps pt ni pu pv pw nl oi px py pz om qa qb qc oq qd qe qf qg bk">Serving Inferences</h2><p id="03fe" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The most important part of building a production-ready machine learning model is being able to access it.</p><p id="2888" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For this purpose, we build a fastapi client that serves sentiment analysis responses. We utilize pydantic to ensure structure for the input and output. The model that we use is the base sentiment analysis pipeline from huggingface’s transformers library, allowing us to begin testing with a pre-trained model.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="d0d6" class="ql ne fq qi b bg qm qn l qo qp"># Filename: main.py<br/>from fastapi import FastAPI<br/>from pydantic import BaseModel<br/>from transformers import pipeline<br/><br/>app = FastAPI()<br/>classifier = pipeline("sentiment-analysis")<br/><br/>class TextInput(BaseModel):<br/>    text: str<br/><br/>class SentimentOutput(BaseModel):<br/>    text: str<br/>    sentiment: str<br/>    score: float<br/><br/>@app.post("/predict", response_model=SentimentOutput)<br/>async def predict_sentiment(input_data: TextInput):<br/>    result = classifier(input_data.text)[0]<br/>    return SentimentOutput(<br/>        text=input_data.text,<br/>        sentiment=result["label"],<br/>        score=result["score"]<br/>    )</span></pre><p id="6b8c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To ensure that our work is reproducible, we can use a requirements.txt file and pip.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="b2c9" class="ql ne fq qi b bg qm qn l qo qp"># Filename: requirements.txt<br/># Note: This has all required packages for the final result. <br/><br/>fastapi==0.68.1<br/>uvicorn==0.15.0<br/>transformers==4.30.0<br/>torch==2.0.0<br/>pydantic==1.10.0<br/>numpy==1.24.3<br/>sentencepiece==0.1.99<br/>protobuf==3.20.3<br/>prometheus-client==0.17.1</span></pre><p id="9311" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To install this, initialize <a class="af nc" href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/" rel="noopener ugc nofollow" target="_blank">venv in your files</a> and run:<code class="cx qq qr qs qi b">pip install -r requirements.txt</code>.</p><p id="d34d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To host this API simply run: <code class="cx qq qr qs qi b">uvicorn main:app --reload</code>.</p><p id="907c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now you have an api that you can query using:</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="d3b4" class="ql ne fq qi b bg qm qn l qo qp">curl -X POST "http://localhost:8000/predict" \<br/>  -H "Content-Type: application/json" \<br/>  -d '{"text": "I love using FastAPI!"}'</span></pre><p id="5ac7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">or any API tool you wish (i.e. <a class="af nc" href="https://www.postman.com/" rel="noopener ugc nofollow" target="_blank">Postman</a>). You should get a result back that includes the text query, the sentiment predicted, and the confidence of the prediction.</p><p id="2799" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We will be using GitHub for CI/CD later, so I would recommend <a class="af nc" href="https://git-scm.com/docs/gittutorial" rel="noopener ugc nofollow" target="_blank">initializing and using git in this directory</a>.</p><p id="5a53" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We now have a locally hosted machine learning inference API.</p></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="3d93" class="pq ne fq bf nf pr ps pt ni pu pv pw nl oi px py pz om qa qb qc oq qd qe qf qg bk">Further Improving Reproducibility</h2><p id="2807" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">To allow our code to have more consistent execution, we will utilize Docker. Docker simulates a lightweight environment that allows applications to run in isolated containers, similar to virtual machines. This isolation ensures that applications can execute consistently across any computer with Docker installed, regardless of the underlying system.</p><p id="5f77" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Firstly, <a class="af nc" href="https://docs.docker.com/desktop/" rel="noopener ugc nofollow" target="_blank">set up Docker for your given operating system.</a></p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="e799" class="ql ne fq qi b bg qm qn l qo qp"># Filename: Dockerfile<br/><br/># Use the official Python 3.9 slim image as the base<br/>FROM python:3.9-slim<br/><br/># Set the working directory inside the container to /app<br/>WORKDIR /app<br/><br/># Copy the requirements.txt file to the working directory<br/>COPY requirements.txt .<br/><br/># Install the Python dependencies listed in requirements.txt<br/>RUN pip install -r requirements.txt<br/><br/># Copy the main application file (main.py) to the working directory<br/>COPY main.py .<br/><br/># Define the command to run the FastAPI application with Uvicorn<br/>CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]</span></pre><p id="fe49" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">At this point, you should have the directory as below.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="cb5e" class="ql ne fq qi b bg qm qn l qo qp">your-project/<br/>├── Dockerfile<br/>├── requirements.txt<br/>└── main.py</span></pre><p id="36c9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, you can build the image and run this API using:</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="194d" class="ql ne fq qi b bg qm qn l qo qp"># Build the Docker image<br/>docker build -t sentiment-api .<br/><br/># Run the container<br/>docker run -p 8000:8000 sentiment-api</span></pre><p id="be53" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You should now be able to query just as you did before.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="0ddf" class="ql ne fq qi b bg qm qn l qo qp">curl -X POST "http://localhost:8000/predict" \<br/>  -H "Content-Type: application/json" \<br/>  -d '{"text": "I love using FastAPI!"}'</span></pre><p id="ef38" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We now have a containerized, locally hosted machine learning inference API.</p></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="bf82" class="pq ne fq bf nf pr ps pt ni pu pv pw nl oi px py pz om qa qb qc oq qd qe qf qg bk">Adding Basic Monitoring</h2><p id="d16a" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In machine learning applications, monitoring is crucial for understanding model performance and ensuring it meets expected accuracy and efficiency. Tools like <a class="af nc" href="https://prometheus.io/docs/tutorials/getting_started/" rel="noopener ugc nofollow" target="_blank">Prometheus</a> help track metrics such as prediction latency, request counts, and model output distributions, enabling you to identify issues like model drift or resource bottlenecks. This proactive approach ensures that your ML models remain effective over time and can adapt to changing data or usage patterns. In our case, we are focused on prediction time, requests, and gathering information about our queries.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="6109" class="ql ne fq qi b bg qm qn l qo qp">from fastapi import FastAPI<br/>from pydantic import BaseModel<br/>from transformers import pipeline<br/>from prometheus_client import Counter, Histogram, start_http_server<br/>import time<br/><br/># Start prometheus metrics server on port 8001<br/>start_http_server(8001)<br/><br/>app = FastAPI()<br/><br/># Metrics<br/>PREDICTION_TIME = Histogram('prediction_duration_seconds', 'Time spent processing prediction')<br/>REQUESTS = Counter('prediction_requests_total', 'Total requests')<br/>SENTIMENT_SCORE = Histogram('sentiment_score', 'Histogram of sentiment scores', buckets=[0.0, 0.25, 0.5, 0.75, 1.0])<br/><br/>class TextInput(BaseModel):<br/>    text: str<br/><br/>class SentimentOutput(BaseModel):<br/>    text: str<br/>    sentiment: str<br/>    score: float<br/><br/>@app.post("/predict", response_model=SentimentOutput)<br/>async def predict_sentiment(input_data: TextInput):<br/>    REQUESTS.inc()<br/>    start_time = time.time()<br/>    <br/>    result = classifier(input_data.text)[0]<br/>    <br/>    score = result["score"]<br/>    SENTIMENT_SCORE.observe(score)  # Record the sentiment score<br/>    <br/>    PREDICTION_TIME.observe(time.time() - start_time)<br/>    <br/>    return SentimentOutput(<br/>        text=input_data.text,<br/>        sentiment=result["label"],<br/>        score=score<br/>    )</span></pre></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="b08f" class="pq ne fq bf nf pr ps pt ni pu pv pw nl oi px py pz om qa qb qc oq qd qe qf qg bk">Utilizing a Custom Model</h2><p id="ebe0" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">While the process of building and fine-tuning a model is not the intent of this project, it is important to understand how a model can be added to this process.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="b7b5" class="ql ne fq qi b bg qm qn l qo qp"># Filename: train.py<br/><br/>import torch<br/>from transformers import AutoTokenizer, AutoModelForSequenceClassification<br/>from datasets import load_dataset<br/>from torch.utils.data import DataLoader<br/><br/>def train_model():<br/>    # Load dataset<br/>    full_dataset = load_dataset("stanfordnlp/imdb", split="train")<br/>    dataset = full_dataset.shuffle(seed=42).select(range(10000))<br/><br/>    model_name = "distilbert-base-uncased"<br/>    tokenizer = AutoTokenizer.from_pretrained(model_name)<br/>    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)<br/><br/>    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)<br/>    <br/>    # Use GPU if available<br/>    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>    model.to(device)<br/><br/>    model.train()<br/><br/>    # Create a DataLoader for batching<br/>    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)<br/><br/>    # Training loop<br/>    num_epochs = 3  # Set the number of epochs<br/>    for epoch in range(num_epochs):<br/>        total_loss = 0<br/>        for batch in dataloader:<br/>            inputs = tokenizer(batch["text"], truncation=True, padding=True, return_tensors="pt", max_length=512).to(device)<br/>            labels = torch.tensor(batch["label"]).to(device)<br/>            <br/>            optimizer.zero_grad()<br/>            outputs = model(**inputs, labels=labels)<br/>            loss = outputs.loss<br/>            <br/>            loss.backward()<br/>            optimizer.step()<br/>            total_loss += loss.item()<br/>        <br/>        avg_loss = total_loss / len(dataloader)<br/>        print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}")<br/><br/>    # Save the model<br/>    model.save_pretrained("./model/")<br/>    tokenizer.save_pretrained("./model/")<br/><br/>    # Test the model with sample sentences<br/>    test_sentences = [<br/>        "This movie was fantastic!",<br/>        "I absolutely hated this film.",<br/>        "It was just okay, not great.",<br/>        "An absolute masterpiece!",<br/>        "Waste of time!",<br/>        "A beautiful story and well acted.",<br/>        "Not my type of movie.",<br/>        "It could have been better.",<br/>        "A thrilling adventure from start to finish!",<br/>        "Very disappointing."<br/>    ]<br/><br/>    # Switch model to evaluation mode<br/>    model.eval()<br/><br/>    # Prepare tokenizer for test inputs<br/>    inputs = tokenizer(test_sentences, truncation=True, padding=True, return_tensors="pt", max_length=512).to(device)<br/>    <br/>    with torch.no_grad():<br/>        outputs = model(**inputs)<br/>        predictions = torch.argmax(outputs.logits, dim=1)<br/><br/>    # Print predictions<br/>    for sentence, prediction in zip(test_sentences, predictions):<br/>        sentiment = "positive" if prediction.item() == 1 else "negative"<br/>        print(f"Input: \"{sentence}\" -&gt; Predicted sentiment: {sentiment}")<br/><br/># Call the function to train the model and test it<br/>train_model()</span></pre><p id="9daa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To make sure that we can query our new model that we have trained we have to update a few of our existing files. For instance, in <code class="cx qq qr qs qi b">main.py</code> we now use the model from <code class="cx qq qr qs qi b">./model</code> and load it as a pretrained model. Additionally, for comparison’s sake, we add now have two endpoints to use, <code class="cx qq qr qs qi b">/predict/naive</code> and <code class="cx qq qr qs qi b">predict/trained</code>.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="4891" class="ql ne fq qi b bg qm qn l qo qp"># Filename: main.py<br/><br/>from fastapi import FastAPI<br/>from pydantic import BaseModel<br/>from transformers import AutoModelForSequenceClassification, AutoTokenizer<br/>from transformers import pipeline<br/>from prometheus_client import Counter, Histogram, start_http_server<br/>import time<br/><br/># Start prometheus metrics server on port 8001<br/>start_http_server(8001)<br/><br/>app = FastAPI()<br/><br/># Load the trained model and tokenizer from the local directory<br/>model_path = "./model"  # Path to your saved model<br/>tokenizer = AutoTokenizer.from_pretrained(model_path)<br/>trained_model = AutoModelForSequenceClassification.from_pretrained(model_path)<br/><br/># Create pipelines<br/>naive_classifier = pipeline("sentiment-analysis", device=-1)<br/>trained_classifier = pipeline("sentiment-analysis", model=trained_model, tokenizer=tokenizer, device=-1)<br/><br/># Metrics<br/>PREDICTION_TIME = Histogram('prediction_duration_seconds', 'Time spent processing prediction')<br/>REQUESTS = Counter('prediction_requests_total', 'Total requests')<br/>SENTIMENT_SCORE = Histogram('sentiment_score', 'Histogram of sentiment scores', buckets=[0.0, 0.25, 0.5, 0.75, 1.0])<br/><br/>class TextInput(BaseModel):<br/>    text: str<br/><br/>class SentimentOutput(BaseModel):<br/>    text: str<br/>    sentiment: str<br/>    score: float<br/><br/>@app.post("/predict/naive", response_model=SentimentOutput)<br/>async def predict_naive_sentiment(input_data: TextInput):<br/>    REQUESTS.inc()<br/>    start_time = time.time()<br/>    <br/>    result = naive_classifier(input_data.text)[0]<br/>    <br/>    score = result["score"]<br/>    SENTIMENT_SCORE.observe(score)  # Record the sentiment score<br/>    <br/>    PREDICTION_TIME.observe(time.time() - start_time)<br/>    <br/>    return SentimentOutput(<br/>        text=input_data.text,<br/>        sentiment=result["label"],<br/>        score=score<br/>    )<br/><br/>@app.post("/predict/trained", response_model=SentimentOutput)<br/>async def predict_trained_sentiment(input_data: TextInput):<br/>    REQUESTS.inc()<br/>    start_time = time.time()<br/>    <br/>    result = trained_classifier(input_data.text)[0]<br/>    <br/>    score = result["score"]<br/>    SENTIMENT_SCORE.observe(score)  # Record the sentiment score</span></pre><p id="8ff4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We also must update our Dockerfile to include our model files.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="055b" class="ql ne fq qi b bg qm qn l qo qp"># Filename: Dockerfile<br/>FROM python:3.9-slim<br/><br/>WORKDIR /app<br/><br/>COPY requirements.txt .<br/>RUN pip install -r requirements.txt<br/><br/>COPY main.py .<br/>COPY ./model ./model<br/><br/>CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]</span></pre><p id="2119" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Importantly, if you are using git, make sure that you add the <code class="cx qq qr qs qi b">pytorch_model.bin</code> file to <a class="af nc" href="https://git-lfs.com/" rel="noopener ugc nofollow" target="_blank">git lfs</a>, so that you can push to GitHub. git lfs allows you to use version control on very large files.</p></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="595d" class="pq ne fq bf nf pr ps pt ni pu pv pw nl oi px py pz om qa qb qc oq qd qe qf qg bk">Adding Testing and CI/CD</h2><p id="3db9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">CI/CD and testing streamline the deployment of machine learning models by ensuring that code changes are automatically integrated, tested, and deployed, which reduces the risk of errors and enhances model reliability. This process promotes continuous improvement and faster iteration cycles, allowing teams to deliver high-quality, production-ready models more efficiently. Firstly, we create two very basic tests to ensure that our model is performing acceptably.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="a2ad" class="ql ne fq qi b bg qm qn l qo qp"># Filename: test_model.py<br/><br/>import pytest<br/>from fastapi.testclient import TestClient<br/>from main import app<br/><br/>client = TestClient(app)<br/><br/>def test_positive_sentiment():<br/>    response = client.post(<br/>        "/predict/trained",<br/>        json={"text": "This is amazing!"}<br/>    )<br/>    assert response.status_code == 200<br/>    data = response.json()<br/>    assert data["sentiment"] == "LABEL_1"<br/>    assert data["score"] &gt; 0.5<br/><br/><br/>def test_negative_sentiment():<br/>    response = client.post(<br/>        "/predict/trained",<br/>        json={"text": "This is terrible!"}<br/>    )<br/>    assert response.status_code == 200<br/>    data = response.json()<br/>    assert data["sentiment"] == "LABEL_0"<br/>    assert data["score"] &lt; 0.5</span></pre><p id="014b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">To test your code, you can simply run <code class="cx qq qr qs qi b">pytest</code> or <code class="cx qq qr qs qi b">python -m pytest</code> while your endpoint is running.</p><p id="f696" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">However, we will add automated testing CI/CD (continuous integration and continuous delivery) when pushed to GitHub.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="6b12" class="ql ne fq qi b bg qm qn l qo qp"># Filename: .github/workflows/ci_cd.yml<br/><br/>name: CI/CD<br/><br/>on: [push]<br/><br/>jobs:<br/>  test:<br/>    runs-on: ubuntu-latest<br/>    steps:<br/>      - name: Checkout code<br/>        uses: actions/checkout@v2<br/>        with:<br/>          lfs: true<br/><br/>      - name: Set up Python<br/>        uses: actions/setup-python@v2<br/>        with:<br/>          python-version: '3.9'<br/><br/>      - name: Install dependencies<br/>        run: |<br/>          pip install -r requirements.txt<br/>          pip install pytest httpx<br/><br/>      - name: Run tests<br/>        run: pytest</span></pre><p id="bf2f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Our final project structure should appear as below.</p><pre class="mm mn mo mp mq qh qi qj bp qk bb bk"><span id="56c3" class="ql ne fq qi b bg qm qn l qo qp">sentiment-analysis-project/<br/>├── .github/<br/>│   └── workflows/<br/>│       └── ci_cd.yml<br/>├── test_model.py<br/>├── main.py<br/>├── Dockerfile<br/>├── requirements.txt<br/>└── train.py</span></pre><p id="f50c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, whenever we push to GitHub, it will run an automated process that checks out the code, sets up a Python 3.9 environment, installs dependencies, and runs our tests using pytest.</p></div></div></div><div class="ab cb pi pj pk pl" role="separator"><span class="pm by bm pn po pp"/><span class="pm by bm pn po pp"/><span class="pm by bm pn po"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d7e7" class="nd ne fq bf nf ng qt gq ni nj qu gt nl nm qv no np nq qw ns nt nu qx nw nx ny bk">Conclusion</h1><p id="1d99" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this project, we’ve developed a production-ready sentiment analysis API that highlights key aspects of deploying machine learning models. While it doesn’t encompass every facet of the field, it provides a representative sampling of essential tasks involved in the process. By examining these components, I hope to clarify concepts you may have encountered but weren’t quite sure how they fit together in a practical setting.</p></div></div></div></div>    
</body>
</html>