- en: How to Estimate Depth from a Single Image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä»å•å¼ å›¾åƒä¸­ä¼°è®¡æ·±åº¦
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25](https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25](https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25)
- en: Run and evaluate monocular depth estimation models with Hugging Face and FiftyOne
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Hugging Faceå’ŒFiftyOneè¿è¡Œå¹¶è¯„ä¼°å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹
- en: '[](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    Â·10 min readÂ·Jan 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2024å¹´1æœˆ25æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
- en: Monocular Depth heat maps generated with Marigold on NYU depth v2 images. Image
    courtesy of the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Marigoldåœ¨NYUæ·±åº¦v2å›¾åƒä¸Šç”Ÿæˆçš„å•ç›®æ·±åº¦çƒ­å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Humans view the world through two eyes. One of the primary benefits of this
    *binocular* vision is the ability to perceive *depth* â€” how near or far objects
    are. The human brain infers object depths by comparing the pictures captured by
    left and right eyes at the same time and interpreting the disparities. This process
    is known as [stereopsis](https://en.wikipedia.org/wiki/Stereopsis).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: äººç±»é€šè¿‡ä¸¤åªçœ¼ç›çœ‹ä¸–ç•Œã€‚åŒçœ¼è§†åŠ›çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯èƒ½å¤Ÿæ„ŸçŸ¥*æ·±åº¦*â€”â€”ç‰©ä½“ç¦»è¿‘è¿˜æ˜¯è¿œã€‚äººè„‘é€šè¿‡æ¯”è¾ƒå·¦çœ¼å’Œå³çœ¼åŒæ—¶æ•æ‰åˆ°çš„å›¾åƒï¼Œå¹¶è§£é‡Šè¿™äº›å·®å¼‚ï¼Œä»è€Œæ¨æ–­ç‰©ä½“çš„æ·±åº¦ã€‚è¿™ä¸ªè¿‡ç¨‹è¢«ç§°ä¸º[ç«‹ä½“è§†è§‰](https://en.wikipedia.org/wiki/Stereopsis)ã€‚
- en: Just as depth perception plays a crucial role in human vision and navigation,
    the ability to estimate depth is critical for a wide range of computer vision
    applications, from autonomous driving to robotics, and even augmented reality.
    Yet a slew of practical considerations from spatial limitations to budgetary constraints
    often limit these applications to a single camera.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæ·±åº¦æ„ŸçŸ¥åœ¨äººç±»è§†è§‰å’Œå¯¼èˆªä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ä¸€æ ·ï¼Œä¼°è®¡æ·±åº¦çš„èƒ½åŠ›å¯¹äºå¹¿æ³›çš„è®¡ç®—æœºè§†è§‰åº”ç”¨ä¹Ÿè‡³å…³é‡è¦ï¼Œä»è‡ªåŠ¨é©¾é©¶åˆ°æœºå™¨äººï¼Œå†åˆ°å¢å¼ºç°å®ã€‚ç„¶è€Œï¼Œç”±äºç©ºé—´é™åˆ¶å’Œé¢„ç®—çº¦æŸç­‰ä¸€ç³»åˆ—å®é™…è€ƒè™‘ï¼Œå¸¸å¸¸ä½¿è¿™äº›åº”ç”¨ä»…é™äºå•ä¸€æ‘„åƒå¤´ã€‚
- en: '[Monocular depth estimation](https://paperswithcode.com/task/monocular-depth-estimation)
    (MDE) is the task of predicting the depth of a scene from a single image. Depth
    computation from a single image is inherently ambiguous, as there are multiple
    ways to project the same 3D scene onto the 2D plane of an image. As a result,
    MDE is a challenging task that requires (either explicitly or implicitly) factoring
    in many cues such as object size, occlusion, and perspective.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[å•ç›®æ·±åº¦ä¼°è®¡](https://paperswithcode.com/task/monocular-depth-estimation)ï¼ˆMDEï¼‰æ˜¯ä»å•å¼ å›¾åƒä¸­é¢„æµ‹åœºæ™¯æ·±åº¦çš„ä»»åŠ¡ã€‚ç”±äºä»å•å¼ å›¾åƒè®¡ç®—æ·±åº¦æœ¬è´¨ä¸Šæ˜¯æ¨¡ç³Šçš„ï¼Œå› ä¸ºåŒä¸€ä¸ª3Dåœºæ™¯å¯ä»¥æœ‰å¤šç§æ–¹å¼æŠ•å½±åˆ°å›¾åƒçš„2Då¹³é¢ä¸Šï¼Œå› æ­¤MDEæ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œéœ€è¦ï¼ˆæ˜¾å¼æˆ–éšå¼ï¼‰è€ƒè™‘è®¸å¤šçº¿ç´¢ï¼Œå¦‚ç‰©ä½“å¤§å°ã€é®æŒ¡å’Œé€è§†ã€‚'
- en: In this post, we will illustrate how to load and visualize depth map data, run
    monocular depth estimation models, and evaluate depth predictions. We will do
    so using data from the [SUN RGB-D](https://rgbd.cs.princeton.edu/) dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•åŠ è½½å’Œå¯è§†åŒ–æ·±åº¦å›¾æ•°æ®ï¼Œè¿è¡Œå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œå¹¶è¯„ä¼°æ·±åº¦é¢„æµ‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¥è‡ª [SUN RGB-D](https://rgbd.cs.princeton.edu/)
    æ•°æ®é›†çš„æ•°æ®è¿›è¡Œæ¼”ç¤ºã€‚
- en: 'In particular, we will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š
- en: '[Loading and visualizing SUN-RGBD ground truth depth maps](#15fc)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[åŠ è½½å’Œå¯è§†åŒ–SUN-RGBDçœŸå®æ·±åº¦å›¾](#15fc)'
- en: '[Running inference with Marigold and DPT](#fd8c)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨Marigoldå’ŒDPTè¿›è¡Œæ¨ç†](#fd8c)'
- en: '[Evaluating relative depth predictions](#1924)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[è¯„ä¼°ç›¸å¯¹æ·±åº¦é¢„æµ‹](#1924)'
- en: 'We will use the Hugging Face [transformers](https://huggingface.co/docs/transformers/index)
    and [diffusers](https://huggingface.co/docs/diffusers/index) libraries for inference,
    [FiftyOne](https://github.com/voxel51/fiftyone) for data management and visualization,
    and [scikit-image](https://scikit-image.org/) for evaluation metrics. All of these
    libraries are open source and free to use. *Disclaimer: I work at Voxel51, the
    lead maintainers of one of these libraries (FiftyOne).*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨Hugging Faceçš„[transformers](https://huggingface.co/docs/transformers/index)å’Œ[diffusers](https://huggingface.co/docs/diffusers/index)åº“è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨[FiftyOne](https://github.com/voxel51/fiftyone)è¿›è¡Œæ•°æ®ç®¡ç†å’Œå¯è§†åŒ–ï¼Œä½¿ç”¨[scikit-image](https://scikit-image.org/)è¿›è¡Œè¯„ä¼°æŒ‡æ ‡ã€‚æ‰€æœ‰è¿™äº›åº“éƒ½æ˜¯å¼€æºçš„ï¼Œå¯ä»¥å…è´¹ä½¿ç”¨ã€‚*å…è´£å£°æ˜ï¼šæˆ‘åœ¨Voxel51å·¥ä½œï¼Œè¯¥å…¬å¸æ˜¯è¿™äº›åº“ä¹‹ä¸€ï¼ˆFiftyOneï¼‰çš„ä¸»è¦ç»´æŠ¤è€…ã€‚*
- en: 'Before we get started, make sure you have all of the necessary libraries installed:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“ï¼š
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then weâ€™ll import the modules weâ€™ll be using throughout the post:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†å¯¼å…¥åœ¨æ•´ä¸ªæ–‡ç« ä¸­ä½¿ç”¨çš„æ¨¡å—ï¼š
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading and Visualizing SUN-RGBD Depth Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ è½½å’Œå¯è§†åŒ–SUN-RGBDæ·±åº¦æ•°æ®
- en: The [SUN RGB-D dataset](https://rgbd.cs.princeton.edu/) contains 10,335 RGB-D
    images, each of which has a corresponding RGB image, depth image, and camera intrinsics.
    It contains images from the [NYU depth v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html),
    Berkeley [B3DO](http://kinectdata.com/), and [SUN3D](https://sun3d.cs.princeton.edu/)
    datasets. SUN RGB-D is [one of the most popular](https://paperswithcode.com/dataset/sun-rgb-d)
    datasets for monocular depth estimation and semantic segmentation tasks!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[SUN RGB-Dæ•°æ®é›†](https://rgbd.cs.princeton.edu/)åŒ…å«10,335å¼ RGB-Då›¾åƒï¼Œæ¯å¼ å›¾åƒéƒ½æœ‰å¯¹åº”çš„RGBå›¾åƒã€æ·±åº¦å›¾åƒå’Œç›¸æœºå†…å‚ã€‚å®ƒåŒ…å«æ¥è‡ª[NYU
    Depth v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)ã€ä¼¯å…‹åˆ©[B3DO](http://kinectdata.com/)å’Œ[SUN3D](https://sun3d.cs.princeton.edu/)æ•°æ®é›†çš„å›¾åƒã€‚SUN
    RGB-Dæ˜¯[æœ€å—æ¬¢è¿çš„](https://paperswithcode.com/dataset/sun-rgb-d)å•ç›®æ·±åº¦ä¼°è®¡å’Œè¯­ä¹‰åˆ†å‰²ä»»åŠ¡æ•°æ®é›†ä¹‹ä¸€ï¼'
- en: ğŸ’¡For this walkthrough, we will only use the NYU depth v2 portions. NYU depth
    v2 is [permissively licensed for commercial use](https://github.com/dwofk/fast-depth/blob/master/LICENSE)
    (MIT), and can be [downloaded from Hugging Face](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)
    directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡å¯¹äºæœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬åªä½¿ç”¨NYU Depth v2éƒ¨åˆ†ã€‚NYU Depth v2æ˜¯[å…è®¸å•†ä¸šä½¿ç”¨çš„è®¸å¯](https://github.com/dwofk/fast-depth/blob/master/LICENSE)ï¼ˆMITè®¸å¯è¯ï¼‰ï¼Œå¯ä»¥ç›´æ¥[ä»Hugging
    Faceä¸‹è½½](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)ã€‚
- en: Downloading the Raw Data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸‹è½½åŸå§‹æ•°æ®
- en: 'First, download the SUN RGB-D dataset from [here](https://rgbd.cs.princeton.edu/)
    and unzip it, or use the following command to download it directly:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä»[è¿™é‡Œ](https://rgbd.cs.princeton.edu/)ä¸‹è½½SUN RGB-Dæ•°æ®é›†å¹¶è§£å‹ï¼Œæˆ–è€…ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›´æ¥ä¸‹è½½ï¼š
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then unzip it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè§£å‹å®ƒï¼š
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you want to use the dataset for other tasks, you can fully convert the annotations
    and load them into your `fiftyone.Dataset`. However, for this tutorial, we will
    only be using the depth images, so we will only use the RGB images and the depth
    images (stored in the `depth_bfx` sub-directories).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³å°†æ•°æ®é›†ç”¨äºå…¶ä»–ä»»åŠ¡ï¼Œä½ å¯ä»¥å®Œå…¨è½¬æ¢æ³¨é‡Šå¹¶åŠ è½½åˆ°ä½ çš„`fiftyone.Dataset`ä¸­ã€‚ä½†æ˜¯ï¼Œå¯¹äºæœ¬æ•™ç¨‹ï¼Œæˆ‘ä»¬åªä½¿ç”¨æ·±åº¦å›¾åƒï¼Œæ‰€ä»¥æˆ‘ä»¬åªä¼šä½¿ç”¨RGBå›¾åƒå’Œæ·±åº¦å›¾åƒï¼ˆå­˜å‚¨åœ¨`depth_bfx`å­ç›®å½•ä¸­ï¼‰ã€‚
- en: Creating the Dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ•°æ®é›†
- en: 'Because we are just interested in getting the point across, weâ€™ll restrict
    ourselves to the first 20 samples, which are all from the NYU Depth v2 portion
    of the dataset:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬åªå…³æ³¨ä¼ è¾¾æ ¸å¿ƒå†…å®¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é™åˆ¶åœ¨å‰20ä¸ªæ ·æœ¬ï¼Œè¿™äº›æ ·æœ¬éƒ½æ¥è‡ªNYU Depth v2æ•°æ®é›†éƒ¨åˆ†ï¼š
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we are storing the depth maps as [heatmaps](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps).
    Everything is represented in terms of normalized, *relative* distances, where
    255 represents the maximum distance in the scene and 0 represents the minimum
    distance in the scene. This is a common way to represent depth maps, although
    it is far from the only way to do so. If we were interested in *absolute* distances,
    we could store sample-wise parameters for the minimum and maximum distances in
    the scene, and use these to reconstruct the absolute distances from the relative
    distances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬å°†æ·±åº¦å›¾åƒå­˜å‚¨ä¸º[çƒ­å›¾](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps)ã€‚æ‰€æœ‰å†…å®¹éƒ½ä»¥æ ‡å‡†åŒ–çš„*ç›¸å¯¹*è·ç¦»è¡¨ç¤ºï¼Œå…¶ä¸­255ä»£è¡¨åœºæ™¯ä¸­çš„æœ€å¤§è·ç¦»ï¼Œ0ä»£è¡¨åœºæ™¯ä¸­çš„æœ€å°è·ç¦»ã€‚è¿™æ˜¯è¡¨ç¤ºæ·±åº¦å›¾åƒçš„å¸¸è§æ–¹å¼ï¼Œå°½ç®¡è¿™ä¸æ˜¯å”¯ä¸€çš„æ–¹å¼ã€‚å¦‚æœæˆ‘ä»¬å…³æ³¨*ç»å¯¹*è·ç¦»ï¼Œæˆ‘ä»¬å¯ä»¥å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„æœ€å°å’Œæœ€å¤§è·ç¦»å‚æ•°ï¼Œå¹¶ä½¿ç”¨è¿™äº›å‚æ•°ä»ç›¸å¯¹è·ç¦»ä¸­é‡å»ºç»å¯¹è·ç¦»ã€‚
- en: Visualizing Ground Truth Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯è§†åŒ–çœŸå®æ•°æ®
- en: 'With heatmaps stored on our samples, we can visualize the ground truth data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†å­˜å‚¨åœ¨æ ·æœ¬ä¸­çš„çƒ­å›¾ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–åœ°é¢çœŸå®æ•°æ®ï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/e13d2736bd24bfe61ef3629acc934b4a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e13d2736bd24bfe61ef3629acc934b4a.png)'
- en: Ground truth depth maps for samples from the SUN RGB-D dataset. Image courtesy
    of the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªSUN RGB-Dæ•°æ®é›†çš„æ ·æœ¬çš„åœ°é¢çœŸå®æ·±åº¦å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: When working with depth maps, the color scheme and opacity of the heatmap are
    important. Iâ€™m colorblind, so I find that the [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)
    colormap with opacity turned all the way up works best for me.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†æ·±åº¦å›¾æ—¶ï¼Œçƒ­å›¾çš„è‰²å½©æ–¹æ¡ˆå’Œé€æ˜åº¦éå¸¸é‡è¦ã€‚æˆ‘æ˜¯è‰²ç›²ï¼Œå› æ­¤æˆ‘å‘ç°[viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)è‰²å›¾å¹¶å°†é€æ˜åº¦è°ƒåˆ°æœ€å¤§æœ€é€‚åˆæˆ‘ã€‚
- en: '![](../Images/bcf625a36ee7d97a1afe4d32d72d2046.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcf625a36ee7d97a1afe4d32d72d2046.png)'
- en: Visibility settings for heatmaps. Image courtesy of the author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: çƒ­å›¾çš„å¯è§†åŒ–è®¾ç½®ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Ground Truth?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ°é¢çœŸå®å€¼ï¼Ÿ
- en: 'Inspecting these RGB images and depth maps, we can see that there are some
    inaccuracies in the ground truth depth maps. For example, in this image, the dark
    rift through the center of the image is actually the *farthest* part of the scene,
    but the ground truth depth map shows it as the *closest* part of the scene:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ£€æŸ¥è¿™äº›RGBå›¾åƒå’Œæ·±åº¦å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ°é¢çœŸå®æ·±åº¦å›¾ä¸­å­˜åœ¨ä¸€äº›ä¸å‡†ç¡®ä¹‹å¤„ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™å¼ å›¾åƒä¸­ï¼Œå›¾åƒä¸­å¿ƒçš„æ·±è‰²è£‚ç¼å®é™…ä¸Šæ˜¯åœºæ™¯ä¸­æœ€*è¿œ*çš„éƒ¨åˆ†ï¼Œä½†åœ°é¢çœŸå®æ·±åº¦å›¾å´æ˜¾ç¤ºå®ƒæ˜¯åœºæ™¯ä¸­*æœ€è¿‘*çš„éƒ¨åˆ†ï¼š
- en: '![](../Images/5089c29d7fffe32d2eb44bb9fba5f018.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5089c29d7fffe32d2eb44bb9fba5f018.png)'
- en: Issue in ground truth depth data for sample from the SUN RGB-D dataset. Image
    courtesy of the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªSUN RGB-Dæ•°æ®é›†çš„æ ·æœ¬çš„åœ°é¢çœŸå®æ·±åº¦æ•°æ®é—®é¢˜ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'This is one of the key challenges for MDE tasks: ground truth data is hard
    to come by, and is often noisy! Itâ€™s essential to be aware of this while evaluating
    your MDE models.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯MDEä»»åŠ¡ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ï¼šåœ°é¢çœŸå®æ•°æ®å¾ˆéš¾è·å¾—ï¼Œè€Œä¸”é€šå¸¸å­˜åœ¨å™ªå£°ï¼åœ¨è¯„ä¼°ä½ çš„MDEæ¨¡å‹æ—¶ï¼Œäº†è§£è¿™ä¸€ç‚¹è‡³å…³é‡è¦ã€‚
- en: Running Monocular Depth Estimation Models
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿è¡Œå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹
- en: Now that we have our dataset loaded in, we can run monocular depth estimation
    models on our RGB images!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åŠ è½½äº†æ•°æ®é›†ï¼Œå¯ä»¥å¯¹æˆ‘ä»¬çš„RGBå›¾åƒè¿è¡Œå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼
- en: For a long time, the state-of-the-art models for monocular depth estimation
    such as [DORN](https://github.com/hufu6371/DORN) and [DenseDepth](https://github.com/ialhashim/DenseDepth)
    were built with convolutional neural networks. Recently, however, both transformer-based
    models such as [DPT](https://huggingface.co/docs/transformers/model_doc/dpt) and
    [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn), and diffusion-based
    models like [Marigold](https://huggingface.co/Bingxin/Marigold) have achieved
    remarkable results!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: é•¿æ—¶é—´ä»¥æ¥ï¼Œåƒ[DORN](https://github.com/hufu6371/DORN)å’Œ[DenseDepth](https://github.com/ialhashim/DenseDepth)è¿™æ ·çš„å•ç›®æ·±åº¦ä¼°è®¡çš„æœ€å…ˆè¿›æ¨¡å‹éƒ½æ˜¯åŸºäºå·ç§¯ç¥ç»ç½‘ç»œæ„å»ºçš„ã€‚ç„¶è€Œï¼Œæœ€è¿‘ï¼ŒåŸºäºå˜æ¢å™¨çš„æ¨¡å‹ï¼Œå¦‚[DPT](https://huggingface.co/docs/transformers/model_doc/dpt)å’Œ[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)ï¼Œä»¥åŠåŸºäºæ‰©æ•£çš„æ¨¡å‹ï¼Œå¦‚[Marigold](https://huggingface.co/Bingxin/Marigold)ï¼Œéƒ½å–å¾—äº†æ˜¾è‘—çš„æˆæœï¼
- en: In this section, weâ€™ll show you how to generate MDE depth map predictions with
    both DPT and Marigold. In both cases, you can optionally run the model locally
    with the respective Hugging Face library, or run remotely with [Replicate](https://replicate.com/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨DPTå’ŒMarigoldç”ŸæˆMDEæ·±åº¦å›¾é¢„æµ‹ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨å„è‡ªçš„Hugging Faceåº“åœ¨æœ¬åœ°è¿è¡Œæ¨¡å‹ï¼Œæˆ–è€…é€šè¿‡[Replicate](https://replicate.com/)è¿›è¡Œè¿œç¨‹è¿è¡Œã€‚
- en: 'To run via Replicate, install the Python client:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¦é€šè¿‡Replicateè¿è¡Œï¼Œè¯·å®‰è£…Pythonå®¢æˆ·ç«¯ï¼š
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And export your Replicate API token:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶å¯¼å‡ºä½ çš„Replicate APIä»¤ç‰Œï¼š
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ğŸ’¡ With Replicate, it might take a minute for the model to load into memory on
    the server (cold-start problem), but once it does the prediction should only take
    a few seconds. Depending on your local compute resources, running on server may
    give you massive speedups compared to running locally, especially for Marigold
    and other diffusion-based depth-estimation approaches.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ ä½¿ç”¨Replicateæ—¶ï¼Œæ¨¡å‹åŠ è½½åˆ°æœåŠ¡å™¨å†…å­˜å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆå†·å¯åŠ¨é—®é¢˜ï¼‰ï¼Œä½†ä¸€æ—¦åŠ è½½å®Œæˆï¼Œé¢„æµ‹åº”è¯¥åªéœ€å‡ ç§’é’Ÿã€‚æ ¹æ®ä½ çš„æœ¬åœ°è®¡ç®—èµ„æºï¼Œä¸æœ¬åœ°è¿è¡Œç›¸æ¯”ï¼Œä½¿ç”¨æœåŠ¡å™¨è¿è¡Œå¯èƒ½ä¼šå¤§å¤§æé«˜é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºMarigoldå’Œå…¶ä»–åŸºäºæ‰©æ•£çš„æ·±åº¦ä¼°è®¡æ–¹æ³•ã€‚
- en: Monocular Depth Estimation with DPT
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨DPTè¿›è¡Œå•ç›®æ·±åº¦ä¼°è®¡
- en: The first model we will run is a dense-prediction transformer (DPT). DPT models
    have found utility in both MDE and semantic segmentation â€” tasks that require
    â€œdenseâ€, pixel-level predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆè¿è¡Œä¸€ä¸ªå¯†é›†é¢„æµ‹å˜æ¢å™¨ï¼ˆDPTï¼‰ã€‚DPTæ¨¡å‹åœ¨å•ç›®æ·±åº¦ä¼°è®¡ï¼ˆMDEï¼‰å’Œè¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸­éå¸¸æœ‰ç”¨ï¼Œè¿™äº›ä»»åŠ¡éœ€è¦â€œå¯†é›†â€çš„åƒç´ çº§é¢„æµ‹ã€‚
- en: The checkpoint below uses [MiDaS](https://github.com/isl-org/MiDaS/tree/master),
    which returns the [inverse depth map](https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/),
    so we have to invert it back to get a comparable depth map.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹çš„æ£€æŸ¥ç‚¹ä½¿ç”¨äº†[MiDaS](https://github.com/isl-org/MiDaS/tree/master)ï¼Œå®ƒè¿”å›çš„æ˜¯[åå‘æ·±åº¦å›¾](https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/)ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°†å…¶åè½¬å›æ¥ï¼Œä»¥è·å¾—å¯æ¯”è¾ƒçš„æ·±åº¦å›¾ã€‚
- en: 'To run locally with `transformers`, first we load the model and image processor:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æœ¬åœ°ä½¿ç”¨`transformers`è¿è¡Œï¼Œé¦–å…ˆåŠ è½½æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨ï¼š
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we encapsulate the code for inference on a sample, including pre and
    post processing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æ¨ç†ä»£ç å°è£…åœ¨ä¸€ä¸ªæ ·æœ¬ä¸­ï¼ŒåŒ…æ‹¬é¢„å¤„ç†å’Œåå¤„ç†ï¼š
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we are storing predictions in a `label_field` field on our samples, represented
    with a heatmap just like the ground truth labels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†é¢„æµ‹ç»“æœå­˜å‚¨åœ¨æ ·æœ¬çš„`label_field`å­—æ®µä¸­ï¼Œç”¨çƒ­å›¾è¡¨ç¤ºï¼Œå’ŒçœŸå®æ ‡ç­¾ä¸€æ ·ã€‚
- en: Note that in the `apply_dpt_model()` function, between the model's forward pass
    and the heatmap generation, notice that we make a call to `torch.nn.functional.interpolate()`.
    This is because the model's forward pass is run on a downsampled version of the
    image, and we want to return a heatmap that is the same size as the original image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œåœ¨`apply_dpt_model()`å‡½æ•°ä¸­ï¼Œåœ¨æ¨¡å‹çš„å‰å‘ä¼ é€’å’Œçƒ­å›¾ç”Ÿæˆä¹‹é—´ï¼Œæˆ‘ä»¬è°ƒç”¨äº†`torch.nn.functional.interpolate()`ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹çš„å‰å‘ä¼ é€’æ˜¯åœ¨å›¾åƒçš„ä¸‹é‡‡æ ·ç‰ˆæœ¬ä¸Šè¿è¡Œçš„ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›è¿”å›ä¸€ä¸ªä¸åŸå§‹å›¾åƒå¤§å°ç›¸åŒçš„çƒ­å›¾ã€‚
- en: Why do we need to do this? If we just want to *look* at the heatmaps, this would
    not matter. But if we want to compare the ground truth depth maps to the modelâ€™s
    predictions on a per-pixel basis, we need to make sure that they are the same
    size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦è¿™æ ·åšï¼Ÿå¦‚æœæˆ‘ä»¬åªæ˜¯æƒ³*æŸ¥çœ‹*çƒ­å›¾ï¼Œè¿™ä¸€ç‚¹å¹¶ä¸é‡è¦ã€‚ä½†å¦‚æœæˆ‘ä»¬æƒ³é€åƒç´ åœ°æ¯”è¾ƒçœŸå®æ·±åº¦å›¾å’Œæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œå°±éœ€è¦ç¡®ä¿å®ƒä»¬çš„å¤§å°ä¸€è‡´ã€‚
- en: 'All that is left to do is iterate through the dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„å°±æ˜¯éå†æ•°æ®é›†ï¼š
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/92fca2436322f33052cdabbccf4c1a59.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92fca2436322f33052cdabbccf4c1a59.png)'
- en: Relative depth maps predicted by a hybrid MiDaS DPT model on SUN RGB-D sample
    images. Image courtesy of the author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ··åˆMiDaS DPTæ¨¡å‹åœ¨SUN RGB-Dæ ·æœ¬å›¾åƒä¸Šé¢„æµ‹çš„ç›¸å¯¹æ·±åº¦å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'To run with Replicate, you can use [this](https://replicate.com/cjwbw/midas)
    model. Here is what the API looks like:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨Replicateè¿è¡Œï¼Œä½ å¯ä»¥ä½¿ç”¨[è¿™ä¸ª](https://replicate.com/cjwbw/midas)æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯APIçš„æ ·å¼ï¼š
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Monocular Depth Estimation with Marigold
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Marigoldè¿›è¡Œå•ç›®æ·±åº¦ä¼°è®¡
- en: Stemming from their tremendous success in text-to-image contexts, diffusion
    models are being applied to an ever-broadening range of problems. [Marigold](https://huggingface.co/Bingxin/Marigold)
    â€œrepurposesâ€ diffusion-based image generation models for monocular depth estimation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æºäºåœ¨æ–‡æœ¬åˆ°å›¾åƒé¢†åŸŸçš„å·¨å¤§æˆåŠŸï¼Œæ‰©æ•£æ¨¡å‹æ­£åœ¨è¢«åº”ç”¨äºè¶Šæ¥è¶Šå¹¿æ³›çš„é—®é¢˜ã€‚[Marigold](https://huggingface.co/Bingxin/Marigold)
    â€œé‡æ–°åˆ©ç”¨â€åŸºäºæ‰©æ•£çš„å›¾åƒç”Ÿæˆæ¨¡å‹è¿›è¡Œå•ç›®æ·±åº¦ä¼°è®¡ã€‚
- en: 'To run Marigold locally, you will need to clone the git repository:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¦åœ¨æœ¬åœ°è¿è¡ŒMarigoldï¼Œä½ éœ€è¦å…‹éš†è¿™ä¸ªgitä»“åº“ï¼š
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This repository introduces a new diffusers pipeline, `MarigoldPipeline`, which
    makes applying Marigold easy:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä»“åº“ä»‹ç»äº†ä¸€ä¸ªæ–°çš„æ‰©æ•£å™¨ç®¡é“`MarigoldPipeline`ï¼Œä½¿å¾—åº”ç”¨Marigoldå˜å¾—æ›´åŠ ç®€å•ï¼š
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Post-processing of the output depth image is then needed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥éœ€è¦å¯¹è¾“å‡ºçš„æ·±åº¦å›¾åƒè¿›è¡Œåå¤„ç†ã€‚
- en: 'To instead run via Replicate, we can create an `apply_marigold_model()` function
    in analogy with the DPT case above and iterate over the samples in our dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ”¹ä¸ºé€šè¿‡Replicateè¿è¡Œï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ª`apply_marigold_model()`å‡½æ•°ï¼Œç±»ä¼¼äºä¸Šé¢çš„DPTæ¡ˆä¾‹ï¼Œå¹¶éå†æ•°æ®é›†ä¸­çš„æ ·æœ¬ï¼š
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
- en: Relative depth maps predicted with Marigold endpoint on SUN RGB-D sample images.
    Image courtesy of the author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Marigoldç«¯ç‚¹åœ¨SUN RGB-Dæ ·æœ¬å›¾åƒä¸Šé¢„æµ‹çš„ç›¸å¯¹æ·±åº¦å›¾ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Evaluating Monocular Depth Estimation Models
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°å•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹
- en: 'Now that we have predictions from multiple models, letâ€™s evaluate them! We
    will leverage `scikit-image` to apply three simple metrics commonly used for monocular
    depth estimation: [root mean squared error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)
    (RMSE), [peak signal to noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)
    (PSNR), and [structural similarity index](https://en.wikipedia.org/wiki/Structural_similarity)
    (SSIM).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œè®©æˆ‘ä»¬æ¥è¯„ä¼°å®ƒä»¬ï¼æˆ‘ä»¬å°†åˆ©ç”¨`scikit-image`æ¥åº”ç”¨ä¸‰ä¸ªå¸¸ç”¨äºå•ç›®æ·±åº¦ä¼°è®¡çš„ç®€å•æŒ‡æ ‡ï¼š[å‡æ–¹æ ¹è¯¯å·®](https://en.wikipedia.org/wiki/Root-mean-square_deviation)ï¼ˆRMSEï¼‰ã€[å³°å€¼ä¿¡å™ªæ¯”](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)ï¼ˆPSNRï¼‰å’Œ[ç»“æ„ç›¸ä¼¼æ€§æŒ‡æ•°](https://en.wikipedia.org/wiki/Structural_similarity)ï¼ˆSSIMï¼‰ã€‚
- en: ğŸ’¡Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores
    indicate better predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡è¾ƒé«˜çš„PSNRå’ŒSSIMåˆ†æ•°è¡¨ç¤ºæ›´å¥½çš„é¢„æµ‹ï¼Œè€Œè¾ƒä½çš„RMSEåˆ†æ•°è¡¨ç¤ºæ›´å¥½çš„é¢„æµ‹ã€‚
- en: Note that the specific values I arrive at are a consequence of the specific
    pre-and-post processing steps I performed along the way. What matters is the relative
    performance!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæˆ‘å¾—åˆ°çš„å…·ä½“æ•°å€¼æ˜¯æˆ‘åœ¨æ­¤è¿‡ç¨‹ä¸­æ‰§è¡Œçš„ç‰¹å®šå‰åå¤„ç†æ­¥éª¤çš„ç»“æœã€‚é‡è¦çš„æ˜¯ç›¸å¯¹æ€§èƒ½ï¼
- en: 'We will define the evaluation routine:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å®šä¹‰è¯„ä¼°æµç¨‹ï¼š
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And then apply the evaluation to the predictions from both models:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°†è¯„ä¼°åº”ç”¨äºä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ï¼š
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Computing average performance for a certain model/metric is as simple as calling
    the datasetâ€™s `mean()`method on that field:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æŸä¸ªæ¨¡å‹/åº¦é‡æ ‡å‡†çš„å¹³å‡æ€§èƒ½ï¼Œåªéœ€å¯¹è¯¥å­—æ®µè°ƒç”¨æ•°æ®é›†çš„`mean()`æ–¹æ³•ï¼š
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: All of the metrics seem to agree that DPT outperforms Marigold. However, it
    is important to note that these metrics are not perfect. For example, RMSE is
    very sensitive to outliers, and SSIM is not very sensitive to small errors. For
    a more thorough evaluation, we can filter by these metrics in the app in order
    to visualize what the model is doing well and what it is doing poorly â€” or where
    the metrics are failing to capture the modelâ€™s performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„åº¦é‡æ ‡å‡†ä¼¼ä¹éƒ½ä¸€è‡´è®¤ä¸º DPT ä¼˜äº Marigoldã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æ³¨æ„ï¼Œè¿™äº›åº¦é‡æ ‡å‡†å¹¶ä¸å®Œç¾ã€‚ä¾‹å¦‚ï¼ŒRMSE å¯¹å¼‚å¸¸å€¼éå¸¸æ•æ„Ÿï¼Œè€Œ SSIM
    å¯¹å°è¯¯å·®ä¸å¤ªæ•æ„Ÿã€‚ä¸ºäº†æ›´å…¨é¢çš„è¯„ä¼°ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åº”ç”¨ç¨‹åºä¸­é€šè¿‡è¿™äº›åº¦é‡æ ‡å‡†è¿›è¡Œç­›é€‰ï¼Œä»¥å¯è§†åŒ–æ¨¡å‹åšå¾—å¥½çš„åœ°æ–¹å’Œåšå¾—å·®çš„åœ°æ–¹â€”â€”æˆ–è€…æ˜¯åº¦é‡æ ‡å‡†æœªèƒ½æ•æ‰åˆ°æ¨¡å‹è¡¨ç°çš„åœ°æ–¹ã€‚
- en: 'Finally, toggling masks on and off is a great way to visualize the differences
    between the ground truth and the modelâ€™s predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ‡æ¢é®ç½©çš„å¼€å…³æ˜¯å¯è§†åŒ–çœŸå®å€¼ä¸æ¨¡å‹é¢„æµ‹å·®å¼‚çš„å¥½æ–¹æ³•ï¼š
- en: '![](../Images/9cc8c71a8a38c238f74bdcfea80a543b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cc8c71a8a38c238f74bdcfea80a543b.png)'
- en: Visual comparison of heatmaps predicted by the two MDE models and the ground
    truth. Image courtesy of the author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä¸¤ç§ MDE æ¨¡å‹é¢„æµ‹çš„çƒ­åŠ›å›¾ä¸çœŸå®å€¼çš„è§†è§‰å¯¹æ¯”ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: To recap, we learned how to run monocular depth estimation models on our data,
    how to evaluate the predictions using common metrics, and how to visualize the
    results. We also learned that monocular depth estimation is a notoriously difficult
    task.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸Šè¿è¡Œå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œå¦‚ä½•ä½¿ç”¨å¸¸è§çš„åº¦é‡æ ‡å‡†è¯„ä¼°é¢„æµ‹ç»“æœï¼Œä»¥åŠå¦‚ä½•å¯è§†åŒ–ç»“æœã€‚æˆ‘ä»¬è¿˜äº†è§£åˆ°ï¼Œå•ç›®æ·±åº¦ä¼°è®¡æ˜¯ä¸€ä¸ªå…¬è®¤çš„å›°éš¾ä»»åŠ¡ã€‚
- en: Data quality and quantity are severely limiting factors; models often struggle
    to generalize to new environments; and metrics are not always good indicators
    of model performance. The specific numeric values quantifying model performance
    can depend greatly on your processing pipeline. And even your qualitative assessment
    of predicted depth maps can be heavily influenced by your color schemes and opacity
    scales.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡å’Œæ•°é‡æ˜¯ä¸¥é‡çš„é™åˆ¶å› ç´ ï¼›æ¨¡å‹å¾€å¾€éš¾ä»¥æ¨å¹¿åˆ°æ–°çš„ç¯å¢ƒï¼›è€Œä¸”åº¦é‡æ ‡å‡†å¹¶ä¸æ€»æ˜¯å¥½çš„æ¨¡å‹æ€§èƒ½æŒ‡ç¤ºå™¨ã€‚é‡åŒ–æ¨¡å‹æ€§èƒ½çš„å…·ä½“æ•°å€¼å¯èƒ½ä¼šå› ä½ çš„å¤„ç†æµç¨‹è€Œæœ‰æ‰€ä¸åŒã€‚å³ä½¿æ˜¯ä½ å¯¹é¢„æµ‹æ·±åº¦å›¾çš„å®šæ€§è¯„ä¼°ï¼Œä¹Ÿå¯èƒ½ä¼šå—åˆ°ä½ çš„è‰²å½©æ–¹æ¡ˆå’Œä¸é€æ˜åº¦æ¯”ä¾‹çš„å¼ºçƒˆå½±å“ã€‚
- en: 'If thereâ€™s one thing you take away from this post, I hope it is this: it is
    mission-critical that you look at the depth maps themselves, and not just the
    metrics!'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»è¿™ç¯‡æ–‡ç« ä¸­å­¦åˆ°ä¸€ä»¶äº‹ï¼Œæˆ‘å¸Œæœ›æ˜¯è¿™ä¸€ç‚¹ï¼šæŸ¥çœ‹æ·±åº¦å›¾æœ¬èº«ï¼Œè€Œä¸ä»…ä»…æ˜¯åº¦é‡æ ‡å‡†ï¼Œè¿™ä¸€ç‚¹è‡³å…³é‡è¦ï¼
- en: 'Note: All images are courtesy of the author. The sub-dataset used for all workflows
    and visuals presented in this post is NYU depth v2, which is [permissively licensed
    for commercial use](https://github.com/dwofk/fast-depth/blob/master/LICENSE) (MIT).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ‰€æœ‰å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚æœ¬ç¯‡æ–‡ç« ä¸­å±•ç¤ºçš„æ‰€æœ‰å·¥ä½œæµå’Œè§†è§‰æ•ˆæœä½¿ç”¨çš„å­æ•°æ®é›†æ˜¯ NYU depth v2ï¼Œè¯¥æ•°æ®é›†æ˜¯[å…è®¸å•†ä¸šä½¿ç”¨çš„å¼€æºè®¸å¯è¯](https://github.com/dwofk/fast-depth/blob/master/LICENSE)ï¼ˆMITï¼‰ã€‚
