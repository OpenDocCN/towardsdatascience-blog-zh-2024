- en: How to Estimate Depth from a Single Image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何从单张图像中估计深度
- en: 原文：[https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25](https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25](https://towardsdatascience.com/how-to-estimate-depth-from-a-single-image-7f421d86b22d?source=collection_archive---------0-----------------------#2024-01-25)
- en: Run and evaluate monocular depth estimation models with Hugging Face and FiftyOne
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Hugging Face和FiftyOne运行并评估单目深度估计模型
- en: '[](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[![Jacob
    Marks, Ph.D.](../Images/94d9832b8706d1044e3195386613bfab.png)](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    [Jacob Marks, Ph.D.](https://medium.com/@jacob_marks?source=post_page---byline--7f421d86b22d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    ·10 min read·Jan 25, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7f421d86b22d--------------------------------)
    ·10分钟阅读·2024年1月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
- en: Monocular Depth heat maps generated with Marigold on NYU depth v2 images. Image
    courtesy of the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Marigold在NYU深度v2图像上生成的单目深度热图。图片由作者提供。
- en: Humans view the world through two eyes. One of the primary benefits of this
    *binocular* vision is the ability to perceive *depth* — how near or far objects
    are. The human brain infers object depths by comparing the pictures captured by
    left and right eyes at the same time and interpreting the disparities. This process
    is known as [stereopsis](https://en.wikipedia.org/wiki/Stereopsis).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过两只眼睛看世界。双眼视力的一个主要优点是能够感知*深度*——物体离近还是远。人脑通过比较左眼和右眼同时捕捉到的图像，并解释这些差异，从而推断物体的深度。这个过程被称为[立体视觉](https://en.wikipedia.org/wiki/Stereopsis)。
- en: Just as depth perception plays a crucial role in human vision and navigation,
    the ability to estimate depth is critical for a wide range of computer vision
    applications, from autonomous driving to robotics, and even augmented reality.
    Yet a slew of practical considerations from spatial limitations to budgetary constraints
    often limit these applications to a single camera.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 就像深度感知在人类视觉和导航中发挥着关键作用一样，估计深度的能力对于广泛的计算机视觉应用也至关重要，从自动驾驶到机器人，再到增强现实。然而，由于空间限制和预算约束等一系列实际考虑，常常使这些应用仅限于单一摄像头。
- en: '[Monocular depth estimation](https://paperswithcode.com/task/monocular-depth-estimation)
    (MDE) is the task of predicting the depth of a scene from a single image. Depth
    computation from a single image is inherently ambiguous, as there are multiple
    ways to project the same 3D scene onto the 2D plane of an image. As a result,
    MDE is a challenging task that requires (either explicitly or implicitly) factoring
    in many cues such as object size, occlusion, and perspective.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[单目深度估计](https://paperswithcode.com/task/monocular-depth-estimation)（MDE）是从单张图像中预测场景深度的任务。由于从单张图像计算深度本质上是模糊的，因为同一个3D场景可以有多种方式投影到图像的2D平面上，因此MDE是一项具有挑战性的任务，需要（显式或隐式）考虑许多线索，如物体大小、遮挡和透视。'
- en: In this post, we will illustrate how to load and visualize depth map data, run
    monocular depth estimation models, and evaluate depth predictions. We will do
    so using data from the [SUN RGB-D](https://rgbd.cs.princeton.edu/) dataset.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将展示如何加载和可视化深度图数据，运行单目深度估计模型，并评估深度预测。我们将使用来自 [SUN RGB-D](https://rgbd.cs.princeton.edu/)
    数据集的数据进行演示。
- en: 'In particular, we will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将涵盖以下内容：
- en: '[Loading and visualizing SUN-RGBD ground truth depth maps](#15fc)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[加载和可视化SUN-RGBD真实深度图](#15fc)'
- en: '[Running inference with Marigold and DPT](#fd8c)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Marigold和DPT进行推理](#fd8c)'
- en: '[Evaluating relative depth predictions](#1924)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[评估相对深度预测](#1924)'
- en: 'We will use the Hugging Face [transformers](https://huggingface.co/docs/transformers/index)
    and [diffusers](https://huggingface.co/docs/diffusers/index) libraries for inference,
    [FiftyOne](https://github.com/voxel51/fiftyone) for data management and visualization,
    and [scikit-image](https://scikit-image.org/) for evaluation metrics. All of these
    libraries are open source and free to use. *Disclaimer: I work at Voxel51, the
    lead maintainers of one of these libraries (FiftyOne).*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Hugging Face的[transformers](https://huggingface.co/docs/transformers/index)和[diffusers](https://huggingface.co/docs/diffusers/index)库进行推理，使用[FiftyOne](https://github.com/voxel51/fiftyone)进行数据管理和可视化，使用[scikit-image](https://scikit-image.org/)进行评估指标。所有这些库都是开源的，可以免费使用。*免责声明：我在Voxel51工作，该公司是这些库之一（FiftyOne）的主要维护者。*
- en: 'Before we get started, make sure you have all of the necessary libraries installed:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，确保你已经安装了所有必要的库：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we’ll import the modules we’ll be using throughout the post:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将导入在整个文章中使用的模块：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Loading and Visualizing SUN-RGBD Depth Data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载和可视化SUN-RGBD深度数据
- en: The [SUN RGB-D dataset](https://rgbd.cs.princeton.edu/) contains 10,335 RGB-D
    images, each of which has a corresponding RGB image, depth image, and camera intrinsics.
    It contains images from the [NYU depth v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html),
    Berkeley [B3DO](http://kinectdata.com/), and [SUN3D](https://sun3d.cs.princeton.edu/)
    datasets. SUN RGB-D is [one of the most popular](https://paperswithcode.com/dataset/sun-rgb-d)
    datasets for monocular depth estimation and semantic segmentation tasks!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[SUN RGB-D数据集](https://rgbd.cs.princeton.edu/)包含10,335张RGB-D图像，每张图像都有对应的RGB图像、深度图像和相机内参。它包含来自[NYU
    Depth v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html)、伯克利[B3DO](http://kinectdata.com/)和[SUN3D](https://sun3d.cs.princeton.edu/)数据集的图像。SUN
    RGB-D是[最受欢迎的](https://paperswithcode.com/dataset/sun-rgb-d)单目深度估计和语义分割任务数据集之一！'
- en: 💡For this walkthrough, we will only use the NYU depth v2 portions. NYU depth
    v2 is [permissively licensed for commercial use](https://github.com/dwofk/fast-depth/blob/master/LICENSE)
    (MIT), and can be [downloaded from Hugging Face](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)
    directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 💡对于本教程，我们只使用NYU Depth v2部分。NYU Depth v2是[允许商业使用的许可](https://github.com/dwofk/fast-depth/blob/master/LICENSE)（MIT许可证），可以直接[从Hugging
    Face下载](https://huggingface.co/datasets/sayakpaul/nyu_depth_v2)。
- en: Downloading the Raw Data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载原始数据
- en: 'First, download the SUN RGB-D dataset from [here](https://rgbd.cs.princeton.edu/)
    and unzip it, or use the following command to download it directly:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从[这里](https://rgbd.cs.princeton.edu/)下载SUN RGB-D数据集并解压，或者使用以下命令直接下载：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And then unzip it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后解压它：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you want to use the dataset for other tasks, you can fully convert the annotations
    and load them into your `fiftyone.Dataset`. However, for this tutorial, we will
    only be using the depth images, so we will only use the RGB images and the depth
    images (stored in the `depth_bfx` sub-directories).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将数据集用于其他任务，你可以完全转换注释并加载到你的`fiftyone.Dataset`中。但是，对于本教程，我们只使用深度图像，所以我们只会使用RGB图像和深度图像（存储在`depth_bfx`子目录中）。
- en: Creating the Dataset
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: 'Because we are just interested in getting the point across, we’ll restrict
    ourselves to the first 20 samples, which are all from the NYU Depth v2 portion
    of the dataset:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只关注传达核心内容，所以我们将限制在前20个样本，这些样本都来自NYU Depth v2数据集部分：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here we are storing the depth maps as [heatmaps](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps).
    Everything is represented in terms of normalized, *relative* distances, where
    255 represents the maximum distance in the scene and 0 represents the minimum
    distance in the scene. This is a common way to represent depth maps, although
    it is far from the only way to do so. If we were interested in *absolute* distances,
    we could store sample-wise parameters for the minimum and maximum distances in
    the scene, and use these to reconstruct the absolute distances from the relative
    distances.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们将深度图像存储为[热图](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps)。所有内容都以标准化的*相对*距离表示，其中255代表场景中的最大距离，0代表场景中的最小距离。这是表示深度图像的常见方式，尽管这不是唯一的方式。如果我们关注*绝对*距离，我们可以存储每个样本的最小和最大距离参数，并使用这些参数从相对距离中重建绝对距离。
- en: Visualizing Ground Truth Data
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化真实数据
- en: 'With heatmaps stored on our samples, we can visualize the ground truth data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 有了存储在样本中的热图，我们可以可视化地面真实数据：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/e13d2736bd24bfe61ef3629acc934b4a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e13d2736bd24bfe61ef3629acc934b4a.png)'
- en: Ground truth depth maps for samples from the SUN RGB-D dataset. Image courtesy
    of the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 来自SUN RGB-D数据集的样本的地面真实深度图。图片由作者提供。
- en: When working with depth maps, the color scheme and opacity of the heatmap are
    important. I’m colorblind, so I find that the [viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)
    colormap with opacity turned all the way up works best for me.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理深度图时，热图的色彩方案和透明度非常重要。我是色盲，因此我发现[viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)色图并将透明度调到最大最适合我。
- en: '![](../Images/bcf625a36ee7d97a1afe4d32d72d2046.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcf625a36ee7d97a1afe4d32d72d2046.png)'
- en: Visibility settings for heatmaps. Image courtesy of the author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 热图的可视化设置。图片由作者提供。
- en: Ground Truth?
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地面真实值？
- en: 'Inspecting these RGB images and depth maps, we can see that there are some
    inaccuracies in the ground truth depth maps. For example, in this image, the dark
    rift through the center of the image is actually the *farthest* part of the scene,
    but the ground truth depth map shows it as the *closest* part of the scene:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查这些RGB图像和深度图，我们可以看到地面真实深度图中存在一些不准确之处。例如，在这张图像中，图像中心的深色裂缝实际上是场景中最*远*的部分，但地面真实深度图却显示它是场景中*最近*的部分：
- en: '![](../Images/5089c29d7fffe32d2eb44bb9fba5f018.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5089c29d7fffe32d2eb44bb9fba5f018.png)'
- en: Issue in ground truth depth data for sample from the SUN RGB-D dataset. Image
    courtesy of the author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 来自SUN RGB-D数据集的样本的地面真实深度数据问题。图片由作者提供。
- en: 'This is one of the key challenges for MDE tasks: ground truth data is hard
    to come by, and is often noisy! It’s essential to be aware of this while evaluating
    your MDE models.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是MDE任务中的一个关键挑战：地面真实数据很难获得，而且通常存在噪声！在评估你的MDE模型时，了解这一点至关重要。
- en: Running Monocular Depth Estimation Models
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行单目深度估计模型
- en: Now that we have our dataset loaded in, we can run monocular depth estimation
    models on our RGB images!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据集，可以对我们的RGB图像运行单目深度估计模型！
- en: For a long time, the state-of-the-art models for monocular depth estimation
    such as [DORN](https://github.com/hufu6371/DORN) and [DenseDepth](https://github.com/ialhashim/DenseDepth)
    were built with convolutional neural networks. Recently, however, both transformer-based
    models such as [DPT](https://huggingface.co/docs/transformers/model_doc/dpt) and
    [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn), and diffusion-based
    models like [Marigold](https://huggingface.co/Bingxin/Marigold) have achieved
    remarkable results!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，像[DORN](https://github.com/hufu6371/DORN)和[DenseDepth](https://github.com/ialhashim/DenseDepth)这样的单目深度估计的最先进模型都是基于卷积神经网络构建的。然而，最近，基于变换器的模型，如[DPT](https://huggingface.co/docs/transformers/model_doc/dpt)和[GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)，以及基于扩散的模型，如[Marigold](https://huggingface.co/Bingxin/Marigold)，都取得了显著的成果！
- en: In this section, we’ll show you how to generate MDE depth map predictions with
    both DPT and Marigold. In both cases, you can optionally run the model locally
    with the respective Hugging Face library, or run remotely with [Replicate](https://replicate.com/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何使用DPT和Marigold生成MDE深度图预测。在这两种情况下，你可以选择使用各自的Hugging Face库在本地运行模型，或者通过[Replicate](https://replicate.com/)进行远程运行。
- en: 'To run via Replicate, install the Python client:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过Replicate运行，请安装Python客户端：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And export your Replicate API token:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 并导出你的Replicate API令牌：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 💡 With Replicate, it might take a minute for the model to load into memory on
    the server (cold-start problem), but once it does the prediction should only take
    a few seconds. Depending on your local compute resources, running on server may
    give you massive speedups compared to running locally, especially for Marigold
    and other diffusion-based depth-estimation approaches.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 💡 使用Replicate时，模型加载到服务器内存可能需要一些时间（冷启动问题），但一旦加载完成，预测应该只需几秒钟。根据你的本地计算资源，与本地运行相比，使用服务器运行可能会大大提高速度，特别是对于Marigold和其他基于扩散的深度估计方法。
- en: Monocular Depth Estimation with DPT
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用DPT进行单目深度估计
- en: The first model we will run is a dense-prediction transformer (DPT). DPT models
    have found utility in both MDE and semantic segmentation — tasks that require
    “dense”, pixel-level predictions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先运行一个密集预测变换器（DPT）。DPT模型在单目深度估计（MDE）和语义分割等任务中非常有用，这些任务需要“密集”的像素级预测。
- en: The checkpoint below uses [MiDaS](https://github.com/isl-org/MiDaS/tree/master),
    which returns the [inverse depth map](https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/),
    so we have to invert it back to get a comparable depth map.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的检查点使用了[MiDaS](https://github.com/isl-org/MiDaS/tree/master)，它返回的是[反向深度图](https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/)，因此我们需要将其反转回来，以获得可比较的深度图。
- en: 'To run locally with `transformers`, first we load the model and image processor:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地使用`transformers`运行，首先加载模型和图像处理器：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we encapsulate the code for inference on a sample, including pre and
    post processing:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将推理代码封装在一个样本中，包括预处理和后处理：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we are storing predictions in a `label_field` field on our samples, represented
    with a heatmap just like the ground truth labels.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将预测结果存储在样本的`label_field`字段中，用热图表示，和真实标签一样。
- en: Note that in the `apply_dpt_model()` function, between the model's forward pass
    and the heatmap generation, notice that we make a call to `torch.nn.functional.interpolate()`.
    This is because the model's forward pass is run on a downsampled version of the
    image, and we want to return a heatmap that is the same size as the original image.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在`apply_dpt_model()`函数中，在模型的前向传递和热图生成之间，我们调用了`torch.nn.functional.interpolate()`。这是因为模型的前向传递是在图像的下采样版本上运行的，而我们希望返回一个与原始图像大小相同的热图。
- en: Why do we need to do this? If we just want to *look* at the heatmaps, this would
    not matter. But if we want to compare the ground truth depth maps to the model’s
    predictions on a per-pixel basis, we need to make sure that they are the same
    size.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要这样做？如果我们只是想*查看*热图，这一点并不重要。但如果我们想逐像素地比较真实深度图和模型的预测结果，就需要确保它们的大小一致。
- en: 'All that is left to do is iterate through the dataset:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是遍历数据集：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/92fca2436322f33052cdabbccf4c1a59.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92fca2436322f33052cdabbccf4c1a59.png)'
- en: Relative depth maps predicted by a hybrid MiDaS DPT model on SUN RGB-D sample
    images. Image courtesy of the author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混合MiDaS DPT模型在SUN RGB-D样本图像上预测的相对深度图。图片由作者提供。
- en: 'To run with Replicate, you can use [this](https://replicate.com/cjwbw/midas)
    model. Here is what the API looks like:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Replicate运行，你可以使用[这个](https://replicate.com/cjwbw/midas)模型。以下是API的样式：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Monocular Depth Estimation with Marigold
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Marigold进行单目深度估计
- en: Stemming from their tremendous success in text-to-image contexts, diffusion
    models are being applied to an ever-broadening range of problems. [Marigold](https://huggingface.co/Bingxin/Marigold)
    “repurposes” diffusion-based image generation models for monocular depth estimation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 源于在文本到图像领域的巨大成功，扩散模型正在被应用于越来越广泛的问题。[Marigold](https://huggingface.co/Bingxin/Marigold)
    “重新利用”基于扩散的图像生成模型进行单目深度估计。
- en: 'To run Marigold locally, you will need to clone the git repository:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要在本地运行Marigold，你需要克隆这个git仓库：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This repository introduces a new diffusers pipeline, `MarigoldPipeline`, which
    makes applying Marigold easy:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个仓库介绍了一个新的扩散器管道`MarigoldPipeline`，使得应用Marigold变得更加简单：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Post-processing of the output depth image is then needed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来需要对输出的深度图像进行后处理。
- en: 'To instead run via Replicate, we can create an `apply_marigold_model()` function
    in analogy with the DPT case above and iterate over the samples in our dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果改为通过Replicate运行，我们可以创建一个`apply_marigold_model()`函数，类似于上面的DPT案例，并遍历数据集中的样本：
- en: '[PRE14]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d3b08d99c0c36b77d0d0c0bb16c53f4.png)'
- en: Relative depth maps predicted with Marigold endpoint on SUN RGB-D sample images.
    Image courtesy of the author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Marigold端点在SUN RGB-D样本图像上预测的相对深度图。图片由作者提供。
- en: Evaluating Monocular Depth Estimation Models
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估单目深度估计模型
- en: 'Now that we have predictions from multiple models, let’s evaluate them! We
    will leverage `scikit-image` to apply three simple metrics commonly used for monocular
    depth estimation: [root mean squared error](https://en.wikipedia.org/wiki/Root-mean-square_deviation)
    (RMSE), [peak signal to noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)
    (PSNR), and [structural similarity index](https://en.wikipedia.org/wiki/Structural_similarity)
    (SSIM).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了多个模型的预测结果，让我们来评估它们！我们将利用`scikit-image`来应用三个常用于单目深度估计的简单指标：[均方根误差](https://en.wikipedia.org/wiki/Root-mean-square_deviation)（RMSE）、[峰值信噪比](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)（PSNR）和[结构相似性指数](https://en.wikipedia.org/wiki/Structural_similarity)（SSIM）。
- en: 💡Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores
    indicate better predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 💡较高的PSNR和SSIM分数表示更好的预测，而较低的RMSE分数表示更好的预测。
- en: Note that the specific values I arrive at are a consequence of the specific
    pre-and-post processing steps I performed along the way. What matters is the relative
    performance!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我得到的具体数值是我在此过程中执行的特定前后处理步骤的结果。重要的是相对性能！
- en: 'We will define the evaluation routine:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义评估流程：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And then apply the evaluation to the predictions from both models:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将评估应用于两个模型的预测：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Computing average performance for a certain model/metric is as simple as calling
    the dataset’s `mean()`method on that field:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 计算某个模型/度量标准的平均性能，只需对该字段调用数据集的`mean()`方法：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: All of the metrics seem to agree that DPT outperforms Marigold. However, it
    is important to note that these metrics are not perfect. For example, RMSE is
    very sensitive to outliers, and SSIM is not very sensitive to small errors. For
    a more thorough evaluation, we can filter by these metrics in the app in order
    to visualize what the model is doing well and what it is doing poorly — or where
    the metrics are failing to capture the model’s performance.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的度量标准似乎都一致认为 DPT 优于 Marigold。然而，重要的是要注意，这些度量标准并不完美。例如，RMSE 对异常值非常敏感，而 SSIM
    对小误差不太敏感。为了更全面的评估，我们可以在应用程序中通过这些度量标准进行筛选，以可视化模型做得好的地方和做得差的地方——或者是度量标准未能捕捉到模型表现的地方。
- en: 'Finally, toggling masks on and off is a great way to visualize the differences
    between the ground truth and the model’s predictions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，切换遮罩的开关是可视化真实值与模型预测差异的好方法：
- en: '![](../Images/9cc8c71a8a38c238f74bdcfea80a543b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cc8c71a8a38c238f74bdcfea80a543b.png)'
- en: Visual comparison of heatmaps predicted by the two MDE models and the ground
    truth. Image courtesy of the author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由两种 MDE 模型预测的热力图与真实值的视觉对比。图片由作者提供。
- en: Conclusion
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: To recap, we learned how to run monocular depth estimation models on our data,
    how to evaluate the predictions using common metrics, and how to visualize the
    results. We also learned that monocular depth estimation is a notoriously difficult
    task.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们学习了如何在我们的数据上运行单目深度估计模型，如何使用常见的度量标准评估预测结果，以及如何可视化结果。我们还了解到，单目深度估计是一个公认的困难任务。
- en: Data quality and quantity are severely limiting factors; models often struggle
    to generalize to new environments; and metrics are not always good indicators
    of model performance. The specific numeric values quantifying model performance
    can depend greatly on your processing pipeline. And even your qualitative assessment
    of predicted depth maps can be heavily influenced by your color schemes and opacity
    scales.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量和数量是严重的限制因素；模型往往难以推广到新的环境；而且度量标准并不总是好的模型性能指示器。量化模型性能的具体数值可能会因你的处理流程而有所不同。即使是你对预测深度图的定性评估，也可能会受到你的色彩方案和不透明度比例的强烈影响。
- en: 'If there’s one thing you take away from this post, I hope it is this: it is
    mission-critical that you look at the depth maps themselves, and not just the
    metrics!'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这篇文章中学到一件事，我希望是这一点：查看深度图本身，而不仅仅是度量标准，这一点至关重要！
- en: 'Note: All images are courtesy of the author. The sub-dataset used for all workflows
    and visuals presented in this post is NYU depth v2, which is [permissively licensed
    for commercial use](https://github.com/dwofk/fast-depth/blob/master/LICENSE) (MIT).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所有图片由作者提供。本篇文章中展示的所有工作流和视觉效果使用的子数据集是 NYU depth v2，该数据集是[允许商业使用的开源许可证](https://github.com/dwofk/fast-depth/blob/master/LICENSE)（MIT）。
