- en: 'GPT Model: How Does it Work?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT模型：它是如何工作的？
- en: 原文：[https://towardsdatascience.com/gpt-model-how-does-it-work-74bbcc2e97d1?source=collection_archive---------1-----------------------#2024-02-21](https://towardsdatascience.com/gpt-model-how-does-it-work-74bbcc2e97d1?source=collection_archive---------1-----------------------#2024-02-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gpt-model-how-does-it-work-74bbcc2e97d1?source=collection_archive---------1-----------------------#2024-02-21](https://towardsdatascience.com/gpt-model-how-does-it-work-74bbcc2e97d1?source=collection_archive---------1-----------------------#2024-02-21)
- en: Let’s look together under the hood with Python and PyTorch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们一起用Python和PyTorch深入探索它的工作原理吧
- en: '[](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[![Dmitrii
    Eliuseev](../Images/7c48f0c016930ead59ddb785eaf3e0e6.png)](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)
    [Dmitrii Eliuseev](https://dmitryelj.medium.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)
    ·9 min read·Feb 21, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--74bbcc2e97d1--------------------------------)
    ·阅读时间：9分钟·2024年2月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b8e5cd4404f0635a88b139c733b2a8c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8e5cd4404f0635a88b139c733b2a8c1.png)'
- en: Image by Hal Gatewood, [Unsplash](https://unsplash.com/@halacious)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Hal Gatewood，[Unsplash](https://unsplash.com/@halacious)
- en: During the last few years, the buzz around AI has been enormous, and the main
    trigger of all this is obviously the advent of GPT-based large language models.
    Interestingly, this approach itself is not new. LSTM (long short-term memory)
    neural networks were created in 1997, and a famous paper, “Attention is All You
    Need,” was published in 2017; both were the cornerstones of modern natural language
    processing. But only in 2020 will the results of GPT-3 be good enough, not only
    for academic papers but also for the real world.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，围绕人工智能的热议一直非常庞大，而这一切的主要推动力显然是基于GPT的大型语言模型的出现。有趣的是，这种方法本身并不新颖。LSTM（长短期记忆）神经网络是1997年创建的，而2017年发表的著名论文《Attention
    is All You Need》则为现代自然语言处理奠定了基石。但直到2020年，GPT-3的效果才足够好，不仅仅是在学术论文中得到认可，也能在现实世界中得到应用。
- en: Nowadays, everyone can chat with GPT in a web browser, but probably less than
    1% of people actually know how it works. Smart and witty answers from the model
    can force people to think that they are talking with an *intelligent being*, but
    is it so? Well, the best way to figure it out is to see how it works. In this
    article, we will take a real GPT model from OpenAI, run it locally, and see step-by-step
    what is going on under the hood.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，任何人都可以在网页浏览器中与GPT聊天，但可能不到1%的人真正了解它是如何工作的。模型给出的聪明且机智的回答让人产生与*智能生物*对话的错觉，但真的是这样吗？好吧，了解它的最好方式就是看看它是如何工作的。在本文中，我们将拿OpenAI的一个真实GPT模型来做演示，在本地运行它，并一步步查看它的工作原理。
- en: This article is intended for beginners and people who are interested in programming
    and data science. I will illustrate my steps with Python, but deep Python understanding
    will not be required.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文面向初学者以及对编程和数据科学感兴趣的读者。我将通过Python来演示我的步骤，但不需要对Python有深入的理解。
- en: Let’s get into it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Loading The Model
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载模型
