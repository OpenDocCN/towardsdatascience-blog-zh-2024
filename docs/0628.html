<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>PyTorch and MLX for Apple Silicon</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>PyTorch and MLX for Apple Silicon</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-and-mlx-for-apple-silicon-4f35b9f60e39?source=collection_archive---------0-----------------------#2024-03-08">https://towardsdatascience.com/pytorch-and-mlx-for-apple-silicon-4f35b9f60e39?source=collection_archive---------0-----------------------#2024-03-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1010" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A side-by-side CNN implementation and comparison</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mikecvet.medium.com/?source=post_page---byline--4f35b9f60e39--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mike Cvet" class="l ep by dd de cx" src="../Images/93545a0c873515a599ba094ad51ee915.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*YYihSP6cl5ApCLzzu-55KA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--4f35b9f60e39--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mikecvet.medium.com/?source=post_page---byline--4f35b9f60e39--------------------------------" rel="noopener follow">Mike Cvet</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--4f35b9f60e39--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/8ca713060459ca7ca5b77df6b814fd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mq8-RUV1TNxQcE4gxRVsTA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">All images by author</figcaption></figure><p id="337d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A few months ago, <a class="af ny" href="https://www.theverge.com/2023/12/6/23990678/apple-foundation-models-generative-ai-mlx" rel="noopener ugc nofollow" target="_blank">Apple quietly released</a> the first public version of its <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/index.html" rel="noopener ugc nofollow" target="_blank">MLX framework</a>, which fills a space in between <a class="af ny" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank">PyTorch</a>, <a class="af ny" href="https://numpy.org/doc/stable/index.html" rel="noopener ugc nofollow" target="_blank">NumPy</a> and <a class="af ny" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank">Jax</a>, but optimized for Apple Silicon. Much like those libraries, MLX is a Python-fronted API whose underlying operations are largely implemented in C++.</p><p id="bcbe" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below are some observations of the similarities and differences between MLX and PyTorch. I implemented a bespoke convolutional neural network using PyTorch and its Apple Silicon GPU hardware support, and tested it on a few different datasets. In particular, the <a class="af ny" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank">MNIST dataset</a>, and the <a class="af ny" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">CIFAR-10</a> and <a class="af ny" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">CIFAR-100</a> datasets.</p><p id="22b6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All the code discussed below can be <a class="af ny" href="https://github.com/mikecvet/cnn/" rel="noopener ugc nofollow" target="_blank">found here</a>.</p><ul class=""><li id="3f2a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx nz oa ob bk"><a class="af ny" href="#c37c" rel="noopener ugc nofollow">Approach</a></li><li id="5c9b" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk"><a class="af ny" href="#7afa" rel="noopener ugc nofollow">Notes on MLX</a></li><li id="2e9a" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk"><a class="af ny" href="#8947" rel="noopener ugc nofollow">Performance</a></li><li id="7e9d" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk"><a class="af ny" href="#755f" rel="noopener ugc nofollow">Final Thoughts</a></li></ul><h1 id="c37c" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Approach</h1><p id="de60" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">I implemented the model with PyTorch first, since I’m more familiar with the framework. The model has a series of convolutional and pooling layers, followed by a few linear layers with dropout.</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="be39" class="pm oi fq pj b bg pn po l pp pq"># First block: Conv =&gt; ReLU =&gt; MaxPool<br/>self.conv1 = Conv2d(in_channels=channels, out_channels=20, kernel_size=(5, 5), padding=2)<br/>self.relu1 = ReLU()<br/>self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))<br/><br/># Second block: Conv =&gt; ReLU =&gt; MaxPool<br/>self.conv2 = Conv2d(in_channels=20, out_channels=50, kernel_size=(5, 5), padding=2)<br/>self.relu2 = ReLU()<br/>self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))<br/><br/># Third block: Conv =&gt; ReLU =&gt; MaxPool layers<br/>self.conv3 = Conv2d(in_channels=50, out_channels=final_out_channels, kernel_size=(5, 5), padding=2)<br/>self.relu3 = ReLU()<br/>self.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))<br/><br/># Fourth block: Linear =&gt; Dropout =&gt; ReLU layers<br/>self.linear1 = Linear(in_features=fully_connected_input_size, out_features=fully_connected_input_size // 2)<br/>self.dropout1 = Dropout(p=0.3)<br/>self.relu3 = ReLU()<br/><br/># Fifth block: Linear =&gt; Dropout layers<br/>self.linear2 = Linear(in_features=fully_connected_input_size // 2, out_features=fully_connected_input_size // 4)<br/>self.dropout2 = Dropout(p=0.3)<br/><br/># Sixth block: Linear =&gt; Dropout layers<br/>self.linear3 = Linear(in_features=fully_connected_input_size // 4, out_features=classes)<br/>self.dropout3 = Dropout(p=0.3)<br/><br/>self.logSoftmax = LogSoftmax(dim=1)</span></pre><p id="c54f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This architecture is overkill for MNIST dataset classification, but I wanted something with some complexity to compare the two frameworks. I tested this against the CIFAR datasets, which approached around 40% accuracy; not amazing, but I suppose decent for something that isn’t a <a class="af ny" href="https://en.wikipedia.org/wiki/Residual_neural_network" rel="noopener ugc nofollow" target="_blank">ResNet</a>.</p><p id="fc98" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After finishing this implementation, I wrote a parallel implementation leveraging MLX. I happily discovered that <em class="pr">most</em> of the PyTorch implementation could be directly re-used, after importing the necessary MLX modules and replacing the PyTorch ones.</p><p id="4574" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For example, the MLX version of the above code is <a class="af ny" href="https://github.com/mikecvet/cnn/blob/main/src/python/mlx/model.py#L42" rel="noopener ugc nofollow" target="_blank">here</a>; it's identical aside from a couple of differences in named parameters.</p><h1 id="7afa" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Notes on MLX</h1><p id="6954" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">MLX has some interesting properties worth calling out.</p><h2 id="6ca2" class="ps oi fq bf oj pt pu pv om pw px py op nl pz qa qb np qc qd qe nt qf qg qh qi bk">Array</h2><p id="e6c9" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">MLX’s <code class="cx qj qk ql pj b"><a class="af ny" href="https://ml-explore.github.io/mlx/build/html/python/array.html" rel="noopener ugc nofollow" target="_blank">array</a></code> class takes the place of <code class="cx qj qk ql pj b"><a class="af ny" href="https://pytorch.org/docs/stable/tensors.html" rel="noopener ugc nofollow" target="_blank">Tensor</a></code>; much of the documentation compares it to NumPy’s <code class="cx qj qk ql pj b"><a class="af ny" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" rel="noopener ugc nofollow" target="_blank">ndarray</a></code>, however it is also the datatype used and returned by the various <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/python/nn.html" rel="noopener ugc nofollow" target="_blank">neural network layers</a> available in the framework.</p><p id="83ee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx qj qk ql pj b">array</code> works mostly as you’d expect, though I did have a bit of trouble converting back and forth between deeply-nested <code class="cx qj qk ql pj b">np.ndarrays</code> and <code class="cx qj qk ql pj b">mlx.arrays</code> necessitating some <a class="af ny" href="https://github.com/mikecvet/cnn/blob/main/src/python/imagedata.py#L107" rel="noopener ugc nofollow" target="_blank">list type shuffling</a> to make things work.</p><h2 id="fd83" class="ps oi fq bf oj pt pu pv om pw px py op nl pz qa qb np qc qd qe nt qf qg qh qi bk">Lazy Computation</h2><p id="fb7e" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk"><a class="af ny" href="https://ml-explore.github.io/mlx/build/html/python/ops.html" rel="noopener ugc nofollow" target="_blank">Operations</a> in MLX are <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/usage/lazy_evaluation.html" rel="noopener ugc nofollow" target="_blank">lazily evaluated</a>; meaning that the only computation executed in the lazily-built compute graph is that which generates outputs <em class="pr">actually used</em> by the program.</p><p id="61f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are two ways to force evaluation of the results of operations (such as inference):</p><ul class=""><li id="10fb" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx nz oa ob bk">Calling <code class="cx qj qk ql pj b">mlx.eval()</code> on the output</li><li id="ab06" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">Referencing the value of a variable for any reason; for example when logging or within conditional statements</li></ul><p id="cf7c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This can be a little tricky when trying to manage the performance of the code, since a reference (even an incidental one) to any value triggers an evaluation of that variable as well as all intermediate variables within the graph. For example:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="89bd" class="pm oi fq pj b bg pn po l pp pq">def classify(X, y):<br/>  model = MyModel()                   # Not yet initialized<br/>  p = model(X)                        # Not yet computed<br/>  loss = mlx.nn.losses.nll_loss(p, y) # Not yet computed<br/><br/>  print(f"loss value: {loss}") # Inits `model`, computes `loss` _and_ `p`<br/>  mlx.eval(p)                  # No-op<br/><br/>  # Without the print() above, would return `p` and lazy `loss`<br/>  return p, loss </span></pre><p id="07bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This behavior also makes a little difficult to build one-to-one benchmarks between PyTorch and MLX-based models. Since training loops may not evaluate outputs within the loop itself, its computation needs to be forced in order to track the time of the actual operations.</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="9efb" class="pm oi fq pj b bg pn po l pp pq">test_start = time.perf_counter_ns() # Start time block<br/>accuracy, _ = eval(test_data_loader, model, n)<br/>mx.eval(accuracy) # Force calculation within measurement block<br/>test_end = time.perf_counter_ns() # End time block</span></pre><p id="8954" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There’s a tradeoff between accumulating a large implicit computation graph, and regularly forcing the evaluation of that graph during training. For example, I was able to lazily run through all of this model’s training epochs over the dataset in just a few seconds. However, the eventual evaluation of that (presumably enormous) implicit graph took roughly the same amount of time as <code class="cx qj qk ql pj b">eval</code>’ing after each batch. This is probably not always the case.</p><h2 id="8d88" class="ps oi fq bf oj pt pu pv om pw px py op nl pz qa qb np qc qd qe nt qf qg qh qi bk">Compilation</h2><p id="6250" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">MLX provides the ability to optimize the execution of pure functions through <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/usage/compile.html" rel="noopener ugc nofollow" target="_blank">compilation</a>. These can be either a direct call to <code class="cx qj qk ql pj b">mlx.compile()</code> or an annotation (<code class="cx qj qk ql pj b">@mlx.compile</code>) on a pure function (without side effects).</p><p id="42b2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are a few gotchas related to state mutation when using compiled functions; these are discussed <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/usage/compile.html#pure-functions" rel="noopener ugc nofollow" target="_blank">in the docs</a>.</p><p id="dcee" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af ny" href="https://ml-explore.github.io/mlx/build/html/dev/extensions.html" rel="noopener ugc nofollow" target="_blank">It seems like</a> this results in a compilation of logic into <a class="af ny" href="https://developer.apple.com/documentation/metal" rel="noopener ugc nofollow" target="_blank">Metal Shader Language</a> to be run on the GPU (I explored MSL earlier <a class="af ny" rel="noopener" target="_blank" href="/programming-apple-gpus-through-go-and-metal-shading-language-a0e7a60a3dba">here</a>).</p><h2 id="ce0a" class="ps oi fq bf oj pt pu pv om pw px py op nl pz qa qb np qc qd qe nt qf qg qh qi bk">API Compatibility and Code Conventions</h2><p id="a770" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">As mentioned above, it was pretty easy to convert much of my PyTorch code into MLX-based equivalents. A few differences though:</p><ul class=""><li id="59e9" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx nz oa ob bk">Some of the neural network layers discretely expect different configurations of inputs. For example, <code class="cx qj qk ql pj b"><a class="af ny" href="https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.Conv2d.html#mlx.nn.Conv2d" rel="noopener ugc nofollow" target="_blank">mlx.nn.Conv2d</a></code> expects input images in <code class="cx qj qk ql pj b">NHWC</code> format (with <code class="cx qj qk ql pj b">C</code> representing the channels dimensionality), while <code class="cx qj qk ql pj b"><a class="af ny" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="noopener ugc nofollow" target="_blank">torch.nn.Conv2d</a></code> expects <code class="cx qj qk ql pj b">NCHW</code> ; there are a few other examples of this. This required some <a class="af ny" href="https://github.com/mikecvet/cnn/blob/main/src/python/imagedata.py#L62" rel="noopener ugc nofollow" target="_blank">conditional tensor/array shuffling</a>.</li><li id="623e" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">There is unfortunately no analog to the relative <em class="pr">joy</em> that are PyTorch <a class="af ny" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html" rel="noopener ugc nofollow" target="_blank">Datasets and DataLoaders</a> being currently provided by MLX; instead I had to craft <a class="af ny" href="https://github.com/mikecvet/cnn/blob/main/src/python/mlx/dataset.py" rel="noopener ugc nofollow" target="_blank">something resembling them</a> by hand.</li><li id="b42b" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">Model implementations, deriving from <code class="cx qj qk ql pj b">nn.Module</code>, aren’t expected to override <code class="cx qj qk ql pj b">forward()</code> but rather <code class="cx qj qk ql pj b">__call__()</code> for inference</li><li id="12c3" class="nc nd fq ne b go oc ng nh gr od nj nk nl oe nn no np of nr ns nt og nv nw nx nz oa ob bk">I assume because of the potential for function compilation, as well as the lazy evaluation support mentioned above, the process of training using MLX optimizers is a bit different than with a typical PyTorch model. Working with the latter, one is used to the standard format of something like:</li></ul><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="b800" class="pm oi fq pj b bg pn po l pp pq">for X, y in dataloader:<br/>  p = model(X)<br/>  loss = loss_fn(p, y)<br/>  optimizer.zero_grad()<br/>  loss.backward()<br/>  optimizer.step()</span></pre><p id="e0bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">MLX encourages, and seems to expect, a format resembling the following, taken from <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/examples/mlp.html" rel="noopener ugc nofollow" target="_blank">the docs</a> and <a class="af ny" href="https://github.com/ml-explore/mlx-examples/blob/main/mnist/main.py" rel="noopener ugc nofollow" target="_blank">one of the repository examples</a>:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="9669" class="pm oi fq pj b bg pn po l pp pq">def loss_fn(model, X, y):<br/>  return nn.losses.cross_entropy(model(X), y, reduction="mean")<br/><br/>loss_and_grad_fn = nn.value_and_grad(model, loss_fn)<br/><br/>@partial(mx.compile, inputs=model.state, outputs=model.state)<br/>def step(X, y):<br/>  loss, grads = loss_and_grad_fn(model, X, y)<br/>  optimizer.update(model, grads)<br/>  return loss<br/><br/># batch_iterate is a custom generator function<br/>for X, y in batch_iterate(batch_size, train_images, train_labels):<br/>  loss = step(X, y)</span></pre><p id="3de6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Which is fine, but a bit more involved than I was expecting. Otherwise, everything felt very familiar.</p><h1 id="8947" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Performance</h1><p id="b1c5" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Note that all results below are from my MacBook Air M2.</p><p id="f1f5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This CNN has three configurations: <code class="cx qj qk ql pj b">PyTorch CPU</code>, <code class="cx qj qk ql pj b">PyTorch GPU</code>, and <code class="cx qj qk ql pj b">MLX GPU</code>. As a sanity check, over 30 epochs, here’s how the three compare in terms of accuracy and loss:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qm"><img src="../Images/dfbc12d31193594274e89c2795303e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eq92UuN8suIBmmuvYzNYmw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Accuracy and Loss over 30 epochs; visualization code available in the linked repository</figcaption></figure><p id="b443" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The results here are all in the same ballpark, though it’s interesting that the MLX-based model appears to converge more quickly than the PyTorch-based ones.</p><p id="cb18" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In addition, it seems like the accuracy of the MLX model is consistently slightly below that of the PyTorch-based models. I’m not sure what accounts for that discrepancy.</p><p id="e54f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In terms of runtime performance, I had other interesting results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/9602a40fc15378305f4337bb63c9f5bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcEpBOjJXAj1XzYLwaNicQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Training epoch runtime variance across the three model configurations</figcaption></figure><p id="401c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When training the model, the PyTorch-based model on the CPU unsurprisingly took the most time, from a minimum of 36 to a maximum of 45 seconds per epoch. The MLX-based model, running on the GPU, had a range of about 21–27 seconds per epoch. PyTorch running on the GPU, via the <code class="cx qj qk ql pj b">MPS device</code> , was the clear winner in this regard, with epochs ranging from 10–14 seconds.</p><p id="aae9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Classification over the test dataset of ten thousand images tells a different story.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qn"><img src="../Images/f823d919de4e496efd43687e2ee17212.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*amLbKxtXbf6DXTOSqaEUvA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Total time taken by each model variant to classify all 10k images in the test dataset; batches of 512</figcaption></figure><p id="bced" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While it took the CPU-based model around 1700ms to classify all 10k images in batches of 512, the GPU-based models completed this task in 1100ms for MLX and 850ms for PyTorch.</p><p id="93d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, when classifying the images individually rather than in batches:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/eedcde9e07e536cdae2ad04d5d076ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Kz60TfWInZg3Gasb4FalA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Total time taken by each model variant to classify all 10k images in the test dataset; single images at a time over ten thousand.</figcaption></figure><p id="ad8b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Apple Silicon uses a <a class="af ny" href="https://www.apple.com/newsroom/2021/10/introducing-m1-pro-and-m1-max-the-most-powerful-chips-apple-has-ever-built/" rel="noopener ugc nofollow" target="_blank">unified memory model</a>, which means that when setting the data and model GPU device to <code class="cx qj qk ql pj b">mps</code> in PyTorch via something like <code class="cx qj qk ql pj b">.to(torch.device(“mps”))</code> , <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/usage/unified_memory.html" rel="noopener ugc nofollow" target="_blank">there is no actual movement of data</a> to physical GPU-specific memory. So it seems like the overhead associated with PyTorch’s initialization of Apple Silicon GPUs for code execution is fairly heavy. As seen further above, it works great during parallel batch workloads. But for individual record classification after training, it was far outperformed by whatever MLX is doing under the hood to spin up GPU execution more quickly.</p><h2 id="8536" class="ps oi fq bf oj pt pu pv om pw px py op nl pz qa qb np qc qd qe nt qf qg qh qi bk">Profiling</h2><p id="e99c" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">Taking a quick look at some <code class="cx qj qk ql pj b">cProfile</code> output for the MLX-based model, ordered by cumulative execution time:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="ca98" class="pm oi fq pj b bg pn po l pp pq">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/>      426   86.564    0.203   86.564    0.203 {built-in method mlx.core.eval}<br/>        1    2.732    2.732   86.271   86.271 /Users/mike/code/cnn/src/python/mlx/cnn.py:48(train)<br/>    10051    0.085    0.000    0.625    0.000 /Users/mike/code/cnn/src/python/mlx/model.py:80(__call__)<br/>    30153    0.079    0.000    0.126    0.000 /Users/mike/Library/Python/3.9/lib/python/site-packages/mlx/nn/layers/pooling.py:23(_sliding_windows)<br/>    30153    0.072    0.000    0.110    0.000 /Users/mike/Library/Python/3.9/lib/python/site-packages/mlx/nn/layers/convolution.py:122(__call__)<br/>        1    0.062    0.062    0.062    0.062 {built-in method _posixsubprocess.fork_exec}<br/>    40204    0.055    0.000    0.055    0.000 {built-in method relu}<br/>    10051    0.054    0.000    0.054    0.000 {built-in method mlx.core.mean}<br/>      424    0.050    0.000    0.054    0.000 {built-in method step}</span></pre><p id="4d81" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We some time spent here in a few layer functions, with the bulk of time spent in <code class="cx qj qk ql pj b">mlx.core.eval()</code>, which makes sense since it’s at this point in the graph that things are actually being computed.</p><p id="93da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Using <code class="cx qj qk ql pj b"><a class="af ny" href="https://github.com/tlkh/asitop" rel="noopener ugc nofollow" target="_blank">asitop</a></code> to visualize the underlying timeseries <code class="cx qj qk ql pj b">powertools</code> data from MacOS:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/5a312cd1ca69b90f989e437fbd571f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1NUahuHpvva5q6BuiG51Vg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">asitop power history — MLX model run</figcaption></figure><p id="71fa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can see that the GPU is fully saturated during the training of this model, at its <a class="af ny" href="https://www.notebookcheck.net/Apple-MacBook-Air-M2-review-The-faster-10-core-GPU-isn-t-worth-it.640427.0.html" rel="noopener ugc nofollow" target="_blank">maximum clock speed of 1398 MHz</a>.</p><p id="075b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now compare to the PyTorch GPU variant:</p><pre class="mm mn mo mp mq pi pj pk bp pl bb bk"><span id="9467" class="pm oi fq pj b bg pn po l pp pq">   ncalls  tottime  percall  cumtime  percall filename:lineno(function)<br/>    15585   41.385    0.003   41.385    0.003 {method 'item' of 'torch._C.TensorBase' objects}<br/>    20944    6.473    0.000    6.473    0.000 {built-in method torch.stack}<br/>    31416    1.865    0.000    1.865    0.000 {built-in method torch.conv2d}<br/>    41888    1.559    0.000    1.559    0.000 {built-in method torch.relu}<br/>    31416    1.528    0.000    1.528    0.000 {built-in method torch._C._nn.linear}<br/>    31416    1.322    0.000    1.322    0.000 {built-in method torch.max_pool2d}<br/>    10472    1.064    0.000    1.064    0.000 {built-in method torch._C._nn.nll_loss_nd}<br/>    31416    0.952    0.000    7.537    0.001 /Users/mike/Library/Python/3.9/lib/python/site-packages/torch/utils/data/_utils/collate.py:88(collate)<br/>      424    0.855    0.002    0.855    0.002 {method 'run_backward' of 'torch._C._EngineBase' objects}<br/>        5    0.804    0.161   19.916    3.983 /Users/mike/code/cnn/src/python/pytorch/cnn.py:176(eval)</span></pre><p id="7c43" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Interestingly, the top function appears to be <code class="cx qj qk ql pj b">Tensor.item()</code>, which is called in various places in the code to calculate loss and accuracy, and possibly also within some of the layers referenced lower in the stack. Removing the tracking of loss and accuracy during training would probably have a noticeable improvement on overall training performance.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qq"><img src="../Images/fc384f2bf49b488781ec37aa290cc236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRgChzVGDpNu3wTnLYbFaw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">asitop power history — PyTorch GPU model run</figcaption></figure><p id="badc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Compared to the MLX model, the PyTorch variant doesn’t seem to have saturated the GPU during training (I didn’t see it breach 95%), and has a higher balance of usage on the CPU’s <a class="af ny" href="https://developer.apple.com/news/?id=vk3m204o" rel="noopener ugc nofollow" target="_blank">E cores and P cores</a>.</p><p id="6190" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s interesting that the MLX model makes heavier use of the GPU, but trains considerably more slowly.</p><p id="70ad" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Neither model (CPU or GPU-based) appears to have engaged the ANE (<a class="af ny" href="https://github.com/hollance/neural-engine" rel="noopener ugc nofollow" target="_blank">Apple Neural Engine</a>).</p><h1 id="755f" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Final Thoughts</h1><p id="d518" class="pw-post-body-paragraph nc nd fq ne b go pd ng nh gr pe nj nk nl pf nn no np pg nr ns nt ph nv nw nx fj bk">MLX was easy to pick up, and that should be the case for anyone with experience using PyTorch and NumPy. Though some of the <a class="af ny" href="https://ml-explore.github.io/mlx/build/html/usage/quick_start.html" rel="noopener ugc nofollow" target="_blank">developer documentation</a> is a bit thin, given the intent to provide tools compatible with those frameworks’ APIs, it’s easy enough to fill in any gaps with the corresponding PyTorch or NumPy docs (for example, SGD [<a class="af ny" href="https://ml-explore.github.io/mlx/build/html/python/optimizers/_autosummary/mlx.optimizers.SGD.html" rel="noopener ugc nofollow" target="_blank">1</a>] [<a class="af ny" href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html" rel="noopener ugc nofollow" target="_blank">2</a>]).</p><p id="4e10" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The overall performance of the MLX model was pretty good; I wasn’t sure whether I was expecting it to consistently outperform PyTorch’s <code class="cx qj qk ql pj b">mps</code> device support, or not. While it seemed like training was considerably faster through PyTorch on the GPU, single-item prediction, particularly at scale, was much faster through MLX for this model. Whether that’s an effect of of my MLX configuration, or just the properties of the framework, its hard to say (and if its the former — feel free to leave an issue on GitHub!)</p></div></div></div></div>    
</body>
</html>