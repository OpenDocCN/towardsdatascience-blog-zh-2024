- en: A Case for Bagging and Boosting as Data Scientists’ Best Friends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-case-for-bagging-and-boosting-as-data-scientists-best-friends-3acdd74d28e0?source=collection_archive---------8-----------------------#2024-12-17](https://towardsdatascience.com/a-case-for-bagging-and-boosting-as-data-scientists-best-friends-3acdd74d28e0?source=collection_archive---------8-----------------------#2024-12-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Leveraging wisdom of the crowd in ML models.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page---byline--3acdd74d28e0--------------------------------)[![Farzad
    Nobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page---byline--3acdd74d28e0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3acdd74d28e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3acdd74d28e0--------------------------------)
    [Farzad Nobar](https://medium.com/@fmnobar?source=post_page---byline--3acdd74d28e0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3acdd74d28e0--------------------------------)
    ·14 min read·Dec 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98e546113b8fb841ce7edcd7ac141df4.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Luca Upper](https://unsplash.com/@lucaupper?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/balloon-on-sky-Z-4kOr93RCI?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, we take resources such as Wikipedia or Reddit for granted —
    these resources rely on the collective knowledge of individual contributors to
    serve us with *mostly* accurate information, which is sometimes called the “wisdom
    of the crowd”. The idea is that the collective decision can be more accurate than
    any individual’s judgement, since we can each have our own implicit biases and/or
    lack of knowledge, resulting in some level of error in our judgement. Collectively,
    these errors might be offset by each other — for example, we can compensate for
    someone else’s lack of knowledge/expertise in one area, while they would make
    up for ours in other areas. Applying this idea to machine learning results in
    “ensemble” methods.
  prefs: []
  type: TYPE_NORMAL
- en: At a very high level, we train machine learning models in order to make predictions
    about the future. In other words, we provide the models with the training data
    with the hope that model can make good predictions about the future. But what
    if we could train several machine learning models and then somehow aggregate their
    opinions about the predictions? It turns out, this can be a very useful approach,
    which is widely used in the industry.
  prefs: []
  type: TYPE_NORMAL
