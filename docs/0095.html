<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Build a Semantic Search Engine for Emojis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Build a Semantic Search Engine for Emojis</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10">https://towardsdatascience.com/how-to-build-a-semantic-search-engine-for-emojis-ef4c75e3f7be?source=collection_archive---------8-----------------------#2024-01-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c250" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Find The Sentiment You’re Looking For 🔍🤔😀🚀</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jacob Marks, Ph.D." class="l ep by dd de cx" src="../Images/94d9832b8706d1044e3195386613bfab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*yeOWVwqab7MArjMlb2x4jw@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jacob_marks?source=post_page---byline--ef4c75e3f7be--------------------------------" rel="noopener follow">Jacob Marks, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ef4c75e3f7be--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/38c4d89af0b8bd1549b7af17e735042e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nBxFjs3i8vKVL3IfzfVaA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Semantic search over emojis for “halloween” using a custom emoji search engine.</figcaption></figure><p id="aa79" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you’ve ever used Google Docs, or Slack, you may have noticed that when you type a “:” immediately followed by another character, a list of emojis pops up:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ny"><img src="../Images/22de006beada9fd015206018637aeaf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*76B-Le-kDh4y22ev"/></div></div></figure><p id="20d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since I discovered this, I’ve been making <em class="nz">major </em>use out of the feature. I add emojis into way more of my messages, blog posts, and other written works than I ever imagined I would. I actually got so accustomed to this means of adding emojis that I installed <a class="af oa" href="https://matthewpalmer.net/rocket/" rel="noopener ugc nofollow" target="_blank">Rocket</a> — a free app that brings the same emoji searchability to all text boxes and text editors on the computer. It’s a game changer.</p><p id="bafc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But as I’ve used these emoji search engines more and more, I’ve noticed a frustrating limitation: all of the searches are based on the <em class="nz">exact </em>text in your query and in the name and description of the emoji. Essentially, you need to search for something incredibly precisely for any results to show up.</p><p id="568d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s an example: if we search for “audio”, not a single result shows up:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ob"><img src="../Images/8a07cd9f11d0ee6dbaebc12f5c01ab83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IOD_fIbqjRGWX1ab"/></div></div></figure><p id="d891" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This isn’t because the set of emojis is lacking in the audio category. If we were to type in “music” or “speaker”, we would get a long list of results. Instead, it has to do with the fact that the specific string of text “audio” does not show up in the name or textual description associated with any of the emojis.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk oc"><img src="../Images/4df2c1363dab033580d0ac83ffbf4870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yHjsbmp04RjorxIt"/></div></div></figure><p id="bcc0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This relatively minor inconvenience bothered me so much that I decided to build this:</p><p id="a449" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By “this”, I mean an open-source semantic emoji search engine, with both UI-centric and CLI versions. The Python CLI library can be found <a class="af oa" href="https://github.com/jacobmarks/emoji_search" rel="noopener ugc nofollow" target="_blank">here</a>, and the UI-centric version can be found <a class="af oa" href="https://github.com/jacobmarks/emoji-search-plugin" rel="noopener ugc nofollow" target="_blank">here</a>. You can also play around with a hosted (also free) version of the UI emoji search engine online <a class="af oa" href="https://try.fiftyone.ai/datasets/emojis/samples" rel="noopener ugc nofollow" target="_blank">here</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/2bb7629d1dedcae260e9e34cf7d77451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SFMx9NMh0UnQPjot"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="oe">Command line version of the Semantic Emoji Search Engine</em></figcaption></figure><p id="187a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Building this was not as simple or straightforward as I initially hoped. It took a lot of experimentation, and a lot of ideas I thought were quite clever fell essentially flat. But in the end, I was able to create an emoji search engine that works fairly well.</p><p id="0083" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s how I built it, what worked, and what didn’t, and the lessons learned along the way.</p><ul class=""><li id="a43d" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx of og oh bk"><a class="af oa" href="#ae15" rel="noopener ugc nofollow">What is an Emoji</a></li><li id="81b6" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk"><a class="af oa" href="#a881" rel="noopener ugc nofollow">The Data</a></li><li id="fbf6" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk"><a class="af oa" href="#2396" rel="noopener ugc nofollow">Emojis versus Images and Text</a></li><li id="8e24" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk"><a class="af oa" href="#89d6" rel="noopener ugc nofollow">Bridging the Modality Gap</a></li><li id="8b0d" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk"><a class="af oa" href="#733f" rel="noopener ugc nofollow">Using the Emoji Search Engine</a></li></ul><h1 id="ae15" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">What is an Emoji</h1><p id="845d" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Before building a semantic search engine for emojis, it’s worth briefly explaining what exactly an emoji is. The term <em class="nz">emoji</em> derives from the Japanese kanji 絵 (eh) meaning <em class="nz">picture</em>, and 文字 (mōji) meaning letter or character. Essentially, this means that an emoji is etymologically a pictogram, and while it is connected to the English word <em class="nz">emotion, </em>it is not an “emotion icon” — that is an <a class="af oa" href="https://en.wikipedia.org/wiki/Emoticon" rel="noopener ugc nofollow" target="_blank">emoticon</a>.</p><p id="7477" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Along with <a class="af oa" href="https://en.wikipedia.org/wiki/List_of_Unicode_characters#Latin_script" rel="noopener ugc nofollow" target="_blank">alphanumeric characters</a>, <a class="af oa" href="https://en.wikipedia.org/wiki/Latin_Extended-B#African_letters_for_clicks" rel="noopener ugc nofollow" target="_blank">African click sounds</a>, <a class="af oa" href="https://en.wikipedia.org/wiki/List_of_Unicode_characters#Mathematical_symbols" rel="noopener ugc nofollow" target="_blank">mathematical</a> and <a class="af oa" href="https://en.wikipedia.org/wiki/List_of_Unicode_characters#Geometric_Shapes" rel="noopener ugc nofollow" target="_blank">geometric symbols</a>, <a class="af oa" href="https://en.wikipedia.org/wiki/List_of_Unicode_characters#Dingbats" rel="noopener ugc nofollow" target="_blank">dingbats</a>, and <a class="af oa" href="https://en.wikipedia.org/wiki/List_of_Unicode_characters#Control_codes" rel="noopener ugc nofollow" target="_blank">computer control sequences</a>, emojis can be represented as Unicode characters, making them computer-readable. Unlike alphanumeric characters and other symbols, however, emojis are <em class="nz">maintained</em> by the <a class="af oa" href="https://home.unicode.org/" rel="noopener ugc nofollow" target="_blank">Unicode Consortium</a><em class="nz">.</em> The consortium solicits proposals for new emojis, and regularly selects which emojis will be added to the standard.</p><p id="5dc6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At the time of writing, in November 2023, there are <a class="af oa" href="https://home.unicode.org/emoji/about-emoji/" rel="noopener ugc nofollow" target="_blank">more than 3,600 recognized emojis</a>, symbolizing a wide range of ideas and sentiments. Some emojis are represented by a single unicode character, or <em class="nz">code-point</em>. For example, the “grinning face” emoji, 😀, is represented in unicode as U+1F600.</p><p id="e364" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Others are represented with sequences of code-points. These sequences, which combine single code-point emojis with the zero-width-joiner unicode character, are known as ZWJ sequences, and allow for the combining of concepts, in much the same way as <a class="af oa" href="https://en.wikipedia.org/wiki/Kangxi_radical" rel="noopener ugc nofollow" target="_blank">Chinese radicals</a> can be combined to create a character that tells a story. As an example, the emoji 👨‍👩‍👧is a zero-width joining of the emojis for <em class="nz">man </em>👨(U+1F468), <em class="nz">woman </em>👩(​​U+1F469), and <em class="nz">girl </em>👧(U+1F467), connected by the ZWJ code-point U+200D:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="a7a8" class="ps oo fq pp b bg pt pu l pv pw">👨‍👩‍👧 = U+1F468 U+200D U+1F469 U+200D U+1F467</span></pre><p id="d374" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">According to the Unicode Consortium, 92% of the world’s online population uses emojis in their communications, and the ten most-used emojis in 2021 were: 😂 ❤️ 🤣 👍 😭 🙏 😘 🥰 😍 😊.</p><h1 id="a881" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Starting with the Data</h1><p id="30eb" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Given that emojis are pictographs of sorts, I wanted to utilize both textual and visual information in the search process. My initial hypothesis was that for many emojis, the name — the text string used to invoke the emoji — conveys but a fraction of its meaning. This can be due to many reasons, from the limitations of natural language, to the additional meanings imbued by cultures and visual similarities. In order to truly bring the full essence of the emoji to bear, I needed to make use of visual information.</p><p id="2cc3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I found this <a class="af oa" href="https://www.kaggle.com/datasets/subinium/emojiimage-dataset" rel="noopener ugc nofollow" target="_blank">Kaggle Emojis dataset</a> from 2021, which has data about 1816 emojis, including the emoji representation, the text associated with it, the unicode code (or codes), and a <a class="af oa" href="https://en.wikipedia.org/wiki/Base64" rel="noopener ugc nofollow" target="_blank">base64</a> encoded image. Here’s what the first few rows of the dataset look like, loaded as a pandas DataFrame:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/ac2785f516f75b2549052b5388d10910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xFNhla8SY9N0eOT0"/></div></div></figure><p id="6748" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There are separate columns with names <code class="cx px py pz pp b">Apple</code>, <code class="cx px py pz pp b">Google</code>, <code class="cx px py pz pp b">Facebook</code>, etc. because the emoji renders differently depending on the computer, website, or application. I decoded the images from base64 and converted them into <a class="af oa" href="https://pypi.org/project/Pillow/" rel="noopener ugc nofollow" target="_blank">Pillow</a> images. Here is the first image from the Kaggle dataset (grinning face):</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="c006" class="ps oo fq pp b bg pt pu l pv pw">import base64<br/>from io import BytesIO<br/>from PIL import Image<br/><br/>## decode and convert first row Apple image<br/>im_str = df.Apple[0].replace('data:image/png;base64,', '')<br/>im = Image.open(BytesIO(base64.b64decode(im_str)))<br/>im</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/d2948f4c8927c66d864a400e50897b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/0*jMigNnMfeOFx79Ka"/></div></figure><p id="98e4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Upon conversion, however, it became clear that the images were very low resolution. This one, for instance, is only 72x72 pixels. To improve the quality of the images that I was going to pass into downstream models, and to improve the quality of the experience in the eventual UI-based application, I passed all of these low-resolution images into <a class="af oa" href="https://replicate.com/nightmareai/real-esrgan" rel="noopener ugc nofollow" target="_blank">Real-ESRGAN</a> to 10x the resolution.</p><p id="9554" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is what the resulting images looked like:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/4d6b93d5b04f05729c9f7dc4e48f6164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xfUv9CAj5u5eTl3I"/></div></div></figure><p id="1204" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Not all of the emojis had images for all of the image columns in the pandas DataFrame, so I used the first viable base64 encoding for each row.</p><h1 id="2396" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Emojis Versus Images and Text</h1><p id="b751" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Before diving any deeper, I want to emphasize one crucial element of emojis that makes them so special, and deserving of their own semantic search engine: in a sense, they are <em class="nz">both</em> images and text. From the human perspective, we can represent each emoji as a unicode character, on the same playing field as text characters, and we can represent it as a standalone image, both of which we saw in the previous section. Said another way, if we squint with one eye, we can see a pictogram as a picture, and if we squint with the other eye, we can see the same pictogram as text.</p><p id="23c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Computers, however, are not known for their ability to squint. While a computer may be able to display a unicode code-point as an emoji, a machine learning model may not have a good way of interpreting the emoji as text or images.</p><p id="dfd4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Whenever I’m working on semantic search applications that connect images and text, I start with a family of models known as <a class="af oa" href="https://github.com/openai/CLIP" rel="noopener ugc nofollow" target="_blank">contrastive language image pre-training</a> (CLIP). These models are trained on image-text pairs to generate similar vector representations or <a class="af oa" rel="noopener" target="_blank" href="/neural-network-embeddings-explained-4d028e6f0526"><em class="nz">embeddings</em></a> for images and their captions, and dissimilar vectors when images are paired with other text strings. There are multiple CLIP-style models, including <a class="af oa" href="https://github.com/mlfoundations/open_clip" rel="noopener ugc nofollow" target="_blank">OpenCLIP</a> and <a class="af oa" href="https://github.com/facebookresearch/metaclip" rel="noopener ugc nofollow" target="_blank">MetaCLIP</a>, but for simplicity we’ll focus on the original CLIP model from OpenAI. No model is perfect, and at a fundamental level there is no <em class="nz">right </em>way to compare images and text, but CLIP certainly provides a good starting point.</p><h1 id="fe26" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Interpreting Emojis as Text</h1><p id="9c2c" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">At a high level, language models process input text by converting it into an ordered sequence of <em class="nz">tokens</em>, and then <em class="nz">encoding </em>the tokens and positional information in a dense numerical vector. Each language model has its own <em class="nz">vocabulary</em> of tokens to decompose a text string into, spanning from individual letters to complete words. Some tokens are easily interpretable by a human, while others are not, and in the case of CLIP, the vocabulary has 49,408 entries.</p><p id="e5f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s see an explicit example. Assuming the CLIP library is installed, we can <em class="nz">tokenize </em>a text string “a dog” with:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="df92" class="ps oo fq pp b bg pt pu l pv pw">import clip<br/>text_tokens = clip.tokenize("a dog")<br/>print(text_tokens)</span></pre><pre class="qb po pp pq bp pr bb bk"><span id="dbb0" class="ps oo fq pp b bg pt pu l pv pw">tensor([[49406,   320,  1929, 49407,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,<br/>             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)</span></pre><p id="52ed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The output tensor contains four nonzero entries: 49406, 320, 1929, and 49407. To make sense of these, we can map these values back to keys in the <a class="af oa" href="https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/vocab.json" rel="noopener ugc nofollow" target="_blank">CLIP vocabulary dictionary</a>. The first number, 49406, corresponds to the key “&lt;|startoftext|&gt;”, and the last number, 49407 corresponds to the key “&lt;|endoftext|&gt;”. These are special tokens denoting the beginning and end of the text string to be encoded. The second number, 320, maps back to “a&lt;/w&gt;”, which signifies the character “a” followed by a new word. Finally, 1929 is the value for key “dog&lt;/w&gt;”.</p><p id="e63b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we try to tokenize a string containing an emoji, however, we quickly run into a hitch: emojis don’t get tokenized in the same way as other characters do. Let’s start with the dog emoji 🐶:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="f6ff" class="ps oo fq pp b bg pt pu l pv pw">clip.tokenize("🐶")<br/>## [49406, 10631, 49407, 0, ...]</span></pre><p id="cefb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Doing a reverse lookup for the key associated with 10,631, we get the token “ðŁĲ¶&lt;/w&gt;”. But if we pass this string into the tokenizer, we get a completely different set of token IDs:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="b549" class="ps oo fq pp b bg pt pu l pv pw">clip.tokenize("ðŁĲ¶")<br/>## [49406, 127, 108, 40419, 72, 329, 126, 370, 49407, 0, ...]</span></pre><p id="a1be" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An even more curious case concerns the flag emojis. If we take the emoji for the flag of Cameroon, for instance, we get:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="7179" class="ps oo fq pp b bg pt pu l pv pw">clip.tokenize("🇨🇲")<br/>## [49406, 8989, 366, 49407, 0, ...]</span></pre><p id="4f6f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The two non-start/end tokens here correspond to “ðŁĩ¨ðŁĩ” and “²&lt;/w&gt;”. If we plug the first of these back into the tokenizer, we get another completely different set of token IDs, but the second maps back to itself.</p><p id="86b1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Things get even more precarious when we start comparing embeddings of text strings with embeddings of emojis, parsed as text strings via this tokenizer. After all, we want to find the most relevant emojis given a <em class="nz">text query</em>. We can use the <a class="af oa" href="https://medium.com/@milana.shxanukova15/cosine-distance-and-cosine-similarity-a5da0e4d9ded" rel="noopener">cosine distance</a> as a way to measure how similar or different two vectors are — and by proxy the inputs that generated those embedding vectors are. A distance of 0 means that two vectors are completely aligned, and a distance of 1 implies that two vectors are orthogonal. If we wanted to treat emojis as text, we would want the name for an emoji to be relatively close to the tokenized emoji in the embedding space, but this is not always the case!</p><p id="198d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The utility below will compare an emoji and a list of text prompts:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="6f52" class="ps oo fq pp b bg pt pu l pv pw">!pip install fiftyone<br/><br/>from scipy.spatial.distance import cosine<br/>import fiftyone.zoo as foz<br/><br/>model = foz.load_zoo_model("clip-vit-base32-torch")<br/><br/>def compare_emoji_to_texts(emoji, texts):<br/>    emoji_embedding = model.embed_prompt(emoji)<br/>    text_embeddings = model.embed_prompts(texts)<br/><br/>    for text, text_embedding in zip(texts, text_embeddings):<br/>        print(f"Dist b/w {emoji} and {text}: {cosine(emoji_embedding, text_embedding):.4f}")</span></pre><p id="c5e7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s an example, where according to CLIP, the encoding for the “birthday” emoji 🎂is closer to “man” than “birthday”, closer to “dog” than “birthday present”, and closer to “car” than “candle”, “date”, or “holiday”:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="012a" class="ps oo fq pp b bg pt pu l pv pw">texts=​​["birthday", "birthday present", "cake", "candle", "car", "date", "dog", "holiday", "man"]<br/><br/>compare_emoji_to_texts("🎂", texts)</span></pre><pre class="qb po pp qc qd ay qe bk"><span id="571f" class="qf oo fq pp b hw qg qh l im pw">Dist b/w 🎂 and birthday: 0.1205<br/>Dist b/w 🎂 and birthday present: 0.1385<br/>Dist b/w 🎂 and cake: 0.1238<br/>Dist b/w 🎂 and candle: 0.2030<br/>Dist b/w 🎂 and car: 0.1610<br/>Dist b/w 🎂 and date: 0.1921<br/>Dist b/w 🎂 and dog: 0.1344<br/>Dist b/w 🎂 and holiday: 0.1844<br/>Dist b/w 🎂 and man: 0.0849</span></pre><p id="3646" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Sometimes, the emoji and its name (and similar concepts) are close together in the embedding space, but sometimes they are most certainly not.</p><p id="71fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also go the other way and retrieve the emojis whose embeddings most closely match the embedding of an input text prompt. For instance, for the input “love”, we get the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/9df5407cefbb3a9eb346f00af77ca861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KDSN5KCjEn6Zp5ks"/></div></div></figure><p id="cc93" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Of course, we can do way better than this!</p><h1 id="9389" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Interpreting Emojis as Images</h1><p id="6363" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">The high-resolution images of emojis that we generated using Real-ESRGAN provide an alternative pathway to searching through our emojis: treating emojis as <em class="nz">images</em>. We can use CLIP’s vision encoder to embed the images into the same vector space, and then query these image embeddings with our input text prompt.</p><p id="8a67" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For applications like cross-modal retrieval (or semantically searching images with text), CLIP typically works best when the image embeddings are compared to a text prompt that is the user’s query wrapped in the phrase “A photo of &lt;query&gt;”. As an example, the image embedding for a photo of a dog will be closer (in terms of the angle between the vectors) to the embedding of “A photo of a dog” than the embedding of the raw query “dog”.</p><p id="8d27" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, when I used this template, the results were underwhelming. For instance, here are the 25 top results for the query “A photo of a dog”:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/d281ba379d7de5eb874e930f9e631113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-HN3CcYtewvmoE40"/></div></div></figure><p id="a0ea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Because emojis aren’t exactly <em class="nz">photos</em>, I decided to dig a little deeper into this and try out a few templating, or wrapping strategies. To cover my bases, I test five formats for text prompts:</p><ol class=""><li id="0ec4" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qi og oh bk">&lt;emoji_name&gt;</li><li id="52de" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">A photo of a &lt;emoji_name&gt;</li><li id="246a" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">An emoji of &lt;emoji_name&gt;</li><li id="cf54" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">A photo of a &lt;emoji_name&gt; emoji</li><li id="9bb2" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">A &lt;emoji_name&gt; emoji</li></ol><p id="b5f1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I generated embeddings for all 1816 emojis with each of these methods, and computed the CLIPScore (cosine similarity multiplied by 100) of these vectors with the corresponding image embedding vectors.</p><p id="323c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here were the aggregate results:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="0de9" class="ps oo fq pp b bg pt pu l pv pw">Method        Min       Mean     Max<br/>A             16.96     29.04    37.49<br/>B             15.85     29.47    38.43<br/>C             18.94     33.25    44.60<br/>D             19.47     32.59    42.57<br/>E             18.95     31.83    43.35</span></pre><p id="0e1e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From these statistics, I thought that the “An emoji of” descriptors were the best fit of the bunch, as they had the highest mean and max. But when I tried to use this, the results were again less than ideal. They seemed to preference faces (e.g. 😄😢🙃👦👧), to the detriment of other emojis like symbols, animals, and flags. When it came to semantic emoji searches, I found that entering the raw text tended to work best. In other words, the CLIP embedding of “dog” worked better than “A photo of a dog”, or “An emoji of a dog”.</p><p id="3d7b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">There were a few takeaways from this:</p><ul class=""><li id="5075" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx of og oh bk">Overall image-text “alignment” isn’t necessarily important for semantic search</li><li id="15a0" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk">The images of the emojis encode (to some degree) the fact that they are not photos</li><li id="e6c3" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx of og oh bk">The word “emoji” biases CLIP toward faces</li></ul><h1 id="89d6" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Bridging the Modality Gap</h1><p id="ee8e" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">By this point, I had come to the conclusion that treating emojis as just images or just text leaves a lot of rich information on the table. To build a robust semantic emoji search engine, I wanted to incorporate both textual and image information, and bridge the gap between these two modalities.</p><p id="f69f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I tried generating descriptions of the emoji images using Adept’s multimodal <a class="af oa" href="https://www.adept.ai/blog/fuyu-8b" rel="noopener ugc nofollow" target="_blank">Fuyu-8b</a> model, but these descriptions proved far too detailed; I tried using other CLIP-style models like <a class="af oa" href="https://github.com/facebookresearch/metaclip" rel="noopener ugc nofollow" target="_blank">MetaCLIP</a>, but saw the same behavior as in CLIP; I even tried using <a class="af oa" href="https://openai.com/research/gpt-4v-system-card" rel="noopener ugc nofollow" target="_blank">GPT-4V</a> to generate captions for the emoji images, but was cut off by OpenAI because the rate limit for the model is 100 queries per day.</p><p id="187c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the end, I was able to pass the emoji unicode characters into the base GPT-4 API with the prompt:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="1b56" class="ps oo fq pp b bg pt pu l pv pw">QUERY_TEXT = """<br/>Your task is to write a brief description of the emoji {emoji}, in the format 'A photo of a ...'.  For example, 'A photo of a dog'. Do not include the emoji name or unicode in your description. Do not include the skin tone of the emoji. Do not include the word yellow in your response.  You may include the word 'emoji' in your description, but it is not necessary. Your description should be a single phrase, of no more than 10 words.<br/>"""</span></pre><p id="7a0a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After post-processing these captions, I removed the “A photo of” prefix and used these descriptions in the semantic search pipeline.</p><p id="b34b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The emoji search engine works as follows, taking in an input <em class="nz">query</em>:</p><ol class=""><li id="30b5" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qi og oh bk">Generate a set of 100 candidate emojis (out of 1816) with an image similarity search that compares the image embeddings to the query embedding. Save this ordering, <em class="nz">clip_image_ordering.</em></li><li id="eddf" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">Order these candidate emojis by the similarity of the CLIP embeddings of the emoji names to the query’s embedding (<em class="nz">clip_name_ordering</em>).</li><li id="9b81" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">Using a <a class="af oa" href="https://ai.plainenglish.io/decoding-sentence-representations-a-comprehensive-guide-to-cross-encoders-and-bi-encoders-67c4ac16e35f" rel="noopener ugc nofollow" target="_blank">cross-encoder</a>, order the emojis based on the similarity of their name (<em class="nz">cross_encoder_name_ordering</em>) and their description generated by GPT-4 (<em class="nz">cross_encoder_description_ordering</em>) to the query.</li><li id="d4fc" class="nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx qi og oh bk">Combine all four orderings using <a class="af oa" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html" rel="noopener ugc nofollow" target="_blank">reciprocal rank fusion</a>, and return the top results!</li></ol><p id="1946" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The resulting search engine isn’t perfect, but it does a decent job at incorporating textual and visual information. Because using a cross-encoder is more computationally expensive (and higher latency), this is reserved for the pared-down set of candidates. I use the <code class="cx px py pz pp b">distilroberta-base</code> checkpoint with the <code class="cx px py pz pp b">CrossEncoder</code> class from the <a class="af oa" href="https://www.sbert.net/index.html" rel="noopener ugc nofollow" target="_blank">Sentence Transformers</a> library.</p><p id="7fe2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When all of these steps are combined, this is the result:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/2ddaf850b6a4265b791b6faf02d0df4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RI752_UCaEEqK_xI.gif"/></div></div></figure><p id="97c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Again, it isn’t perfect. But it’s not bad!</p><h1 id="733f" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Using the Emoji Search Engine</h1><p id="4e27" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">There are three ways to use this emoji search engine: hosted (free), locally via UI (open source), or locally via command line (also open source). All three options are quite easy!</p><h1 id="1df1" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Online</h1><p id="ea1a" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Head over to <a class="af oa" href="https://try.fiftyone.ai/datasets/emojis/samples" rel="noopener ugc nofollow" target="_blank">try.fiftyone.ai/datasets/emojis</a>, sign in (it’s free), and click on the emoji button in the menu above the grid of images. That’s it!</p><h1 id="0a3a" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Locally via the UI</h1><p id="8415" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">If you want to perform emoji searches locally with the same visual interface, you can do so with the <a class="af oa" href="https://github.com/jacobmarks/emoji-search-plugin" rel="noopener ugc nofollow" target="_blank">Emoji Search plugin</a> for <a class="af oa" href="https://github.com/voxel51/fiftyone" rel="noopener ugc nofollow" target="_blank">FiftyOne</a>.</p><p id="db4d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, install FiftyOne:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="8f27" class="ps oo fq pp b bg pt pu l pv pw">pip install fiftyone</span></pre><p id="705b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then download the Emoji Search plugin and install its requirements:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="aad6" class="ps oo fq pp b bg pt pu l pv pw">fiftyone plugins download https://github.com/jacobmarks/emoji-search-plugin<br/>fiftyone plugins requirements @jacobmarks/emoji_search --install</span></pre><p id="4269" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Launch the FiftyOne App:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="2ca4" class="ps oo fq pp b bg pt pu l pv pw">fiftyone app launch</span></pre><p id="2cab" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Click on the “browse operations” text, search for “emoji”, and click on the entry “Create Emoji Dataset”. This will download the high-resolution images of the emojis, along with embeddings and all other relevant data. At the top left of the app, click in the “Select dataset” box and select “Emojis”. Now you should see the same UI as in the hosted version.</p><h1 id="da79" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Locally via the CLI</h1><p id="6946" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Finally, you can search via the command line using the <a class="af oa" href="https://github.com/jacobmarks/emoji_search" rel="noopener ugc nofollow" target="_blank">Emoji Search</a> Python CLI library. Install the package from GitHub repository with:</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="9ec5" class="ps oo fq pp b bg pt pu l pv pw">pip install git+https://github.com/jacobmarks/emoji_search.git</span></pre><p id="0e8d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then you can start searching using the <code class="cx px py pz pp b">emoji-search</code> command, followed by the text query (with or without quotation marks).</p><pre class="mm mn mo mp mq po pp pq bp pr bb bk"><span id="4129" class="ps oo fq pp b bg pt pu l pv pw">emoji-search beautiful sunset</span></pre><pre class="qb po pp pq bp pr bb bk"><span id="9466" class="ps oo fq pp b bg pt pu l pv pw">+-------+-----------------+---------+<br/>| Emoji |       Name       Unicode  |<br/>+-------+-----------------+---------+<br/>|  🌞   |   sun with face | U+1F31E |<br/>|  🌇   |      sunset     | U+1F307 |<br/>|  🌅   |      sunrise    | U+1F305 |<br/>|  🔆   |   bright button | U+1F506 |<br/>|  🌆   |cityscape at dusk| U+1F306 |<br/>+-------+-----------------+---------+</span></pre><p id="70e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The first search you perform will download embeddings to your device if necessary.<br/>All three versions support copying an emoji to clipboard with <a class="af oa" href="https://pypi.org/project/pyperclip/" rel="noopener ugc nofollow" target="_blank">pyperclip</a>. In the UI, click on the image for an emoji, and you’ll see a copy button appear in the menu. In the CLI, pass the <code class="cx px py pz pp b">-c</code> argument to copy the top result to clipboard.</p><h1 id="4e4d" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">Conclusion</h1><p id="951b" class="pw-post-body-paragraph nc nd fq ne b go pj ng nh gr pk nj nk nl pl nn no np pm nr ns nt pn nv nw nx fj bk">Emojis might seem like a silly subject to obsess over. And in practice, the utility of a semantic emoji search engine over lexical emoji search may be somewhat limited. The real value in this endeavor is in understanding the boundaries and overlaps between two modalities we traditionally think of as distinct: images and text. Emojis sit squarely in this intersection and as such, they allow us to probe the strengths and weaknesses — the capabilities and limitations of today’s multimodal models.</p><p id="6efb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The Semantic Emoji Search Engine I ended up building is far from perfect. Frankly, emojis have subjectivity, connoting different things for different people, that is impossible to precisely bottle up. But going back to the motivating example, when I type in “an audio player”, I get some solid results:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk od"><img src="../Images/1861d91728d8781810ec0c2072406e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*85dhWwv3n6-1wrAw"/></div></div></figure><p id="6287" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">I’ll end with a quote from <a class="af oa" href="https://en.wikipedia.org/wiki/Nancy_Gibbs" rel="noopener ugc nofollow" target="_blank">Nancy Gibbs</a>, a Professor at the Harvard Kennedy School and former managing editor for <em class="nz">TIME </em>magazine:</p><blockquote class="qk ql qm"><p id="1a89" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="fq">What makes emojis special is the fact that [they have] helped millions express themselves better than even the wide array of words in the Oxford dictionary [could].</em></p><p id="8f32" class="nc nd nz ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Nancy Gibbs</p></blockquote></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a9bd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="nz">Note: All images in article created by the author unless otherwise noted</em></p></div></div></div></div>    
</body>
</html>