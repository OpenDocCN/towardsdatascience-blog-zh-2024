- en: 'Mastering RAG Systems: From Fundamentals to Advanced, with Strategic Component
    Evaluation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09](https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Elevating your RAG System: A step-by-step guide to advanced enhancements via
    LLM evaluation, with a real-world data use case'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    ·29 min read·Apr 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a0272c5930ac85a330d3af9c48fb8c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: This article will guide you through building an advanced Retrieval-Augmented
    Generation (RAG) pipeline using the `llama-index` framework.
  prefs: []
  type: TYPE_NORMAL
- en: A Retrieval-Augmented Generation (RAG) system is a framework that makes generative
    AI models more accurate and reliable by using information from outside sources.
    In the context of this project, legal documents will be used as the external knowledge
    base.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we’ll start by establishing a basic RAG system before illustrating
    how to include advanced features. One of the challenges in constructing such a
    system is deciding on the best components for the pipeline. We will attempt to
    answer this by evaluating the critical components of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This article serves as a practical tutorial for implementing RAG systems, including
    their evaluation. While it doesn’t delve deeply into theoretical aspects, it will
    explain the concepts used in this article as thoroughly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Table of materiels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: · [Overview](#690f)
  prefs: []
  type: TYPE_NORMAL
- en: · [Set-up](#6324)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [1- Code](#e9f3)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [2- Data](#83e0)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3- Raw data transformation](#ab50)
  prefs: []
  type: TYPE_NORMAL
- en: · [Basic RAG system](#6587)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [1- Data ingestion](#0e34)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [2- Indexing and storage](#64f9)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3- Querying](#a526)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [4- Evaluation](#dd2b)
  prefs: []
  type: TYPE_NORMAL
- en: · [Evaluating embeddings](#5e0e)
  prefs: []
  type: TYPE_NORMAL
- en: · [Evaluating advanced features](#d423)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [1- Windowing](#f5f5)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [2- Hybrid search](#6ee9)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [3- Query rewriting](#2b7c)
  prefs: []
  type: TYPE_NORMAL
- en: · [Routing](#f638)
  prefs: []
  type: TYPE_NORMAL
- en: · [Conclusion](#24c9)
  prefs: []
  type: TYPE_NORMAL
- en: · [References](#7830)
  prefs: []
  type: TYPE_NORMAL
- en: · [To reach out](#3cdf)
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article unfolds in these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- We’ll start by building a **basic** RAG system using France’s Civil Code
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 2- Next, we’ll compare various **embedding** methods from OpenAI, Mistral, and
    open-source models, evaluating their context relevance.
  prefs: []
  type: TYPE_NORMAL
- en: 3- Advanced concepts like **windowing**, **hybrid search**, and **query rewriting**
    will be explored to improve the RAG system. The effectiveness of these techniques
    will be assessed using a set of evaluation queries.
  prefs: []
  type: TYPE_NORMAL
- en: 4- Lastly, we’ll index more law codes and demonstrate how to use the llama-index
    **Routing** feature to select the appropriate index and obtain the correct response.
  prefs: []
  type: TYPE_NORMAL
- en: Set-up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**1- Code**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To access the code for this project, execute the following command to clone
    the corresponding repository from GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, you’ll need to install the necessary packages. We used [poetry](https://python-poetry.org/)
    as our package manager for better handling of project dependencies. Install poetry
    using the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can find more about poetry installation [here](https://python-poetry.org/docs/#installing-with-the-official-installer).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, at the root of the project, use the following command to install the
    Python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For this project, it is required to have a Python environment running a version
    in the range of 3.9 to 3.11\. We strongly recommend the creation of a virtual
    environment to isolate your project’s package dependencies, ensuring they do not
    conflict with those installed globally on your system.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need **Docker** for this project.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, ensure the `OPENAI_API_KEY` environment variable is defined as we will
    utilize both OpenAI's `gpt-3.5-turbo` LLM and Ada embeddings in this project.
    If you're interested in conducting the experiment with Mistral embeddings, you
    must obtain an API key from the [Mistral platform](https://console.mistral.ai/).
    Then you need to create the corresponding environment variable `MISTRAL_API_KEY`.
  prefs: []
  type: TYPE_NORMAL
- en: 2- Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our RAG’s knowledge base consists of examples from the French law codes. A law
    Code is a comprehensive piece of legislation designed to authoritatively and logically
    set out the principles and rules in a specific area of law.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the **civil code** aims to reform and codify French laws related
    to private or civil law. This includes areas such as property, contracts, family
    law, and personal status.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a code is a set of ordered articles that are associated with certain
    metadata such as the chapter, title, section, etc …
  prefs: []
  type: TYPE_NORMAL
- en: Currently, there are approximately 78 legal codes in force in France. The French
    government publishes these codes for free on a website called [Légifrance](https://www.legifrance.gouv.fr/).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this project, you have two options to create the knowledge base:'
  prefs: []
  type: TYPE_NORMAL
- en: Load fresh data from the Legifrance API, which has an [open data status](https://www.legifrance.gouv.fr/contenu/pied-de-page/open-data-et-api).
    You can find Instructions on creating the API keys [here](https://github.com/HamzaG737/legal-code-rag/tree/main?tab=readme-ov-file#get-legifrance-api-keys).
    Then we’ll use the Python library [pylegifrance](https://github.com/rdassignies/pylegifrance)
    to request specific codes from the API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Load processed data from the repository at `./data/legifrance/ {code_name}.json`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since law codes change frequently, we recommend loading data directly from the
    API if you are interested in getting the latest versions of the law Codes. However,
    creating the API keys can be a bit tedious. If you’re in a rush and don’t need
    the latest content, you can load the data locally, which is the default setting.
  prefs: []
  type: TYPE_NORMAL
- en: If you have the API keys and want to reload the data, set the `reload_data`
    argument to `True` when creating the query engine. This engine represents our
    end-to-end RAG pipeline (we'll explain the query engine concept later).
  prefs: []
  type: TYPE_NORMAL
- en: 3- Raw data transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section is for the readers interested to know how we transformed the data
    we got from the legifrance API.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `./data_ingestion/preprocess_legifrance_data.py` we preprocess the data
    coming from the API using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We request a certain Code’s content from the API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We retrieve the articles content recursively from the API response json.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We deduplicate the articles and perform some cleaning, like removing the some
    html tags, stripping text, etc …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data that we obtain at the end of this process is a list of articles where
    each article is represented by its content, its metadata (for instance the title,
    section, paragraph …) and its number. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the english translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: An article will serve as the basic unit of data and will be encapsulated in
    what we call a `node` . More on this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Basic RAG system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A basic RAG system contains **four** important steps. We will examine each of
    these steps and illustrate their application in our project.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd2c73bcf52db6d8e915faa4f1fe0a54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Illustration of the basic RAG system applied on law Codes. Image generated
    by author using the [**Diagrams: Show Me GPT**](https://helpful.dev/)'
  prefs: []
  type: TYPE_NORMAL
- en: 1- Data ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This phase involves the collection and preprocessing of relevant data from
    a variety of sources, such as PDF files, databases, APIs, websites, and more.
    This stage is closely tied to two key concepts: **documents** and **nodes**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the terminology of LlamaIndex, a `Document` refers to a container that encapsulates
    any data source, like a PDF, an API output, or data retrieved from a database.
    A `Node`, on the other hand, is the basic unit of data in LlamaIndex, representing
    a “chunk” of a source Document. Nodes carry metadata linking them to the document
    they belong to and to other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In our project, a document would be the full civil Code text and a node an article
    of this code. However, since we get the data already parsed into different articles
    from the API, we won’t need to chunk the documents and therefore prevent all the
    errors induced by this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to create the nodes is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: First we load the articles in `try_load_data` as mentioned in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We chunk the long articles into “sub-articles” so that we can embed them without
    truncation. This is done in `_chunk_long_articles`. The full code for the chunking
    can be found on`data_ingestion/ nodes_processing.py`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We create nodes using llama-index `TextNode` class. At the end of this step,
    our data consists of a list of nodes where each node has a text, an id and its
    metadata. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The metadata will be used for both the embedding and the LLM context. Specifically,
    we will embed the metadata as key-value pairs concatenated with the content, following
    the `text_template` format. This format will also be used to represent the node
    when forming the LLM context. If desired, you can exclude some or all of the metadata
    for the embedding or the LLM by changing the `excluded_embed_metadata_keys` and/or
    `excluded_embed_metadata_keys`.
  prefs: []
  type: TYPE_NORMAL
- en: 2- Indexing and storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Indexing** is the process of creating a data structure that enables data
    queries. This typically involves generating `vector embeddings`, which are numerical
    representations that capture the core essence of the data. Additionally, various
    metadata strategies can be adopted to efficiently retrieve information that is
    contextually relevant. The embedding model is used not only to embed the documents
    during the index construction phase, but also to embed any queries we make using
    the query engine in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Storing** is the subsequent phase where the created index and other metadata
    are saved to prevent the need for re-indexing. This ensures that once data is
    organized and indexed, it remains accessible and retrievable without undergoing
    the indexing process again.'
  prefs: []
  type: TYPE_NORMAL
- en: The `vector embeddings` can be stored and persisted in a `vector database`.
    In the next section we will see how to create and run this database locally.
  prefs: []
  type: TYPE_NORMAL
- en: '**a- Setting-up the vector database**'
  prefs: []
  type: TYPE_NORMAL
- en: '[Qdrant](https://qdrant.tech/) will be used as a vector database to store and
    index the articles embeddings along with the meta-data. We will run the server
    locally using Qdrant official docker image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First you can pull the image from the Dockerhub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Following this, run the Qdrant service with the command below. This will also
    map the necessary ports and designate a local directory (`./qdrant_storage`) for
    data storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the setup complete, we can interact with the Qdrant service using its
    Python client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**b- Indexing and storing the data**'
  prefs: []
  type: TYPE_NORMAL
- en: Below is the code to index and store the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic of the indexing function is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: First we create the Qdrant client to interact with the vector database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We check the number of nodes in the desired collection. This collection name
    is provided by `code_nodes.nodes_config` and corresponds to the current experiment
    (that is a concatenation of the code name, the embedding method and eventually
    the advanced RAG technique(s) like *base, window-nodes, hybrid search…* more on
    these experiments later). If the number of nodes is different than the current
    number, we create a new index:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Else we return the existing index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The full code to create or retrieve indexes can be found in the `./retriever/get_retriever.py`
    module. You can also find more information about this phase on llama-index [documentation](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/).
  prefs: []
  type: TYPE_NORMAL
- en: 3- Querying
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a user query, we compute the similarity between the query embedding and
    the indexed node embeddings to find the most similar data. The content of these
    similar nodes is then used to generate a context. This context enables the Language
    Model to synthesize a response for the user.
  prefs: []
  type: TYPE_NORMAL
- en: We define a `query engine` based on the index created in the previous section.
    This engine is an end-to-end pipeline that takes a natural language query and
    returns a response, as well as the context retrieved and passed to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`update_prompts_for_query_engine` is a function that allows us to change the
    prompt of the response synthesizer, i.e the LLM responsible of taking the context
    as input and generating an answer for the user.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this new template, we emphasize the importance of preventing hallucinations.
    We also instruct the LLM to always reference the code name, such as the civil
    code, and the article number before providing a response. Moreover, we instruct
    it to reply in the language of the query to support multilingual queries.
  prefs: []
  type: TYPE_NORMAL
- en: The `create_query_engine` function in the `query_engine` module is the entrypoint
    for creating the RAG pipeline. The arguments for this function define the RAG
    configuration parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code below creates a basic query engine and generates a response based
    on a specific query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can find the full code to create the query engine in `./query/query_engine.py`
    . More infos about the llama-index query engine can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/).
  prefs: []
  type: TYPE_NORMAL
- en: 4- Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Evaluation](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/)
    is crucial to assess the performance of the RAG pipeline and validate the decisions
    made regarding its components.'
  prefs: []
  type: TYPE_NORMAL
- en: In this project, we will employ an LLM-based evaluation method to assess the
    quality of the results. This evaluation will involve a golden LLM analyzing a
    set of responses against certain metrics to ensure the effectiveness and accuracy
    of the RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, a RAG system can be evaluated in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Response Evaluation**: Does the response align with the retrieved context
    and the query?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval Evaluation**: Are the data sources retrieved relevant to the query?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LLama-index` provides several evaluators to compute various metrics, such
    as **faithfulness**, **context relevancy**, and **answer relevancy**. You can
    find more details about these metrics in the evaluation section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the process for creating evaluators in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, we generate `n` questions using an LLM (in our case, gpt-4) about
    the legal code we want to evaluate. For cost reasons, we''ve set `n=50`, but a
    larger number could provide a more confident assessment of the system''s performance.
    An LLM-generated question example is “What are the legal consequences of bigamy
    in France?” or in French: “Quelles sont les conséquences juridiques de la bigamie
    en France?”. Note that this approach has limitations as the questions generated
    by the LLM may not mirror the actual distribution of questions from real users.
    You can find the module for questions generation is `evaluation/generate_questions.py`
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the list of generated questions, an evaluation metric, and a query engine,
    we generate a list of responses. Each response is scored between 0 and 1 by the
    evaluator, which also provides a feedback text to explain its scoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llama-index` provides evaluators for different metrics, such as the `ContextRelevancyEvaluator`
    to compute the context relevancy. In addition, we override the `evaluate_response`
    method of these evaluators to take into account the metadata when embedding and
    creating context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Below is the code to obtain evaluation results from a specific evaluator, such
    as the context relevancy evaluator. It’s worth noting that you can generate these
    evaluations in an asynchronous mode for quicker results. However, due to OpenAI
    rate limits, we performed these evaluations sequentially.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Finally to get a score for a certain pipeline (defined by its query engine),
    we average the scores given by the evaluator to each query.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete code for evaluation is located in `evaluation/eval_with_llamaindex.py`.
  prefs: []
  type: TYPE_NORMAL
- en: For the upcoming RAG pipeline evaluations, we will use 50 questions generated
    by GPT-4 based on the French civil code. These questions can be found on `./data/questions_code_civil.json`
    . The gold LLM used for evaluation is `gpt-3.5-turbo`.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**All the evaluation experiments can be found on this** [**notebook**](https://github.com/HamzaG737/legal-code-rag/blob/main/notebooks/evaluate_with_llamaindex.ipynb)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first evaluate the embeddings of the civil codearticles (or nodes)*.*
    In order to choose the set of embeddings models we will be evaluating, we relied
    on the Huggingface [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    for retrieval on french data. Hence we will be evaluating these three models :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text-Embedding-Ada-002** from OpenAI that is state of the art for this task
    at the time of writing this article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**mistral-embed** from Mistral AI that comes as a close second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multilingual-e5-large** from Infloat as the open source contender. We will
    also use Qdrant’s [**fast-embed**](https://qdrant.tech/articles/fastembed/)framework
    to improve the efficiency of the embeddings creation, mainly by quantizing the
    weights and using ONNX as runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can generate these embeddings with llama-index by importing the corresponding
    integration. For instance, we import the module `FastEmbedEmbedding` to generate
    the `e5-large` embeddings with the`fast-embed` framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The full embeddings definition can be found in `retriever/embeddings.py`
  prefs: []
  type: TYPE_NORMAL
- en: The metric that we will be measuring is the context relevancy, i.e the relevancy
    of the retrieved context relative to the user query.
  prefs: []
  type: TYPE_NORMAL
- en: Here is the code to launch the evaluations for this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now here is the final table of results. The `embedding_time` field represents
    the time it took us to embed the full `~2800` articles of the civil code.
  prefs: []
  type: TYPE_NORMAL
- en: We observe that `text-embedding-data-002` outperforms the other two embedding
    models in both score and embedding time. The gap between `mistral-embed` and `ada`
    from one side, and the `multilingual-e5-large` from the other, is quite significant.
    Note that the ranking of these embedding methods aligns with the leaderboard mentioned
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: However, bear in mind that our evaluation is based on only 50 questions generated
    by an LLM. Thus, it may not fully represent performance in a real-world scenario,
    especially when differentiating between mistral-embed and ada, where the gap is
    relatively small.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating advanced features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1- Windowing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first advanced feature we’ll explore is augmenting node content with the
    content from preceding and succeeding nodes. This makes sense within the context
    of law, where neighboring articles are closely related.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use `k` to parameterize windowing, representing the number of nodes before
    and after to add to the current node. For example, `k=1` means that the current
    node content will be augmented with the previous and next nodes.
  prefs: []
  type: TYPE_NORMAL
- en: For a given node, the original content will be used for embedding, while the
    augmented content will create the context for the response generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we perform node windowing:'
  prefs: []
  type: TYPE_NORMAL
- en: We create a custom function to generate the augmented nodes. Although Llama-index
    has its `SentenceWindowNodeParser` class for this task, it requires whole documents
    as inputs and then performs splitting to create nodes. So we took inspiration
    from this class and created our own window parser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we modify the previously created `CodeNodes` class to add the possibility
    of using neighbouring nodes for augmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: A key point to note is the `MetadataReplacementPostProcessor`. This class is
    used to signal that we need to replace the node content with the content found
    in the `target_metadata_key` field, right before passing the retrieved data to
    the LLM. Hence this field will contain the augmented content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lastly, we need to incorporate the post-processors into the query engine:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In this experiment, we will compare the base RAG pipeline to two other pipelines.
    These additional pipelines use node windowing with `k=1` and `k=2` respectively.
    We will use the same set of questions and context relevancy metrics to evaluate
    these pipelines. Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: We can observe that augmenting node content with neighbouring nodes can improve
    the context relevancy score. The small margin of improvement is probably due to
    an already high score (0.89).
  prefs: []
  type: TYPE_NORMAL
- en: We will also measure the **faithfulness** for this experiment. In llama-index,
    the `FaithfulnessEvaluator` module determines if a query engine’s response matches
    any source nodes, indicating whether the response was hallucinated or not. It’s
    important to note that this evaluator provides a binary score (0 if the response
    is a hallucination, otherwise 1). This is in contrast to the **context relevancy**
    score, which can range between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: In our evaluation of 50 questions, we found that only one response was a hallucination
    in the base experiment, while there were five hallucinations in the windowing
    pipeline with `k=2`. This suggests that adding more context, especially irrelevant
    context, can induce more hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: In subsequent experiments, we will maintain the base setup without node augmentation.
    This decision is guided by our observation that the slight increase in relevance
    does not offset the higher rate of hallucinations in responses.
  prefs: []
  type: TYPE_NORMAL
- en: 2- Hybrid search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Hybrid search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/)
    refers to combining search results from `sparse` and `dense` vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Dense vectors` represent data points as compact vectors of continuous values,
    offering a rich, nuanced understanding of the data’s features. These vectors,
    often result from deep learning models such as OpenAI, Mistral, e5-large, etc…
    So far, we have used only dense vectors during the retrieval phase.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Sparse vectors`, in contrast, are primarily zeros and are produced through
    specialized models such as TF-IDF, BM25, etc. They excel at identifying specific
    keywords and minor details, differentiating them from the more semantically rich
    dense vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement hybrid search in our RAG pipeline, we need to modify some parts
    of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: First we need to enable hybrid search in our vector database by setting `enable_hybrid=True`
    in the Qdrant client definition. This will run sparse vector generation locally
    using the `"naver/efficient-splade-VI-BT-large-doc"` model from Huggingface, in
    addition to generating the usual dense vectors with OpenAI’s ada model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When defining the query engine in `query_engine = index.as_query_engine(**kwargs)`,
    we need to define `sparse_top_k` along with `similarity_top_k` . `sparse_top_k`
    represents how many nodes will be retrieved from each dense and sparse query.
    For example, if `sparse_top_k=5` is set, that means we will retrieve 5 nodes using
    sparse vectors and 5 nodes using dense vectors. `similarity_top_k` controls the
    final number of returned nodes. In the above setting, we end up with 10 nodes.
    A fusion algorithm is then applied to rank and order the nodes from different
    vector spaces ([relative score fusion](https://weaviate.io/blog/hybrid-search-fusion-algorithms#relative-score-fusion)
    in this case). `similarity_top_k=5` means the top five nodes after fusion are
    returned. Here is the new definition of the `query_engine` after adding these
    options:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can adjust the weight for both vector and keyword searches by varying an
    `alpha` parameter. An `alpha` equal to 1 corresponds to a pure vector search,
    while an `alpha` equal to 0 corresponds to a pure keyword search. Below is the
    final definition of the `query_engine` after incorporating this alpha parameter:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To assess the relevance of hybrid search, we conducted three experiments with
    varying `alpha` parameters: `alpha = 0.2, 0.5, 0.8`. Note that the base experiment
    corresponds to `alpha=1`, representing a pure vector search.'
  prefs: []
  type: TYPE_NORMAL
- en: Here are the final results for the context relevancy and faithfulness metrics.
    Please note that embedding the `~2800` nodes with the `"naver/efficient-splade-VI-BT-large-doc"`
    model took us about `27 minutes` on an M1 Mac without a GPU, and it used almost
    all the system memory. Therefore, you might need a GPU to speed up the embedding
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The hybrid-search with `alpha=0.8` shows a slight improvement in the context
    relevancy score, but putting more weight on keyword search decreases the scores.
    The faithfulness scores are overall preserved.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the evaluation questions are quite general and lean more towards
    semantic search than exact-match search. However, in real-world scenarios, a user
    might ask for a specific article from the code, such as “what does the article
    x.y discuss?” In such cases, a keyword search can be helpful. Thus, we’ll retain
    the hybrid-search for future experiments with `alpha=0.8`, despite an additional
    hallucination case compared to the pure vector search case.
  prefs: []
  type: TYPE_NORMAL
- en: 3- Query rewriting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Query rewriting](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/)
    involves generating various questions similar to a specific query. This process
    can be used for disambiguation, error correction, or adapting the query to a specific
    knowledge base that supports the RAG system.'
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `QueryFusionRetriever` from llama-index for query rewriting.
    This module generates similar queries to the user query, retrieves and reranks
    the top `n` nodes from each generated query, including the original one, using
    the `Reciprocal Rerank Fusion` algorithm. This method, detailed in this [paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf),
    offers an efficient way to rerank retrieved nodes without excessive computation
    or dependence on external models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is the Python code for this task (located in `query/query_engine.py`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`num_queries` represents the total number of queries : `num_queries-1` generated
    queries, plus `1` for the original query. For subsequent experiments, we set `num_queries=4`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QUERY_GEN_PROMPT` is a prompt specifically designed for generating similar
    queries. We create a custom version of this prompt for the civil code knowledge
    base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Up until now, the evaluation questions we used were already clean and tailored
    for the civil code, as we instructed GPT-4 to generate them this way. However,
    as we mentioned earlier, this may not accurately represent the real-world users,
    who may ask ambiguous, error-prone, and short questions. Therefore, we generated
    another `50` questions using GPT-4, instructing the LLM to generate questions
    with these more realistic characteristics. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Or the english translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use this new evaluation dataset to compare the performance of pipelines
    with and without query rewriting. The comparison will be based on two metrics:
    **Faithfulness** and **Answer Relevancy**. Similar to context relevancy, answer
    relevancy measures the relevance of the answer to the user query, providing a
    score between 0 and 1\. The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that query rewriting improves not only faithfulness, where the model
    doesn’t hallucinate at all in the 50 questions, but also slightly boosts the answer
    relevancy score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how an original question was rewritten into three other
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Original question: `Can I refuse an inheritance?`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three generated queries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Routing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Until now, our knowledge base includes only one legal code for each RAG pipeline.
    Naturally, one might wonder how to add more legal codes, such as the penal code
    or traffic laws code, and how to construct a system that retrieves data from the
    correct legal code(s) when given a query. We can accomplish this in at least three
    ways with llama-index:'
  prefs: []
  type: TYPE_NORMAL
- en: Store all data from the different legal codes in the same index and build the
    query engine just like before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store all data in the same index but add the legal code name as metadata. During
    a query, the llama-index `[Auto-retriever](https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_auto_retriever/)`
    module infers a set of metadata filters and the appropriate query string to pass
    to the vector database. The `auto-retriever` must determine the correct code name
    and then execute a similarity search on the corresponding nodes to retrieve the
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Store every Legal Code nodes in a separate index, and use llama-index `Routing`
    feature to select the most relevant index or indices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this project, we opted for the last option. A more effective approach would
    be to evaluate these three options and select the best one. However we did not
    perform this evaluation due to time and space constraints, as this article is
    already extensive.
  prefs: []
  type: TYPE_NORMAL
- en: '[Routers](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)
    are modules that take in a query and a set of **choices** and return one or more
    selected choices.'
  prefs: []
  type: TYPE_NORMAL
- en: The selection process is conducted by an LLM. In our use case, the selector
    receives the legal code descriptions as input and returns one or more query engines.
    Each engine represents a RAG system for a single code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how we experiment routing in this project:'
  prefs: []
  type: TYPE_NORMAL
- en: We first define the list of law Codes to consider and their corresponding descriptions.
    The codes that we used for the routing experiment are the civil code, the taxes
    code, the intellectual property code, the traffic laws code and the labor code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We instantiate the `query_engine` for each code, as previously done. Then we
    wrap every `query_engine` in a `QueryEngineTool` module along with its description.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Finally we create the Router query engine using the list of tools and the llama-index
    `LLMMultiSelector` module. LLM selectors put the choices as a text dump into a
    prompt and use LLM text completion endpoint to make decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note that to keep things simple, we did not include any advanced features in
    the individual codes query engines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s test the router query engine on a query with two themes: labor code
    and tax code. This would allow us to see whether or not the router selects the
    correct indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Or its english translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can check the response given by the router query engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The important result here is the `selector_result`, which shows that the multi-selector
    correctly identified the labor code (index 4) and the tax code (index 1) as the
    relevant data sources for answering the user query.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the evaluation experiments plus the routing query engine definition
    in the notebook `./notebooks/evaluate_with_llamaindex.ipynb` .
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we discussed the transition from a basic RAG system to an advanced
    one, incorporating windowing, hybrid-search, and query rewriting, using the `llama-index`
    framework. We also explored how to evaluate certain components of the RAG pipeline,
    like the embeddings and the previously mentioned features, enabling informed decisions
    about their relevance, rather than relying on intuition.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at how routing allows us to select relevant indexes using LLM
    decision making.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some suggestions to further enhance the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this project, we used `gpt-3.5-turbo` as the LLM for generating responses,
    due to its ease of use and low cost. Similar to how we assessed various components
    of the RAG system, we could evaluate the LLM based on metrics like answer relevance
    and faithfulness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same comment applies to using `gpt-3.5-turbo` as an evaluator. Employing
    a more capable Language Model might enhance the evaluation accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, we’d assess all combinations of the RAG pipeline using various metrics.
    However, this might be costly and potentially unfeasible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might also be worthwhile to consider evaluating other advanced techniques
    in our system, such as the use of external models for reranking, using agents,
    etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also enhance the multilingual support of the RAG pipeline. One approach
    could be to set up a chain of LLMs to perform the following tasks: First, translating
    the query from the original language to French, then generating the response as
    usual, and finally, translating the response back to the original language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, readers looking to integrate a similar RAG system for legal queries
    should proceed cautiously due to the complexity and potential impact of legal
    advice. The risk of such systems generating **hallucinations** is a concern. Our
    evaluation, despite having high faithfulness scores, is limited by a small number
    and a limited scope of the questions, along with the inherent constraints of the
    evaluator module.
  prefs: []
  type: TYPE_NORMAL
- en: To address this, we recommend adding additional layers of evaluation and safety
    protocols. Moreover, despite the project showcasing the capabilities of advanced
    RAG settings for production, it is not yet a finalized system. This underscores
    the importance of continuous refinement and thorough testing before full-scale
    deployment. Finally, users should also be reminded that the system’s advice does
    not replace professional legal counsel.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[llama-index high level concepts.](https://docs.llamaindex.ai/en/stable/getting_started/concepts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[llama-index evaluation](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Qdrant hybrid search.](https://docs.llamaindex.ai/en/v0.10.20/examples/vector_stores/qdrant_hybrid.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Qdrant fast-embed](https://qdrant.tech/articles/fastembed/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[llama-index routers](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning
    Methods](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) Paper'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reach out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
