- en: 'Mastering RAG Systems: From Fundamentals to Advanced, with Strategic Component
    Evaluation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精通RAG系统：从基础到高级，通过战略性组件评估
- en: 原文：[https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09](https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09](https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09)
- en: 'Elevating your RAG System: A step-by-step guide to advanced enhancements via
    LLM evaluation, with a real-world data use case'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升你的RAG系统：通过LLM评估进行高级增强的逐步指南，附带实际数据使用案例
- en: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[![Hamza
    Gharbi](../Images/da96d29dfde486875d9a4ed932879aef.png)](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    [Hamza Gharbi](https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    ·29 min read·Apr 9, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------)
    ·29分钟阅读·2024年4月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7a0272c5930ac85a330d3af9c48fb8c1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a0272c5930ac85a330d3af9c48fb8c1.png)'
- en: Image generated by DALL-E.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由DALL-E生成的图像。
- en: This article will guide you through building an advanced Retrieval-Augmented
    Generation (RAG) pipeline using the `llama-index` framework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将引导你通过使用`llama-index`框架构建一个先进的检索增强生成（RAG）管道。
- en: A Retrieval-Augmented Generation (RAG) system is a framework that makes generative
    AI models more accurate and reliable by using information from outside sources.
    In the context of this project, legal documents will be used as the external knowledge
    base.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）系统是一个框架，它通过使用外部来源的信息，使生成性AI模型更准确、更可靠。在本项目中，法律文档将作为外部知识库使用。
- en: In this tutorial, we’ll start by establishing a basic RAG system before illustrating
    how to include advanced features. One of the challenges in constructing such a
    system is deciding on the best components for the pipeline. We will attempt to
    answer this by evaluating the critical components of the pipeline.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将从建立一个基本的RAG系统开始，然后再说明如何加入高级特性。构建此类系统的挑战之一是决定管道中最佳的组件。我们将通过评估管道中的关键组件来尝试回答这个问题。
- en: This article serves as a practical tutorial for implementing RAG systems, including
    their evaluation. While it doesn’t delve deeply into theoretical aspects, it will
    explain the concepts used in this article as thoroughly as possible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文作为实现RAG系统的实用教程，包括它们的评估。尽管它没有深入探讨理论方面的内容，但它会尽可能详细地解释文章中使用的概念。
- en: Table of materiels
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 材料表
- en: · [Overview](#690f)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [概述](#690f)
- en: · [Set-up](#6324)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · [设置](#6324)
- en: ∘ [1- Code](#e9f3)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [1- 代码](#e9f3)
- en: ∘ [2- Data](#83e0)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [2- 数据](#83e0)
- en: ∘ [3- Raw data transformation](#ab50)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3- 原始数据转换](#ab50)
- en: · [Basic RAG system](#6587)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: · [基础RAG系统](#6587)
- en: ∘ [1- Data ingestion](#0e34)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [1- 数据摄取](#0e34)
- en: ∘ [2- Indexing and storage](#64f9)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [2- 索引和存储](#64f9)
- en: ∘ [3- Querying](#a526)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3- 查询](#a526)
- en: ∘ [4- Evaluation](#dd2b)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [4- 评估](#dd2b)
- en: · [Evaluating embeddings](#5e0e)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: · [评估嵌入](#5e0e)
- en: · [Evaluating advanced features](#d423)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: · [评估高级特性](#d423)
- en: ∘ [1- Windowing](#f5f5)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [1- 窗口化](#f5f5)
- en: ∘ [2- Hybrid search](#6ee9)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [2- 混合搜索](#6ee9)
- en: ∘ [3- Query rewriting](#2b7c)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ∘ [3- 查询重写](#2b7c)
- en: · [Routing](#f638)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: · [路由](#f638)
- en: · [Conclusion](#24c9)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: · [结论](#24c9)
- en: · [References](#7830)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考资料](#7830)
- en: · [To reach out](#3cdf)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: · [联系](#3cdf)
- en: Overview
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'This article unfolds in these steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 本文按以下步骤展开：
- en: 1- We’ll start by building a **basic** RAG system using France’s Civil Code
    data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 1- 我们将从使用法国《民法典》数据构建一个**基础的** RAG 系统开始。
- en: 2- Next, we’ll compare various **embedding** methods from OpenAI, Mistral, and
    open-source models, evaluating their context relevance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2- 接下来，我们将比较 OpenAI、Mistral 和开源模型的各种**嵌入**方法，评估它们的上下文相关性。
- en: 3- Advanced concepts like **windowing**, **hybrid search**, and **query rewriting**
    will be explored to improve the RAG system. The effectiveness of these techniques
    will be assessed using a set of evaluation queries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 3- 我们将探讨像**窗口化**、**混合搜索**和**查询重写**等高级概念，以改进 RAG 系统。这些技术的有效性将通过一组评估查询进行评估。
- en: 4- Lastly, we’ll index more law codes and demonstrate how to use the llama-index
    **Routing** feature to select the appropriate index and obtain the correct response.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 4- 最后，我们将索引更多的法律条文，并演示如何使用 llama-index 的**路由**功能来选择适当的索引并获得正确的响应。
- en: Set-up
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: '**1- Code**'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1- 代码**'
- en: 'To access the code for this project, execute the following command to clone
    the corresponding repository from GitHub:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本项目的代码，请执行以下命令，从 GitHub 克隆相应的仓库：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, you’ll need to install the necessary packages. We used [poetry](https://python-poetry.org/)
    as our package manager for better handling of project dependencies. Install poetry
    using the following command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要安装必要的包。我们使用了 [poetry](https://python-poetry.org/) 作为我们的包管理器，以更好地处理项目依赖。使用以下命令安装
    poetry。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can find more about poetry installation [here](https://python-poetry.org/docs/#installing-with-the-official-installer).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [这里](https://python-poetry.org/docs/#installing-with-the-official-installer)了解更多关于
    poetry 安装的信息。
- en: 'Then, at the root of the project, use the following command to install the
    Python packages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在项目根目录下，使用以下命令安装 Python 包：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this project, it is required to have a Python environment running a version
    in the range of 3.9 to 3.11\. We strongly recommend the creation of a virtual
    environment to isolate your project’s package dependencies, ensuring they do not
    conflict with those installed globally on your system.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本项目，要求运行的 Python 环境版本在 3.9 至 3.11 之间。我们强烈建议创建虚拟环境来隔离项目的包依赖，确保它们不会与系统中全局安装的包发生冲突。
- en: You will also need **Docker** for this project.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目还需要**Docker**。
- en: Finally, ensure the `OPENAI_API_KEY` environment variable is defined as we will
    utilize both OpenAI's `gpt-3.5-turbo` LLM and Ada embeddings in this project.
    If you're interested in conducting the experiment with Mistral embeddings, you
    must obtain an API key from the [Mistral platform](https://console.mistral.ai/).
    Then you need to create the corresponding environment variable `MISTRAL_API_KEY`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，确保定义了 `OPENAI_API_KEY` 环境变量，因为我们将在本项目中使用 OpenAI 的 `gpt-3.5-turbo` LLM 和 Ada
    嵌入。如果你有兴趣使用 Mistral 嵌入进行实验，你必须从 [Mistral 平台](https://console.mistral.ai/) 获取 API
    密钥。然后，你需要创建相应的环境变量 `MISTRAL_API_KEY`。
- en: 2- Data
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2- 数据
- en: Our RAG’s knowledge base consists of examples from the French law codes. A law
    Code is a comprehensive piece of legislation designed to authoritatively and logically
    set out the principles and rules in a specific area of law.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 RAG 的知识库由法国法律条文中的示例组成。法律条文是一个全面的立法文件，旨在权威且逻辑地阐明某一特定法律领域的原则和规则。
- en: For instance, the **civil code** aims to reform and codify French laws related
    to private or civil law. This includes areas such as property, contracts, family
    law, and personal status.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，**民法典**旨在改革和编纂与私人或民事法相关的法国法律。包括财产、合同、家庭法和个人身份等领域。
- en: In general, a code is a set of ordered articles that are associated with certain
    metadata such as the chapter, title, section, etc …
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，法律条文是一组有序的条款，通常与某些元数据（如章节、标题、节等）相关联……
- en: Currently, there are approximately 78 legal codes in force in France. The French
    government publishes these codes for free on a website called [Légifrance](https://www.legifrance.gouv.fr/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，法国大约有 78 部有效的法律代码。法国政府通过一个名为 [Légifrance](https://www.legifrance.gouv.fr/)
    的网站免费发布这些法律代码。
- en: 'For this project, you have two options to create the knowledge base:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本项目，你有两种方法可以创建知识库：
- en: Load fresh data from the Legifrance API, which has an [open data status](https://www.legifrance.gouv.fr/contenu/pied-de-page/open-data-et-api).
    You can find Instructions on creating the API keys [here](https://github.com/HamzaG737/legal-code-rag/tree/main?tab=readme-ov-file#get-legifrance-api-keys).
    Then we’ll use the Python library [pylegifrance](https://github.com/rdassignies/pylegifrance)
    to request specific codes from the API.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Legifrance API加载最新数据，该API具有[开放数据状态](https://www.legifrance.gouv.fr/contenu/pied-de-page/open-data-et-api)。你可以在[这里](https://github.com/HamzaG737/legal-code-rag/tree/main?tab=readme-ov-file#get-legifrance-api-keys)找到创建API密钥的说明。然后，我们将使用Python库[pylegifrance](https://github.com/rdassignies/pylegifrance)从API请求特定的法律条文。
- en: Load processed data from the repository at `./data/legifrance/ {code_name}.json`.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`./data/legifrance/{code_name}.json`加载处理后的数据。
- en: Since law codes change frequently, we recommend loading data directly from the
    API if you are interested in getting the latest versions of the law Codes. However,
    creating the API keys can be a bit tedious. If you’re in a rush and don’t need
    the latest content, you can load the data locally, which is the default setting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于法律条文频繁变化，我们建议直接从API加载数据，如果你希望获取最新版本的法律条文。然而，创建API密钥可能有些繁琐。如果你时间紧迫且不需要最新内容，可以选择本地加载数据，这是默认设置。
- en: If you have the API keys and want to reload the data, set the `reload_data`
    argument to `True` when creating the query engine. This engine represents our
    end-to-end RAG pipeline (we'll explain the query engine concept later).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有API密钥并且希望重新加载数据，可以在创建查询引擎时将`reload_data`参数设置为`True`。该引擎代表我们的端到端RAG管道（稍后我们会解释查询引擎的概念）。
- en: 3- Raw data transformation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3- 原始数据转换
- en: This section is for the readers interested to know how we transformed the data
    we got from the legifrance API.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本节面向那些有兴趣了解我们如何从Legifrance API转换数据的读者。
- en: 'In `./data_ingestion/preprocess_legifrance_data.py` we preprocess the data
    coming from the API using the following steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在`./data_ingestion/preprocess_legifrance_data.py`中，我们使用以下步骤对来自API的数据进行预处理：
- en: We request a certain Code’s content from the API.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从API请求某个法律条文的内容。
- en: We retrieve the articles content recursively from the API response json.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们递归地从API响应的JSON中检索文章内容。
- en: We deduplicate the articles and perform some cleaning, like removing the some
    html tags, stripping text, etc …
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们去重文章并进行一些清理，如去除部分HTML标签、修剪文本等……
- en: 'The data that we obtain at the end of this process is a list of articles where
    each article is represented by its content, its metadata (for instance the title,
    section, paragraph …) and its number. For example:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在此过程中最终获得的数据是一系列文章，其中每篇文章由其内容、元数据（例如标题、章节、段落等）以及其编号表示。例如：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And here is the english translation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是英文翻译：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: An article will serve as the basic unit of data and will be encapsulated in
    what we call a `node` . More on this in the next section.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇文章将作为数据的基本单元，并将封装在我们称之为`节点`的结构中。更多内容将在下一节讨论。
- en: Basic RAG system
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的RAG系统
- en: A basic RAG system contains **four** important steps. We will examine each of
    these steps and illustrate their application in our project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的RAG系统包含**四个**重要步骤。我们将逐一考察这些步骤，并说明它们在我们项目中的应用。
- en: '![](../Images/dd2c73bcf52db6d8e915faa4f1fe0a54.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd2c73bcf52db6d8e915faa4f1fe0a54.png)'
- en: 'Illustration of the basic RAG system applied on law Codes. Image generated
    by author using the [**Diagrams: Show Me GPT**](https://helpful.dev/)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '法律条文上应用的基本RAG系统示意图。图片由作者使用[**Diagrams: Show Me GPT**](https://helpful.dev/)生成。'
- en: 1- Data ingestion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1- 数据摄取
- en: 'This phase involves the collection and preprocessing of relevant data from
    a variety of sources, such as PDF files, databases, APIs, websites, and more.
    This stage is closely tied to two key concepts: **documents** and **nodes**.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此阶段涉及从各种来源（如PDF文件、数据库、API、网站等）收集和预处理相关数据。此阶段与两个关键概念密切相关：**文档**和**节点**。
- en: In the terminology of LlamaIndex, a `Document` refers to a container that encapsulates
    any data source, like a PDF, an API output, or data retrieved from a database.
    A `Node`, on the other hand, is the basic unit of data in LlamaIndex, representing
    a “chunk” of a source Document. Nodes carry metadata linking them to the document
    they belong to and to other nodes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在LlamaIndex的术语中，`文档`是指封装任何数据源的容器，如PDF文件、API输出或从数据库中检索的数据。而`节点`则是LlamaIndex中数据的基本单元，代表文档源的一个“块”。节点携带元数据，将它们与所属的文档以及其他节点关联起来。
- en: In our project, a document would be the full civil Code text and a node an article
    of this code. However, since we get the data already parsed into different articles
    from the API, we won’t need to chunk the documents and therefore prevent all the
    errors induced by this process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的项目中，文档将是完整的《民法典》文本，而节点则是该法典的一条条文。然而，由于我们通过 API 已经获得了按条文划分的不同文章数据，因此不需要对文档进行分块，从而避免了由此过程引起的所有错误。
- en: 'The code to create the nodes is the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 创建节点的代码如下：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: First we load the articles in `try_load_data` as mentioned in the previous section.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们加载 `try_load_data` 中的文章，如前一节所述。
- en: We chunk the long articles into “sub-articles” so that we can embed them without
    truncation. This is done in `_chunk_long_articles`. The full code for the chunking
    can be found on`data_ingestion/ nodes_processing.py`
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将长文章分割成“子文章”，以便我们能够无截断地嵌入它们。这是在 `_chunk_long_articles` 中完成的。分块的完整代码可以在`data_ingestion/nodes_processing.py`中找到。
- en: 'We create nodes using llama-index `TextNode` class. At the end of this step,
    our data consists of a list of nodes where each node has a text, an id and its
    metadata. For example:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 llama-index 的 `TextNode` 类创建节点。在这一步结束时，我们的数据由一个节点列表组成，每个节点都有文本、ID 和其元数据。例如：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The metadata will be used for both the embedding and the LLM context. Specifically,
    we will embed the metadata as key-value pairs concatenated with the content, following
    the `text_template` format. This format will also be used to represent the node
    when forming the LLM context. If desired, you can exclude some or all of the metadata
    for the embedding or the LLM by changing the `excluded_embed_metadata_keys` and/or
    `excluded_embed_metadata_keys`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据将用于嵌入和 LLM 上下文。具体来说，我们将把元数据作为键值对嵌入，并与内容连接，遵循 `text_template` 格式。此格式也将用于在形成
    LLM 上下文时表示节点。如果需要，您可以通过更改 `excluded_embed_metadata_keys` 和/或 `excluded_embed_metadata_keys`
    来排除部分或全部元数据，用于嵌入或 LLM。
- en: 2- Indexing and storage
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2- 索引与存储
- en: '**Indexing** is the process of creating a data structure that enables data
    queries. This typically involves generating `vector embeddings`, which are numerical
    representations that capture the core essence of the data. Additionally, various
    metadata strategies can be adopted to efficiently retrieve information that is
    contextually relevant. The embedding model is used not only to embed the documents
    during the index construction phase, but also to embed any queries we make using
    the query engine in the future.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**索引**是创建一种数据结构的过程，该结构能够支持数据查询。通常这包括生成`向量嵌入`，这些嵌入是数值表示，能够捕捉数据的核心本质。此外，可以采用多种元数据策略，以有效地检索与上下文相关的信息。嵌入模型不仅用于在索引构建阶段嵌入文档，还用于嵌入我们将来使用查询引擎进行的任何查询。'
- en: '**Storing** is the subsequent phase where the created index and other metadata
    are saved to prevent the need for re-indexing. This ensures that once data is
    organized and indexed, it remains accessible and retrievable without undergoing
    the indexing process again.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**存储**是后续阶段，其中创建的索引和其他元数据将被保存，以防止重新索引的需要。这确保了一旦数据被组织和索引，它就可以访问并检索，而无需重新进行索引过程。'
- en: The `vector embeddings` can be stored and persisted in a `vector database`.
    In the next section we will see how to create and run this database locally.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`向量嵌入`可以存储并持久化在一个 `向量数据库` 中。在下一节中，我们将看到如何在本地创建和运行这个数据库。'
- en: '**a- Setting-up the vector database**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**a- 设置向量数据库**'
- en: '[Qdrant](https://qdrant.tech/) will be used as a vector database to store and
    index the articles embeddings along with the meta-data. We will run the server
    locally using Qdrant official docker image.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[Qdrant](https://qdrant.tech/) 将作为向量数据库，用于存储和索引文章的嵌入及其元数据。我们将使用 Qdrant 官方的
    Docker 镜像在本地运行服务器。'
- en: 'First you can pull the image from the Dockerhub:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以从 Dockerhub 拉取镜像：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Following this, run the Qdrant service with the command below. This will also
    map the necessary ports and designate a local directory (`./qdrant_storage`) for
    data storage:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，运行以下命令启动 Qdrant 服务。这也将映射必要的端口，并指定一个本地目录（`./qdrant_storage`）用于数据存储：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With the setup complete, we can interact with the Qdrant service using its
    Python client:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，我们可以使用 Python 客户端与 Qdrant 服务进行交互：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**b- Indexing and storing the data**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**b- 索引与存储数据**'
- en: Below is the code to index and store the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于索引和存储数据的代码。
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The logic of the indexing function is quite simple:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 索引函数的逻辑非常简单：
- en: First we create the Qdrant client to interact with the vector database.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们创建 Qdrant 客户端来与向量数据库进行交互。
- en: 'We check the number of nodes in the desired collection. This collection name
    is provided by `code_nodes.nodes_config` and corresponds to the current experiment
    (that is a concatenation of the code name, the embedding method and eventually
    the advanced RAG technique(s) like *base, window-nodes, hybrid search…* more on
    these experiments later). If the number of nodes is different than the current
    number, we create a new index:'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们检查所需集合中的节点数量。该集合名称由`code_nodes.nodes_config`提供，代表当前实验（即代码名称、嵌入方法以及可能的高级RAG技术，如*base,
    window-nodes, hybrid search…* 更多关于这些实验的内容稍后介绍）。如果节点数量与当前数量不同，我们将创建一个新索引：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Else we return the existing index:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 否则我们返回现有的索引：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The full code to create or retrieve indexes can be found in the `./retriever/get_retriever.py`
    module. You can also find more information about this phase on llama-index [documentation](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建或检索索引的完整代码可以在`./retriever/get_retriever.py`模块中找到。你还可以在 llama-index [文档](https://docs.llamaindex.ai/en/stable/understanding/storing/storing/)中找到更多关于此阶段的信息。
- en: 3- Querying
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3- 查询
- en: Given a user query, we compute the similarity between the query embedding and
    the indexed node embeddings to find the most similar data. The content of these
    similar nodes is then used to generate a context. This context enables the Language
    Model to synthesize a response for the user.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用户查询，我们计算查询嵌入与索引节点嵌入之间的相似度，以找到最相似的数据。这些相似节点的内容随后被用来生成上下文。这个上下文使得语言模型能够为用户合成一个响应。
- en: We define a `query engine` based on the index created in the previous section.
    This engine is an end-to-end pipeline that takes a natural language query and
    returns a response, as well as the context retrieved and passed to the LLM.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于上一节创建的索引定义一个`查询引擎`。这个引擎是一个端到端管道，接收自然语言查询并返回响应，以及检索到并传递给LLM的上下文。
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`update_prompts_for_query_engine` is a function that allows us to change the
    prompt of the response synthesizer, i.e the LLM responsible of taking the context
    as input and generating an answer for the user.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`update_prompts_for_query_engine`是一个函数，允许我们更改响应合成器的提示，即负责将上下文作为输入并为用户生成答案的LLM。'
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this new template, we emphasize the importance of preventing hallucinations.
    We also instruct the LLM to always reference the code name, such as the civil
    code, and the article number before providing a response. Moreover, we instruct
    it to reply in the language of the query to support multilingual queries.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个新模板中，我们强调防止幻觉的必要性。我们还指示LLM在提供响应之前始终参考代码名称，如民法典，以及条文号。此外，我们指示它用查询的语言进行回复，以支持多语言查询。
- en: The `create_query_engine` function in the `query_engine` module is the entrypoint
    for creating the RAG pipeline. The arguments for this function define the RAG
    configuration parameters.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_query_engine`函数在`query_engine`模块中是创建RAG管道的入口。此函数的参数定义了RAG配置参数。'
- en: 'The code below creates a basic query engine and generates a response based
    on a specific query:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建一个基本的查询引擎，并根据特定查询生成响应：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You can find the full code to create the query engine in `./query/query_engine.py`
    . More infos about the llama-index query engine can be found [here](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`./query/query_engine.py`中找到创建查询引擎的完整代码。更多关于llama-index查询引擎的信息可以在[这里](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)找到。
- en: 4- Evaluation
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4- 评估
- en: '[Evaluation](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/)
    is crucial to assess the performance of the RAG pipeline and validate the decisions
    made regarding its components.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[评估](https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/)对于评估RAG管道的性能至关重要，并验证关于其组件所做的决策。'
- en: In this project, we will employ an LLM-based evaluation method to assess the
    quality of the results. This evaluation will involve a golden LLM analyzing a
    set of responses against certain metrics to ensure the effectiveness and accuracy
    of the RAG pipeline.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将采用基于LLM的评估方法来评估结果的质量。这项评估将涉及一个金标准LLM，按照一定的指标分析一组响应，以确保RAG管道的有效性和准确性。
- en: 'Typically, a RAG system can be evaluated in two ways:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，RAG系统可以通过两种方式进行评估：
- en: '**Response Evaluation**: Does the response align with the retrieved context
    and the query?'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**响应评估**：响应是否与检索到的上下文和查询一致？'
- en: '**Retrieval Evaluation**: Are the data sources retrieved relevant to the query?'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索评估**：检索到的数据源是否与查询相关？'
- en: '`LLama-index` provides several evaluators to compute various metrics, such
    as **faithfulness**, **context relevancy**, and **answer relevancy**. You can
    find more details about these metrics in the evaluation section.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`LLama-index`提供了几种评估器来计算不同的指标，如**忠实度**、**上下文相关性**和**回答相关性**。您可以在评估部分找到有关这些指标的更多细节。'
- en: 'Here’s the process for creating evaluators in this project:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在此项目中创建评估器的过程：
- en: 'Initially, we generate `n` questions using an LLM (in our case, gpt-4) about
    the legal code we want to evaluate. For cost reasons, we''ve set `n=50`, but a
    larger number could provide a more confident assessment of the system''s performance.
    An LLM-generated question example is “What are the legal consequences of bigamy
    in France?” or in French: “Quelles sont les conséquences juridiques de la bigamie
    en France?”. Note that this approach has limitations as the questions generated
    by the LLM may not mirror the actual distribution of questions from real users.
    You can find the module for questions generation is `evaluation/generate_questions.py`
    .'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最初，我们使用LLM（在我们这个案例中是gpt-4）生成`n`个问题，关于我们想要评估的法律代码。出于成本考虑，我们设置了`n=50`，但更大的问题数量可能会提供对系统性能更有信心的评估。一个LLM生成的问题示例是：“法国重婚的法律后果是什么？”或者法语版本：“Quelles
    sont les conséquences juridiques de la bigamie en France ?”。请注意，这种方法有其局限性，因为LLM生成的问题可能无法反映真实用户提问的实际分布。您可以在`evaluation/generate_questions.py`找到问题生成模块。
- en: Using the list of generated questions, an evaluation metric, and a query engine,
    we generate a list of responses. Each response is scored between 0 and 1 by the
    evaluator, which also provides a feedback text to explain its scoring.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成的问题列表、评估指标和查询引擎，我们生成响应列表。每个响应由评估器在0到1之间打分，评估器还提供反馈文本来解释其评分。
- en: '`llama-index` provides evaluators for different metrics, such as the `ContextRelevancyEvaluator`
    to compute the context relevancy. In addition, we override the `evaluate_response`
    method of these evaluators to take into account the metadata when embedding and
    creating context.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llama-index`提供了不同指标的评估器，例如`ContextRelevancyEvaluator`用于计算上下文相关性。此外，我们重写了这些评估器的`evaluate_response`方法，以在嵌入和创建上下文时考虑元数据。'
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Below is the code to obtain evaluation results from a specific evaluator, such
    as the context relevancy evaluator. It’s worth noting that you can generate these
    evaluations in an asynchronous mode for quicker results. However, due to OpenAI
    rate limits, we performed these evaluations sequentially.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是从特定评估器（例如上下文相关性评估器）获取评估结果的代码。值得注意的是，您可以以异步模式生成这些评估，以便更快地获得结果。然而，由于OpenAI的速率限制，我们是按顺序执行这些评估的。
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Finally to get a score for a certain pipeline (defined by its query engine),
    we average the scores given by the evaluator to each query.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，为了获取某个管道（由其查询引擎定义）的得分，我们计算评估器给每个查询的分数的平均值。
- en: The complete code for evaluation is located in `evaluation/eval_with_llamaindex.py`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 评估的完整代码位于`evaluation/eval_with_llamaindex.py`。
- en: For the upcoming RAG pipeline evaluations, we will use 50 questions generated
    by GPT-4 based on the French civil code. These questions can be found on `./data/questions_code_civil.json`
    . The gold LLM used for evaluation is `gpt-3.5-turbo`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将进行的RAG管道评估，我们将使用由GPT-4基于法国民法典生成的50个问题。这些问题可以在`./data/questions_code_civil.json`找到。用于评估的金标准LLM是`gpt-3.5-turbo`。
- en: Evaluating embeddings
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估嵌入
- en: '**All the evaluation experiments can be found on this** [**notebook**](https://github.com/HamzaG737/legal-code-rag/blob/main/notebooks/evaluate_with_llamaindex.ipynb)**.**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有的评估实验可以在这个** [**notebook**](https://github.com/HamzaG737/legal-code-rag/blob/main/notebooks/evaluate_with_llamaindex.ipynb)
    **中找到。**'
- en: 'We will first evaluate the embeddings of the civil codearticles (or nodes)*.*
    In order to choose the set of embeddings models we will be evaluating, we relied
    on the Huggingface [leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
    for retrieval on french data. Hence we will be evaluating these three models :'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先评估民法典条文（或节点）的嵌入。为了选择我们将要评估的嵌入模型集，我们依赖于Huggingface的[排行榜](https://huggingface.co/spaces/mteb/leaderboard)，用于在法语数据上的检索。因此，我们将评估这三种模型：
- en: '**Text-Embedding-Ada-002** from OpenAI that is state of the art for this task
    at the time of writing this article.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Text-Embedding-Ada-002**来自OpenAI，是目前（撰写本文时）在此任务中最先进的技术。'
- en: '**mistral-embed** from Mistral AI that comes as a close second.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**mistral-embed**来自Mistral AI，紧随其后。'
- en: '**multilingual-e5-large** from Infloat as the open source contender. We will
    also use Qdrant’s [**fast-embed**](https://qdrant.tech/articles/fastembed/)framework
    to improve the efficiency of the embeddings creation, mainly by quantizing the
    weights and using ONNX as runtime.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**multilingual-e5-large**来自Infloat，作为开源候选者。我们还将使用Qdrant的[**fast-embed**](https://qdrant.tech/articles/fastembed/)框架来提高嵌入创建的效率，主要通过量化权重并使用ONNX作为运行时。'
- en: 'We can generate these embeddings with llama-index by importing the corresponding
    integration. For instance, we import the module `FastEmbedEmbedding` to generate
    the `e5-large` embeddings with the`fast-embed` framework:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过导入相应的集成来使用llama-index生成这些嵌入。例如，我们导入`FastEmbedEmbedding`模块，使用`fast-embed`框架生成`e5-large`嵌入：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The full embeddings definition can be found in `retriever/embeddings.py`
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的嵌入定义可以在`retriever/embeddings.py`中找到。
- en: The metric that we will be measuring is the context relevancy, i.e the relevancy
    of the retrieved context relative to the user query.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将衡量的指标是上下文相关性，即检索到的上下文与用户查询的相关性。
- en: Here is the code to launch the evaluations for this experiment.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是启动本次实验评估的代码。
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Now here is the final table of results. The `embedding_time` field represents
    the time it took us to embed the full `~2800` articles of the civil code.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这是最终的结果表格。`embedding_time`字段表示我们将约`2800`篇《民法典》文章进行嵌入所花费的时间。
- en: We observe that `text-embedding-data-002` outperforms the other two embedding
    models in both score and embedding time. The gap between `mistral-embed` and `ada`
    from one side, and the `multilingual-e5-large` from the other, is quite significant.
    Note that the ranking of these embedding methods aligns with the leaderboard mentioned
    earlier.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，`text-embedding-data-002`在得分和嵌入时间上都优于其他两个嵌入模型。`mistral-embed`和`ada`与`multilingual-e5-large`之间的差距相当显著。请注意，这些嵌入方法的排名与之前提到的排行榜一致。
- en: However, bear in mind that our evaluation is based on only 50 questions generated
    by an LLM. Thus, it may not fully represent performance in a real-world scenario,
    especially when differentiating between mistral-embed and ada, where the gap is
    relatively small.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请记住，我们的评估仅基于由LLM生成的50个问题。因此，它可能无法完全代表真实场景中的表现，尤其是在区分`mistral-embed`和`ada`时，因为这两者之间的差距相对较小。
- en: Evaluating advanced features
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估高级功能
- en: 1- Windowing
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1- 窗口化
- en: The first advanced feature we’ll explore is augmenting node content with the
    content from preceding and succeeding nodes. This makes sense within the context
    of law, where neighboring articles are closely related.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索的第一个高级功能是通过前后相邻节点的内容来增强节点内容。在法律领域中，这种做法是有意义的，因为相邻的条款通常紧密相关。
- en: We’ll use `k` to parameterize windowing, representing the number of nodes before
    and after to add to the current node. For example, `k=1` means that the current
    node content will be augmented with the previous and next nodes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`k`来参数化窗口化，表示当前节点前后需要添加的节点数量。例如，`k=1`意味着当前节点的内容将与前后节点一起增强。
- en: For a given node, the original content will be used for embedding, while the
    augmented content will create the context for the response generation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的节点，将使用原始内容进行嵌入，而增强后的内容将用于生成响应的上下文。
- en: 'Here’s how we perform node windowing:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何进行节点窗口化的：
- en: We create a custom function to generate the augmented nodes. Although Llama-index
    has its `SentenceWindowNodeParser` class for this task, it requires whole documents
    as inputs and then performs splitting to create nodes. So we took inspiration
    from this class and created our own window parser.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个自定义函数来生成增强后的节点。尽管Llama-index有其`SentenceWindowNodeParser`类来执行此任务，但它要求输入的是完整文档，然后进行分割以创建节点。因此，我们从这个类获得灵感，创建了自己的窗口解析器。
- en: '[PRE20]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we modify the previously created `CodeNodes` class to add the possibility
    of using neighbouring nodes for augmentation:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们修改之前创建的`CodeNodes`类，增加使用相邻节点进行增强的功能：
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A key point to note is the `MetadataReplacementPostProcessor`. This class is
    used to signal that we need to replace the node content with the content found
    in the `target_metadata_key` field, right before passing the retrieved data to
    the LLM. Hence this field will contain the augmented content.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个关键点是`MetadataReplacementPostProcessor`。这个类用于指示我们需要在将检索到的数据传递给LLM之前，将节点内容替换为在`target_metadata_key`字段中找到的内容。因此，这个字段将包含增强后的内容。
- en: 'Lastly, we need to incorporate the post-processors into the query engine:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们需要将后处理器集成到查询引擎中：
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In this experiment, we will compare the base RAG pipeline to two other pipelines.
    These additional pipelines use node windowing with `k=1` and `k=2` respectively.
    We will use the same set of questions and context relevancy metrics to evaluate
    these pipelines. Here are the results:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在此实验中，我们将比较基本 RAG 管道与另外两个管道。这些额外的管道分别使用 `k=1` 和 `k=2` 的节点窗口。我们将使用相同的一组问题和上下文相关性指标来评估这些管道。以下是结果：
- en: We can observe that augmenting node content with neighbouring nodes can improve
    the context relevancy score. The small margin of improvement is probably due to
    an already high score (0.89).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，通过增加邻近节点的内容来增强节点的上下文相关性得分。小幅的提升可能是由于已经很高的得分（0.89）。
- en: We will also measure the **faithfulness** for this experiment. In llama-index,
    the `FaithfulnessEvaluator` module determines if a query engine’s response matches
    any source nodes, indicating whether the response was hallucinated or not. It’s
    important to note that this evaluator provides a binary score (0 if the response
    is a hallucination, otherwise 1). This is in contrast to the **context relevancy**
    score, which can range between 0 and 1.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将衡量本次实验的**忠实度**。在 llama-index 中，`FaithfulnessEvaluator` 模块用于判断查询引擎的回答是否与任何源节点匹配，从而确定回答是否为幻觉。需要注意的是，该评估器提供一个二元得分（如果回答是幻觉则为
    0，否则为 1）。这与**上下文相关性**得分不同，后者的范围是 0 到 1。
- en: 'Here are the results:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果：
- en: In our evaluation of 50 questions, we found that only one response was a hallucination
    in the base experiment, while there were five hallucinations in the windowing
    pipeline with `k=2`. This suggests that adding more context, especially irrelevant
    context, can induce more hallucinations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对 50 个问题的评估中，我们发现基本实验中只有一个回答是幻觉，而在使用`k=2`的窗口管道中有五个幻觉。这表明，添加更多上下文，特别是无关上下文，可能会诱发更多幻觉。
- en: In subsequent experiments, we will maintain the base setup without node augmentation.
    This decision is guided by our observation that the slight increase in relevance
    does not offset the higher rate of hallucinations in responses.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的实验中，我们将保持基本设置，不进行节点扩展。这个决定基于我们的观察，即相关性的小幅增加并不足以抵消回答中的更高幻觉率。
- en: 2- Hybrid search
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2- 混合搜索
- en: '[Hybrid search](https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/)
    refers to combining search results from `sparse` and `dense` vectors.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[混合搜索](https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/)是指结合`稀疏`和`稠密`向量的搜索结果。'
- en: '`Dense vectors` represent data points as compact vectors of continuous values,
    offering a rich, nuanced understanding of the data’s features. These vectors,
    often result from deep learning models such as OpenAI, Mistral, e5-large, etc…
    So far, we have used only dense vectors during the retrieval phase.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`稠密向量`将数据点表示为连续值的紧凑向量，提供对数据特征的丰富且微妙的理解。这些向量通常是通过深度学习模型（如 OpenAI、Mistral、e5-large
    等）生成的……到目前为止，我们在检索阶段仅使用了稠密向量。'
- en: '`Sparse vectors`, in contrast, are primarily zeros and are produced through
    specialized models such as TF-IDF, BM25, etc. They excel at identifying specific
    keywords and minor details, differentiating them from the more semantically rich
    dense vectors.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，`稀疏向量`主要是零，通过诸如 TF-IDF、BM25 等专业模型生成。它们擅长识别特定关键词和细节，使其与更具语义丰富性的稠密向量有所不同。
- en: 'To implement hybrid search in our RAG pipeline, we need to modify some parts
    of the code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的 RAG 管道中实现混合搜索，我们需要修改代码的某些部分：
- en: First we need to enable hybrid search in our vector database by setting `enable_hybrid=True`
    in the Qdrant client definition. This will run sparse vector generation locally
    using the `"naver/efficient-splade-VI-BT-large-doc"` model from Huggingface, in
    addition to generating the usual dense vectors with OpenAI’s ada model.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们需要通过在 Qdrant 客户端定义中设置`enable_hybrid=True`来启用混合搜索。这将使用 Huggingface 中的 `"naver/efficient-splade-VI-BT-large-doc"`
    模型在本地生成稀疏向量，同时使用 OpenAI 的 ada 模型生成常规的稠密向量。
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When defining the query engine in `query_engine = index.as_query_engine(**kwargs)`,
    we need to define `sparse_top_k` along with `similarity_top_k` . `sparse_top_k`
    represents how many nodes will be retrieved from each dense and sparse query.
    For example, if `sparse_top_k=5` is set, that means we will retrieve 5 nodes using
    sparse vectors and 5 nodes using dense vectors. `similarity_top_k` controls the
    final number of returned nodes. In the above setting, we end up with 10 nodes.
    A fusion algorithm is then applied to rank and order the nodes from different
    vector spaces ([relative score fusion](https://weaviate.io/blog/hybrid-search-fusion-algorithms#relative-score-fusion)
    in this case). `similarity_top_k=5` means the top five nodes after fusion are
    returned. Here is the new definition of the `query_engine` after adding these
    options:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在定义`query_engine = index.as_query_engine(**kwargs)`时，我们需要同时定义`sparse_top_k`和`similarity_top_k`。`sparse_top_k`表示每个稀疏和密集查询将检索多少个节点。例如，设置`sparse_top_k=5`意味着我们将使用稀疏向量检索5个节点，使用密集向量检索5个节点。`similarity_top_k`控制返回节点的最终数量。在上述设置中，我们最终会得到10个节点。接着，应用一种融合算法来对来自不同向量空间的节点进行排序和排列（此处使用的是[相对评分融合](https://weaviate.io/blog/hybrid-search-fusion-algorithms#relative-score-fusion)）。`similarity_top_k=5`意味着返回融合后的前五个节点。以下是添加这些选项后的`query_engine`的新定义：
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can adjust the weight for both vector and keyword searches by varying an
    `alpha` parameter. An `alpha` equal to 1 corresponds to a pure vector search,
    while an `alpha` equal to 0 corresponds to a pure keyword search. Below is the
    final definition of the `query_engine` after incorporating this alpha parameter:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过调整`alpha`参数来调整向量搜索和关键词搜索的权重。`alpha`等于1表示纯向量搜索，而`alpha`等于0表示纯关键词搜索。以下是结合此`alpha`参数后的`query_engine`最终定义：
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To assess the relevance of hybrid search, we conducted three experiments with
    varying `alpha` parameters: `alpha = 0.2, 0.5, 0.8`. Note that the base experiment
    corresponds to `alpha=1`, representing a pure vector search.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估混合搜索的相关性，我们进行了三次不同`alpha`参数的实验：`alpha = 0.2, 0.5, 0.8`。请注意，基础实验对应`alpha=1`，表示纯向量搜索。
- en: Here are the final results for the context relevancy and faithfulness metrics.
    Please note that embedding the `~2800` nodes with the `"naver/efficient-splade-VI-BT-large-doc"`
    model took us about `27 minutes` on an M1 Mac without a GPU, and it used almost
    all the system memory. Therefore, you might need a GPU to speed up the embedding
    process.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于上下文相关性和忠实度指标的最终结果。请注意，使用`"naver/efficient-splade-VI-BT-large-doc"`模型嵌入`~2800`个节点大约花费了我们`27分钟`，并且该过程在没有GPU的M1
    Mac上进行，几乎占用了所有系统内存。因此，您可能需要GPU来加速嵌入过程。
- en: The hybrid-search with `alpha=0.8` shows a slight improvement in the context
    relevancy score, but putting more weight on keyword search decreases the scores.
    The faithfulness scores are overall preserved.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`alpha=0.8`的混合搜索在上下文相关性评分上略有提升，但将更多权重放在关键词搜索上会降低评分。忠实度评分总体保持不变。'
- en: Remember that the evaluation questions are quite general and lean more towards
    semantic search than exact-match search. However, in real-world scenarios, a user
    might ask for a specific article from the code, such as “what does the article
    x.y discuss?” In such cases, a keyword search can be helpful. Thus, we’ll retain
    the hybrid-search for future experiments with `alpha=0.8`, despite an additional
    hallucination case compared to the pure vector search case.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，评估问题相对一般，更偏向语义搜索而非精确匹配搜索。然而，在现实场景中，用户可能会要求查找代码中的特定文章，例如“文章x.y讨论了什么？”在这种情况下，关键词搜索可能会有所帮助。因此，尽管与纯向量搜索相比增加了额外的幻觉案例，我们仍将保留`alpha=0.8`的混合搜索供未来实验使用。
- en: 3- Query rewriting
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3- 查询重写
- en: '[Query rewriting](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/)
    involves generating various questions similar to a specific query. This process
    can be used for disambiguation, error correction, or adapting the query to a specific
    knowledge base that supports the RAG system.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[查询重写](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/)涉及生成与特定查询相似的各种问题。此过程可用于消歧义、错误修正或将查询适应于支持RAG系统的特定知识库。'
- en: We will use the `QueryFusionRetriever` from llama-index for query rewriting.
    This module generates similar queries to the user query, retrieves and reranks
    the top `n` nodes from each generated query, including the original one, using
    the `Reciprocal Rerank Fusion` algorithm. This method, detailed in this [paper](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf),
    offers an efficient way to rerank retrieved nodes without excessive computation
    or dependence on external models.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自llama-index的`QueryFusionRetriever`进行查询重写。这个模块生成与用户查询相似的查询，从每个生成的查询中检索并重新排序前`n`个节点，包括原始查询，使用`Reciprocal
    Rerank Fusion`算法。该方法在这篇[论文](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)中有详细介绍，提供了一种高效的方式来重新排序检索到的节点，而不会产生过多的计算或依赖外部模型。
- en: 'Below is the Python code for this task (located in `query/query_engine.py`):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此任务的Python代码（位于`query/query_engine.py`）：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`num_queries` represents the total number of queries : `num_queries-1` generated
    queries, plus `1` for the original query. For subsequent experiments, we set `num_queries=4`.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_queries`表示总查询数：`num_queries-1`个生成的查询，加上`1`个原始查询。对于后续实验，我们设置`num_queries=4`。'
- en: '`QUERY_GEN_PROMPT` is a prompt specifically designed for generating similar
    queries. We create a custom version of this prompt for the civil code knowledge
    base.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QUERY_GEN_PROMPT`是专门用于生成类似查询的提示。我们为民法典知识库创建了这个提示的自定义版本。'
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Up until now, the evaluation questions we used were already clean and tailored
    for the civil code, as we instructed GPT-4 to generate them this way. However,
    as we mentioned earlier, this may not accurately represent the real-world users,
    who may ask ambiguous, error-prone, and short questions. Therefore, we generated
    another `50` questions using GPT-4, instructing the LLM to generate questions
    with these more realistic characteristics. Here are some examples:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在，我们使用的评估问题已经很干净，并且针对民法典进行了定制，因为我们指示GPT-4按照这种方式生成它们。然而，正如我们之前提到的，这可能无法准确代表现实中的用户，因为他们可能会提出模糊、容易出错或简短的问题。因此，我们使用GPT-4生成了另外`50`个问题，并指示LLM生成具有这些更现实特征的问题。以下是一些例子：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Or the english translation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 或者是英文翻译：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will use this new evaluation dataset to compare the performance of pipelines
    with and without query rewriting. The comparison will be based on two metrics:
    **Faithfulness** and **Answer Relevancy**. Similar to context relevancy, answer
    relevancy measures the relevance of the answer to the user query, providing a
    score between 0 and 1\. The results are as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个新的评估数据集来比较带有和不带有查询重写的管道的表现。比较将基于两个指标：**准确性**和**答案相关性**。类似于上下文相关性，答案相关性衡量答案与用户查询的相关性，得分范围从0到1。结果如下：
- en: We observe that query rewriting improves not only faithfulness, where the model
    doesn’t hallucinate at all in the 50 questions, but also slightly boosts the answer
    relevancy score.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，查询重写不仅提高了准确性，模型在50个问题中完全没有虚构内容，而且稍微提高了答案相关性得分。
- en: 'Here is an example of how an original question was rewritten into three other
    questions:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，展示了原始问题如何被重写成另外三个问题：
- en: 'Original question: `Can I refuse an inheritance?`'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始问题：`我可以拒绝继承吗？`
- en: 'The three generated queries:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个生成的查询：
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Routing
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 路由
- en: 'Until now, our knowledge base includes only one legal code for each RAG pipeline.
    Naturally, one might wonder how to add more legal codes, such as the penal code
    or traffic laws code, and how to construct a system that retrieves data from the
    correct legal code(s) when given a query. We can accomplish this in at least three
    ways with llama-index:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的知识库中每个RAG管道只包含一个合法代码。自然地，人们可能会想，如何添加更多的合法代码，例如刑法或交通法规，以及如何构建一个系统，当给定查询时，能够从正确的法律代码中检索数据。我们可以通过至少三种方式使用llama-index来实现这一目标：
- en: Store all data from the different legal codes in the same index and build the
    query engine just like before.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将不同法律代码中的所有数据存储在同一个索引中，并像以前一样构建查询引擎。
- en: Store all data in the same index but add the legal code name as metadata. During
    a query, the llama-index `[Auto-retriever](https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_auto_retriever/)`
    module infers a set of metadata filters and the appropriate query string to pass
    to the vector database. The `auto-retriever` must determine the correct code name
    and then execute a similarity search on the corresponding nodes to retrieve the
    data.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有数据存储在同一个索引中，但将法律代码名称作为元数据。在查询时，llama-index的`[Auto-retriever](https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_auto_retriever/)`模块推断出一组元数据过滤器和适当的查询字符串，以传递给向量数据库。`auto-retriever`必须确定正确的代码名称，然后在相应的节点上执行相似度搜索以检索数据。
- en: Store every Legal Code nodes in a separate index, and use llama-index `Routing`
    feature to select the most relevant index or indices.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个法律代码节点存储在单独的索引中，并使用llama-index的`Routing`功能选择最相关的索引或多个索引。
- en: In this project, we opted for the last option. A more effective approach would
    be to evaluate these three options and select the best one. However we did not
    perform this evaluation due to time and space constraints, as this article is
    already extensive.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们选择了最后一个选项。更有效的方法是评估这三种选择并选出最佳方案。然而，由于时间和空间的限制，我们没有进行这个评估，因为这篇文章已经相当详细。
- en: '[Routers](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)
    are modules that take in a query and a set of **choices** and return one or more
    selected choices.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[路由器](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)是接受查询和一组**选择**并返回一个或多个选择的模块。'
- en: The selection process is conducted by an LLM. In our use case, the selector
    receives the legal code descriptions as input and returns one or more query engines.
    Each engine represents a RAG system for a single code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 选择过程由LLM执行。在我们的用例中，选择器接收法律代码描述作为输入，并返回一个或多个查询引擎。每个引擎代表一个单一代码的RAG系统。
- en: 'Here is how we experiment routing in this project:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在这个项目中进行路由实验的方式：
- en: We first define the list of law Codes to consider and their corresponding descriptions.
    The codes that we used for the routing experiment are the civil code, the taxes
    code, the intellectual property code, the traffic laws code and the labor code.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先定义了要考虑的法律代码列表及其对应的描述。我们在路由实验中使用的代码包括民法典、税法典、知识产权法典、交通法规法典和劳动法典。
- en: '[PRE31]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We instantiate the `query_engine` for each code, as previously done. Then we
    wrap every `query_engine` in a `QueryEngineTool` module along with its description.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为每个代码实例化了`query_engine`，如前所述。然后，我们将每个`query_engine`与其描述一起包装在`QueryEngineTool`模块中。
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Finally we create the Router query engine using the list of tools and the llama-index
    `LLMMultiSelector` module. LLM selectors put the choices as a text dump into a
    prompt and use LLM text completion endpoint to make decisions.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用工具列表和llama-index的`LLMMultiSelector`模块创建了路由查询引擎。LLM选择器将选项作为文本转储放入提示中，并使用LLM文本完成端点来做出决策。
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note that to keep things simple, we did not include any advanced features in
    the individual codes query engines.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了简化，我们没有在单个代码查询引擎中包含任何高级功能。
- en: 'Now, let’s test the router query engine on a query with two themes: labor code
    and tax code. This would allow us to see whether or not the router selects the
    correct indexes.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在包含两个主题的查询上测试路由查询引擎：劳动法和税法。这将使我们看到路由器是否选择了正确的索引。
- en: 'Query:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：
- en: '[PRE34]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Or its english translation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 或者它的英文翻译：
- en: '[PRE35]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we can check the response given by the router query engine:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查路由查询引擎给出的响应：
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The important result here is the `selector_result`, which shows that the multi-selector
    correctly identified the labor code (index 4) and the tax code (index 1) as the
    relevant data sources for answering the user query.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的重要结果是`selector_result`，它显示多选择器正确地识别了劳动法（索引4）和税法（索引1）作为回答用户查询的相关数据源。
- en: You can find all the evaluation experiments plus the routing query engine definition
    in the notebook `./notebooks/evaluate_with_llamaindex.ipynb` .
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在笔记本`./notebooks/evaluate_with_llamaindex.ipynb`中找到所有的评估实验和路由查询引擎定义。
- en: Conclusion
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we discussed the transition from a basic RAG system to an advanced
    one, incorporating windowing, hybrid-search, and query rewriting, using the `llama-index`
    framework. We also explored how to evaluate certain components of the RAG pipeline,
    like the embeddings and the previously mentioned features, enabling informed decisions
    about their relevance, rather than relying on intuition.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了如何从一个基础的RAG系统过渡到一个高级系统，采用窗口化、混合搜索和查询重写，并使用`llama-index`框架。我们还探讨了如何评估RAG管道的某些组件，如嵌入和前述功能，从而做出关于其相关性的知情决策，而不是仅凭直觉。
- en: We also looked at how routing allows us to select relevant indexes using LLM
    decision making.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了路由如何通过LLM决策使我们能够选择相关的索引。
- en: 'Here are some suggestions to further enhance the pipeline:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些进一步优化管道的建议：
- en: Throughout this project, we used `gpt-3.5-turbo` as the LLM for generating responses,
    due to its ease of use and low cost. Similar to how we assessed various components
    of the RAG system, we could evaluate the LLM based on metrics like answer relevance
    and faithfulness.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个项目中，我们使用了`gpt-3.5-turbo`作为LLM来生成响应，因为它易于使用且成本较低。与我们评估RAG系统的各个组件类似，我们也可以根据如答案相关性和准确性等指标来评估LLM。
- en: The same comment applies to using `gpt-3.5-turbo` as an evaluator. Employing
    a more capable Language Model might enhance the evaluation accuracy.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样的评论适用于将`gpt-3.5-turbo`作为评估器。使用更强大的语言模型可能会提高评估的准确性。
- en: Ideally, we’d assess all combinations of the RAG pipeline using various metrics.
    However, this might be costly and potentially unfeasible.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想情况下，我们会使用各种指标评估RAG管道的所有组合。然而，这可能代价高昂，并且可能不可行。
- en: It might also be worthwhile to consider evaluating other advanced techniques
    in our system, such as the use of external models for reranking, using agents,
    etc.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也许值得考虑在我们的系统中评估其他先进技术，例如使用外部模型进行重排序、使用代理等。
- en: 'We can also enhance the multilingual support of the RAG pipeline. One approach
    could be to set up a chain of LLMs to perform the following tasks: First, translating
    the query from the original language to French, then generating the response as
    usual, and finally, translating the response back to the original language.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以增强RAG管道的多语言支持。一个方法是建立一个LLM链，执行以下任务：首先将查询从原始语言翻译成法语，然后像往常一样生成响应，最后将响应翻译回原始语言。
- en: In conclusion, readers looking to integrate a similar RAG system for legal queries
    should proceed cautiously due to the complexity and potential impact of legal
    advice. The risk of such systems generating **hallucinations** is a concern. Our
    evaluation, despite having high faithfulness scores, is limited by a small number
    and a limited scope of the questions, along with the inherent constraints of the
    evaluator module.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，考虑集成类似RAG系统来处理法律查询的读者应该谨慎行事，因为法律建议的复杂性和潜在影响。此类系统生成**虚假信息**的风险是一个需要关注的问题。尽管我们的评估在忠实度方面得分较高，但由于问题数量较少，且问题范围有限，加上评估模块本身的局限性，评估结果仍然存在不足。
- en: To address this, we recommend adding additional layers of evaluation and safety
    protocols. Moreover, despite the project showcasing the capabilities of advanced
    RAG settings for production, it is not yet a finalized system. This underscores
    the importance of continuous refinement and thorough testing before full-scale
    deployment. Finally, users should also be reminded that the system’s advice does
    not replace professional legal counsel.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一问题，我们建议增加额外的评估层次和安全协议。此外，尽管该项目展示了高级RAG设置在生产中的能力，但它还不是一个最终定型的系统。这凸显了在全面部署之前进行持续完善和彻底测试的重要性。最后，用户还应当被提醒，系统的建议并不能替代专业的法律咨询。
- en: References
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[llama-index high level concepts.](https://docs.llamaindex.ai/en/stable/getting_started/concepts/)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama-index高级概念](https://docs.llamaindex.ai/en/stable/getting_started/concepts/)'
- en: '[llama-index evaluation](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation/)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama-index评估](https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation/)'
- en: '[Qdrant hybrid search.](https://docs.llamaindex.ai/en/v0.10.20/examples/vector_stores/qdrant_hybrid.html)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Qdrant混合搜索](https://docs.llamaindex.ai/en/v0.10.20/examples/vector_stores/qdrant_hybrid.html)'
- en: '[Qdrant fast-embed](https://qdrant.tech/articles/fastembed/)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Qdrant快速嵌入](https://qdrant.tech/articles/fastembed/)'
- en: '[llama-index routers](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[llama-index路由器](https://docs.llamaindex.ai/en/stable/module_guides/querying/router/)'
- en: '[Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning
    Methods](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) Paper'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[互惠排名融合方法优于康多塞方法和单独的排名学习方法](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)
    论文'
- en: To reach out
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LinkedIn : [https://www.linkedin.com/in/hamza-gharbi-043045151/](https://www.linkedin.com/in/hamza-gharbi-043045151/)'
- en: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Twitter : [https://twitter.com/HamzaGh25079790](https://twitter.com/HamzaGh25079790)'
