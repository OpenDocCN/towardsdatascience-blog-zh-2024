<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Florence-2: Advancing Multiple Vision Tasks with a Single VLM Model</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14">https://towardsdatascience.com/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?source=collection_archive---------2-----------------------#2024-10-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="de8a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A Guided Exploration of Florence-2's Zero-Shot Capabilities: Captioning, Object Detection, Segmentation and OCR.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lihi Gur Arie, PhD" class="l ep by dd de cx" src="../Images/7a1eb30725a95159401c3672fa5f43ab.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M0YTyQAxsWWqI8MLBi2xrA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lihigurarie?source=post_page---byline--435d251976d0--------------------------------" rel="noopener follow">Lihi Gur Arie, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--435d251976d0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/d06dac442af9954dd35b05fe0f06a790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4RuOxbUk_cy_1o3jZ5qxg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image annotations by Author. Original image from <a class="af nc" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">Pexels</a>.</figcaption></figure><h2 id="60cd" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">Introduction</h2><p id="84e0" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">In recent years, the field of computer vision has witnessed the rise of foundation models that enable image annotation without the need for training custom models. We’ve seen models like <a class="af nc" rel="noopener" target="_blank" href="/clip-creating-image-classifiers-without-data-b21c72b741fa?sk=88fdd2c1a132538015968df3f49b64b1">CLIP</a> [2] for classification, <a class="af nc" rel="noopener" target="_blank" href="/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e">GroundingDINO</a> [3] for object detection, and SAM [4] for segmentation — each excelling in its domain. But what if we had a single model capable of handling all these tasks together?</p><blockquote class="ou ov ow"><p id="6d8a" class="ob oc ox od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">If you don’t have a paid Medium account, you can read for free <a class="af nc" rel="noopener" target="_blank" href="/florence-2-mastering-multiple-vision-tasks-with-a-single-vlm-model-435d251976d0?sk=e25bdee736a9aa9ace1ca80b98a036a4">here</a>.</p></blockquote><p id="a42c" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">In this tutorial we introduce Florence-2 [1]— a novel, open-source Vision-Language Model (VLM) designed to handle a diverse range of vision and multimodal tasks, including captioning, object detection, segmentation and OCR.</p><p id="3e1b" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Accompanied by a Colab notebook, we’ll explore Florence-2’s zero-shot capabilities to annotate an image of an old camera.</p><h1 id="279a" class="pd ne fq bf nf pe pf gq nj pg ph gt nn pi pj pk pl pm pn po pp pq pr ps pt pu bk">Florence-2</h1><p id="f1b5" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr">Background</strong></p><p id="3041" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 was released by Microsoft in June 2024. It was designed to perform multiple vision tasks within a single model. It is an open-source model, available on <a class="af nc" href="https://huggingface.co/microsoft/Florence-2-large" rel="noopener ugc nofollow" target="_blank">Hugging Face</a> under the permissive MIT licence.</p><p id="f63b" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Despite its relatively small size, with versions of 0.23B &amp; 0.77B parameters, Florence-2 achieves state-of-the-art (SOTA) performance. Its compact size enables efficient deployment on devices with limited computing resources, while ensuring fast inference speeds.</p><p id="c2e4" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">The model was pre-trained on an enormous, high quality dataset called FLD-5B, consisting of 5.4B annotations on 126 million images. This allows Florence-2 to excel in zero-shot performance on many tasks without requiring additional training.</p><p id="755c" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">The original open-source weights of the Florence-2 model support the following tasks:</p><figure class="mm mn mo mp mq mr"><div class="pv io l ed"><div class="pw px l"/></div></figure><p id="aeb8" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Additional unsupported tasks can be added by fine-tuning the model.</p><p id="5e90" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">Task Format</strong></p><p id="2d57" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Inspired by Large Language Models (LLMs), Florence-2 was designed as a sequence-to-sequence model. It takes an image and text instructions as inputs, and outputs text results. The input or output text may represent plain text or a region in the image. The region format varies depending on the task:</p><ul class=""><li id="1c9c" class="ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot py pz qa bk"><strong class="od fr">Bounding Boxes: </strong><code class="cx qb qc qd qe b">'&lt;X1&gt;&lt;Y1&gt;&lt;X2&gt;&lt;Y2&gt;’</code> for object detection tasks. The tokens represent the coordinates of the top-left and bottom-right corners of the box.</li><li id="bde5" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><strong class="od fr">Quad Boxes</strong>: <code class="cx qb qc qd qe b">'&lt;X1&gt;&lt;Y1&gt;&lt;X2&gt;&lt;Y2&gt;&lt;X3&gt;&lt;Y3&gt;&lt;X4&gt;&lt;Y4&gt;’</code> for text detection, using the coordinates of the four corners that enclose the text.</li><li id="13a2" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><strong class="od fr">Polygon</strong>: <code class="cx qb qc qd qe b">'&lt;X1&gt;&lt;Y1&gt;...,&lt;Xn&gt;&lt;Yn&gt;’</code> for segmentation tasks, where the coordinates represent the vertices of the polygon in clockwise order.</li></ul><p id="aa2d" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">Architecture</strong></p><p id="c811" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 is built using a standard encoder-decoder transformer architecture. Here’s how the process works:</p><ol class=""><li id="a8a7" class="ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot qk pz qa bk">The input image is embedded by a DaViT vision encoder [5].</li><li id="8835" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot qk pz qa bk">The text prompt is embedded using BART [6], utilizing an extended tokenizer and word embedding layer.</li><li id="2b7c" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot qk pz qa bk">Both the vision and text embeddings are concatenated.</li><li id="fdf8" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot qk pz qa bk">These concatenated embeddings are processed by a transformer-based multi-modal encoder-decoder to generate the response.</li><li id="02a8" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot qk pz qa bk">During training, the model minimizes the cross-entropy loss, similar to standard language models.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ql"><img src="../Images/83b1b171555b8ba0c1e9513a11913d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgN5ojBuiXe1GQyfWkHwRQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">An illustration of Florence-2's architecture. Source: <a class="af nc" href="https://arxiv.org/abs/2311.06242" rel="noopener ugc nofollow" target="_blank">link</a><em class="qm">.</em></figcaption></figure></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="166a" class="pd ne fq bf nf pe qv gq nj pg qw gt nn pi qx pk pl pm qy po pp pq qz ps pt pu bk">Code implementation</h1><p id="d27f" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr">Loading Florence-2 model and a sample image</strong></p><p id="6341" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">After installing and importing the necessary libraries (as demonstrated in the accompanying Colab notebook), we begin by loading the Florence-2 model, processor and the input image of a camera:</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="28ae" class="rd ne fq qe b bg re rf l rg rh">#Load model:<br/>model_id = ‘microsoft/Florence-2-large’<br/>model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype='auto').eval().cuda()<br/>processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)<br/><br/>#Load image:<br/>image = Image.open(img_path)</span></pre><p id="01d6" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">Auxiliary Functions</strong></p><p id="0aa7" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">In this tutorial, we will use several auxiliary functions. The most important is the <code class="cx qb qc qd qe b">run_example</code> core function, which generates a response from the Florence-2 model.</p><p id="9c18" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">The <code class="cx qb qc qd qe b">run_example</code> function combines the task prompt with any additional text input (if provided) into a single prompt. Using the <code class="cx qb qc qd qe b">processor</code>, it generates text and image embeddings that serve as inputs to the model. The magic happens during the <code class="cx qb qc qd qe b">model.generate</code> step, where the model’s response is generated. Here’s a breakdown of some key parameters:</p><ul class=""><li id="abf9" class="ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot py pz qa bk"><strong class="od fr">max_new_tokens=1024</strong>: Sets the maximum length of the output, allowing for detailed responses.</li><li id="5825" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><strong class="od fr">do_sample=False</strong>: Ensures a deterministic response.</li><li id="5abc" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><strong class="od fr">num_beams=3</strong>: Implements beam search with the top 3 most likely tokens at each step, exploring multiple potential sequences to find the best overall output.</li><li id="896d" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><strong class="od fr">early_stopping=False</strong>: Ensures beam search continues until all beams reach the maximum length or an end-of-sequence token is generated.</li></ul><p id="ca8b" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Lastly, the model’s output is decoded and post-processed with <code class="cx qb qc qd qe b">processor.batch_decode</code> and <code class="cx qb qc qd qe b">processor.post_process_generation</code> to produce the final text response, which is returned by the <code class="cx qb qc qd qe b">run_example</code> function.</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="0a3b" class="rd ne fq qe b bg re rf l rg rh">def run_example(image, task_prompt, text_input=''):<br/><br/>    prompt = task_prompt + text_input<br/><br/>    inputs = processor(text=prompt, images=image, return_tensors=”pt”).to(‘cuda’, torch.float16)<br/><br/>    generated_ids = model.generate(<br/>        input_ids=inputs[“input_ids”].cuda(),<br/>        pixel_values=inputs[“pixel_values”].cuda(),<br/>        max_new_tokens=1024,<br/>        do_sample=False,<br/>        num_beams=3,<br/>        early_stopping=False,<br/>    )<br/><br/>    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]<br/>    parsed_answer = processor.post_process_generation(<br/>        generated_text,<br/>        task=task_prompt,<br/>        image_size=(image.width, image.height)<br/>    )<br/><br/>    return parsed_answer</span></pre><p id="f2e1" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Additionally, we utilize auxiliary functions to visualize the results (<code class="cx qb qc qd qe b">draw_bbox</code> ,<code class="cx qb qc qd qe b">draw_ocr_bboxes</code> and <code class="cx qb qc qd qe b">draw_polygon</code>) and handle the conversion between bounding boxes formats (<code class="cx qb qc qd qe b">convert_bbox_to_florence-2</code> and <code class="cx qb qc qd qe b">convert_florence-2_to_bbox</code>). These can be explored in the attached Colab notebook.</p><h1 id="e594" class="pd ne fq bf nf pe pf gq nj pg ph gt nn pi pj pk pl pm pn po pp pq pr ps pt pu bk">Tasks</h1><p id="ac94" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Florence-2 can perform a variety of visual tasks. Let’s explore some of its capabilities, starting with image captioning.</p><h2 id="bace" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk"><strong class="al"><em class="qm">1. Captioning Generation Related Tasks:</em></strong></h2><p id="261c" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr">1.1 Generate Captions</strong></p><p id="145c" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 can generate image captions at various levels of detail, using the <code class="cx qb qc qd qe b">'&lt;CAPTION&gt;'</code> , <code class="cx qb qc qd qe b">'&lt;DETAILED_CAPTION&gt;'</code> or <code class="cx qb qc qd qe b">'&lt;MORE_DETAILED_CAPTION&gt;'</code> task prompts.</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="95f1" class="rd ne fq qe b bg re rf l rg rh">print (run_example(image, task_prompt='&lt;CAPTION&gt;'))<br/># Output: 'A black camera sitting on top of a wooden table.'<br/><br/>print (run_example(image, task_prompt='&lt;DETAILED_CAPTION&gt;'))<br/># Output: 'The image shows a black Kodak V35 35mm film camera sitting on top of a wooden table with a blurred background.'<br/><br/>print (run_example(image, task_prompt='&lt;MORE_DETAILED_CAPTION&gt;'))<br/># Output: 'The image is a close-up of a Kodak VR35 digital camera. The camera is black in color and has the Kodak logo on the top left corner. The body of the camera is made of wood and has a textured grip for easy handling. The lens is in the center of the body and is surrounded by a gold-colored ring. On the top right corner, there is a small LCD screen and a flash. The background is blurred, but it appears to be a wooded area with trees and greenery.'</span></pre><p id="a620" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">The model accurately describes the image and its surrounding. It even identifies the camera’s brand and model, demonstrating its OCR ability. However, in the <code class="cx qb qc qd qe b">'&lt;MORE_DETAILED_CAPTION&gt;'</code> task there are minor inconsistencies, which is expected from a zero-shot model.</p><p id="c7b1" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">1.2 Generate Caption for a Given Bounding Box</strong></p><p id="1107" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 can generate captions for specific regions of an image defined by bounding boxes. For this, it takes the bounding box location as input. You can extract the category with <code class="cx qb qc qd qe b">'&lt;REGION_TO_CATEGORY&gt;'</code> or a description with <code class="cx qb qc qd qe b">'&lt;REGION_TO_DESCRIPTION&gt;'</code> .</p><p id="d1b5" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">For your convenience, I added a widget to the Colab notebook that enables you to draw a bounding box on the image, and code to convert it to Florence-2 format.</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="1d50" class="rd ne fq qe b bg re rf l rg rh">task_prompt = '&lt;REGION_TO_CATEGORY&gt;'<br/>box_str = '&lt;loc_335&gt;&lt;loc_412&gt;&lt;loc_653&gt;&lt;loc_832&gt;'<br/>results = run_example(image, task_prompt, text_input=box_str)<br/># Output: 'camera lens'</span></pre><pre class="ri ra qe rb bp rc bb bk"><span id="f08a" class="rd ne fq qe b bg re rf l rg rh">task_prompt = '&lt;REGION_TO_DESCRIPTION&gt;'<br/>box_str = '&lt;loc_335&gt;&lt;loc_412&gt;&lt;loc_653&gt;&lt;loc_832&gt;'<br/>results = run_example(image, task_prompt, text_input=box_str)<br/># Output: 'camera'</span></pre><p id="0d64" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">In this case, the <code class="cx qb qc qd qe b">'&lt;REGION_TO_CATEGORY&gt;'</code> identified the lens, while the <code class="cx qb qc qd qe b">'&lt;REGION_TO_DESCRIPTION&gt;'</code> was less specific. However, this performance may vary with different images.</p><h2 id="84bc" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">2. Object Detection Related Tasks:</h2><p id="f321" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk"><strong class="od fr">2.1 Generate Bounding Boxes and Text for Objects</strong></p><p id="07d0" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 can identify densely packed regions in the image, and to provide their bounding box coordinates and their related labels or captions. To extract bounding boxes with labels, use the <code class="cx qb qc qd qe b">’&lt;OD&gt;’</code>task prompt:</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="e981" class="rd ne fq qe b bg re rf l rg rh">results = run_example(image, task_prompt='&lt;OD&gt;')<br/>draw_bbox(image, results['&lt;OD&gt;'])</span></pre><p id="8151" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">To extract bounding boxes with captions, use <code class="cx qb qc qd qe b">'&lt;DENSE_REGION_CAPTION&gt;'</code> task prompt:</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="0fdd" class="rd ne fq qe b bg re rf l rg rh">task_prompt results = run_example(image, task_prompt= '&lt;DENSE_REGION_CAPTION&gt;')<br/>draw_bbox(image, results['&lt;DENSE_REGION_CAPTION&gt;'])</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rj"><img src="../Images/0efcbdf15dc83e62153b6a752144f800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3x6P8ggYyCRrxSzZ4rKfrQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The image on the left shows the results of the ’&lt;OD&gt;’ task prompt, while the image on the right demonstrates ‘&lt;DENSE_REGION_CAPTION&gt;’</figcaption></figure><p id="6226" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">2.2 Text Grounded Object Detection</strong></p><p id="18ac" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">Florence-2 can also perform text-grounded object detection. By providing specific object names or descriptions as input, Florence-2 detects bounding boxes around the specified objects.</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="9b16" class="rd ne fq qe b bg re rf l rg rh">task_prompt = '&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'<br/>results = run_example(image,task_prompt, text_input=”lens. camera. table. logo. flash.”)<br/>draw_bbox(image, results['&lt;CAPTION_TO_PHRASE_GROUNDING&gt;'])</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rk"><img src="../Images/4a18ac8e5ecfafaded830f3246ee3953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*e9Eyx37pBnXjQkRJ2suhdQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">CAPTION_TO_PHRASE_GROUNDING task with the text input: “lens. camera. table. logo. flash.”</figcaption></figure><h2 id="1c92" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">3. Segmentation Related Tasks:</h2><p id="bb7e" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Florence-2 can also generate segmentation polygons grounded by text (<code class="cx qb qc qd qe b">'&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;'</code>) or by bounding boxes (<code class="cx qb qc qd qe b">'&lt;REGION_TO_SEGMENTATION&gt;'</code>):</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="a19f" class="rd ne fq qe b bg re rf l rg rh">results = run_example(image, task_prompt='&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;', text_input=”camera”)<br/>draw_polygons(image, results[task_prompt])</span></pre><pre class="ri ra qe rb bp rc bb bk"><span id="c3a1" class="rd ne fq qe b bg re rf l rg rh">results = run_example(image, task_prompt='&lt;REGION_TO_SEGMENTATION&gt;', text_input="&lt;loc_345&gt;&lt;loc_417&gt;&lt;loc_648&gt;&lt;loc_845&gt;")<br/>draw_polygons(output_image, results['&lt;REGION_TO_SEGMENTATION&gt;'])</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rl"><img src="../Images/d3545d83b63447e4043f6d333e0c4d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpXEKVwORrILn6-QYrh0QA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The image on the left shows the results of the REFERRING_EXPRESSION_SEGMENTATION task with ‘camera’ text as input. The image on the right demonstrates REGION_TO_SEGMENTATION task with a bounding box around the lens provided as input.</figcaption></figure><h2 id="8916" class="nd ne fq bf nf ng nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa bk">4. OCR Related Tasks:</h2><p id="47dd" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Florence-2 demonstrates strong OCR capabilities. It can extract text from an image with the <code class="cx qb qc qd qe b">'&lt;OCR&gt;'</code> task prompt, and extract both text and its location with <code class="cx qb qc qd qe b">'&lt;OCR_WITH_REGION&gt;'</code> :</p><pre class="mm mn mo mp mq ra qe rb bp rc bb bk"><span id="a91d" class="rd ne fq qe b bg re rf l rg rh">results = run_example(image,task_prompt)<br/>draw_ocr_bboxes(image, results['&lt;OCR_WITH_REGION&gt;'])</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk rm"><img src="../Images/3cab509233de43f25be60c54ce24c3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*xJrd71dPLSIwGFkXFOIAwQ.png"/></div></figure><h1 id="607c" class="pd ne fq bf nf pe pf gq nj pg ph gt nn pi pj pk pl pm pn po pp pq pr ps pt pu bk">Concluding Remarks</h1><p id="e7fc" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Florence-2 is a versatile Vision-Language Model (VLM), capable of handling multiple vision tasks within a single model. Its zero-shot capabilities are impressive across diverse tasks such as image captioning, object detection, segmentation and OCR. While Florence-2 performs well out-of-the-box, additional fine-tuning can further adapt the model to new tasks or improve its performance on unique, custom datasets.</p></div></div></div><div class="ab cb qn qo qp qq" role="separator"><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt qu"/><span class="qr by bm qs qt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1e58" class="pd ne fq bf nf pe qv gq nj pg qw gt nn pi qx pk pl pm qy po pp pq qz ps pt pu bk">Thank you for reading!</h1><p id="e371" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">Congratulations on making it all the way here. Click 👍 to show your appreciation and raise the algorithm self esteem 🤓</p><p id="a06d" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk"><strong class="od fr">Want to learn more?</strong></p><ul class=""><li id="d4da" class="ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot py pz qa bk"><a class="af nc" href="https://medium.com/@lihigurarie" rel="noopener"><strong class="od fr">Explore</strong></a> additional articles I’ve written</li><li id="f41e" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk"><a class="af nc" href="https://medium.com/@lihigurarie/subscribe" rel="noopener"><strong class="od fr">Subscribe</strong></a><strong class="od fr"> </strong>to get notified when I publish articles</li><li id="efdc" class="ob oc fq od b go qf of og gr qg oi oj no qh ol om ns qi oo op nw qj or os ot py pz qa bk">Follow me on <a class="af nc" href="https://www.linkedin.com/in/lihi-gur-arie/" rel="noopener ugc nofollow" target="_blank"><strong class="od fr">Linkedin</strong></a></li></ul><h1 id="b5f8" class="pd ne fq bf nf pe pf gq nj pg ph gt nn pi pj pk pl pm pn po pp pq pr ps pt pu bk">Full Code as Colab notebook:</h1><figure class="mm mn mo mp mq mr"><div class="pv io l ed"><div class="pw px l"/></div></figure><h1 id="d6b6" class="pd ne fq bf nf pe pf gq nj pg ph gt nn pi pj pk pl pm pn po pp pq pr ps pt pu bk">References</h1><p id="caf8" class="pw-post-body-paragraph ob oc fq od b go oe of og gr oh oi oj no ok ol om ns on oo op nw oq or os ot fj bk">[0] Code on Colab Notebook: <a class="af nc" href="https://gist.github.com/Lihi-Gur-Arie/427ecce6a5c7f279d06f3910941e0145" rel="noopener ugc nofollow" target="_blank">link</a></p><p id="304c" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[1] Florence-2: <a class="af nc" href="https://arxiv.org/pdf/2311.06242" rel="noopener ugc nofollow" target="_blank">Advancing a Unified Representation for a Variety of Vision Tasks</a>.</p><p id="8f52" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[2] CLIP: <a class="af nc" href="https://arxiv.org/pdf/2103.00020v1" rel="noopener ugc nofollow" target="_blank">Learning Transferable Visual Models From Natural Language Supervision</a>.</p><p id="a794" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[3] Grounding DINO: <a class="af nc" href="https://arxiv.org/abs/2303.05499" rel="noopener ugc nofollow" target="_blank">Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a>.</p><p id="c6bc" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[4] SAM2: <a class="af nc" href="https://arxiv.org/pdf/2408.00714" rel="noopener ugc nofollow" target="_blank">Segment Anything in Images and Videos</a>.</p><p id="249a" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[5] DaViT: <a class="af nc" href="https://arxiv.org/abs/2204.03645" rel="noopener ugc nofollow" target="_blank">Dual Attention Vision Transformers</a>.</p><p id="663d" class="pw-post-body-paragraph ob oc fq od b go oy of og gr oz oi oj no pa ol om ns pb oo op nw pc or os ot fj bk">[6] BART: <a class="af nc" href="https://arxiv.org/pdf/1910.13461" rel="noopener ugc nofollow" target="_blank">Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>.</p></div></div></div></div>    
</body>
</html>