- en: How to Use Re-Ranking for Better LLM RAG Retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-use-re-ranking-for-better-llm-rag-retrieval-243f89414266?source=collection_archive---------0-----------------------#2024-05-02](https://towardsdatascience.com/how-to-use-re-ranking-for-better-llm-rag-retrieval-243f89414266?source=collection_archive---------0-----------------------#2024-05-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building an advanced local LLM RAG pipeline with two-step retrieval using open-source
    bi-encoders and cross-encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@leoneversberg?source=post_page---byline--243f89414266--------------------------------)[![Dr.
    Leon Eversberg](../Images/56dc3579a29933f7047a9ce60be4697a.png)](https://medium.com/@leoneversberg?source=post_page---byline--243f89414266--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--243f89414266--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--243f89414266--------------------------------)
    [Dr. Leon Eversberg](https://medium.com/@leoneversberg?source=post_page---byline--243f89414266--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--243f89414266--------------------------------)
    ·9 min read·May 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/451f8f4e1cb91219a3a0449a305140d9.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM chatbot with two-stage RAG retrieval I built with access to Wikipedia.
    Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots based on Large Language Models (LLMs) can be improved by providing
    external knowledge through Retrieval Augmented Generation (RAG).
  prefs: []
  type: TYPE_NORMAL
- en: This external knowledge can reduce wrong answers (hallucinations) and also give
    the model access to information that was not part of its training data.
  prefs: []
  type: TYPE_NORMAL
- en: With RAG, we feed our LLM information, such as PDF documents, or Wikipedia articles,
    as additional context in our prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/031cb1f48b211c1e5d5d74b73112aa54.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The basic RAG pipeline: an encoder model and a vector database are used to
    efficiently search for relevant document chunks. Image from my article [How to
    Build a Local Open-Source LLM Chatbot With RAG](https://medium.com/towards-data-science/how-to-build-a-local-open-source-llm-chatbot-with-rag-f01f73e2a131)'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, RAG chatbots follow the old principle of data science: **garbage in,
    garbage out**. If the document retrieval fails, the LLM model has no chance of
    providing a good answer.'
  prefs: []
  type: TYPE_NORMAL
- en: An improvement to the basic RAG pipeline is the use of a **re-ranker**. A re-ranker
    takes the user’s question and all the initially retrieved documents as input and
    re-ranks these documents based on how closely they match the question.
  prefs: []
  type: TYPE_NORMAL
