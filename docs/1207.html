<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Recreating PyTorch from Scratch (with GPU Support and Automatic Differentiation)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14">https://towardsdatascience.com/recreating-pytorch-from-scratch-with-gpu-support-and-automatic-differentiation-8f565122a3cc?source=collection_archive---------0-----------------------#2024-05-14</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f495" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Build your own deep learning framework based on C/C++, CUDA, and Python, with GPU support and automatic differentiation</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lucas de Lima Nogueira" class="l ep by dd de cx" src="../Images/76edd8ee4005d4c0b8bd476261eb06ae.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6SXwrSnYs8I7IjjWUJsIQg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@lucasdelimanogueira?source=post_page---byline--8f565122a3cc--------------------------------" rel="noopener follow">Lucas de Lima Nogueira</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--8f565122a3cc--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">24 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 14, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">19</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/7fb280914af3d0239f5a27ae8d414ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nD4KJG4Jk0sFb5uTujNoPA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author with the assistance of AI (<a class="af nc" href="https://copilot.microsoft.com/images/create" rel="noopener ugc nofollow" target="_blank">https://copilot.microsoft.com/images/create</a>)</figcaption></figure><h1 id="f624" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Introduction</h1><p id="cd49" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">For many years I have been using PyTorch to construct and train deep learning models. Even though I have learned its syntax and rules, something has always aroused my curiosity: what is happening internally during these operations? How does all of this work?</p><p id="dcd4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you have gotten here, you probably have the same questions. If I ask you how to create and train a model in PyTorch, you will probably come up with something similar to the code below:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="7214" class="pe ne fq pb b bg pf pg l ph pi">import torch<br/>import torch.nn as nn<br/>import torch.optim as optim<br/><br/>class MyModel(nn.Module):<br/>    def __init__(self):<br/>        super(MyModel, self).__init__()<br/>        self.fc1 = nn.Linear(1, 10)<br/>        self.sigmoid = nn.Sigmoid()<br/>        self.fc2 = nn.Linear(10, 1)<br/><br/>    def forward(self, x):<br/>        out = self.fc1(x)<br/>        out = self.sigmoid(out)<br/>        out = self.fc2(out)<br/>        <br/>        return out<br/><br/>...<br/><br/>model = MyModel().to(device)<br/>criterion = nn.MSELoss()<br/>optimizer = optim.SGD(model.parameters(), lr=0.001)<br/><br/>for epoch in range(epochs):<br/>    for x, y in ...<br/>        <br/>        x = x.to(device)<br/>        y = y.to(device)<br/><br/>        outputs = model(x)<br/>        loss = criterion(outputs, y)<br/>        <br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()</span></pre><p id="a155" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">But what if I ask you how does this backward step works? Or, for instance, what happens when you reshape a tensor? Is the data rearranged internally? How does that happens? Why is PyTorch so fast? How does PyTorch handle GPU operations? These are the types of questions that have always intrigued me, and I imagine they also intrigue you. Thus, in order to better understand these concepts, what is better than building your own tensor library <strong class="ob fr"><em class="pj">from scratch</em></strong>? And that is what you will learn in this article!</p><h1 id="cd25" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">#1 — Tensor</h1><p id="f3a2" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In order to construct a <em class="pj">tensor library</em>, the first concept you need to learn obviously is: what is a tensor?</p><p id="b772" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You may have an intuitive idea that a tensor is a mathematical concept of a n-dimensional data structure that contains some numbers. But here we need to understand how to model this data structure from a computational perspective. We can think of a tensor as consisting of the data itself and also some metadata describing aspects of the tensor such as its shape or the device it lives in (i.e. CPU memory, GPU memory…).</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pk"><img src="../Images/126751c339f220fec6bc0a40690fb069.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*WV3ZCYzUs1IPJ0if2ZQU3Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="6277" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There is also a less popular metadata that you may have never heard of, called <em class="pj">stride</em>. This concept is very important to understand the internals of tensor data rearrangement, so we need to discuss it a little more.</p><p id="cc50" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Imagine a 2-D tensor with shape [4, 8], illustrated below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pl"><img src="../Images/169e1865490d4603d66dc792052c1458.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*LUQifklZNSJuDWCXLCQb7w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">4x8 Tensor (Image by Author)</figcaption></figure><p id="2119" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The data (i.e. float numbers) of a tensor is actually stored as a 1-dimensional array on memory:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pm"><img src="../Images/edc491c716df7cab51cd1ca4a61ff296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*maqNuiifynJlAbBdWhFOgw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">1-D dimensional data array of the tensor (Image by Author)</figcaption></figure><p id="fecc" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">So, in order to represent this 1-dimensional array as a N-dimensional tensor, we use strides. Basically the idea is the following:</p><p id="3d3a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We have a matrix with 4 rows and 8 columns. Considering that all of its elements are organized by rows on the 1-dimensional array, if we want to access the value at position [2, 3], we need to traverse 2 rows (of 8 elements each) plus 3 positions. In mathematical terms, we need to traverse 3 + 2 * 8 elements on the 1-dimensional array:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pm"><img src="../Images/644a13550e4601385357edc5bb88071b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*OkJTU6jJfZ_bEJUpJwpggg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="9d70" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">So this ‘8’ is the <em class="pj">stride</em> of the <em class="pj">second</em> dimension. In this case, it is the information of how many elements I need to traverse on the array to “jump” to other positions on the <em class="pj">second</em> <em class="pj">dimension.</em></p><p id="b98b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Thus, for accessing the element [i, j] of a 2-dimensional tensor with shape [shape_0, shape_1], we basically need to access the element at position j + i * shape_1</p><p id="8caa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, let us imagine a 3-dimensional tensor:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pn"><img src="../Images/bc1c13d004a29b88a4f73e8b11a51483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*956v8yJS-bEQA4l698eshw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">5x4x8 Tensor (Image by Author)</figcaption></figure><p id="ae7d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can think of this 3-dimensional tensor as a sequence of matrices. For example, you can think of this [5, 4, 8] tensor as 5 matrices of shape [4, 8].</p><p id="3391" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, in order to access the element at position [1, 2, 7], you need to traverse 1 complete matrix of shape [4,8], 2 rows of shape [8] and 7 columns of shape [1]. So, you need to traverse (1 * 4 * 8) + (2 * 8) + (7 * 1) positions on the 1-dimensional array.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk po"><img src="../Images/292c1ea36eb8d9c10528822680f5c698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBpZiCzLuF0xHViddC_lGg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="9f19" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Thus, to access the element [i][j][k] of a 3-D tensor with [shape_0, shape_1, shape_2] on the 1-D data array, you do:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pp"><img src="../Images/81de413fc9cf4c48a4a1b0c764dea74f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_534cP0lZwKzTT4v8zchqw.png"/></div></div></figure><p id="3647" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This shape_1 * shape_2 is the <em class="pj">stride </em>of the first dimension, the shape_2 is the <em class="pj">stride </em>of the second dimension and 1 is the stride of the third dimension.</p><p id="5843" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Then, in order to generalize:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pq"><img src="../Images/e3ef816d9241ae378b6b92a5d7a09bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4NnkTMr2hO1C6-PKkms-Q.png"/></div></div></figure><p id="cd40" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Where the <em class="pj">strides</em> of each dimension can be calculated using the product of the next dimension tensor shapes:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pr"><img src="../Images/7b9a7f16cd0d11e4767e681da1993aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*XT8B0T22JQlaNMG9x2NeuQ.png"/></div></figure><p id="2866" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Then we set stride[n-1] = 1.</p><p id="0a38" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">On our tensor example of shape [5, 4, 8] we would have strides = [4*8, 8, 1] = [32, 8, 1]</p><p id="18ed" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can test on your own:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="1d75" class="pe ne fq pb b bg pf pg l ph pi">import torch<br/><br/>torch.rand([5, 4, 8]).stride()<br/>#(32, 8, 1)</span></pre><p id="8306" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Ok, but why do we need shapes and strides? Beyond accessing elements of N-dimensional tensors stored as 1-dimensional arrays, this concept can be used to manipulate tensor arrangements very easily.</p><p id="9b13" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For example, to reshape a tensor, you only need to set the new shape and calculate the new strides based on it! (since the new shape guarantees the same number of elements)</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="ec45" class="pe ne fq pb b bg pf pg l ph pi">import torch<br/><br/>t = torch.rand([5, 4, 8])<br/><br/>print(t.shape)<br/># [5, 4, 8]<br/><br/>print(t.stride())<br/># [32, 8, 1]<br/><br/>new_t = t.reshape([4, 5, 2, 2, 2])<br/><br/>print(new_t.shape)<br/># [4, 5, 2, 2, 2]<br/><br/>print(new_t.stride())<br/># [40, 8, 4, 2, 1]</span></pre><p id="6a8a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Internally, the tensor is still stored as the same 1-dimensional array. The reshape method did not change the order of the elements within the array! That’s amazing, isn’t? 😁</p><p id="887d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can verify on your own using the following function that accesses the internal 1-dimensional array on PyTorch:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="35ec" class="pe ne fq pb b bg pf pg l ph pi">import ctypes<br/><br/>def print_internal(t: torch.Tensor):<br/>    print(<br/>        torch.frombuffer(<br/>            ctypes.string_at(t.data_ptr(), t.storage().nbytes()), dtype=t.dtype<br/>        )<br/>    )<br/><br/>print_internal(t)<br/># [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...<br/><br/>print_internal(new_t)<br/># [0.0752, 0.5898, 0.3930, 0.9577, 0.2276, 0.9786, 0.1009, 0.138, ...</span></pre><p id="7e5f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Or for instance, you want to transpose two axes. Internally, you just need to swap the respective strides!</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b42e" class="pe ne fq pb b bg pf pg l ph pi">t = torch.arange(0, 24).reshape(2, 3, 4)<br/>print(t)<br/># [[[ 0,  1,  2,  3],<br/>#   [ 4,  5,  6,  7],<br/>#   [ 8,  9, 10, 11]],<br/> <br/>#  [[12, 13, 14, 15],<br/>#   [16, 17, 18, 19],<br/>#   [20, 21, 22, 23]]]<br/><br/>print(t.shape)<br/># [2, 3, 4]<br/><br/>print(t.stride())<br/># [12, 4, 1]<br/><br/>new_t = t.transpose(0, 1)<br/>print(new_t)<br/># [[[ 0,  1,  2,  3],<br/>#   [12, 13, 14, 15]],<br/><br/>#  [[ 4,  5,  6,  7],<br/>#   [16, 17, 18, 19]],<br/><br/>#  [[ 8,  9, 10, 11],<br/>#   [20, 21, 22, 23]]]<br/><br/>print(new_t.shape)<br/># [3, 2, 4]<br/><br/>print(new_t.stride())<br/># [4, 12, 1]</span></pre><p id="3740" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you print the internal array, both have the same values:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e753" class="pe ne fq pb b bg pf pg l ph pi">print_internal(t)<br/># [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]<br/><br/>print_internal(new_t)<br/># [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]</span></pre><p id="132e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">However, the stride of <code class="cx ps pt pu pb b">new_t</code> now does not match with the equation I showed above. This is due to the fact that the tensor is now not contiguous. That means that although the internal array remains the same, the order of its values in memory does not match with the actual order of the tensor.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="83c9" class="pe ne fq pb b bg pf pg l ph pi">t.is_contiguous()<br/># True<br/><br/>new_t.is_contiguous()<br/># False</span></pre><p id="a29d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">This means the accessing the non-contiguous elements in sequence is less efficient (as the real tensor elements is not ordered in sequence on memory). In order to fix that, we can do:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="22da" class="pe ne fq pb b bg pf pg l ph pi">new_t_contiguous = new_t.contiguous()<br/><br/>print(new_t_contiguous.is_contiguous())<br/># True</span></pre><p id="eb5a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If we analyze the internal array, its order now matches with the actual tensor order now, which can provide better memory access efficiency:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="3f7d" class="pe ne fq pb b bg pf pg l ph pi">print(new_t)<br/># [[[ 0,  1,  2,  3],<br/>#   [12, 13, 14, 15]],<br/><br/>#  [[ 4,  5,  6,  7],<br/>#   [16, 17, 18, 19]],<br/><br/>#  [[ 8,  9, 10, 11],<br/>#   [20, 21, 22, 23]]]<br/><br/>print_internal(new_t)<br/># [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]<br/><br/>print_internal(new_t_contiguous)<br/># [ 0,  1,  2,  3, 12, 13, 14, 15,  4,  5,  6,  7, 16, 17, 18, 19,  8,  9, 10, 11, 20, 21, 22, 23]</span></pre><p id="2ab7" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now that we comprehend how a tensor is modeled, let us start creating our library!</p><p id="49ed" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">I will call it <em class="pj">Norch</em>, which stands for NOT PyTorch, and also makes an allusion to my last name, Nogueira 😁</p><p id="327d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The first thing to know is that although PyTorch is used through Python, internally it runs C/C++. So we will first create our internals C/C++ functions.</p><p id="5289" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We can first define a tensor as a struct to store its data and metadata, and create a function to instantiate it:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e9cc" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>#include &lt;stdio.h&gt;<br/>#include &lt;stdlib.h&gt;<br/>#include &lt;string.h&gt;<br/>#include &lt;math.h&gt;<br/><br/>typedef struct {<br/>    float* data;<br/>    int* strides;<br/>    int* shape;<br/>    int ndim;<br/>    int size;<br/>    char* device;<br/>} Tensor;<br/><br/>Tensor* create_tensor(float* data, int* shape, int ndim) {<br/>    <br/>    Tensor* tensor = (Tensor*)malloc(sizeof(Tensor));<br/>    if (tensor == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/>    tensor-&gt;data = data;<br/>    tensor-&gt;shape = shape;<br/>    tensor-&gt;ndim = ndim;<br/><br/>    tensor-&gt;size = 1;<br/>    for (int i = 0; i &lt; ndim; i++) {<br/>        tensor-&gt;size *= shape[i];<br/>    }<br/><br/>    tensor-&gt;strides = (int*)malloc(ndim * sizeof(int));<br/>    if (tensor-&gt;strides == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/>    int stride = 1;<br/>    for (int i = ndim - 1; i &gt;= 0; i--) {<br/>        tensor-&gt;strides[i] = stride;<br/>        stride *= shape[i];<br/>    }<br/>    <br/>    return tensor;<br/>}</span></pre><p id="61bc" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In order to access some element, we can take advantage of strides, as we learned before:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="dac9" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>float get_item(Tensor* tensor, int* indices) {<br/>    int index = 0;<br/>    for (int i = 0; i &lt; tensor-&gt;ndim; i++) {<br/>        index += indices[i] * tensor-&gt;strides[i];<br/>    }<br/><br/>    float result;<br/>    result = tensor-&gt;data[index];<br/><br/>    return result;<br/>}</span></pre><p id="2e66" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, we can create the tensor operations. I will show some examples and you can find the complete version in the repository linked at the end of this article.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="a976" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/cpu.cpp<br/><br/>void add_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {<br/>    <br/>    for (int i = 0; i &lt; tensor1-&gt;size; i++) {<br/>        result_data[i] = tensor1-&gt;data[i] + tensor2-&gt;data[i];<br/>    }<br/>}<br/><br/>void sub_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {<br/>    <br/>    for (int i = 0; i &lt; tensor1-&gt;size; i++) {<br/>        result_data[i] = tensor1-&gt;data[i] - tensor2-&gt;data[i];<br/>    }<br/>}<br/><br/>void elementwise_mul_tensor_cpu(Tensor* tensor1, Tensor* tensor2, float* result_data) {<br/>    <br/>    for (int i = 0; i &lt; tensor1-&gt;size; i++) {<br/>        result_data[i] = tensor1-&gt;data[i] * tensor2-&gt;data[i];<br/>    }<br/>}<br/><br/>void assign_tensor_cpu(Tensor* tensor, float* result_data) {<br/><br/>    for (int i = 0; i &lt; tensor-&gt;size; i++) {<br/>        result_data[i] = tensor-&gt;data[i];<br/>    }<br/>}<br/><br/>...</span></pre><p id="b36c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">After that we are able to create our other tensor functions that will call these operations:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="6624" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {<br/>    if (tensor1-&gt;ndim != tensor2-&gt;ndim) {<br/>        fprintf(stderr, "Tensors must have the same number of dimensions %d and %d for addition\n", tensor1-&gt;ndim, tensor2-&gt;ndim);<br/>        exit(1);<br/>    }<br/><br/>    int ndim = tensor1-&gt;ndim;<br/>    int* shape = (int*)malloc(ndim * sizeof(int));<br/>    if (shape == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/><br/>    for (int i = 0; i &lt; ndim; i++) {<br/>        if (tensor1-&gt;shape[i] != tensor2-&gt;shape[i]) {<br/>            fprintf(stderr, "Tensors must have the same shape %d and %d at index %d for addition\n", tensor1-&gt;shape[i], tensor2-&gt;shape[i], i);<br/>            exit(1);<br/>        }<br/>        shape[i] = tensor1-&gt;shape[i];<br/>    }        <br/>    float* result_data = (float*)malloc(tensor1-&gt;size * sizeof(float));<br/>    if (result_data == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/>    add_tensor_cpu(tensor1, tensor2, result_data);<br/>    <br/>    return create_tensor(result_data, shape, ndim, device);<br/>}</span></pre><p id="779a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As mentioned before, the tensor reshaping does not modify the internal data array:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="18fa" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>Tensor* reshape_tensor(Tensor* tensor, int* new_shape, int new_ndim) {<br/><br/>    int ndim = new_ndim;<br/>    int* shape = (int*)malloc(ndim * sizeof(int));<br/>    if (shape == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/><br/>    for (int i = 0; i &lt; ndim; i++) {<br/>        shape[i] = new_shape[i];<br/>    }<br/><br/>    // Calculate the total number of elements in the new shape<br/>    int size = 1;<br/>    for (int i = 0; i &lt; new_ndim; i++) {<br/>        size *= shape[i];<br/>    }<br/><br/>    // Check if the total number of elements matches the current tensor's size<br/>    if (size != tensor-&gt;size) {<br/>        fprintf(stderr, "Cannot reshape tensor. Total number of elements in new shape does not match the current size of the tensor.\n");<br/>        exit(1);<br/>    }<br/><br/>    float* result_data = (float*)malloc(tensor-&gt;size * sizeof(float));<br/>    if (result_data == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/>    assign_tensor_cpu(tensor, result_data);<br/>    return create_tensor(result_data, shape, ndim, device);<br/>}</span></pre><p id="6a53" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Although we can now do some tensor operations, nobody deserves to run it using C/C++, right? Let us start building our Python wrapper!</p><p id="816b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">There are a lot of options to run C/C++ code using Python, such as <em class="pj">Pybind11</em> and <em class="pj">Cython.</em> For our example I will use <em class="pj">ctypes.</em></p><p id="7914" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The basically structure of <em class="pj">ctypes</em> is shown below:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e242" class="pe ne fq pb b bg pf pg l ph pi">//C code<br/>#include &lt;stdio.h&gt;<br/><br/>float add_floats(float a, float b) {<br/>    return a + b;<br/>}</span></pre><pre class="pv pa pb pc bp pd bb bk"><span id="c190" class="pe ne fq pb b bg pf pg l ph pi"># Compile<br/>gcc -shared -o add_floats.so -fPIC add_floats.c</span></pre><pre class="pv pa pb pc bp pd bb bk"><span id="1f4f" class="pe ne fq pb b bg pf pg l ph pi"># Python code<br/>import ctypes<br/><br/># Load the shared library<br/>lib = ctypes.CDLL('./add_floats.so')<br/><br/># Define the argument and return types for the function<br/>lib.add_floats.argtypes = [ctypes.c_float, ctypes.c_float]<br/>lib.add_floats.restype = ctypes.c_float<br/><br/># Convert python float to c_float type <br/>a = ctypes.c_float(3.5)<br/>b = ctypes.c_float(2.2)<br/><br/># Call the C function<br/>result = lib.add_floats(a, b)<br/>print(result)<br/># 5.7</span></pre><p id="2d74" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As you can see, it is very intuitive. After you compile the C/C++ code, you can use it on Python with <em class="pj">ctypes </em>very easily. You only need to define the arguments &amp; return c_types of the function, convert the variable to its respective c_types and call the function. For more complex types such as arrays (float lists) you can use pointers.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="aec6" class="pe ne fq pb b bg pf pg l ph pi">data = [1.0, 2.0, 3.0]<br/>data_ctype = (ctypes.c_float * len(data))(*data)<br/><br/>lib.some_array_func.argstypes = [ctypes.POINTER(ctypes.c_float)]<br/><br/>...<br/><br/>lib.some_array_func(data)</span></pre><p id="7bab" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And for struct types we can create our own c_type:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="1b45" class="pe ne fq pb b bg pf pg l ph pi">class CustomType(ctypes.Structure):<br/>    _fields_ = [<br/>        ('field1', ctypes.POINTER(ctypes.c_float)),<br/>        ('field2', ctypes.POINTER(ctypes.c_int)),<br/>        ('field3', ctypes.c_int),<br/>    ]<br/><br/># Can be used as ctypes.POINTER(CustomType)</span></pre><p id="b6f1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">After this brief explanation, let us construct the Python wrapper for our tensor C/C++ library!</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b453" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>import ctypes<br/><br/>class CTensor(ctypes.Structure):<br/>    _fields_ = [<br/>        ('data', ctypes.POINTER(ctypes.c_float)),<br/>        ('strides', ctypes.POINTER(ctypes.c_int)),<br/>        ('shape', ctypes.POINTER(ctypes.c_int)),<br/>        ('ndim', ctypes.c_int),<br/>        ('size', ctypes.c_int),<br/>    ]<br/><br/>class Tensor:<br/>    os.path.abspath(os.curdir)<br/>    _C = ctypes.CDLL("COMPILED_LIB.so"))<br/><br/>    def __init__(self):<br/>        <br/>        data, shape = self.flatten(data)<br/>        self.data_ctype = (ctypes.c_float * len(data))(*data)<br/>        self.shape_ctype = (ctypes.c_int * len(shape))(*shape)<br/>        self.ndim_ctype = ctypes.c_int(len(shape))<br/>       <br/>        self.shape = shape<br/>        self.ndim = len(shape)<br/><br/>        Tensor._C.create_tensor.argtypes = [ctypes.POINTER(ctypes.c_float), ctypes.POINTER(ctypes.c_int), ctypes.c_int]<br/>        Tensor._C.create_tensor.restype = ctypes.POINTER(CTensor)<br/><br/>        self.tensor = Tensor._C.create_tensor(<br/>            self.data_ctype,<br/>            self.shape_ctype,<br/>            self.ndim_ctype,<br/>        )<br/>        <br/>    def flatten(self, nested_list):<br/>        """<br/>        This method simply convert a list type tensor to a flatten tensor with its shape<br/>        <br/>        Example:<br/>        <br/>        Arguments:  <br/>            nested_list: [[1, 2, 3], [-5, 2, 0]]<br/>        Return:<br/>            flat_data: [1, 2, 3, -5, 2, 0]<br/>            shape: [2, 3]<br/>        """<br/>        def flatten_recursively(nested_list):<br/>            flat_data = []<br/>            shape = []<br/>            if isinstance(nested_list, list):<br/>                for sublist in nested_list:<br/>                    inner_data, inner_shape = flatten_recursively(sublist)<br/>                    flat_data.extend(inner_data)<br/>                shape.append(len(nested_list))<br/>                shape.extend(inner_shape)<br/>            else:<br/>                flat_data.append(nested_list)<br/>            return flat_data, shape<br/>        <br/>        flat_data, shape = flatten_recursively(nested_list)<br/>        return flat_data, shape<br/></span></pre><p id="6552" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we include the Python tensor operations to call the C/C++ operations.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="17b1" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>def __getitem__(self, indices):<br/>    """<br/>    Access tensor by index tensor[i, j, k...]<br/>    """<br/><br/>    if len(indices) != self.ndim:<br/>        raise ValueError("Number of indices must match the number of dimensions")<br/>    <br/>    Tensor._C.get_item.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int)]<br/>    Tensor._C.get_item.restype = ctypes.c_float<br/>                                       <br/>    indices = (ctypes.c_int * len(indices))(*indices)<br/>    value = Tensor._C.get_item(self.tensor, indices)  <br/>    <br/>    return value<br/><br/>def reshape(self, new_shape):<br/>    """<br/>    Reshape tensor<br/>    result = tensor.reshape([1,2])<br/>    """<br/>    new_shape_ctype = (ctypes.c_int * len(new_shape))(*new_shape)<br/>    new_ndim_ctype = ctypes.c_int(len(new_shape))<br/>    <br/>    Tensor._C.reshape_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(ctypes.c_int), ctypes.c_int]<br/>    Tensor._C.reshape_tensor.restype = ctypes.POINTER(CTensor)<br/>    result_tensor_ptr = Tensor._C.reshape_tensor(self.tensor, new_shape_ctype, new_ndim_ctype)   <br/><br/>    result_data = Tensor()<br/>    result_data.tensor = result_tensor_ptr<br/>    result_data.shape = new_shape.copy()<br/>    result_data.ndim = len(new_shape)<br/>    result_data.device = self.device<br/><br/>    return result_data<br/><br/>def __add__(self, other):<br/>    """<br/>    Add tensors<br/>    result = tensor1 + tensor2<br/>    """<br/>  <br/>    if self.shape != other.shape:<br/>        raise ValueError("Tensors must have the same shape for addition")<br/>    <br/>    Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]<br/>    Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)<br/><br/>    result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)<br/><br/>    result_data = Tensor()<br/>    result_data.tensor = result_tensor_ptr<br/>    result_data.shape = self.shape.copy()<br/>    result_data.ndim = self.ndim<br/>    result_data.device = self.device<br/><br/>    return result_data<br/><br/># Include the other operations:<br/># __str__<br/># __sub__ (-)<br/># __mul__ (*)<br/># __matmul__ (@)<br/># __pow__ (**)<br/># __truediv__ (/)<br/># log<br/># ...</span></pre><p id="f77f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">If you got here, you are now capable to run the code and start doing some tensor operations!</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="856d" class="pe ne fq pb b bg pf pg l ph pi">import norch<br/><br/>tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]])<br/>tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]])<br/><br/>result = tensor1 + tensor2<br/>print(result[0, 0])<br/># 4 </span></pre><h1 id="dd31" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">#2 — GPU Support</h1><p id="44fb" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">After creating the basic structure of our library, now we will take it to a new level. It is well-known that you can call <code class="cx ps pt pu pb b">.to("cuda")</code> to send data to GPU and run math operations faster. I will assume that you have basic knowledge on how CUDA works, but if you do not, you can read my other article: <a class="af nc" rel="noopener" target="_blank" href="/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66">CUDA tutorial.</a> I will wait here. 😊</p><p id="2421" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">…</p><p id="f7b9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For those in a hurry, a simple introduction here:</p><p id="5b7d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Basically, all of our code until here is running on CPU memory. Altough for single operations CPUs are faster, the advantage of GPUs relies on its parallelization capabilities. While CPU design aims to execute a sequence of operations (threads) fast (but can only execute dozens of them), the GPU design aims to execute millions of operations in parallel (by sacrificing individual threads performance).</p><p id="69c8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Thus, we can take advantage of this capability to perform operations in parallel. For example, in a million-sized tensor addition, instead of adding the elements of each index sequentially inside a loop, using a GPU we can add all of them in parallel at once. In order to do that, we can use CUDA, which is a platform developed by NVIDIA to enable developers to integrate GPU support to their software applications.</p><p id="9d0c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In order to do that, you can use CUDA C/C++, which is a simple C/C++ based interface designed to run specific GPU operations (such as copy data from CPU memory to GPU memory).</p><p id="00ec" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The code below basically uses some CUDA C/C++ functions to copy data from CPU to GPU, and run the AddTwoArrays function (also called kernel) on a total of N GPU threads in parallel, each responsible to add a different element of the array.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="c61a" class="pe ne fq pb b bg pf pg l ph pi">#include &lt;stdio.h&gt;<br/><br/>// CPU version for comparison<br/>void AddTwoArrays_CPU(flaot A[], float B[], float C[]) {<br/>    for (int i = 0; i &lt; N; i++) {<br/>        C[i] = A[i] + B[i];<br/>    }<br/>}<br/><br/>// Kernel definition<br/>__global__ void AddTwoArrays_GPU(float A[], float B[], float C[]) {<br/>    int i = threadIdx.x;<br/>    C[i] = A[i] + B[i];<br/>}<br/><br/>int main() {<br/><br/>    int N = 1000; // Size of the arrays<br/>    float A[N], B[N], C[N]; // Arrays A, B, and C<br/><br/>    ...<br/><br/>    float *d_A, *d_B, *d_C; // Device pointers for arrays A, B, and C<br/><br/>    // Allocate memory on the device for arrays A, B, and C<br/>    cudaMalloc((void **)&amp;d_A, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_B, N * sizeof(float));<br/>    cudaMalloc((void **)&amp;d_C, N * sizeof(float));<br/><br/>    // Copy arrays A and B from host to device<br/>    cudaMemcpy(d_A, A, N * sizeof(float), cudaMemcpyHostToDevice);<br/>    cudaMemcpy(d_B, B, N * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    // Kernel invocation with N threads<br/>    AddTwoArrays_GPU&lt;&lt;&lt;1, N&gt;&gt;&gt;(d_A, d_B, d_C);<br/>    <br/>    // Copy vector C from device to host<br/>    cudaMemcpy(C, d_C, N * sizeof(float), cudaMemcpyDeviceToHost);<br/><br/>}</span></pre><p id="4550" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As you can notice, instead of adding each element pair per operation, we run all of the adding operations in parallel, getting rid of the loop instruction.</p><p id="385b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">After this brief introduction, we can go back to our tensor library.</p><p id="8a67" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The first step is to create a function to send tensor data from CPU to GPU and vice versa.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="90ac" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>void to_device(Tensor* tensor, char* target_device) {<br/>    if ((strcmp(target_device, "cuda") == 0) &amp;&amp; (strcmp(tensor-&gt;device, "cpu") == 0)) {<br/>        cpu_to_cuda(tensor);<br/>    }<br/><br/>    else if ((strcmp(target_device, "cpu") == 0) &amp;&amp; (strcmp(tensor-&gt;device, "cuda") == 0)) {<br/>        cuda_to_cpu(tensor);<br/>    }<br/>}</span></pre><pre class="pv pa pb pc bp pd bb bk"><span id="676f" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/cuda.cu<br/><br/>__host__ void cpu_to_cuda(Tensor* tensor) {<br/>    <br/>    float* data_tmp;<br/>    cudaMalloc((void **)&amp;data_tmp, tensor-&gt;size * sizeof(float));<br/>    cudaMemcpy(data_tmp, tensor-&gt;data, tensor-&gt;size * sizeof(float), cudaMemcpyHostToDevice);<br/><br/>    tensor-&gt;data = data_tmp;<br/><br/>    const char* device_str = "cuda";<br/>    tensor-&gt;device = (char*)malloc(strlen(device_str) + 1);<br/>    strcpy(tensor-&gt;device, device_str); <br/><br/>    printf("Successfully sent tensor to: %s\n", tensor-&gt;device);<br/>}<br/><br/>__host__ void cuda_to_cpu(Tensor* tensor) {<br/>    float* data_tmp = (float*)malloc(tensor-&gt;size * sizeof(float));<br/><br/>    cudaMemcpy(data_tmp, tensor-&gt;data, tensor-&gt;size * sizeof(float), cudaMemcpyDeviceToHost);<br/>    cudaFree(tensor-&gt;data);<br/><br/>    tensor-&gt;data = data_tmp;<br/><br/>    const char* device_str = "cpu";<br/>    tensor-&gt;device = (char*)malloc(strlen(device_str) + 1);<br/>    strcpy(tensor-&gt;device, device_str); <br/><br/>    printf("Successfully sent tensor to: %s\n", tensor-&gt;device);<br/>}</span></pre><p id="556f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The Python wrapper:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="d940" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>def to(self, device):<br/>    self.device = device<br/>    self.device_ctype = self.device.encode('utf-8')<br/>  <br/>    Tensor._C.to_device.argtypes = [ctypes.POINTER(CTensor), ctypes.c_char_p]<br/>    Tensor._C.to_device.restype = None<br/>    Tensor._C.to_device(self.tensor, self.device_ctype)<br/>  <br/>    return self</span></pre><p id="9a38" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Then, we create GPU versions for all of our tensor operations. I will write examples for addition and subtraction:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="80d6" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/cuda.cu<br/><br/>#define THREADS_PER_BLOCK 128<br/><br/>__global__ void add_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {<br/>    <br/>    int i = blockIdx.x * blockDim.x + threadIdx.x;<br/>    if (i &lt; size) {<br/>        result_data[i] = data1[i] + data2[i];<br/>    }<br/>}<br/><br/>__host__ void add_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {<br/>    <br/>    int number_of_blocks = (tensor1-&gt;size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;<br/>    add_tensor_cuda_kernel&lt;&lt;&lt;number_of_blocks, THREADS_PER_BLOCK&gt;&gt;&gt;(tensor1-&gt;data, tensor2-&gt;data, result_data, tensor1-&gt;size);<br/><br/>    cudaError_t error = cudaGetLastError();<br/>    if (error != cudaSuccess) {<br/>        printf("CUDA error: %s\n", cudaGetErrorString(error));<br/>        exit(-1);<br/>    }<br/><br/>    cudaDeviceSynchronize();<br/>}<br/><br/>__global__ void sub_tensor_cuda_kernel(float* data1, float* data2, float* result_data, int size) {<br/>   <br/>    int i = blockIdx.x * blockDim.x + threadIdx.x;<br/>    if (i &lt; size) {<br/>        result_data[i] = data1[i] - data2[i];<br/>    }<br/>}<br/><br/>__host__ void sub_tensor_cuda(Tensor* tensor1, Tensor* tensor2, float* result_data) {<br/>    <br/>    int number_of_blocks = (tensor1-&gt;size + THREADS_PER_BLOCK - 1) / THREADS_PER_BLOCK;<br/>    sub_tensor_cuda_kernel&lt;&lt;&lt;number_of_blocks, THREADS_PER_BLOCK&gt;&gt;&gt;(tensor1-&gt;data, tensor2-&gt;data, result_data, tensor1-&gt;size);<br/><br/>    cudaError_t error = cudaGetLastError();<br/>    if (error != cudaSuccess) {<br/>        printf("CUDA error: %s\n", cudaGetErrorString(error));<br/>        exit(-1);<br/>    }<br/><br/>    cudaDeviceSynchronize();<br/>}<br/><br/>...<br/></span></pre><p id="5912" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Subsequently, we include a new tensor attribute <code class="cx ps pt pu pb b">char* device</code> on the <code class="cx ps pt pu pb b">tensor.cpp</code> and we can use it to select where the operations will be run (CPU or GPU):</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e825" class="pe ne fq pb b bg pf pg l ph pi">//norch/csrc/tensor.cpp<br/><br/>Tensor* add_tensor(Tensor* tensor1, Tensor* tensor2) {<br/>    if (tensor1-&gt;ndim != tensor2-&gt;ndim) {<br/>        fprintf(stderr, "Tensors must have the same number of dimensions %d and %d for addition\n", tensor1-&gt;ndim, tensor2-&gt;ndim);<br/>        exit(1);<br/>    }<br/><br/>    if (strcmp(tensor1-&gt;device, tensor2-&gt;device) != 0) {<br/>        fprintf(stderr, "Tensors must be on the same device: %s and %s\n", tensor1-&gt;device, tensor2-&gt;device);<br/>        exit(1);<br/>    }<br/><br/>    char* device = (char*)malloc(strlen(tensor1-&gt;device) + 1);<br/>    if (device != NULL) {<br/>        strcpy(device, tensor1-&gt;device);<br/>    } else {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(-1);<br/>    }<br/>    int ndim = tensor1-&gt;ndim;<br/>    int* shape = (int*)malloc(ndim * sizeof(int));<br/>    if (shape == NULL) {<br/>        fprintf(stderr, "Memory allocation failed\n");<br/>        exit(1);<br/>    }<br/><br/>    for (int i = 0; i &lt; ndim; i++) {<br/>        if (tensor1-&gt;shape[i] != tensor2-&gt;shape[i]) {<br/>            fprintf(stderr, "Tensors must have the same shape %d and %d at index %d for addition\n", tensor1-&gt;shape[i], tensor2-&gt;shape[i], i);<br/>            exit(1);<br/>        }<br/>        shape[i] = tensor1-&gt;shape[i];<br/>    }        <br/><br/>    if (strcmp(tensor1-&gt;device, "cuda") == 0) {<br/><br/>        float* result_data;<br/>        cudaMalloc((void **)&amp;result_data, tensor1-&gt;size * sizeof(float));<br/>        add_tensor_cuda(tensor1, tensor2, result_data);<br/>        return create_tensor(result_data, shape, ndim, device);<br/>    } <br/>    else {<br/>        float* result_data = (float*)malloc(tensor1-&gt;size * sizeof(float));<br/>        if (result_data == NULL) {<br/>            fprintf(stderr, "Memory allocation failed\n");<br/>            exit(1);<br/>        }<br/>        add_tensor_cpu(tensor1, tensor2, result_data);<br/>        return create_tensor(result_data, shape, ndim, device);<br/>    }     <br/>}</span></pre><p id="0c79" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now our library has GPU support!</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="42f6" class="pe ne fq pb b bg pf pg l ph pi">import norch<br/><br/>tensor1 = norch.Tensor([[1, 2, 3], [3, 2, 1]]).to("cuda")<br/>tensor2 = norch.Tensor([[3, 2, 1], [1, 2, 3]]).to("cuda")<br/><br/>result = tensor1 + tensor2</span></pre><h1 id="a291" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">#3 — Automatic Differentation (Autograd)</h1><p id="37e6" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">One of the main reasons why PyTorch got so popular is due to its Autograd module. It is a core component that allows automatic differentiation in order to compute gradients (crucial for training models with optimization algorithms such as gradient descent). By calling a single method <code class="cx ps pt pu pb b">.backward()</code>, it computes all of the gradients from previous tensor operations:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="f307" class="pe ne fq pb b bg pf pg l ph pi">x = torch.tensor([[1., 2, 3], [3., 2, 1]], requires_grad=True)<br/># [[1,  2,  3],<br/>#  [3,  2., 1]]<br/><br/>y = torch.tensor([[3., 2, 1], [1., 2, 3]], requires_grad=True)<br/># [[3,  2, 1],<br/>#  [1,  2, 3]]<br/><br/>L = ((x - y) ** 3).sum()<br/><br/>L.backward()<br/><br/># You can access gradients of x and y<br/>print(x.grad)<br/># [[12, 0, 12],<br/>#  [12, 0, 12]]<br/><br/>print(y.grad)<br/># [[-12, 0, -12],<br/>#  [-12, 0, -12]]<br/><br/># In order to minimize z, you can use that for gradient descent:<br/># x = x - learning_rate * x.grad<br/># y = y - learning_rate * y.grad<br/></span></pre><p id="5fb2" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In order to understand what is happening, let’s try to replicate the same procedure manually:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/255a2823dc15242c9d7414c9175d073d.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*i4OX2CgZZ32y68SKm-Q-Yg.png"/></div></figure><p id="9441" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s first calculate:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/3d5353e2d05c964798dfe75e81572db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*quYDTpV1CgLB9XHOdZ8z7w.png"/></div></figure><p id="faaa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Note that <em class="pj">x</em> is a matrix, thus we need to calculate the derivative of L with respect to each element individually. Additionally, L is a summation over all elements, but it is important to remember that for each elements, the other elements does not interfere on its derivative. Therefore, we obtain the following term:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/612ffe8a8a672f3c2ebac55743b1a82c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*sd3rRzDCvm_JOXMGmK9DmQ.png"/></div></figure><p id="b42f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">By applying the chain rule for each term, we differentiate the outer function and multiply by the derivative of the inner function:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/97a8bb17dd8c8e8153ee9eb9d5b77606.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Mjcr2Rz1iQKGkFeFkWgusg.png"/></div></figure><p id="019e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Where:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/0834be1c1e2772168593070107e386de.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*S_9a4Ao28qJu7H3iy6JMog.png"/></div></figure><p id="85dd" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Finally:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qb"><img src="../Images/4dec4240d0004de073e923b6d937831d.png" data-original-src="https://miro.medium.com/v2/resize:fit:264/format:webp/1*wD_9KbkGjVX3bpr_0oRy3A.png"/></div></figure><p id="fe56" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Thus, we have the following final equation to calculate the derivative of L with respect to <em class="pj">x:</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/1f6843db3a0d35b765a74b1241ac61ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*hDowfaP8hY9HZ77XAPEaFw.png"/></div></figure><p id="13a8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Substituting the values into the equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/95897f1570971bf15b239d7182f2b8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*DvvKNaQZ4PXAGrULC5jblg.png"/></div></figure><p id="bfc8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Calculating the result, we get the same values we obtained with PyTorch:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/ac68ee4bb235624b9c9c1c0df4a69ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*hV60yCCG9K4brFJP2xGfKQ.png"/></div></figure><p id="44f6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, let’s analyze what we just did:</p><p id="6c0c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Basically, we observed all the operations involved in reserved order: a summation, a power of 3 and a subtraction. Then, we applied chain of rule, calculating the derivative of each operation and recursively calculated the derivative for the next operation. So, first we need an implementation of the derivative for different math operations:</p><p id="8a98" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For addition:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qf"><img src="../Images/f0bea287a54af0930d57584959d48ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*oHuEsU_8iFtJYRkxw_1ztQ.png"/></div></figure><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e3c8" class="pe ne fq pb b bg pf pg l ph pi"># norch/autograd/functions.py<br/><br/>class AddBackward:<br/>    def __init__(self, x, y):<br/>        self.input = [x, y]<br/><br/>    def backward(self, gradient):<br/>        return [gradient, gradient]</span></pre><p id="e894" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For sin:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/0751c6d99ffe7a0d1b1c359ffbcd68b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*P5FrvOaileaPdS7lVn3JBg.png"/></div></figure><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="7c47" class="pe ne fq pb b bg pf pg l ph pi"># norch/autograd/functions.py<br/><br/>class SinBackward:<br/>    def __init__(self, x):<br/>        self.input = [x]<br/><br/>    def backward(self, gradient):<br/>        x = self.input[0]<br/>        return [x.cos() * gradient]</span></pre><p id="c7cc" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For cosine:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qg"><img src="../Images/4359a9a560ad9072af54cf872019b3bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*guMAvbweIZf7e_7hozEjug.png"/></div></figure><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="5544" class="pe ne fq pb b bg pf pg l ph pi"># norch/autograd/functions.py<br/><br/>class CosBackward:<br/>    def __init__(self, x):<br/>        self.input = [x]<br/><br/>    def backward(self, gradient):<br/>        x = self.input[0]<br/>        return [- x.sin() * gradient]</span></pre><p id="edf1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For element-wise multiplication:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/3564658c2002c0917544fbbef510374c.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*oLWoP34sORaMMrvnME0zuw.png"/></div></figure><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="d2e9" class="pe ne fq pb b bg pf pg l ph pi"># norch/autograd/functions.py<br/><br/>class ElementwiseMulBackward:<br/>    def __init__(self, x, y):<br/>        self.input = [x, y]<br/><br/>    def backward(self, gradient):<br/>        x = self.input[0]<br/>        y = self.input[1]<br/>        return [y * gradient, x * gradient]</span></pre><p id="31cf" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For summation:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="96cf" class="pe ne fq pb b bg pf pg l ph pi"># norch/autograd/functions.py<br/><br/>class SumBackward:<br/>    def __init__(self, x):<br/>        self.input = [x]<br/><br/>    def backward(self, gradient):<br/>        # Since sum reduces a tensor to a scalar, gradient is broadcasted to match the original shape.<br/>        return [float(gradient.tensor.contents.data[0]) * self.input[0].ones_like()]<br/></span></pre><p id="e526" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can access the GitHub repository link at the end of the article to explore other operations.</p><p id="7925" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now that we have derivative expressions for each operation, we can proceed with implementing the recursive backward chain rule. We can set a <code class="cx ps pt pu pb b">requires_grad</code> argument for our tensor to indicate that we want to store the gradients of this tensor. If true, we will store the gradients for each tensor operation. For instance:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="d4de" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>def __add__(self, other):<br/>    <br/>  if self.shape != other.shape:<br/>      raise ValueError("Tensors must have the same shape for addition")<br/>  <br/>  Tensor._C.add_tensor.argtypes = [ctypes.POINTER(CTensor), ctypes.POINTER(CTensor)]<br/>  Tensor._C.add_tensor.restype = ctypes.POINTER(CTensor)<br/>  <br/>  result_tensor_ptr = Tensor._C.add_tensor(self.tensor, other.tensor)<br/>  <br/>  result_data = Tensor()<br/>  result_data.tensor = result_tensor_ptr<br/>  result_data.shape = self.shape.copy()<br/>  result_data.ndim = self.ndim<br/>  result_data.device = self.device<br/>  <br/>  result_data.requires_grad = self.requires_grad or other.requires_grad<br/>  if result_data.requires_grad:<br/>      result_data.grad_fn = AddBackward(self, other)</span></pre><p id="953a" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Then, implement the <code class="cx ps pt pu pb b">.backward()</code> method:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="fe3d" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>def backward(self, gradient=None):<br/>    if not self.requires_grad:<br/>        return<br/>    <br/>    if gradient is None:<br/>        if self.shape == [1]:<br/>            gradient = Tensor([1]) # dx/dx = 1 case<br/>        else:<br/>            raise RuntimeError("Gradient argument must be specified for non-scalar tensors.")<br/><br/>    if self.grad is None:<br/>        self.grad = gradient<br/><br/>    else:<br/>        self.grad += gradient<br/><br/>    if self.grad_fn is not None: # not a leaf<br/>        grads = self.grad_fn.backward(gradient) # call the operation backward<br/>        for tensor, grad in zip(self.grad_fn.input, grads):<br/>            if isinstance(tensor, Tensor):<br/>                tensor.backward(grad) # recursively call the backward again for the gradient expression (chain rule)</span></pre><p id="d076" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Finally, just implement <code class="cx ps pt pu pb b">.zero_grad()</code> to zero the gradient of a tensor, and <code class="cx ps pt pu pb b">.detach()</code> to remove the tensor autograd history:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="3d12" class="pe ne fq pb b bg pf pg l ph pi"># norch/tensor.py<br/><br/>def zero_grad(self):<br/>    self.grad = None<br/><br/>def detach(self):<br/>    self.grad = None<br/>    self.grad_fn = None</span></pre><p id="30c4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Congratulations! You just created a complete tensor library with GPU support and automatic differentiation! Now we can create the <code class="cx ps pt pu pb b">nn</code> and <code class="cx ps pt pu pb b">optim</code> modules to train some deep learning models more easily.</p><h1 id="6aca" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">#4 — nn and optim modules</h1><p id="12b4" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The <code class="cx ps pt pu pb b">nn</code> is a module for building neural networks and deep learning models and <code class="cx ps pt pu pb b">optim</code> is related to optimization algorithms to train these models. In order to recreate them, the first thing to do is implementing a Parameter, which simply is a trainable tensor, with the same operations, but with <code class="cx ps pt pu pb b">requires_grad</code> set always as <code class="cx ps pt pu pb b">True</code> and with some random initialization technique.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="d6aa" class="pe ne fq pb b bg pf pg l ph pi"># norch/nn/parameter.py<br/><br/>from norch.tensor import Tensor<br/>from norch.utils import utils<br/>import random<br/><br/>class Parameter(Tensor):<br/>    """<br/>    A parameter is a trainable tensor.<br/>    """<br/>    def __init__(self, shape):<br/>        data = utils.generate_random_list(shape=shape)<br/>        super().__init__(data, requires_grad=True)</span></pre><pre class="pv pa pb pc bp pd bb bk"><span id="ffab" class="pe ne fq pb b bg pf pg l ph pi"># norch/utisl/utils.py<br/><br/>def generate_random_list(shape):<br/>    """<br/>    Generate a list with random numbers and shape 'shape'<br/>    [4, 2] --&gt; [[rand1, rand2], [rand3, rand4], [rand5, rand6], [rand7, rand8]]<br/>    """<br/>    if len(shape) == 0:<br/>        return []<br/>    else:<br/>        inner_shape = shape[1:]<br/>        if len(inner_shape) == 0:<br/>            return [random.uniform(-1, 1) for _ in range(shape[0])]<br/>        else:<br/>            return [generate_random_list(inner_shape) for _ in range(shape[0])]</span></pre><p id="36f9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">By using parameters, we can start contructing modules:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="e3a6" class="pe ne fq pb b bg pf pg l ph pi"># norch/nn/module.py<br/><br/>from .parameter import Parameter<br/>from collections import OrderedDict<br/>from abc import ABC<br/>import inspect<br/><br/>class Module(ABC):<br/>    """<br/>    Abstract class for modules<br/>    """<br/>    def __init__(self):<br/>        self._modules = OrderedDict()<br/>        self._params = OrderedDict()<br/>        self._grads = OrderedDict()<br/>        self.training = True<br/><br/>    def forward(self, *inputs, **kwargs):<br/>        raise NotImplementedError<br/><br/>    def __call__(self, *inputs, **kwargs):<br/>        return self.forward(*inputs, **kwargs)<br/><br/>    def train(self):<br/>        self.training = True<br/>        for param in self.parameters():<br/>            param.requires_grad = True<br/><br/>    def eval(self):<br/>        self.training = False<br/>        for param in self.parameters():<br/>            param.requires_grad = False<br/><br/>    def parameters(self):<br/>        for name, value in inspect.getmembers(self):<br/>            if isinstance(value, Parameter):<br/>                yield self, name, value<br/>            elif isinstance(value, Module):<br/>                yield from value.parameters()<br/><br/>    def modules(self):<br/>        yield from self._modules.values()<br/><br/>    def gradients(self):<br/>        for module in self.modules():<br/>            yield module._grads<br/><br/>    def zero_grad(self):<br/>        for _, _, parameter in self.parameters():<br/>            parameter.zero_grad()<br/><br/>    def to(self, device):<br/>        for _, _, parameter in self.parameters():<br/>            parameter.to(device)<br/><br/>        return self<br/>    <br/>    def inner_repr(self):<br/>        return ""<br/><br/>    def __repr__(self):<br/>        string = f"{self.get_name()}("<br/>        tab = "   "<br/>        modules = self._modules<br/>        if modules == {}:<br/>            string += f'\n{tab}(parameters): {self.inner_repr()}'<br/>        else:<br/>            for key, module in modules.items():<br/>                string += f"\n{tab}({key}): {module.get_name()}({module.inner_repr()})"<br/>        return f'{string}\n)'<br/>    <br/>    def get_name(self):<br/>        return self.__class__.__name__<br/>    <br/>    def __setattr__(self, key, value):<br/>        self.__dict__[key] = value<br/><br/>        if isinstance(value, Module):<br/>            self._modules[key] = value<br/>        elif isinstance(value, Parameter):<br/>            self._params[key] = value</span></pre><p id="1607" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">For example, we can construct our custom modules by inheriting from <code class="cx ps pt pu pb b">nn.Module</code>, or we can use some previously created modules, such as the <code class="cx ps pt pu pb b">linear</code>, which implements the <strong class="ob fr"><em class="pj">y</em></strong><em class="pj"> = W</em><strong class="ob fr"><em class="pj">x </em></strong><em class="pj">+ b </em>operation.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="b316" class="pe ne fq pb b bg pf pg l ph pi"># norch/nn/modules/linear.py<br/><br/>from ..module import Module<br/>from ..parameter import Parameter<br/><br/>class Linear(Module):<br/>    def __init__(self, input_dim, output_dim):<br/>        super().__init__()<br/>        self.input_dim = input_dim<br/>        self.output_dim = output_dim<br/>        self.weight = Parameter(shape=[self.output_dim, self.input_dim])<br/>        self.bias = Parameter(shape=[self.output_dim, 1])<br/><br/>    def forward(self, x):<br/>        z = self.weight @ x + self.bias<br/>        return z<br/><br/>    def inner_repr(self):<br/>        return f"input_dim={self.input_dim}, output_dim={self.output_dim}, " \<br/>               f"bias={True if self.bias is not None else False}"</span></pre><p id="695d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now we can implement some loss and activation functions. For instance, a mean squared error loss and a sigmoid function:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="4671" class="pe ne fq pb b bg pf pg l ph pi"># norch/nn/loss.py<br/><br/>from .module import Module<br/> <br/>class MSELoss(Module):<br/>    def __init__(self):<br/>      pass<br/><br/>    def forward(self, predictions, labels):<br/>        assert labels.shape == predictions.shape, \<br/>            "Labels and predictions shape does not match: {} and {}".format(labels.shape, predictions.shape)<br/>        <br/>        return ((predictions - labels) ** 2).sum() / predictions.numel<br/><br/>    def __call__(self, *inputs):<br/>        return self.forward(*inputs)</span></pre><pre class="pv pa pb pc bp pd bb bk"><span id="4cb0" class="pe ne fq pb b bg pf pg l ph pi"># norch/nn/activation.py<br/><br/>from .module import Module<br/>import math<br/><br/>class Sigmoid(Module):<br/>    def __init__(self):<br/>        super().__init__()<br/><br/>    def forward(self, x):<br/>        return 1.0 / (1.0 + (math.e) ** (-x)) </span></pre><p id="7399" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Finally, create the optimizers. On our example I will implement the Stochastic Gradient Descent algorithm:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="6408" class="pe ne fq pb b bg pf pg l ph pi"># norch/optim/optimizer.py<br/><br/>from abc import ABC<br/>from norch.tensor import Tensor<br/><br/>class Optimizer(ABC):<br/>    """<br/>    Abstract class for optimizers<br/>    """<br/><br/>    def __init__(self, parameters):<br/>        if isinstance(parameters, Tensor):<br/>            raise TypeError("parameters should be an iterable but got {}".format(type(parameters)))<br/>        elif isinstance(parameters, dict):<br/>            parameters = parameters.values()<br/><br/>        self.parameters = list(parameters)<br/><br/>    def step(self):<br/>        raise NotImplementedError<br/>    <br/>    def zero_grad(self):<br/>        for module, name, parameter in self.parameters:<br/>            parameter.zero_grad()<br/><br/><br/>class SGD(Optimizer):<br/>    def __init__(self, parameters, lr=1e-1, momentum=0):<br/>        super().__init__(parameters)<br/>        self.lr = lr<br/>        self.momentum = momentum<br/>        self._cache = {'velocity': [p.zeros_like() for (_, _, p) in self.parameters]}<br/><br/>    def step(self):<br/>        for i, (module, name, _) in enumerate(self.parameters):<br/>            parameter = getattr(module, name)<br/><br/>            velocity = self._cache['velocity'][i]<br/><br/>            velocity = self.momentum * velocity - self.lr * parameter.grad<br/><br/>            updated_parameter = parameter + velocity<br/><br/>            setattr(module, name, updated_parameter)<br/><br/>            self._cache['velocity'][i] = velocity<br/><br/>            parameter.detach()<br/>            velocity.detach()</span></pre><p id="a52b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">And, that’s it! We just created our own deep learning framework! 🥳</p><p id="9172" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Let’s do some training:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="ebdb" class="pe ne fq pb b bg pf pg l ph pi">import norch<br/>import norch.nn as nn<br/>import norch.optim as optim<br/>import random<br/>import math<br/><br/>random.seed(1)<br/><br/>class MyModel(nn.Module):<br/>    def __init__(self):<br/>        super(MyModel, self).__init__()<br/>        self.fc1 = nn.Linear(1, 10)<br/>        self.sigmoid = nn.Sigmoid()<br/>        self.fc2 = nn.Linear(10, 1)<br/><br/>    def forward(self, x):<br/>        out = self.fc1(x)<br/>        out = self.sigmoid(out)<br/>        out = self.fc2(out)<br/>        <br/>        return out<br/><br/>device = "cuda"<br/>epochs = 10<br/><br/>model = MyModel().to(device)<br/>criterion = nn.MSELoss()<br/>optimizer = optim.SGD(model.parameters(), lr=0.001)<br/>loss_list = []<br/><br/>x_values = [0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ,<br/>        4.4,  4.8,  5.2,  5.6,  6. ,  6.4,  6.8,  7.2,  7.6,  8. ,  8.4,<br/>        8.8,  9.2,  9.6, 10. , 10.4, 10.8, 11.2, 11.6, 12. , 12.4, 12.8,<br/>       13.2, 13.6, 14. , 14.4, 14.8, 15.2, 15.6, 16. , 16.4, 16.8, 17.2,<br/>       17.6, 18. , 18.4, 18.8, 19.2, 19.6, 20.]<br/><br/>y_true = []<br/>for x in x_values:<br/>    y_true.append(math.pow(math.sin(x), 2))<br/><br/><br/>for epoch in range(epochs):<br/>    for x, target in zip(x_values, y_true):<br/>        x = norch.Tensor([[x]]).T<br/>        target = norch.Tensor([[target]]).T<br/><br/>        x = x.to(device)<br/>        target = target.to(device)<br/><br/>        outputs = model(x)<br/>        loss = criterion(outputs, target)<br/>        <br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss[0]:.4f}')<br/>    loss_list.append(loss[0])<br/><br/># Epoch [1/10], Loss: 1.7035<br/># Epoch [2/10], Loss: 0.7193<br/># Epoch [3/10], Loss: 0.3068<br/># Epoch [4/10], Loss: 0.1742<br/># Epoch [5/10], Loss: 0.1342<br/># Epoch [6/10], Loss: 0.1232<br/># Epoch [7/10], Loss: 0.1220<br/># Epoch [8/10], Loss: 0.1241<br/># Epoch [9/10], Loss: 0.1270<br/># Epoch [10/10], Loss: 0.1297</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qi"><img src="../Images/e7662c657102cedd3717cdac495019f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*qWcKDOLIVb5Hh5obBmjk0A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="5068" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The model was successfully created and trained using our custom deep learning framework!</p><p id="fc9f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">You can check the complete code <a class="af nc" href="https://github.com/lucasdelimanogueira/PyNorch" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h1 id="ece8" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Conclusion</h1><p id="66ac" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this post we covered the basic concepts of what is a tensor, how it is modeled and more advanced topics such as CUDA and Autograd. We successfully created a deep learning framework with GPU support and automatic differentiation. I hope this post helped you to briefly understand how PyTorch works under the hood.</p><p id="6011" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In future posts, I will try to cover more advanced topics such as distributed training (multinode / multi GPU) and memory management. Please let me know what you think or what you would like me to write about next in the comments! Thanks so much for reading! 😊</p><p id="3b4f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Also, follow me here and on my <a class="af nc" href="https://www.linkedin.com/in/lucas-de-lima-nogueira/" rel="noopener ugc nofollow" target="_blank">LinkedIn profile</a> to stay updated on my latest articles!</p><h1 id="1ab0" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">References</h1><p id="645e" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><a class="af nc" href="https://github.com/lucasdelimanogueira/PyNorch" rel="noopener ugc nofollow" target="_blank">PyNorch</a> — GitHub repository of this project.</p><p id="55d4" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://medium.com/towards-data-science/why-deep-learning-models-run-faster-on-gpus-a-brief-introduction-to-cuda-programming-035272906d66" rel="noopener">Tutorial CUDA</a> — A brief introduction on how CUDA works.</p><p id="26a3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank">PyTorch</a> — PyTorch documentation.</p><p id="669e" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://martinlwx.github.io/en/how-to-reprensent-a-tensor-or-ndarray/" rel="noopener ugc nofollow" target="_blank">MartinLwx’s blog</a> — Tutorial on strides.</p><p id="352f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://ajcr.net/stride-guide-part-1/" rel="noopener ugc nofollow" target="_blank">Strides tutorial</a> — Another tutorial about strides.</p><p id="59f3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="http://blog.ezyang.com/2019/05/pytorch-internals/" rel="noopener ugc nofollow" target="_blank">PyTorch internals</a> — A guide on how PyTorch is structured.</p><p id="138b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://github.com/arthurdjn/nets" rel="noopener ugc nofollow" target="_blank">Nets</a> — A PyTorch recreation using NumPy.</p><p id="6994" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><a class="af nc" href="https://www.youtube.com/watch?v=RxmBukb-Om4&amp;t=935s" rel="noopener ugc nofollow" target="_blank">Autograd</a> — A live coding of an Autograd library.</p></div></div></div></div>    
</body>
</html>