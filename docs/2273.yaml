- en: The Math Behind Kernel Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-kernel-density-estimation-5deca75cba38?source=collection_archive---------1-----------------------#2024-09-17](https://towardsdatascience.com/the-math-behind-kernel-density-estimation-5deca75cba38?source=collection_archive---------1-----------------------#2024-09-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the foundations, concepts, and math of kernel density estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@z.nay854?source=post_page---byline--5deca75cba38--------------------------------)[![Zackary
    Nay](../Images/b2eca451f39b1227ba868befe969f4ff.png)](https://medium.com/@z.nay854?source=post_page---byline--5deca75cba38--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5deca75cba38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5deca75cba38--------------------------------)
    [Zackary Nay](https://medium.com/@z.nay854?source=post_page---byline--5deca75cba38--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5deca75cba38--------------------------------)
    ·14 min read·Sep 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b02965740dfdbe0f77e416a5b6eb7e3.png)'
  prefs: []
  type: TYPE_IMG
- en: The Kernel Density Estimator is a fundamental non-parametric method that is
    a versatile tool for uncovering the hidden distributions of your data. This article
    delves into the mathematical foundations of the estimator, provides guidance on
    choosing the optimal bandwidth, and briefly touches on the choice of kernel functions
    and other related topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 1: Introduction**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose I give you the following sample of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d7b5704844ecc01c61ef6440a044011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the first and easiest steps in analyzing sample data is to generate
    a histogram, for our data we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52fdb96422c868e48af5ae2cfcb9a638.png)'
  prefs: []
  type: TYPE_IMG
- en: Not very useful, and we are no closer to understanding what the underlying distribution
    is. There is also the additional problem that, in practice, data rarely exhibit
    the sharp rectangular structure produced by a histogram. The kernel density estimator
    provides a remedy, and it is a non-parametric way to estimate the probability
    density function of a random variable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/815e4c24ba9ea67563e2a705b1e06d04.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Part 2: Derivation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following derivation takes inspiration from Bruce E. Hansen’s “Lecture Notes
    on Nonparametric” (2009). If you are interested in learning more you can refer
    to his original lecture notes [here](https://users.ssc.wisc.edu/~bhansen/718/NonParametrics1.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we wanted to estimate a probability density function, f(t), from a
    sample of data. A good starting place would be to estimate the cumulative distribution
    function, F(t), using the [empirical distribution function](https://en.wikipedia.org/wiki/Empirical_distribution_function#:~:text=The%20empirical%20distribution%20function%20is,to%20the%20Glivenko%E2%80%93Cantelli%20theorem.)
    (EDF). Let X1, …, Xn be independent, identically distributed real random variables
    with the common cumulative distribution function F(t). The EDF is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/124f9bd1a658b206a5279dfd27173ddd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, by the strong law of large numbers, as n approaches infinity, the EDF
    converges almost surely to F(t). Now, the EDF is a step function that could look
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8c9f6aac2ad35cb70daede61f52be29.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, if we were to try and find an estimator for f(t) by taking the derivative
    of the EDF, we would get a scaled sum of [Dirac delta functions](https://en.wikipedia.org/wiki/Dirac_delta_function),
    which is not very helpful. Instead let us consider using the t[wo-point central
    difference formula](https://home.cc.umanitoba.ca/~farhadi/Math2120/Numerical%20Differentiation.pdf)
    of the estimator as an approximation of the derivative. Which, for a small h>0,
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/59ae71a5b9101204a61466fda298a3b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now define the function k(u) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39d48ca6e7d2b0c4db071cd0eaddc43c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we have that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/130318df46d4d42879bfb8b4ef0d52e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Which is a special case of the kernel density estimator, where here k is the
    uniform kernel function. More generally, a kernel function is a non-negative function
    from the reals to the reals which satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d490e3f1e63f3b07459eb3f97598cde8.png)'
  prefs: []
  type: TYPE_IMG
- en: We will assume that all kernels discussed in this article are symmetric, hence
    we have that k(-u) = k(u).
  prefs: []
  type: TYPE_NORMAL
- en: 'The moment of a kernel, which gives insights into the shape and behavior of
    the kernel function, is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a624958e05b0beb451fdc6b1e39836a.png)'
  prefs: []
  type: TYPE_IMG
- en: Lastly, the order of a kernel is defined as the first non-zero moment.
  prefs: []
  type: TYPE_NORMAL
- en: We can only minimize the error of the kernel density estimator by either changing
    the h value (bandwidth), or the kernel function. The bandwidth parameter has a
    much larger impact on the resulting estimate than the kernel function but is also
    much more difficult to choose. To demonstrate the influence of the h value, take
    the following two kernel density estimates. A Gaussian kernel was used to estimate
    a sample generated from a standard normal distribution, the only difference between
    the estimators is the chosen h value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8202c10fcb27e1b851f069889558d274.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Quite a dramatic difference.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us look at the impact of changing the kernel function while keeping
    the bandwidth constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c99738f2f355e32578811f64b2dd879c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While visually there is a large difference in the tails, the overall shape of
    the estimators are similar across the different kernel functions. Therefore, I
    will focus primarily focus on finding the optimal bandwidth for the estimator.
    Now, let’s explore some of the properties of the kernel density estimator, including
    its bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 3: Properties of the Kernel Density Estimator**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first fact that we will need to utilize is that the integral of the estimator
    across the real line is 1\. To prove this fact, we will need to make use of the
    change of u-substitution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e33cf2e2049c29aca18e8a093ca66c13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Employing that u-substitution we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f3a57706737f576377668a492d07d83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can find the mean of the estimated density:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed503405029561f4a7976cd6a585ae03.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the mean of the estimated density is simply the sample mean.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us find the second moment of the estimator.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfe471ec7996859cc904c593b9a23c86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can then find the variance of the estimated density:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c822ab90eee6dff0793fa3b7c7ddaa8.png)'
  prefs: []
  type: TYPE_IMG
- en: To find the optimal bandwidth and kernel, we will be minimizing the mean squared
    error of the estimator. To achieve this, we will first need to find the bias and
    variance of the estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The expected value of a random variable, X, with a probability density function
    of f, can be calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1675a80c94a4c62e844235a9ee73b25f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3f155d13144fd99e7f129ce6610dc563.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where z is a dummy variable of integration. We can then use a change of variables
    to get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0a469d13dbaa87efedfbe3e45fac73f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33a5697549c9c8fc7aef8cc5cc5ac786.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Most of the time, however, the integral above is not analytically solvable.
    Therefore, we will have to approximate f(x+ hu) by using its Taylor expansion.
    As a reminder, the Taylor expansion of f(x) around a is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e21d33426eddb14757b89477692c5e9c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, assume f(x) is differentiable to the v+1 term. For f(x+hu) the expansion
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0e29e10c63fa4e7688400f1b90e4c1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then for a v-order kernel, we can take the expression out to the v’th term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e062240689da239fe38625a7e2a75c84.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where the last term is the remainder, which vanishes faster than h as h approaches
    0\. Now, assuming that k is a v’th order kernel function, we have that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7854ca857959f4e956f84b66bff38be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9618d12cb67889aa94553603e126194a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus we have that the bias of the estimator is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aeab30eaee8173e839d4b93f05452fbf.png)'
  prefs: []
  type: TYPE_IMG
- en: The upper bound for the variance can be obtained via the following calculation
    [[Chen 2].](https://faculty.washington.edu/yenchic/17Sp_403/Lec7-density.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4082e2eee9c96f468b9e2bd8e068cdae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/349e7cdc90db769123596f25b306f7f0.png)'
  prefs: []
  type: TYPE_IMG
- en: This term is also known as the roughness and can be denoted as R(k).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can get an expression for the mean squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c1d3c9c4559f6512c1fd4194a06d4ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Where AMSE is short for asymptotic mean squared error. We can then minimize
    the asymptotic mean integrated square error defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02ada6a1f661f96e4fbc20bc99bbf909.png)'
  prefs: []
  type: TYPE_IMG
- en: 'to find what bandwidth will lead to the lowest error (Silverman, 1986):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c473fd6806cfa8781ba6b8a6a67c3ebb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, by setting the equation to 0 and simplifying we find that the optimal
    bandwidth for a kernel of order v is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b95e30c3665889767077d44b94e3e699.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The more familiar expression is for second order kernels, where the bandwidth
    that minimizes the AMISE is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53599fe5eeb322d7a7b1003ac5a7276d.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this solution may be a letdown as we require knowing the distribution
    that we are estimating to find the optimal bandwidth. In practice, we would not
    have this distribution if we were using the kernel density estimator in the first
    place.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 4: Bandwidth Selection**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Despite not being able to find the bandwidth that minimizes the mean integrated
    square error, there are several methods available to choose a bandwidth without
    knowing the underlying distribution. **It is important to note, however, that
    a larger h value will cause your estimator to have less variance but greater bias,
    while a smaller bandwidth will produce a rougher estimate with less bias (Eidous
    et al., 2010).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Some methods to find a bandwidth include using:'
  prefs: []
  type: TYPE_NORMAL
- en: '1: The Solve-the-Equation Rule'
  prefs: []
  type: TYPE_NORMAL
- en: '2: Least Squares Cross-Validation'
  prefs: []
  type: TYPE_NORMAL
- en: '3: Biased Cross-Validation'
  prefs: []
  type: TYPE_NORMAL
- en: '4: The Direct Plug in Method'
  prefs: []
  type: TYPE_NORMAL
- en: '5: The Contrast Method'
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that depending on the sample size and skewness of your
    data, the ‘best’ method to choose your bandwidth changes. Most packages in Python
    allow you to use Silverman's proposed method, where you directly plug in some
    distribution (typically normal) for f and then compute the bandwidth based upon
    the kernel function that you have chosen. This procedure, known as **Silverman’s
    Rule of Thumb,** provides a relatively simple estimate for the bandwidth. However,
    it often tends to overestimate, resulting in a smoother estimate with lower variance
    but higher bias. Silverman’s Rule of Thumb also specifically does not perform
    well for bimodal distributions, and there are more accurate techniques available
    for those cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you assume that your data is normally distributed with a mean of 0, use
    the sample standard deviation, and apply the Epanechnikov kernel (discussed below),
    you can select the bandwidth using the Rule of Thumb via the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15398a17b0be9cacc757582fe73205c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Eidous et al. found the Contrast Method to have the best performance compared
    to the other methods I listed. However, this method has drawbacks, as it increases
    the number of parameters to be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: The cross validation method is another good choice in bandwidth selection method,
    as it often leads to a small bias but a large variance (Heidenreich et al, 2013).
    It is most useful for when you are looking for a bandwidth selector that tends
    to undershoot the optimal bandwidth. To keep this article not overly long I will
    only be going over the Ordinary Least Squares Cross Validation method. The method
    tends to work well for rather wiggly densities and a moderate sample size of around
    50 to 100\. If you have a very small or large sample size this [paper](https://core.ac.uk/download/pdf/159147961.pdf)
    is a good resource to find another way to choose your bandwidth. As pointed out
    by Heidenreich, “**it definitely makes a difference which bandwidth selector is
    chosen; not only in numerical terms but also for the quality of density estimation**”.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick refresher, when we are creating a model we reserve some of our sample
    as a validation set to see how our model performs on the sample that it has not
    been trained on. K Fold Cross Validation minimizes the impact of possibly selecting
    a test set that misrepresents the dataset by splitting the dataset into K parts
    and then training the model K times and allowing each part to be the validation
    set once.
  prefs: []
  type: TYPE_NORMAL
- en: Leave One Out Cross Validation is K Fold Cross Validation taken to the extreme,
    where for a sample size of n, we train the model n times and leave out one sample
    each time. [Here](https://machinelearningmastery.com/loocv-for-evaluating-machine-learning-algorithms/)
    is an article that goes more in-depth into the method in the context of training
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us turn back to the AMISE expression. Instead of investigating the asymptotic
    mean integrated square error, we will minimize the mean integrated square error
    (MISE). First, we can expand the square:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d07064b850466ae87e1c1924a01c17d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have no control over the expected value of the last term, we can seek
    to minimize the first two terms and drop the third. Then, because X is an unbiased
    estimator for E[X], we can find the first term directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e82bb063a7a2eccf4342d6c403d000a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the convolution of k with k is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57fa4edb38fda6db5b996091bd9dd4df.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bba7957ebdfc234384c04e6ebd32959b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, for the first term, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dfba3c5255b22e2d8c8b1b0a13996df6.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we can approximate the second term by Monte Carlo methods. First, as discussed
    earlier, the density function is equivalent to the derivative of the cumulative
    distribution function, which we can approximate via the empirical distribution
    function. Then, the integral can be approximated by the average value of the estimator
    over the [sample](https://jwmi.github.io/BMS/chapter5-monte-carlo.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f96f9260cac17a62a41b9d40be4bb682.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then the least squares cross validation selector (LSCV) is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4fc33d8d1ce23ecd2bd8b275c7012fc.png)![](../Images/f372f058f5adfa4cc5fd14285e1ffcbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then get the final selector defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d69b80d31ae868cc3d74514a1728103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The optimal bandwidth is the h value that minimizes LSCV, defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3088842523b4847cd1a05dd2477eb061.png)'
  prefs: []
  type: TYPE_IMG
- en: The LSCV(h) function can have multiple local minimums, hence the optimal bandwidth
    that you find can be sensitive to the interval chosen. It is useful to graph the
    function and then visually investigate where the global minimum lies.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 5: Optimal Kernel Selection**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we are working with a second order kernel (which is typical), the choice
    in kernel selection is much more straightforward than bandwidth. **Under the criteria
    of minimizing the AMISE, the Epanechnikov** **kernel is an optimal kernel.** The
    full proof can be found in this paper by [Muler](https://projecteuclid.org/journals/annals-of-statistics/volume-12/issue-2/Smooth-Optimum-Kernel-Estimators-of-Densities-Regression-Curves-and-Modes/10.1214/aos/1176346523.full).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfe9062c4a6e3b26a7b94a474d190507.png)'
  prefs: []
  type: TYPE_IMG
- en: There are other kernels which are as efficient as the Epanechnikov kernel, which
    are also touched on in Muler’s paper. However, I wouldn’t worry too much about
    your choice of kernel function, the choice in bandwidth is much more important.
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 6: Further Topics and Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Adaptive Bandwidths
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the proposed ways to improve the Kernel Density Estimator is through
    an adaptive bandwidth. An adaptive bandwidth adjusts the bandwidth for each data
    point, increasing it when the density of the sample data is lower and decreasing
    it when the density is higher. While promising in theory, an adaptive bandwidth
    has been shown to perform quite poorly in the univariate case ([Terrel, Scott
    1992](https://projecteuclid.org/journals/annals-of-statistics/volume-20/issue-3/Variable-Kernel-Density-Estimation/10.1214/aos/1176348768.full)).
    While it may be better for larger dimensional spaces, for the one-dimensional
    case I believe it is best to stick with a constant bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate Kernel Density Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The kernel density estimator can also be extended to higher dimensions, where
    the kernel is a radial basis function or is a product of multiple kernel functions.
    The approach does suffer from the curse of dimensionality, where as the dimension
    grows larger the number of data points needed to produce a useful estimator grows
    exponentially. It is also computationally expensive and is not a great method
    for analyzing high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Nonparametric multivariate density estimation is still a very active field,
    with [Masked Autoregressive Flow](https://arxiv.org/abs/1705.07057) appearing
    to be quite a new and promising approach.
  prefs: []
  type: TYPE_NORMAL
- en: Real World Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kernel density estimation has numerous applications across disciplines. First,
    it has been shown to improve machine learning algorithms such as in the case of
    the flexible naive Bayes classifier.
  prefs: []
  type: TYPE_NORMAL
- en: It has also been used to estimate [traffic accident density](https://www.sciencedirect.com/science/article/pii/S2095756415305808).
    The linked paper uses the KDE to help make a model that indicates the risk of
    traffic accidents in different cities across Japan.
  prefs: []
  type: TYPE_NORMAL
- en: Another fun use is in [seismolog](https://pubs.geoscienceworld.org/ssa/bssa/article-abstract/86/2/353/120015/Kernel-estimation-methods-for-seismic-hazard-area)y,
    where the KDE has been used for modelling the risk of earthquakes in different
    locations.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The kernel density estimator is an excellent addition to the data analysts’
    toolbox. While a histogram is certainly a fine way of analyzing data without using
    any underlying assumptions, the kernel density estimator provides a solid alternative
    for univariate data. For higher dimensional data, or for when computational time
    is a concern, I would recommend looking elsewhere. Nonetheless, the KDE is an
    intuitive, powerful and versatile tool in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unless otherwise noted, all images are by the author.*'
  prefs: []
  type: TYPE_NORMAL
