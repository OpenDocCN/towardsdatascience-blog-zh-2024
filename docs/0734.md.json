["```py\ndef parse_dataset(file_path):\n    \"\"\"\n    Parse the BioCreative Dataset.\n\n    Args:\n    - file_path (str): Path to the file containing the documents.\n\n    Returns:\n    - list of dict: A list where each element is a dictionary representing a document.\n    \"\"\"\n    documents = []\n    current_doc = None\n\n    with open(file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            line = line.strip()\n            if not line:\n                continue\n            if \"|t|\" in line:\n                if current_doc:\n                    documents.append(current_doc)\n                id_, title = line.split(\"|t|\", 1)\n                current_doc = {'id': id_, 'title': title, 'abstract': '', 'annotations': []}\n            elif \"|a|\" in line:\n                _, abstract = line.split(\"|a|\", 1)\n                current_doc['abstract'] = abstract\n            else:\n                parts = line.split(\"\\t\")\n                if parts[1] == \"CID\":\n                    continue\n                annotation = {\n                    'text': parts[3],\n                    'type': parts[4],\n                    'identifier': parts[5]\n                }\n                current_doc['annotations'].append(annotation)\n\n        if current_doc:\n            documents.append(current_doc)\n\n    return documents\n\ndef deduplicate_annotations(documents):\n    \"\"\"\n    Filter documents to ensure annotation consistency.\n\n    Args:\n    - documents (list of dict): The list of documents to be checked.\n    \"\"\"\n    for doc in documents:\n        doc[\"annotations\"] = remove_duplicates(doc[\"annotations\"])\n\ndef remove_duplicates(dict_list):\n    \"\"\"\n    Remove duplicate dictionaries from a list of dictionaries.\n\n    Args:\n    - dict_list (list of dict): A list of dictionaries from which duplicates are to be removed.\n\n    Returns:\n    - list of dict: A list of dictionaries after removing duplicates.\n    \"\"\"\n    unique_dicts = []  \n    seen = set()\n\n    for d in dict_list:\n        dict_tuple = tuple(sorted(d.items()))\n        if dict_tuple not in seen:\n            seen.add(dict_tuple)\n            unique_dicts.append(d)\n\n    return unique_dicts\n```", "```py\ndef calculate_entity_metrics(gt, pred):\n    \"\"\"\n    Calculate precision, recall, and F1-score for entity recognition.\n\n    Args:\n    - gt (list of dict): A list of dictionaries representing the ground truth entities. \n                         Each dictionary should have a key \"text\" with the entity text.\n    - pred (list of dict): A list of dictionaries representing the predicted entities.\n                           Similar to `gt`, each dictionary should have a key \"text\".\n\n    Returns:\n    tuple: A tuple containing precision, recall, and F1-score (in that order).\n    \"\"\"\n    ground_truth_set = set([x[\"text\"].lower() for x in gt])\n    predicted_set = set([x[\"text\"].lower() for x in pred])\n\n    # True positives are predicted items that are in the ground truth\n    true_positives = len(predicted_set.intersection(ground_truth_set))\n\n    # Precision calculation\n    if len(predicted_set) == 0:\n        precision = 0\n    else:\n        precision = true_positives / len(predicted_set)\n\n    # Recall calculation\n    if len(ground_truth_set) == 0:\n        recall = 0\n    else:\n        recall = true_positives / len(ground_truth_set)\n\n    # F1-score calculation\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n\n    return precision, recall, f1_score\n\ndef calculate_mesh_metrics(gt, pred):\n    \"\"\"\n    Calculate precision, recall, and F1-score for matching MeSH (Medical Subject Headings) codes.\n\n    Args:\n    - gt (list of dict): Ground truth data\n    - pred (list of dict): Predicted data\n\n    Returns:\n    tuple: A tuple containing precision, recall, and F1-score (in that order).\n    \"\"\"\n    ground_truth = []\n\n    for item in gt:\n        mesh_codes = item[\"identifier\"]\n        if mesh_codes == \"-1\":\n            mesh_codes = \"None\"\n        mesh_codes_split = mesh_codes.split(\"|\")\n        for elem in mesh_codes_split:\n            combined_elem = {\"entity\": item[\"text\"].lower(), \"identifier\": elem}\n            if combined_elem not in ground_truth:\n                ground_truth.append(combined_elem)\n\n    predicted = []\n    for item in pred:\n        mesh_codes = item[\"identifier\"]\n        mesh_codes_split = mesh_codes.strip().split(\"|\")\n        for elem in mesh_codes_split:\n            combined_elem = {\"entity\": item[\"text\"].lower(), \"identifier\": elem}\n            if combined_elem not in predicted:\n                predicted.append(combined_elem)\n    # True positives are predicted items that are in the ground truth\n    true_positives = len([x for x in predicted if x in ground_truth])\n\n    # Precision calculation\n    if len(predicted) == 0:\n        precision = 0\n    else:\n        precision = true_positives / len(predicted)\n\n    # Recall calculation\n    if len(ground_truth) == 0:\n        recall = 0\n    else:\n        recall = true_positives / len(ground_truth)\n\n    # F1-score calculation\n    if precision + recall == 0:\n        f1_score = 0\n    else:\n        f1_score = 2 * (precision * recall) / (precision + recall)\n\n    return precision, recall, f1_score\n```", "```py\nmodel = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\",  torch_dtype=torch.bfloat16).cuda()\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\nmodel.eval()\n\nmistral_few_shot_answers = []\nfor item in tqdm(test_set_subsample):\n    few_shot_prompt_messages = build_few_shot_prompt(SYSTEM_PROMPT, item, few_shot_example)\n    input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors = \"pt\").cuda()\n    outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    \n    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554\n    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n    mistral_few_shot_answers.append(parse_answer(gen_text.strip()))\n```", "```py\nfrom rank_bm25 import BM25Okapi\nfrom typing import List, Tuple, Dict\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\n\nclass BM25Retriever:\n    \"\"\"\n    A class for retrieving documents using the BM25 algorithm.\n\n    Attributes:\n        index (List[int, str]): A dictionary with document IDs as keys and document texts as values.\n        tokenized_docs (List[List[str]]): Tokenized version of the documents in `processed_index`.\n        bm25 (BM25Okapi): An instance of the BM25Okapi model from the rank_bm25 package.\n    \"\"\"\n\n    def __init__(self, docs_with_ids: Dict[int, str]):\n        \"\"\"\n        Initializes the BM25Retriever with a dictionary of documents.\n\n        Args:\n            docs_with_ids (List[List[str, str]]): A dictionary with document IDs as keys and document texts as values.\n        \"\"\"\n        self.index = docs_with_ids\n        self.tokenized_docs = self._tokenize_docs([x[1] for x in self.index])\n        self.bm25 = BM25Okapi(self.tokenized_docs)\n\n    def _tokenize_docs(self, docs: List[str]) -> List[List[str]]:\n        \"\"\"\n        Tokenizes the documents using NLTK's word_tokenize.\n\n        Args:\n            docs (List[str]): A list of documents to be tokenized.\n\n        Returns:\n            List[List[str]]: A list of tokenized documents.\n        \"\"\"\n        return [word_tokenize(doc.lower()) for doc in docs]\n\n    def query(self, query: str, top_n: int = 10) -> List[Tuple[int, float]]:\n        \"\"\"\n        Queries the BM25 model and retrieves the top N documents with their scores.\n\n        Args:\n            query (str): The query string.\n            top_n (int): The number of top documents to retrieve.\n\n        Returns:\n            List[Tuple[int, float]]: A list of tuples, each containing a document ID and its BM25 score.\n        \"\"\"\n        tokenized_query = word_tokenize(query.lower())\n        scores = self.bm25.get_scores(tokenized_query)\n        doc_scores_with_ids = [(doc_id, scores[i]) for i, (doc_id, _) in enumerate(self.index)]\n        top_doc_ids_and_scores = sorted(doc_scores_with_ids, key=lambda x: x[1], reverse=True)[:top_n]\n        return [x[0] for x in top_doc_ids_and_scores]\n```", "```py\ndef process_index(index):\n    \"\"\"\n    Processes the initial document index to combine aliases, canonical names, and definitions into a single text index.\n\n    Args:\n    - index (Dict): The MeSH knowledge base\n    Returns:\n        List[List[int, str]]: A dictionary with document IDs as keys and combined text indices as values.\n    \"\"\"\n    processed_index = []\n    for key, value in tqdm(index.items()):\n        assert(type(value[\"aliases\"]) != list)\n        aliases_text = \" \".join(value[\"aliases\"].split(\",\"))\n        text_index = (aliases_text + \" \" +  value.get(\"canonical_name\", \"\")).strip()\n        if \"definition\" in value:\n            text_index += \" \" + value[\"definition\"]\n        processed_index.append([value[\"concept_id\"], text_index])\n    return processed_index\n\nmesh_data = read_jsonl_file(\"mesh_2020.jsonl\")\nprocess_mesh_kb(mesh_data)\nmesh_data_kb = {x[\"concept_id\"]:x for x in mesh_data}\nmesh_data_dict = process_index({x[\"concept_id\"]:x for x in mesh_data})\nretriever = BM25Retriever(mesh_data_dict)\n```", "```py\nmistral_rag_answers = {10:[], 30:[], 50:[]}\n\nfor k in [10,30,50]:\n    for item in tqdm(test_set_subsample):\n        relevant_mesh_ids = retriever.query(item[\"title\"] + \" \" + item[\"abstract\"], top_n = k)\n        relevant_contexts = [mesh_data_kb[x] for x in relevant_mesh_ids]\n        rag_prompt = build_rag_prompt(SYSTEM_RAG_PROMPT, item, relevant_contexts)\n        input_ids = tokenizer.apply_chat_template(rag_prompt, tokenize=True, return_tensors = \"pt\").cuda()\n        outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    \n        gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n        mistral_rag_answers[k].append(parse_answer(gen_text.strip()))\n```", "```py\nentity_scores_at_k = {}\nmesh_scores_at_k = {}\n\nfor key, value in mistral_rag_answers.items():\n    entity_scores = [calculate_entity_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, value)]\n    macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)\n    macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)\n    macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)\n    entity_scores_at_k[key] = {\"macro-precision\": macro_precision_entity, \"macro-recall\": macro_recall_entity, \"macro-f1\": macro_f1_entity}\n\n    mesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, value)]\n    macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)\n    macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)\n    macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)\n    mesh_scores_at_k[key] = {\"macro-precision\": macro_precision_mesh, \"macro-recall\": macro_recall_mesh, \"macro-f1\": macro_f1_mesh}\n```", "```py\nentity_mesh_data_dict = [[x[\"concept_id\"] , \" \".join(x[\"aliases\"].split(\",\")) + \" \" + x[\"canonical_name\"]] for x in mesh_data]\nentity_retriever = BM25Retriever(entity_mesh_data_dict)\n\nparsed_entities_few_shot = [[y[\"text\"] for y in x] for x in mistral_few_shot_answers]\nretrieved_answers = []\n\nfor item in tqdm(parsed_entities_few_shot):\n    answer_element = []\n    for entity in item:\n        retrieved_mesh_ids = entity_retriever.query(entity, top_n = 1)\n        answer_element.append({\"text\": entity, \"identifier\":retrieved_mesh_ids[0]})\n    retrieved_answers.append(answer_element)\n\nmesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]\nmacro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(metric_scores)\nmacro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(metric_scores)\nmacro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(metric_scores)\n```", "```py\nfrom datasets import load_dataset\nimport json\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom datasets import DatasetDict\nfrom transformers import AutoTokenizer, BitsAndBytesConfig\nimport torch\nfrom trl import SFTTrainer\nfrom peft import LoraConfig\nfrom transformers import TrainingArguments\nfrom helpers import *\n\ndef read_jsonl_file(file_path):\n    \"\"\"\n    Parses a JSONL (JSON Lines) file and returns a list of dictionaries.\n\n    Args:\n        file_path (str): The path to the JSONL file to be read.\n\n    Returns:\n        list of dict: A list where each element is a dictionary representing\n            a JSON object from the file.\n    \"\"\"\n    jsonl_lines = []\n    with open(file_path, 'r', encoding=\"utf-8\") as file:\n        for line in file:\n            json_object = json.loads(line)\n            jsonl_lines.append(json_object)\n\n    return jsonl_lines\n\ndef convert_to_template(data):\n    messages = []\n    messages.append({\"role\": \"user\", \"content\": data[\"question\"]})\n    messages.append({\"role\": \"assistant\", \"content\": data[\"answer\"]})\n\n    return tokenizer.apply_chat_template(messages, tokenize = False)\n\nmesh_dataset = parse_dataset(\"CDR_TrainingSet.PubTator.txt\")\n```", "```py\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# set pad_token_id equal to the eos_token_id if not set\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = \"right\"\n\n# Set reasonable default for models without max length\nif tokenizer.model_max_length > 100_000:\n  tokenizer.model_max_length = 512\n```", "```py\nprepared_dataset = []\nsystem_prompt = \"Answer the question factually and precisely.\"\nentity_prompt = \"What are the chemical and disease related entities present in this biomedical text?\"\nprepared_dataset = []\n\ndef prepare_instructions(elem):\n    entities = []\n    for x in elem[\"annotations\"]:\n        if x[\"text\"] not in entities:\n            entities.append(x[\"text\"])\n\n    return {\"question\": system_prompt + \"\\n\" + entity_prompt + \"\\n\" + elem[\"title\"] + \" \" + elem[\"abstract\"] , \"answer\": \"The entities are:\" + \",\".join(entities)}\n\nquestions = [prepare_instructions(x) for x in tqdm(mesh_dataset)]\nchat_format_questions = [{\"text\": convert_to_template(x)} for x in tqdm(questions)]\n\ndf = pd.DataFrame(chat_format_questions)\ntrain_dataset = Dataset.from_pandas(df)\n```", "```py\nquantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\ndevice_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n\nmodel_kwargs = dict(\n    torch_dtype=torch.bfloat16,\n    use_cache=False, # set to False as we're going to use gradient checkpointing\n    device_map=device_map,\n    quantization_config=quantization_config,\n)\n```", "```py\noutput_dir = 'entity_finetune'\n\n# based on config\ntraining_args = TrainingArguments(\n    bf16=True, # specify bf16=True instead when training on GPUs that support bf16\n    do_eval=False,\n    # evaluation_strategy=\"no\",\n    gradient_accumulation_steps=1,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    learning_rate=1.0e-04,\n    log_level=\"info\",\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    lr_scheduler_type=\"cosine\",\n    max_steps=-1,\n    num_train_epochs=5,\n    output_dir=output_dir,\n    overwrite_output_dir=True,\n    per_device_eval_batch_size=1, \n    per_device_train_batch_size=8,\n    save_strategy=\"no\",\n    save_total_limit=None,\n    seed=42,\n)\n\n# based on config\npeft_config = LoraConfig(\n        r=16,\n        lora_alpha=16,\n        lora_dropout=0.1,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\ntrainer = SFTTrainer(\n        model=model_id,\n        model_init_kwargs=model_kwargs,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=train_dataset,\n        dataset_text_field=\"text\",\n        tokenizer=tokenizer,\n        packing = True,\n        peft_config=peft_config,\n        max_seq_length=tokenizer.model_max_length,\n    )\n\ntrain_result = trainer.train()\ntrainer.save_model(output_dir)\n```", "```py\ndef parse_entities_from_trained_model(content):\n    \"\"\"\n    Extracts a list of entities from the output of a trained model.\n\n    Args:\n    - content (str): The raw string output from a trained model.\n\n    Returns:\n    - list of str: A list of entities extracted from the model's output.\n    \"\"\"\n    return content.split(\"The entities are:\")[-1].split(\",\")\n\nmistral_few_shot_answers = []\nfor item in tqdm(test_set_subsample):\n    few_shot_prompt_messages = build_entity_prompt(item)\n    # input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors = \"pt\").cuda()\n    prompt = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=False)\n    tensors = tokenizer(prompt, return_tensors=\"pt\")\n    input_ids = tensors.input_ids.cuda()\n    attention_mask = tensors.attention_mask.cuda()\n    outputs = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=200, do_sample=False)    \n    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554\n    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]\n    mistral_few_shot_answers.append(parse_entities_from_trained_model(gen_text.strip()))\n```", "```py\nparsed_entities_few_shot = [[y[\"text\"] for y in x] for x in mistral_few_shot_answers]\nretrieved_answers = []\n\nfor item in tqdm(parsed_entities_few_shot):\n    answer_element = []\n    for entity in item:\n        retrieved_mesh_ids = entity_ranker.query(entity, top_n = 1)\n        answer_element.append({\"identifier\":retrieved_mesh_ids[0], \"text\":entity})\n    retrieved_answers.append(answer_element)\n\nentity_scores = [calculate_entity_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]\nmacro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)\nmacro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)\nmacro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)\n\nmesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]\nmacro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)\nmacro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)\nmacro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)\n```", "```py\nfrom scispacy.linking import EntityLinker\nimport spacy, scispacy\nimport pandas as pd\nfrom helpers import *\nfrom tqdm import tqdm\n\n#code for setting up MeSH linker referred from https://github.com/allenai/scispacy/issues/355\nconfig = {\n    \"resolve_abbreviations\": True,  \n    \"linker_name\": \"mesh\", \n    \"max_entities_per_mention\":1\n}\n\nnlp = spacy.load(\"en_ner_bc5cdr_md\")\nnlp.add_pipe(\"scispacy_linker\", config=config) \n\nlinker = nlp.get_pipe(\"scispacy_linker\")\n\ndef extract_mesh_ids(text):\n    mesh_entity_pairs = []\n    doc = nlp(text)\n    for e in doc.ents:\n        if e._.kb_ents:\n            cui = e._.kb_ents[0][0]\n            mesh_entity_pairs.append({\"text\": e.text, \"identifier\": cui})\n        else:\n            mesh_entity_pairs.append({\"text\": e.text, \"identifier\": \"None\"})\n\n    return mesh_entity_pairs\n```", "```py\nall_mesh_ids = []\nfor item in tqdm(test_set_subsample):\n    text = item[\"title\"] + \" \" + item[\"abstract\"]\n    mesh_ids = extract_mesh_ids(text)\n    all_mesh_ids.append(mesh_ids)\n\nentity_scores = [calculate_entity_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, all_mesh_ids)]\nmacro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)\nmacro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)\nmacro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)\n\nmesh_scores = [calculate_mesh_metrics(gt[\"annotations\"],pred) for gt, pred in zip(test_set_subsample, all_mesh_ids)]\nmacro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(entity_scores)\nmacro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(entity_scores)\nmacro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(entity_scores)\n```"]