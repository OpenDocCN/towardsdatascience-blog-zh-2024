<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Mastering RAG Systems: From Fundamentals to Advanced, with Strategic Component Evaluation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Mastering RAG Systems: From Fundamentals to Advanced, with Strategic Component Evaluation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09">https://towardsdatascience.com/mastering-rag-systems-from-fundamentals-to-advanced-with-strategic-component-evaluation-3551be31858f?source=collection_archive---------3-----------------------#2024-04-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f40e" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Elevating your RAG System: A step-by-step guide to advanced enhancements via LLM evaluation, with a real-world data use case</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Hamza Gharbi" class="l ep by dd de cx" src="../Images/da96d29dfde486875d9a4ed932879aef.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*o0ycpDpLUYnE4ABl"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@hamzagharbi_19502?source=post_page---byline--3551be31858f--------------------------------" rel="noopener follow">Hamza Gharbi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3551be31858f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">29 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">7</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/7a0272c5930ac85a330d3af9c48fb8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AGTjABg-dFXy1JePkmXAg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated by DALL-E.</figcaption></figure><p id="b040" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article will guide you through building an advanced Retrieval-Augmented Generation (RAG) pipeline using the <code class="cx ny nz oa ob b">llama-index</code> framework.</p><p id="481f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A Retrieval-Augmented Generation (RAG) system is a framework that makes generative AI models more accurate and reliable by using information from outside sources. In the context of this project, legal documents will be used as the external knowledge base.</p><p id="c82b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this tutorial, we’ll start by establishing a basic RAG system before illustrating how to include advanced features. One of the challenges in constructing such a system is deciding on the best components for the pipeline. We will attempt to answer this by evaluating the critical components of the pipeline.</p><p id="6ad3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This article serves as a practical tutorial for implementing RAG systems, including their evaluation. While it doesn’t delve deeply into theoretical aspects, it will explain the concepts used in this article as thoroughly as possible.</p><h1 id="f1c9" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Table of materiels</h1><p id="0df2" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">· <a class="af pd" href="#690f" rel="noopener ugc nofollow">Overview</a><br/>· <a class="af pd" href="#6324" rel="noopener ugc nofollow">Set-up</a><br/> ∘ <a class="af pd" href="#e9f3" rel="noopener ugc nofollow">1- Code</a><br/> ∘ <a class="af pd" href="#83e0" rel="noopener ugc nofollow">2- Data</a><br/> ∘ <a class="af pd" href="#ab50" rel="noopener ugc nofollow">3- Raw data transformation</a><br/>· <a class="af pd" href="#6587" rel="noopener ugc nofollow">Basic RAG system</a><br/> ∘ <a class="af pd" href="#0e34" rel="noopener ugc nofollow">1- Data ingestion</a><br/> ∘ <a class="af pd" href="#64f9" rel="noopener ugc nofollow">2- Indexing and storage</a><br/> ∘ <a class="af pd" href="#a526" rel="noopener ugc nofollow">3- Querying</a><br/> ∘ <a class="af pd" href="#dd2b" rel="noopener ugc nofollow">4- Evaluation</a><br/>· <a class="af pd" href="#5e0e" rel="noopener ugc nofollow">Evaluating embeddings</a><br/>· <a class="af pd" href="#d423" rel="noopener ugc nofollow">Evaluating advanced features</a><br/> ∘ <a class="af pd" href="#f5f5" rel="noopener ugc nofollow">1- Windowing</a><br/> ∘ <a class="af pd" href="#6ee9" rel="noopener ugc nofollow">2- Hybrid search</a><br/> ∘ <a class="af pd" href="#2b7c" rel="noopener ugc nofollow">3- Query rewriting</a><br/>· <a class="af pd" href="#f638" rel="noopener ugc nofollow">Routing</a><br/>· <a class="af pd" href="#24c9" rel="noopener ugc nofollow">Conclusion</a><br/>· <a class="af pd" href="#7830" rel="noopener ugc nofollow">References</a><br/>· <a class="af pd" href="#3cdf" rel="noopener ugc nofollow">To reach out</a></p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="690f" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Overview</h1><p id="8e93" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">This article unfolds in these steps:</p><p id="ffae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">1- We’ll start by building a <strong class="ne fr">basic</strong> RAG system using France’s Civil Code data.</p><p id="8ac2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">2- Next, we’ll compare various <strong class="ne fr">embedding</strong> methods from OpenAI, Mistral, and open-source models, evaluating their context relevance.</p><p id="3c04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">3- Advanced concepts like <strong class="ne fr">windowing</strong>,<strong class="ne fr"> hybrid search</strong>, and <strong class="ne fr">query rewriting</strong> will be explored to improve the RAG system. The effectiveness of these techniques will be assessed using a set of evaluation queries.</p><p id="84c6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">4- Lastly, we’ll index more law codes and demonstrate how to use the llama-index <strong class="ne fr">Routing</strong> feature to select the appropriate index and obtain the correct response.</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6324" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Set-up</h1><h2 id="e9f3" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk"><strong class="al">1- Code</strong></h2><p id="d34f" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">To access the code for this project, execute the following command to clone the corresponding repository from GitHub:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="f5da" class="ql od fq ob b bg qm qn l qo qp">git clone git@github.com:HamzaG737/legal-code-rag.git</span></pre><p id="0e71" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, you’ll need to install the necessary packages. We used <a class="af pd" href="https://python-poetry.org/" rel="noopener ugc nofollow" target="_blank">poetry</a> as our package manager for better handling of project dependencies. Install poetry using the following command.</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="acbb" class="ql od fq ob b bg qm qn l qo qp">curl -sSL https://install.python-poetry.org | python3 -</span></pre><p id="86ed" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can find more about poetry installation <a class="af pd" href="https://python-poetry.org/docs/#installing-with-the-official-installer" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="25da" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then, at the root of the project, use the following command to install the Python packages:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="6e7c" class="ql od fq ob b bg qm qn l qo qp">poetry install</span></pre><p id="48f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For this project, it is required to have a Python environment running a version in the range of 3.9 to 3.11. We strongly recommend the creation of a virtual environment to isolate your project’s package dependencies, ensuring they do not conflict with those installed globally on your system.</p><p id="7aa3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You will also need <strong class="ne fr">Docker</strong> for this project.</p><p id="4347" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, ensure the <code class="cx ny nz oa ob b">OPENAI_API_KEY</code> environment variable is defined as we will utilize both OpenAI's <code class="cx ny nz oa ob b">gpt-3.5-turbo</code> LLM and Ada embeddings in this project. If you're interested in conducting the experiment with Mistral embeddings, you must obtain an API key from the <a class="af pd" href="https://console.mistral.ai/" rel="noopener ugc nofollow" target="_blank">Mistral platform</a>. Then you need to create the corresponding environment variable <code class="cx ny nz oa ob b">MISTRAL_API_KEY</code>.</p><h2 id="83e0" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">2- Data</h2><p id="c70d" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Our RAG’s knowledge base consists of examples from the French law codes. A law Code is a comprehensive piece of legislation designed to authoritatively and logically set out the principles and rules in a specific area of law.</p><p id="029f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For instance, the <strong class="ne fr">civil code</strong> aims to reform and codify French laws related to private or civil law. This includes areas such as property, contracts, family law, and personal status.</p><p id="77bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In general, a code is a set of ordered articles that are associated with certain metadata such as the chapter, title, section, etc …</p><p id="d01c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Currently, there are approximately 78 legal codes in force in France. The French government publishes these codes for free on a website called <a class="af pd" href="https://www.legifrance.gouv.fr/" rel="noopener ugc nofollow" target="_blank">Légifrance</a>.</p><p id="629b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For this project, you have two options to create the knowledge base:</p><ul class=""><li id="28fa" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Load fresh data from the Legifrance API, which has an <a class="af pd" href="https://www.legifrance.gouv.fr/contenu/pied-de-page/open-data-et-api" rel="noopener ugc nofollow" target="_blank">open data status</a>. You can find Instructions on creating the API keys <a class="af pd" href="https://github.com/HamzaG737/legal-code-rag/tree/main?tab=readme-ov-file#get-legifrance-api-keys" rel="noopener ugc nofollow" target="_blank">here</a>. Then we’ll use the Python library <a class="af pd" href="https://github.com/rdassignies/pylegifrance" rel="noopener ugc nofollow" target="_blank">pylegifrance</a> to request specific codes from the API.</li><li id="a0ea" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Load processed data from the repository at <code class="cx ny nz oa ob b">./data/legifrance/ {code_name}.json</code>.</li></ul><p id="de60" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since law codes change frequently, we recommend loading data directly from the API if you are interested in getting the latest versions of the law Codes. However, creating the API keys can be a bit tedious. If you’re in a rush and don’t need the latest content, you can load the data locally, which is the default setting.</p><p id="6f65" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you have the API keys and want to reload the data, set the <code class="cx ny nz oa ob b">reload_data</code> argument to <code class="cx ny nz oa ob b">True</code> when creating the query engine. This engine represents our end-to-end RAG pipeline (we'll explain the query engine concept later).</p><h2 id="ab50" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">3- Raw data transformation</h2><p id="4c4a" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">This section is for the readers interested to know how we transformed the data we got from the legifrance API.</p><p id="a92c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In <code class="cx ny nz oa ob b">./data_ingestion/preprocess_legifrance_data.py</code> we preprocess the data coming from the API using the following steps:</p><ul class=""><li id="9ba7" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">We request a certain Code’s content from the API.</li><li id="c449" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We retrieve the articles content recursively from the API response json.</li><li id="6a83" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We deduplicate the articles and perform some cleaning, like removing the some html tags, stripping text, etc …</li></ul><p id="db37" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The data that we obtain at the end of this process is a list of articles where each article is represented by its content, its metadata (for instance the title, section, paragraph …) and its number. For example:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="2e2d" class="ql od fq ob b bg qm qn l qo qp">{'content': "Ni le propriétaire, ni l'usufruitier, ne sont tenus de rebâtir ce qui est tombé de vétusté, ou ce qui a été détruit par cas fortuit.",<br/> 'num': '607',<br/> 'livre': 'Des biens et des différentes modifications de la propriété',<br/> 'titre': "De l'usufruit, de l'usage et de l'habitation",<br/> 'chapitre': "De l'usufruit",<br/> 'section': "Des obligations de l'usufruitier"}</span></pre><p id="c8b0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And here is the english translation:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="48de" class="ql od fq ob b bg qm qn l qo qp">{<br/>  "content": "Neither the owner nor the usufructuary are required to rebuild what has fallen into disrepair or what has been destroyed by an act of chance.",<br/>  "num": "607",<br/>  "book": "Of Property and the Various Modifications of Ownership",<br/>  "title": "Of Usufruct, Use and Habitation",<br/>  "chapter": "Of Usufruct",<br/>  "section": "The Obligations of the Usufructuary"<br/>}</span></pre><p id="296d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">An article will serve as the basic unit of data and will be encapsulated in what we call a <code class="cx ny nz oa ob b">node</code> . More on this in the next section.</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6587" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Basic RAG system</h1><p id="f9fe" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">A basic RAG system contains <strong class="ne fr">four</strong> important steps. We will examine each of these steps and illustrate their application in our project.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/dd2c73bcf52db6d8e915faa4f1fe0a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jeD-sd14inwDpVZe67MbUw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Illustration of the basic RAG system applied on law Codes. Image generated by author using the <a class="af pd" href="https://helpful.dev/" rel="noopener ugc nofollow" target="_blank"><strong class="bf oe">Diagrams: Show Me GPT</strong></a></figcaption></figure><h2 id="0e34" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">1- Data ingestion</h2><p id="0b09" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">This phase involves the collection and preprocessing of relevant data from a variety of sources, such as PDF files, databases, APIs, websites, and more. This stage is closely tied to two key concepts: <strong class="ne fr">documents</strong> and <strong class="ne fr">nodes</strong>.</p><p id="1751" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the terminology of LlamaIndex, a <code class="cx ny nz oa ob b">Document</code> refers to a container that encapsulates any data source, like a PDF, an API output, or data retrieved from a database. A <code class="cx ny nz oa ob b">Node</code>, on the other hand, is the basic unit of data in LlamaIndex, representing a “chunk” of a source Document. Nodes carry metadata linking them to the document they belong to and to other nodes.</p><p id="1226" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our project, a document would be the full civil Code text and a node an article of this code. However, since we get the data already parsed into different articles from the API, we won’t need to chunk the documents and therefore prevent all the errors induced by this process.</p><p id="116e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code to create the nodes is the following:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="f9a0" class="ql od fq ob b bg qm qn l qo qp">from dataclasses import dataclass<br/>from typing import List<br/>import os<br/>from uuid import uuid4<br/>from llama_index.core.schema import TextNode<br/><br/>from loguru import logger<br/><br/>from utils import load_json<br/>from .preprocess_legifrance_data import get_code_articles<br/>from .constants import path_dir_data<br/><br/>@dataclass<br/>class CodeNodes:<br/>    code_name: str<br/>    max_words_per_node: int = 4000<br/>    _n_truncated_articles: int = 0<br/>    ...<br/><br/>    def __post_init__(self):<br/>        self.articles = self.try_load_data()<br/>        self.nodes = self.create_nodes(self.articles)<br/>        ...<br/><br/>    def create_nodes(self, list_articles: List[dict]):<br/>        nodes = [<br/>            TextNode(<br/>                text=article["content"],<br/>                id_=str(uuid4()),<br/>                metadata=self._parse_metadata(article),<br/>            )<br/>            for article in list_articles<br/>        ]<br/><br/>        return nodes<br/><br/>    def try_load_data(self) -&gt; List[dict]:<br/>        path = os.path.join(path_dir_data, f"{self.code_name}.json")<br/>        try:<br/>            code_articles = load_json(path=path)<br/>        except FileNotFoundError:<br/>            logger.warning(<br/>                f"File not found at path {path}. Fetching data from Legifrance."<br/>            )<br/>            code_articles = get_code_articles(code_name=self.code_name)<br/>        truncated_articles = self._chunk_long_articles(code_articles)<br/>        return truncated_articles<br/><br/>    def _parse_metadata(self, article: dict) -&gt; dict:<br/>        metadata = {k: v for k, v in article.items() if k not in ["content", "num"]}<br/>        metadata = {<br/>            "Nom du code": self.code_name,<br/>            **metadata,<br/>            "Article numero": article["num"],<br/>        }<br/>        return metadata<br/><br/>    def _chunk_long_articles(self, articles: List[dict]) -&gt; List[dict]:<br/>        ...</span></pre><ul class=""><li id="8f24" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">First we load the articles in <code class="cx ny nz oa ob b">try_load_data</code> as mentioned in the previous section.</li><li id="b3c5" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We chunk the long articles into “sub-articles” so that we can embed them without truncation. This is done in <code class="cx ny nz oa ob b">_chunk_long_articles</code>. The full code for the chunking can be found on<code class="cx ny nz oa ob b">data_ingestion/ nodes_processing.py</code></li><li id="8294" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We create nodes using llama-index <code class="cx ny nz oa ob b">TextNode</code> class. At the end of this step, our data consists of a list of nodes where each node has a text, an id and its metadata. For example:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="2842" class="ql od fq ob b bg qm qn l qo qp">TextNode(<br/>  id_='c0ee164f-ae58-47c8-8e5b-e82f55ac98a9',<br/>  embedding=None,<br/>  metadata={<br/>    'Nom du code': 'Code civil',<br/>    'livre': 'Des différentes manières dont on acquiert la propriété',<br/>    'titre': 'Des libéralités',<br/>    'chapitre': 'Des dispositions testamentaires.',<br/>    'section': 'Du legs universel.',<br/>    'id': '1003',<br/>    'Article numero': '1003'<br/>  },<br/>  excluded_embed_metadata_keys=[],<br/>  excluded_embed_metadata_keys=[],<br/>  relationships={},<br/>  text="Le legs universel est la disposition testamentaire par laquelle le testateur donne à une ou plusieurs personnes l'universalité des biens qu'il laissera à son décès.",<br/>  start_char_idx=None,<br/>  end_char_idx=None,<br/>  text_template='{metadata_str}\n\n{content}',<br/>  metadata_template='{key}: {value}',<br/>  metadata_seperator='\n'<br/>)<br/></span></pre><p id="8568" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The metadata will be used for both the embedding and the LLM context. Specifically, we will embed the metadata as key-value pairs concatenated with the content, following the <code class="cx ny nz oa ob b">text_template</code> format. This format will also be used to represent the node when forming the LLM context. If desired, you can exclude some or all of the metadata for the embedding or the LLM by changing the <code class="cx ny nz oa ob b">excluded_embed_metadata_keys</code> and/or <code class="cx ny nz oa ob b">excluded_embed_metadata_keys</code>.</p><h2 id="64f9" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">2- Indexing and storage</h2><p id="c5f6" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><strong class="ne fr">Indexing</strong> is the process of creating a data structure that enables data queries. This typically involves generating <code class="cx ny nz oa ob b">vector embeddings</code>, which are numerical representations that capture the core essence of the data. Additionally, various metadata strategies can be adopted to efficiently retrieve information that is contextually relevant. The embedding model is used not only to embed the documents during the index construction phase, but also to embed any queries we make using the query engine in the future.</p><p id="6e9f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Storing</strong> is the subsequent phase where the created index and other metadata are saved to prevent the need for re-indexing. This ensures that once data is organized and indexed, it remains accessible and retrievable without undergoing the indexing process again.</p><p id="7e96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx ny nz oa ob b">vector embeddings</code> can be stored and persisted in a <code class="cx ny nz oa ob b">vector database</code>. In the next section we will see how to create and run this database locally.</p><p id="969e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">a- Setting-up the vector database</strong></p><p id="f7f7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af pd" href="https://qdrant.tech/" rel="noopener ugc nofollow" target="_blank">Qdrant</a> will be used as a vector database to store and index the articles embeddings along with the meta-data. We will run the server locally using Qdrant official docker image.</p><p id="bb48" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First you can pull the image from the Dockerhub:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="7b69" class="ql od fq ob b bg qm qn l qo qp">docker pull qdrant/qdrant</span></pre><p id="a36d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Following this, run the Qdrant service with the command below. This will also map the necessary ports and designate a local directory (<code class="cx ny nz oa ob b">./qdrant_storage</code>) for data storage:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="7b79" class="ql od fq ob b bg qm qn l qo qp">docker run -p 6333:6333 -p 6334:6334 \<br/>    -v $(pwd)/qdrant_storage:/qdrant/storage:z \<br/>    qdrant/qdrant</span></pre><p id="1218" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With the setup complete, we can interact with the Qdrant service using its Python client:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="7896" class="ql od fq ob b bg qm qn l qo qp">from qdrant_client import QdrantClient<br/><br/>client = QdrantClient("localhost", port=6333)</span></pre><p id="b07c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">b- Indexing and storing the data</strong></p><p id="47a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below is the code to index and store the data.</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="a3d3" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core import VectorStoreIndex<br/>from llama_index.core import StorageContext<br/>from llama_index.embeddings.mistralai import MistralAIEmbedding<br/>from llama_index.embeddings.openai import OpenAIEmbedding<br/>from llama_index.embeddings.fastembed import FastEmbedEmbedding<br/>from llama_index.vector_stores.qdrant import QdrantVectorStore<br/>from qdrant_client import QdrantClient<br/>from qdrant_client.http.exceptions import UnexpectedResponse<br/>from loguru import logger<br/><br/>from data_ingestion.nodes_processing import CodeNodes<br/><br/><br/>def index_given_nodes(<br/>    code_nodes: CodeNodes,<br/>    embedding_model: MistralAIEmbedding | OpenAIEmbedding | FastEmbedEmbedding,<br/>    hybrid_search: bool,<br/>    recreate_collection: bool = False,<br/>) -&gt; VectorStoreIndex:<br/>    """<br/>    Given a list of nodes, create a new index or use an existing one. <br/>    <br/>    Parameters<br/>    ----------<br/>    code_nodes : CodeNodes<br/>        The nodes to index.<br/>    embedding_model : MistralAIEmbedding | OpenAIEmbedding | FastEmbedEmbedding<br/>        The embedding model to use.<br/>    hybrid_search : bool<br/>        Whether to enable hybrid search.<br/>    recreate_collection : bool, optional<br/>        Whether to recreate the collection, by default False.<br/>    <br/>    """<br/><br/>    collection_name = code_nodes.nodes_config<br/>    client = QdrantClient("localhost", port=6333)<br/>    if recreate_collection:<br/>        client.delete_collection(collection_name)<br/><br/>    try:<br/>        count = client.count(collection_name).count<br/>    except UnexpectedResponse:<br/>        count = 0<br/><br/>    if count == len(code_nodes.nodes):<br/>        logger.info(f"Found {count} existing nodes. Using the existing collection.")<br/>        vector_store = QdrantVectorStore(<br/>            collection_name=collection_name,<br/>            client=client,<br/>            enable_hybrid=hybrid_search,<br/>        )<br/>        return VectorStoreIndex.from_vector_store(<br/>            vector_store,<br/>            embed_model=embedding_model,<br/>        )<br/>    logger.info(<br/>        f"Found {count} existing nodes. Creating a new index with {len(code_nodes.nodes)} nodes. This may take a while."<br/>    )<br/>    if count &gt; 0:<br/>        client.delete_collection(collection_name)<br/><br/>    vector_store = QdrantVectorStore(<br/>        collection_name=collection_name,<br/>        client=client,<br/>        enable_hybrid=hybrid_search,<br/>    )<br/>    storage_context = StorageContext.from_defaults(vector_store=vector_store)<br/>    index = VectorStoreIndex(<br/>        code_nodes.nodes,<br/>        storage_context=storage_context,<br/>        embed_model=embedding_model,<br/>    )<br/>    return index</span></pre><p id="fa95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The logic of the indexing function is quite simple:</p><ul class=""><li id="90df" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">First we create the Qdrant client to interact with the vector database.</li><li id="0161" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We check the number of nodes in the desired collection. This collection name is provided by <code class="cx ny nz oa ob b">code_nodes.nodes_config</code> and corresponds to the current experiment (that is a concatenation of the code name, the embedding method and eventually the advanced RAG technique(s) like <em class="qz">base, window-nodes, hybrid search… </em>more on these experiments later). If the number of nodes is different than the current number, we create a new index:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="fb60" class="ql od fq ob b bg qm qn l qo qp">storage_context = StorageContext.from_defaults(vector_store=vector_store)<br/>index = VectorStoreIndex(<br/>        code_nodes.nodes,<br/>        storage_context=storage_context,<br/>        embed_model=embedding_model,<br/>    )</span></pre><p id="11f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Else we return the existing index:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="c111" class="ql od fq ob b bg qm qn l qo qp">index = VectorStoreIndex.from_vector_store(<br/>            vector_store,<br/>            embed_model=embedding_model,<br/>        )</span></pre><p id="3bf8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The full code to create or retrieve indexes can be found in the <code class="cx ny nz oa ob b">./retriever/get_retriever.py</code> module. You can also find more information about this phase on llama-index <a class="af pd" href="https://docs.llamaindex.ai/en/stable/understanding/storing/storing/" rel="noopener ugc nofollow" target="_blank">documentation</a>.</p><h2 id="a526" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">3- Querying</h2><p id="34b5" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Given a user query, we compute the similarity between the query embedding and the indexed node embeddings to find the most similar data. The content of these similar nodes is then used to generate a context. This context enables the Language Model to synthesize a response for the user.</p><p id="95bf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We define a <code class="cx ny nz oa ob b">query engine</code> based on the index created in the previous section. This engine is an end-to-end pipeline that takes a natural language query and returns a response, as well as the context retrieved and passed to the LLM.</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="9d55" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core.query_engine import BaseQueryEngine<br/>from llama_index.core import VectorStoreIndex<br/>from llama_index.core import PromptTemplate<br/><br/>def get_query_engine_based_on_index(<br/>    index: VectorStoreIndex,<br/>    similarity_top_k: int = 5,<br/>) -&gt; BaseQueryEngine:<br/><br/>    query_engine = index.as_query_engine(similarity_top_k=similarity_top_k)<br/><br/>    query_engine = update_prompts_for_query_engine(query_engine)<br/>    return query_engine</span></pre><p id="f20f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">update_prompts_for_query_engine</code> is a function that allows us to change the prompt of the response synthesizer, i.e the LLM responsible of taking the context as input and generating an answer for the user.</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="eb19" class="ql od fq ob b bg qm qn l qo qp">def update_prompts_for_query_engine(query_engine: BaseQueryEngine) -&gt; BaseQueryEngine:<br/><br/>    new_tmpl_str = (<br/>        "Below is the specific context required to answer the upcoming query. You must base your response solely on this context, strictly avoiding the use of external knowledge or assumptions..\n"<br/>        "---------------------\n"<br/>        "{context_str}\n"<br/>        "---------------------\n"<br/>        "Given this context, please formulate your response to the following query. It is imperative that your answer explicitly mentions any relevant code name and article number by using the format 'according to code X and article Y,...'. Ensure your response adheres to these instructions to maintain accuracy and relevance."<br/>        "Furthermore, it is crucial to respond in the same language in which the query is presented. This requirement is to ensure the response is directly applicable and understandable in the context of the query provided."<br/>        "Query: {query_str}\n"<br/>        "Answer: "<br/>    )<br/>    new_tmpl = PromptTemplate(new_tmpl_str)<br/>    query_engine.update_prompts({"response_synthesizer:text_qa_template": new_tmpl})<br/>    return query_engine</span></pre><p id="86e1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this new template, we emphasize the importance of preventing hallucinations. We also instruct the LLM to always reference the code name, such as the civil code, and the article number before providing a response. Moreover, we instruct it to reply in the language of the query to support multilingual queries.</p><p id="3128" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx ny nz oa ob b">create_query_engine</code> function in the <code class="cx ny nz oa ob b">query_engine</code> module is the entrypoint for creating the RAG pipeline. The arguments for this function define the RAG configuration parameters.</p><p id="1e1a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code below creates a basic query engine and generates a response based on a specific query:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="ce6e" class="ql od fq ob b bg qm qn l qo qp">from query.query_engine import create_query_engine<br/><br/>query_engine = create_query_engine()<br/><br/>response = query_engine.query("What are the conditions required for a marriage to be considered valid ?")<br/><br/>print(response)</span></pre><p id="2298" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can find the full code to create the query engine in <code class="cx ny nz oa ob b">./query/query_engine.py</code> . More infos about the llama-index query engine can be found <a class="af pd" href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/" rel="noopener ugc nofollow" target="_blank">here</a>.</p><h2 id="dd2b" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">4- Evaluation</h2><p id="de4c" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/understanding/evaluating/evaluating/" rel="noopener ugc nofollow" target="_blank">Evaluation</a> is crucial to assess the performance of the RAG pipeline and validate the decisions made regarding its components.</p><p id="bf80" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this project, we will employ an LLM-based evaluation method to assess the quality of the results. This evaluation will involve a golden LLM analyzing a set of responses against certain metrics to ensure the effectiveness and accuracy of the RAG pipeline.</p><p id="9d9f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Typically, a RAG system can be evaluated in two ways:</p><ul class=""><li id="7a08" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Response Evaluation</strong>: Does the response align with the retrieved context and the query?</li><li id="1c68" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><strong class="ne fr">Retrieval Evaluation</strong>: Are the data sources retrieved relevant to the query?</li></ul><p id="4bf2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">LLama-index</code> provides several evaluators to compute various metrics, such as <strong class="ne fr">faithfulness</strong>, <strong class="ne fr">context relevancy</strong>, and <strong class="ne fr">answer relevancy</strong>. You can find more details about these metrics in the evaluation section.</p><p id="4c5f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s the process for creating evaluators in this project:</p><ul class=""><li id="1f23" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Initially, we generate <code class="cx ny nz oa ob b">n</code> questions using an LLM (in our case, gpt-4) about the legal code we want to evaluate. For cost reasons, we've set <code class="cx ny nz oa ob b">n=50</code>, but a larger number could provide a more confident assessment of the system's performance. An LLM-generated question example is “What are the legal consequences of bigamy in France?” or in French: “Quelles sont les conséquences juridiques de la bigamie en France?”. Note that this approach has limitations as the questions generated by the LLM may not mirror the actual distribution of questions from real users. You can find the module for questions generation is <code class="cx ny nz oa ob b">evaluation/generate_questions.py</code> .</li><li id="6c0e" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Using the list of generated questions, an evaluation metric, and a query engine, we generate a list of responses. Each response is scored between 0 and 1 by the evaluator, which also provides a feedback text to explain its scoring.</li><li id="3283" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><code class="cx ny nz oa ob b">llama-index</code> provides evaluators for different metrics, such as the <code class="cx ny nz oa ob b">ContextRelevancyEvaluator</code> to compute the context relevancy. In addition, we override the <code class="cx ny nz oa ob b">evaluate_response</code> method of these evaluators to take into account the metadata when embedding and creating context.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="2a7c" class="ql od fq ob b bg qm qn l qo qp">from typing import Any, Optional, Sequence<br/><br/>from llama_index.core.evaluation import ContextRelevancyEvaluator, EvaluationResult<br/>from llama_index.core.response import Response<br/>from llama_index.core.schema import MetadataMode<br/><br/><br/>class CustomContextRelevancyEvaluator(ContextRelevancyEvaluator):<br/>    def __init__(self, **kwargs):<br/>        super().__init__(**kwargs)<br/><br/>    def evaluate_response(<br/>        self,<br/>        query: Optional[str] = None,<br/>        response: Optional[Response] = None,<br/>        metadata_mode: MetadataMode = MetadataMode.ALL,<br/>        **kwargs: Any,<br/>    ) -&gt; EvaluationResult:<br/>        """Run evaluation with query string and generated Response object.<br/><br/>        Subclasses can override this method to provide custom evaluation logic and<br/>        take in additional arguments.<br/>        """<br/>        response_str: Optional[str] = None<br/>        contexts: Optional[Sequence[str]] = None<br/>        if response is not None:<br/>            response_str = response.response<br/>            contexts = [<br/>                node.get_content(metadata_mode=metadata_mode)<br/>                for node in response.source_nodes<br/>            ]<br/><br/>        return self.evaluate(<br/>            query=query, response=response_str, contexts=contexts, **kwargs<br/>        )</span></pre><ul class=""><li id="7166" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Below is the code to obtain evaluation results from a specific evaluator, such as the context relevancy evaluator. It’s worth noting that you can generate these evaluations in an asynchronous mode for quicker results. However, due to OpenAI rate limits, we performed these evaluations sequentially.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="236b" class="ql od fq ob b bg qm qn l qo qp">from typing import Literal, List<br/><br/>from llama_index.core.query_engine import BaseQueryEngine<br/>from llama_index.llms.openai import OpenAI<br/>from llama_index.core.evaluation import EvaluationResult<br/>from llama_index.core.schema import MetadataMode<br/>from tqdm import tqdm<br/><br/>from utils import load_json<br/>from evaluation.custom_evaluators import CustomContextRelevancyEvaluator<br/><br/><br/>PATH_EVAL_CODE_CIVIL = "./data/questions_code_civil.json"<br/>PATH_EVAL_CODE_DE_LA_ROUTE = "./data/questions_code_de_la_route.json"<br/><br/><br/>def evaluate_one_metric(<br/>    query_engine: BaseQueryEngine,<br/>    code_name: Literal["code_civil", "code_de_la_route"],<br/>    metadata_mode: MetadataMode,<br/>    llm_for_eval: str = "gpt-3.5-turbo",<br/>) -&gt; List[EvaluationResult]:<br/><br/>    evaluator = CustomContextRelevancyEvaluator(<br/>        llm=OpenAI(temperature=0, model=llm_for_eval)<br/>    )<br/><br/>    eval_data: List[str] = load_json(globals()[f"PATH_EVAL_{code_name.upper()}"])<br/><br/>    results = []<br/>    for question in tqdm(eval_data):<br/>        response = query_engine.query(question)<br/>        eval_result = evaluator.evaluate_response(<br/>            query=question, response=response, metadata_mode=metadata_mode<br/>        )<br/><br/>        results.append(eval_result)<br/><br/>    return results</span></pre><ul class=""><li id="3ba8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Finally to get a score for a certain pipeline (defined by its query engine), we average the scores given by the evaluator to each query.</li></ul><p id="5d45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The complete code for evaluation is located in <code class="cx ny nz oa ob b">evaluation/eval_with_llamaindex.py</code>.</p><p id="ca3c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For the upcoming RAG pipeline evaluations, we will use 50 questions generated by GPT-4 based on the French civil code. These questions can be found on <code class="cx ny nz oa ob b">./data/questions_code_civil.json</code> . The gold LLM used for evaluation is <code class="cx ny nz oa ob b">gpt-3.5-turbo</code>.</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5e0e" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Evaluating embeddings</h1><p id="3efd" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><strong class="ne fr">All the evaluation experiments can be found on this </strong><a class="af pd" href="https://github.com/HamzaG737/legal-code-rag/blob/main/notebooks/evaluate_with_llamaindex.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">notebook</strong></a><strong class="ne fr">.</strong></p><p id="9415" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will first evaluate the embeddings of the civil code<em class="qz"> </em>articles (or nodes)<em class="qz">.</em> In order to choose the set of embeddings models we will be evaluating, we relied on the Huggingface <a class="af pd" href="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener ugc nofollow" target="_blank">leaderboard</a> for retrieval on french data. Hence we will be evaluating these three models :</p><ul class=""><li id="0305" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><strong class="ne fr">Text-Embedding-Ada-002</strong> from OpenAI that is state of the art for this task at the time of writing this article.</li><li id="c57e" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><strong class="ne fr">mistral-embed </strong>from Mistral AI that comes as a close second.</li><li id="03ad" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><strong class="ne fr">multilingual-e5-large </strong>from Infloat as the open source contender. We will also use Qdrant’s <a class="af pd" href="https://qdrant.tech/articles/fastembed/" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr">fast-embed</strong></a><strong class="ne fr"> </strong>framework to improve the efficiency of the embeddings creation, mainly by quantizing the weights and using ONNX as runtime.</li></ul><p id="9212" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can generate these embeddings with llama-index by importing the corresponding integration. For instance, we import the module <code class="cx ny nz oa ob b">FastEmbedEmbedding</code> to generate the <code class="cx ny nz oa ob b">e5-large</code> embeddings with the<code class="cx ny nz oa ob b">fast-embed</code> framework:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="2fab" class="ql od fq ob b bg qm qn l qo qp">from llama_index.embeddings.fastembed import FastEmbedEmbedding<br/><br/>embeddings_model_name = "intfloat/multilingual-e5-large"<br/>embed_model = FastEmbedEmbedding(model_name=embeddings_model_name)</span></pre><p id="9e3e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The full embeddings definition can be found in <code class="cx ny nz oa ob b">retriever/embeddings.py</code></p><p id="9cd4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The metric that we will be measuring is the context relevancy, i.e the relevancy of the retrieved context relative to the user query.</p><p id="af54" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is the code to launch the evaluations for this experiment.</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="6eaa" class="ql od fq ob b bg qm qn l qo qp">from query.query_engine import create_query_engine<br/>from evaluation.eval_with_llamaindex import evaluate_multiple_experiments<br/><br/><br/>query_engine_fastembed = create_query_engine(embedding_model="intfloat/multilingual-e5-large")<br/>query_engine_mistral = create_query_engine(embedding_model="mistral-embed")<br/>query_engine_ada = create_query_engine(embedding_model="text-embedding-ada-002")<br/><br/><br/>exp_to_query_engine = {<br/>    "mistral_embed": query_engine_mistral,<br/>    "fastembed": query_engine_fastembed,<br/>    "ada": query_engine_ada,<br/>}<br/><br/>scores_df, deeps_df = evaluate_multiple_experiments(<br/>    experiment_to_query_engine=exp_to_query_engine,<br/>    general_exp_name="embeddings",<br/>    list_metrics=["context_relevancy"],<br/>    code_name="code_civil"<br/>)</span></pre><p id="6cc5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now here is the final table of results. The <code class="cx ny nz oa ob b">embedding_time</code> field represents the time it took us to embed the full <code class="cx ny nz oa ob b">~2800</code> articles of the civil code.</p><figure class="mm mn mo mp mq mr"><div class="ra io l ed"><div class="rb rc l"/></div></figure><p id="cef0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We observe that <code class="cx ny nz oa ob b">text-embedding-data-002</code> outperforms the other two embedding models in both score and embedding time. The gap between <code class="cx ny nz oa ob b">mistral-embed</code> and <code class="cx ny nz oa ob b">ada</code> from one side, and the <code class="cx ny nz oa ob b">multilingual-e5-large</code> from the other, is quite significant. Note that the ranking of these embedding methods aligns with the leaderboard mentioned earlier.</p><p id="f6e3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">However, bear in mind that our evaluation is based on only 50 questions generated by an LLM. Thus, it may not fully represent performance in a real-world scenario, especially when differentiating between mistral-embed and ada, where the gap is relatively small.</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d423" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Evaluating advanced features</h1><h2 id="f5f5" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">1- Windowing</h2><p id="c48d" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The first advanced feature we’ll explore is augmenting node content with the content from preceding and succeeding nodes. This makes sense within the context of law, where neighboring articles are closely related.</p><p id="d1c0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We’ll use <code class="cx ny nz oa ob b">k</code> to parameterize windowing, representing the number of nodes before and after to add to the current node. For example, <code class="cx ny nz oa ob b">k=1</code> means that the current node content will be augmented with the previous and next nodes.</p><p id="5d62" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For a given node, the original content will be used for embedding, while the augmented content will create the context for the response generation.</p><p id="443e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here’s how we perform node windowing:</p><ul class=""><li id="1475" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">We create a custom function to generate the augmented nodes. Although Llama-index has its <code class="cx ny nz oa ob b">SentenceWindowNodeParser</code> class for this task, it requires whole documents as inputs and then performs splitting to create nodes. So we took inspiration from this class and created our own window parser.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="5ea0" class="ql od fq ob b bg qm qn l qo qp">from typing import List<br/>from llama_index.core.schema import TextNode<br/>from tqdm import tqdm<br/><br/>from .constants import possible_headers<br/><br/>WINDOW_METADATA_KEY = "window"<br/>ORIGINAL_TEXT_METADATA_KEY = "original_text"<br/><br/><br/>def add_window_nodes(nodes: List[TextNode], window_size: int = 3):<br/>    for i, node in tqdm(<br/>        enumerate(nodes), total=len(nodes), desc="Adding window nodes ..."<br/>    ):<br/>        window_nodes = nodes[<br/>            max(0, i - window_size) : min(i + window_size + 1, len(nodes))<br/>        ]<br/><br/>        node.metadata[WINDOW_METADATA_KEY] = "\n".join(<br/>            [n.get_content("llm") for n in window_nodes]<br/>        )<br/>        node.metadata[ORIGINAL_TEXT_METADATA_KEY] = node.text<br/><br/>        # exclude window metadata from embed and llm<br/>        node.excluded_embed_metadata_keys.extend(<br/>            [WINDOW_METADATA_KEY, ORIGINAL_TEXT_METADATA_KEY]<br/>        )<br/><br/>        node.excluded_llm_metadata_keys.extend(<br/>            [WINDOW_METADATA_KEY, ORIGINAL_TEXT_METADATA_KEY]<br/>        )<br/><br/>    # since articles metadata (like title, chapter, etc ...) will be incorporated in WINDOW_METADATA_KEY,<br/>    # we can exclude them from the llm metadata.<br/><br/>    for node in nodes:<br/>        node.excluded_llm_metadata_keys.extend(possible_headers)<br/><br/>    return nodes</span></pre><ul class=""><li id="a28b" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Then we modify the previously created <code class="cx ny nz oa ob b">CodeNodes</code> class to add the possibility of using neighbouring nodes for augmentation:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="f899" class="ql od fq ob b bg qm qn l qo qp">from dataclasses import dataclass<br/>from typing import List<br/>from llama_index.core.postprocessor import MetadataReplacementPostProcessor<br/><br/>from loguru import logger<br/><br/>from .window_nodes import add_window_nodes<br/><br/><br/>@dataclass<br/>class CodeNodes:<br/>    code_name: str<br/>    use_window_nodes: bool<br/>    nodes_window_size: int = 3<br/>    max_words_per_node: int = 4000<br/>    _n_truncated_articles: int = 0<br/><br/>    def __post_init__(self):<br/>        self.articles = self.try_load_data()<br/>        self.nodes = self.create_nodes(self.articles)<br/>        code_name_no_spaces = self.code_name.replace(" ", "_")<br/>        self.nodes_config = f"{code_name_no_spaces}_base"<br/>        self.post_processors = []<br/>        if self.use_window_nodes:<br/>            logger.info("Adding window nodes ...")<br/>            self.nodes = add_window_nodes(self.nodes, self.nodes_window_size)<br/>            self.nodes_config = f"{code_name_no_spaces}_window"<br/>            self.post_processors.append(<br/>                MetadataReplacementPostProcessor(target_metadata_key="window")<br/>            )<br/><br/>    def create_nodes(self, list_articles: List[dict]): ...<br/><br/>    def try_load_data(self) -&gt; List[dict]: ...<br/><br/>    def _parse_metadata(self, article: dict) -&gt; dict: ...<br/><br/>    def _chunk_long_articles(self, articles: List[dict]) -&gt; List[dict]: ...</span></pre><ul class=""><li id="d413" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">A key point to note is the <code class="cx ny nz oa ob b">MetadataReplacementPostProcessor</code>. This class is used to signal that we need to replace the node content with the content found in the <code class="cx ny nz oa ob b">target_metadata_key</code> field, right before passing the retrieved data to the LLM. Hence this field will contain the augmented content.</li><li id="2cad" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Lastly, we need to incorporate the post-processors into the query engine:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="c43c" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core.query_engine import BaseQueryEngine<br/>from llama_index.core import VectorStoreIndex<br/><br/><br/>def get_query_engine_based_on_index(<br/>    index: VectorStoreIndex,<br/>    postprocessors_list: list,<br/>    similarity_top_k: int = 5,<br/>) -&gt; BaseQueryEngine:<br/><br/>    query_engine = index.as_query_engine(<br/>        similarity_top_k=similarity_top_k, node_postprocessors=postprocessors_list<br/>    )<br/><br/>    query_engine = update_prompts_for_query_engine(query_engine)<br/>    return query_engine</span></pre><p id="11e5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this experiment, we will compare the base RAG pipeline to two other pipelines. These additional pipelines use node windowing with <code class="cx ny nz oa ob b">k=1</code> and <code class="cx ny nz oa ob b">k=2</code> respectively. We will use the same set of questions and context relevancy metrics to evaluate these pipelines. Here are the results:</p><figure class="mm mn mo mp mq mr"><div class="ra io l ed"><div class="rb rc l"/></div></figure><p id="ab56" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can observe that augmenting node content with neighbouring nodes can improve the context relevancy score. The small margin of improvement is probably due to an already high score (0.89).</p><p id="acf7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will also measure the <strong class="ne fr">faithfulness</strong> for this experiment. In llama-index, the <code class="cx ny nz oa ob b">FaithfulnessEvaluator</code> module determines if a query engine’s response matches any source nodes, indicating whether the response was hallucinated or not. It’s important to note that this evaluator provides a binary score (0 if the response is a hallucination, otherwise 1). This is in contrast to the <strong class="ne fr">context relevancy</strong> score, which can range between 0 and 1.</p><p id="1687" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the results:</p><figure class="mm mn mo mp mq mr"><div class="ra io l ed"><div class="rb rc l"/></div></figure><p id="65ec" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In our evaluation of 50 questions, we found that only one response was a hallucination in the base experiment, while there were five hallucinations in the windowing pipeline with <code class="cx ny nz oa ob b">k=2</code>. This suggests that adding more context, especially irrelevant context, can induce more hallucinations.</p><p id="8f0a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In subsequent experiments, we will maintain the base setup without node augmentation. This decision is guided by our observation that the slight increase in relevance does not offset the higher rate of hallucinations in responses.</p><h2 id="6ee9" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">2- Hybrid search</h2><p id="90dc" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/qdrant_hybrid/" rel="noopener ugc nofollow" target="_blank">Hybrid search</a> refers to combining search results from <code class="cx ny nz oa ob b">sparse</code> and <code class="cx ny nz oa ob b">dense</code> vectors.</p><p id="132e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">Dense vectors </code>represent data points as compact vectors of continuous values, offering a rich, nuanced understanding of the data’s features. These vectors, often result from deep learning models such as OpenAI, Mistral, e5-large, etc… So far, we have used only dense vectors during the retrieval phase.</p><p id="e5ea" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><code class="cx ny nz oa ob b">Sparse vectors</code>, in contrast, are primarily zeros and are produced through specialized models such as TF-IDF, BM25, etc. They excel at identifying specific keywords and minor details, differentiating them from the more semantically rich dense vectors.</p><p id="618c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To implement hybrid search in our RAG pipeline, we need to modify some parts of the code:</p><ul class=""><li id="cd80" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">First we need to enable hybrid search in our vector database by setting <code class="cx ny nz oa ob b">enable_hybrid=True</code> in the Qdrant client definition. This will run sparse vector generation locally using the <code class="cx ny nz oa ob b">"naver/efficient-splade-VI-BT-large-doc"</code> model from Huggingface, in addition to generating the usual dense vectors with OpenAI’s ada model.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="5666" class="ql od fq ob b bg qm qn l qo qp">vector_store = QdrantVectorStore(<br/>    collection_name=collection_name,<br/>    client=client,<br/>    enable_hybrid=True,<br/>)</span></pre><ul class=""><li id="f7fb" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">When defining the query engine in <code class="cx ny nz oa ob b">query_engine = index.as_query_engine(**kwargs)</code><strong class="ne fr"> </strong>, we need to define <code class="cx ny nz oa ob b">sparse_top_k</code> along with <code class="cx ny nz oa ob b">similarity_top_k</code> . <code class="cx ny nz oa ob b">sparse_top_k</code> represents how many nodes will be retrieved from each dense and sparse query. For example, if <code class="cx ny nz oa ob b">sparse_top_k=5</code> is set, that means we will retrieve 5 nodes using sparse vectors and 5 nodes using dense vectors. <code class="cx ny nz oa ob b">similarity_top_k</code> controls the final number of returned nodes. In the above setting, we end up with 10 nodes. A fusion algorithm is then applied to rank and order the nodes from different vector spaces (<a class="af pd" href="https://weaviate.io/blog/hybrid-search-fusion-algorithms#relative-score-fusion" rel="noopener ugc nofollow" target="_blank">relative score fusion</a> in this case). <code class="cx ny nz oa ob b">similarity_top_k=5</code> means the top five nodes after fusion are returned. Here is the new definition of the <code class="cx ny nz oa ob b">query_engine</code> after adding these options:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="d0a5" class="ql od fq ob b bg qm qn l qo qp">query_engine = index.as_query_engine(<br/>    similarity_top_k=5, <br/>    sparse_top_k=5, <br/>    vector_store_query_mode="hybrid", <br/>    **kwargs<br/>)</span></pre><ul class=""><li id="ed5f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">We can adjust the weight for both vector and keyword searches by varying an <code class="cx ny nz oa ob b">alpha</code> parameter. An <code class="cx ny nz oa ob b">alpha</code> equal to 1 corresponds to a pure vector search, while an <code class="cx ny nz oa ob b">alpha</code> equal to 0 corresponds to a pure keyword search. Below is the final definition of the <code class="cx ny nz oa ob b">query_engine</code> after incorporating this alpha parameter:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="d246" class="ql od fq ob b bg qm qn l qo qp">query_engine = index.as_query_engine(<br/>    similarity_top_k=5, <br/>    sparse_top_k=5, <br/>    alpha=0.5,<br/>    vector_store_query_mode="hybrid", <br/>    **kwargs<br/>)</span></pre><p id="8cb2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To assess the relevance of hybrid search, we conducted three experiments with varying <code class="cx ny nz oa ob b">alpha</code> parameters: <code class="cx ny nz oa ob b">alpha = 0.2, 0.5, 0.8</code>. Note that the base experiment corresponds to <code class="cx ny nz oa ob b">alpha=1</code>, representing a pure vector search.</p><p id="7a6a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are the final results for the context relevancy and faithfulness metrics. Please note that embedding the <code class="cx ny nz oa ob b">~2800</code> nodes with the <code class="cx ny nz oa ob b">"naver/efficient-splade-VI-BT-large-doc"</code> model took us about <code class="cx ny nz oa ob b">27 minutes</code> on an M1 Mac without a GPU, and it used almost all the system memory. Therefore, you might need a GPU to speed up the embedding process.</p><figure class="mm mn mo mp mq mr"><div class="ra io l ed"><div class="rb rc l"/></div></figure><p id="f4ca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The hybrid-search with <code class="cx ny nz oa ob b">alpha=0.8</code> shows a slight improvement in the context relevancy score, but putting more weight on keyword search decreases the scores. The faithfulness scores are overall preserved.</p><p id="7942" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Remember that the evaluation questions are quite general and lean more towards semantic search than exact-match search. However, in real-world scenarios, a user might ask for a specific article from the code, such as “what does the article x.y discuss?” In such cases, a keyword search can be helpful. Thus, we’ll retain the hybrid-search for future experiments with <code class="cx ny nz oa ob b">alpha=0.8</code>, despite an additional hallucination case compared to the pure vector search case.</p><h2 id="2b7c" class="pr od fq bf oe ps pt pu oh pv pw px ok nl py pz qa np qb qc qd nt qe qf qg qh bk">3- Query rewriting</h2><p id="632e" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion/" rel="noopener ugc nofollow" target="_blank">Query rewriting</a> involves generating various questions similar to a specific query. This process can be used for disambiguation, error correction, or adapting the query to a specific knowledge base that supports the RAG system.</p><p id="70dd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will use the <code class="cx ny nz oa ob b">QueryFusionRetriever</code> from llama-index for query rewriting. This module generates similar queries to the user query, retrieves and reranks the top <code class="cx ny nz oa ob b">n</code> nodes from each generated query, including the original one, using the <code class="cx ny nz oa ob b">Reciprocal Rerank Fusion</code> algorithm. This method, detailed in this <a class="af pd" href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf" rel="noopener ugc nofollow" target="_blank">paper</a>, offers an efficient way to rerank retrieved nodes without excessive computation or dependence on external models.</p><p id="f0f0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below is the Python code for this task (located in <code class="cx ny nz oa ob b">query/query_engine.py</code>):</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="1256" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core.query_engine import BaseQueryEngine, RetrieverQueryEngine<br/>from llama_index.core import VectorStoreIndex<br/>from llama_index.core.retrievers import QueryFusionRetriever<br/><br/>from query.constants import QUERY_GEN_PROMPT<br/><br/><br/>def get_query_fusion_retrieval(<br/>    index: VectorStoreIndex,<br/>    postprocessors_list: list,<br/>    similarity_top_k: int = 5,<br/>    sparse_top_k: int = 0,<br/>    hybrid_search_alpha: float = 0.5,<br/>    hybrid_search: bool = False,<br/>    num_generated_questions: int = 4,<br/>) -&gt; BaseQueryEngine:<br/>    kwargs = {"similarity_top_k": similarity_top_k}<br/>    if hybrid_search:<br/>        kwargs.update(<br/>            {<br/>                "vector_store_query_mode": "hybrid",<br/>                "sparse_top_k": sparse_top_k,<br/>                "alpha": hybrid_search_alpha,<br/>            }<br/>        )<br/><br/>    retriever = index.as_retriever(**kwargs)<br/><br/>    retriever = QueryFusionRetriever(<br/>        [retriever],<br/>        similarity_top_k=similarity_top_k,<br/>        num_queries=num_generated_questions,  # set this to 1 to disable query generation<br/>        mode="reciprocal_rerank",<br/>        use_async=False,<br/>        verbose=False,<br/>        query_gen_prompt=QUERY_GEN_PROMPT,<br/>    )<br/><br/>    query_engine = RetrieverQueryEngine.from_args(<br/>        retriever=retriever, node_postprocessors=postprocessors_list<br/>    )<br/>    query_engine = update_prompts_for_query_engine(query_engine)<br/>    return query_engine</span></pre><ul class=""><li id="28da" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk"><code class="cx ny nz oa ob b">num_queries</code> represents the total number of queries : <code class="cx ny nz oa ob b">num_queries-1</code> generated queries, plus <code class="cx ny nz oa ob b">1</code> for the original query. For subsequent experiments, we set <code class="cx ny nz oa ob b">num_queries=4</code>.</li><li id="3f48" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><code class="cx ny nz oa ob b">QUERY_GEN_PROMPT</code> is a prompt specifically designed for generating similar queries. We create a custom version of this prompt for the civil code knowledge base.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="502c" class="ql od fq ob b bg qm qn l qo qp">QUERY_GEN_PROMPT = (<br/>    "You are a helpful assistant that generates multiple search queries based on a "<br/>    "single input query. Generate {num_queries} search queries, one on each line, "<br/>    "related to the following input query. The queries must be in French and specifically "<br/>    "adapted to query the French Civil Code, and they must address the ambiguities in the input query:\n"<br/>    "Query: {query}\n"<br/>    "Queries:\n"<br/>)</span></pre><p id="1913" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Up until now, the evaluation questions we used were already clean and tailored for the civil code, as we instructed GPT-4 to generate them this way. However, as we mentioned earlier, this may not accurately represent the real-world users, who may ask ambiguous, error-prone, and short questions. Therefore, we generated another <code class="cx ny nz oa ob b">50</code> questions using GPT-4, instructing the LLM to generate questions with these more realistic characteristics. Here are some examples:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="1d25" class="ql od fq ob b bg qm qn l qo qp">[<br/>    "C'est quoi un contrat de mariage?",<br/>    "On peut changer de prénom facilement?",<br/>    "Qu'est-ce qui se passe si on trouve un trésor chez quelqu'un d'autre?",<br/>    "Comment on fait pour adopter un enfant?",<br/>    "C'est quoi exactement une servitude?",<br/>    "Si je construis un truc chez le voisin sans faire exprès, je dois le démolir?",<br/>]</span></pre><p id="d552" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Or the english translation:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="1d72" class="ql od fq ob b bg qm qn l qo qp">[<br/>    "What is a marriage contract?",<br/>    "Can we change our first name easily?",<br/>    "What happens if we find a treasure on someone else's property?",<br/>    "How do we adopt a child?",<br/>    "What exactly is an easement?",<br/>    "If I accidentally build something on the neighbor's property, do I have to demolish it?",<br/>]</span></pre><p id="d692" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will use this new evaluation dataset to compare the performance of pipelines with and without query rewriting. The comparison will be based on two metrics: <strong class="ne fr">Faithfulness</strong> and <strong class="ne fr">Answer Relevancy</strong>. Similar to context relevancy, answer relevancy measures the relevance of the answer to the user query, providing a score between 0 and 1. The results are as follows:</p><figure class="mm mn mo mp mq mr"><div class="ra io l ed"><div class="rb rc l"/></div></figure><p id="7f1f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We observe that query rewriting improves not only faithfulness, where the model doesn’t hallucinate at all in the 50 questions, but also slightly boosts the answer relevancy score.</p><p id="f39c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is an example of how an original question was rewritten into three other questions:</p><ul class=""><li id="5844" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Original question: <code class="cx ny nz oa ob b">Can I refuse an inheritance?</code></li><li id="f744" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">The three generated queries:</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="f809" class="ql od fq ob b bg qm qn l qo qp"><br/>1 - What are the legal procedures for refusing an inheritance according to the French Civil Code?<br/>2 - What are the rights and obligations of an heir who wishes to renounce a succession in France?<br/>3 - How does the renunciation of an inheritance proceed under the French Civil Code?</span></pre></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f638" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Routing</h1><p id="254b" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Until now, our knowledge base includes only one legal code for each RAG pipeline. Naturally, one might wonder how to add more legal codes, such as the penal code or traffic laws code, and how to construct a system that retrieves data from the correct legal code(s) when given a query. We can accomplish this in at least three ways with llama-index:</p><ul class=""><li id="1a41" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Store all data from the different legal codes in the same index and build the query engine just like before.</li><li id="492e" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Store all data in the same index but add the legal code name as metadata. During a query, the llama-index <code class="cx ny nz oa ob b"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/examples/vector_stores/chroma_auto_retriever/" rel="noopener ugc nofollow" target="_blank">Auto-retriever</a></code> module infers a set of metadata filters and the appropriate query string to pass to the vector database. The <code class="cx ny nz oa ob b">auto-retriever</code> must determine the correct code name and then execute a similarity search on the corresponding nodes to retrieve the data.</li><li id="9728" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Store every Legal Code nodes in a separate index, and use llama-index <code class="cx ny nz oa ob b">Routing</code> feature to select the most relevant index or indices.</li></ul><p id="92a9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this project, we opted for the last option. A more effective approach would be to evaluate these three options and select the best one. However we did not perform this evaluation due to time and space constraints, as this article is already extensive.</p><p id="70aa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/router/" rel="noopener ugc nofollow" target="_blank">Routers</a> are modules that take in a query and a set of <strong class="ne fr">choices</strong> and return one or more selected choices.</p><p id="2561" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The selection process is conducted by an LLM. In our use case, the selector receives the legal code descriptions as input and returns one or more query engines. Each engine represents a RAG system for a single code.</p><p id="81fc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is how we experiment routing in this project:</p><ul class=""><li id="8882" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">We first define the list of law Codes to consider and their corresponding descriptions. The codes that we used for the routing experiment are the civil code, the taxes code, the intellectual property code, the traffic laws code and the labor code.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="5af3" class="ql od fq ob b bg qm qn l qo qp">codes_to_description = {<br/>    "Code civil": "Code civil: code juridique qui regroupe les lois relatives au droit civil français, c’est-à-dire l'ensemble des règles qui déterminent le statut des personnes (livre Ier), celui des biens (livre II) et celui des relations entre les personnes privées (livres III et IV).",<br/>    "Code général des impôts": "Code général des impôts: code juridique qui regroupe les lois relatives aux impôts en France, c’est-à-dire l'ensemble des règles qui déterminent les impôts et les taxes.",<br/>    "Code de la propriété intellectuelle": "Code de la propriété intellectuelle: code juridique qui regroupe les lois relatives à la propriété intellectuelle en France, c’est-à-dire l'ensemble des règles qui déterminent les droits des auteurs, des artistes-interprètes, des producteurs de phonogrammes et de vidéogrammes et des entreprises de communication audiovisuelle.",<br/>    "Code de la route": "Code de la route: code juridique qui regroupe les lois relatives à la circulation routière en France, c’est-à-dire l'ensemble des règles qui déterminent les droits et les devoirs des usagers de la route.",<br/>    "Code du travail": "Code du travail: code juridique qui regroupe les lois relatives au droit du travail en France, c’est-à-dire l'ensemble des règles qui déterminent les droits et les devoirs des employeurs et des salariés.",<br/>}</span></pre><ul class=""><li id="fb8f" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">We instantiate the <code class="cx ny nz oa ob b">query_engine</code> for each code, as previously done. Then we wrap every <code class="cx ny nz oa ob b">query_engine</code> in a <code class="cx ny nz oa ob b">QueryEngineTool</code> module along with its description.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="da7f" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core.tools import QueryEngineTool<br/><br/>from query.query_engine import create_query_engine<br/><br/>def get_tools():<br/>    tools = []<br/>    for code_name, code_description in codes_to_description.items():<br/>        query_engine = create_query_engine(code_name=code_name)<br/>        tool = QueryEngineTool.from_defaults(<br/>            query_engine=query_engine,<br/>            description=code_description,<br/>        )<br/>        tools.append(tool)<br/>    return tools</span></pre><ul class=""><li id="0ca8" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Finally we create the Router query engine using the list of tools and the llama-index <code class="cx ny nz oa ob b">LLMMultiSelector</code> module. LLM selectors put the choices as a text dump into a prompt and use LLM text completion endpoint to make decisions.</li></ul><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="dc7c" class="ql od fq ob b bg qm qn l qo qp">from llama_index.core.query_engine import RouterQueryEngine<br/>from llama_index.core.selectors import LLMMultiSelector<br/><br/>def create_routing_engine():<br/>    query_engine_tools = get_tools()<br/>    query_engine = RouterQueryEngine(<br/>        selector=LLMMultiSelector.from_defaults(),<br/>        query_engine_tools=query_engine_tools,<br/>    )<br/>    return query_engine</span></pre><p id="ef6d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that to keep things simple, we did not include any advanced features in the individual codes query engines.</p><p id="9c45" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, let’s test the router query engine on a query with two themes: labor code and tax code. This would allow us to see whether or not the router selects the correct indexes.</p><p id="4766" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Query:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="4f44" class="ql od fq ob b bg qm qn l qo qp">Quelles sont les conditions pour bénéficier d'un congé sabbatique \n Comment sont imposées les plus-values immobilières</span></pre><p id="67d2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Or its english translation:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="ab86" class="ql od fq ob b bg qm qn l qo qp">What are the conditions to qualify for a sabbatical leave? \n How are capital gains on real estate taxed?</span></pre><p id="dce6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we can check the response given by the router query engine:</p><pre class="mm mn mo mp mq qi ob qj bp qk bb bk"><span id="28c7" class="ql od fq ob b bg qm qn l qo qp">{<br/>  "response": "Pour bénéficier d'un congé sabbatique, le salarié doit justifier d'une ancienneté minimale dans l'entreprise, cumulée sur plusieurs périodes non consécutives, ainsi que de six années d'activité professionnelle. De plus, il ne doit pas avoir bénéficié depuis une durée minimale, dans la même entreprise, d'un congé sabbatique, d'un congé pour création d'entreprise ou d'un congé spécifique d'une durée d'au moins six mois.\n\nLes plus-values provenant de la cession en cours d'exploitation des éléments de l'actif immobilisé et réalisées avant l'entrée en vigueur des dispositions spécifiques ne sont pas comprises dans le bénéfice imposable si le contribuable s'engage à réinvestir dans des immobilisations dans son entreprise dans un délai de trois ans.",<br/>  "source_nodes": [<br/>    {<br/>      ....<br/>      "text": "A défaut de convention ou d'accord mentionné à l'article L. 3142-32, le salarié informe l'employeur de la date de départ en congé sabbatique qu'il a choisie et de la durée de ce congé, par tout moyen conférant date certaine, au moins trois mois à l'avance."<br/>    },<br/>    {<br/>      ....<br/>      "text": "L'employeur informe le salarié de son accord sur la date de départ choisie du congé sabbatique ou de son report par tout moyen conférant date certaine."<br/>    },<br/>    {<br/>      ....<br/>      "text": "A défaut de convention ou d'accord mentionné à l'article L. 3142-32, le départ en congé peut être différé par l'employeur dans les conditions mentionnées au premier alinéa de l'article L. 3142-29, de telle sorte que le pourcentage des salariés simultanément absents de l'entreprise au titre du congé sabbatique ne dépasse pas 1,5 % de l'effectif de cette entreprise, jusqu'à la date à laquelle cette condition de taux est remplie ou que le nombre de jours d'absence au titre du congé sabbatique ne dépasse pas 1,5 % du nombre de jours de travail effectués dans les douze mois précédant le départ en congé. Pour permettre le départ en congé d'un salarié, cette période de douze mois est prolongée dans la limite de quarante-huit mois."<br/>    },<br/>    {<br/>      ....<br/>      "text": "Le salarié a droit à un congé sabbatique pendant lequel son contrat de travail est suspendu.\nLe droit à ce congé est ouvert au salarié justifiant, à la date de départ en congé, d'une ancienneté minimale dans l'entreprise, cumulée, le cas échéant, sur plusieurs périodes non consécutives, ainsi que de six années d'activité professionnelle et n'ayant pas bénéficié depuis une durée minimale, dans la même entreprise, d'un congé sabbatique, d'un congé pour création d'entreprise ou d'un congé spécifique mentionné à l'article L. 6323-17-1 d'une durée d'au moins six mois. L'ancienneté acquise dans toute autre entreprise du même groupe, au sens de l'article L. 2331-1, est prise en compte au titre de l'ancienneté dans l'entreprise."<br/>    }, ...<br/>  ],<br/>  "metadata": {<br/>    "selector_result": {<br/>      "selections": [<br/>        {<br/>          "index": 4,<br/>          "reason": "Le Code du travail regroupe les lois relatives au droit du travail en France, ce qui inclut les conditions pour bénéficier d'un congé sabbatique."<br/>        },<br/>        {<br/>          "index": 1,<br/>          "reason": "Le Code général des impôts regroupe les lois relatives aux impôts en France, ce qui inclut les règles sur l'imposition des plus-values immobilières."<br/>        }<br/>      ]<br/>    }<br/>  }<br/>}</span></pre><p id="33e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The important result here is the <code class="cx ny nz oa ob b">selector_result</code>, which shows that the multi-selector correctly identified the labor code (index 4) and the tax code (index 1) as the relevant data sources for answering the user query.</p><p id="6516" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can find all the evaluation experiments plus the routing query engine definition in the notebook <code class="cx ny nz oa ob b">./notebooks/evaluate_with_llamaindex.ipynb</code> .</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="24c9" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">Conclusion</h1><p id="4087" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">In this article, we discussed the transition from a basic RAG system to an advanced one, incorporating windowing, hybrid-search, and query rewriting, using the<code class="cx ny nz oa ob b"> llama-index</code> framework. We also explored how to evaluate certain components of the RAG pipeline, like the embeddings and the previously mentioned features, enabling informed decisions about their relevance, rather than relying on intuition.</p><p id="a431" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We also looked at how routing allows us to select relevant indexes using LLM decision making.</p><p id="37f8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here are some suggestions to further enhance the pipeline:</p><ul class=""><li id="bfdf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qq qr qs bk">Throughout this project, we used <code class="cx ny nz oa ob b">gpt-3.5-turbo</code> as the LLM for generating responses, due to its ease of use and low cost. Similar to how we assessed various components of the RAG system, we could evaluate the LLM based on metrics like answer relevance and faithfulness.</li><li id="ed2f" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">The same comment applies to using <code class="cx ny nz oa ob b">gpt-3.5-turbo</code> as an evaluator. Employing a more capable Language Model might enhance the evaluation accuracy.</li><li id="6bd6" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Ideally, we’d assess all combinations of the RAG pipeline using various metrics. However, this might be costly and potentially unfeasible.</li><li id="0da6" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">It might also be worthwhile to consider evaluating other advanced techniques in our system, such as the use of external models for reranking, using agents, etc.</li><li id="5f44" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">We can also enhance the multilingual support of the RAG pipeline. One approach could be to set up a chain of LLMs to perform the following tasks: First, translating the query from the original language to French, then generating the response as usual, and finally, translating the response back to the original language.</li></ul><p id="f9b5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In conclusion, readers looking to integrate a similar RAG system for legal queries should proceed cautiously due to the complexity and potential impact of legal advice. The risk of such systems generating <strong class="ne fr">hallucinations</strong> is a concern. Our evaluation, despite having high faithfulness scores, is limited by a small number and a limited scope of the questions, along with the inherent constraints of the evaluator module.</p><p id="3bd6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To address this, we recommend adding additional layers of evaluation and safety protocols. Moreover, despite the project showcasing the capabilities of advanced RAG settings for production, it is not yet a finalized system. This underscores the importance of continuous refinement and thorough testing before full-scale deployment. Finally, users should also be reminded that the system’s advice does not replace professional legal counsel.</p></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7830" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">References</h1><ul class=""><li id="8f09" class="nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx qq qr qs bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/getting_started/concepts/" rel="noopener ugc nofollow" target="_blank">llama-index high level concepts.</a></li><li id="3661" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/optimizing/evaluation/evaluation/" rel="noopener ugc nofollow" target="_blank">llama-index evaluation</a></li><li id="ff4e" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><a class="af pd" href="https://docs.llamaindex.ai/en/v0.10.20/examples/vector_stores/qdrant_hybrid.html" rel="noopener ugc nofollow" target="_blank">Qdrant hybrid search.</a></li><li id="af21" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><a class="af pd" href="https://qdrant.tech/articles/fastembed/" rel="noopener ugc nofollow" target="_blank">Qdrant fast-embed</a></li><li id="61e2" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><a class="af pd" href="https://docs.llamaindex.ai/en/stable/module_guides/querying/router/" rel="noopener ugc nofollow" target="_blank">llama-index routers</a></li><li id="f725" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk"><a class="af pd" href="https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf" rel="noopener ugc nofollow" target="_blank">Reciprocal Rank Fusion outperforms Condorcet and individual Rank Learning Methods</a> Paper</li></ul></div></div></div><div class="ab cb pe pf pg ph" role="separator"><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk pl"/><span class="pi by bm pj pk"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3cdf" class="oc od fq bf oe of pm gq oh oi pn gt ok ol po on oo op pp or os ot pq ov ow ox bk">To reach out</h1><ul class=""><li id="a325" class="nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx qq qr qs bk">LinkedIn : <a class="af pd" href="https://www.linkedin.com/in/hamza-gharbi-043045151/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/hamza-gharbi-043045151/</a></li><li id="04d6" class="nc nd fq ne b go qt ng nh gr qu nj nk nl qv nn no np qw nr ns nt qx nv nw nx qq qr qs bk">Twitter : <a class="af pd" href="https://twitter.com/HamzaGh25079790" rel="noopener ugc nofollow" target="_blank">https://twitter.com/HamzaGh25079790</a></li></ul></div></div></div></div>    
</body>
</html>