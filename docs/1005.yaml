- en: The Math Behind “The Curse of Dimensionality”
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “维度灾难”背后的数学原理
- en: 原文：[https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20](https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20](https://towardsdatascience.com/the-math-behind-the-curse-of-dimensionality-cf8780307d74?source=collection_archive---------0-----------------------#2024-04-20)
- en: Dive into the “Curse of Dimensionality” concept and understand the math behind
    all the surprising phenomena that arise in high dimensions.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解“维度灾难”这一概念，并理解所有在高维中出现的惊人现象背后的数学原理。
- en: '[](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)[![Maxime
    Wolf](../Images/259b3659d0e6dd1d0f0eec4ae92d02e9.png)](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)
    [Maxime Wolf](https://medium.com/@maxwolf34?source=post_page---byline--cf8780307d74--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)
    ·9 min read·Apr 20, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--cf8780307d74--------------------------------)
    ·阅读时长9分钟·2024年4月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/60a2c60842e2da9c024edd998aef3721.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60a2c60842e2da9c024edd998aef3721.png)'
- en: Image from Dall-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Dall-E
- en: In the realm of machine learning, handling high-dimensional vectors is not just
    common; it’s essential. This is illustrated by the architecture of popular models
    like Transformers. For instance, BERT uses 768-dimensional vectors to encode the
    tokens of the input sequences it processes and to better capture complex patterns
    in the data. Given that our brain struggles to visualize anything beyond 3 dimensions,
    the use of 768-dimensional vectors is quite mind-blowing!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，处理高维向量不仅是常见的，而且是至关重要的。这一点可以通过流行模型如Transformer的架构来说明。例如，BERT使用768维的向量来编码其处理的输入序列的标记，并更好地捕捉数据中的复杂模式。考虑到我们的大脑难以想象超过三维的任何事物，使用768维的向量确实令人震惊！
- en: While some Machine and Deep Learning models excel in these high-dimensional
    scenarios, they also present many challenges. In this article, we will explore
    the concept of the “curse of dimensionality”, explain some interesting phenomena
    associated with it, delve into the mathematics behind these phenomena, and discuss
    their general implications for your Machine Learning models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些机器学习和深度学习模型在这些高维场景中表现出色，但它们也带来了许多挑战。在本文中，我们将探讨“维度灾难”这一概念，解释与之相关的一些有趣现象，深入研究这些现象背后的数学原理，并讨论它们对你的机器学习模型的普遍影响。
- en: Note that detailed mathematical proofs related to this article are [available
    on my website](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/)
    as a supplementary extension to this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与本文相关的详细数学证明[可以在我的网站上找到](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/)，作为本文的补充延伸。
- en: What is the curse of dimensionality?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是维度灾难？
- en: People often assume that geometric concepts familiar in three dimensions behave
    similarly in higher-dimensional spaces. **This is not the case**. As dimension
    increases, many interesting and counterintuitive phenomena arise. The “Curse of
    Dimensionality” is a term invented by Richard Bellman (famous mathematician) that
    refers to all these surprising effects.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常常认为在三维空间中熟悉的几何概念在更高维空间中也会表现得类似，**但事实并非如此**。随着维度的增加，许多有趣且反直觉的现象会出现。“维度灾难”是著名数学家理查德·贝尔曼（Richard
    Bellman）提出的术语，用来描述所有这些令人惊讶的效应。
- en: 'What is so special about high-dimension is how the “volume” of the space (we’ll
    explore that in more detail soon) is growing **exponentially**. Take a graduated
    line (in one dimension) from 1 to 10\. There are 10 integers on this line. Extend
    that in 2 dimensions: it is now a square with 10 × 10 = 100 points with integer
    coordinates. Now consider “only” 80 dimensions: you would already have **10⁸⁰
    points** which is the number of atoms in the universe.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高维空间的特殊之处在于其“体积”（我们很快会更详细地探讨）是**指数增长**的。假设有一条从1到10的刻度线（1维），这条线有10个整数。将其扩展到2维：它现在是一个边长为10
    × 10 = 100的正方形，包含100个整数坐标的点。现在考虑“仅仅”80维：你将已经有**10⁸⁰个点**，这相当于宇宙中的原子数量。
- en: In other words, as the dimension increases, the volume of the space grows exponentially,
    resulting in data becoming **increasingly sparse**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，随着维度的增加，空间的体积呈指数增长，导致数据变得**越来越稀疏**。
- en: High-dimensional spaces are “empty”
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高维空间是“空的”
- en: 'Consider this other example. We want to calculate the farthest distance between
    two points in a unit hypercube (where each side has a length of 1):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是这样的。我们想计算在一个单位超立方体内两个点之间的最大距离（每条边的长度为1）：
- en: In **1 dimension** (the hypercube is a line segment from 0 to 1), the maximum
    distance is simply 1.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**1维**（超立方体是从0到1的线段）中，最大距离仅为1。
- en: In **2 dimensions** (the hypercube forms a square), the maximum distance is
    the distance between the opposite corners [0,0] and [1,1], which is √2, calculated
    using the Pythagorean theorem.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**2维**（超立方体形成一个正方形）中，最大距离是对角线两点[0,0]和[1,1]之间的距离，即√2，通过毕达哥拉斯定理计算得出。
- en: Extending this concept to **n dimensions**, the distance between the points
    at [0,0,…,0] and [1,1,…,1] is √n. This formula arises because each additional
    dimension adds a square of 1 to the sum under the square root (again by the Pythagorean
    theorem).
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这个概念扩展到**n维**，位于[0,0,…,0]和[1,1,…,1]的两点之间的距离是√n。这个公式产生的原因是每增加一个维度，都为平方根下的和添加一个1的平方（再次应用毕达哥拉斯定理）。
- en: Interestingly, as the number of dimensions n increases, the largest distance
    within the hypercube grows at an O(√n) rate. This phenomenon illustrates a **diminishing
    returns effect**, where increases in dimensional space lead to proportionally
    smaller gains in spatial distance. More details on this effect and its implications
    will be explored in the following sections of this article.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，随着维度n的增加，超立方体内的最大距离以O(√n)的速度增长。这个现象展示了**递减收益效应**，即维度空间的增加导致空间距离的增益成比例地减少。关于这一效应及其影响的更多细节将在本文接下来的部分中讨论。
- en: The notion of distance in high dimensions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高维中的距离概念
- en: Let’s dive deeper into the notion of distances we started exploring in the previous
    section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下在上一节中开始探索的距离概念。
- en: We had our first glimpse of how high-dimensional spaces render the notion of
    distance almost **meaningless**. But what does this really mean, and can we mathematically
    visualize this phenomenon?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次看到了高维空间如何使得距离的概念几乎变得**毫无意义**。但这到底意味着什么，我们能否在数学上可视化这种现象呢？
- en: 'Let’s consider an experiment, using the same n-dimensional unit hypercube we
    defined before. First, we generate a dataset by randomly sampling many points
    in this cube: we effectively simulate a multivariate uniform distribution. Then,
    we sample another point (a “query” point) from that distribution and observe the
    **distance from its nearest and farthest neighbor in our dataset**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个实验，使用之前定义的n维单位超立方体。首先，我们通过在这个立方体中随机采样许多点来生成一个数据集：我们实际上模拟了一个多变量均匀分布。然后，我们从这个分布中抽取另一个点（一个“查询”点），并观察**其最近邻和最远邻在数据集中的距离**。
- en: Here is the corresponding Python code.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对应的Python代码。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also plot these distances:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制这些距离：
- en: '![](../Images/3883ee3555329e4c7243c79e4f003a80.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3883ee3555329e4c7243c79e4f003a80.png)'
- en: Distances to nearest and farthest points as n increases (Image by the author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着n增加，最近邻和最远邻的距离（图由作者提供）
- en: Using a log scale, we observe that the **relative** difference between the distance
    to the nearest and farthest neighbor tends to decrease as the dimension increases.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数尺度，我们观察到，最近邻和最远邻之间的**相对**差异随着维度的增加而趋于减小。
- en: 'This is a very unintuitive behavior: as explained in the previous section,
    points are very sparse from each other because of the exponentially increasing
    volume of the space, but at the same time, the **relative** distances between
    points become smaller.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常违反直觉的现象：正如前一部分所解释的那样，由于空间体积的指数增长，点之间的距离非常稀疏，但与此同时，点之间的**相对**距离变得更小。
- en: The notion of nearest neighbors vanishes
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最近邻的概念消失了
- en: This means that the very concept of **distance** becomes less relevant and less
    discriminative as the dimension of the space increases. As you can imagine, it
    poses problems for Machine Learning algorithms that solely rely on distances such
    as kNN.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，**距离**的概念随着空间维度的增加变得越来越不相关，也越来越难以区分。正如你可以想象的那样，这会给完全依赖于距离的机器学习算法（如kNN）带来问题。
- en: 'The maths: the n-ball'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学：n-球
- en: We will now talk about some other interesting phenomena. For this, we’ll need
    the **n-ball**. A n-ball is the generalization of a ball in n dimensions. The
    n-ball of radius R is the collection of points at a distance at most R from the
    center of the space 0.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论一些其他有趣的现象。为此，我们需要使用**n-球**。n-球是n维空间中球的推广。半径为R的n-球是距离空间中心0的点集合，其距离最大为R。
- en: 'Let’s consider a radius of 1\. The 1-ball is the segment [-1, 1]. The 2-ball
    is the disk delimited by the unit circle, whose equation is x² + y² ≤ 1\. The
    3-ball (what we normally call a “ball”) has the equation x² + y² + z² ≤ 1\. As
    you understand, we can extend that definition to any dimension:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑半径为1的情况。1-球是区间[-1, 1]。2-球是由单位圆限定的圆盘，其方程为x² + y² ≤ 1。3-球（我们通常所说的“球”）的方程是x²
    + y² + z² ≤ 1。正如你所理解的，我们可以将这一定义扩展到任意维度：
- en: '![](../Images/2f779b2a5adbb8ae99f962d43451eec8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f779b2a5adbb8ae99f962d43451eec8.png)'
- en: 'The question now is: what is the volume of this ball? This is not an easy question
    and requires quite a lot of maths, which I won’t detail here. However, you can
    find all the details on my website, in [my post about the volume of the n-ball](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：这个球的体积是多少？这个问题并不简单，需要用到很多数学知识，我在这里不会详细说明。然而，你可以在我的网站上找到所有的细节，参见[我关于n-球体积的文章](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/)。
- en: After a lot of fun (integral calculus), you can prove that the volume of the
    n-ball can be expressed as follows, where Γ denotes the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一番有趣的（积分学）推导后，你可以证明n-球的体积可以表示为如下形式，其中Γ表示[伽马函数](https://en.wikipedia.org/wiki/Gamma_function)。
- en: '![](../Images/bfc169b77c2a4a0f38826c57c315581b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfc169b77c2a4a0f38826c57c315581b.png)'
- en: For example, with R = 1 and n = 2, the volume is πR², because Γ(2) = 1\. This
    is indeed the “volume” of the 2-ball (also called the “area” of a circle in this
    case).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于R = 1和n = 2，体积为πR²，因为Γ(2) = 1。这确实是2-球的“体积”（在这种情况下也叫做“圆的面积”）。
- en: However, beyond being an interesting mathematical challenge, the volume of the
    n-ball also has some very surprising properties.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了作为一个有趣的数学挑战，n-球的体积还有一些非常惊人的性质。
- en: As the dimension n increases, the volume of the n-ball converges to 0.
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着维度n的增加，n-球的体积趋近于0。
- en: This is true for every radius, but let’s visualize this phenomenon with a few
    values of R.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点对于每个半径都适用，但让我们通过几个R值来直观地展示这一现象。
- en: '![](../Images/102da1b7faa74a52428ed07b0c7410e9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/102da1b7faa74a52428ed07b0c7410e9.png)'
- en: Volume of the n-ball for different radii as the dimension increases (Image by
    the author)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度增加，不同半径的n-球体积（图片来源：作者）
- en: As you can see, it only converges to 0, but it starts by increasing and then
    decreases to 0\. For R = 1, the ball with the largest volume is the 5-ball, and
    the value of n that reaches the maximum shifts to the right as R increases.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它最终趋近于0，但开始时会增加，之后又下降到0。对于R = 1，体积最大的球是5-球，且随着R的增大，达到最大值的n值向右移动。
- en: Here are the first values of the volume of the unit n-ball, up to n = 10.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是单位n-球体积的前几个值，直到n = 10。
- en: '![](../Images/312677bf51a884e6ab4922cfb844281b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/312677bf51a884e6ab4922cfb844281b.png)'
- en: Volume of the unit n-ball for different values of n (Image by the author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 不同n值的单位n-球体积（图片来源：作者）
- en: The volume of a high-dimensional unit ball is concentrated near its surface.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高维单位球的体积集中在其表面附近。
- en: 'For small dimensions, the volume of a ball looks quite “homogeneous”: this
    is not the case in high dimensions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于低维度，球的体积看起来相当“均匀”：但在高维度中情况并非如此。
- en: '![](../Images/317563e523354c4b7f351acfbec15f6d.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/317563e523354c4b7f351acfbec15f6d.png)'
- en: A spherical shell
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个球壳
- en: Let’s consider an n-ball with radius R and another with radius R-dR where dR
    is very small. The portion of the n-ball between these 2 balls is called a “shell”
    and corresponds to the portion of the ball near its surface (see the visualization
    above in 3D). We can compute the ratio of the “inner” volume of the ball and the
    volume of the thin shell only.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个半径为 R 的 n-球和另一个半径为 R-dR 的 n-球，其中 dR 非常小。两个球之间的部分被称为“壳”，对应于球体表面附近的部分（见上图中的
    3D 可视化）。我们可以计算球体“内部”体积与薄壳体积的比率。
- en: '![](../Images/1f1fe49de6e6372109d690b4fd622993.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f1fe49de6e6372109d690b4fd622993.png)'
- en: Ratio (inner volume / total volume) as n increases (Image by the author)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 n 增加，比例（内部体积 / 总体积）（作者提供的图像）
- en: 'As we can see, it converges very quickly to 0: almost all the volume is near
    the surface in high dimensional spaces. For instance, for R = 1, dR = 0.05, and
    n = 50, about 92.3% of the volume is concentrated in the thin shell. This shows
    that in higher dimensions, the volume is in “corners”. This is again related to
    the distortion of the concept of distance we have seen earlier.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，它迅速收敛到 0：在高维空间中，几乎所有的体积都集中在表面附近。例如，对于 R = 1，dR = 0.05 和 n = 50，约 92.3%
    的体积集中在薄壳中。这表明，在更高维度下，体积集中在“角落”里。这再次与我们之前看到的距离概念的扭曲有关。
- en: Note that the volume of the unit hypercube (here, denoting a cube centered at
    zero with a side length of 2) is 2ⁿ. The unit sphere is basically “empty” in very
    high dimensions, while the unit hypercube, in contrast, gets exponentially more
    points. Again, this shows how the idea of a “nearest neighbor” of a point loses
    its effectiveness because there is almost no point within a distance R of a query
    point q when n is large.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，单位超立方体的体积（这里指的是一个以零为中心、边长为 2 的立方体）是 2ⁿ。单位球在非常高的维度下基本上是“空的”，而单位超立方体则与之相反，获得了指数级增长的更多点。再一次，这表明了“最近邻”概念的效果失效，因为当维度
    n 很大时，查询点 q 周围几乎没有点位于距离 R 内。
- en: Curse of dimensionality, overfitting, and Occam’s Razor
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度诅咒、过拟合和奥卡姆剃刀
- en: 'The curse of dimensionality is closely related to the overfitting principle.
    Because of the exponential growth of the volume of the space with the dimension,
    we need very large datasets to adequately capture and model high-dimensional patterns.
    Even worse: we need a number of samples that grows **exponentially** with the
    dimension to overcome this limitation. This scenario, characterized by many features
    yet relatively few data points, is particularly **prone to overfitting**.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 维度诅咒与过拟合原则密切相关。由于空间体积随着维度的指数增长，我们需要非常大的数据集来充分捕捉和建模高维模式。更糟糕的是：我们需要一个随着维度指数增长的样本数量来克服这一局限性。这种情况，特征很多而数据点相对较少，特别**容易过拟合**。
- en: Occam’s Razor suggests that **simpler models are generally better than complex
    ones** because they are less likely to overfit. This principle is particularly
    relevant in high-dimensional contexts (where the curse of dimensionality plays
    a role) because it encourages the reduction of model complexity.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 奥卡姆剃刀建议，**更简单的模型通常比复杂的模型更好**，因为它们不太可能过拟合。这个原则在高维上下文中特别重要（维度诅咒发挥作用的地方），因为它鼓励减少模型复杂度。
- en: Applying Occam’s Razor principle in high-dimensional scenarios can mean reducing
    the dimensionality of the problem itself (via methods like PCA, feature selection,
    etc.), thereby **mitigating some effects of the curse of dimensionality**. Simplifying
    the model’s structure or the feature space helps in managing the sparse data distribution
    and in making distance metrics more meaningful again. For instance, dimensionality
    reduction is a very common **preliminary step** before applying the kNN algorithm.
    More recent methods, such as **ANNs** (Approximate Nearest Neighbors) also emerge
    as a way to deal with high-dimensional scenarios.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维场景中应用奥卡姆剃刀原则，意味着减少问题本身的维度（通过 PCA、特征选择等方法），从而**缓解维度诅咒的某些影响**。简化模型结构或特征空间有助于管理稀疏的数据分布，并使得距离度量重新变得有意义。例如，降维是应用
    kNN 算法之前一个非常常见的**初步步骤**。更近的技术，如**ANNs**（近似最近邻），也作为应对高维场景的一种方法。
- en: Blessing of dimensionality?
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度的祝福？
- en: '![](../Images/8b198eacbd49528b84b7eff1706e8a13.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b198eacbd49528b84b7eff1706e8a13.png)'
- en: Image by Dall-E
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由 Dall-E 提供
- en: While we’ve outlined the challenges of high-dimensional settings in machine
    learning, there are also **some advantages**!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们概述了高维设置在机器学习中的挑战，但也有**一些优势**！
- en: High dimensions can enhance **linear separability**, making techniques like
    kernel methods more effective.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维度可以增强**线性可分性**，使得像核方法这样的技术更加有效。
- en: Additionally, deep learning architectures are **particularly adept** at navigating
    and extracting complex patterns from high-dimensional spaces.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，深度学习架构**尤其擅长**在高维空间中导航并提取复杂模式。
- en: 'As always with Machine Learning, **this is a trade-off**: leveraging these
    advantages involves balancing the increased computational demands with potential
    gains in model performance.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 和机器学习中的常见情况一样，**这是一种权衡**：利用这些优势需要在增加的计算需求与潜在的模型性能提升之间找到平衡。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully, this gives you an idea of how “weird” geometry can be in high-dimension
    and the many challenges it poses for machine learning model development. We saw
    how, in high-dimensional spaces, data is very sparse but also tends to be concentrated
    in the corners, and distances lose their usefulness. For a deeper dive into the
    n-ball and mathematical proofs, I encourage you to visit [the extended of this
    article on my website](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能让你对高维中的“奇异”几何和它对机器学习模型开发所带来的种种挑战有所了解。我们看到，在高维空间中，数据非常稀疏，但也倾向于集中在角落，并且距离失去了它的有效性。如果你想更深入了解n-球和数学证明，欢迎访问[我网站上的这篇文章的扩展版](https://www.maximewolf.com/blog/2024/The-Volume-of-the-nball/)。
- en: While the “curse of dimensionality” outlines significant limitations in high-dimensional
    spaces, it’s exciting to see how modern deep learning models are increasingly
    adept at navigating these complexities. Consider the embedding models or the latest
    LLMs, for example, which utilize very high-dimensional vectors to more effectively
    discern and model textual patterns.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“维度灾难”揭示了高维空间中的重大限制，但看到现代深度学习模型在应对这些复杂性方面越来越娴熟，令人兴奋。例如，考虑嵌入模型或最新的LLM，它们利用非常高维的向量更有效地识别和建模文本模式。
- en: 'Want to learn more about Transformers and how they transform your data under
    the hood? Check out my previous article:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于Transformers以及它们如何在幕后转换你的数据吗？请查看我之前的文章：
- en: '[](/transformers-how-do-they-transform-your-data-72d69e383e0d?source=post_page-----cf8780307d74--------------------------------)
    [## Transformers: How Do They Transform Your Data?'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/transformers-how-do-they-transform-your-data-72d69e383e0d?source=post_page-----cf8780307d74--------------------------------)
    [## Transformers: 它们如何转换你的数据？'
- en: Diving into the Transformers architecture and what makes them unbeatable at
    language tasks
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入了解Transformers架构及其为何在语言任务中无可匹敌
- en: towardsdatascience.com](/transformers-how-do-they-transform-your-data-72d69e383e0d?source=post_page-----cf8780307d74--------------------------------)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/transformers-how-do-they-transform-your-data-72d69e383e0d?source=post_page-----cf8780307d74--------------------------------)
- en: Feel free to connect on [LinkedIn](https://www.linkedin.com/in/maxime-wolf/)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随时在[LinkedIn](https://www.linkedin.com/in/maxime-wolf/)上与我联系
- en: Follow me on [GitHub](https://github.com/maxime7770) for more content
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [GitHub](https://github.com/maxime7770) 上关注我，获取更多内容
- en: 'Visit my website: [maximewolf.com](http://maximewolf.com)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问我的网站：[maximewolf.com](http://maximewolf.com)
- en: 'References:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] “Volume of an n-ball.” Wikipedia, [https://en.wikipedia.org/wiki/Volume_of_an_n-ball](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] “n-球的体积。” 维基百科， [https://en.wikipedia.org/wiki/Volume_of_an_n-ball](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)'
- en: '[2] “Curse of Dimensionality.” Wikipedia, [https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] “维度灾难。” 维基百科， [https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Blessing_of_dimensionality)'
- en: '[3] Peterson, Ivars. “An Adventure in the Nth Dimension.” American Scientist,
    [https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension](https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Peterson, Ivars. “Nth维的冒险。” 美国科学家， [https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension](https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension)'
- en: '[4] Zhang, Yuxi. “Curse of Dimensionality.” Geek Culture on Medium, July 2021,
    [https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f](https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zhang, Yuxi. “维度灾难。” Geek Culture on Medium，2021年7月，[https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f](https://medium.com/geekculture/curse-of-dimensionality-e97ba916cb8f)'
- en: '[5] “Approximate Nearest Neighbors (ANN).” Activeloop, [https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/](https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] “近似最近邻（ANN）。” Activeloop, [https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/](https://www.activeloop.ai/resources/glossary/approximate-nearest-neighbors-ann/)'
