<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Predicting metadata for humanitarian datasets with LLMs part 2 ‚Äî An alternative to fine-tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Predicting metadata for humanitarian datasets with LLMs part 2 ‚Äî An alternative to fine-tuning</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03">https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="gr gs gt gu gv ab"><div><div class="ab gw"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------" rel="noopener follow"><div class="l gx gy by gz ha"><div class="l ed"><img alt="Matthew Harris" class="l ep by dd de cx" src="../Images/4fa3264bb8a028633cd8d37093c16214.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*SQpPIBppBtQGfoSP_sAeaQ.png"/><div class="hb by l dd de em n hc eo"/></div></div></a></div></div><div class="hd ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------" rel="noopener follow"><div class="l he hf by gz hg"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hh cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hb by l br hh em n hc eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hi ab q"><div class="ab q hj"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hk hl bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hm" data-testid="authorName" href="https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------" rel="noopener follow">Matthew Harris</a></p></div></div></div><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hk hl dx"><button class="hp hq ah ai aj ak al am an ao ap aq ar hr hs ht" disabled="">Follow</button></p></div></div></span></div></div><div class="l hu"><span class="bf b bg z dx"><div class="ab cn hv hw hx"><div class="hy hz ab"><div class="bf b bg z dx ab ia"><span class="ib l hu">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hm ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------" rel="noopener follow"><p class="bf b bg z ic id ie if ig ih ii ij bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hn ho" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">29 min read</span><div class="ik il l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Aug 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp im in io ip iq ir is it iu iv iw ix iy iz ja jb"><div class="h k w ea eb q"><div class="jr l"><div class="ab q js jt"><div class="pw-multi-vote-icon ed ib ju jv jw"><div class=""><div class="jx jy jz ka kb kc kd am ke kf kg jw"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kh ki kj kk kl km kn"><p class="bf b dy z dx"><span class="jy">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao jx ko kp ab q ee kq kr" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="ks"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="kt k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ku an ao ap hr kv kw kx" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ky cn"><div class="l ae"><div class="ab cb"><div class="kz la lb lc ld le ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ku an ao ap hr lf lg kr lh li lj lk ll s lm ln lo lp lq lr ls u lt lu lv"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="lz ma mb mc md me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx ly"><img src="../Images/0002f919edfe77b2a945716650c673f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DUv-l9mO533S2C5oygl6-Q.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Source: GPT-4o</figcaption></figure><p id="256f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">TL;DR</p><p id="e186" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">In the humanitarian response world there can be tens of thousands of tabular (CSV and Excel) datasets, many of which contain critical information for helping save lives. Data can be provided by hundreds of different organizations with different naming conventions, languages and data standards, so having information (metadata) about what each column represents in tables is important for finding the right data and understanding how it fits together. Much of this metadata is set manually, which is time-consuming and error prone, so any automatic method can have a real effect towards helping people. In this article we revisit a previous analysis ‚Äú</em><a class="af no" href="https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d" rel="noopener"><em class="nn">Predicting Metadata of Humanitarian Datasets with GPT 3</em></a><em class="nn">‚Äù to see how advances in the last 18 months open the way for more efficient and less time-consuming methods for setting metadata on tabular data.</em></p><p id="72bd" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">Using metadata-tagged CSV and Excel datasets from the </em><a class="af no" href="https://data.humdata.org/" rel="noopener ugc nofollow" target="_blank"><em class="nn">Humanitarian Data Exchange</em></a><em class="nn"> (HDX) we show that fine-tuning GPT-4o-mini works well for predicting </em><a class="af no" href="https://hxlstandard.org/" rel="noopener ugc nofollow" target="_blank"><em class="nn">Humanitarian Exchange Language</em></a><em class="nn"> (HXL) tags and attributes for the most common tags related to location and dates. However, for less well-represented tags and attributes the technique can be a bit limited due to poor quality training data where humans have made mistakes in manually labelling data or simply aren‚Äôt using all possible HXL metadata combinations. It also has the limitation of not being able to adjust when the metadata standard changes, since the training data would not reflect those changes.</em></p><p id="8971" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">Given more powerful LLMs are now available, we tested a technique to directly prompt GPT-4o or GPT-4o-mini rather than fine-tuning, providing the full HXL core schema definition in the system prompt now that larger context windows are available. This approach was shown to be more accurate than fine-tuning when using GPT-4o, able to support rarer HXL tags and attributes and requiring no custom training data, making it easier to manage and deploy. It is however more expensive, but not if using GPT-4o-mini, albeit with a slight decrease in performance. Using this approach we provide a simple Python class in a </em><a class="af no" href="https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014" rel="noopener ugc nofollow" target="_blank"><em class="nn">GitHub Gist</em></a><em class="nn"> that can be used in data processing pipelines to automatically add HXL metadata tags and attributes to tabular datasets.</em></p><h1 id="8ed1" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Generative AI moves fast!</h1><p id="fa77" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">About 18 months ago I wrote a blog post <a class="af no" href="https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d" rel="noopener">Predicting Metadata of Humanitarian Datasets with GPT 3</a>.</p><p id="4510" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">That‚Äôs right, with GPT 3, not even 3.5! üôÇ</p><p id="9da7" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Even so, back then Large Language Model (LLM) fine-tuning produced great performance for predicting <a class="af no" href="https://hxlstandard.org/" rel="noopener ugc nofollow" target="_blank">Humanitarian Exchange Language</a> (HXL) metadata fields for tabular datasets on the amazing <a class="af no" href="https://data.humdata.org/" rel="noopener ugc nofollow" target="_blank">Humanitarian Data Exchange</a> (HDX). In that study, the training data represented the distribution of HXL data on HDX and so was comprised of the most common tags relating to location and dates. These are very important for linking different datasets together in location and time, a crucial factor in using data to optimize humanitarian response.</p><p id="06ea" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The LLM field has since advanced ‚Ä¶ a <em class="nn">LOT</em>.</p><p id="3d45" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">So in this article, we will revisit the technique, expand it to cover less frequent HXL tags and attributes and explore other options now available to us for situations where a complex, high-cardinality taxonomy needs to be applied to data. We will also explore the ability to predict less frequent HXL standard tags and attributes not currently represented in the human-labeled training data.</p><h1 id="0df3" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Setup</h1><p id="ade9" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">You can follow along with this analysis by opening these notebooks in <a class="af no" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">Google Colab</a> or running them locally:</p><ul class=""><li id="751c" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm os ot ou bk"><a class="af no" href="https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb" rel="noopener ugc nofollow" target="_blank">generate-test-train-data.ipynb</a> ‚Äî A notebook for creating test and training datasets</li><li id="4dd3" class="mp mq fq mr b ms ov mu mv mw ow my mz na ox nc nd ne oy ng nh ni oz nk nl nm os ot ou bk"><a class="af no" href="https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb" rel="noopener ugc nofollow" target="_blank">openai-hxl-prediction.ipynb</a> ‚Äî Notebook exploring fine-tuning and prompting for predicting HXL datasets</li></ul><p id="e25d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Please refer to the <a class="af no" href="https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md" rel="noopener ugc nofollow" target="_blank">README</a> in the repo for installation instructions.</p><h1 id="1242" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">HXL Data from the Humanitarian Data Exchange</h1><p id="90c5" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">For this study, and with help from the HDX team, we will use data extracted from the HDX platform using a crawler process they run to track the use of HXL metadata tags and attributes on the platform. You can find great HXL resources on <a class="af no" href="https://github.com/HXLStandard" rel="noopener ugc nofollow" target="_blank">GitHub</a>, but if you want to follow along with this analysis I have also saved the source data on Google Drive as the crawler will take days to process the hundreds of thousands of tabular datasets on HDX.</p><p id="964c" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The data looks like this, with one row per HXL-tagged table column ‚Ä¶</p><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx pa"><img src="../Images/1d1abdc669b37dc43cb697223ff9d3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avac1p7dkVMtrxhCytGaVA.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Example of data used in this study, with a row per tabular data column.</figcaption></figure><h1 id="a2d7" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">The core HXL Schema</h1><p id="3641" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">The <a class="af no" href="https://hxlstandard.org/standard/1-1final/postcards/" rel="noopener ugc nofollow" target="_blank">HXL postcard</a> is a really great overview of the most common HXL tags and attributes in the core schema. For our analysis, we will apply the full standard as found on <a class="af no" href="https://data.humdata.org/dataset/hxl-core-schemas" rel="noopener ugc nofollow" target="_blank">HDX</a> which provides a <a class="af no" href="https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">spreadsheet</a> of supported tags and attributes ‚Ä¶</p><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx pg"><img src="../Images/c72d8b2f6bce80bae6e654de56ba7af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3z0jg0aiLl7jn29_IFdZw.png"/></div></div><figcaption class="mk ml mm lw lx mn mo bf b bg z dx">Excerpt of the ‚ÄúCore HXL Schema‚Äù used for this study, as found on the <a class="af no" href="https://data.humdata.org/dataset/hxl-core-schemas" rel="noopener ugc nofollow" target="_blank">Humanitarian Data Exchange</a></figcaption></figure><h1 id="b6e3" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Data Processing</h1><p id="1366" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">The <a class="af no" href="https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb" rel="noopener ugc nofollow" target="_blank">generate-test-train-data.ipynb</a> notebook provides all the steps taken to create test and training datasets, but here are some key points to note:</p><p id="f3eb" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">1. Removal of automatic pipeline repeat HXL data</strong></p><p id="2463" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In this study, I removed duplicate data created by automated pipelines that upload data to HDX, by using an MDF hash of column names in each tabular dataset (CSV and Excel files). For example, a CSV file of population statistics created by an organization is often very similar for each country-specific CSV or Excel file, so we only take one example. This has a balancing effect on the data, providing more variation of HXL tags and attributes by removing very similar repeat data.</p><p id="3e5c" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">2. Constraining data to valid HXL</strong></p><p id="a418" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">About 50% of the HDX data with HXL tags uses a tag or attribute which are not specified in the <a class="af no" href="https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">HXL Core Schema</a>, so this data is removed from training and test sets.</p><p id="c299" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">3. Data enrichment</strong></p><p id="3b57" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As a (mostly!) human being, when deciding what HXL tags and attributes to use on a column, I take a peek at the data for that column and also the data as a whole in the table. For this analysis we do the same for the LLM fine-tuning and prompt data, adding in data excerpts for each column. A table description is also added using an LLM (GPT-3.5-Turbo) summary of the data to make them consistent, as summaries on HDX can vary in form, ranging from pages to a few words.</p><p id="6aff" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">4. Carefully splitting data to create train/test sets</strong></p><p id="82e6" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Many machine learning pipelines split data randomly to create training and test sets. However, for HDX data this would result in columns and files from the same organization being in train and test. I felt this was a bit too easy for testing predictions and so instead split the data by organizations to ensure organizations in the test set were not in the training data. Additionally, subsidiaries of the same parent organization ‚Äî eg ‚Äúocha-iraq‚Äù and ‚Äúocha-libya‚Äù ‚Äî were not allowed to be in both the training and test sets, again to make the predictions more realistic. My aim was to test prediction with organizations as if their data had never been seen before.</p><p id="8319" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">After all of the above and down-sampling to save costs, we are left with <strong class="mr fr">2,883</strong> rows in the training set and <strong class="mr fr">485</strong> rows in the test set.</p><h1 id="3ee0" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Creating JSONL fine-tuning prompt files</h1><p id="782d" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">In my original article I opted for using a completion model, but with the release of <a class="af no" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" rel="noopener ugc nofollow" target="_blank">GPT-4o-mini</a> I instead generated prompts appropriate for fine-tuning a <em class="nn">chat</em> model (see <a class="af no" href="https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned" rel="noopener ugc nofollow" target="_blank">here</a> for more information about the available models).</p><p id="bd56" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Each prompt has the form ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="2fc3" class="pl nq fq pi b bg pm pn l po pp">{<br/>  "messages": [<br/>    {<br/>      "role": "system", <br/>      "content": "&lt;SYSTEM PROMPT&gt;"<br/>    }, <br/>    {<br/>      "role": "user", <br/>      "content": "&lt;INPUT PROMPT&gt;"<br/>    }, <br/>    {<br/>      "role": "assistant", <br/>      "content": "&lt;EXPECTED OUTPUT&gt;"<br/>    }<br/>  ]<br/>}<br/></span></pre><p id="19bb" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Note: The above has been formatted for clarity, but JSONL will have everything in one line per record.</p><p id="ddd8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Using the data excerpts, LLM_generated table description, column name we collated, we can now generate prompts which look like this ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="29bb" class="pl nq fq pi b bg pm pn l po pp">{<br/>  "messages": [<br/>    {<br/>      "role": "system", <br/>      "content": "You are an assistant that replies with HXL tags and attributes"<br/>    }, <br/>    {<br/>      "role": "user", <br/>      "content": "What are the HXL tags and attributes for a column with these details? <br/>                    resource_name='admin1-summaries-earthquake.csv'; <br/>                    dataset_description='The dataset contains earthquake data for various <br/>                                         administrative regions in Afghanistan, <br/>                                         including country name, admin1 name, latitude, <br/>                                         longitude, aggregation type, indicator name, <br/>                                         and indicator value. The data includes maximum <br/>                                         earthquake values recorded in different regions, <br/>                                         with corresponding latitude and longitude coordinates. <br/>                                         The dataset provides insights into the seismic <br/>                                         activity in different administrative areas of <br/>                                         Afghanistan.'; <br/>                   column_name:'indicator'; <br/>                   examples: ['earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake']"<br/>      }, <br/>      {<br/>        "role": "assistant", <br/>        "content": "#indicator+name"<br/>      }<br/>  ]<br/>}</span></pre><h1 id="204e" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Fine-tuning GPT-4o-mini</h1><p id="6014" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">We now have test and training files in the right format for fine-tuning an OpenAI chat model, so let‚Äôs tune our model ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="b8a2" class="pl nq fq pi b bg pm pn l po pp">def fine_tune_model(train_file, model_name="gpt-4o-mini"):<br/>    """<br/>    Fine-tune an OpenAI model using training data.<br/><br/>    Args:<br/>        prompt_file (str): The file containing the prompts to use for fine-tuning.<br/>        model_name (str): The name of the model to fine-tune. Default is "davinci-002".<br/><br/>    Returns:<br/>        str: The ID of the fine-tuned model.<br/>    """<br/><br/>    # Upload file to OpenAI for fine-tuning<br/>    file = client.files.create(<br/>        file=open(train_file, "rb"),<br/>        purpose="fine-tune"<br/>    )<br/>    file_id = file.id<br/>    print(f"Uploaded training file with ID: {file_id}")<br/><br/>    # Start the fine-tuning job<br/>    ft = client.fine_tuning.jobs.create(<br/>        training_file=file_id,<br/>        model=model_name<br/>    )<br/>    ft_id = ft.id<br/>    print(f"Fine-tuning job started with ID: {ft_id}")<br/><br/>    # Monitor the status of the fine-tuning job<br/>    ft_result = client.fine_tuning.jobs.retrieve(ft_id)<br/>    while ft_result.status != 'succeeded':<br/>        print(f"Current status: {ft_result.status}")<br/>        time.sleep(120)  # Wait for 60 seconds before checking again<br/>        ft_result = client.fine_tuning.jobs.retrieve(ft_id)<br/>        if 'failed' in ft_result.status.lower():<br/>            sys.exit()<br/><br/>    print(f"Fine-tuning job {ft_id} succeeded!")<br/><br/>    # Retrieve the fine-tuned model<br/>    fine_tuned_model = ft_result.fine_tuned_model<br/>    print(f"Fine-tuned model: {fine_tuned_model}")<br/><br/>    return fine_tuned_model<br/><br/>model = fine_tune_model("hxl_chat_prompts_train.jsonl", model_name="gpt-4o-mini-2024-07-18")</span></pre><p id="5e0f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">In the above we are using the new <a class="af no" href="https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/" rel="noopener ugc nofollow" target="_blank">GPT-4-mini model</a>, which from OpenAI is currently free to fine-tune ‚Ä¶</p><blockquote class="pq pr ps"><p id="d448" class="mp mq nn mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">‚ÄúNow through September 23, GPT-4o mini is free to fine-tune up to a daily limit of 2M training tokens. Overages over 2M training tokens will be charged at $3.00/1M tokens. Starting September 24, fine-tuning training will cost $3.00/1M tokens. Check out the <a class="af no" href="http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D" rel="noopener ugc nofollow" target="_blank">fine-tuning docs</a> for more details on free access.‚Äù</p></blockquote><p id="c12f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Even at $3.00/1 Million tokens, the costs are quite low for this task, coming out at about $7 a fine-tuning run for just over 2 million tokens in the test file. Bearing in mind, fine-tuning should be a rare event for this particular task, once we have such a model it can be reused.</p><p id="342e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The fine-tuning produces the following output ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="db16" class="pl nq fq pi b bg pm pn l po pp">Uploaded training file with ID: file-XXXXXXXXXXXXXXX<br/>Fine-tuning job started with ID: ftjob-XXXXXXXXXXXXXXX<br/>Current status: validating_files<br/>Current status: validating_files<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Current status: running<br/>Fine-tuning job ftjob-XXXXXXXXXXXXXXX succeeded!<br/>Fine-tuned model: ft:gpt-4o-mini-2024-07-18::XXXXXXX</span></pre><p id="97fd" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It took about 45 minutes.</p><h1 id="df54" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Testing our fine-tuned model to predict HXL</h1><p id="f944" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">Now that we have a nice new shiny fine-tuned model for predicting HXL tags and attributes, we can use the test file to take it for a spin ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="c910" class="pl nq fq pi b bg pm pn l po pp">def make_chat_predictions(prompts, model, temperature=0.1, max_tokens=13):<br/>    """<br/>    Generate chat predictions based on given prompts using the OpenAI chat model.<br/><br/>    Args:<br/>        prompts (list): A list of prompts, where each prompt is a dictionary containing a list of messages.<br/>                        Each message in the list has a 'role' (either 'system', 'user', or 'assistant') and 'content'.<br/>        model (str): The name or ID of the OpenAI chat model to use for predictions.<br/>        temperature (float, optional): Controls the randomness of the predictions. Higher values (e.g., 0.5) make the<br/>                                       output more random, while lower values (e.g., 0.1) make it more deterministic.<br/>                                       Defaults to 0.1.<br/>        max_tokens (int, optional): The maximum number of tokens in the predicted response. Defaults to 13.<br/><br/>    Returns:<br/>        pandas.DataFrame: A DataFrame containing the results of the chat predictions. Each row in the DataFrame<br/>                          corresponds to a prompt and includes the prompt messages, the actual message, and the<br/>                          predicted message.<br/><br/>    """<br/>    results = []<br/>    for p in prompts:<br/>        actual = p["messages"][-1]["content"]<br/>        p["messages"] = p["messages"][0:2]<br/>        completion = client.chat.completions.create(<br/>            model=model,<br/>            messages=p["messages"],<br/>            temperature=temperature,<br/>            max_tokens=max_tokens<br/>        )<br/>        predicted = completion.choices[0].message.content<br/>        predicted = filter_for_schema(predicted)<br/><br/>        res = {<br/>            "prompt": p["messages"],<br/>            "actual": actual,<br/>            "predicted": predicted<br/>        }<br/><br/>        print(f"Predicted: {predicted}; Actual: {actual}")<br/><br/>        results.append(res)<br/><br/>    results = pd.DataFrame(results)<br/><br/>    return results<br/><br/>def filter_for_schema(text):<br/>    """<br/>    Filters the input text to extract approved HXL schema tokens.<br/><br/>    Args:<br/>        text (str): The input text to be filtered.<br/><br/>    Returns:<br/>        str: The filtered text containing only approved HXL schema tokens.<br/>    """<br/><br/>    if " " in text:<br/>        text = text.replace(" ","")<br/><br/>    tokens_raw = text.split("+")<br/>    tokens = [tokens_raw[0]]<br/>    for t in tokens_raw[1:]:<br/>        tokens.append(f"+{t}")<br/><br/>    filtered = []<br/>    for t in tokens:<br/>        if t in APPROVED_HXL_SCHEMA:<br/>            if t not in filtered:<br/>                filtered.append(t)<br/>    filtered = "".join(filtered)<br/><br/>    if len(filtered) &gt; 0 and filtered[0] != '#':<br/>        filtered = ""<br/><br/>    return filtered<br/><br/>def output_prediction_metrics(results, prediction_field="predicted", actual_field="actual"):<br/>    """<br/>    Prints out model performance report for HXL tag prediction. Metrics are for<br/>    just predicting tags, as well as predicting tags and attributes.<br/><br/>    Parameters<br/>    ----------<br/>    results : dataframe<br/>        Dataframe of results<br/>    prediction_field : str<br/>        Field name of element with prediction. Handy for comparing raw and post-processed predictions.<br/>    actual_field: str<br/>        Field name of the actual result for comparison with prediction<br/>    """<br/>    y_test = []<br/>    y_pred = []<br/>    y_justtag_test = []<br/>    y_justtag_pred = []<br/>    for index, r in results.iterrows():<br/>        if actual_field not in r and predicted_field not in r:<br/>            print("Provided results do not contain expected values.")<br/>            sys.exit()<br/>        y_pred.append(r[prediction_field])<br/>        y_test.append(r[actual_field])<br/>        actual_tag = r[actual_field].split("+")[0]<br/>        predicted_tag = r[prediction_field].split("+")[0]<br/>        y_justtag_test.append(actual_tag)<br/>        y_justtag_pred.append(predicted_tag)<br/><br/>    print(f"LLM results for {prediction_field}, {len(results)} predictions ...")<br/>    print("\nJust HXL tags ...\n")<br/>    print(f"Accuracy: {round(accuracy_score(y_justtag_test, y_justtag_pred),2)}")<br/>    print(<br/>        f"Precision: {round(precision_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/>    print(<br/>        f"Recall: {round(recall_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/>    print(<br/>        f"F1: {round(f1_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/><br/>    print(f"\nTags and attributes with {prediction_field} ...\n")<br/>    print(f"Accuracy: {round(accuracy_score(y_test, y_pred),2)}")<br/>    print(<br/>        f"Precision: {round(precision_score(y_test, y_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/>    print(<br/>        f"Recall: {round(recall_score(y_test, y_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/>    print(<br/>        f"F1: {round(f1_score(y_test, y_pred, average='weighted', zero_division=0),2)}"<br/>    )<br/><br/>    return<br/><br/><br/>with open(TEST_FILE) as f:<br/>    X_test = [json.loads(line) for line in f]<br/><br/>results = make_chat_predictions(X_test, model)<br/><br/>output_prediction_metrics(results)<br/><br/>print("Done")</span></pre><p id="7f97" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Noting in the above that all predictions are filtered for allowed tags and attributes as defined in the HXL standard.</p><p id="6ecc" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This gives the following results ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="ce74" class="pl nq fq pi b bg pm pn l po pp">LLM results for predicted, 458 predictions ...<br/><br/>Just HXL tags ...<br/><br/>Accuracy: 0.83<br/>Precision: 0.85<br/>Recall: 0.83<br/>F1: 0.82<br/><br/>Tags and attributes with predicted ...<br/><br/>Accuracy: 0.61<br/>Precision: 0.6<br/>Recall: 0.61<br/>F1: 0.57<br/></span></pre><p id="f588" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">‚ÄòJust HXL Tags‚Äô means predicting the first part of the HXL, for example if the full HXL is #affected+infected+f, the model correctly got the #affected part correct. ‚ÄòTags and attributes‚Äô means predicting the full HXL string, ie ‚Äò#affected+infected+f‚Äô, a much harder challenge due to all the combinations possible.</p><p id="d9ff" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The performance isn‚Äôt perfect, but not that bad, especially as we have balanced the dataset to reduce the number of location and date tags and attributes (ie made this study a bit more challenging). There are tens of thousands of humanitarian response tables without HDX, even the above performance would likely add value.</p><p id="b1e9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let‚Äôs look into cases where predictions didn‚Äôt agree with human-labeled data ‚Ä¶</p><h1 id="7117" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Reviewing the Human Labeled HXL Data</h1><p id="6abf" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">The predictions were saved to a spreadsheet, and I manually went through most of the predictions that didn‚Äôt agree with the labels. You can find this analysis <a class="af no" href="https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&amp;ouid=107814789436940136200&amp;rtpof=true&amp;sd=true" rel="noopener ugc nofollow" target="_blank">here</a> and summarized below ‚Ä¶</p><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx pt"><img src="../Images/dfbd71d99ab9103a6ad4e7e1673cdcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqdtgBmT3dYeJSvdcKCoBg.png"/></div></div></figure><p id="b298" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">What‚Äôs interesting is that in some cases the LLM is actually correct, for example in adding <em class="nn">additional</em> HXL attributes which the human labeled data doesn‚Äôt include. There are also cases where the human labeled HXL was perfectly reasonable, but the LLM predicted another tag or attribute that could also be interpreted as correct. For example a #region can also be an #admin1 in some countries, and whether something is an +id or +code is sometimes difficult to decide, both are appropriate.</p><p id="4497" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Using the above categories, I created a new test set where the expected HXL tags were corrected. On re-running the prediction we get improved results ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="5e11" class="pl nq fq pi b bg pm pn l po pp"><br/>Just HXL tags ...<br/><br/>Accuracy: 0.88<br/>Precision: 0.88<br/>Recall: 0.88<br/>F1: 0.88<br/><br/>Tags and attributes with predicted ...<br/><br/>Accuracy: 0.66<br/>Precision: 0.71<br/>Recall: 0.66<br/>F1: 0.66</span></pre><h1 id="04d2" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Predicting HXL without Fine-tuning, instead only prompting GPT-4o</h1><p id="1c2f" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">The above shows that the human-labeled data itself can be incorrect. The HXL standard is designed excellently, but can be a challenge to memorize for developers and data scientists when setting HXL tags and attributes on data. There are some <a class="af no" href="https://hxlstandard.org/tools/" rel="noopener ugc nofollow" target="_blank">amazing tools</a> already provided by the HXL team, but sometimes the HXL is still incorrect. This introduces a problem to the fine-tuning approach which relies on this human-labeled data for training, especially for less well represented tags and attributes that humans are not using very often. It also has the limitation of not being able to adjust when the metadata standard changes, since the training data would not reflect those changes.</p><p id="47e0" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Since the initial analysis 18 months ago various LLM providers have advanced their models significantly. OpenAI of course released <a class="af no" href="https://openai.com/index/hello-gpt-4o/" rel="noopener ugc nofollow" target="_blank">GPT-4o</a> as their flagship product, which importantly has a context window of 128k tokens and is another data point suggesting costs of foundational models are decreasing (see for example GPT-4-Turbo compared to GPT-4o <a class="af no" href="https://huggingface.co/spaces/philschmid/llm-pricing" rel="noopener ugc nofollow" target="_blank">here</a>). Given these factors, I wondered ‚Ä¶</p><p id="7c61" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr"><em class="nn">If models are becoming more powerful and less expensive to use, could we avoid fine-tuning altogether and use them to predict HXL tags and attributes by prompting alone?</em></strong></p><p id="04aa" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Not only could this mean less engineering work to clean data and fine-tune models, it may have a big advantage in being able to include HXL tags and attributes which are not included in the human-labeled training data but are part of the HXL standard. This is one potentially huge advantage of powerful LLMs, being able to classify with zero- and few-shot prompting.</p><h1 id="9b41" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Creating a prompt for predicting HXL</h1><p id="ebdd" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">Models like GPT-4o are trained on web data, so I thought I‚Äôd first do a test using one of our prompts to see if it already knew everything there was to know about HXL tags ‚Ä¶</p><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx pu"><img src="../Images/b39b540c5990d5719eae62d8834191e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNOxo7i_39NzWoDe4F3p_g.png"/></div></div></figure><p id="ba3f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">What we see is that it seems to know about HXL syntax, but the answer is incorrect (the correct answer is ‚Äò#affected+infected‚Äô), and it has chosen tags and attributes that are not in the HXL standard. It‚Äôs actually similar to what we see with human-tagged HXL.</p><p id="b7d9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">How about we provide the most important parts of the <a class="af no" href="https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&amp;gid=319251406#gid=319251406" rel="noopener ugc nofollow" target="_blank">HXL standard</a> in the system prompt?</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="f952" class="pl nq fq pi b bg pm pn l po pp">def generate_hxl_standard_prompt(local_data_file):<br/>  """<br/>  Generate a standard prompt for predicting Humanitarian Markup Language (HXL) tags and attributes.<br/><br/>  Args:<br/>    local_data_file (str): The path to the local data file containing core hashtags and attributes.<br/><br/>  Returns:<br/>    str: The generated HXL standard prompt.<br/><br/>  """<br/><br/>  core_hashtags = pd.read_excel(local_data_file, sheet_name='Core hashtags')<br/>  core_hashtags = core_hashtags.loc[core_hashtags["Release status"] == "Released"]<br/>  core_hashtags = core_hashtags[["Hashtag", "Hashtag long description", "Sample HXL"]]<br/><br/>  core_attributes = pd.read_excel(local_data_file, sheet_name='Core attributes')<br/>  core_attributes = core_attributes.loc[core_attributes["Status"] == "Released"]<br/>  core_attributes = core_attributes[["Attribute", "Attribute long description", "Suggested hashtags (selected)"]]<br/><br/>  print(core_hashtags.shape)<br/>  print(core_attributes.shape)<br/><br/>  core_hashtags = core_hashtags.to_dict(orient='records')<br/>  core_attributes = core_attributes.to_dict(orient='records')<br/><br/>  hxl_prompt= f"""<br/>  You are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:<br/><br/>  CORE HASHTAGS:<br/><br/>  {json.dumps(core_hashtags,indent=4)}<br/><br/>  CORE ATTRIBUTES:<br/><br/>  {json.dumps(core_attributes, indent=4)}<br/><br/>  Key points:<br/><br/>  - ALWAYS predict hash tags<br/>  - NEVER predict a tag which is not a valid core hashtag<br/>  - NEVER start with a core hashtag, you must always start with a core hashtag<br/>  - Always try and predict an attribute if possible<br/>  - Do not use attribute +code if the data examples are human readable names<br/><br/>  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.<br/><br/>  """<br/><br/>  print(len(hxl_prompt.split(" ")))<br/>  print(hxl_prompt)<br/>  return hxl_prompt</span></pre><p id="403c" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This gives us a prompt like this ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="5822" class="pl nq fq pi b bg pm pn l po pp">You are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:<br/><br/>  CORE HASHTAGS:<br/><br/>  [<br/>    {<br/>        "Hashtag": "#access",<br/>        "Hashtag long description": "Accessiblity and constraints on access to a market, distribution point, facility, etc.",<br/>        "Sample HXL": "#access +type"<br/>    },<br/>    {<br/>        "Hashtag": "#activity",<br/>        "Hashtag long description": "A programme, project, or other activity. This hashtag applies to all levels; use the attributes +activity, +project, or +programme to distinguish different hierarchical levels.",<br/>        "Sample HXL": "#activity +project"<br/>    },<br/>    {<br/>        "Hashtag": "#adm1",<br/>        "Hashtag long description": "Top-level subnational administrative area (e.g. a governorate in Syria).",<br/>        "Sample HXL": "#adm1 +code"<br/>    },<br/>    {<br/>        "Hashtag": "#adm2",<br/>        "Hashtag long description": "Second-level subnational administrative area (e.g. a subdivision in Bangladesh).",<br/>        "Sample HXL": "#adm2 +name"<br/>    },<br/>    {<br/>        "Hashtag": "#adm3",<br/>        "Hashtag long description": "Third-level subnational administrative area (e.g. a subdistrict in Afghanistan).",<br/>        "Sample HXL": "#adm3 +code"<br/>    },<br/>    {<br/>        "Hashtag": "#adm4",<br/>        "Hashtag long description": "Fourth-level subnational administrative area (e.g. a barangay in the Philippines).",<br/>        "Sample HXL": "#adm4 +name"<br/>    },<br/>    {<br/>        "Hashtag": "#adm5",<br/>        "Hashtag long description": "Fifth-level subnational administrative area (e.g. a ward of a city).",<br/>        "Sample HXL": "#adm5 +code"<br/>    },<br/>    {<br/>        "Hashtag": "#affected",<br/>        "Hashtag long description": "Number of people or households affected by an emergency. Subset of #population; superset of #inneed.",<br/>        "Sample HXL": "#affected +f +children"<br/>    },<br/>    {<br/>        "Hashtag": "#beneficiary",<br/>        "Hashtag long description": "General (non-numeric) information about a person or group meant to benefit from aid activities, e.g. \"lactating women\".",<br/>        "Sample HXL": "#beneficiary +name"<br/>    },<br/>    {<br/>        "Hashtag": "#capacity",<br/>        "Hashtag long description": "The response capacity of the entity being described (e.g. \"25 beds\").",<br/>        "Sample HXL": "#capacity +num"<br/>    },<br/>   <br/>... Truncated for brevity<br/><br/>    },<br/>    {<br/>        "Hashtag": "#targeted",<br/>        "Hashtag long description": "Number of people or households targeted for humanitarian assistance. Subset of #inneed; superset of #reached.",<br/>        "Sample HXL": "#targeted +f +adult"<br/>    },<br/>    {<br/>        "Hashtag": "#value",<br/>        "Hashtag long description": "A monetary value, such as the price of goods in a market, a project budget, or the amount of cash transferred to beneficiaries. May be used together with #currency in financial or cash data.",<br/>        "Sample HXL": "#value +transfer"<br/>    }<br/>]<br/><br/>  CORE ATTRIBUTES:<br/><br/>  [<br/>    {<br/>        "Attribute": "+abducted",<br/>        "Attribute long description": "Hashtag refers to people who have been abducted.",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached"<br/>    },<br/>    {<br/>        "Attribute": "+activity",<br/>        "Attribute long description": "The implementers classify this activity as an \"activity\" proper  (may imply different hierarchical levels in different contexts).",<br/>        "Suggested hashtags (selected)": "#activity"<br/>    },<br/>    {<br/>        "Attribute": "+adolescents",<br/>        "Attribute long description": "Adolescents, loosely defined (precise age range varies); may overlap +children and +adult.  You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+adolescents +age12_17\".",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"<br/>    },<br/>    {<br/>        "Attribute": "+adults",<br/>        "Attribute long description": "Adults, loosely defined (precise age range varies); may overlap +adolescents and +elderly. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+adults +age18_64\".",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"<br/>    },<br/>    {<br/>        "Attribute": "+approved",<br/>        "Attribute long description": "Date or time when something was approved.",<br/>        "Suggested hashtags (selected)": "#date"<br/>    },<br/>    {<br/>        "Attribute": "+bounds",<br/>        "Attribute long description": "Boundary data (e.g. inline GeoJSON).",<br/>        "Suggested hashtags (selected)": "#geo"<br/>    },<br/>    {<br/>        "Attribute": "+budget",<br/>        "Attribute long description": "Used with #value to indicate that the amount is planned/approved/budgeted rather than actually spent.",<br/>        "Suggested hashtags (selected)": "#value"<br/>    },<br/>    {<br/>        "Attribute": "+canceled",<br/>        "Attribute long description": "Date or time when something (e.g. an #activity) was canceled.",<br/>        "Suggested hashtags (selected)": "#date"<br/>    },<br/>    {<br/>        "Attribute": "+children",<br/>        "Attribute long description": "The associated hashtag applies to non-adults, loosely defined (precise age range varies; may overlap +infants and +adolescents). You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+children +age3_11\".",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"<br/>    },<br/>    {<br/>        "Attribute": "+cluster",<br/>        "Attribute long description": "Identifies a sector as a formal IASC humanitarian cluster.",<br/>        "Suggested hashtags (selected)": "#sector"<br/>    },<br/>    {<br/>        "Attribute": "+code",<br/>        "Attribute long description": "A unique, machine-readable code.",<br/>        "Suggested hashtags (selected)": "#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc, #beneficiary, #activity, #org, #sector, #subsector, #indicator, #output, #crisis, #cause, #impact, #severity, #service, #need, #currency, #item, #need, #service, #channel, #modality, #event, #group, #status"<br/>    },<br/>    {<br/>        "Attribute": "+converted",<br/>        "Attribute long description": "Date or time used for converting a monetary value to another currency.",<br/>        "Suggested hashtags (selected)": "#date"<br/>    },<br/>    {<br/>        "Attribute": "+coord",<br/>        "Attribute long description": "Geodetic coordinates (lat+lon together).",<br/>        "Suggested hashtags (selected)": "#geo"<br/>    },<br/>    {<br/>        "Attribute": "+dest",<br/>        "Attribute long description": "Place of destination (intended or actual).",<br/>        "Suggested hashtags (selected)": "#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc"<br/>    },<br/>    {<br/>        "Attribute": "+displaced",<br/>        "Attribute long description": "Displaced people or households. Refers to all types of displacement: use +idps or +refugees to be more specific.",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"<br/>    },<br/>    {<br/>        "Attribute": "+elderly",<br/>        "Attribute long description": "Elderly people, loosely defined (precise age range varies). May overlap +adults. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+elderly +age65plus\".",<br/>        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"<br/>    },<br/><br/><br/>... Truncated for brevity<br/><br/>    {<br/>        "Attribute": "+url",<br/>        "Attribute long description": "The data consists of web links related to the main hashtag (e.g. for an #org, #service, #activity, #loc, etc).",<br/>        "Suggested hashtags (selected)": "#contact, #org, #activity, #service, #meta"<br/>    },<br/>    {<br/>        "Attribute": "+used",<br/>        "Attribute long description": "Refers to a #service, #item, etc. that affected people have actually consumed or otherwise taken advantage of.",<br/>        "Suggested hashtags (selected)": "#service, #item"<br/>    }<br/>]<br/><br/>  Key points:<br/><br/>  - ALWAYS predict hash tags<br/>  - NEVER predict a tag which is not a valid core hashtag<br/>  - NEVER start with a core hashtag, you must always start with a core hashtag<br/>  - Always try and predict an attribute if possible<br/><br/>  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.</span></pre><p id="e55a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It‚Äôs pretty long (the above has been truncated), but encapsulates the HXL standard.</p><p id="6027" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Another advantage of the direct prompt method is that we can also ask for the LLM to provide its reasoning when predicting HXL. This can of course include hallucination, but I‚Äôve always found it useful for refining prompts.</p><p id="452a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">For the user prompt, we will use the same information that we used for fine-tuning, to include excerpt and LLM-generated table summary ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="e0d6" class="pl nq fq pi b bg pm pn l po pp">What are the HXL tags and attributes for a column with these details? resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC Appeals Data for South Sudan8.csv'; <br/>   dataset_description='The dataset contains information on various <br/>                        appeals and events related to South Sudan, <br/>                        including details such as the type of appeal, <br/>                        status, sector, amount requested and funded, <br/>                        start and end dates, as well as country-specific <br/>                        information like country code, region, and average <br/>                        household size. The data includes appeals for <br/>                        different crises such as floods, population <br/>                        movements, cholera outbreaks, and Ebola preparedness, <br/>                        with details on beneficiaries and confirmation needs. <br/>                        The dataset also includes metadata such as IDs, <br/>                        names, and translation modules for countries and regions.'; <br/>   column_name:'aid'; <br/>   examples: ['18401', '17770', '17721', '16858', '15268', '15113', '14826', '14230', '12788', '9286', '8561']</span></pre><p id="2036" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Putting it all together, and prompting both GPT-4o-mini and GPT-4o for comparison ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="766d" class="pl nq fq pi b bg pm pn l po pp">def call_gpt(prompt, system_prompt, model, temperature, top_p, max_tokens):<br/>    """<br/>    Calls the GPT model to generate a response based on the given prompt and system prompt.<br/><br/>    Args:<br/>        prompt (str): The user's input prompt.<br/>        system_prompt (str): The system's input prompt.<br/>        model (str): The name or ID of the GPT model to use.<br/>        temperature (float): Controls the randomness of the generated output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more deterministic.<br/>        top_p (float): Controls the diversity of the generated output. Higher values (e.g., 0.8) make the output more diverse, while lower values (e.g., 0.2) make it more focused.<br/>        max_tokens (int): The maximum number of tokens to generate in the response.<br/><br/>    Returns:<br/>        dict or None: The generated response as a dictionary object, or None if an error occurred during generation.<br/>    """<br/>    response = client.chat.completions.create(<br/>        model=model,<br/>        messages= [<br/>            {"role": "system", "content": system_prompt},<br/>            {"role": "user", "content": prompt}<br/>        ],<br/>        max_tokens=2000,<br/>        temperature=temperature,<br/>        top_p=top_p,<br/>        frequency_penalty=0,<br/>        presence_penalty=0,<br/>        stop=None,<br/>        stream=False,<br/>        response_format={ "type": "json_object" }<br/>    )<br/><br/>    result = response.choices[0].message.content<br/>    result = result.replace("```json","").replace("```","")<br/>    try:<br/>        result = json.loads(result)<br/>        result["predicted"] = result["predicted"].replace(" ","")<br/>    except:<br/>        print(result)<br/>        result = None<br/>    return result<br/><br/>def make_prompt_predictions(prompts, model, temperature=0.1, top_p=0.1, \<br/>                            max_tokens=2000, debug=False, actual_field="actual"):<br/>    """<br/>    Generate predictions for a given set of prompts using the specified model.<br/><br/>    Args:<br/>        prompts (pandas.DataFrame): A DataFrame containing the prompts to generate predictions for.<br/>        model (str): The name of the model to use for prediction.<br/>        temperature (float, optional): The temperature parameter for the model's sampling. Defaults to 0.1.<br/>        top_p (float, optional): The top-p parameter for the model's sampling. Defaults to 0.1.<br/>        max_tokens (int, optional): The maximum number of tokens to generate for each prompt. Defaults to 2000.<br/>        debug (bool, optional): Whether to print debug information during prediction. Defaults to False.<br/>        actual_field (str, optional): The name of the column in the prompts DataFrame that contains the actual values. Defaults to "actual".<br/><br/>    Returns:<br/>        pandas.DataFrame: A DataFrame containing the results of the predictions, including the prompt, actual value, predicted value, and reasoning.<br/><br/>    """<br/><br/>    num_prompts = len(prompts)<br/>    print(f"Number of prompts: {num_prompts}")<br/><br/>    results = []<br/>    for index, p in prompts.iterrows():<br/><br/>        if index % 50 == 0:<br/>            print(f"{index/num_prompts*100:.2f}% complete")<br/><br/>        prompt = p["prompt"]<br/>        prompt = ast.literal_eval(prompt)<br/>        prompt = prompt[1]["content"]<br/>        actual = p[actual_field]<br/><br/>        result = call_gpt(prompt, hxl_prompt, model, temperature, top_p, max_tokens)<br/><br/>        if result is None:<br/>            print("    !!!!! No LLM result")<br/>            predicted = ""<br/>            reasoning = ""<br/>        else:<br/>            predicted = result["predicted"]<br/>            reasoning = result["reasoning"]<br/><br/>        if debug is True:<br/>            print(f"Actual: {actual}; Predicted: {predicted}; Reasoning: {reasoning}")<br/><br/>        results.append({<br/>            "prompt": prompt,<br/>            "actual": actual,<br/>            "predicted": predicted,<br/>            "reasoning": reasoning<br/>        })<br/><br/>    results = pd.DataFrame(results)<br/><br/>    print(f"\n\n===================== {model} Results =========================\n\n")<br/>    output_prediction_metrics(results)<br/>    print(f"\n\n=================================================================")<br/><br/>    results["match"] = results['predicted'] == results['actual']<br/>    results.to_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-{model}-results.xlsx", index=False)<br/><br/>    return results<br/><br/>for model in ["gpt-4o-mini","gpt-4o"]:<br/>  print(f"Model: {model}")<br/>  results = make_prompt_predictions(X_test, model, temperature=0.1, top_p=0.1, max_tokens=2000)</span></pre><p id="1a3d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">We get ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="b077" class="pl nq fq pi b bg pm pn l po pp">===================== gpt-4o-mini Results =========================<br/><br/>LLM results for predicted, 458 predictions ...<br/><br/>Just HXL tags ...<br/><br/>Accuracy: 0.77<br/>Precision: 0.83<br/>Recall: 0.77<br/>F1: 0.77<br/><br/>Tags and attributes with predicted ...<br/><br/>Accuracy: 0.53<br/>Precision: 0.54<br/>Recall: 0.53<br/>F1: 0.5<br/><br/>===================== gpt-4o Results =========================<br/><br/><br/>LLM results for predicted, 458 predictions ...<br/><br/>Just HXL tags ...<br/><br/>Accuracy: 0.86<br/>Precision: 0.86<br/>Recall: 0.86<br/>F1: 0.85<br/><br/>Tags and attributes with predicted ...<br/><br/>Accuracy: 0.71<br/>Precision: 0.7<br/>Recall: 0.71<br/>F1: 0.69<br/><br/><br/>=================================================================</span></pre><p id="1de9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As a reminder, the fine-tuned model produced the following results ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="4ac8" class="pl nq fq pi b bg pm pn l po pp">Just HXL tags ...<br/><br/>Accuracy: 0.83<br/>Precision: 0.85<br/>Recall: 0.83<br/>F1: 0.82<br/><br/>Tags and attributes with predicted ...<br/><br/>Accuracy: 0.61<br/>Precision: 0.6<br/>Recall: 0.61<br/>F1: 0.57</span></pre><p id="7a2a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">How does prompting-only GPT-4o compare with GPT-4o-mini?</em></p><p id="9783" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Looking at the above, we see that GPT-4o-mini prompting-only predicts just tags with 77% accuracy, which is less than GPT-4o-mini fine-tuning (83%) and GPT-4o prompting-only (86%). That said the performance is still good and would improve HXL coverage even if used as-is.</p><p id="3a5b" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">How does prompting-only compare with the fine-tuned model?</em></p><p id="54ce" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">GPT-4o prompting-only gave the best results of all models, with 86% accuracy on tags and 71% on tags and attributes. In fact, the performance could well be better after a bit more analysis of the test data to correct incorrect human-labeled tags,.</p><p id="1eec" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let‚Äôs take a closer look at the times GPT-4o got it wrong ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="3f8d" class="pl nq fq pi b bg pm pn l po pp">df = pd.read_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-gpt-4o-results.xlsx")<br/><br/>breaks = df[df["match"]==False]<br/>print(breaks.shape)<br/><br/>for index, row in breaks.iterrows():<br/>  print("\n======================================== ")<br/>  pprint.pp(f"\nPrompt: {row['prompt']}")<br/>  print()<br/>  print(f"Actual", row["actual"])<br/>  print(f"Predicted", row["predicted"])<br/>  print()<br/>  pprint.pp(f'Reasoning: \n{row["reasoning"]}')</span></pre><pre class="pv ph pi pj bp pk bb bk"><span id="d9d9" class="pl nq fq pi b bg pm pn l po pp">'\n'<br/> 'Prompt: What are the HXL tags and attributes for a column with these '<br/> 'details? '<br/> "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "<br/> "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "<br/> 'contains information on various appeals and events related to South Sudan, '<br/> 'including details such as the type of appeal, status, sector, amount '<br/> 'requested and funded, start and end dates, as well as country-specific '<br/> 'information like country code, region, and average household size. The data '<br/> 'includes appeals for different crises such as floods, population movements, '<br/> 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '<br/> 'confirmation needs. The dataset also includes metadata such as IDs, names, '<br/> "and translation modules for countries and regions.'; column_name:'dtype.id'; "<br/> "examples: ['12', '5', '1', '1', '12', '12', '1', '6', '1', '1', '7']")<br/><br/>Actual #cause+id<br/>Predicted #meta+id<br/><br/>('Reasoning: \n'<br/> "The column 'dtype.id' contains numeric identifiers (e.g., '12', '5', '1') "<br/> 'which are likely to be internal identifiers for data records. According to '<br/> 'the HXL standard, the appropriate hashtag for internal identifiers is '<br/> "'#meta' with the attribute '+id'.")<br/><br/>======================================== <br/>('\n'<br/> 'Prompt: What are the HXL tags and attributes for a column with these '<br/> 'details? '<br/> "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "<br/> "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "<br/> 'contains information on various appeals and events related to South Sudan, '<br/> 'including details such as the type of appeal, status, sector, amount '<br/> 'requested and funded, start and end dates, as well as country-specific '<br/> 'information like country code, region, and average household size. The data '<br/> 'includes appeals for different crises such as floods, population movements, '<br/> 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '<br/> 'confirmation needs. The dataset also includes metadata such as IDs, names, '<br/> "and translation modules for countries and regions.'; "<br/> "column_name:'dtype.name'; examples: ['Flood', 'Population Movement', "<br/> "'Epidemic', 'Epidemic', 'Flood', 'Flood', 'Epidemic', 'Complex Emergency', "<br/> "'Epidemic', 'Epidemic', 'Civil Unrest']")<br/><br/>Actual #cause+name<br/>Predicted #event+type<br/><br/>('Reasoning: \n'<br/> "The examples provided in the column ('Flood', 'Population Movement', "<br/> "'Epidemic', 'Complex Emergency', 'Civil Unrest') describe different types of "<br/> 'events or incidents within a crisis or emergency. According to the HXL '<br/> 'standard, the appropriate hashtag for this type of data is #event, and the '<br/> 'attribute +type is used to specify the type or category of the event.')<br/><br/>======================================== <br/>('\n'<br/> 'Prompt: What are the HXL tags and attributes for a column with these '<br/> 'details? '<br/> "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "<br/> "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "<br/> 'contains information on various appeals and events related to South Sudan, '<br/> 'including details such as the type of appeal, status, sector, amount '<br/> 'requested and funded, start and end dates, as well as country-specific '<br/> 'information like country code, region, and average household size. The data '<br/> 'includes appeals for different crises such as floods, population movements, '<br/> 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '<br/> 'confirmation needs. The dataset also includes metadata such as IDs, names, '<br/> "and translation modules for countries and regions.'; "<br/> "column_name:'status_display'; examples: ['Active', 'Active', 'Closed', "<br/> "'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', "<br/> "'Closed']")<br/><br/>Actual #status+name<br/>Predicted #status+code<br/><br/>('Reasoning: \n'<br/> "The column 'status_display' contains values such as 'Active' and 'Closed', "<br/> 'which describe the status of appeals or events. The appropriate HXL hashtag '<br/> 'for project or activity status is #status. Since the values are categorical '<br/> 'and represent different statuses, the attribute +code is suitable to '<br/> 'indicate these status codes.')<br/><br/>======================================== <br/>('\n'<br/> 'Prompt: What are the HXL tags and attributes for a column with these '<br/> 'details? '<br/> "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "<br/> "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "<br/> 'contains information on various appeals and events related to South Sudan, '<br/> 'including details such as the type of appeal, status, sector, amount '<br/> 'requested and funded, start and end dates, as well as country-specific '<br/> 'information like country code, region, and average household size. The data '<br/> 'includes appeals for different crises such as floods, population movements, '<br/> 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '<br/> 'confirmation needs. The dataset also includes metadata such as IDs, names, '<br/> "and translation modules for countries and regions.'; "<br/> "column_name:'region.id'; examples: ['0', '0', '0', '0', '0', '0', '0', '0', "<br/> "'0', '0', '0']")<br/><br/>Actual #adm1+code<br/>Predicted #region+id<br/><br/>('Reasoning: \n'<br/> "The column 'region.id' contains numeric identifiers for regions, which "<br/> 'aligns with the HXL tag #region and the attribute +id. The examples provided '<br/> 'are all numeric, indicating that these are likely unique identifiers for '<br/> 'regions.')<br/><br/>======================================== </span></pre><p id="b285" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Notice how we now have a ‚ÄòReasoning‚Äô field to indicate why the tags were chosen. This is useful and would be an important part for refining the prompt to improve performance.</p><p id="d17d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Looking at the sample above, we see some familiar scenarios that were found when analyzing the fine-tuned model failed predictions ‚Ä¶</p><ul class=""><li id="f928" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm os ot ou bk">+id and +code ambiguity</li><li id="d094" class="mp mq fq mr b ms ov mu mv mw ow my mz na ox nc nd ne oy ng nh ni oz nk nl nm os ot ou bk">#region and #adm1 used interchangeably</li><li id="1676" class="mp mq fq mr b ms ov mu mv mw ow my mz na ox nc nd ne oy ng nh ni oz nk nl nm os ot ou bk">#event versus more detailed tags like #cause</li></ul><p id="b918" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">These seem to fall into the category where two tags are possible for a given column given their HXL definition. But there are some real discrepancies which would need more investigation.</p><p id="6546" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">That said, using GPT-4o to predict HXL tags and attributes yields the best results, and I believe at an acceptable level given a lot of data is missing HXL metadata altogether and many of the datasets which have it have incorrect tags and attributes.</p><h1 id="4c9b" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Cost Comparison</h1><p id="8f5d" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">Let‚Äôs see how costs compare with each technique and model ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="69cb" class="pl nq fq pi b bg pm pn l po pp"> def num_tokens_from_string(string: str, encoding_name: str) -&gt; int:<br/>    """<br/>    Returns the number of tokens in a text string using toktoken.<br/>    See: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb<br/><br/>    Args:<br/>        string (str): The text string to count the tokens for.<br/>        encoding_name (str): The name of the encoding to use.<br/><br/>    Returns:<br/>        num_tokens: The number of tokens in the text string.<br/>    <br/>    """<br/>    encoding = tiktoken.get_encoding(encoding_name)<br/>    num_tokens = len(encoding.encode(string))<br/>    return num_tokens<br/><br/>def calc_costs(data, model, method="prompting"):<br/>  """<br/>  Calculate token costs for a given dataset, method and model.<br/>  Note: Only for inference costs, not fine-tuning<br/><br/>  Args:<br/>    data (pandas.DataFrame): The data to get the tokens for.<br/>    method (str, optional): The method to use. Defaults to "prompting".<br/>    model (str): The model to use, eg "gpt-4o-mini"<br/><br/>  Returns:<br/>    input_tokens: The number of input tokens.<br/>    output_tokens: The number of output tokens.<br/><br/>  """<br/>  # See https://openai.com/api/pricing/<br/>  price = {<br/>      "gpt-4o-mini": {<br/>          "input": 0.150,<br/>          "output": 0.600<br/>      },<br/>      "gpt-4o": {<br/>          "input": 5.00,<br/>          "output": 15.00<br/>      }<br/>  }<br/>  input_tokens = 0<br/>  output_tokens = 0<br/>  for index, p in data.iterrows():<br/>      prompt = p["prompt"]<br/>      prompt = ast.literal_eval(prompt)<br/>      input = prompt[1]["content"] <br/>      # If prompting, we must include system prompt<br/>      if method == "prompting":<br/>        input += " " + hxl_prompt<br/>      output = p["Corrected actual"]<br/>      input_tokens += num_tokens_from_string(str(input), "cl100k_base")<br/>      output_tokens += num_tokens_from_string(str(output), "cl100k_base") <br/><br/>  input_cost = input_tokens / 1000000 * price[model]["input"]<br/>  output_cost = output_tokens / 1000000 * price[model]["output"]<br/><br/>  print(f"\nFor {data.shape[0]} table columns where we predicted HXL tags ...")<br/>  print(f"{method} prediction with model {model}, {input_tokens} input tokens = ${input_cost}")<br/>  print(f"Fine-tuning prediction GPT-4o-mini {output_tokens} output tokens = ${output_cost}\n")<br/><br/>hxl_prompt = generate_hxl_standard_prompt(HXL_SCHEMA_LOCAL_FILE, debug=False)<br/>X_test2 = pd.read_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-fine-tune-prediction-results-review.xlsx", sheet_name=0)<br/><br/>calc_costs(X_test2, method="fine-tuning", model="gpt-4o-mini")<br/>calc_costs(X_test2, method="prompting", model="gpt-4o-mini")<br/>calc_costs(X_test2, method="prompting", model="gpt-4o")</span></pre><p id="54de" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Which gives ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="4f7f" class="pl nq fq pi b bg pm pn l po pp">For 458 table columns where we predicted HXL tags ...<br/>fine-tuning prediction with model gpt-4o-mini, 99738 input tokens = $0.014960699999999999<br/>Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006<br/><br/><br/>For 458 table columns where we predicted HXL tags ...<br/>prompting prediction with model gpt-4o-mini, 2688812 input tokens = $0.4033218<br/>Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006<br/><br/><br/>For 458 table columns where we predicted HXL tags ...<br/>prompting prediction with model gpt-4o, 2688812 input tokens = $13.44406<br/>Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.030015000000000003</span></pre><p id="76e9" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Note: the above is only for the inference cost, there will be a very small additional cost in generating table data summaries with GPT-3.5.</p><p id="609c" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Given the test set, predicting HXL for <strong class="mr fr">458 columns</strong> ‚Ä¶</p><p id="a5b0" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Fine-tuning</strong>:</p><p id="c86b" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">As expected, inference costs for the fine-tuned GPT-4o mini model (which cost about $7 to fine-tune) are very low about $0.02.</p><p id="181e" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Prediction-only</strong>:</p><ul class=""><li id="9983" class="mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm os ot ou bk">GPT-4o prediction only is expensive, because of the HXL standard being passed in to the system prompt every time, and comes out at $13.44.</li><li id="d75c" class="mp mq fq mr b ms ov mu mv mw ow my mz na ox nc nd ne oy ng nh ni oz nk nl nm os ot ou bk">GPT-4o-mini, albeit with reduced performance, is a more reasonable $0.40.</li></ul><p id="5241" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">So ease of use comes with a cost if using GPT-4o, but GPT-4o-mini is an attractive alternative.</p><p id="4791" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Finally, it‚Äôs worth noting that in many cases, setting HXL tags might not to be real time, for example for a crawler process that corrects already uploaded datasets. This would mean that the new <a class="af no" href="https://platform.openai.com/docs/guides/batch/overview" rel="noopener ugc nofollow" target="_blank">OpenAI batch API</a> could be used, reducing costs by 50%.</p><h1 id="7db3" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">A Python class for predicting HXL Tags</h1><p id="4b88" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">Putting this all together, I created a Github gist <a class="af no" href="https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014" rel="noopener ugc nofollow" target="_blank">hxl_utils.py</a>. Check this out from GitHub and place the file in your current working directory.</p><p id="250f" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let‚Äôs download a file to test it with ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="e44a" class="pl nq fq pi b bg pm pn l po pp"># See HDX for this file: https://data.humdata.org/dataset/sudan-acled-conflict-data<br/>DATAFILE_URL="https://data.humdata.org/dataset/5efad450-8b15-4867-b7b3-8a25b455eed8/resource/3352a0d8-2996-4e70-b618-3be58699be7f/download/sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx"<br/>local_data_file = f"{LOCAL_DATA_DIR}/{DATAFILE_URL.split('/')[-1]}"<br/><br/># Save data file locally <br/>urllib.request.urlretrieve(DATAFILE_URL, local_data_file)<br/><br/># Read it to get a dataframe<br/>df = pd.read_excel(local_data_file, sheet_name=1)</span></pre><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx pw"><img src="../Images/f4d88e404896f574504c6d75b455d9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xtMtLkva8r0BBAYHiRXZ0w.png"/></div></div></figure><p id="7875" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">And using this dataframe, let‚Äôs predict HXL tags ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="0858" class="pl nq fq pi b bg pm pn l po pp">from hxl_utils import HXLUtils<br/><br/>hxl_utils = HXLUtils(LOCAL_DATA_DIR, model="gpt-4o")<br/>data = hxl_utils.add_hxl(df,"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx")<br/><br/>print("\n\nAFTER: \n\n")<br/>display(data)</span></pre><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx px"><img src="../Images/33bafdfd979641e3cc932f24d4992577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtRoYcz7CMzrvf2osiVGlw.png"/></div></div></figure><p id="effa" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">And there we have it, some lovely HXL tags!</p><p id="5682" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Let‚Äôs see how well GPT-4o-mini does ‚Ä¶</p><pre class="pb pc pd pe pf ph pi pj bp pk bb bk"><span id="b71b" class="pl nq fq pi b bg pm pn l po pp">hxl_utils = HXLUtils(LOCAL_DATA_DIR, model="gpt-4o-mini")<br/>data = hxl_utils.add_hxl(df,"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx")</span></pre><p id="39cf" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Which gives ‚Ä¶</p><figure class="pb pc pd pe pf me lw lx paragraph-image"><div role="button" tabindex="0" class="mf mg ed mh bh mi"><div class="lw lx py"><img src="../Images/551db548c232ea8e6d577b657dee9021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pxfv6M_6rfuaNB10jUGxQw.png"/></div></div></figure><p id="60d7" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Pretty good! gpt-4o gave ‚Äú#affected+killed+num‚Äù for the last column, where ‚Äúgpt-4o-mini‚Äù gave ‚Äú#affected+num‚Äù, but this could likely be resolved with some deft prompt engineering.</p><p id="c3a8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Admittedly this wasn‚Äôt a terribly challenging dataset, but it was able to correctly predict tags for events and fatalities, which are less frequent than location and dates.</p><h1 id="450d" class="np nq fq bf nr ns nt nu nv nw nx ny nz oa ob oc od oe of og oh oi oj ok ol om bk">Future Work</h1><p id="b6d3" class="pw-post-body-paragraph mp mq fq mr b ms on mu mv mw oo my mz na op nc nd ne oq ng nh ni or nk nl nm fj bk">I think a big takeaway here is that the direct-prompting technique produces good results without the need for training. Yes, more expensive for inference, but maybe not if a data scientist is required to curate incorrectly human-labeled fine-tuning data. It would depend on the organization and metadata use-case.</p><p id="1665" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">Here are some areas that might be considered in future work ‚Ä¶</p><p id="3da8" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Improved test data</strong></p><p id="7c97" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">This analysis did a quick review of the test set to correct HXL tags which were incorrect in the data or had multiple possible values. More time could be spent on this, as always in machine learning, ground truth is key.</p><p id="f9b7" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Prompt engineering and hyperparameter tuning</strong></p><p id="1d8a" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The above analysis uses very basic prompts with no real engineering or strategies applied, these could definitely be improved for better performance. With an evaluation set and a framework such as <a class="af no" href="https://github.com/microsoft/promptflow" rel="noopener ugc nofollow" target="_blank">Promptflow</a>, prompt variants could be tested. Additionally we might add more context data, for example in deciding administrative levels, which can vary per country. Finally, we have used fixed hyperparameters for temperature and top_p, as well as completion token length. All these could be tuned leading to better performance.</p><p id="5914" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Cost optimization</strong></p><p id="f42d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">The prompting-only approach definitely appears to be a strong option and simplifies how an organization can automatically set HXL tags on their data using GPT-4o. There are of course cost implications with this model, being a more expensive, but predictions occur only on low-volume schema changes, not when the underlying data itself changes, and with new options for <a class="af no" href="https://openai.com/api/pricing/" rel="noopener ugc nofollow" target="_blank">batch submission</a> on OpenAI and ever decreasing LLM costs, this technique appears viable for many organizations. GPT-4o-mini also performs well and is a fraction of the cost.</p><p id="775d" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><strong class="mr fr">Application to other metadata standards</strong></p><p id="8a69" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk">It would be interesting to apply this technique to other metadata and labeling standards, I‚Äôm sure many organizations are already using LLMs for this.</p><p id="faa4" class="pw-post-body-paragraph mp mq fq mr b ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm fj bk"><em class="nn">Please like this article if inclined and I‚Äôd be delighted if you followed me! You can find more articles </em><a class="af no" href="https://medium.com/@astrobagel" rel="noopener"><em class="nn">here</em></a><em class="nn">.</em></p></div></div></div></div>    
</body>
</html>