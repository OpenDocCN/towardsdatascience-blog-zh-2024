- en: How to Improve Model Quality Without Building Larger Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08](https://towardsdatascience.com/how-to-improve-model-quality-without-building-larger-models-d6c8e76a86fe?source=collection_archive---------5-----------------------#2024-10-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Going into the Google DeepMind’s “Scaling LLM Test-Time Compute Optimally can
    be More Effective than Scaling Model Parameters”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Matthew
    Gunton](../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png)](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    [Matthew Gunton](https://medium.com/@mgunton7?source=post_page---byline--d6c8e76a86fe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d6c8e76a86fe--------------------------------)
    ·11 min read·Oct 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e481b2de6c28675a6758b1d595d75bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Flux.1 12B
  prefs: []
  type: TYPE_NORMAL
- en: Recently OpenAI unveiled their newest model o1\. Rather than highlight the parameter
    size of this model, OpenAI instead showcased that the model performs significantly
    better because it takes more time. When you ask the model a question, it will
    often taken multiple seconds to respond — a far cry from the millisecond speed
    most people now expect with Large Language Models (LLMs). Nevertheless, this extra
    time appears to pay off as o1 scores substantially higher than other models on
    the LMSYS Chatbot Arena.
  prefs: []
  type: TYPE_NORMAL
- en: Given this leap in performance, the question everyone is asking is, How did
    they do this?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acc74e222738f0b3c3d8b5d28e79cf8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Screen Capture of [Lmsys Chatbot Arena](https://lmarena.ai/) Math Rankings on
    9/23/2024
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI has not publicly stated how they achieved these results, there
    have been a few papers recently that are good candidates for what is happening
    behind the scenes. One such paper is [“Scaling LLM Test-Time Compute Optimally
    can be More Effective than Scaling Model Parameters”](https://arxiv.org/pdf/2408.03314).
    This goes into how you can leverage…
  prefs: []
  type: TYPE_NORMAL
