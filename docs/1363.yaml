- en: Deep Dive into Anthropicâ€™s Sparse Autoencoders by Hand âœï¸
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢è®¨Anthropicçš„ç¨€ç–è‡ªç¼–ç å™¨ âœï¸
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31](https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31](https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709?source=collection_archive---------1-----------------------#2024-05-31)
- en: Explore the concepts behind the interpretability quest for LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¯è§£é‡Šæ€§è¿½æ±‚èƒŒåçš„æ¦‚å¿µ
- en: '[](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)[![Srijanie
    Dey, PhD](../Images/2b3292a3b22d712d91d0bfc14df64446.png)](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)
    [Srijanie Dey, PhD](https://medium.com/@srijanie.dey?source=post_page---byline--eebe0ef59709--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)
    Â·11 min readÂ·May 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eebe0ef59709--------------------------------)
    Â·é˜…è¯»æ—¶é•¿ï¼š11åˆ†é’ŸÂ·2024å¹´5æœˆ31æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/81130790966487c80fb82406f9fe2482.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81130790966487c80fb82406f9fe2482.png)'
- en: Image by author (Zephyra, the protector of Lumaria by my 4-year old)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…ï¼ˆZephyraï¼Œå¢ç›åˆ©äºšçš„å®ˆæŠ¤è€…ï¼Œç”±æˆ‘çš„4å²å­©å­ç»˜åˆ¶ï¼‰
- en: '*â€œIn the mystical lands of Lumaria, where ancient magic filled the air, lived
    Zephyra, the Ethereal Griffin. With the body of a lion and the wings of an eagle,
    Zephyra was the revered protector of the Codex of Truths, an ancient script holding
    the universeâ€™s secrets.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œåœ¨ç¥ç§˜çš„å¢ç›åˆ©äºšå¤§åœ°ä¸Šï¼Œå¤è€çš„é­”æ³•å¼¥æ¼«åœ¨ç©ºæ°”ä¸­ï¼Œç”Ÿæ´»ç€Zephyraï¼Œè¿™åªä»¥ç‹®èº«é¹°ç¿¼ä¸ºç‰¹å¾çš„ç©ºçµç‹®é¹«ã€‚Zephyraæ˜¯ã€ŠçœŸç†æ³•å…¸ã€‹çš„å´‡é«˜å®ˆæŠ¤è€…ï¼Œè¿™æœ¬å¤è€çš„æ–‡çŒ®è•´è—ç€å®‡å®™çš„ç§˜å¯†ã€‚*'
- en: '*Nestled in a sacred cave, the Codex was safeguarded by Zephyraâ€™s viridescent
    eyes, which could see through deception to unveil pure truths. One day, a dark
    sorcerer descended on the lands of Lumaria and sought to shroud the world in ignorance
    by concealing the Codex. The villagers called upon Zephyra, who soared through
    the skies, as a beacon of hope. With a majestic sweep of the wings, Zephyra created
    a protective barrier of light around the grove, repelling the sorcerer and exposing
    the truths.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œåœ¨ä¸€åº§ç¥åœ£çš„æ´çªŸä¸­ï¼Œæ³•å…¸ç”±Zephyraé‚£åŒç»¿è‰²çš„çœ¼ç›å®ˆæŠ¤ï¼Œå¥¹èƒ½çœ‹ç ´è™šä¼ªï¼Œæ­ç¤ºçº¯ç²¹çš„çœŸç†ã€‚ä¸€å¤©ï¼Œä¸€ä½é»‘æš—å·«å¸ˆé™ä¸´å¢ç›åˆ©äºšå¤§åœ°ï¼Œè¯•å›¾é€šè¿‡éšè—ã€Šæ³•å…¸ã€‹å°†ä¸–ç•Œç¬¼ç½©åœ¨æ— çŸ¥ä¹‹ä¸­ã€‚æ‘æ°‘ä»¬å‘¼å”¤Zephyraï¼Œå¥¹å¦‚å¸Œæœ›çš„ç¯å¡”åœ¨ç©ºä¸­ç¿±ç¿”ã€‚éšç€ä¸€é˜µé›„ä¼Ÿçš„ç¿…è†€æŒ¥åŠ¨ï¼ŒZephyraåœ¨æ ‘æ—å‘¨å›´åˆ›é€ äº†ä¸€ä¸ªä¿æŠ¤å…‰éšœï¼Œå‡»é€€äº†å·«å¸ˆï¼Œæ­ç¤ºäº†çœŸç†ã€‚â€*'
- en: '*After a long duel, it was concluded that the dark sorcerer was no match to
    Zephyraâ€™s light. Through her courage and vigilance, the true light kept shining
    over Lumaria. And as time went by, Lumaria was guided to prosperity under Zephyraâ€™s
    protection and its path stayed illuminated by the truths Zephyra safeguarded.
    And this is how Zephyraâ€™s legend lived on!â€*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œç»è¿‡ä¸€åœºé•¿ä¹…çš„å†³æ–—ï¼Œæœ€ç»ˆç»“è®ºæ˜¯é»‘æš—å·«å¸ˆæ— æ³•ä¸Zephyraçš„å…‰èŠ’æŠ—è¡¡ã€‚é€šè¿‡å¥¹çš„å‹‡æ°”å’Œè­¦è§‰ï¼ŒçœŸæ­£çš„å…‰æ˜ç»§ç»­ç…§è€€ç€å¢ç›åˆ©äºšã€‚éšç€æ—¶é—´æµé€ï¼Œå¢ç›åˆ©äºšåœ¨Zephyraçš„å®ˆæŠ¤ä¸‹èµ°å‘ç¹è£ï¼Œè€Œå¥¹æ‰€å®ˆæŠ¤çš„çœŸç†ä¹Ÿå§‹ç»ˆç…§äº®ç€è¿™ç‰‡åœŸåœ°ã€‚è¿™å°±æ˜¯Zephyraä¼ å¥‡å»¶ç»­ä¸‹å»çš„æ–¹å¼ï¼â€*'
- en: Anthropicâ€™s journey â€˜towards extracting interpretable featuresâ€™
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Anthropicçš„â€œæå–å¯è§£é‡Šç‰¹å¾ä¹‹æ—…â€
- en: Following the story of Zephyra, Anthropic AI delved into the expedition of extracting
    meaningful features in a model. The idea behind this investigation lies in understanding
    how different components in a neural network interact with one another and what
    role each component plays.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿½éšZephyraçš„æ•…äº‹ï¼ŒAnthropic AIæ·±å…¥æ¢è®¨äº†ä»æ¨¡å‹ä¸­æå–æœ‰æ„ä¹‰ç‰¹å¾çš„æ¢ç´¢è¿‡ç¨‹ã€‚è¿™é¡¹ç ”ç©¶çš„æ ¸å¿ƒæ€æƒ³åœ¨äºç†è§£ç¥ç»ç½‘ç»œä¸­ä¸åŒç»„ä»¶å¦‚ä½•ç›¸äº’ä½œç”¨ï¼Œä»¥åŠæ¯ä¸ªç»„ä»¶æ‰€æ‰®æ¼”çš„è§’è‰²ã€‚
- en: 'According to the paper **â€œ**[**Towards Monosemanticity: Decomposing Language
    Models With Dictionary Learning**](https://transformer-circuits.pub/2023/monosemantic-features/index.html)**â€**
    a Sparse Autoencoder is able to successfully extract meaningful features from
    a model. In other words, Sparse Autoencoders help break down the problem of â€˜polysemanticityâ€™
    â€” neural activations that correspond to several meanings/interpretations at once
    by focusing on sparsely activating features that hold a single interpretation
    â€” in other words, are more one-directional.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®è®ºæ–‡**â€œ**[**è¿ˆå‘å•ä¸€è¯­ä¹‰æ€§ï¼šé€šè¿‡å­—å…¸å­¦ä¹ åˆ†è§£è¯­è¨€æ¨¡å‹**](https://transformer-circuits.pub/2023/monosemantic-features/index.html)**â€**ï¼Œç¨€ç–è‡ªåŠ¨ç¼–ç å™¨èƒ½å¤ŸæˆåŠŸåœ°ä»æ¨¡å‹ä¸­æå–æœ‰æ„ä¹‰çš„ç‰¹å¾ã€‚æ¢å¥è¯è¯´ï¼Œç¨€ç–è‡ªåŠ¨ç¼–ç å™¨å¸®åŠ©è§£å†³â€˜å¤šä¹‰æ€§â€™é—®é¢˜â€”â€”ç¥ç»æ¿€æ´»åŒæ—¶å¯¹åº”å¤šä¸ªæ„ä¹‰/è§£é‡Šï¼Œé€šè¿‡ä¸“æ³¨äºç¨€ç–æ¿€æ´»é‚£äº›åªåŒ…å«å•ä¸€è§£é‡Šçš„ç‰¹å¾â€”â€”æ¢å¥è¯è¯´ï¼Œå®ƒä»¬æ›´å…·å•å‘æ€§ã€‚
- en: To understand how all of it is done, we have these beautiful handiworks on [Autoencoders](https://lnkd.in/g2rM9iV2)
    and [Sparse Autoencoders](https://www.linkedin.com/posts/tom-yeh_claude-autoencoder-aibyhand-activity-7199774212759183362-msKU/?)
    by Prof. [Tom Yeh](https://www.linkedin.com/in/tom-yeh/) that explain the behind-the-scenes
    workings of these phenomenal mechanisms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£è¿™ä¸€åˆ‡æ˜¯å¦‚ä½•å®Œæˆçš„ï¼Œæˆ‘ä»¬å¯ä»¥å‚è€ƒ[è‡ªåŠ¨ç¼–ç å™¨](https://lnkd.in/g2rM9iV2)å’Œ[ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨](https://www.linkedin.com/posts/tom-yeh_claude-autoencoder-aibyhand-activity-7199774212759183362-msKU/?)çš„ç²¾å½©ä½œå“ï¼Œè¿™äº›ä½œå“ç”±[Tom
    Yehæ•™æˆ](https://www.linkedin.com/in/tom-yeh/)åˆ›ä½œï¼Œè§£é‡Šäº†è¿™äº›ç¥å¥‡æœºåˆ¶çš„å¹•åå·¥ä½œåŸç†ã€‚
- en: (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the
    above-mentioned LinkedIn posts, which I have edited with his permission. )
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆä»¥ä¸‹æ‰€æœ‰å›¾ç‰‡ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œå‡æ¥è‡ªTom Yehæ•™æˆä¸Šé¢æåˆ°çš„LinkedInå¸–å­ï¼Œå·²è·ä»–çš„è®¸å¯è¿›è¡Œç¼–è¾‘ã€‚ï¼‰
- en: To begin, let us first let us first explore what an Autoencoder is and how it
    works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ¢è®¨ä¸€ä¸‹ä»€ä¹ˆæ˜¯è‡ªåŠ¨ç¼–ç å™¨ï¼Œå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: What is an Autoencoder?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯è‡ªåŠ¨ç¼–ç å™¨ï¼Ÿ
- en: Imagine a writer has his desk strewn with different papers â€” some are his notes
    for the story he is writing, some are copies of final drafts, some are again illustrations
    for his action-packed story. Now amidst this chaos, it is hard to find the important
    parts â€” more so when the writer is in a hurry and the publisher is on the phone
    demanding a book in two days. Thankfully, the writer has a very efficient assistant
    â€” this assistant makes sure the cluttered desk is cleaned regularly, grouping
    similar items, organizing and putting things into their right place. And as and
    when needed, the assistant would retrieve the correct items for the writer, helping
    him meet the deadlines set by his publisher.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸ªä½œå®¶ï¼Œä»–çš„æ¡Œå­ä¸Šæ•£ä¹±ç€å„ç§çº¸å¼ â€”â€”æœ‰çš„æ˜¯ä»–å†™æ•…äº‹çš„ç¬”è®°ï¼Œæœ‰çš„æ˜¯æœ€ç»ˆç¨¿çš„å‰¯æœ¬ï¼Œè¿˜æœ‰çš„æ˜¯ä¸ºä»–çš„åŠ¨ä½œæ•…äº‹ç»˜åˆ¶çš„æ’å›¾ã€‚åœ¨è¿™ç§æ··ä¹±ä¸­ï¼Œå¾ˆéš¾æ‰¾åˆ°é‡è¦çš„éƒ¨åˆ†â€”â€”å°¤å…¶æ˜¯å½“ä½œå®¶å¾ˆåŒ†å¿™ï¼Œå‡ºç‰ˆç¤¾åœ¨ç”µè¯ä¸­å‚¬ä¿ƒä»–ä¸¤å¤©å†…äº¤ä¹¦æ—¶ã€‚å¹¸è¿çš„æ˜¯ï¼Œä½œå®¶æœ‰ä¸€ä¸ªéå¸¸é«˜æ•ˆçš„åŠ©æ‰‹â€”â€”è¿™ä¸ªåŠ©æ‰‹ç¡®ä¿æ‚ä¹±çš„æ¡Œé¢è¢«å®šæœŸæ¸…ç†ï¼Œç±»ä¼¼çš„ç‰©å“è¢«åˆ†ç»„æ•´ç†ï¼Œå¹¶æŠŠä¸œè¥¿æ”¾åˆ°åˆé€‚çš„ä½ç½®ã€‚è€Œä¸”ï¼Œå½“ä½œå®¶éœ€è¦æ—¶ï¼ŒåŠ©æ‰‹ä¼šå¸®åŠ©ä»–å¿«é€Ÿæ‰¾åˆ°æ­£ç¡®çš„ç‰©å“ï¼Œå¸®åŠ©ä»–æŒ‰æ—¶å®Œæˆå‡ºç‰ˆç¤¾è®¾å®šçš„æˆªæ­¢æ—¥æœŸã€‚
- en: Well, the name of this assistant is Autoencoder. It mainly has two functions
    â€” encoding and decoding. Encoding refers to condensing input data and extracting
    the essential features (organization). Decoding is the process of reconstructing
    original data from encoded representation while aiming to minimize information
    loss (retrieval).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè¿™ä¸ªåŠ©æ‰‹çš„åå­—å«åšè‡ªåŠ¨ç¼–ç å™¨ã€‚å®ƒä¸»è¦æœ‰ä¸¤ä¸ªåŠŸèƒ½â€”â€”ç¼–ç å’Œè§£ç ã€‚ç¼–ç æ˜¯æŒ‡å‹ç¼©è¾“å…¥æ•°æ®å¹¶æå–å‡ºå…³é”®ç‰¹å¾ï¼ˆç»„ç»‡ï¼‰ã€‚è§£ç åˆ™æ˜¯ä»ç¼–ç è¡¨ç¤ºä¸­é‡å»ºåŸå§‹æ•°æ®çš„è¿‡ç¨‹ï¼Œç›®æ ‡æ˜¯å°½é‡å‡å°‘ä¿¡æ¯ä¸¢å¤±ï¼ˆæ¢å¤ï¼‰ã€‚
- en: Now letâ€™s look at how this assistant works.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªåŠ©æ‰‹æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: How does an Autoencoder Work?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è‡ªåŠ¨ç¼–ç å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: 'Given : Four training examples **X1, X2, X3, X4.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šï¼šå››ä¸ªè®­ç»ƒæ ·æœ¬ **X1, X2, X3, X4.**
- en: '[1] Auto'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[1] ç¼–ç å™¨'
- en: The first step is to copy the training examples to targets **Yâ€™**. The Autoencoderâ€™s
    work is to reconstruct these training examples. Since the targets are the training
    examples themselves, the word ***â€˜Autoâ€™*** is used which is Greek for ***â€˜selfâ€™***.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯å°†è®­ç»ƒæ ·æœ¬å¤åˆ¶åˆ°ç›®æ ‡ **Yâ€™**ã€‚è‡ªåŠ¨ç¼–ç å™¨çš„å·¥ä½œæ˜¯é‡å»ºè¿™äº›è®­ç»ƒæ ·æœ¬ã€‚ç”±äºç›®æ ‡å°±æ˜¯è®­ç»ƒæ ·æœ¬æœ¬èº«ï¼Œæ‰€ä»¥ä½¿ç”¨äº†è¯***â€˜Autoâ€™***ï¼Œå®ƒæ˜¯å¸Œè…Šè¯­çš„***â€˜è‡ªæˆ‘â€™***æ„æ€ã€‚
- en: '[2] Encoder : Layer 1 +ReLU'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2] ç¼–ç å™¨ï¼šç¬¬ä¸€å±‚ +ReLU'
- en: As we have seen in all our previous models, a simple weight and bias matrix
    coupled with ReLU is powerful and is able to do wonders. Thus, by using the first
    Encoding layer we reduce the size of the original feature set from 4x4 to 3x4.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ä¹‹å‰çš„æ‰€æœ‰æ¨¡å‹ä¸­æ‰€çœ‹åˆ°çš„ï¼Œç®€å•çš„æƒé‡å’Œåç½®çŸ©é˜µç»“åˆReLUéå¸¸å¼ºå¤§ï¼Œèƒ½å¤Ÿåšå‡ºæƒŠäººçš„æ•ˆæœã€‚å› æ­¤ï¼Œé€šè¿‡ä½¿ç”¨ç¬¬ä¸€ä¸ªç¼–ç å±‚ï¼Œæˆ‘ä»¬å°†åŸå§‹ç‰¹å¾é›†çš„å¤§å°ä»4x4å‡å°‘åˆ°3x4ã€‚
- en: '![](../Images/c2c2896b0761620ef71f79dd24b7de1a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c2896b0761620ef71f79dd24b7de1a.png)'
- en: 'A quick recap:'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç®€è¦å›é¡¾ï¼š
- en: ''
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Linear transformation** : The input embedding vector is multiplied by the
    weight matrix W and then added with the bias vector **b**,'
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**çº¿æ€§å˜æ¢**ï¼šè¾“å…¥åµŒå…¥å‘é‡ä¸æƒé‡çŸ©é˜µWç›¸ä¹˜ï¼Œç„¶åä¸åç½®å‘é‡**b**ç›¸åŠ ï¼Œ'
- en: ''
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: z = **W**x+**b**, where **W** is the weight matrix, x is our word embedding
    and **b** is the bias vector.
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: z = **W**x + **b**ï¼Œå…¶ä¸­**W**æ˜¯æƒé‡çŸ©é˜µï¼Œxæ˜¯æˆ‘ä»¬çš„è¯åµŒå…¥ï¼Œ**b**æ˜¯åç½®å‘é‡ã€‚
- en: ''
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ReLU activation function** : Next, we apply the ReLU to this intermediate
    z.'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ReLUæ¿€æ´»å‡½æ•°**ï¼šæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ReLUåº”ç”¨äºè¿™ä¸ªä¸­é—´çš„zã€‚'
- en: ''
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ReLU returns the element-wise maximum of the input and zero. Mathematically,
    **h** = max{0,z}.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ReLUè¿”å›è¾“å…¥å’Œé›¶ä¹‹é—´çš„é€å…ƒç´ æœ€å¤§å€¼ã€‚æ•°å­¦ä¸Šï¼Œ**h** = max{0, z}ã€‚
- en: '[3] Encoder : Layer 2 + ReLU'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[3] ç¼–ç å™¨ï¼šç¬¬äºŒå±‚ + ReLU'
- en: The output of the previous layer is processed by the second Encoder layer which
    reduces the input size further to 2x3\. This is where the extraction of relevant
    features occurs. This layer is also called the â€˜bottleneckâ€™ since the outputs
    in this layer have much lower features than the input features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸€å±‚çš„è¾“å‡ºç”±ç¬¬äºŒä¸ªç¼–ç å™¨å±‚å¤„ç†ï¼Œè¯¥å±‚è¿›ä¸€æ­¥å°†è¾“å…¥å¤§å°å‡å°åˆ°2x3ã€‚è¿™æ—¶ï¼Œç›¸å…³ç‰¹å¾çš„æå–å‘ç”Ÿåœ¨æ­¤å±‚ã€‚è¿™ä¸ªå±‚ä¹Ÿå«åšâ€œç“¶é¢ˆâ€ï¼Œå› ä¸ºè¯¥å±‚çš„è¾“å‡ºç‰¹å¾è¿œå°äºè¾“å…¥ç‰¹å¾ã€‚
- en: '![](../Images/486e9e67fbbfac831061f662d28f156c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/486e9e67fbbfac831061f662d28f156c.png)'
- en: '[4] Decoder : Layer 1 + ReLU'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[4] è§£ç å™¨ï¼šç¬¬ä¸€å±‚ + ReLU'
- en: Once the encoding process is complete, the next step is to decode the relevant
    features to build â€˜backâ€™ the final output. To do so, we multiply the features
    from the last step with corresponding weights and biases and apply the ReLU layer.
    The result is a 3x4 matrix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç¼–ç è¿‡ç¨‹å®Œæˆï¼Œä¸‹ä¸€æ­¥æ˜¯è§£ç ç›¸å…³ç‰¹å¾ï¼Œä»¥æ„å»ºæœ€ç»ˆè¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä¸Šä¸€æ­¥çš„ç‰¹å¾ä¸ç›¸åº”çš„æƒé‡å’Œåç½®ç›¸ä¹˜ï¼Œå¹¶åº”ç”¨ReLUå±‚ã€‚ç»“æœæ˜¯ä¸€ä¸ª3x4çš„çŸ©é˜µã€‚
- en: '![](../Images/9f9ffd4521b3c28b6eba26be3b1c2100.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f9ffd4521b3c28b6eba26be3b1c2100.png)'
- en: '[5] Decoder : Layer 2 + ReLU'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[5] è§£ç å™¨ï¼šç¬¬äºŒå±‚ + ReLU'
- en: A second Decoder layer (weight, biases + ReLU) applies on the previous output
    to give the final result which is the reconstructed 4x4 matrix. We do so to get
    back to original dimension in order to compare the results with our original target.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªè§£ç å™¨å±‚ï¼ˆæƒé‡ã€åç½® + ReLUï¼‰åº”ç”¨äºä¹‹å‰çš„è¾“å‡ºï¼Œç»™å‡ºæœ€ç»ˆç»“æœï¼Œå³é‡å»ºçš„4x4çŸ©é˜µã€‚æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†æ¢å¤åŸå§‹ç»´åº¦ï¼Œä»¥ä¾¿å°†ç»“æœä¸æˆ‘ä»¬çš„åŸå§‹ç›®æ ‡è¿›è¡Œæ¯”è¾ƒã€‚
- en: '![](../Images/47b3bd18b0209d34df20b5cb9f242b97.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47b3bd18b0209d34df20b5cb9f242b97.png)'
- en: '[6] Loss Gradients & BackPropagation'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[6] æŸå¤±æ¢¯åº¦å’Œåå‘ä¼ æ’­'
- en: Once the output from the decoder layer is obtained, we calculate the gradients
    of the Mean Square Error (MSE) between the **outputs (Y)** and the **targets (Yâ€™)**.
    To do so, we find **2*(Y-Yâ€™)** , which gives us the final gradients that activate
    the backpropagation process and updates the weights and biases accordingly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦è·å¾—è§£ç å™¨å±‚çš„è¾“å‡ºï¼Œæˆ‘ä»¬è®¡ç®—**è¾“å‡ºï¼ˆYï¼‰**å’Œ**ç›®æ ‡ï¼ˆYâ€™ï¼‰**ä¹‹é—´çš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰çš„æ¢¯åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ±‚è§£**2*(Y-Yâ€™)**ï¼Œè¿™ç»™å‡ºäº†æœ€ç»ˆçš„æ¢¯åº¦ï¼Œæ¿€æ´»åå‘ä¼ æ’­è¿‡ç¨‹ï¼Œå¹¶ç›¸åº”åœ°æ›´æ–°æƒé‡å’Œåç½®ã€‚
- en: '![](../Images/286ca27d2139e0c693ec2fdcee1050b0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/286ca27d2139e0c693ec2fdcee1050b0.png)'
- en: Now that we understand how the Autoencoder works, itâ€™s time to explore how its
    **sparse variation** is able to achieve interpretability for large language models
    (LLMs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬äº†è§£äº†è‡ªç¼–ç å™¨çš„å·¥ä½œåŸç†ï¼Œæ¥ä¸‹æ¥å°±è¯¥æ¢è®¨å®ƒçš„**ç¨€ç–å˜ä½“**å¦‚ä½•å®ç°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¯è§£é‡Šæ€§ã€‚
- en: Sparse Autoencoder â€” How does it work?
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨€ç–è‡ªç¼–ç å™¨â€”â€”å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ
- en: 'To start with, suppose we are given:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œå‡è®¾æˆ‘ä»¬ç»™å®šäº†ï¼š
- en: The output of a transformer after the feed-forward layer has processed it, i.e.
    let us assume we have the model activations for five tokens (X). They are good
    but they do not shed light on how the model arrives at its decision or makes the
    predictions.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ç»è¿‡å‰é¦ˆå±‚å¤„ç†åçš„å˜æ¢å™¨è¾“å‡ºï¼Œå³å‡è®¾æˆ‘ä»¬æœ‰äº”ä¸ªæ ‡è®°ï¼ˆXï¼‰çš„æ¨¡å‹æ¿€æ´»å€¼ã€‚å®ƒä»¬å¾ˆå¥½ï¼Œä½†å¹¶ä¸èƒ½æ­ç¤ºæ¨¡å‹å¦‚ä½•å¾—å‡ºå†³ç­–æˆ–è¿›è¡Œé¢„æµ‹ã€‚
- en: '![](../Images/5c01141ce07bd9ff3330b673178216f6.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c01141ce07bd9ff3330b673178216f6.png)'
- en: 'The prime question here is:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ä¸»è¦é—®é¢˜æ˜¯ï¼š
- en: Is it possible to map each activation (3D) to a higher-dimension space (6D)
    that will help with the understanding?
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ˜¯å¦æœ‰å¯èƒ½å°†æ¯ä¸ªæ¿€æ´»ï¼ˆ3Dï¼‰æ˜ å°„åˆ°ä¸€ä¸ªæ›´é«˜ç»´åº¦ç©ºé—´ï¼ˆ6Dï¼‰ï¼Œä»¥å¸®åŠ©ç†è§£ï¼Ÿ
- en: '[1] Encoder : Linear Layer'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[1] ç¼–ç å™¨ï¼šçº¿æ€§å±‚'
- en: The first step in the Encoder layer is to multiply the input **X** with encoder
    weights and add biases (as done in the first step of an Autoencoder).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨å±‚çš„ç¬¬ä¸€æ­¥æ˜¯å°†è¾“å…¥**X**ä¸ç¼–ç å™¨æƒé‡ç›¸ä¹˜ï¼Œå¹¶æ·»åŠ åç½®ï¼ˆå¦‚åœ¨è‡ªç¼–ç å™¨çš„ç¬¬ä¸€æ­¥ä¸­æ‰€åšï¼‰ã€‚
- en: '![](../Images/312068c487f5a71ba089baf15668d291.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/312068c487f5a71ba089baf15668d291.png)'
- en: '[2] Encoder : ReLU'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[2] ç¼–ç å™¨ï¼šReLU'
- en: The next sub-step is to apply the ReLU activation function to add non-linearity
    and suppress negative activations. This suppression leads to many features being
    set to 0 which enables the concept of sparsity â€” outputting sparse and interpretable
    features ***f.***
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€å­æ­¥éª¤æ˜¯åº”ç”¨ReLUæ¿€æ´»å‡½æ•°ï¼ŒåŠ å…¥éçº¿æ€§å¹¶æŠ‘åˆ¶è´Ÿæ¿€æ´»ã€‚è¿™ç§æŠ‘åˆ¶å¯¼è‡´è®¸å¤šç‰¹å¾è¢«è®¾ä¸º0ï¼Œä»è€Œå®ç°ç¨€ç–æ€§æ¦‚å¿µâ€”â€”è¾“å‡ºç¨€ç–ä¸”å¯è§£é‡Šçš„ç‰¹å¾***f.***
- en: Interpretability happens when we have only one or two positive features. If
    we examine ***f6***, we can see **X2** and **X3** are positive, and may say that
    both have â€˜Mountainâ€™ in common.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åªæœ‰ä¸€ä¸ªæˆ–ä¸¤ä¸ªæ­£ç‰¹å¾æ—¶ï¼Œå°±ä¼šå‘ç”Ÿå¯è§£é‡Šæ€§ã€‚å¦‚æœæˆ‘ä»¬æ£€æŸ¥***f6***ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°**X2**å’Œ**X3**æ˜¯æ­£çš„ï¼Œå¹¶å¯ä»¥è¯´å®ƒä»¬ä¸¤ä¸ªæœ‰â€˜å±±â€™è¿™ä¸€å…±åŒç‚¹ã€‚
- en: '![](../Images/7b8b7f37cd47ccde17cf92263a8a113b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b8b7f37cd47ccde17cf92263a8a113b.png)'
- en: '[3] Decoder : Reconstruction'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[3] è§£ç å™¨ : é‡å»º'
- en: Once we are done with the encoder, we proceed to the decoder step. We multiply
    ***f***with decoder weights and add biases. This outputs **Xâ€™**, which is the
    reconstruction of **X** from interpretable features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å®Œæˆç¼–ç å™¨æ­¥éª¤ï¼Œæˆ‘ä»¬å°±è¿›å…¥è§£ç å™¨æ­¥éª¤ã€‚æˆ‘ä»¬å°†***f***ä¸è§£ç å™¨çš„æƒé‡ç›¸ä¹˜å¹¶æ·»åŠ åç½®ã€‚è¿™æ ·è¾“å‡ºçš„**Xâ€™**å°±æ˜¯ä»å¯è§£é‡Šç‰¹å¾é‡å»ºçš„**X**ã€‚
- en: '![](../Images/0ef0969dd32ed6d5a1a46041619c2efe.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ef0969dd32ed6d5a1a46041619c2efe.png)'
- en: As done in an Autoencoder, we want **Xâ€™** to be as close to **X** as possible.
    To ensure that, further training is essential.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚åŒåœ¨è‡ªç¼–ç å™¨ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¸Œæœ›**Xâ€™**å°½å¯èƒ½æ¥è¿‘**X**ã€‚ä¸ºäº†ç¡®ä¿è¿™ä¸€ç‚¹ï¼Œè¿›ä¸€æ­¥çš„è®­ç»ƒæ˜¯å¿…ä¸å¯å°‘çš„ã€‚
- en: '[4] Decoder : Weights'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[4] è§£ç å™¨ : æƒé‡'
- en: As an intermediary step, we compute the L2 norm for each of the weights in this
    step. We keep them aside to be used later.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªä¸­é—´æ­¥éª¤ï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªæƒé‡çš„L2èŒƒæ•°ï¼Œå¹¶å°†å®ƒä»¬ä¿ç•™ä¸‹æ¥ä»¥ä¾¿ç¨åä½¿ç”¨ã€‚
- en: '![](../Images/9208c5b6e536090638fa96455a2dc40b.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9208c5b6e536090638fa96455a2dc40b.png)'
- en: '**L2-norm**'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**L2-èŒƒæ•°**'
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Also known as Euclidean norm, L2-norm calculates the magnitude of a vector
    using the formula: ||x||â‚‚ = âˆš(Î£áµ¢ xáµ¢Â²).'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¹Ÿç§°ä¸ºæ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼ŒL2èŒƒæ•°ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—å‘é‡çš„å¤§å°ï¼š||x||â‚‚ = âˆš(Î£áµ¢ xáµ¢Â²)ã€‚
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, it sums the squares of each component and then takes the square
    root over the result. This norm provides a straightforward way to quantify the
    length or distance of a vector in Euclidean space.
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œå®ƒå¯¹æ¯ä¸ªåˆ†é‡çš„å¹³æ–¹è¿›è¡Œæ±‚å’Œï¼Œç„¶åå¯¹ç»“æœå–å¹³æ–¹æ ¹ã€‚è¿™ä¸ªèŒƒæ•°æä¾›äº†ä¸€ç§ç›´æ¥çš„æ–¹å¼æ¥é‡åŒ–å‘é‡åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´ä¸­çš„é•¿åº¦æˆ–è·ç¦»ã€‚
- en: Training
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒ
- en: 'As mentioned earlier, a Sparse Autoencoder instils extensive training to get
    the reconstructed **Xâ€™** closer to **X**. To illustrate that, we proceed to the
    next steps below:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œç¨€ç–è‡ªç¼–ç å™¨é€šè¿‡å¹¿æ³›çš„è®­ç»ƒä½¿å¾—é‡å»ºçš„**Xâ€™**å°½å¯èƒ½æ¥è¿‘**X**ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
- en: '[5] Sparsity : L1 Loss'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[5] ç¨€ç–æ€§ : L1æŸå¤±'
- en: The goal here is to obtain as many values close to zero / zero as possible.
    We do so by invoking **L1 sparsity** to penalize the absolute values of the weights
    â€” the core idea being that we want to make the sum as small as possible.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ç›®æ ‡æ˜¯å°½å¯èƒ½è·å¾—æ¥è¿‘é›¶/é›¶çš„å€¼ã€‚æˆ‘ä»¬é€šè¿‡è°ƒç”¨**L1ç¨€ç–æ€§**æ¥æƒ©ç½šæƒé‡çš„ç»å¯¹å€¼â€”â€”æ ¸å¿ƒæ€æƒ³æ˜¯æˆ‘ä»¬å¸Œæœ›ä½¿å¾—å’Œå°½å¯èƒ½å°ã€‚
- en: '![](../Images/f8aa5583bdaebc8564e7bd8b44ddd399.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8aa5583bdaebc8564e7bd8b44ddd399.png)'
- en: '**L1-loss**'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**L1-æŸå¤±**'
- en: ''
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The L1-loss is calculated as the sum of the absolute values of the weights:
    L1 = Î»Î£|w|, where Î» is a regularization parameter.'
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: L1-æŸå¤±æ˜¯æƒé‡ç»å¯¹å€¼ä¹‹å’Œï¼šL1 = Î»Î£|w|ï¼Œå…¶ä¸­Î»æ˜¯æ­£åˆ™åŒ–å‚æ•°ã€‚
- en: ''
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This encourages many weights to become zero, simplifying the model and thus
    enhancing **interpretability**.
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä¿ƒä½¿è®¸å¤šæƒé‡å˜ä¸ºé›¶ï¼Œä»è€Œç®€åŒ–äº†æ¨¡å‹ï¼Œå¹¶å¢å¼ºäº†**å¯è§£é‡Šæ€§**ã€‚
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, L1 helps build the focus on the most relevant features while
    also preventing overfitting, improving model generalization, and reducing computational
    complexity.
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼ŒL1æœ‰åŠ©äºå°†æ³¨æ„åŠ›é›†ä¸­åœ¨æœ€ç›¸å…³çš„ç‰¹å¾ä¸Šï¼ŒåŒæ—¶é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ”¹å–„æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶å‡å°‘è®¡ç®—å¤æ‚æ€§ã€‚
- en: '[6] Sparsity : Gradient'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[6] ç¨€ç–æ€§ : æ¢¯åº¦'
- en: The next step is to calculate **L1**â€™s gradients which -1 for positive values.
    Thus, for all values of ***f >0*** , the result will be set to -1.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥æ˜¯è®¡ç®—**L1**çš„æ¢¯åº¦ï¼Œå¯¹äºæ­£å€¼ä¸º-1ã€‚å› æ­¤ï¼Œå¯¹äºæ‰€æœ‰å€¼ä¸º***f >0***çš„æƒ…å†µï¼Œç»“æœå°†è¢«è®¾å®šä¸º-1ã€‚
- en: '![](../Images/5e85338940e15e3475ce8be719ff40e3.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e85338940e15e3475ce8be719ff40e3.png)'
- en: '**How does L1 penalty push weights towards zero?**'
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**L1æƒ©ç½šå¦‚ä½•å°†æƒé‡æ¨å‘é›¶ï¼Ÿ**'
- en: ''
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The gradient of the L1 penalty pushes weights towards zero through a process
    that applies a constant force, regardless of the weightâ€™s current value. Hereâ€™s
    how it works (all images in this sub-section are by author):'
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: L1æƒ©ç½šçš„æ¢¯åº¦é€šè¿‡ä¸€ä¸ªè¿‡ç¨‹å°†æƒé‡æ¨å‘é›¶ï¼Œè¿™ä¸ªè¿‡ç¨‹æ–½åŠ ä¸€ä¸ªæ’å®šçš„åŠ›ï¼Œæ— è®ºæƒé‡çš„å½“å‰å€¼å¦‚ä½•ã€‚ä¸‹é¢æ˜¯å®ƒçš„å·¥ä½œåŸç†ï¼ˆæœ¬å°èŠ‚ä¸­çš„æ‰€æœ‰å›¾ç‰‡å‡ä¸ºä½œè€…æä¾›ï¼‰ï¼š
- en: ''
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The L1 penalty is expressed as:'
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: L1æƒ©ç½šè¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/04df48be6e16080c94467c08c9f248ef.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04df48be6e16080c94467c08c9f248ef.png)'
- en: 'The gradient of this penalty with respect to a weight ***w*** is:'
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ©ç½šç›¸å¯¹äºæƒé‡***w***çš„æ¢¯åº¦æ˜¯ï¼š
- en: '![](../Images/52a15cda96d611eb04f7e52d79a4a817.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52a15cda96d611eb04f7e52d79a4a817.png)'
- en: 'where ***sign(w)*** is:'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…¶ä¸­***sign(w)***æ˜¯ï¼š
- en: '![](../Images/5d0377f88ae46308b533ea7284c90ab7.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d0377f88ae46308b533ea7284c90ab7.png)'
- en: 'During gradient descent, the update rule for weights is:'
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸­ï¼Œæƒé‡çš„æ›´æ–°è§„åˆ™æ˜¯ï¼š
- en: '![](../Images/c4cf2e8036e3f8abe10def8af14ab467.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4cf2e8036e3f8abe10def8af14ab467.png)'
- en: where ğ° is the learning rate.
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…¶ä¸­ğ°æ˜¯å­¦ä¹ ç‡ã€‚
- en: ''
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The **constant subtraction (or addition)** of **Î»** from the weight value (depending
    on its sign) decreases the absolute value of the weight. If the weight is small
    enough, this process can drive it to exactly zero.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**å¸¸é‡å‡æ³•ï¼ˆæˆ–åŠ æ³•ï¼‰**æ“ä½œä¸­çš„**Î»**ï¼ˆæ ¹æ®å…¶ç¬¦å·ï¼‰ä¼šå‡å°æƒé‡å€¼çš„ç»å¯¹å€¼ã€‚å¦‚æœæƒé‡è¶³å¤Ÿå°ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯ä»¥å°†å…¶å®Œå…¨é©±åŠ¨ä¸ºé›¶ã€‚'
- en: '[7] Sparsity : Zero'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[7] ç¨€ç–æ€§ï¼šé›¶'
- en: For all other values that are already zero, we keep them unchanged since they
    have already been zeroed out.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å·²ç»æ˜¯é›¶çš„å…¶ä»–å€¼ï¼Œæˆ‘ä»¬ä¿æŒå®ƒä»¬ä¸å˜ï¼Œå› ä¸ºå®ƒä»¬å·²ç»è¢«å½’é›¶ã€‚
- en: '![](../Images/35dcfe7c72319bdffd087a7baa4fe776.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35dcfe7c72319bdffd087a7baa4fe776.png)'
- en: '[8] Sparsity : Weight'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[8] ç¨€ç–æ€§ï¼šæƒé‡'
- en: We multiple each row of the gradient matrix obtained in Step 6 by the corresponding
    decoder weights obtained in Step 4\. This step is crucial as it prevents the model
    from learning large weights which would add incorrect information while reconstructing
    the results.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç¬¬6æ­¥ä¸­è·å¾—çš„æ¢¯åº¦çŸ©é˜µçš„æ¯ä¸€è¡Œä¸ç¬¬4æ­¥ä¸­è·å¾—çš„ç›¸åº”è§£ç å™¨æƒé‡ç›¸ä¹˜ã€‚è¿™ä¸ªæ­¥éª¤è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒé˜²æ­¢æ¨¡å‹å­¦ä¹ åˆ°è¾ƒå¤§çš„æƒé‡ï¼Œé¿å…åœ¨é‡å»ºç»“æœæ—¶åŠ å…¥é”™è¯¯çš„ä¿¡æ¯ã€‚
- en: '![](../Images/306a1ce823111e9d815e4b7f1fd15802.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/306a1ce823111e9d815e4b7f1fd15802.png)'
- en: '[9] Reconstruction : MSE Loss'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[9] é‡å»ºï¼šå‡æ–¹è¯¯å·®æŸå¤±'
- en: We use the Mean Square Error or the **L2** loss function to calculate the difference
    between **Xâ€™** and **X**. The goal as seen previously is to minimize the error
    to the lowest value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨å‡æ–¹è¯¯å·®æˆ–**L2**æŸå¤±å‡½æ•°æ¥è®¡ç®—**Xâ€™**å’Œ**X**ä¹‹é—´çš„å·®å¼‚ã€‚å¦‚å‰æ‰€è¿°ï¼Œç›®æ ‡æ˜¯å°†è¯¯å·®æœ€å°åŒ–åˆ°æœ€ä½å€¼ã€‚
- en: '![](../Images/f4383312900f938f95dc354d99c296fe.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4383312900f938f95dc354d99c296fe.png)'
- en: '[10] Reconstruction : Gradient'
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[10] é‡å»ºï¼šæ¢¯åº¦'
- en: The gradient of **L2** loss is **2*(Xâ€™-X)**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2**æŸå¤±çš„æ¢¯åº¦æ˜¯**2*(Xâ€™-X)**ã€‚'
- en: And hence as seen for the original Autoencoders, we run backpropagation to update
    the weights and the biases. The catch here is finding a good balance between sparsity
    and reconstruction.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ­£å¦‚åŸå§‹è‡ªç¼–ç å™¨æ‰€ç¤ºï¼Œæˆ‘ä»¬è¿è¡Œåå‘ä¼ æ’­æ¥æ›´æ–°æƒé‡å’Œåå·®ã€‚è¿™é‡Œçš„å…³é”®æ˜¯æ‰¾åˆ°ç¨€ç–æ€§å’Œé‡å»ºä¹‹é—´çš„è‰¯å¥½å¹³è¡¡ã€‚
- en: '![](../Images/8b70731a04ecd7810360275f13324e41.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b70731a04ecd7810360275f13324e41.png)'
- en: And with this, we come to the end of this very clever and intuitive way of learning
    how a model understands an idea and the direction it takes to generate a response.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·ï¼Œæˆ‘ä»¬å°±ç»“æŸäº†è¿™ä¸€éå¸¸å·§å¦™ä¸”ç›´è§‚çš„å­¦ä¹ æ–¹æ³•ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ¨¡å‹æ˜¯å¦‚ä½•ç†è§£ä¸€ä¸ªæ¦‚å¿µï¼Œå¹¶åœ¨ç”Ÿæˆå“åº”æ—¶é‡‡å–çš„æ–¹å‘ã€‚
- en: 'To summarize:'
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»ç»“ï¼š
- en: 'An **Autoencoder** overall consists of two parts : **Encoder** and **Decoder**.
    The **Encoder** uses weights and biases coupled with the ReLU activation function
    to compress the initial input features into a lower dimension, trying to capture
    only the relevant parts. The **Decoder** on the other hand takes the output of
    the Encoder and works to reconstruct the input features back to their original
    state. Since the targets in an Autoencoder are the initial features themselves,
    hence the use of the word â€˜autoâ€™. The aim, as is for standard neural networks,
    is to achieve the lowest error (difference) between the target and the input features
    â€” and it is achieved by propagating the gradient of the error through the network
    while updating the weights and biases.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ª**è‡ªç¼–ç å™¨**æ•´ä½“ç”±ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆï¼š**ç¼–ç å™¨**å’Œ**è§£ç å™¨**ã€‚**ç¼–ç å™¨**åˆ©ç”¨æƒé‡å’Œåå·®ä»¥åŠReLUæ¿€æ´»å‡½æ•°å°†åˆå§‹è¾“å…¥ç‰¹å¾å‹ç¼©åˆ°ä¸€ä¸ªè¾ƒä½çš„ç»´åº¦ï¼ŒåŠ›å›¾ä»…æ•æ‰ç›¸å…³çš„éƒ¨åˆ†ã€‚å¦ä¸€æ–¹é¢ï¼Œ**è§£ç å™¨**åˆ™æ¥æ”¶ç¼–ç å™¨çš„è¾“å‡ºï¼Œå¹¶åŠªåŠ›å°†è¾“å…¥ç‰¹å¾é‡å»ºå›å…¶åŸå§‹çŠ¶æ€ã€‚ç”±äºè‡ªç¼–ç å™¨çš„ç›®æ ‡å°±æ˜¯åˆå§‹ç‰¹å¾æœ¬èº«ï¼Œå› æ­¤æ‰ç§°ä¹‹ä¸ºâ€œè‡ªâ€ã€‚ç›®æ ‡å’Œæ ‡å‡†ç¥ç»ç½‘ç»œä¸€æ ·ï¼Œæ˜¯é€šè¿‡ä¼ æ’­è¯¯å·®çš„æ¢¯åº¦å¹¶æ›´æ–°æƒé‡å’Œåå·®æ¥å®ç°ç›®æ ‡ç‰¹å¾å’Œè¾“å…¥ç‰¹å¾ä¹‹é—´çš„æœ€å°è¯¯å·®ï¼ˆå·®å¼‚ï¼‰ã€‚
- en: 'A **Sparse Autoencoder** consists of all the components as a standard Autoencoder
    along with a few more additions. The key here is the different approach in the
    training step. Since the aim here is to retrieve the interpretable features, we
    want to zero out those values which hold relatively less meaning. Once the encoder
    uses ReLU to suppress the negative values, we go a step further and use L1-Loss
    on the result to encourage sparsity by penalizing the absolute values of the weights.
    This is achieved by adding a penalty term to the loss function, which is the sum
    of the absolute values of the weights: Î»Î£|w|. The weights that remain non-zero
    are those that are crucial for the modelâ€™s performance.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¨€ç–è‡ªç¼–ç å™¨**ç”±ä¸æ ‡å‡†è‡ªç¼–ç å™¨ç›¸åŒçš„æ‰€æœ‰ç»„ä»¶ä»¥åŠä¸€äº›é¢å¤–çš„ç»„æˆéƒ¨åˆ†æ„æˆã€‚è¿™é‡Œçš„å…³é”®æ˜¯è®­ç»ƒæ­¥éª¤ä¸­çš„ä¸åŒæ–¹æ³•ã€‚ç”±äºç›®æ ‡æ˜¯æå–å¯è§£é‡Šçš„ç‰¹å¾ï¼Œæˆ‘ä»¬å¸Œæœ›å°†é‚£äº›ç›¸å¯¹æ„ä¹‰è¾ƒå°çš„å€¼ç½®ä¸ºé›¶ã€‚ä¸€æ—¦ç¼–ç å™¨ä½¿ç”¨ReLUæŠ‘åˆ¶è´Ÿå€¼ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ä½¿ç”¨L1æŸå¤±å¯¹ç»“æœè¿›è¡Œå¤„ç†ï¼Œé€šè¿‡æƒ©ç½šæƒé‡çš„ç»å¯¹å€¼æ¥ä¿ƒè¿›ç¨€ç–æ€§ã€‚è¿™æ˜¯é€šè¿‡å‘æŸå¤±å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ªæƒ©ç½šé¡¹æ¥å®ç°çš„ï¼Œè¯¥æƒ©ç½šé¡¹æ˜¯æƒé‡ç»å¯¹å€¼ä¹‹å’Œï¼šÎ»Î£|w|ã€‚é‚£äº›ä¿æŒéé›¶çš„æƒé‡æ˜¯å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦çš„ã€‚'
- en: Extracting Interpretable features using Sparsity
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ©ç”¨ç¨€ç–æ€§æå–å¯è§£é‡Šç‰¹å¾
- en: As humans, our brains activate only a small subset of neurons in response to
    specific stimuli. Likewise, Sparse Autoencoders learn a sparse representation
    of the input by leveraging sparsity constraints like **L1** regularization. By
    doing so, a Sparse Autoencoder is able to extract interpretable features from
    complex data thus enhancing the simplicity and interpretability of the learned
    features. This selective activation mirroring biological neural processes helps
    focus on the most relevant aspects of the input data making the models more robust
    and efficient.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºäººç±»ï¼Œæˆ‘ä»¬çš„å¤§è„‘ä»…å¯¹ç‰¹å®šåˆºæ¿€æ¿€æ´»ä¸€å°éƒ¨åˆ†ç¥ç»å…ƒã€‚åŒæ ·ï¼Œç¨€ç–è‡ªç¼–ç å™¨é€šè¿‡åˆ©ç”¨ç¨€ç–æ€§çº¦æŸï¼Œå¦‚**L1**æ­£åˆ™åŒ–ï¼Œå­¦ä¹ è¾“å…¥çš„ç¨€ç–è¡¨ç¤ºã€‚é€šè¿‡è¿™æ ·åšï¼Œç¨€ç–è‡ªç¼–ç å™¨èƒ½å¤Ÿä»å¤æ‚æ•°æ®ä¸­æå–å¯è§£é‡Šçš„ç‰¹å¾ï¼Œä»è€Œå¢å¼ºæ‰€å­¦ä¹ ç‰¹å¾çš„ç®€æ´æ€§å’Œå¯è§£é‡Šæ€§ã€‚è¿™ç§ç±»ä¼¼ç”Ÿç‰©ç¥ç»è¿‡ç¨‹çš„é€‰æ‹©æ€§æ¿€æ´»æœ‰åŠ©äºèšç„¦äºè¾“å…¥æ•°æ®ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ï¼Œä½¿æ¨¡å‹æ›´åŠ é²æ£’å’Œé«˜æ•ˆã€‚
- en: With Anthropicâ€™s endeavor to understand interpretability in AI models, their
    initiative highlights the need for transparent and understandable AI systems,
    especially as they become more integrated into critical decision-making processes.
    By focusing on creating models that are both powerful and interpretable, Anthropic
    contributes to the development of AI that can be trusted and effectively utilized
    in real-world applications.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€Anthropicè‡´åŠ›äºç†è§£AIæ¨¡å‹ä¸­çš„å¯è§£é‡Šæ€§ï¼Œå…¶å€¡è®®å¼ºè°ƒäº†é€æ˜å’Œæ˜“ç†è§£çš„AIç³»ç»Ÿçš„å¿…è¦æ€§ï¼Œå°¤å…¶æ˜¯åœ¨AIç³»ç»Ÿè¶Šæ¥è¶Šå¤šåœ°èå…¥å…³é”®å†³ç­–è¿‡ç¨‹æ—¶ã€‚é€šè¿‡ä¸“æ³¨äºåˆ›å»ºæ—¢å¼ºå¤§åˆå¯è§£é‡Šçš„æ¨¡å‹ï¼ŒAnthropicä¸ºå¼€å‘å¯ä¿¡èµ–ä¸”èƒ½æœ‰æ•ˆåº”ç”¨äºç°å®ä¸–ç•Œçš„AIåšå‡ºäº†è´¡çŒ®ã€‚
- en: In conclusion, **Sparse Autoencoders** are vital for extracting interpretable
    features, enhancing model robustness, and ensuring efficiency. The ongoing work
    on understanding these powerful models and how they make inferences underscore
    the growing importance of interpretability in AI, paving the way for more transparent
    AI systems. It remains to see how these concepts evolve and driving us towards
    a future that entails a safe integration of AI in our lives!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä¹‹ï¼Œ**ç¨€ç–è‡ªç¼–ç å™¨**å¯¹äºæå–å¯è§£é‡Šçš„ç‰¹å¾ã€å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§å¹¶ç¡®ä¿æ•ˆç‡è‡³å…³é‡è¦ã€‚å¯¹è¿™äº›å¼ºå¤§æ¨¡å‹çš„ç†è§£å·¥ä½œä»¥åŠå®ƒä»¬å¦‚ä½•è¿›è¡Œæ¨ç†ï¼Œçªæ˜¾äº†AIå¯è§£é‡Šæ€§æ—¥ç›Šé‡è¦çš„è¶‹åŠ¿ï¼Œä¸ºæ›´åŠ é€æ˜çš„AIç³»ç»Ÿé“ºå¹³äº†é“è·¯ã€‚æœªæ¥å¦‚ä½•å‘å±•è¿™äº›æ¦‚å¿µï¼Œå¹¶æ¨åŠ¨æˆ‘ä»¬è¿ˆå‘ä¸€ä¸ªå°†AIå®‰å…¨åœ°èå…¥æˆ‘ä»¬ç”Ÿæ´»çš„æœªæ¥ï¼Œå€¼å¾—æœŸå¾…ï¼
- en: '*P.S. If you would like to work through this exercise on your own, here is
    a link to a blank template for your use.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*é™„è¨€ï¼šå¦‚æœä½ æƒ³è‡ªå·±å®Œæˆè¿™ä¸ªç»ƒä¹ ï¼Œä¸‹é¢æœ‰ä¸€ä¸ªç©ºç™½æ¨¡æ¿çš„é“¾æ¥ä¾›ä½ ä½¿ç”¨ã€‚*'
- en: '[Blank Template for hand-exercise](https://drive.google.com/file/d/1xiAjdlWCAzhj-I-YOb7wSMeroUOQzdlE/view?usp=sharing)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[ç©ºç™½æ¨¡æ¿ç”¨äºæ‰‹å·¥ç»ƒä¹ ](https://drive.google.com/file/d/1xiAjdlWCAzhj-I-YOb7wSMeroUOQzdlE/view?usp=sharing)'
- en: Now go have fun and help Zephyr keep the Codex of Truth safe!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å»ç©å§ï¼Œå¸®åŠ©Zephyrä¿æŒã€ŠçœŸç†æ³•å…¸ã€‹çš„å®‰å…¨ï¼
- en: '![](../Images/eb16ec0901108e38f05e6bac5825305e.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb16ec0901108e38f05e6bac5825305e.png)'
- en: '*Once again special thanks to* [*Prof. Tom Yeh*](https://www.linkedin.com/in/tom-yeh/)
    *for supporting this work!*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*å†æ¬¡ç‰¹åˆ«æ„Ÿè°¢* [*Tom Yehæ•™æˆ*](https://www.linkedin.com/in/tom-yeh/) *å¯¹æœ¬é¡¹å·¥ä½œçš„æ”¯æŒï¼*'
- en: 'References:'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[1] Towards Monosemanticity: Decomposing Language Models With Dictionary Learning,
    Bricken et al. Oct 2023 [https://transformer-circuits.pub/2023/monosemantic-features/index.html](https://transformer-circuits.pub/2023/monosemantic-features/index.html)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] æœç€å•ä¹‰æ€§ï¼šé€šè¿‡å­—å…¸å­¦ä¹ åˆ†è§£è¯­è¨€æ¨¡å‹ï¼ŒBrickenç­‰äººï¼Œ2023å¹´10æœˆ [https://transformer-circuits.pub/2023/monosemantic-features/index.html](https://transformer-circuits.pub/2023/monosemantic-features/index.html)'
- en: '[2] Scaling Monosemanticity: Extracting Interpretable Features from Claude
    3 Sonnet, Templeton et al. May 2024 [https://transformer-circuits.pub/2024/scaling-monosemanticity/](https://transformer-circuits.pub/2024/scaling-monosemanticity/)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] æ‰©å±•å•ä¹‰æ€§ï¼šä»Claude 3 Sonnetä¸­æå–å¯è§£é‡Šç‰¹å¾ï¼ŒTempletonç­‰äººï¼Œ2024å¹´5æœˆ [https://transformer-circuits.pub/2024/scaling-monosemanticity/](https://transformer-circuits.pub/2024/scaling-monosemanticity/)'
