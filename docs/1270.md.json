["```py\nimport torch\nfrom tabulate import tabulate\n\nf32_type = torch.float32\nbf16_type = torch.bfloat16\ne4m3_type = torch.float8_e4m3fn\ne5m2_type = torch.float8_e5m2\n\n# collect finfo for each type\ntable = []\nfor dtype in [f32_type, bf16_type, e4m3_type, e5m2_type]:\n    numbits = 32 if dtype == f32_type else 16 if dtype == bf16_type else 8\n    info = torch.finfo(dtype)\n    table.append([info.dtype, numbits, info.max, \n                  info.min, info.smallest_normal, info.eps])\n\nheaders = ['data type', 'bits', 'max', 'min', 'smallest normal', 'eps']\nprint(tabulate(table, headers=headers))\n\n'''\nOutput:\n\ndata type      bits          max           min  smallest normal          eps\n-------------  ----  -----------  ------------  ---------------  -----------\nfloat32          32  3.40282e+38  -3.40282e+38      1.17549e-38  1.19209e-07\nbfloat16         16  3.38953e+38  -3.38953e+38      1.17549e-38    0.0078125\nfloat8_e4m3fn     8          448          -448         0.015625        0.125\nfloat8_e5m2       8        57344        -57344      6.10352e-05         0.25\n'''\n```", "```py\ndevice=\"cuda\"\ne4m3 = torch.tensor(1., device=device, dtype=e4m3_type)\ne5m2 = torch.tensor(1., device=device, dtype=e5m2_type)\n```", "```py\nx = torch.randn(2, 2, device=device, dtype=f32_type)\nx_bf16 = x.to(bf16_type)\nx_e4m3 = x.to(e4m3_type)\nx_e5m2 = x.to(e5m2_type)\n\nprint(tabulate([[‘float32’, *x.cpu().flatten().tolist()],\n                [‘bfloat16’, *x_bf16.cpu().flatten().tolist()],\n                [‘float8_e4m3fn’, *x_e4m3.cpu().flatten().tolist()],\n                [‘float8_e5m2’, *x_e5m2.cpu().flatten().tolist()]],\n               headers=[‘data type’, ‘x1’, ‘x2’, ‘x3’, ‘x4’]))\n\n'''\nThe sample output demonstrates the dynamic range of the different types:\n\ndata type                  x1              x2              x3              x4\n-------------  --------------  --------------  --------------  --------------\nfloat32        2.073093891143  -0.78251332044  -0.47084918620  -1.32557279110\nbfloat16       2.078125        -0.78125        -0.4707031      -1.328125\nfloat8_e4m3fn  2.0             -0.8125         -0.46875        -1.375\nfloat8_e5m2    2.0             -0.75           -0.5            -1.25\n-------------  --------------  --------------  --------------  --------------\n'''\n```", "```py\noutput, output_amax = torch._scaled_mm(\n        torch.randn(16,16, device=device).to(e4m3_type),\n        torch.randn(16,16, device=device).to(e4m3_type).t(),\n        bias=torch.randn(16, device=device).to(bf16_type),\n        out_dtype=e4m3_type,\n        scale_a=torch.tensor(1.0, device=device),\n        scale_b=torch.tensor(1.0, device=device)\n    )\n```", "```py\nimport torch\nfrom timm.models.vision_transformer import VisionTransformer\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport time\n\n#float8 imports\nfrom float8_experimental import config\nfrom float8_experimental.float8_linear import Float8Linear\nfrom float8_experimental.float8_linear_utils import (\n    swap_linear_with_float8_linear,\n    sync_float8_amax_and_scale_history\n)\n\n#float8 configuration (see documentation)\nconfig.enable_amax_init = False\nconfig.enable_pre_and_post_forward = False\n\n# model configuration controls:\nfp8_type = True # toggle to change floating-point precision\ncompile_model = True # toggle to enable model compilation\nbatch_size = 32 if fp8_type else 16 # control batch size\n\ndevice = torch.device('cuda')\n\n# use random data\nclass FakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n    def __getitem__(self, index):\n        rand_image = torch.randn([3, 256, 256], dtype=torch.float32)\n        label = torch.tensor(data=[index % 1024], dtype=torch.int64)\n        return rand_image, label\n\n# get data loader\ndef get_data(batch_size):\n    ds = FakeDataset()\n    return DataLoader(\n           ds,\n           batch_size=batch_size, \n           num_workers=os.cpu_count(),\n           pin_memory=True\n         )\n\n# define the timm model\ndef get_model():\n    model = VisionTransformer(\n        class_token=False,\n        global_pool=\"avg\",\n        img_size=256,\n        embed_dim=1280,\n        num_classes=1024,\n        depth=32,\n        num_heads=16\n    )\n    if fp8_type:\n        swap_linear_with_float8_linear(model, Float8Linear)\n    return model\n\n# define the training step\ndef train_step(inputs, label, model, optimizer, criterion):\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        outputs = model(inputs)\n        loss = criterion(outputs, label)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    if fp8_type:\n        sync_float8_amax_and_scale_history(model)\n    optimizer.step()\n\nmodel = get_model()\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = torch.nn.CrossEntropyLoss()\ntrain_loader = get_data(batch_size)\n\n# copy the model to the GPU\nmodel = model.to(device)\nif compile_model:\n    # compile model\n    model = torch.compile(model)\nmodel.train()\n\nt0 = time.perf_counter()\nsumm = 0\ncount = 0\n\nfor step, data in enumerate(train_loader):\n    # copy data to GPU\n    inputs = data[0].to(device=device, non_blocking=True)\n    label = data[1].squeeze(-1).to(device=device, non_blocking=True)\n\n    # train step\n    train_step(inputs, label, model, optimizer, criterion)\n\n    # capture step time\n    batch_time = time.perf_counter() - t0\n    if step > 10:  # skip first steps\n        summ += batch_time\n        count += 1\n    t0 = time.perf_counter()\n    if step > 50:\n        break\n\nprint(f'average step time: {summ / count}')\n```", "```py\nimport transformer_engine.pytorch as te\n\n# swap all linear layers with te.Linear\ndef simple_swap(model):\n    for submodule_name, submodule in model.named_modules():\n        if isinstance(submodule, torch.nn.Linear):\n            print(submodule_name)\n            path_in_state_dict = submodule_name.split('.')\n            current_module = model\n\n            # traverse to leaf module\n            leaf_path = path_in_state_dict[:-1]\n            leaf_name = path_in_state_dict[-1]\n            for child_name in leaf_path:\n                current_module = getattr(current_module, child_name)\n\n            # perform a swap\n            old_leaf = getattr(current_module, leaf_name)\n            new_leaf = te.Linear(old_leaf.in_features, \n                                 old_leaf.out_features, \n                                 old_leaf.bias is not None)\n            setattr(current_module, leaf_name, new_leaf)\n\ndef get_model():\n    model = VisionTransformer(\n        class_token=False,\n        global_pool=\"avg\",\n        img_size=256,\n        embed_dim=1280,\n        num_classes=1024,\n        depth=32,\n        num_heads=16\n    )\n    simple_swap(model)\n    return model\n\ndef train_step(inputs, label, model, optimizer, criterion):\n    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n        with te.fp8_autocast(enabled=True):\n            outputs = model(inputs)\n        loss = criterion(outputs, label)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n```"]