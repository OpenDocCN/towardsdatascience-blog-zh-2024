["```py\ntorch==2.3.0\ntransformers==4.41.2\nxformers==0.0.26.post1\nflash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.5.8/flash_attn-2.5.8+cu122torch2.3cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\naccelerate==0.31.0\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\n# This will not run on our 24GB GPU!\nmodel = SentenceTransformer(\"Alibaba-NLP/gte-Qwen2-7B-instruct\", trust_remote_code=True)\nembeddings = model.encode(list_of_examples)\n```", "```py\nimport transformers\nimport torch\n\nmodel_path = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\nmodel = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(\"cuda\")\n```", "```py\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n```", "```py\ntexts = [\"example text 1\", \"example text 2 of different length\"]\nmax_length = 32768\nbatch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n```", "```py\nwith torch.no_grad():\n    outputs = model(**batch_dict)\n    embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n```", "```py\ndef last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    # checks whether there is any padding (where attention mask = 0 for a given text)\n    no_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n    # if no padding - only would happen if batch size of 1 or all sequnces have the same length, then take the last tokens as the embeddings\n    if no_padding:\n        return last_hidden_states[:, -1]\n    # otherwise use the last non padding token for each text in the batch\n    sequence_lengths = attention_mask.sum(dim=1) - 1\n    batch_size = last_hidden_states.shape[0]\n    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengthsLetâ€™s break down what happened in the above code snippets! \n```", "```py\ntexts = [\"example text 1\", \"example text 2 of different length\"]\n```", "```py\n>>> batch_dict\n{'input_ids': tensor([[  8687,   1467,    220,     16, 151643, 151643, 151643],\n        [  8687,   1467,    220,     17,    315,   2155,   3084]],\n       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n```", "```py\n>>> batch_dict.input_ids.shape\ntorch.Size([2, 7])\n>>> batch_dict.attention_mask.shape\ntorch.Size([2, 7])\n```", "```py\n>>> outputs.last_hidden_state.shape\ntorch.Size([2, 7, 3584])\n```", "```py\n>>> attention_mask.shape[0]\n2\n>>> attention_mask[:, -1]\ntensor([0, 1], device='cuda:0')\n```", "```py\n>>> sequence_lengths = attention_mask.sum(dim=1) - 1\n>>> sequence_lengths\ntensor([3, 6], device='cuda:0')\n```", "```py\n>>> torch.arange(batch_size, device=last_hidden_states.device)\ntensor([0, 1], device='cuda:0')\n```", "```py\n>>> embeddings = last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n>>> embeddings.shape\ntorch.Size([2, 3584])\n```", "```py\nimport numpy as np\nimport numpy.typing as npt\nimport torch\nimport transformers\n\nDEVICE = torch.device(\"cuda\")\n\ndef last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    # checks whether there is any padding (where attention mask = 0 for a given text)\n    no_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n    # if no padding - only would happen if batch size of 1 or all sequnces have the same length, then take the last tokens as the embeddings\n    if no_padding:\n        return last_hidden_states[:, -1]\n    # otherwise use the last non padding token for each text in the batch\n    sequence_lengths = attention_mask.sum(dim=1) - 1\n    batch_size = last_hidden_states.shape[0]\n    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths\n\ndef encode_with_qwen_model(\n    model: transformers.PreTrainedModel,\n    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,\n    texts: list[str],\n    max_length: int = 32768,\n) -> npt.NDArray[np.float16]:\n    batch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**batch_dict)\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n    return embeddings.cpu().numpy()\n\ndef main() -> None:\n    model_path = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n    model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(DEVICE)\n    print(\"Loaded tokeniser and model\")\n\n    texts_to_encode = [\"example text 1\", \"example text 2 of different length\"]\n    embeddings = encode_with_qwen_model(model, tokenizer, texts_to_encode)\n    print(embeddings.shape)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nimport gc\n\ndef flush() -> None:\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\ndef bytes_to_giga_bytes(bytes_: float) -> float:\n    return bytes_ / 1024 / 1024 / 1024\n```", "```py\ntorch.cuda.max_memory_allocated()\n```", "```py\nimport gc\n\nimport numpy as np\nimport numpy.typing as npt\nimport torch\nimport transformers\n\nDEVICE = torch.device(\"cuda\")\n\ndef last_token_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n    # checks whether there is any padding (where attention mask = 0 for a given text)\n    left_padding = attention_mask[:, -1].sum() == attention_mask.shape[0]\n    # if no padding - only would happen if batch size of 1 or all sequences have the same length, then take the last tokens as the embeddings\n    if left_padding:\n        return last_hidden_states[:, -1]\n    # otherwise use the last non padding token for each text in the batch\n    sequence_lengths = attention_mask.sum(dim=1) - 1\n    batch_size = last_hidden_states.shape[0]\n    return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\ndef encode_with_qwen_model(\n    model: transformers.PreTrainedModel,\n    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,\n    texts: list[str] | str,\n    max_length: int = 32768,\n) -> npt.NDArray[np.float16]:\n    batch_dict = tokenizer(texts, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(**batch_dict)\n        embeddings = last_token_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n    return embeddings.cpu().numpy()\n\ndef flush() -> None:\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\ndef bytes_to_giga_bytes(bytes_: float) -> float:\n    return bytes_ / 1024 / 1024 / 1024\n\ndef memory_usage_experiments(\n    model: transformers.PreTrainedModel,\n    tokenizer: transformers.tokenization_utils.PreTrainedTokenizer | transformers.tokenization_utils_fast.PreTrainedTokenizerFast,\n) -> None:\n    model_size = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n    print(f\"Most gpu usage on model loaded: {model_size} GB\\n\")\n    sentence = \"This sentence should have minimum eight tokens. \"\n    all_texts = [sentence, sentence * 100, sentence * 1000, sentence * 2000, sentence * 3000, sentence * 4000]\n    for texts in all_texts:\n        batch_dict = tokenizer(texts, max_length=32768, padding=True, truncation=True, return_tensors=\"pt\")\n        encode_with_qwen_model(model, tokenizer, texts)\n        max_mem = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n        print(f\"Sequence length: {batch_dict.input_ids.shape[-1]}. Most gpu usage: {max_mem} GB. Effective usage: {max_mem - model_size} GB\\n\")\n        flush()\n\ndef main() -> None:\n    model_path = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n    model = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, torch_dtype=torch.float16).to(DEVICE)\n    print(\"Loaded tokeniser and model\")\n\n    memory_usage_experiments(model, tokenizer)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nMost gpu usage on model loaded: 14.958292961120605 GB\n\nSequence length: 9\\. Most gpu usage: 14.967926502227783 GB. Effective usage: 0.009633541107177734 GB\n\nSequence length: 801\\. Most gpu usage: 15.11520528793335 GB. Effective usage: 0.15691232681274414 GB\n\nSequence length: 8001\\. Most gpu usage: 16.45930576324463 GB. Effective usage: 1.5010128021240234 GB\n\nSequence length: 16001\\. Most gpu usage: 17.944651126861572 GB. Effective usage: 2.986358165740967 GB\n\nSequence length: 24001\\. Most gpu usage: 19.432421684265137 GB. Effective usage: 4.474128723144531 GB\n\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.13 GiB. GPU \n```", "```py\nmodel_path = \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\nmodel = transformers.AutoModel.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\").to(\"cuda\")\n```"]