- en: How I Streamline My Research and Presentation with LlamaIndex Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-i-streamline-my-research-and-presentation-with-llamaindex-workflows-3d75a9a10564?source=collection_archive---------3-----------------------#2024-09-10](https://towardsdatascience.com/how-i-streamline-my-research-and-presentation-with-llamaindex-workflows-3d75a9a10564?source=collection_archive---------3-----------------------#2024-09-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An example of orchestrating AI workflow with reliability, flexibility, and controllability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lzchen.cs?source=post_page---byline--3d75a9a10564--------------------------------)[![Lingzhen
    Chen](../Images/9014cbac032238d8a5c9f4708ba6ffcb.png)](https://medium.com/@lzchen.cs?source=post_page---byline--3d75a9a10564--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--3d75a9a10564--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--3d75a9a10564--------------------------------)
    [Lingzhen Chen](https://medium.com/@lzchen.cs?source=post_page---byline--3d75a9a10564--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--3d75a9a10564--------------------------------)
    ·16 min read·Sep 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'LlamaIndex recently introduced a new feature: Workflows. It’s very useful for
    those who want to create an AI solution that’s both reliable and flexible. How
    so? Because it allows you to define customized steps with a control flow. It supports
    loops, feedback, and error handling. It’s like an AI enabled pipeline. But unlike
    typical pipelines which are usually implemented as Directed Acyclic Graphs (DAG),
    workflows also enable cyclical executions, making them a good candidate for implementing
    agentic and other more complex process.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex?source=post_page-----3d75a9a10564--------------------------------)
    [## Introducing workflows beta: a new way to create complex AI applications with
    LlamaIndex …'
  prefs: []
  type: TYPE_NORMAL
- en: LlamaIndex is a simple, flexible data framework for connecting custom data sources
    to large language models (LLMs).
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.llamaindex.ai](https://www.llamaindex.ai/blog/introducing-workflows-beta-a-new-way-to-create-complex-ai-applications-with-llamaindex?source=post_page-----3d75a9a10564--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I’m going to show how I use LlamaIndex Workflows to streamline
    my process for researching the most recent advancements on a topic, and then making
    that research into a PowerPoint presentation.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to finding new research publications or papers, [ArXiv.org](http://ArXiv.org)
    is my main source. However, there are A LOT of papers on this site. As of September
    2024, there are approximately 2.5 million papers on ArXiv, including 17,000 that
    were submitted just in August (the statistics are [here](https://arxiv.org/stats/monthly_submissions)).
    Even restricted to a single topic, it’s a lot of content to read through. But
    it is not a new problem. For a long time, academic researchers had to look through
    a large amount of works for conducting their own. The rise of the large language
    model (LLM) in the last two years has presented us tools such as [ResearchGPT](https://www.researchgpt.com/),
    [papersGPT](https://jessezhang.org/llmdemo?via=topaitools), and many custom GPTs
    built for specific research purposes on the [OpenAI](https://chatgpt.com/gpts)
    platform, which aid document search, summarization, and presentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'While these tools are useful, I chose to build my own workflow using LlamaIndex
    Workflows for several key reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: I already have a specific research process, and I would like to keep it but
    have better efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I want to leverage LLMs and agentic behavior and keep control of most of the
    steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not my aim to only get a final PowerPoint presentation; I also want access
    to intermediate results to observe, fine-tune, and troubleshoot throughout the
    process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I need an all-in-one solution that handles everything end-to-end, without switching
    between different tools for tasks like summarization and slide creation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I can easily extend or modify the workflow in the future if my requirements
    evolve.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I will set up a workflow where the user gives a research topic (e.g. “*Using
    GenAI to produce power point slides*”) and will pull several papers from arxiv.org
    site and then use LLM to summarize each of them. More specifically, some key insights
    I want summarized include: type of approach, components of the model, pre-trained
    or fine-tuned method, dataset, evaluation method metrics and conclusion. The output
    of all of this would be a PowerPoint presentation with one slide per paper that
    contains key insights from the summary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before I explain how I approached implementing this workflow, it is important
    to understand two key concepts in LlamaIndex Workflows: `Event` and `Step`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Step`: Steps are the building blocks of a workflow. These are Python functions
    that represent individual components of the workflow. Each step does specific
    task, such as sending web query, getting LLM responses, or processing data. Steps
    can interact with other steps by receiving and emitting events. Steps can also
    access a shared context, which enables state management across different steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Event`: Events act as the data carriers and the flow controllers of the workflow,
    which are implemented as Pydantic objects. They control the execution path of
    the workflow, making it dynamic and flexible. Users can customize the attributes
    of events. Two special types of predefined events `StartEvent` and `StopEvent`
    controls the entry and exit points of the workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LlamaIndex offers [several notebook examples](https://docs.llamaindex.ai/en/stable/understanding/workflows/)
    and [video series](https://www.youtube.com/@LlamaIndex/videos) that cover these
    concepts in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to basic components, further, in my workflow I also made use of:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Asynchronous and parallel execution**: To increase efficiency while completing
    several items concurrently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nested workflows**: More complex hierarchy in the workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured output from LLMs**: To ensure the data is structured when flowing
    between steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Varying LLM models**: To allow for using models with different capabilities
    and inference speeds between steps (`gpt-4o` and `gpt-4o-mini`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic session for code execution**: To allow executing code in an isolated
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Individual agents at different steps**: To use specific agents for particular
    tasks within the process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the full code for this workflow on [Github](https://github.com/lz-chen/research-agent).
    To run it, you will need API keys for Tavily search, Semantic Scholar and Azure
    OpenAI (As this is implemented with Azure resources, but you can easily switch
    it to OpenAI or other models with LlamaIndex). In the following sections, I’ll
    walk through some of the key details and steps involved in building this workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The Main workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main workflow is made up of two nested sub-workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`summary_gen`: This sub-workflow finds research papers on the given topic and
    generates summaries. It carries out the searching for papers by web querying and
    uses LLM to get insights and summaries as instructed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`slide_gen`: this sub-workflow is responsible for generating a PowerPoint slide
    deck using the summaries from the previous step. It formats the slides using a
    provided PowerPoint template and generates them by creating and executing Python
    code using the `python-pptx` library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/96f12d1104e4621f16076bdf2db26ea8.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the main workflow (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The Summary Generation Sub-workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a closer look at the these sub-workflows. Firstly, the `summary_gen`
    workflow, which is pretty straightforward. It follows a simple linear process.
    It basically serves as a “data processing” workflow, with some steps sending a
    request to an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b509b343f1fef98f8c97632785d934f.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary generation workflow (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow starts by getting a user input (a research topic) and run through
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tavily_query`: Queries with the Tavily API to get academic papers related
    to the topic as a structured response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`get_paper_with_citations`: For each paper returned from the Tavily query,
    the step retrieves the paper metadata along with that of the cited paper using
    the SemanticScholar API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filter_papers`: Since not all citations retrieved are directly relevant to
    the original topic, this step refines the result. The titles and abstracts of
    each paper are sent to the LLM to assess their relevance. This step is defined
    as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here in the `process_citation()` function, we use the [FunctionCallingProgram](https://docs.llamaindex.ai/en/stable/examples/output_parsing/function_program/)
    from LlamaIndex to get a structured response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`download_papers`: This step gathers all filtered papers, prioritizes them
    based on relevance score and availability on ArXiv, and downloads the most relevant
    ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`paper2summary_dispatcher`: Each downloaded paper is prepared for summary generation
    by setting up paths for storing the images and the summaries. This step uses `self.send_event()`
    to enable the parallel execution of the `paper2summary` step for each paper. It
    also sets the number of papers in the workflow context with a variable `ctx.data[“n_pdfs”]`
    so that the later steps know how many papers they are expected to process in total.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`paper2summary`: For each paper, it converts the PDF into images, which are
    then sent to the LLM for summarization. Once the summary is generated, it is saved
    in a markdown file for future reference. Particularly, the summary generated here
    is quite elaborated, like a small article, so not quite suitable yet for putting
    directly in the presentation. But it is kept so that the user can view these intermediate
    results. In one of the later steps, we will make this information more presentable.
    The prompt provided to the LLM includes key instructions to ensure accurate and
    concise summaries:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`finish`: The workflow collects all generated summaries, verifies they are
    correctly stored, and logs the completion of the process, and return a `StopEvent`
    as a final result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If this workflow were to run independently, execution would end here. However,
    since this is just a sub-workflow of the main process, upon completion, the next
    sub-workflow — `slide_gen` — is triggered.
  prefs: []
  type: TYPE_NORMAL
- en: The Slide Generation Sub-workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This workflow generates slides based on the summaries created in the previous
    step. Here is an overview of the `slide_gen` workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/573a6a77435fb934c9b90afff1c6acce.png)'
  prefs: []
  type: TYPE_IMG
- en: Slides generation workflow (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'When the previous sub-workflow finishes, and the summary markdown files are
    ready, this workflow starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_summaries`: This step reads the content of the summary files, triggers
    a `SummaryEvent` for each file utilizing again `self.send_event()` to enable concurrent
    execution for faster processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summary2outline`: This step makes summaries into slide outline texts by using LLM.
    It shortens the summaries into sentences or bullet points for putting in the presentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gather_feedback_outline`: In this step, it presents the the user the proposed
    slide outline alongside the paper summary for them to review. The user provides
    feedback, which may trigger an `OutlineFeedbackEvent` if revisions are necessary.
    This feedback loop continues with the `summary2outline` step until the user approves
    the final outline, at which point an `OutlineOkEvent` is triggered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`outlines_with_layout`: It augments every slide outline by including page layout
    details from the given PowerPoint template, using LLM. This stage saves the content
    and design for all slide pages in a JSON file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`slide_gen`: It uses a **ReAct agent** to make slide decks based on given outlines
    and layout details. This agent has a [code interpreter tool](https://llamahub.ai/l/tools/llama-index-tools-azure-code-interpreter?from=all)
    to run and correct code in an isolated environment and a layout-checking tool
    to look at the given PowerPoint template information. The agent is prompted to
    use `python-pptx` to create the slides and can observe and fix mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`validate_slides`: Checks the slide deck to make sure it meets the given standards.
    This step involves turning the slides into images and having the LLM visually
    inspect them for correct content and consistent style according to the guidelines.
    Depending on what the LLM finds, it will either send out a `SlideValidationEvent`
    if there are problems or a `StopEvent` if everything looks good.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The criteria used for validation are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`modify_slides`: Should the slides fail the validation check, the previous
    step sends a `SlideValidationEvent` event. Here another **ReAct agent** updates
    the slides according to validator feedback, with the updated slides being saved
    and returned to be validated again. This verification loop could occur several
    times according to the `max_validation_retries` variable attributes of the `SlideGenWorkflow`
    class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To run the full workflow end-to-end, we initiate the process by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s look at an example of an intermediate summary generated for the paper
    [*LayoutGPT: Compositional Visual Planning and Generation with Large Language
    Models*](https://arxiv.org/abs/2305.15393)*:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Not surprisingly, summarization isn’t a particularly challenging task for an
    LLM. By just providing the paper as images, the LLM effectively captures all the
    key aspects outlined in the prompt and adheres to the styling instructions quite
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the final results, here are a few examples of the generated presentation
    slides:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fb32b9b194336ba795dc312262a1d0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated slides (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ca329172991fe913dde582b525f045c.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated slides (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: When filling out summary content following the layout from the template, keeping
    the text in the style of the template, putting the summarized points in bullet
    formats, and including all relevant papers needed in the slides, the workflow
    does well. These is one issue that sometimes the text in the main content placeholder
    is not resized to fit the text box. The text spill over the slide boundary. This
    type of errors can probably be fixed by using more targeted slide validation prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I showed how I use LlamaIndex Workflows for streamlining my
    research and presentation process, from querying the academic papers, to generating
    the final PowerPoint slide deck. Here are a few of my thoughts and observations,
    from implementing this workflow, as well as some potential aspects in my mind
    for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '`**gpt-4o**` **model vs** `**gpt-4o-mini**` **model**: While it is claimed
    that `gpt-4o-mini`’s performance is comparable to `gpt-4o`, I see `gpt-4o-mini`
    clearly had trouble completing complex tasks such as planning and fixing errors
    during its use as ReAct agent in the workflow. However, it did perform adequately
    in simpler tasks such as summarizing content.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating intermediate files**: Generating intermediate files (the summary
    markdown files, and summary layout JSON files) was a useful method to remove the
    burden that the agent has to keep track of the content and the style of the slide,
    while coming up with code for generating the slides.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling edge cases**: Running the workflow from end-to-end revealed many
    of edge cases, especially in validating styles of the slides specifically. Currently,
    this is now handled by modifying related prompts iteratively. But I think facilitating
    some type of collaboration and human-in-the-loop mechanisms would greatly help
    with this, also to provide a higher level of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The limitations of python-pptx**. The workflow is limited based on what python-pptx
    can actually render and manipulate in PowerPoint slides. So it is worthwhile to
    further consider other potential ways of efficient slide generation, such as using
    VBA for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Agents and Tools for Summary Generation**: Instead of a strict step-by-step
    process for summary generation, using one or multiple agents with access to tools
    (currently step functions) can allow the workflow to be more flexible and adaptable
    to future changes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhance Human-in-the-loop Interactions**. The current implementation doesn’t
    allow for many user interactions. Making the end-user more of a part of the workflow,
    especially for tasks that involve user judgment like validation and refinement
    of content can be very beneficial. One way to do it is to add more steps that the workflow can ask
    the user for validation and consider the users feedback. Involvement of a human
    is invaluable for fixing mistakes and making changes in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query engines for paper**. It is also possible to build [query engines](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/)
    of each paper so that the users can ask questions and modify the summary as they
    want. This contributes to more personalized result of the workflow.'
  prefs: []
  type: TYPE_NORMAL
- en: With all being said, LlamaIndex workflow is a very flexible and customizable
    tool for making complex and tailored AI solutions. It gave me the freedom to define
    my process with both controllability and flexibility while being able to leverage
    many built-in tools from the library.
  prefs: []
  type: TYPE_NORMAL
- en: What’s next?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, the main improvement will be to **implement more human-in-the-loop**
    type of features. For example, allowing for more interactive checkpoints where
    the user could override step executions when they need to, by incorporating the
    interactive steps into the workflow and providing the user an opportunity to check
    if the workflow is producing satisfying outputs at any stage. Consistent with
    the goal of being able to give a better user experience, it is also a good addition
    to build a **Streamlit frontend**, to provide more insights into the execution
    of the workflow. Having a frontend would also make the user experience better
    by letting user monitor the process of the workflow at real time and adjust the
    trajectory accordingly faster. In addition, getting user feedback and validation,
    visualizing the intermediate and final output would add transparency to the workflow.
    So stay tuned for the next article for these changes!😃
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! Check out my [GitHub](https://github.com/lz-chen/research-agent)
    for the complete implementation. I look forward to hearing your thoughts, input,
    and feedbacks. I work as a Data Science Consultant at [Inmeta](https://inmeta.no/),
    part of [Crayon Group](https://www.crayon.com/no/). Feel free to connect with
    me on [LinkedIn](https://www.linkedin.com/in/lingzhen-chen-76720680/).😊
  prefs: []
  type: TYPE_NORMAL
