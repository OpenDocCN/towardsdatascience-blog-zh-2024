# ä½¿ç”¨ LLM é¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®ç¬¬äºŒéƒ¨åˆ†â€”â€”å¾®è°ƒçš„æ›¿ä»£æ–¹æ³•

> åŸæ–‡ï¼š[`towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03`](https://towardsdatascience.com/predicting-metadata-for-humanitarian-datasets-with-llms-part-2-an-alternative-to-fine-tuning-953a49c657cf?source=collection_archive---------5-----------------------#2024-08-03)

[](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)![Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------) [Matthew Harris](https://medium.com/@astrobagel?source=post_page---byline--953a49c657cf--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--953a49c657cf--------------------------------) Â·29 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 8 æœˆ 3 æ—¥

--

![](img/0002f919edfe77b2a945716650c673f7.png)

æ¥æºï¼šGPT-4o

TL;DR

*åœ¨äººé“ä¸»ä¹‰å“åº”é¢†åŸŸï¼Œå¯èƒ½ä¼šæœ‰æˆåƒä¸Šä¸‡çš„è¡¨æ ¼æ•°æ®é›†ï¼ˆCSV å’Œ Excelï¼‰ï¼Œå…¶ä¸­è®¸å¤šåŒ…å«æ‹¯æ•‘ç”Ÿå‘½çš„å…³é”®ä¿¡æ¯ã€‚æ•°æ®å¯èƒ½ç”±æ•°ç™¾ä¸ªä¸åŒçš„ç»„ç»‡æä¾›ï¼Œä¸”å‘½åçº¦å®šã€è¯­è¨€å’Œæ•°æ®æ ‡å‡†å„å¼‚ï¼Œå› æ­¤äº†è§£è¡¨æ ¼ä¸­æ¯ä¸€åˆ—çš„å«ä¹‰ï¼ˆå…ƒæ•°æ®ï¼‰å¯¹äºæ‰¾åˆ°åˆé€‚çš„æ•°æ®å¹¶ç†è§£å…¶å¦‚ä½•ç»„åˆè‡³å…³é‡è¦ã€‚å¤§éƒ¨åˆ†å…ƒæ•°æ®æ˜¯æ‰‹åŠ¨è®¾ç½®çš„ï¼Œè¿™æ—¢è€—æ—¶åˆå®¹æ˜“å‡ºé”™ï¼Œå› æ­¤ä»»ä½•è‡ªåŠ¨åŒ–æ–¹æ³•éƒ½å¯èƒ½åœ¨å¸®åŠ©äººä»¬æ–¹é¢äº§ç”Ÿå®é™…å½±å“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬é‡æ–°å®¡è§†äº†å…ˆå‰çš„åˆ†æâ€œ*[*ä½¿ç”¨ GPT-3 é¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®*](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)*â€ï¼Œä»¥äº†è§£è¿‡å» 18 ä¸ªæœˆçš„è¿›å±•å¦‚ä½•ä¸ºæ›´é«˜æ•ˆã€èŠ‚çœæ—¶é—´çš„æ–¹æ³•é“ºå¹³é“è·¯ï¼Œç”¨äºè®¾ç½®è¡¨æ ¼æ•°æ®çš„å…ƒæ•°æ®ã€‚*â€

*é€šè¿‡ä½¿ç”¨å¸¦æœ‰å…ƒæ•°æ®æ ‡ç­¾çš„ CSV å’Œ Excel æ•°æ®é›†ï¼Œæ¥è‡ªäº* [*äººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°*](https://data.humdata.org/) *(HDX)ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å¾®è°ƒ GPT-4o-mini åœ¨é¢„æµ‹* [*äººé“ä¸»ä¹‰äº¤æ¢è¯­è¨€*](https://hxlstandard.org/) *(HXL)æ ‡ç­¾å’Œå±æ€§æ—¶çš„è‰¯å¥½æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºä¸ä½ç½®å’Œæ—¥æœŸç›¸å…³çš„æœ€å¸¸è§æ ‡ç­¾ã€‚ç„¶è€Œï¼Œå¯¹äºé‚£äº›è¾ƒå°‘å‡ºç°çš„æ ‡ç­¾å’Œå±æ€§ï¼Œè¿™ä¸€æŠ€æœ¯å¯èƒ½ä¼šå—åˆ°è®­ç»ƒæ•°æ®è´¨é‡ä¸ä½³çš„é™åˆ¶ï¼ŒåŸå› åœ¨äºäººå·¥æ ‡ç­¾é”™è¯¯æˆ–äººä»¬æ²¡æœ‰ä½¿ç”¨æ‰€æœ‰å¯èƒ½çš„ HXL å…ƒæ•°æ®ç»„åˆã€‚å®ƒè¿˜å­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œå³å½“å…ƒæ•°æ®æ ‡å‡†å‘ç”Ÿå˜åŒ–æ—¶ï¼Œå®ƒæ— æ³•è¿›è¡Œè°ƒæ•´ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸ä¼šåæ˜ è¿™äº›å˜åŒ–ã€‚*

*é‰´äºç°åœ¨æœ‰äº†æ›´å¼ºå¤§çš„ LLM å¯ç”¨ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸€ç§æŠ€æœ¯ï¼Œç›´æ¥æç¤º GPT-4o æˆ– GPT-4o-miniï¼Œè€Œä¸æ˜¯è¿›è¡Œå¾®è°ƒï¼Œåœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›å®Œæ•´çš„ HXL æ ¸å¿ƒæ¶æ„å®šä¹‰ï¼Œå› ä¸ºç°åœ¨å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ã€‚äº‹å®è¯æ˜ï¼Œå½“ä½¿ç”¨ GPT-4o æ—¶ï¼Œè¿™ç§æ–¹æ³•æ¯”å¾®è°ƒæ›´å‡†ç¡®ï¼Œèƒ½å¤Ÿæ”¯æŒè¾ƒå°‘è§çš„ HXL æ ‡ç­¾å’Œå±æ€§ï¼Œå¹¶ä¸”ä¸éœ€è¦å®šåˆ¶çš„è®­ç»ƒæ•°æ®ï¼Œä½¿å¾—ç®¡ç†å’Œéƒ¨ç½²æ›´åŠ ç®€ä¾¿ã€‚ç„¶è€Œï¼Œå®ƒçš„æˆæœ¬è¾ƒé«˜ï¼Œä½†å¦‚æœä½¿ç”¨ GPT-4o-miniï¼Œåˆ™æˆæœ¬è¾ƒä½ï¼Œå°½ç®¡æ€§èƒ½ç•¥æœ‰ä¸‹é™ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„ Python ç±»ï¼Œä½äº* [*GitHub Gist*](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014) *ï¼Œå¯ä»¥åœ¨æ•°æ®å¤„ç†ç®¡é“ä¸­è‡ªåŠ¨ä¸ºè¡¨æ ¼æ•°æ®é›†æ·»åŠ  HXL å…ƒæ•°æ®æ ‡ç­¾å’Œå±æ€§ã€‚*

# ç”Ÿæˆæ€§ AI å‘å±•å¾—**éå¸¸**è¿…é€Ÿï¼

å¤§çº¦ 18 ä¸ªæœˆå‰ï¼Œæˆ‘å†™äº†ä¸€ç¯‡åšå®¢æ–‡ç«  [ä½¿ç”¨ GPT-3 é¢„æµ‹äººé“ä¸»ä¹‰æ•°æ®é›†çš„å…ƒæ•°æ®](https://medium.com/towards-data-science/predicting-metadata-for-humanitarian-datasets-using-gpt-3-b104be17716d)ã€‚

æ²¡é”™ï¼Œæ˜¯ä½¿ç”¨ GPT-3ï¼Œä¸æ˜¯ GPT-3.5ï¼ğŸ™‚

å³ä¾¿å¦‚æ­¤ï¼Œæ—©åœ¨é‚£æ—¶ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¾®è°ƒå°±å·²åœ¨é¢„æµ‹[äººé“ä¸»ä¹‰äº¤æ¢è¯­è¨€](https://hxlstandard.org/)ï¼ˆHXLï¼‰å…ƒæ•°æ®å­—æ®µæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå°¤å…¶æ˜¯åœ¨ä»¤äººæƒŠå¹çš„[äººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°](https://data.humdata.org/)ï¼ˆHDXï¼‰ä¸Šçš„è¡¨æ ¼æ•°æ®é›†ã€‚åœ¨é‚£é¡¹ç ”ç©¶ä¸­ï¼Œè®­ç»ƒæ•°æ®ä»£è¡¨äº† HDX ä¸Šçš„ HXL æ•°æ®åˆ†å¸ƒï¼Œå› æ­¤åŒ…å«äº†ä¸ä½ç½®å’Œæ—¥æœŸç›¸å…³çš„æœ€å¸¸è§æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾å¯¹äºå°†ä¸åŒæ•°æ®é›†æŒ‰ä½ç½®å’Œæ—¶é—´å…³è”èµ·æ¥è‡³å…³é‡è¦ï¼Œè¿™æ˜¯åˆ©ç”¨æ•°æ®ä¼˜åŒ–äººé“ä¸»ä¹‰å“åº”çš„ä¸€ä¸ªå…³é”®å› ç´ ã€‚

LLM é¢†åŸŸæ­¤åå·²å–å¾—äº†â€¦ **å·¨å¤§çš„**è¿›å±•ã€‚

å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é‡æ–°å®¡è§†è¿™ä¸€æŠ€æœ¯ï¼Œå°†å…¶æ‰©å±•åˆ°æ¶µç›–ä¸å¤ªå¸¸è§çš„ HXL æ ‡ç­¾å’Œå±æ€§ï¼Œå¹¶æ¢è®¨ç›®å‰å¯ç”¨çš„å…¶ä»–é€‰é¡¹ï¼Œé€‚ç”¨äºéœ€è¦å°†å¤æ‚ã€é«˜å±‚æ¬¡çš„åˆ†ç±»æ³•åº”ç”¨äºæ•°æ®çš„æƒ…å†µã€‚æˆ‘ä»¬è¿˜å°†æ¢è®¨é¢„æµ‹å½“å‰åœ¨äººä¸ºæ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«çš„è¾ƒå°‘è§çš„ HXL æ ‡å‡†æ ‡ç­¾å’Œå±æ€§çš„èƒ½åŠ›ã€‚

# è®¾ç½®

ä½ å¯ä»¥é€šè¿‡åœ¨[Google Colab](https://colab.research.google.com/)ä¸­æ‰“å¼€è¿™äº›ç¬”è®°æœ¬ï¼Œæˆ–è€…åœ¨æœ¬åœ°è¿è¡Œå®ƒä»¬æ¥è·Ÿéšæœ¬æ¬¡åˆ†æï¼š

+   [generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb) â€” ç”¨äºåˆ›å»ºæµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†çš„ç¬”è®°æœ¬

+   [openai-hxl-prediction.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/openai-hxl-prediction.ipynb) â€” æ¢ç´¢å¾®è°ƒå’Œæç¤ºä»¥é¢„æµ‹ HXL æ•°æ®é›†çš„ç¬”è®°æœ¬

è¯·å‚é˜…ä»“åº“ä¸­çš„[README](https://github.com/datakind/hxl-metadata-prediction/blob/main/README.md)è·å–å®‰è£…è¯´æ˜ã€‚

# æ¥è‡ªäººé“ä¸»ä¹‰æ•°æ®äº¤æ¢å¹³å°çš„ HXL æ•°æ®

å¯¹äºæœ¬ç ”ç©¶ï¼Œåœ¨ HDX å›¢é˜Ÿçš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä» HDX å¹³å°æå–çš„æ•°æ®ï¼Œé€šè¿‡ä»–ä»¬è¿è¡Œçš„çˆ¬è™«è¿‡ç¨‹è·Ÿè¸ªå¹³å°ä¸Š HXL å…ƒæ•°æ®æ ‡ç­¾å’Œå±æ€§çš„ä½¿ç”¨æƒ…å†µã€‚ä½ å¯ä»¥åœ¨[GitHub](https://github.com/HXLStandard)æ‰¾åˆ°å¾ˆæ£’çš„ HXL èµ„æºï¼Œä½†å¦‚æœä½ æƒ³è·Ÿéšæœ¬æ¬¡åˆ†æï¼Œæˆ‘ä¹Ÿå°†æºæ•°æ®ä¿å­˜åˆ°äº† Google Driveï¼Œå› ä¸ºçˆ¬è™«éœ€è¦å‡ å¤©æ—¶é—´æ‰èƒ½å¤„ç† HDX ä¸Šæˆåƒä¸Šä¸‡çš„è¡¨æ ¼æ•°æ®é›†ã€‚

æ•°æ®å¦‚ä¸‹æ‰€ç¤ºï¼Œæ¯ä¸ª HXL æ ‡ç­¾åŒ–çš„è¡¨æ ¼åˆ—ä¸ºä¸€è¡Œâ€¦

![](img/1d1abdc669b37dc43cb697223ff9d3f7.png)

æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„æ•°æ®ç¤ºä¾‹ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªè¡¨æ ¼æ•°æ®åˆ—ã€‚

# æ ¸å¿ƒ HXL æ¶æ„

[HXL æ˜ä¿¡ç‰‡](https://hxlstandard.org/standard/1-1final/postcards/)æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¦‚è¿°ï¼Œå±•ç¤ºäº†æ ¸å¿ƒæ¶æ„ä¸­æœ€å¸¸è§çš„ HXL æ ‡ç­¾å’Œå±æ€§ã€‚å¯¹äºæˆ‘ä»¬çš„åˆ†æï¼Œæˆ‘ä»¬å°†åº”ç”¨[HDX](https://data.humdata.org/dataset/hxl-core-schemas)ä¸Šæä¾›çš„å®Œæ•´æ ‡å‡†ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªæ”¯æŒçš„æ ‡ç­¾å’Œå±æ€§çš„[ç”µå­è¡¨æ ¼](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)â€¦

![](img/c72d8b2f6bce80bae6e654de56ba7af0.png)

æœ¬ç ”ç©¶ä¸­ä½¿ç”¨çš„â€œæ ¸å¿ƒ HXL æ¶æ„â€æ‘˜å½•ï¼Œæ¥æºäº[Humanitarian Data Exchange](https://data.humdata.org/dataset/hxl-core-schemas)

# æ•°æ®å¤„ç†

[generate-test-train-data.ipynb](https://github.com/datakind/hxl-metadata-prediction/blob/main/generate-test-train-data.ipynb)ç¬”è®°æœ¬æä¾›äº†åˆ›å»ºæµ‹è¯•å’Œè®­ç»ƒæ•°æ®é›†çš„æ‰€æœ‰æ­¥éª¤ï¼Œä½†è¿™é‡Œæœ‰ä¸€äº›è¦æ³¨æ„çš„å…³é”®ç‚¹ï¼š

**1\. åˆ é™¤è‡ªåŠ¨åŒ–ç®¡é“é‡å¤çš„ HXL æ•°æ®**

åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘é€šè¿‡ä½¿ç”¨æ¯ä¸ªè¡¨æ ¼æ•°æ®é›†ï¼ˆCSV å’Œ Excel æ–‡ä»¶ï¼‰ä¸­åˆ—åç§°çš„ MDF å“ˆå¸Œï¼Œåˆ é™¤ç”±è‡ªåŠ¨åŒ–ç®¡é“ä¸Šä¼ åˆ° HDX çš„æ•°æ®ä¸­çš„é‡å¤é¡¹ã€‚ä¾‹å¦‚ï¼ŒæŸä¸ªç»„ç»‡åˆ›å»ºçš„äººå£ç»Ÿè®¡ CSV æ–‡ä»¶é€šå¸¸ä¸æ¯ä¸ªç‰¹å®šå›½å®¶çš„ CSV æˆ– Excel æ–‡ä»¶éå¸¸ç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬åªä¿ç•™ä¸€ä¸ªç¤ºä¾‹ã€‚è¿™å¯¹æ•°æ®èµ·åˆ°äº†å¹³è¡¡ä½œç”¨ï¼Œé€šè¿‡åˆ é™¤éå¸¸ç›¸ä¼¼çš„é‡å¤æ•°æ®ï¼Œæä¾›äº†æ›´å¤šçš„ HXL æ ‡ç­¾å’Œå±æ€§å˜å¼‚æ€§ã€‚

**2\. é™åˆ¶æ•°æ®ä¸ºæœ‰æ•ˆçš„ HXL æ ¼å¼**

å¤§çº¦ 50% çš„å¸¦æœ‰ HXL æ ‡ç­¾çš„ HDX æ•°æ®ä½¿ç”¨äº†åœ¨[HXL æ ¸å¿ƒæ¶æ„](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?usp=sharing)ä¸­æœªæŒ‡å®šçš„æ ‡ç­¾æˆ–å±æ€§ï¼Œå› æ­¤è¿™äº›æ•°æ®ä¼šè¢«ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç§»é™¤ã€‚

**3\. æ•°æ®å¢å¼º**

ä½œä¸ºä¸€ä¸ªï¼ˆå¤§éƒ¨åˆ†æ˜¯ï¼ï¼‰äººç±»ï¼Œåœ¨å†³å®šåœ¨æŸä¸€åˆ—ä½¿ç”¨å“ªäº› HXL æ ‡ç­¾å’Œå±æ€§æ—¶ï¼Œæˆ‘ä¼šæŸ¥çœ‹è¯¥åˆ—çš„æ•°æ®ï¼Œä¹Ÿä¼šæŸ¥çœ‹è¡¨æ ¼ä¸­æ‰€æœ‰æ•°æ®ã€‚å¯¹äºè¿™ä¸ªåˆ†æï¼Œæˆ‘ä»¬ä¹Ÿå¯¹ LLM å¾®è°ƒå’Œæç¤ºæ•°æ®åšåŒæ ·çš„å¤„ç†ï¼Œæ·»åŠ æ¯ä¸€åˆ—çš„æ•°æ®æ‘˜å½•ã€‚è¿˜ä½¿ç”¨ LLMï¼ˆGPT-3.5-Turboï¼‰å¯¹æ•°æ®çš„æ‘˜è¦æ¥ä¸ºè¡¨æ ¼æ·»åŠ æè¿°ï¼Œä½¿å®ƒä»¬ä¸€è‡´ï¼Œå› ä¸º HDX ä¸Šçš„æ‘˜è¦æ ¼å¼å„ä¸ç›¸åŒï¼Œå¯èƒ½æ˜¯å‡ é¡µï¼Œä¹Ÿå¯èƒ½æ˜¯å‡ å¥è¯ã€‚

**4\. ä»”ç»†åˆ’åˆ†æ•°æ®ä»¥åˆ›å»ºè®­ç»ƒ/æµ‹è¯•é›†**

è®¸å¤šæœºå™¨å­¦ä¹ ç®¡é“é€šè¿‡éšæœºåˆ’åˆ†æ•°æ®æ¥åˆ›å»ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚ç„¶è€Œï¼Œå¯¹äº HDX æ•°æ®ï¼Œè¿™æ ·åšä¼šå¯¼è‡´æ¥è‡ªåŒä¸€ç»„ç»‡çš„åˆ—å’Œæ–‡ä»¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ã€‚æˆ‘è®¤ä¸ºè¿™ç§æ–¹å¼å¯¹é¢„æµ‹æµ‹è¯•æ¥è¯´æœ‰ç‚¹å¤ªç®€å•äº†ï¼Œå› æ­¤æˆ‘é€‰æ‹©æŒ‰ç»„ç»‡åˆ’åˆ†æ•°æ®ï¼Œç¡®ä¿æµ‹è¯•é›†ä¸­çš„ç»„ç»‡ä¸å‡ºç°åœ¨è®­ç»ƒæ•°æ®ä¸­ã€‚æ­¤å¤–ï¼ŒåŒä¸€æ¯å…¬å¸ä¸‹çš„å­å…¬å¸â€”â€”ä¾‹å¦‚â€œocha-iraqâ€å’Œâ€œocha-libyaâ€â€”â€”ä¹Ÿä¸èƒ½åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ï¼Œä»¥ä½¿é¢„æµ‹æ›´åŠ çœŸå®ã€‚æˆ‘çš„ç›®æ ‡æ˜¯æµ‹è¯•é¢„æµ‹ï¼Œå‡è®¾è¿™äº›ç»„ç»‡çš„æ•°æ®ä»æœªè¢«è§è¿‡ã€‚

åœ¨å®Œæˆä¸Šè¿°æ‰€æœ‰æ­¥éª¤å¹¶è¿›è¡Œé™é‡‡æ ·ä»¥èŠ‚çœæˆæœ¬åï¼Œæˆ‘ä»¬å¾—åˆ°äº†**2,883**è¡Œè®­ç»ƒé›†æ•°æ®å’Œ**485**è¡Œæµ‹è¯•é›†æ•°æ®ã€‚

# åˆ›å»º JSONL å¾®è°ƒæç¤ºæ–‡ä»¶

åœ¨æˆ‘åŸæ¥çš„æ–‡ç« ä¸­ï¼Œæˆ‘é€‰æ‹©ä½¿ç”¨ä¸€ä¸ªå®Œæˆæ¨¡å‹ï¼Œä½†éšç€[GPT-4o-mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)çš„å‘å¸ƒï¼Œæˆ‘æ”¹ä¸ºç”Ÿæˆé€‚åˆå¾®è°ƒ*èŠå¤©*æ¨¡å‹çš„æç¤ºï¼ˆæœ‰å…³å¯ç”¨æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[æ­¤å¤„](https://platform.openai.com/docs/guides/fine-tuning/which-models-can-be-fine-tuned)ï¼‰ã€‚

æ¯ä¸ªæç¤ºçš„æ ¼å¼æ˜¯â€¦

```py
{
  "messages": [
    {
      "role": "system", 
      "content": "<SYSTEM PROMPT>"
    }, 
    {
      "role": "user", 
      "content": "<INPUT PROMPT>"
    }, 
    {
      "role": "assistant", 
      "content": "<EXPECTED OUTPUT>"
    }
  ]
} 
```

æ³¨æ„ï¼šä¸Šè¿°å†…å®¹å·²æ ¼å¼åŒ–ä»¥ä¾¿æ¸…æ™°é˜…è¯»ï¼Œä½† JSONL ä¸­æ¯æ¡è®°å½•ä¼šåœ¨ä¸€è¡Œå†…æ˜¾ç¤ºã€‚

åˆ©ç”¨æ•°æ®æ‘˜å½•ã€LLM ç”Ÿæˆçš„è¡¨æ ¼æè¿°ä»¥åŠæˆ‘ä»¬æ”¶é›†çš„åˆ—åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç”Ÿæˆç±»ä¼¼äºè¿™æ ·çš„æç¤ºâ€¦

```py
{
  "messages": [
    {
      "role": "system", 
      "content": "You are an assistant that replies with HXL tags and attributes"
    }, 
    {
      "role": "user", 
      "content": "What are the HXL tags and attributes for a column with these details? 
                    resource_name='admin1-summaries-earthquake.csv'; 
                    dataset_description='The dataset contains earthquake data for various 
                                         administrative regions in Afghanistan, 
                                         including country name, admin1 name, latitude, 
                                         longitude, aggregation type, indicator name, 
                                         and indicator value. The data includes maximum 
                                         earthquake values recorded in different regions, 
                                         with corresponding latitude and longitude coordinates. 
                                         The dataset provides insights into the seismic 
                                         activity in different administrative areas of 
                                         Afghanistan.'; 
                   column_name:'indicator'; 
                   examples: ['earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake', 'earthquake']"
      }, 
      {
        "role": "assistant", 
        "content": "#indicator+name"
      }
  ]
}
```

# å¾®è°ƒ GPT-4o-mini

æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†é€‚åˆå¾®è°ƒ OpenAI èŠå¤©æ¨¡å‹çš„æµ‹è¯•å’Œè®­ç»ƒæ–‡ä»¶æ ¼å¼ï¼Œå› æ­¤è®©æˆ‘ä»¬å¼€å§‹å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹â€¦

```py
def fine_tune_model(train_file, model_name="gpt-4o-mini"):
    """
    Fine-tune an OpenAI model using training data.

    Args:
        prompt_file (str): The file containing the prompts to use for fine-tuning.
        model_name (str): The name of the model to fine-tune. Default is "davinci-002".

    Returns:
        str: The ID of the fine-tuned model.
    """

    # Upload file to OpenAI for fine-tuning
    file = client.files.create(
        file=open(train_file, "rb"),
        purpose="fine-tune"
    )
    file_id = file.id
    print(f"Uploaded training file with ID: {file_id}")

    # Start the fine-tuning job
    ft = client.fine_tuning.jobs.create(
        training_file=file_id,
        model=model_name
    )
    ft_id = ft.id
    print(f"Fine-tuning job started with ID: {ft_id}")

    # Monitor the status of the fine-tuning job
    ft_result = client.fine_tuning.jobs.retrieve(ft_id)
    while ft_result.status != 'succeeded':
        print(f"Current status: {ft_result.status}")
        time.sleep(120)  # Wait for 60 seconds before checking again
        ft_result = client.fine_tuning.jobs.retrieve(ft_id)
        if 'failed' in ft_result.status.lower():
            sys.exit()

    print(f"Fine-tuning job {ft_id} succeeded!")

    # Retrieve the fine-tuned model
    fine_tuned_model = ft_result.fine_tuned_model
    print(f"Fine-tuned model: {fine_tuned_model}")

    return fine_tuned_model

model = fine_tune_model("hxl_chat_prompts_train.jsonl", model_name="gpt-4o-mini-2024-07-18")
```

åœ¨ä¸Šé¢ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯æ–°çš„[GPT-4-mini æ¨¡å‹](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)ï¼Œè¯¥æ¨¡å‹ç›®å‰ç”± OpenAI æä¾›å…è´¹å¾®è°ƒæœåŠ¡â€¦

> â€œç°åœ¨åˆ° 9 æœˆ 23 æ—¥ï¼ŒGPT-4o mini å¯ä»¥å…è´¹è°ƒä¼˜ï¼Œæœ€å¤šè¾¾åˆ°æ¯æ—¥ 2M è®­ç»ƒä»¤ç‰Œçš„é™åˆ¶ã€‚è¶…è¿‡ 2M è®­ç»ƒä»¤ç‰Œçš„éƒ¨åˆ†å°†æŒ‰$3.00/ç™¾ä¸‡ä»¤ç‰Œæ”¶è´¹ã€‚ä» 9 æœˆ 24 æ—¥èµ·ï¼Œè°ƒä¼˜è®­ç»ƒå°†æ”¶è´¹$3.00/ç™¾ä¸‡ä»¤ç‰Œã€‚æŸ¥çœ‹[è°ƒä¼˜æ–‡æ¡£](http://url3243.email.openai.com/ls/click?upn=u001.IQLfsj4kk-2BK7JhymNusRMkuuWNTB2xtKMTOzsaHXXCxL87wc9xXN3T3-2B7A50MnxBgM-2FSPU6KI18qmN7e0qEq7w-3D-3DYSY8_HWAk4DGcP5bOseprwmP7vlMwrd2PVXgyuPjLpW3O5VwKbv89B-2BC2CHyio6JopT7iV9PDDQbS-2BN2x-2FOMYyECPpE2WpDWUaqXamxCNxLNFb3Rwb-2BHV-2FnmELwjcwafGYmpXvFXZ3a1UDAGj-2FI8RPRJ92m05wFP91cNzwWmQw2EWFsPrLyLakbHisdbOdu-2B4S0ScKBkmbmuJc7Ib-2Ftz7vKHoD5rdIHoytDF68pW1ivyzpO5isDzndxqHjHSEoXNrAMaOs0RnmRsG-2Btwq2onQS1WmIokXr00y08IHtcHQMGB8k2caZ5qZ1FzXlQ7tM-2F42kCwNCt4-2BmFy-2Bt8mm9-2BtTS6Qd9pEf9tpuFFcI14VFgdiiUINrbkZX-2BvxRqD924FparfXWICjMx3q6U3F78-2B0okeN23HKQddDiZ9ufm5tITBwbvTYG4vXxKkrvM1fg-2BY-2FSI1Zgu7AMY95FNOKhHZjjVYIXSEFJh5oN0U3K3ceVerRfgU0o1sp8yLH-2F4yaMjmyNjp9gAL5CiSYfTqIx0hHAETq3DyTWqiJMx5Fpsg8sAiqHj3Dgwqj5hydZgeMopCnrf3Cfo7Uf09kxixficprhjJLtC-2BOYDB9QH3AyxBxKCpKupl026DU1bx7HoE0Rcytak3Zy6lolc6PczWAxmgGmi8bkEWsMxj8VS-2BhSSPF7qHIr0a-2BP020bgEng-2BZL0HUgfiJpig0i4DhENBp-2BQokwZMcgMdFpOhJVou0cF-2BcxDprFi2U2xhrxn5es5vY0TTwpQjqAhs-2BoK-2FZpbE0zkuyQ9tTtlInaU26DOBv1RHaiFTN-2F8GTEHoxvkJ1OHhhds3ATTWUCGwOhUOZ-2Fl5JjWzYdCDPeOgqnxlQd8b1i-2BJuaBRnhUjpQ7TzPnWkCur4qMtI-2BYKM3tD2d0RxTYTYfQ3GoNsZ-2FBo5Mf4Rb3lKQt59vxsLqKYe33qRjeFo12Ke3dS20gxD7Zxtpu57q1z0xuMgwj9uDDqrPTZh9qbUDYGc1IsbRhOAjL5z4kAYR2jGvTi2SFq9f2AiA1swOO3CORlZpwn5Y6BA-3D-3D)ä»¥è·å–æ›´å¤šæœ‰å…³å…è´¹è®¿é—®çš„è¯¦ç»†ä¿¡æ¯ã€‚â€

å³ä½¿æŒ‰$3.00/ç™¾ä¸‡ä¸ªä»¤ç‰Œè®¡ç®—ï¼Œå¯¹äºè¿™ä¸ªä»»åŠ¡æ¥è¯´ï¼Œæˆæœ¬ä¹Ÿç›¸å½“ä½ï¼Œæ•´ä¸ªè°ƒä¼˜è¿‡ç¨‹å¤§çº¦éœ€è¦ 7 ç¾å…ƒï¼Œæµ‹è¯•æ–‡ä»¶ä¸­æœ‰è¶…è¿‡ 200 ä¸‡ä¸ªä»¤ç‰Œã€‚éœ€è¦è®°ä½çš„æ˜¯ï¼Œå¯¹äºè¿™ä¸ªç‰¹å®šä»»åŠ¡ï¼Œè°ƒä¼˜åº”è¯¥æ˜¯ä¸€ä¸ªå°‘è§çš„äº‹ä»¶ï¼Œä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰è¿™æ ·çš„æ¨¡å‹ï¼Œå®ƒå¯ä»¥è¢«é‡å¤ä½¿ç”¨ã€‚

è°ƒä¼˜äº§ç”Ÿäº†ä»¥ä¸‹è¾“å‡ºâ€¦â€¦

```py
Uploaded training file with ID: file-XXXXXXXXXXXXXXX
Fine-tuning job started with ID: ftjob-XXXXXXXXXXXXXXX
Current status: validating_files
Current status: validating_files
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Current status: running
Fine-tuning job ftjob-XXXXXXXXXXXXXXX succeeded!
Fine-tuned model: ft:gpt-4o-mini-2024-07-18::XXXXXXX
```

èŠ±äº†å¤§çº¦ 45 åˆ†é’Ÿã€‚

# æµ‹è¯•æˆ‘ä»¬è°ƒä¼˜åçš„æ¨¡å‹ä»¥é¢„æµ‹ HXL

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç²¾å¿ƒè°ƒä¼˜çš„æ–°æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹ HXL æ ‡ç­¾å’Œå±æ€§ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æµ‹è¯•æ–‡ä»¶æ¥è¿›è¡Œæµ‹è¯•â€¦â€¦

```py
def make_chat_predictions(prompts, model, temperature=0.1, max_tokens=13):
    """
    Generate chat predictions based on given prompts using the OpenAI chat model.

    Args:
        prompts (list): A list of prompts, where each prompt is a dictionary containing a list of messages.
                        Each message in the list has a 'role' (either 'system', 'user', or 'assistant') and 'content'.
        model (str): The name or ID of the OpenAI chat model to use for predictions.
        temperature (float, optional): Controls the randomness of the predictions. Higher values (e.g., 0.5) make the
                                       output more random, while lower values (e.g., 0.1) make it more deterministic.
                                       Defaults to 0.1.
        max_tokens (int, optional): The maximum number of tokens in the predicted response. Defaults to 13.

    Returns:
        pandas.DataFrame: A DataFrame containing the results of the chat predictions. Each row in the DataFrame
                          corresponds to a prompt and includes the prompt messages, the actual message, and the
                          predicted message.

    """
    results = []
    for p in prompts:
        actual = p["messages"][-1]["content"]
        p["messages"] = p["messages"][0:2]
        completion = client.chat.completions.create(
            model=model,
            messages=p["messages"],
            temperature=temperature,
            max_tokens=max_tokens
        )
        predicted = completion.choices[0].message.content
        predicted = filter_for_schema(predicted)

        res = {
            "prompt": p["messages"],
            "actual": actual,
            "predicted": predicted
        }

        print(f"Predicted: {predicted}; Actual: {actual}")

        results.append(res)

    results = pd.DataFrame(results)

    return results

def filter_for_schema(text):
    """
    Filters the input text to extract approved HXL schema tokens.

    Args:
        text (str): The input text to be filtered.

    Returns:
        str: The filtered text containing only approved HXL schema tokens.
    """

    if " " in text:
        text = text.replace(" ","")

    tokens_raw = text.split("+")
    tokens = [tokens_raw[0]]
    for t in tokens_raw[1:]:
        tokens.append(f"+{t}")

    filtered = []
    for t in tokens:
        if t in APPROVED_HXL_SCHEMA:
            if t not in filtered:
                filtered.append(t)
    filtered = "".join(filtered)

    if len(filtered) > 0 and filtered[0] != '#':
        filtered = ""

    return filtered

def output_prediction_metrics(results, prediction_field="predicted", actual_field="actual"):
    """
    Prints out model performance report for HXL tag prediction. Metrics are for
    just predicting tags, as well as predicting tags and attributes.

    Parameters
    ----------
    results : dataframe
        Dataframe of results
    prediction_field : str
        Field name of element with prediction. Handy for comparing raw and post-processed predictions.
    actual_field: str
        Field name of the actual result for comparison with prediction
    """
    y_test = []
    y_pred = []
    y_justtag_test = []
    y_justtag_pred = []
    for index, r in results.iterrows():
        if actual_field not in r and predicted_field not in r:
            print("Provided results do not contain expected values.")
            sys.exit()
        y_pred.append(r[prediction_field])
        y_test.append(r[actual_field])
        actual_tag = r[actual_field].split("+")[0]
        predicted_tag = r[prediction_field].split("+")[0]
        y_justtag_test.append(actual_tag)
        y_justtag_pred.append(predicted_tag)

    print(f"LLM results for {prediction_field}, {len(results)} predictions ...")
    print("\nJust HXL tags ...\n")
    print(f"Accuracy: {round(accuracy_score(y_justtag_test, y_justtag_pred),2)}")
    print(
        f"Precision: {round(precision_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"
    )
    print(
        f"Recall: {round(recall_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"
    )
    print(
        f"F1: {round(f1_score(y_justtag_test, y_justtag_pred, average='weighted', zero_division=0),2)}"
    )

    print(f"\nTags and attributes with {prediction_field} ...\n")
    print(f"Accuracy: {round(accuracy_score(y_test, y_pred),2)}")
    print(
        f"Precision: {round(precision_score(y_test, y_pred, average='weighted', zero_division=0),2)}"
    )
    print(
        f"Recall: {round(recall_score(y_test, y_pred, average='weighted', zero_division=0),2)}"
    )
    print(
        f"F1: {round(f1_score(y_test, y_pred, average='weighted', zero_division=0),2)}"
    )

    return

with open(TEST_FILE) as f:
    X_test = [json.loads(line) for line in f]

results = make_chat_predictions(X_test, model)

output_prediction_metrics(results)

print("Done")
```

ä¸Šè¿°å†…å®¹ä¸­éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰€æœ‰é¢„æµ‹éƒ½ç»è¿‡äº† HXL æ ‡å‡†ä¸­å®šä¹‰çš„å…è®¸æ ‡ç­¾å’Œå±æ€§çš„ç­›é€‰ã€‚

è¿™ç»™å‡ºäº†ä»¥ä¸‹ç»“æœâ€¦â€¦

```py
LLM results for predicted, 458 predictions ...

Just HXL tags ...

Accuracy: 0.83
Precision: 0.85
Recall: 0.83
F1: 0.82

Tags and attributes with predicted ...

Accuracy: 0.61
Precision: 0.6
Recall: 0.61
F1: 0.57 
```

â€œä»… HXL æ ‡ç­¾â€æ˜¯æŒ‡é¢„æµ‹ HXL çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œä¾‹å¦‚ï¼Œå¦‚æœå®Œæ•´çš„ HXL æ˜¯#affected+infected+fï¼Œæ¨¡å‹æ­£ç¡®é¢„æµ‹äº†#affected éƒ¨åˆ†ã€‚â€œæ ‡ç­¾å’Œå±æ€§â€æ˜¯æŒ‡é¢„æµ‹å®Œæ•´çš„ HXL å­—ç¬¦ä¸²ï¼Œå³â€˜#affected+infected+fâ€™ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå­˜åœ¨è®¸å¤šå¯èƒ½çš„ç»„åˆã€‚

æ€§èƒ½å¹¶ä¸å®Œç¾ï¼Œä½†ä¹Ÿä¸ç®—å¤ªå·®ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬å¹³è¡¡äº†æ•°æ®é›†ï¼Œå‡å°‘äº†ä½ç½®å’Œæ—¥æœŸæ ‡ç­¾åŠå±æ€§çš„æ•°é‡ï¼ˆå³è®©è¿™ä¸ªç ”ç©¶ç¨å¾®æ›´å…·æŒ‘æˆ˜æ€§ï¼‰ã€‚å³ä½¿å¦‚æ­¤ï¼Œä»ç„¶æœ‰æˆåƒä¸Šä¸‡çš„äººé“ä¸»ä¹‰å“åº”è¡¨æ ¼æ²¡æœ‰ HDXï¼Œå³ä½¿æ˜¯ä¸Šè¿°æ€§èƒ½ä¹Ÿå¯èƒ½å¸¦æ¥ä»·å€¼ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹é¢„æµ‹ä¸äººå·¥æ ‡æ³¨æ•°æ®ä¸ä¸€è‡´çš„æ¡ˆä¾‹â€¦â€¦

# å®¡æŸ¥äººç±»æ ‡æ³¨çš„ HXL æ•°æ®

é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°ç”µå­è¡¨æ ¼ä¸­ï¼Œæˆ‘æ‰‹åŠ¨æŸ¥çœ‹äº†å¤§å¤šæ•°ä¸æ ‡ç­¾ä¸ä¸€è‡´çš„é¢„æµ‹ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://docs.google.com/spreadsheets/d/19BfVEU4hQJYUrliRKzfu5rXagK8CjoDH/edit?usp=sharing&ouid=107814789436940136200&rtpof=true&sd=true)æ‰¾åˆ°è¿™é¡¹åˆ†æï¼Œå¹¶åœ¨ä¸‹æ–‡è¿›è¡Œæ€»ç»“â€¦â€¦

![](img/dfbd71d99ab9103a6ad4e7e1673cdcbb.png)

æœ‰è¶£çš„æ˜¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒLLM å®é™…ä¸Šæ˜¯æ­£ç¡®çš„ï¼Œä¾‹å¦‚åœ¨æ·»åŠ *é¢å¤–çš„*HXL å±æ€§æ—¶ï¼Œè€Œè¿™äº›å±æ€§åœ¨äººå·¥æ ‡æ³¨çš„æ•°æ®ä¸­æ²¡æœ‰åŒ…å«ã€‚ä¹Ÿæœ‰ä¸€äº›æƒ…å†µä¸‹ï¼Œäººå·¥æ ‡æ³¨çš„ HXL å®Œå…¨åˆç†ï¼Œä½† LLM é¢„æµ‹äº†å¦ä¸€ä¸ªæ ‡ç­¾æˆ–å±æ€§ï¼Œè¿™ä¸ªæ ‡ç­¾æˆ–å±æ€§ä¹Ÿå¯ä»¥è¢«è§£é‡Šä¸ºæ­£ç¡®ã€‚ä¾‹å¦‚ï¼Œåœ¨æŸäº›å›½å®¶ï¼Œ#region ä¹Ÿå¯ä»¥æ˜¯#admin1ï¼Œè€ŒæŸäº›æƒ…å†µä¸‹åˆ¤æ–­ä¸€ä¸ªæ˜¯+id è¿˜æ˜¯+code ä¹Ÿå¾ˆéš¾å†³å®šï¼Œä¸¤è€…éƒ½æ˜¯åˆé€‚çš„ã€‚

ä½¿ç”¨ä¸Šè¿°ç±»åˆ«ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„æµ‹è¯•é›†ï¼Œå…¶ä¸­çº æ­£äº†æœŸæœ›çš„ HXL æ ‡ç­¾ã€‚åœ¨é‡æ–°è¿è¡Œé¢„æµ‹åï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ”¹è¿›çš„ç»“æœâ€¦â€¦

```py
 Just HXL tags ...

Accuracy: 0.88
Precision: 0.88
Recall: 0.88
F1: 0.88

Tags and attributes with predicted ...

Accuracy: 0.66
Precision: 0.71
Recall: 0.66
F1: 0.66
```

# åœ¨æ²¡æœ‰å¾®è°ƒçš„æƒ…å†µä¸‹é¢„æµ‹ HXLï¼Œè€Œä»…ä»…æ˜¯é€šè¿‡æç¤ºæ¥ä½¿ç”¨ GPT-4o

ä¸Šè¿°å†…å®¹è¡¨æ˜äººç±»æ ‡æ³¨çš„æ•°æ®æœ¬èº«å¯èƒ½æ˜¯é”™è¯¯çš„ã€‚HXL æ ‡å‡†è®¾è®¡å¾—éå¸¸å‡ºè‰²ï¼Œä½†å¯¹äºå¼€å‘äººå‘˜å’Œæ•°æ®ç§‘å­¦å®¶æ¥è¯´ï¼Œåœ¨æ•°æ®ä¸Šè®¾ç½® HXL æ ‡ç­¾å’Œå±æ€§æ—¶ï¼Œè®°ä½å®ƒä»¬å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚HXL å›¢é˜Ÿå·²ç»æä¾›äº†ä¸€äº›[ä»¤äººæƒŠå¹çš„å·¥å…·](https://hxlstandard.org/tools/)ï¼Œä½†æœ‰æ—¶ HXL ä»ç„¶æ˜¯é”™è¯¯çš„ã€‚è¿™ç»™ä¾èµ–è¿™äº›äººç±»æ ‡æ³¨æ•°æ®è¿›è¡Œè®­ç»ƒçš„å¾®è°ƒæ–¹æ³•å¸¦æ¥äº†é—®é¢˜ï¼Œå°¤å…¶æ˜¯å¯¹äºé‚£äº›äººç±»ä¸å¸¸ä½¿ç”¨çš„æ ‡ç­¾å’Œå±æ€§ï¼Œè¿™äº›æ ‡ç­¾å’Œå±æ€§çš„è¡¨ç¤ºè¾ƒå°‘ã€‚å®ƒè¿˜å­˜åœ¨ä¸€ä¸ªé™åˆ¶ï¼Œå³æ— æ³•åœ¨å…ƒæ•°æ®æ ‡å‡†å‘ç”Ÿå˜åŒ–æ—¶è¿›è¡Œè°ƒæ•´ï¼Œå› ä¸ºè®­ç»ƒæ•°æ®ä¸ä¼šåæ˜ è¿™äº›å˜åŒ–ã€‚

è‡ª 18 ä¸ªæœˆå‰çš„åˆæ­¥åˆ†æä»¥æ¥ï¼Œå„ä¸ª LLM æä¾›å•†å·²ç»æ˜¾è‘—æé«˜äº†ä»–ä»¬çš„æ¨¡å‹ã€‚OpenAI å½“ç„¶å‘å¸ƒäº†ä»–ä»¬çš„æ——èˆ°äº§å“[GPT-4o](https://openai.com/index/hello-gpt-4o/)ï¼Œå…¶å…·æœ‰ 128k ä¸ª token çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå¦å¤–ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œè¡¨æ˜åŸºç¡€æ¨¡å‹çš„æˆæœ¬æ­£åœ¨ä¸‹é™ï¼ˆä¾‹å¦‚ï¼ŒGPT-4-Turbo ä¸ GPT-4o çš„æ¯”è¾ƒ[è§æ­¤](https://huggingface.co/spaces/philschmid/llm-pricing)ï¼‰ã€‚è€ƒè™‘åˆ°è¿™äº›å› ç´ ï¼Œæˆ‘å¼€å§‹æ€è€ƒâ€¦â€¦

***å¦‚æœæ¨¡å‹å˜å¾—æ›´å¼ºå¤§ä¸”ä½¿ç”¨æˆæœ¬æ›´ä½ï¼Œæˆ‘ä»¬æ˜¯å¦å¯ä»¥å®Œå…¨é¿å…å¾®è°ƒï¼Œä»…é€šè¿‡æç¤ºå°±èƒ½é¢„æµ‹ HXL æ ‡ç­¾å’Œå±æ€§ï¼Ÿ***

è¿™ä¸ä»…æ„å‘³ç€å‡å°‘æ¸…ç†æ•°æ®å’Œå¾®è°ƒæ¨¡å‹çš„å·¥ç¨‹å·¥ä½œï¼Œè¿˜å¯èƒ½å…·æœ‰ä¸€ä¸ªå·¨å¤§ä¼˜åŠ¿ï¼Œå³èƒ½å¤ŸåŒ…æ‹¬äººç±»æ ‡æ³¨çš„è®­ç»ƒæ•°æ®ä¸­æœªåŒ…å«ä½†å±äº HXL æ ‡å‡†çš„ HXL æ ‡ç­¾å’Œå±æ€§ã€‚è¿™æ˜¯å¼ºå¤§ LLM çš„ä¸€ä¸ªæ½œåœ¨å·¨å¤§ä¼˜åŠ¿ï¼Œå¯ä»¥é€šè¿‡é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æç¤ºè¿›è¡Œåˆ†ç±»ã€‚

# ä¸ºé¢„æµ‹ HXL åˆ›å»ºæç¤º

åƒ GPT-4o è¿™æ ·çš„æ¨¡å‹æ˜¯åŸºäºç½‘é¡µæ•°æ®è®­ç»ƒçš„ï¼Œæ‰€ä»¥æˆ‘æƒ³å…ˆåšä¸€ä¸ªæµ‹è¯•ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æç¤ºä¹‹ä¸€æ¥çœ‹çœ‹å®ƒæ˜¯å¦å·²ç»çŸ¥é“å…³äº HXL æ ‡ç­¾çš„æ‰€æœ‰ä¿¡æ¯â€¦

![](img/b39b540c5990d5719eae62d8834191e4.png)

æˆ‘ä»¬çœ‹åˆ°çš„æ˜¯ï¼Œå®ƒä¼¼ä¹çŸ¥é“ HXL çš„è¯­æ³•ï¼Œä½†ç­”æ¡ˆä¸æ­£ç¡®ï¼ˆæ­£ç¡®ç­”æ¡ˆæ˜¯â€˜#affected+infectedâ€™ï¼‰ï¼Œå¹¶ä¸”é€‰æ‹©äº†ä¸åœ¨ HXL æ ‡å‡†ä¸­çš„æ ‡ç­¾å’Œå±æ€§ã€‚è¿™å®é™…ä¸Šç±»ä¼¼äºæˆ‘ä»¬åœ¨äººç±»æ ‡æ³¨çš„ HXL ä¸­çœ‹åˆ°çš„æƒ…å†µã€‚

å¦‚æœæˆ‘ä»¬åœ¨ç³»ç»Ÿæç¤ºä¸­æä¾›[HXL æ ‡å‡†](https://docs.google.com/spreadsheets/d/1En9FlmM8PrbTWgl3UHPF_MXnJ6ziVZFhBbojSJzBdLI/edit?pli=1&gid=319251406#gid=319251406)çš„æœ€é‡è¦éƒ¨åˆ†æ€ä¹ˆæ ·ï¼Ÿ

```py
def generate_hxl_standard_prompt(local_data_file):
  """
  Generate a standard prompt for predicting Humanitarian Markup Language (HXL) tags and attributes.

  Args:
    local_data_file (str): The path to the local data file containing core hashtags and attributes.

  Returns:
    str: The generated HXL standard prompt.

  """

  core_hashtags = pd.read_excel(local_data_file, sheet_name='Core hashtags')
  core_hashtags = core_hashtags.loc[core_hashtags["Release status"] == "Released"]
  core_hashtags = core_hashtags[["Hashtag", "Hashtag long description", "Sample HXL"]]

  core_attributes = pd.read_excel(local_data_file, sheet_name='Core attributes')
  core_attributes = core_attributes.loc[core_attributes["Status"] == "Released"]
  core_attributes = core_attributes[["Attribute", "Attribute long description", "Suggested hashtags (selected)"]]

  print(core_hashtags.shape)
  print(core_attributes.shape)

  core_hashtags = core_hashtags.to_dict(orient='records')
  core_attributes = core_attributes.to_dict(orient='records')

  hxl_prompt= f"""
  You are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:

  CORE HASHTAGS:

  {json.dumps(core_hashtags,indent=4)}

  CORE ATTRIBUTES:

  {json.dumps(core_attributes, indent=4)}

  Key points:

  - ALWAYS predict hash tags
  - NEVER predict a tag which is not a valid core hashtag
  - NEVER start with a core hashtag, you must always start with a core hashtag
  - Always try and predict an attribute if possible
  - Do not use attribute +code if the data examples are human readable names

  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.

  """

  print(len(hxl_prompt.split(" ")))
  print(hxl_prompt)
  return hxl_prompt
```

è¿™ç»™äº†æˆ‘ä»¬å¦‚ä¸‹çš„æç¤ºâ€¦

```py
You are an AI assistant that predicts Humanitarian Markup Language (HXL) tags and attributes for columns of data where the HXL standard is defined as follows:

  CORE HASHTAGS:

  [
    {
        "Hashtag": "#access",
        "Hashtag long description": "Accessiblity and constraints on access to a market, distribution point, facility, etc.",
        "Sample HXL": "#access +type"
    },
    {
        "Hashtag": "#activity",
        "Hashtag long description": "A programme, project, or other activity. This hashtag applies to all levels; use the attributes +activity, +project, or +programme to distinguish different hierarchical levels.",
        "Sample HXL": "#activity +project"
    },
    {
        "Hashtag": "#adm1",
        "Hashtag long description": "Top-level subnational administrative area (e.g. a governorate in Syria).",
        "Sample HXL": "#adm1 +code"
    },
    {
        "Hashtag": "#adm2",
        "Hashtag long description": "Second-level subnational administrative area (e.g. a subdivision in Bangladesh).",
        "Sample HXL": "#adm2 +name"
    },
    {
        "Hashtag": "#adm3",
        "Hashtag long description": "Third-level subnational administrative area (e.g. a subdistrict in Afghanistan).",
        "Sample HXL": "#adm3 +code"
    },
    {
        "Hashtag": "#adm4",
        "Hashtag long description": "Fourth-level subnational administrative area (e.g. a barangay in the Philippines).",
        "Sample HXL": "#adm4 +name"
    },
    {
        "Hashtag": "#adm5",
        "Hashtag long description": "Fifth-level subnational administrative area (e.g. a ward of a city).",
        "Sample HXL": "#adm5 +code"
    },
    {
        "Hashtag": "#affected",
        "Hashtag long description": "Number of people or households affected by an emergency. Subset of #population; superset of #inneed.",
        "Sample HXL": "#affected +f +children"
    },
    {
        "Hashtag": "#beneficiary",
        "Hashtag long description": "General (non-numeric) information about a person or group meant to benefit from aid activities, e.g. \"lactating women\".",
        "Sample HXL": "#beneficiary +name"
    },
    {
        "Hashtag": "#capacity",
        "Hashtag long description": "The response capacity of the entity being described (e.g. \"25 beds\").",
        "Sample HXL": "#capacity +num"
    },

... Truncated for brevity

    },
    {
        "Hashtag": "#targeted",
        "Hashtag long description": "Number of people or households targeted for humanitarian assistance. Subset of #inneed; superset of #reached.",
        "Sample HXL": "#targeted +f +adult"
    },
    {
        "Hashtag": "#value",
        "Hashtag long description": "A monetary value, such as the price of goods in a market, a project budget, or the amount of cash transferred to beneficiaries. May be used together with #currency in financial or cash data.",
        "Sample HXL": "#value +transfer"
    }
]

  CORE ATTRIBUTES:

  [
    {
        "Attribute": "+abducted",
        "Attribute long description": "Hashtag refers to people who have been abducted.",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached"
    },
    {
        "Attribute": "+activity",
        "Attribute long description": "The implementers classify this activity as an \"activity\" proper  (may imply different hierarchical levels in different contexts).",
        "Suggested hashtags (selected)": "#activity"
    },
    {
        "Attribute": "+adolescents",
        "Attribute long description": "Adolescents, loosely defined (precise age range varies); may overlap +children and +adult.  You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+adolescents +age12_17\".",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"
    },
    {
        "Attribute": "+adults",
        "Attribute long description": "Adults, loosely defined (precise age range varies); may overlap +adolescents and +elderly. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+adults +age18_64\".",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"
    },
    {
        "Attribute": "+approved",
        "Attribute long description": "Date or time when something was approved.",
        "Suggested hashtags (selected)": "#date"
    },
    {
        "Attribute": "+bounds",
        "Attribute long description": "Boundary data (e.g. inline GeoJSON).",
        "Suggested hashtags (selected)": "#geo"
    },
    {
        "Attribute": "+budget",
        "Attribute long description": "Used with #value to indicate that the amount is planned/approved/budgeted rather than actually spent.",
        "Suggested hashtags (selected)": "#value"
    },
    {
        "Attribute": "+canceled",
        "Attribute long description": "Date or time when something (e.g. an #activity) was canceled.",
        "Suggested hashtags (selected)": "#date"
    },
    {
        "Attribute": "+children",
        "Attribute long description": "The associated hashtag applies to non-adults, loosely defined (precise age range varies; may overlap +infants and +adolescents). You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+children +age3_11\".",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"
    },
    {
        "Attribute": "+cluster",
        "Attribute long description": "Identifies a sector as a formal IASC humanitarian cluster.",
        "Suggested hashtags (selected)": "#sector"
    },
    {
        "Attribute": "+code",
        "Attribute long description": "A unique, machine-readable code.",
        "Suggested hashtags (selected)": "#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc, #beneficiary, #activity, #org, #sector, #subsector, #indicator, #output, #crisis, #cause, #impact, #severity, #service, #need, #currency, #item, #need, #service, #channel, #modality, #event, #group, #status"
    },
    {
        "Attribute": "+converted",
        "Attribute long description": "Date or time used for converting a monetary value to another currency.",
        "Suggested hashtags (selected)": "#date"
    },
    {
        "Attribute": "+coord",
        "Attribute long description": "Geodetic coordinates (lat+lon together).",
        "Suggested hashtags (selected)": "#geo"
    },
    {
        "Attribute": "+dest",
        "Attribute long description": "Place of destination (intended or actual).",
        "Suggested hashtags (selected)": "#region, #country, #adm1, #adm2, #adm3, #adm4, #adm5, #loc"
    },
    {
        "Attribute": "+displaced",
        "Attribute long description": "Displaced people or households. Refers to all types of displacement: use +idps or +refugees to be more specific.",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"
    },
    {
        "Attribute": "+elderly",
        "Attribute long description": "Elderly people, loosely defined (precise age range varies). May overlap +adults. You can optionally create custom attributes in addition to this to add precise age ranges, e.g. \"+elderly +age65plus\".",
        "Suggested hashtags (selected)": "#affected, #inneed, #targeted, #reached, #population"
    },

... Truncated for brevity

    {
        "Attribute": "+url",
        "Attribute long description": "The data consists of web links related to the main hashtag (e.g. for an #org, #service, #activity, #loc, etc).",
        "Suggested hashtags (selected)": "#contact, #org, #activity, #service, #meta"
    },
    {
        "Attribute": "+used",
        "Attribute long description": "Refers to a #service, #item, etc. that affected people have actually consumed or otherwise taken advantage of.",
        "Suggested hashtags (selected)": "#service, #item"
    }
]

  Key points:

  - ALWAYS predict hash tags
  - NEVER predict a tag which is not a valid core hashtag
  - NEVER start with a core hashtag, you must always start with a core hashtag
  - Always try and predict an attribute if possible

  You must return your result as a JSON record with the fields 'predicted' and 'reasoning', each is of type string.
```

å®ƒç›¸å½“é•¿ï¼ˆä¸Šé¢å·²è¢«æˆªæ–­ï¼‰ï¼Œä½†åŒ…å«äº† HXL æ ‡å‡†çš„è¦ç‚¹ã€‚

ç›´æ¥æç¤ºæ–¹æ³•çš„å¦ä¸€ä¸ªä¼˜åŠ¿æ˜¯ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¦æ±‚ LLM åœ¨é¢„æµ‹ HXL æ—¶æä¾›å…¶æ¨ç†è¿‡ç¨‹ã€‚å½“ç„¶ï¼Œè¿™å¯èƒ½åŒ…æ‹¬å¹»è§‰ï¼Œä½†æˆ‘å‘ç°å®ƒå¯¹äºä¼˜åŒ–æç¤ºéå¸¸æœ‰å¸®åŠ©ã€‚

å¯¹äºç”¨æˆ·æç¤ºï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸å¾®è°ƒæ—¶ç›¸åŒçš„ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ‘˜å½•å’Œ LLM ç”Ÿæˆçš„è¡¨æ ¼æ€»ç»“â€¦

```py
What are the HXL tags and attributes for a column with these details? resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC Appeals Data for South Sudan8.csv'; 
   dataset_description='The dataset contains information on various 
                        appeals and events related to South Sudan, 
                        including details such as the type of appeal, 
                        status, sector, amount requested and funded, 
                        start and end dates, as well as country-specific 
                        information like country code, region, and average 
                        household size. The data includes appeals for 
                        different crises such as floods, population 
                        movements, cholera outbreaks, and Ebola preparedness, 
                        with details on beneficiaries and confirmation needs. 
                        The dataset also includes metadata such as IDs, 
                        names, and translation modules for countries and regions.'; 
   column_name:'aid'; 
   examples: ['18401', '17770', '17721', '16858', '15268', '15113', '14826', '14230', '12788', '9286', '8561']
```

å°†æ‰€æœ‰å†…å®¹ç»¼åˆèµ·æ¥ï¼Œå¹¶åŒæ—¶å¯¹æ¯” GPT-4o-mini å’Œ GPT-4o çš„æç¤ºç»“æœâ€¦

```py
def call_gpt(prompt, system_prompt, model, temperature, top_p, max_tokens):
    """
    Calls the GPT model to generate a response based on the given prompt and system prompt.

    Args:
        prompt (str): The user's input prompt.
        system_prompt (str): The system's input prompt.
        model (str): The name or ID of the GPT model to use.
        temperature (float): Controls the randomness of the generated output. Higher values (e.g., 0.8) make the output more random, while lower values (e.g., 0.2) make it more deterministic.
        top_p (float): Controls the diversity of the generated output. Higher values (e.g., 0.8) make the output more diverse, while lower values (e.g., 0.2) make it more focused.
        max_tokens (int): The maximum number of tokens to generate in the response.

    Returns:
        dict or None: The generated response as a dictionary object, or None if an error occurred during generation.
    """
    response = client.chat.completions.create(
        model=model,
        messages= [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        max_tokens=2000,
        temperature=temperature,
        top_p=top_p,
        frequency_penalty=0,
        presence_penalty=0,
        stop=None,
        stream=False,
        response_format={ "type": "json_object" }
    )

    result = response.choices[0].message.content
    result = result.replace("```json","").replace("```py","")
    try:
        result = json.loads(result)
        result["predicted"] = result["predicted"].replace(" ","")
    except:
        print(result)
        result = None
    return result

def make_prompt_predictions(prompts, model, temperature=0.1, top_p=0.1, \
                            max_tokens=2000, debug=False, actual_field="actual"):
    """
    Generate predictions for a given set of prompts using the specified model.

    Args:
        prompts (pandas.DataFrame): A DataFrame containing the prompts to generate predictions for.
        model (str): The name of the model to use for prediction.
        temperature (float, optional): The temperature parameter for the model's sampling. Defaults to 0.1.
        top_p (float, optional): The top-p parameter for the model's sampling. Defaults to 0.1.
        max_tokens (int, optional): The maximum number of tokens to generate for each prompt. Defaults to 2000.
        debug (bool, optional): Whether to print debug information during prediction. Defaults to False.
        actual_field (str, optional): The name of the column in the prompts DataFrame that contains the actual values. Defaults to "actual".

    Returns:
        pandas.DataFrame: A DataFrame containing the results of the predictions, including the prompt, actual value, predicted value, and reasoning.

    """

    num_prompts = len(prompts)
    print(f"Number of prompts: {num_prompts}")

    results = []
    for index, p in prompts.iterrows():

        if index % 50 == 0:
            print(f"{index/num_prompts*100:.2f}% complete")

        prompt = p["prompt"]
        prompt = ast.literal_eval(prompt)
        prompt = prompt[1]["content"]
        actual = p[actual_field]

        result = call_gpt(prompt, hxl_prompt, model, temperature, top_p, max_tokens)

        if result is None:
            print("    !!!!! No LLM result")
            predicted = ""
            reasoning = ""
        else:
            predicted = result["predicted"]
            reasoning = result["reasoning"]

        if debug is True:
            print(f"Actual: {actual}; Predicted: {predicted}; Reasoning: {reasoning}")

        results.append({
            "prompt": prompt,
            "actual": actual,
            "predicted": predicted,
            "reasoning": reasoning
        })

    results = pd.DataFrame(results)

    print(f"\n\n===================== {model} Results =========================\n\n")
    output_prediction_metrics(results)
    print(f"\n\n=================================================================")

    results["match"] = results['predicted'] == results['actual']
    results.to_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-{model}-results.xlsx", index=False)

    return results

for model in ["gpt-4o-mini","gpt-4o"]:
  print(f"Model: {model}")
  results = make_prompt_predictions(X_test, model, temperature=0.1, top_p=0.1, max_tokens=2000)
```

æˆ‘ä»¬å¾—åˆ°â€¦

```py
===================== gpt-4o-mini Results =========================

LLM results for predicted, 458 predictions ...

Just HXL tags ...

Accuracy: 0.77
Precision: 0.83
Recall: 0.77
F1: 0.77

Tags and attributes with predicted ...

Accuracy: 0.53
Precision: 0.54
Recall: 0.53
F1: 0.5

===================== gpt-4o Results =========================

LLM results for predicted, 458 predictions ...

Just HXL tags ...

Accuracy: 0.86
Precision: 0.86
Recall: 0.86
F1: 0.85

Tags and attributes with predicted ...

Accuracy: 0.71
Precision: 0.7
Recall: 0.71
F1: 0.69

=================================================================
```

æé†’ä¸€ä¸‹ï¼Œå¾®è°ƒåçš„æ¨¡å‹äº§ç”Ÿäº†ä»¥ä¸‹ç»“æœâ€¦

```py
Just HXL tags ...

Accuracy: 0.83
Precision: 0.85
Recall: 0.83
F1: 0.82

Tags and attributes with predicted ...

Accuracy: 0.61
Precision: 0.6
Recall: 0.61
F1: 0.57
```

*ä»…ä½¿ç”¨æç¤ºçš„ GPT-4o ä¸ GPT-4o-mini ç›¸æ¯”å¦‚ä½•ï¼Ÿ*

ä»ä¸Šé¢çš„æ•°æ®æ¥çœ‹ï¼Œæˆ‘ä»¬å‘ç° GPT-4o-mini ä»…ä½¿ç”¨æç¤ºé¢„æµ‹æ ‡ç­¾çš„å‡†ç¡®ç‡ä¸º 77%ï¼Œä½äº GPT-4o-mini å¾®è°ƒåçš„ 83%å’Œ GPT-4o ä»…ä½¿ç”¨æç¤ºçš„ 86%ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ€§èƒ½ä»ç„¶ä¸é”™ï¼Œå³ä¾¿ç›´æ¥ä½¿ç”¨ä¹Ÿèƒ½æ”¹å–„ HXL è¦†ç›–ç‡ã€‚

*ä»…ä½¿ç”¨æç¤ºä¸å¾®è°ƒæ¨¡å‹çš„å¯¹æ¯”å¦‚ä½•ï¼Ÿ*

GPT-4o ä»…ä½¿ç”¨æç¤ºçš„ç»“æœæ˜¯æ‰€æœ‰æ¨¡å‹ä¸­æœ€å¥½çš„ï¼Œåœ¨æ ‡ç­¾ä¸Šçš„å‡†ç¡®ç‡ä¸º 86%ï¼Œåœ¨æ ‡ç­¾å’Œå±æ€§ä¸Šçš„å‡†ç¡®ç‡ä¸º 71%ã€‚å®é™…ä¸Šï¼Œç»è¿‡æ›´å¤šå¯¹æµ‹è¯•æ•°æ®çš„åˆ†æä»¥çº æ­£é”™è¯¯çš„äººç±»æ ‡ç­¾åï¼Œæ€§èƒ½å¯èƒ½ä¼šæ›´å¥½ã€‚

è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ GPT-4o å‡ºé”™çš„æƒ…å†µâ€¦

```py
df = pd.read_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-prompting-only-prediction-gpt-4o-results.xlsx")

breaks = df[df["match"]==False]
print(breaks.shape)

for index, row in breaks.iterrows():
  print("\n======================================== ")
  pprint.pp(f"\nPrompt: {row['prompt']}")
  print()
  print(f"Actual", row["actual"])
  print(f"Predicted", row["predicted"])
  print()
  pprint.pp(f'Reasoning: \n{row["reasoning"]}')
```

```py
'\n'
 'Prompt: What are the HXL tags and attributes for a column with these '
 'details? '
 "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "
 "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "
 'contains information on various appeals and events related to South Sudan, '
 'including details such as the type of appeal, status, sector, amount '
 'requested and funded, start and end dates, as well as country-specific '
 'information like country code, region, and average household size. The data '
 'includes appeals for different crises such as floods, population movements, '
 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '
 'confirmation needs. The dataset also includes metadata such as IDs, names, '
 "and translation modules for countries and regions.'; column_name:'dtype.id'; "
 "examples: ['12', '5', '1', '1', '12', '12', '1', '6', '1', '1', '7']")

Actual #cause+id
Predicted #meta+id

('Reasoning: \n'
 "The column 'dtype.id' contains numeric identifiers (e.g., '12', '5', '1') "
 'which are likely to be internal identifiers for data records. According to '
 'the HXL standard, the appropriate hashtag for internal identifiers is '
 "'#meta' with the attribute '+id'.")

======================================== 
('\n'
 'Prompt: What are the HXL tags and attributes for a column with these '
 'details? '
 "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "
 "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "
 'contains information on various appeals and events related to South Sudan, '
 'including details such as the type of appeal, status, sector, amount '
 'requested and funded, start and end dates, as well as country-specific '
 'information like country code, region, and average household size. The data '
 'includes appeals for different crises such as floods, population movements, '
 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '
 'confirmation needs. The dataset also includes metadata such as IDs, names, '
 "and translation modules for countries and regions.'; "
 "column_name:'dtype.name'; examples: ['Flood', 'Population Movement', "
 "'Epidemic', 'Epidemic', 'Flood', 'Flood', 'Epidemic', 'Complex Emergency', "
 "'Epidemic', 'Epidemic', 'Civil Unrest']")

Actual #cause+name
Predicted #event+type

('Reasoning: \n'
 "The examples provided in the column ('Flood', 'Population Movement', "
 "'Epidemic', 'Complex Emergency', 'Civil Unrest') describe different types of "
 'events or incidents within a crisis or emergency. According to the HXL '
 'standard, the appropriate hashtag for this type of data is #event, and the '
 'attribute +type is used to specify the type or category of the event.')

======================================== 
('\n'
 'Prompt: What are the HXL tags and attributes for a column with these '
 'details? '
 "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "
 "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "
 'contains information on various appeals and events related to South Sudan, '
 'including details such as the type of appeal, status, sector, amount '
 'requested and funded, start and end dates, as well as country-specific '
 'information like country code, region, and average household size. The data '
 'includes appeals for different crises such as floods, population movements, '
 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '
 'confirmation needs. The dataset also includes metadata such as IDs, names, '
 "and translation modules for countries and regions.'; "
 "column_name:'status_display'; examples: ['Active', 'Active', 'Closed', "
 "'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', 'Closed', "
 "'Closed']")

Actual #status+name
Predicted #status+code

('Reasoning: \n'
 "The column 'status_display' contains values such as 'Active' and 'Closed', "
 'which describe the status of appeals or events. The appropriate HXL hashtag '
 'for project or activity status is #status. Since the values are categorical '
 'and represent different statuses, the attribute +code is suitable to '
 'indicate these status codes.')

======================================== 
('\n'
 'Prompt: What are the HXL tags and attributes for a column with these '
 'details? '
 "resource_name='/content/drive/MyDrive/Colab/hxl-metadata-prediction/data/IFRC "
 "Appeals Data for South Sudan8.csv'; dataset_description='The dataset "
 'contains information on various appeals and events related to South Sudan, '
 'including details such as the type of appeal, status, sector, amount '
 'requested and funded, start and end dates, as well as country-specific '
 'information like country code, region, and average household size. The data '
 'includes appeals for different crises such as floods, population movements, '
 'cholera outbreaks, and Ebola preparedness, with details on beneficiaries and '
 'confirmation needs. The dataset also includes metadata such as IDs, names, '
 "and translation modules for countries and regions.'; "
 "column_name:'region.id'; examples: ['0', '0', '0', '0', '0', '0', '0', '0', "
 "'0', '0', '0']")

Actual #adm1+code
Predicted #region+id

('Reasoning: \n'
 "The column 'region.id' contains numeric identifiers for regions, which "
 'aligns with the HXL tag #region and the attribute +id. The examples provided '
 'are all numeric, indicating that these are likely unique identifiers for '
 'regions.')

======================================== 
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰äº†ä¸€ä¸ªâ€œæ¨ç†â€å­—æ®µï¼Œç”¨æ¥è¯´æ˜ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æ ‡ç­¾ã€‚è¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œå¹¶ä¸”å¯¹äºä¼˜åŒ–æç¤ºä»¥æé«˜æ€§èƒ½æ˜¯ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚

ä»ä¸Šé¢çš„ç¤ºä¾‹æ¥çœ‹ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸€äº›ç†Ÿæ‚‰çš„åœºæ™¯ï¼Œè¿™äº›åœºæ™¯å‡ºç°åœ¨åˆ†æå¾®è°ƒæ¨¡å‹å¤±è´¥çš„é¢„æµ‹æ—¶â€¦

+   +id å’Œ+code çš„æ¨¡ç³Šæ€§

+   #region å’Œ#adm1 äº’æ¢ä½¿ç”¨

+   #event ä¸æ›´è¯¦ç»†çš„æ ‡ç­¾å¦‚#cause çš„å¯¹æ¯”

è¿™äº›ä¼¼ä¹å±äºé‚£ç§æ ¹æ® HXL å®šä¹‰ï¼Œç»™å®šåˆ—å¯èƒ½æœ‰ä¸¤ä¸ªæ ‡ç­¾çš„ç±»åˆ«ã€‚ä½†ä¹Ÿæœ‰ä¸€äº›æ˜æ˜¾çš„ä¸ä¸€è‡´ä¹‹å¤„ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œä½¿ç”¨ GPT-4o é¢„æµ‹ HXL æ ‡ç­¾å’Œå±æ€§å¾—å‡ºäº†æœ€å¥½çš„ç»“æœï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯åœ¨å¯æ¥å—çš„æ°´å¹³ï¼Œå› ä¸ºå¾ˆå¤šæ•°æ®å®Œå…¨ç¼ºå¤± HXL å…ƒæ•°æ®ï¼Œä¸”è®¸å¤šåŒ…å«è¿™äº›æ•°æ®çš„é›†åˆæœ‰é”™è¯¯çš„æ ‡ç­¾å’Œå±æ€§ã€‚

# æˆæœ¬æ¯”è¾ƒ

è®©æˆ‘ä»¬çœ‹çœ‹æ¯ç§æŠ€æœ¯å’Œæ¨¡å‹çš„æˆæœ¬æ¯”è¾ƒâ€¦â€¦

```py
 def num_tokens_from_string(string: str, encoding_name: str) -> int:
    """
    Returns the number of tokens in a text string using toktoken.
    See: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb

    Args:
        string (str): The text string to count the tokens for.
        encoding_name (str): The name of the encoding to use.

    Returns:
        num_tokens: The number of tokens in the text string.

    """
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def calc_costs(data, model, method="prompting"):
  """
  Calculate token costs for a given dataset, method and model.
  Note: Only for inference costs, not fine-tuning

  Args:
    data (pandas.DataFrame): The data to get the tokens for.
    method (str, optional): The method to use. Defaults to "prompting".
    model (str): The model to use, eg "gpt-4o-mini"

  Returns:
    input_tokens: The number of input tokens.
    output_tokens: The number of output tokens.

  """
  # See https://openai.com/api/pricing/
  price = {
      "gpt-4o-mini": {
          "input": 0.150,
          "output": 0.600
      },
      "gpt-4o": {
          "input": 5.00,
          "output": 15.00
      }
  }
  input_tokens = 0
  output_tokens = 0
  for index, p in data.iterrows():
      prompt = p["prompt"]
      prompt = ast.literal_eval(prompt)
      input = prompt[1]["content"] 
      # If prompting, we must include system prompt
      if method == "prompting":
        input += " " + hxl_prompt
      output = p["Corrected actual"]
      input_tokens += num_tokens_from_string(str(input), "cl100k_base")
      output_tokens += num_tokens_from_string(str(output), "cl100k_base") 

  input_cost = input_tokens / 1000000 * price[model]["input"]
  output_cost = output_tokens / 1000000 * price[model]["output"]

  print(f"\nFor {data.shape[0]} table columns where we predicted HXL tags ...")
  print(f"{method} prediction with model {model}, {input_tokens} input tokens = ${input_cost}")
  print(f"Fine-tuning prediction GPT-4o-mini {output_tokens} output tokens = ${output_cost}\n")

hxl_prompt = generate_hxl_standard_prompt(HXL_SCHEMA_LOCAL_FILE, debug=False)
X_test2 = pd.read_excel(f"{LOCAL_DATA_DIR}/hxl-metadata-fine-tune-prediction-results-review.xlsx", sheet_name=0)

calc_costs(X_test2, method="fine-tuning", model="gpt-4o-mini")
calc_costs(X_test2, method="prompting", model="gpt-4o-mini")
calc_costs(X_test2, method="prompting", model="gpt-4o")
```

ç»“æœæ˜¯â€¦â€¦

```py
For 458 table columns where we predicted HXL tags ...
fine-tuning prediction with model gpt-4o-mini, 99738 input tokens = $0.014960699999999999
Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006

For 458 table columns where we predicted HXL tags ...
prompting prediction with model gpt-4o-mini, 2688812 input tokens = $0.4033218
Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.0012006

For 458 table columns where we predicted HXL tags ...
prompting prediction with model gpt-4o, 2688812 input tokens = $13.44406
Fine-tuning prediction GPT-4o-mini 2001 output tokens = $0.030015000000000003
```

æ³¨ï¼šä¸Šè¿°ä»…ä¸ºæ¨ç†æˆæœ¬ï¼Œç”Ÿæˆè¡¨æ ¼æ•°æ®æ‘˜è¦æ—¶ä½¿ç”¨ GPT-3.5 å¯èƒ½ä¼šæœ‰éå¸¸å°çš„é¢å¤–è´¹ç”¨ã€‚

ç»™å®šæµ‹è¯•é›†ï¼Œé¢„æµ‹ **458 åˆ—** çš„ HXL æ ‡ç­¾â€¦â€¦

**å¾®è°ƒ**ï¼š

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå¾®è°ƒåçš„ GPT-4o mini æ¨¡å‹ï¼ˆå¾®è°ƒèŠ±è´¹å¤§çº¦ $7ï¼‰æ¨ç†æˆæœ¬éå¸¸ä½ï¼Œçº¦ä¸º $0.02ã€‚

**ä»…é¢„æµ‹**ï¼š

+   ä»…ä½¿ç”¨ GPT-4o è¿›è¡Œé¢„æµ‹çš„æˆæœ¬è¾ƒé«˜ï¼Œå› ä¸ºæ¯æ¬¡éƒ½éœ€è¦å°† HXL æ ‡å‡†ä¼ é€’ç»™ç³»ç»Ÿæç¤ºï¼Œè´¹ç”¨ä¸º $13.44ã€‚

+   GPT-4o-mini å°½ç®¡æ€§èƒ½æœ‰æ‰€ä¸‹é™ï¼Œä½†æ¯æ¬¡è°ƒç”¨çš„è´¹ç”¨æ›´ä¸ºåˆç†ï¼Œä¸º $0.40ã€‚

å› æ­¤ï¼Œä½¿ç”¨ GPT-4o çš„æ˜“ç”¨æ€§æ˜¯æœ‰ä»£ä»·çš„ï¼Œä½† GPT-4o-mini æ˜¯ä¸€ä¸ªå…·æœ‰å¸å¼•åŠ›çš„æ›¿ä»£é€‰æ‹©ã€‚

æœ€åï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œè®¾ç½® HXL æ ‡ç­¾å¯èƒ½ä¸æ˜¯å®æ—¶çš„ï¼Œä¾‹å¦‚å¯¹äºä¿®æ­£å·²ä¸Šä¼ æ•°æ®é›†çš„çˆ¬è™«è¿›ç¨‹ã€‚è¿™æ„å‘³ç€å¯ä»¥ä½¿ç”¨æ–°çš„[OpenAI æ‰¹é‡ API](https://platform.openai.com/docs/guides/batch/overview)ï¼Œä»è€Œå°†æˆæœ¬é™ä½ 50%ã€‚

# ç”¨äºé¢„æµ‹ HXL æ ‡ç­¾çš„ Python ç±»

å°†è¿™äº›å†…å®¹ç»“åˆåœ¨ä¸€èµ·ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ª Github gist [hxl_utils.py](https://gist.github.com/dividor/e693997c1fc7e0d94f8228cebc397014)ã€‚å¯ä»¥ä» GitHub ä¸‹è½½å¹¶å°†æ–‡ä»¶æ”¾å…¥å½“å‰å·¥ä½œç›®å½•ä¸­ã€‚

è®©æˆ‘ä»¬ä¸‹è½½ä¸€ä¸ªæ–‡ä»¶æ¥æµ‹è¯•å®ƒâ€¦â€¦

```py
# See HDX for this file: https://data.humdata.org/dataset/sudan-acled-conflict-data
DATAFILE_URL="https://data.humdata.org/dataset/5efad450-8b15-4867-b7b3-8a25b455eed8/resource/3352a0d8-2996-4e70-b618-3be58699be7f/download/sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx"
local_data_file = f"{LOCAL_DATA_DIR}/{DATAFILE_URL.split('/')[-1]}"

# Save data file locally 
urllib.request.urlretrieve(DATAFILE_URL, local_data_file)

# Read it to get a dataframe
df = pd.read_excel(local_data_file, sheet_name=1)
```

![](img/f4d88e404896f574504c6d75b455d9cb.png)

ä½¿ç”¨è¿™ä¸ªæ•°æ®æ¡†ï¼Œæˆ‘ä»¬æ¥é¢„æµ‹ HXL æ ‡ç­¾â€¦â€¦

```py
from hxl_utils import HXLUtils

hxl_utils = HXLUtils(LOCAL_DATA_DIR, model="gpt-4o")
data = hxl_utils.add_hxl(df,"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx")

print("\n\nAFTER: \n\n")
display(data)
```

![](img/33bafdfd979641e3cc932f24d4992577.png)

å°±è¿™æ ·ï¼Œå¾—åˆ°äº†äº›æ¼‚äº®çš„ HXL æ ‡ç­¾ï¼

è®©æˆ‘ä»¬çœ‹çœ‹ GPT-4o-mini è¡¨ç°å¦‚ä½•â€¦â€¦

```py
hxl_utils = HXLUtils(LOCAL_DATA_DIR, model="gpt-4o-mini")
data = hxl_utils.add_hxl(df,"sudan_hrp_civilian_targeting_events_and_fatalities_by_month-year_as-of-25jul2024.xlsx")
```

ç»“æœæ˜¯â€¦â€¦

![](img/551db548c232ea8e6d577b657dee9021.png)

å¾ˆä¸é”™ï¼gpt-4o ç»™å‡ºäº†â€œ#affected+killed+numâ€ä½œä¸ºæœ€åä¸€åˆ—ï¼Œè€Œâ€œgpt-4o-miniâ€åˆ™ç»™å‡ºäº†â€œ#affected+numâ€ï¼Œä½†è¿™å¾ˆå¯èƒ½å¯ä»¥é€šè¿‡ä¸€äº›å·§å¦™çš„æç¤ºå·¥ç¨‹æ¥è§£å†³ã€‚

å¦ç‡åœ°è¯´ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†ï¼Œä½†å®ƒèƒ½å¤Ÿæ­£ç¡®é¢„æµ‹äº‹ä»¶å’Œæ­»äº¡çš„æ ‡ç­¾ï¼Œè€Œè¿™äº›æ ‡ç­¾æ¯”åœ°ç‚¹å’Œæ—¥æœŸè¦å°‘è§ã€‚

# æœªæ¥å·¥ä½œ

æˆ‘è®¤ä¸ºä¸€ä¸ªé‡è¦çš„æ”¶è·æ˜¯ï¼Œç›´æ¥æç¤ºæŠ€æœ¯èƒ½å¤Ÿåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å–å¾—ä¸é”™çš„ç»“æœã€‚æ˜¯çš„ï¼Œæ¨ç†æˆæœ¬æ›´é«˜ï¼Œä½†å¦‚æœéœ€è¦æ•°æ®ç§‘å­¦å®¶æ•´ç†é”™è¯¯æ ‡æ³¨çš„å¾®è°ƒæ•°æ®ï¼Œä¹Ÿè®¸å¹¶ä¸é‚£ä¹ˆè´µã€‚è¿™å°†å–å†³äºç»„ç»‡å’Œå…ƒæ•°æ®çš„ä½¿ç”¨åœºæ™¯ã€‚

è¿™é‡Œæ˜¯ä¸€äº›å¯èƒ½åœ¨æœªæ¥å·¥ä½œä¸­è€ƒè™‘çš„é¢†åŸŸâ€¦â€¦

**æ”¹è¿›çš„æµ‹è¯•æ•°æ®**

è¿™ä¸ªåˆ†æå¿«é€Ÿå®¡æŸ¥äº†æµ‹è¯•é›†ï¼Œä»¥ä¿®æ­£æ•°æ®ä¸­ä¸æ­£ç¡®çš„ HXL æ ‡ç­¾æˆ–å­˜åœ¨å¤šä¸ªå¯èƒ½å€¼çš„æ ‡ç­¾ã€‚å¯ä»¥åœ¨è¿™æ–¹é¢æŠ•å…¥æ›´å¤šæ—¶é—´ï¼Œæ­£å¦‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåœ°é¢çœŸç›¸æ˜¯å…³é”®ã€‚

**æç¤ºå·¥ç¨‹ä¸è¶…å‚æ•°è°ƒä¼˜**

ä¸Šè¿°åˆ†æä½¿ç”¨äº†éå¸¸åŸºç¡€çš„æç¤ºè¯ï¼Œå¹¶æ²¡æœ‰åº”ç”¨ä»»ä½•çœŸæ­£çš„å·¥ç¨‹æ–¹æ³•æˆ–ç­–ç•¥ï¼Œè¿™äº›æ–¹æ³•è‚¯å®šå¯ä»¥é€šè¿‡æ”¹è¿›æ¥æé«˜æ€§èƒ½ã€‚é€šè¿‡è¯„ä¼°é›†å’Œåƒ[Promptflow](https://github.com/microsoft/promptflow)è¿™æ ·çš„æ¡†æ¶ï¼Œå¯ä»¥æµ‹è¯•ä¸åŒçš„æç¤ºè¯å˜ä½“ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ·»åŠ æ›´å¤šçš„ä¸Šä¸‹æ–‡æ•°æ®ï¼Œä¾‹å¦‚åœ¨å†³å®šè¡Œæ”¿çº§åˆ«æ—¶ï¼Œè¿™å¯èƒ½å› å›½å®¶è€Œå¼‚ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å›ºå®šçš„è¶…å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€top_p ä»¥åŠå®Œæˆæ ‡è®°é•¿åº¦ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥è°ƒæ•´ï¼Œä»è€Œæé«˜æ€§èƒ½ã€‚

**æˆæœ¬ä¼˜åŒ–**

ä»…ä½¿ç”¨æç¤ºè¯çš„æ–¹æ³•æ— ç–‘æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„é€‰æ‹©ï¼Œå¹¶ç®€åŒ–äº†ç»„ç»‡å¦‚ä½•é€šè¿‡ GPT-4o è‡ªåŠ¨ä¸ºå…¶æ•°æ®è®¾ç½® HXL æ ‡ç­¾ã€‚å½“ç„¶ï¼Œè¿™ç§æ–¹æ³•æœ‰æˆæœ¬ä¸Šçš„è€ƒè™‘ï¼Œå› ä¸ºå®ƒè¾ƒä¸ºæ˜‚è´µï¼Œä½†é¢„æµ‹åªå‘ç”Ÿåœ¨ä½é¢‘ç‡çš„æ¨¡å¼å˜åŒ–æ—¶ï¼Œè€Œä¸æ˜¯å½“åº•å±‚æ•°æ®æœ¬èº«å‘ç”Ÿå˜åŒ–æ—¶ï¼Œéšç€ OpenAI æä¾›æ–°çš„[æ‰¹é‡æäº¤](https://openai.com/api/pricing/)é€‰é¡¹ä»¥åŠ LLM æˆæœ¬ä¸æ–­ä¸‹é™ï¼Œè¿™é¡¹æŠ€æœ¯å¯¹è®¸å¤šç»„ç»‡æ¥è¯´æ˜¯å¯è¡Œçš„ã€‚GPT-4o-mini çš„è¡¨ç°ä¹Ÿå¾ˆå¥½ï¼Œä¸”æˆæœ¬åªæ˜¯å…¶ä¸€å°éƒ¨åˆ†ã€‚

**åº”ç”¨äºå…¶ä»–å…ƒæ•°æ®æ ‡å‡†**

å°†è¿™é¡¹æŠ€æœ¯åº”ç”¨äºå…¶ä»–å…ƒæ•°æ®å’Œæ ‡æ³¨æ ‡å‡†ä¼šå¾ˆæœ‰è¶£ï¼Œæˆ‘ç¡®ä¿¡è®¸å¤šç»„ç»‡å·²ç»åœ¨ä½¿ç”¨ LLMs æ¥å®ç°è¿™ä¸€ç‚¹ã€‚

*å¦‚æœä½ æ„¿æ„ï¼Œè¯·ç‚¹èµè¿™ç¯‡æ–‡ç« ï¼Œå¦‚æœä½ å…³æ³¨æˆ‘ï¼Œæˆ‘å°†éå¸¸é«˜å…´ï¼ä½ å¯ä»¥åœ¨* [*è¿™é‡Œ*](https://medium.com/@astrobagel)* æ‰¾åˆ°æ›´å¤šæ–‡ç« ã€‚*
