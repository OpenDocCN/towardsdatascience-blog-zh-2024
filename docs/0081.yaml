- en: From Adaline to Multilayer Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09](https://towardsdatascience.com/from-adaline-to-multilayer-neural-networks-e115e65fee3e?source=collection_archive---------8-----------------------#2024-01-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Setting the foundations right
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[![Pan
    Cretan](../Images/8b3fbab70c0e61f7ca516d2f54b646e5.png)](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    [Pan Cretan](https://medium.com/@cretanpan?source=post_page---byline--e115e65fee3e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e115e65fee3e--------------------------------)
    ·23 min read·Jan 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8f71ed9e1d173f61b5f8ea765b06d34.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Konta Ferenc](https://unsplash.com/@fr3dd87?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In the previous two articles we saw how we can implement a basic classifier
    based on Rosenblatt’s [perceptron](https://medium.com/towards-data-science/classification-with-rosenblatts-perceptron-e7f49e3af562)
    and how this classifier can be improved by using the adaptive linear neuron algorithm
    ([adaline](https://medium.com/towards-data-science/from-the-perceptron-to-adaline-1730e33d41c5)).
    These two articles cover the foundations before attempting to implement an artificial
    neural network with many layers. Moving from adaline to deep learning is a bigger
    leap and many machine learning practitioners will opt directly for an open source
    library like [PyTorch](https://pytorch.org/). Using such a specialised machine
    learning library is of course recommended for developing a model in production,
    but not necessarily for learning the fundamental concepts of multilayer neural
    networks. This article builds a multilayer neural network from scratch. Instead
    of solving a binary classification problem we will focus on a multiclass one.
    We will be using the sigmoid activation function after each layer, including the
    output one. Essentially we train a model that for each input, comprising a vector
    of features, produces a vector with length equal to the number of classes to be
    predicted. Each element of the output vector is in the range [0, 1] and can be
    understood as the “probability” of each class.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the article is to become comfortable with the mathematical notation
    used for describing mathematically neural networks, understand the role of the
    various matrices with weights and biases, and derive the formulas for updating
    the weights and biases to minimise the loss function. The implementation allows
    for any number of hidden layers with arbitrary dimensions. Most tutorials assume
    a fixed architecture but this article uses a carefully chosen mathematical notation
    that supports generalisation. In this way we can also run simple numerical experiments
    to examine the predictive performance as a function of the number and size of
    the hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: As in the earlier articles, I used the online [LaTeX equation editor](https://latexeditor.lagrida.com/)
    to develop the LaTeX code for the equation and then the chrome plugin [Maths Equations
    Anywhere](https://chromewebstore.google.com/detail/math-equations-anywhere/fkioioejambaepmmpepneigdadjpfamh)
    to render the equation into an image. All LaTex code is provided at the end of
    the article if you need to render it again. Getting the notation right is part
    of the journey in machine learning, and essential for understanding neural networks.
    It is vital to scrutinise the formulas, and pay attention to the various indices
    and the rules for matrix multiplication. Implementation in code becomes trivial
    once the model is correctly formulated on paper.
  prefs: []
  type: TYPE_NORMAL
- en: All code used in the article can be found in the accompanying [repository](https://github.com/karpanGit/myBlogs/tree/master/MultilayerNeuralNetworks).
    The article covers the following topics
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [What is a multilayer neural network?](#14a0)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Activation](#48ed)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Loss function](#7fda)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Backpropagation](#d970)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Implementation](#5f28)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Dataset](#2f55)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Training the model](#431d)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Hyperparameter tuning](#15f6)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Conclusions](#eb5c)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [LaTeX code of equations used in the article](#9aa7)
  prefs: []
  type: TYPE_NORMAL
- en: What is a multilayer neural network?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces the architecture of a generalised, feedforward, fully-connected
    multilayer neural network. There are a lot of terms to go through here as we work
    our way through Figure 1 below.
  prefs: []
  type: TYPE_NORMAL
- en: For every prediction, the network accepts a vector of features as input
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dabd645b20fbd402155f26246eebf24.png)'
  prefs: []
  type: TYPE_IMG
- en: that can also be understood as a matrix with shape (1, n⁰). The network uses
    L layers and produces a vector as an output
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95809ead8c78a7209cd11a22846181b9.png)'
  prefs: []
  type: TYPE_IMG
- en: that can be understood as a matrix with shape (1, nᴸ) where nᴸ is the number
    of classes in the multiclass classification problem we need to solve. Every float
    in this matrix lies in the range [0, 1] and the index of the largest element corresponds
    to the predicted class. The (L) notation in the superscript is used to refer to
    a particular layer, in this case the last one.
  prefs: []
  type: TYPE_NORMAL
- en: But how do we generate this prediction? Let’s focus on the first element of
    the first layer (the input is not considered a layer)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c601c9aca303ea8b14cda3be7db7b51.png)'
  prefs: []
  type: TYPE_IMG
- en: We first compute the net input that is essentially an inner product of the input
    vector with a set of weights with the addition of a bias term. The second operation
    is the application of the activation function σ(z) to which we will return later.
    For now it is important to keep in mind that the activation function is essentially
    a scalar operation.
  prefs: []
  type: TYPE_NORMAL
- en: We can compute all elements of the first layer in the same way
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a47528924ac72826b89fc6aaadaec15.png)'
  prefs: []
  type: TYPE_IMG
- en: From the above we can deduce that we have introduced n¹ x n⁰ weights and n¹
    bias terms that will need to be fitted when the model is trained. These calculations
    can also be expressed in matrix form
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/713a0b377225ff41355e489e2cc46e0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Pay close attention to the shape of the matrices. The net output is a result
    of a matrix multiplication of two matrices with shape (1, n⁰) and (n⁰, n¹) that
    results in a matrix with shape (1, n¹), to which we add another matrix with the
    bias terms that has the same (1, n¹) shape. Note that we introduced the transpose
    of the weight matrix. The activation function applies to every element of this
    matrix and hence the activated values of layer 1 are also a matrix with shape
    (1, n¹).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f0b228ffacfcb6ffd8fac45fc0181a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A general multilayer neural network with an arbitrary number of input
    features, number of output classes and number of hidden layers with different
    number of nodes (image by the Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The above can be readily generalised for every layer in the neural network.
    Layer k accepts as input nᵏ⁻¹ values and produces nᵏ activated values
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a2b0d0fc68d1ef5891114ad87975343.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer k introduces nᵏ x nᵏ⁻¹ weights and nᵏ bias terms that will need to be
    fitted when the model is trained. The total number of weights and bias terms is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62393e822cd89c09f35328a18dcf52b7.png)'
  prefs: []
  type: TYPE_IMG
- en: so if we assume an input vector with 784 elements (dimension of a low resolution
    image in gray scale), a single hidden layer with 50 nodes and 10 classes in the
    output we need to optimise 785*50+51*10 = 39,760 parameters. The number of parameters
    grows further if we increase the number of hidden layers and the number of nodes
    in these layers. Optimising an objective function with so many parameters is not
    a trivial undertaking and this is why it took some time from the time adaline
    was introduced until we discovered how to train deep networks in the mid 80s.
  prefs: []
  type: TYPE_NORMAL
- en: This section essentially covers what is known as the forward pass, i.e. how
    we apply a series of matrix multiplications, matrix additions and element wise
    activations to convert the input vector to an output vector. If you pay close
    attention we assumed that the input was a single sample represented as a matrix
    with shape (1, n⁰). The notation holds even if we we feed into the network a batch
    of samples represented as a matrix with shape (N, n⁰). There is only small complexity
    when it comes to the bias terms. If we focus on the first layer we sum a matrix
    with shape (N, n¹) to a bias matrix with shape (1, n¹). For this to work the bias
    matrix has its first row replicated as many times as the number of samples in
    the batch we use in the forward pass. This is such a natural operation that [NumPy](https://numpy.org/)
    does it automatically in what is called [broadcasting](/numpy-broadcasting-4c4cb9dff1e7).
    When we apply forward pass to a batch of inputs it is perhaps cleaner to use capital
    letters for all vectors that become matrices, i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4fdb81338c9c192ddf1dcb2f312c2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that I assumed that broadcasting was applied to the bias terms leading
    to a matrix with as many rows as the number of samples in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: Operating with batches is typical with deep neural networks. We can see that
    as the number of samples N increases we will need more memory to store the various
    matrices and carry out the matrix multiplications. In addition, using only part
    of training set for updating the weights means we will be updating the parameters
    several times in each pass of the training set (epoch) leading to faster convergence.
    There is an additional benefit that is perhaps less obvious. The network uses
    activation functions that, unlike the activation in adaline, are not the identity.
    In fact they are not even linear, which makes the loss function non convex. Using
    batches introduces noise that is believed to help escaping shallow local minima.
    A suitably chosen learning rate further assists with this.
  prefs: []
  type: TYPE_NORMAL
- en: As a final note before we move on, the term feedforward comes from the fact
    that each layer is using as input the output of the previous layer without using
    loops that lead to the so-called recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Activation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Enabling the neural network to solve complex problem requires introducing some
    form of nonlinearity. This is achieved by using an activation function in each
    layer. There are many choices. For this article we will be using the sigmoid (logistic)
    activation function that we can visualise with
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62c64572970b8e726c169a4978ebb6e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Sigmoid (logistic) activation function. Image by the Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The code also includes all imports we will need throughout the article.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function maps any float to the range 0 to 1\. In reality the
    sigmoid is a more suitable activation of the final layer for binary classification
    problems. For multiclass problems it would have been more appropriate to use [softmax](/sigmoid-and-softmax-functions-in-5-minutes-f516c80ea1f9)
    to normalize the output of the neural network to a probability distribution over
    predicted output classes. One way to think about this is that softmax enforces
    that post activation the sum of the entries of the output vector must add up to
    1, that is not the case with sigmoid. Another way to think about it is that the
    sigmoid essentially converts the logits (log odds) to a one-versus-all (OvA) probability.
    Still, we will use the sigmoid activation function to stay as close as possible
    to adaline because the softmax is not an element wise operation and this will
    introduce some complexities in the back propagation algorithm. I leave this as
    an exercise for the reader.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss function used for adaline was the mean square error. In practice a
    multiclass classification problem would use a multiclass cross-entropy loss. In
    order to remain as close to adaline as possible, and to facilitate the analytical
    calculation of the gradients of the loss function with respect to the parameters,
    we will stick on the mean square error loss function. Every sample in the training
    set, belongs to one of the nᴸ classes and hence the loss function can be expressed
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43cd4bb53dcd48571173bf46019f5b23.png)'
  prefs: []
  type: TYPE_IMG
- en: where the first summation is over all samples and the second over classes. The
    above implies that the known class for sample i has been converted to a one-hot-encoding,
    i.e. a matrix with shape (1, nᴸ) containing zeros apart from the element that
    corresponds to the sample class that is one. We adopted one more notation convention
    so that [j] in the superscript is used to refer to sample j. The summation above
    does not need to use all samples in the training set. In practice it will be applied
    in batches of N’ samples with N’<<N.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The loss function is a scalar that depends on tens or hundreds of thousands
    of parameters, comprising weights and bias terms. Typically, these parameters
    are initialised with random numbers and are updated iteratively so that the loss
    function is minimised using the gradient of the loss function with regard to each
    parameter. In the case of adaline, the analytical derivation of the gradients
    was straightforward. For multilayer neural networks the derivation is more involved
    but remains tractable if we adopt a clever strategy. We enter the world of the
    back propagation but fear not. Backpropagation essentially boils down to a successive
    application of the chain differentiation rule from the right to the left.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s come back to the loss function. It depends on the activated values of
    the last layer, so we can first compute the derivatives with regard to those
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e8f729f6d41f269acfe1bd97fc7bdb3.png)'
  prefs: []
  type: TYPE_IMG
- en: The above can be understood as the (j, i) element of a derivate matrix with
    shape (N, nᴸ) and can be written in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17b39acd1aa2c9eda84bee95e10d7d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where both matrices in the right hand side have shape (N, nᴸ). The activated
    values of the last layer are computed by applying the sigmoid activation function
    on each element of the net input matrix of the last layer. Hence, to compute the
    derivatives of the loss function with regard to each element of this net input
    matrix of the last layer we simply need to remind ourselves on how to compute
    the derivative of a nested function with the outer one being the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3184ea54722f9a63720f945d149d8099.png)'
  prefs: []
  type: TYPE_IMG
- en: The star multiplication denotes element wise multiplication. The result of this
    formula is a matrix with shape (N, nᴸ). If you have difficulties computing the
    derivative of the sigmoid function please check [here](https://en.wikipedia.org/wiki/Logistic_function).
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to compute the derivative of the loss function with regard
    to the weights of the L-1 layer; this is the first set of weights we encounter
    when we move from right to left
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89b3c727cfb18427025627e303c5923c.png)'
  prefs: []
  type: TYPE_IMG
- en: This leads to a matrix with the same shape as the weights of the L-1 layer.
    We next need to compute the derivative of the net input of the L layer with regard
    to the weights of the L-1 layer. If we pick one element of the net input matrix
    of the last layer and one of these weights we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02b9f932d9ab82710feaf6ac8f0fc82b.png)'
  prefs: []
  type: TYPE_IMG
- en: If you have trouble to understand the above, think that for every sample j,
    the i element of the net input of the L layer only depends on the weights of the
    L-1 layer for which the first index is also i. Hence, we can eliminate one of
    the summations in the derivative
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e7ebc31a8dbc7e81382424307d664b3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can express all these derivatives in a matrix notation using
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e1edec3574bea4b375e321b1c82c402.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially the implicit summation in the matrix multiplication absorbs the
    summation over the samples. Follow along with the shapes of the multiplied matrices
    and you will see that the resulting derivative matrix has the same shape as the
    weight matrix used to calculate the net input of the L layer. Although the number
    of elements in the resulting matrix is limited to the product of the number of
    nodes of the last two layers (the shape is (nᴸ, nᴸ⁻¹)), the multiplied matrices
    are much larger and hence are typically more memory consuming. Hence, the need
    to use batches when training the model.
  prefs: []
  type: TYPE_NORMAL
- en: The derivatives of the loss function with respect to the bias terms used for
    calculating the net input of the last layer can be computed similarly as for the
    weights to give
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3934777a724a4a9be0a877917bbf1be5.png)'
  prefs: []
  type: TYPE_IMG
- en: that leads to a matrix with shape (1, nᴸ).
  prefs: []
  type: TYPE_NORMAL
- en: We have just computed all derivatives of the loss function with regard to the
    weights and bias terms used for computing the net input of the last layer. We
    now turn our attention to the gradients with the regard to the weights and bias
    terms of the previous layer (these parameters will have the superscript index
    L-2). Hopefully we can start identifying patterns so that we can apply them to
    compute the derivates with regard to the weights and bias terms for k=0,..,L-2\.
    We could see these patterns emerge if we compute the derivative of the loss function
    with regard to the activated values of the L-1 layer. These should form a matrix
    with shape (N, nᴸ⁻¹) that is computed as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa8be214ae7ba029753e82575174d717.png)'
  prefs: []
  type: TYPE_IMG
- en: Once we have the derivatives of the loss with regard to the activated values
    of layer L-1 we can proceed with calculating the derivatives of the loss function
    with regard to the net input of the layer L-1 and then with regard to the weights
    and bias terms with index L-2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s recap how we backpropagate by one layer. We assume we have computed the
    derivative of the loss function with regard to the weights and bias terms with
    index k and we need to compute the derivates of the loss function with regard
    to the weights and bias terms with index k-1\. We need to carry out 4 operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e532d96ea2523945e85fa02b34245556.png)'
  prefs: []
  type: TYPE_IMG
- en: All operations are vectorised. We can already start imaging how we could implement
    these operations in a class. My understanding is that when one uses a specialised
    library to add a fully connected linear layer with an activation function, this
    is what happens behind the scenes! It is nice not to worry about the mathematical
    notation, but my suggestion would be to go through these derivations at least
    once.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we provide the implementation of a generalised, feedforward,
    multilayer neural network. The API draws some analogies to the one found in specialised
    deep learning libraries such as PyTorch
  prefs: []
  type: TYPE_NORMAL
- en: 'The code contains two utility functions: `sigmoid()` applies the sigmoid (logistic)
    activation function to a float (or NumPy array), and `int_to_onehot()` takes a
    list of integers with the class of each sample and returns their one-hot-encoding
    representation.'
  prefs: []
  type: TYPE_NORMAL
- en: The class `MultilayerNeuralNetClassifier` contains the neural net implementation.
    The initialisation constructor assigns random numbers to the weights and bias
    terms of each layer. As an example if we construct a neural network with `layers=[784,
    50, 10]`, we will be using 784 input features, a hidden layer with 50 nodes and
    10 classes as output. This generalised implementation allows changing both the
    number of hidden layers and the number of nodes in the hidden layers. We will
    exploit this when we do hyperparameter tuning later on. For reproducibility we
    use a seed for the random number generator to initialise the weights.
  prefs: []
  type: TYPE_NORMAL
- en: The `forward` method returns the activated values for each layer as a list of
    matrices. The method works with a single sample or an array of samples. The last
    of the returned matrices contains the model predictions for the class membership
    of each sample. Once the model is trained only this matrix is used for making
    predictions. However, whilst the model is being trained we need the activated
    values for all layers as we will see below and this is why the `forward` method
    returns all of them. Assuming that the network was initialised with `layers=[784,
    50, 10]`, the `forward` method will return a list of two matrices, the first one
    with shape (N, 50) and the second one with shape (N, 10), assuming the input `x`
    has N samples, i.e. it is a matrix with shape (N, 784).
  prefs: []
  type: TYPE_NORMAL
- en: The `backward` method implements backpropagation, i.e. all the analytically
    computed derivatives of the loss function as described in the previous section.
    The last layer is special because we need to compute the derivatives of the loss
    function with regard to the model output using the known classes. The first layer
    is special because we need to use the input instead of the activated values of
    the previous layer. The middle layers are all the same. We simply iterate over
    the layers backwards. The code reflects fully the analytically derived formulas.
    By using NumPy we vectorise all operations that speeds up execution. The method
    returns a tuple of two lists. The first list contains the matrices with the derivatives
    of the loss function with regard to the weights of each layer. Assuming that the
    network was initialised with `layers=[784, 50, 10]`, the list will contain two
    matrices with shapes (784, 50) and (50, 10). The second list contains the vectors
    with the derivatives of the loss function with regard to the bias terms of each
    layer. Assuming that the network was initialised with `layers=[784, 50, 10]`,
    the list will contain two vectors with shapes (50, ) and (10,).
  prefs: []
  type: TYPE_NORMAL
- en: Reflecting back on my learnings from this article, I felt that the implementation
    was straightforward. The hardest part was to come up with a robust mathematical
    notation and work out the gradients on paper. Still, it is easy to make mistakes
    that may not be easy to detect even if the optimisation seems to converge. This
    brings me to the special `backward_numerical` method. This method is used for
    neither training the model nor making predictions. It uses finite (central) differences
    to estimate the derivatives of the loss function with regard to the weights and
    bias terms of the chosen layer. The numerical derivatives can be compared with
    the analytically computed ones returned by the `backward` function to ensure that
    the implementation is correct. This method would be too slow to be used for training
    the model as it requires two forward passes for each derivative and in our trivial
    example with `layers=[784, 50, 10]` there are 39,760 such derivatives! But it
    is a lifesaver. Personally I would not have managed to debug the code without
    it. If you want to keep a key message from this article, it would be the usefulness
    of numerical differentiation for double checking your analytically derived gradients.
    We can check the correctness of the gradients with an untrained model
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Gradients look in order!
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will need a dataset for building our first model. A famous one often used
    in pattern recognition experiments is the [MNIST handwritten digits](https://en.wikipedia.org/wiki/MNIST_database).
    We can find more details about this dataset in the OpenML dataset [repository](https://openml.org/search?type=data&status=active&id=554&sort=runs).
    All datasets in OpenML are [subject](https://openml.org/terms) to the [CC BY 4.0
    license](https://creativecommons.org/licenses/by/4.0/) that permits copying, redistributing
    and transforming the material in any medium and for any purpose.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 70,000 digit images and the corresponding labels. Conveniently,
    the digits have been size-normalized and centered in a fixed-size 28x28 image
    by computing the center of mass of the pixels, and translating the image so as
    to position this point at the center of the 28x28 field. The dataset can be conveniently
    retrieved using [scikit-learn](https://scikit-learn.org/)
  prefs: []
  type: TYPE_NORMAL
- en: that prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can see that each image is available as a vector with 784 integers between
    0 and 255 that were converted to floats in the range [-0.5, 0.5]. This is perhaps
    a bit different than the typical feature scaling in scikit-learn where scaling
    happens per feature rather per sample. The class labels were retrieved as strings
    and converted to integers. The dataset is reasonably balanced.
  prefs: []
  type: TYPE_NORMAL
- en: We next visualise ten images for each digit to obtain a feeling on the variations
    in hand writing
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71d8169c956539a18b636b779ab90a9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Randomly selected samples for each digit. Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: We can foresee that some digits may be confused by the model, e.g. the last
    9 resembles 8\. There may also be hand writing variations that are not predicted
    well, such as 7 digits written with a horizontal line in the middle, depending
    on how often such variations are represented in the training set. We now have
    a neural network implementation and a dataset to use it with. In the next section
    we will provide the necessary code for training the model before we look into
    hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first action we need to take is to split the dataset into a training set,
    and an external (hold-out) test set. We can readily do so using scikit-learn
  prefs: []
  type: TYPE_NORMAL
- en: We use stratification so that the percentage of each class is roughly equal
    in both the training set and the external (hold-out) dataset. The external (hold-out)
    test set contains 10,000 samples and will not be used for anything other than
    assessing the model performance. In this section we will use the 60,000 samples
    for training set without any hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: When deriving the gradients of the loss function with regard to the model parameters
    we show that it is necessary to carry out several matrix multiplications and some
    of these matrices have as many rows as the number of samples. Given that typically
    the number of samples is quite large we will need a significant amount of memory.
    To alleviate this we will be using mini batches in the same way we [used](/from-the-perceptron-to-adaline-1730e33d41c5)
    mini batches during the gradient descent optimisation of the adaline model. Typically,
    each batch can contain 100–500 samples. Reducing the batch size increases the
    convergence speed because we make more parameter updates within the the same pass
    of the training set (epoch), but we also increase the noise. We need to strike
    a balance. First we provide a generator that accepts the training set and the
    batch size and returns the batches
  prefs: []
  type: TYPE_NORMAL
- en: The generator returns batches of equal size that by default contain 100 samples.
    The total number of samples may not be a multiple of the batch size and hence
    some samples will not be returned in a given pass through the training set. Th
    number of skipped samples is smaller than the batch size and the set of samples
    left out changes every time the generator is used, assuming we do not reset the
    random number generator. Hence, this is not critical. As we will be passing though
    the training sets multiple times in the different epochs we will eventually use
    the training set fully. The reason for using batches of a constant size is that
    we will be updating the model parameters after each batch and a small batch can
    increase the noise and prevent convergence, especially if the samples in the batch
    happen to be outliers.
  prefs: []
  type: TYPE_NORMAL
- en: When the model is initiated we expect a low accuracy that we can confirm with
  prefs: []
  type: TYPE_NORMAL
- en: that gives an accuracy of approximately 9.5%. This is more or less expected
    for a reasonably balanced dataset as there are 10 classes. We now have the means
    to monitor the loss and the accuracy of each batch passed to the forward pass
    that we will exploit during training. Let’s write the final piece of code to iterate
    over the epochs and mini batches, update the model parameters and monitor how
    the loss and accuracy evolves in both the training set and external (hold-out)
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: Using this function training becomes a single line of code
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We can see that after the ten epochs the accuracy for the training set has reached
    approximately 76%, whilst the accuracy of the external (hold-out) test set is
    slightly higher, indicating that the model has not been overfitted.
  prefs: []
  type: TYPE_NORMAL
- en: The loss of the training set keeps decreasing and hence convergence has not
    been reached yet. The model allows hot starting so we could run another ten epochs
    by repeating the single line of code above. Instead, we will initiate the model
    again and run it for 100 epochs, increasing the batch size to 200 at the same
    time. We provide the complete code for doing so.
  prefs: []
  type: TYPE_NORMAL
- en: We first plot the training loss and its rate of change as a function of the
    epoch number
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3165c82de6bdca3d12babfab2e461d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Training loss and its rate of change as a function of the epoch number. Image
    by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: We can see the model has converged reasonably well as the rate of the change
    of the training loss has become more than two orders of magnitude smaller compared
    to its value at the beginning of the training. I am not sure why we observe a
    reduction in convergence speed at around epoch 10; I can only speculate that the
    optimiser escaped a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: We can also plot the accuracy of the training set and the test set as a function
    of the epoch number
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83e7976dde52eb1f7735294d5840e92b.png)'
  prefs: []
  type: TYPE_IMG
- en: Training set and external (hold-out) test set accuracy as a function of the
    epoch number. Image by the Author.
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy reaches approximately 90% after about 50 epochs for both the training
    set and external (hold-out) test set, suggesting that there is no/little overfitting.
    We just trained our first, custom built multilayer neural network with one hidden
    layer!
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this previous section we chose an arbitrary network architecture and fitted
    the model parameters. In this section we proceed with a basic hyperparameter tuning
    by varying the number of hidden layers (ranging from 1 to 3), the number of nodes
    in the hidden layers (ranging from 10 to 50 in increments of 10) and the learning
    rate (using the values 0.1, 0.2 and 0.3). We kept the batch size constant at 200
    samples per batch. Overall, we tried 45 parameter combinations. We will employ
    6-fold cross validation (not nested) which means 6 model trainings per parameter
    combination, which translates to 270 model trainings in total. In each fold we
    will be using 50,000 samples for training and 10,000 samples for measuring the
    accuracy (called validation in the code). To enhance the chances to achieve convergence
    we will perform 250 epochs for each model fitting. The total execution time was
    ~12 hours on a single processor (Intel Xeon Gold 3.5GHz). This is more or less
    what we can reasonably run on a CPU. The training speed could be increased using
    multiprocessing. In fact, the training would be way faster using a specialised
    deep learning library like PyTorch on GPUs, such as the freely available T4 GPUs
    on [Google Cola](https://colab.research.google.com)b.
  prefs: []
  type: TYPE_NORMAL
- en: This code iterates over all hyperparameter values and folds and stores the loss
    and accuracy for both the training (50,000 samples) and validation (10,000 samples)
    in a [pandas](https://pandas.pydata.org/) dataframe. The dataframe is used to
    find the optimal hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: that produces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can see that there is little benefit in increasing the number of layers.
    Perhaps we could have gained slightly better performance using a larger first
    hidden layer as the hyperparameter tuning hit the bound of 50 nodes. Some mean
    cross-validation accuracies are quite low that could be indicative of poor convergence
    (e.g. when using 3 hidden layers with 10 nodes each). We did not investigate further
    but this would be typically required before concluding on the optimal network
    geometry. I would expect that allowing for more epochs would increase accuracy
    further particular with the larger networks.
  prefs: []
  type: TYPE_NORMAL
- en: A final step is to retrain the model with all samples other than the external
    (hold-out) set that are only used for the final evaluation
  prefs: []
  type: TYPE_NORMAL
- en: The last 5 epochs are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We achieved ~95% accuracy with the external (hold-out) test set. This is magical
    if we consider that we started with a blank piece of paper!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article demonstrated how we can build a multilayer, feedforward, fully
    connected neural network from scratch. The network was used for solving a multiclass
    classification problem. The implementation has been generalised to allow for any
    number of hidden layers with any number of nodes. This facilitates hyperparameter
    tuning by varying the number of layers and units in them. However, we need to
    keep in mind that the loss gradients become smaller and smaller as the depth of
    the neural network increases. This is known as the vanishing gradient problem
    and requires using specialised training algorithms once the depth exceeds a certain
    threshold, which is out of the scope of this article.
  prefs: []
  type: TYPE_NORMAL
- en: Our vanilla implementation of a multilayer neural network has hopefully educational
    value. Using it in practice would require several improvements though. First of
    all, overfitting would need to be addressed, by employing some form of drop out.
    Other improvements, such as the addition of skip-connections and the variation
    of the learning rate during training, may be beneficial too. In addition, the
    network architecture itself can be optimised, e.g. by using a convolutional neural
    network that would be more appropriate for classifying images. Such improvements
    are best attempted using a specialised library like [PyTorch](https://pytorch.org/).
    When developing algorithms from scratch one needs to be wary of the time it takes
    and where to draw the line so that the endeavour remains educational without being
    extremely time consuming. I hope this article strikes a good balance in this sense.
    If you are intrigued I would recommend this [book](https://sebastianraschka.com/books/#machine-learning-with-pytorch-and-scikit-learn)
    for further study.
  prefs: []
  type: TYPE_NORMAL
- en: LaTeX code of equations used in the article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The equations used in the article can be found in the gist below, in case you
    would like to render them again.
  prefs: []
  type: TYPE_NORMAL
