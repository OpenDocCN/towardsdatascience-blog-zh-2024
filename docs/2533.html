<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GraphMuse: A Python Library for Symbolic Music Graph Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GraphMuse: A Python Library for Symbolic Music Graph Processing</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graphmuse-a-python-library-for-symbolic-music-graph-processing-40dbd9baf319?source=collection_archive---------3-----------------------#2024-10-17">https://towardsdatascience.com/graphmuse-a-python-library-for-symbolic-music-graph-processing-40dbd9baf319?source=collection_archive---------3-----------------------#2024-10-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="3b94" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Yes, music and graphs do mix!</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://manoskary.medium.com/?source=post_page---byline--40dbd9baf319--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Emmanouil Karystinaios" class="l ep by dd de cx" src="../Images/120d889f330aa7b433a0668a1224e1c8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*OJefyxSpqBkD14wGUsHeFA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--40dbd9baf319--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://manoskary.medium.com/?source=post_page---byline--40dbd9baf319--------------------------------" rel="noopener follow">Emmanouil Karystinaios</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--40dbd9baf319--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">4</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/1bdf8b9613e922ca5924dc48c06da0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUunhJf4xjHvGXVncwiUzA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image generated with Dall-E 3</figcaption></figure><p id="b05e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this post, we take a look at one of my latest papers and open-source software: the GraphMuse Python library. <br/>But before we dive in, let me introduce you to some basics of symbolic music processing.</p><blockquote class="ny"><p id="15f4" class="nz oa fq bf ob oc od oe of og oh nx dx"><strong class="al">And the story goes…</strong></p></blockquote><p id="8190" class="pw-post-body-paragraph nc nd fq ne b go oi ng nh gr oj nj nk nl ok nn no np ol nr ns nt om nv nw nx fj bk">Symbolic music processing mainly refers to extracting information from musical scores. The term symbolic refers to the symbols present in any form of musical score or notation. A musical score can contain a variety of elements other than notes. Such elements may include time signature, key signature, articulation markings, dynamic markings, and many others. Music scores can exist in many formats such as MIDI, MusicXML, MEI, Kern, ABC, and others.</p><p id="89a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In recent years, Graph Neural Networks (GNNs) have become increasingly popular and have seen success in many domains from biology networks to recommender systems to music analysis. In the music analysis field, GNNs have been used to solve tasks such as harmonic analysis, phrase segmentation, and voice separation.</p><p id="caca" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The idea is simple: every note in a score is a vertex in the graph and edges are defined by the temporal relations between the notes as shown in the figure below.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk on"><img src="../Images/48885482c9c9d223613524bad345738a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PvEckgEwXWbe4zrM8TgDtw.png"/></div></div></figure><p id="fe94" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The edges are separated into 4 categories:</p><ul class=""><li id="cbfa" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oo op oq bk">Notes that start at the same time are connected by the “onset” edge (blue)</li><li id="e348" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx oo op oq bk">Notes that start of at the end of some other note are connected by the “consecutive” edge (red)</li><li id="67e6" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx oo op oq bk">Notes that start in between the start and end of another note are connected the “during” edge (green)</li><li id="2d36" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx oo op oq bk">Finally, whenever there is silence all last note endings are connected to the first upcoming notes by the “silent” edge (yellow)</li></ul><p id="dc90" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This minimal modeling of the graph guarantees that a score will be continuously connected from start to finish without any disconnected subgraphs.</p><h1 id="40b2" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">What is GraphMuse</h1><p id="dba3" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">GraphMuse is a Python Library for training and applying deep graph models for music analysis on musical scores.</p><p id="a492" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GraphMuse contains loaders, models, and utils for symbolic music processing with GNNs. It is built on top of <em class="px">PyTorch</em> and <em class="px">PyTorch Geometric</em> for more flexibility and interoperability.</p><p id="49eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">PyTorch is an open-source machine learning library that enables efficient deep learning model building and supports GPU acceleration. <em class="px">PyTorch Geometric</em> is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications.</p><p id="ae1d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally, GraphMuse provides functionalities to transform musical scores into graphs. Graph creation is implemented in C code with Python bindings to speedup the graph building, up to x300 faster than the previous numpy-based implementation.</p><h1 id="76db" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">The Scientific Foundations</h1><p id="ae00" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">Graphs have been frequently used to analyze and represent music. To cite a few examples, the Tonnetz, Schenkerian analysis, and treelike form analysis are some notable mentions. The advantage of graphs is that they can capture both the hierarchical and the sequential nature of music with the same representation simply by the design of the edges.</p><p id="6af5" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Graph-based symbolic music processing using GNNs came about in 2021 with a performance generation model from the score. Since then many graph models have been introduced with some being the state-of-the-art for music analysis tasks up to the date of this post.</p><p id="f99a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">So, now that I argued for the necessity of graphs let’s face the complexities of designing and training graph models for symbolic music.</p><p id="c688" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The main complexity of graphs and of course, music is that musical pieces are not always of the same length and the graphs that are created from them are not the same size either. Their size might vary considerably: for example, a Bach chorale might have only 200 notes whereas a Beethoven sonata can have well over 5000. In our graphs, the number of notes corresponds directly to the number of vertices in each score graph.</p><p id="eb72" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Training efficiently and fast on score graphs is not a trivial task and would require a sampling method that can maximize the computational resources in terms of both memory and time without deteriorating the performance of the model and sometimes even improving it.</p><p id="7e73" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the training process, sampling involves combining graphs from different scores to create a new graph, often referred to as a “batch” in computer science. Each batch is then fed into the GNN model, where a loss is calculated. This loss is used to backpropagate and update the model’s parameters. This single iteration is called a training step. To optimize the model, this process is repeated many times until the training converges and ideally the model performs optimally.</p><p id="73bc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This all sounds complicated but do not despair because GraphMuse can handle this part for you!!</p><h1 id="2617" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">The Inner Workings of GraphMuse</h1></div></div><div class="mr"><div class="ab cb"><div class="lm py ln pz lo qa cf qb cg qc ci bh"><figure class="mm mn mo mp mq mr qe qf paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/912423daa5f3dedfeb944cd7093845bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*TF6QgSUOssTXsUYOVRJyJA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0e15" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The general graph processing/training pipeline for symbolic music scores within GraphMuse involves the following steps:</p><ol class=""><li id="5a72" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qg op oq bk">Preprocess the database of scores to generate input graphs, GraphMuse can do this for you fast and easy;</li><li id="6993" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">Sample the input graphs to create memory-efficient batches, again GraphMuse got your back;</li><li id="f8d5" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">Form a batch as a new graph with nodes and edges from various sampled input graphs; For each graph, a set of nodes is selected which we call <em class="px">target nodes. </em>The neighbors of the target nodes can also be fetched by demand in a process called node-wise sampling.</li><li id="a9a9" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">Update the target nodes’ representations through graph convolution to create node embeddings. GraphMuse provides some models that you can use, otherwise PyTorch Geometric can also be your friend;</li><li id="3425" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">Use these embeddings for task-specific applications. This part is on you but I am sure you can make it!</li></ol><p id="3933" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that target nodes may include all or a subset of batch nodes depending on the sampling strategy.</p><p id="31cb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now that the process is graphically explained let’s take a closer look at how GraphMuse handles sampling notes from each score.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/bd9eb450636f4f2dc08991ca19ba827a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZTyNDmYsnjSk_Pya47pHHA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Top: sampled notes and their neighbors; Middle: score graph and sampling process; Bottom: sampling process for beats and measures.</figcaption></figure><p id="32fd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Sampling process per score.</strong></p><ol class=""><li id="5468" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qg op oq bk">A randomly selected note (in yellow) is first sampled.</li><li id="d119" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">The boundaries of the target notes are then computed with a budget of 15 notes in this example (pink and yellow notes).</li><li id="4d5f" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">Then the k-hop neighbors are fetched for the targets (light blue for 1-hop and darker blue for 2-hop). The k-hop neighbors are computed with respect to the input graph (depicted with colored edges connecting noteheads in the figure above).</li><li id="d995" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx qg op oq bk">We can also extend the sampling process for the beat and measure elements. Note that the k-hop neighbors need not be strictly related to a time window.</li></ol><p id="3b30" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To maximize the computational resources (i.e. memory) the above process is repeated for many scores at once to create one batch. Using this process, GraphMuse asserts that every sampled segment is going to have the same size of target notes. Every sampled segment can be combined to a new graph which will be of size at most <em class="px">#_scores</em> x <em class="px">#_target_notes. </em>This new graph constitutes the batch for the current training step.</p><h1 id="5f58" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Hands-on with GraphMuse</h1><p id="1af0" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">For the hands-on part let’s try to use GraphMuse and use a model for pitch spelling. The pitch spelling task is about inferring the note name and accidentals when they are absent from the score. An example of this application is when we have a quantized midi and want to create a score such as the example in the figure below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/46ae97eb1f61b6bc624882d6353af232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtwRLqbO7g4OYP5hHFNhqA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Midi file is the input (top) and the music score is the desired output (bottom)</figcaption></figure><p id="87d0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Before installing GraphMuse you will need to install PyTorch and PyTorch Geometric. Check out the appropriate version for your system <a class="af qj" href="https://pytorch.org/get-started/locally/" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr"><em class="px">here</em></strong></a> and <a class="af qj" href="https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html" rel="noopener ugc nofollow" target="_blank"><strong class="ne fr"><em class="px">here</em></strong></a>.</p><p id="a4e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After this step, to install GraphMuse open your preferred terminal and type:</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="6623" class="qo ox fq ql b bg qp qq l qr qs">pip install graphmuse</span></pre><p id="e40a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After installation, let's read a MIDI file from a URL and create the score graph with GraphMuse.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="9335" class="qo ox fq ql b bg qp qq l qr qs">import graphmuse as gm<br/><br/>midi_url_raw = "https://github.com/CPJKU/partitura/raw/refs/heads/main/tests/data/midi/bach_midi_score.mid"<br/>graph = gm.load_midi_to_graph(midi_url_raw)</span></pre><p id="75a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The underlying process reads the file with Partitura and then feeds it through GraphMuse.</p><p id="7c4b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To train our model to handle Pitch Spelling, we first need a dataset of musical scores where the pitch spelling has already been annotated. For this, we’ll be using the ASAP Dataset (licenced under <a class="af qj" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank">CC BY-NC-SA 4.0</a>), which will serve as the foundation for our model’s learning. To get the ASAP Dataset you can download it using git or <a class="af qj" href="https://github.com/cpjku/asap-dataset" rel="noopener ugc nofollow" target="_blank">directly from github</a>:</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="b15e" class="qo ox fq ql b bg qp qq l qr qs">git clone https://github.com/cpjku/asap-dataset.git</span></pre><p id="4a33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The ASAP dataset includes scores and performances of various classical piano pieces. For our use-case we will use only the scores which end in <code class="cx qt qu qv ql b">.musicxml</code>.</p><p id="eccd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As we load this dataset, we’ll need two essential utilities: one to encode pitch spelling and another to handle key signature information, both of which will be converted into numerical labels. Fortunately, these utilities are available within the pre-built pitch spelling model in GraphMuse. Let’s begin by importing all the necessary packages and loading the first score to get started.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="3489" class="qo ox fq ql b bg qp qq l qr qs">import graphmuse as gm<br/>import partitura as pt<br/>import os<br/>import torch<br/>import numpy as np<br/><br/># Directory containing the dataset, change this to the location of your dataset<br/>dataset_dir = "/your/path/to/the/asap-dataset"<br/><br/># Find all the score files in the dataset (they are all named 'xml_score.musicxml')<br/>score_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(dataset_dir) for f in filenames if f == 'xml_score.musicxml']<br/><br/># Use the first 30 scores, change this number to use more or less scores<br/>score_files = score_files[:30]<br/><br/># probe the first score file<br/>score = pt.load_score(score_files[0])<br/># Extract features and note array<br/>features, f_names = gm.utils.get_score_features(score)<br/>na = score.note_array(include_pitch_spelling=True, include_key_signature=True)<br/># Create a graph from the score features<br/>graph = gm.create_score_graph(features, score.note_array())<br/><br/># Get input feature size and metadata from the first graph<br/>in_feats = graph["note"].x.shape[1]<br/>metadata = graph.metadata()<br/><br/># Create a model for pitch spelling prediction<br/>model = gm.nn.models.PitchSpellingGNN(<br/>    in_feats=in_feats, n_hidden=128, out_feats_enc=64, n_layers=2, metadata=metadata, add_seq=True<br/>)<br/><br/># Create encoders for pitch and key signature labels<br/>pe = model.pitch_label_encoder<br/>ke = model.key_label_encoder</span></pre><p id="2849" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, we’ll load the remaining score files from the dataset to continue preparing our data for model training.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="a079" class="qo ox fq ql b bg qp qq l qr qs"># Initialize lists to store graphs and encoders<br/>graphs = []<br/><br/># Process each score file<br/>for score_file in score_files:<br/>    # Load the score<br/>    score = pt.load_score(score_file)<br/><br/>    # Extract features and note array<br/>    features, f_names = gm.utils.get_score_features(score)<br/>    na = score.note_array(include_pitch_spelling=True, include_key_signature=True)<br/><br/>    # Encode pitch and key signature labels<br/>    labels_pitch = pe.encode(na)<br/>    labels_key = ke.encode(na)<br/><br/>    # Create a graph from the score features<br/>    graph = gm.create_score_graph(features, score.note_array())<br/><br/>    # Add encoded labels to the graph<br/>    graph["note"].y_pitch = torch.from_numpy(labels_pitch).long()<br/>    graph["note"].y_key = torch.from_numpy(labels_key).long()<br/><br/>    # Append the graph to the list<br/>    graphs.append(graph)<br/></span></pre><p id="5a0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once the graph structures are ready, we can move on to creating the data loader, which is conveniently provided by GraphMuse. At this stage, we’ll also define standard training components like the loss function and optimizer to guide the learning process.</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="8f83" class="qo ox fq ql b bg qp qq l qr qs"># Create a DataLoader to sample subgraphs from the graphs<br/>loader = gm.loader.MuseNeighborLoader(graphs, subgraph_size=100, batch_size=16, num_neighbors=[3, 3])<br/><br/># Define loss functions for pitch and key prediction<br/>loss_pitch = torch.nn.CrossEntropyLoss()<br/>loss_key = torch.nn.CrossEntropyLoss()<br/><br/># Define the optimizer<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)</span></pre><p id="e70f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let me comment a bit more on the <em class="px">gm.loader.MuseNeighborLoader.<br/></em>This is the core dataloader in GraphMuse and it contains the sampling that was explained in the previous section. <em class="px">subgraph_size </em>refers to the number of target nodes per input graph, <em class="px">batch_size </em>is the number of sampled graphs per batch, and finally, <em class="px">num_neighbors </em>refers to the number of neighbors sampled per sampled node in each layer.</p><p id="88de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">With everything in place, we are finally ready to train the model. So, let’s dive in and start the training process!</p><pre class="mm mn mo mp mq qk ql qm bp qn bb bk"><span id="9f20" class="qo ox fq ql b bg qp qq l qr qs"><br/># Train the model for 5 epochs<br/>for epoch in range(5):<br/>    loss = 0<br/>    i = 0<br/>    for batch in loader:<br/>        # Zero the gradients<br/>        optimizer.zero_grad()<br/><br/>        # Get neighbor masks for nodes and edges for more efficient training<br/>        neighbor_mask_node = {k: batch[k].neighbor_mask for k in batch.node_types}<br/>        neighbor_mask_edge = {k: batch[k].neighbor_mask for k in batch.edge_types}<br/><br/>        # Forward pass through the model<br/>        pred_pitch, pred_key = model(<br/>            batch.x_dict, batch.edge_index_dict, neighbor_mask_node, neighbor_mask_edge,<br/>            batch["note"].batch[batch["note"].neighbor_mask == 0]<br/>        )<br/><br/>        # Compute loss for pitch and key prediction<br/>        loss_pitch_val = loss_pitch(pred_pitch, batch["note"].y_pitch[batch["note"].neighbor_mask == 0])<br/>        loss_key_val = loss_key(pred_key, batch["note"].y_key[batch["note"].neighbor_mask == 0])<br/><br/>        # Total loss<br/>        loss_val = loss_pitch_val + loss_key_val<br/><br/>        # Backward pass and optimization<br/>        loss_val.backward()<br/>        optimizer.step()<br/><br/>        # Accumulate loss<br/>        loss += loss_val.item()<br/>        i += 1<br/><br/>    # Print average loss for the epoch<br/>    print(f"Epoch {epoch} Loss {loss / i}")</span></pre><p id="267a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Hopefully, we’ll soon see the loss function decreasing, a positive sign that our model is effectively learning how to perform pitch spelling. Fingers crossed!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qw"><img src="../Images/f267c41b5f58259804aa4c0806642c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*-7vH_uBVk1jWhvhrIx3-5w.png"/></div></figure><h1 id="cd76" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Why GraphMuse?</h1><p id="2a39" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">GraphMuse is a framework that tries to make the training and deployment of graph models for symbolic music processing easier.</p><p id="9e31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For those who want to retrain, deploy, or finetune previous state-of-the-art models for symbolic music analysis, GraphMuse contains some of the necessary components to re-build and re-train your model faster and more efficiently.</p><p id="8723" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GraphMuse retains its flexibility through its simplicity, for those who want to prototype, innovate, and design new models. It aims to provide a simple set of utilities rather than including complex chained pipelines that can block the innovation process.</p><p id="a90e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For those who want to learn, visualize, and get hands-on experience, GraphMuse is good to get you started. It offers an easy introduction to basic functions and pipelines with a few lines of code. GraphMuse is also linked with <a class="af qj" href="https://github.com/fosfrancesco/musgviz/" rel="noopener ugc nofollow" target="_blank">MusGViz</a>, which allows graphs and scores to be easily visualized together.</p><h1 id="3e60" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Limitations and Future Plans</h1><p id="99e6" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">We cannot talk about the positive aspects of any project without discussing the negative ones as well.</p><p id="a245" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">GraphMuse is a newborn project and in its current state, it is pretty simple. It is focused on covering the essential parts of graph learning rather than being a holistic framework that covers all possibilities. Therefore it still focuses a lot on user-based implementation on many parts of the aforementioned pipeline.</p><p id="4eb6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Like every open-source project in development GraphMuse needs help to grow. So please, if you find bugs or want more features do not hesitate to report, request, or contribute to the GraphMuse GitHub project.</p><p id="3cc9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Last but not least, GraphMuse uses C libraries such as torch-sparse and torch-scatter and has its own C-bindings to accelerate graph creation therefore installation is not always straightforward. The windows installation is more challenging judging from our user testing and user interaction reports, although not impossible (I am running it on Windows myself).</p><p id="a501" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Future plans include:</p><ul class=""><li id="1f7e" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx oo op oq bk">Making installation easier;</li><li id="20b1" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx oo op oq bk">Add more support for models and dataloaders for precise tasks;</li><li id="47f0" class="nc nd fq ne b go or ng nh gr os nj nk nl ot nn no np ou nr ns nt ov nv nw nx oo op oq bk">Grow the open-source community around GraphMuse to keep graph coding for music growing.</li></ul><h1 id="d3e5" class="ow ox fq bf oy oz pa gq pb pc pd gt pe pf pg ph pi pj pk pl pm pn po pp pq pr bk">Conclusion</h1><p id="815e" class="pw-post-body-paragraph nc nd fq ne b go ps ng nh gr pt nj nk nl pu nn no np pv nr ns nt pw nv nw nx fj bk">GraphMuse is a Python library that makes working with music graphs a little bit easier. It focuses on the training aspect of graph-based models for music but aims to retain flexibility when research-based projects require it.</p><p id="2275" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If you would like to support the development and future growth of GraphMuse please star the repo <a class="af qj" href="https://github.com/manoskary/graphmuse" rel="noopener ugc nofollow" target="_blank">here</a> .</p><p id="e45f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Happy graph coding !!!</p><div class="qx qy qz ra rb rc"><a href="https://github.com/manoskary/graphmuse?source=post_page-----40dbd9baf319--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="rd ab ig"><div class="re ab co cb rf rg"><h2 class="bf fr hw z io rh iq ir ri it iv fp bk">GitHub - manoskary/graphmuse: A Graph Deep Learning Library for Music.</h2><div class="rj l"><h3 class="bf b hw z io rh iq ir ri it iv dx">A Graph Deep Learning Library for Music. Contribute to manoskary/graphmuse development by creating an account on…</h3></div><div class="rk l"><p class="bf b dy z io rh iq ir ri it iv dx">github.com</p></div></div><div class="rl l"><div class="rm l rn ro rp rl rq lr rc"/></div></div></a></div><p id="f9b3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[all images are by the author]</p></div></div></div></div>    
</body>
</html>