- en: Why LLMs are not Good for Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llms-coding-chatgpt-python-artificial-intelligence-4ea7a7bbdd93?source=collection_archive---------0-----------------------#2024-02-28](https://towardsdatascience.com/llms-coding-chatgpt-python-artificial-intelligence-4ea7a7bbdd93?source=collection_archive---------0-----------------------#2024-02-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Challenges of Using LLMs for Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page---byline--4ea7a7bbdd93--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page---byline--4ea7a7bbdd93--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4ea7a7bbdd93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4ea7a7bbdd93--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page---byline--4ea7a7bbdd93--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4ea7a7bbdd93--------------------------------)
    ·7 min read·Feb 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf79be70b4ffbf6664f9d4ed83713a50.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made image
  prefs: []
  type: TYPE_NORMAL
- en: Over the past year, Large Language Models (LLMs) have demonstrated astonishing
    capabilities thanks to their natural language understanding. These advanced models
    have not only redefined the standards in Natural Language Processing but also
    populated applications and services.
  prefs: []
  type: TYPE_NORMAL
- en: There has been a rapidly growing interest in using LLMs for coding, with **some
    companies striving to turn natural language processing into code understanding
    and generation**. This task has already highlighted several challenges yet to
    be addressed in using LLMs for coding. Despite these obstacles, this trend has
    led to the development of AI code generator products.
  prefs: []
  type: TYPE_NORMAL
- en: '*Have you ever used ChatGPT for coding?*'
  prefs: []
  type: TYPE_NORMAL
- en: While it can be helpful in some instances, **it often struggles to generate
    efficient and high-quality code.** In this article, we will explore three reasons
    why LLMs are not inherently proficient at coding *“out of the box”:* the tokenizer,
    the complexity of context windows when applied to code and the nature of the training
    itself .
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify the key areas that need improvement is crutial to transform LLMs
    into more effective coding assistants!**'
  prefs: []
  type: TYPE_NORMAL
- en: '#1 LLM Tokenizer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
