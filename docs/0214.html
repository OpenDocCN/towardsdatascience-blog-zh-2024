<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AI for Groups: Build a Multi-User Chat Assistant Using 7B-Class Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AI for Groups: Build a Multi-User Chat Assistant Using 7B-Class Models</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22">https://towardsdatascience.com/ai-for-groups-build-a-multi-user-chat-assistant-using-7b-class-models-7071ca8b4aa0?source=collection_archive---------6-----------------------#2024-01-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c2d4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Have you ever wanted to build an assistant that knows when to talk and when to remain silent? Learn how to do it using open-source models.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jan Jezabek, Ph.D." class="l ep by dd de cx" src="../Images/eb78f321cc347a9e619b2c474e7bc192.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*bMz-eeuTk7Vj2Dh9To3EWw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jjezabek?source=post_page---byline--7071ca8b4aa0--------------------------------" rel="noopener follow">Jan Jezabek, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7071ca8b4aa0--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="6982" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Intelligent chat assistants have become a central application made possible by the recent generative AI progress, with ChatGPT and Bing Chat/Copilot becoming household names. Typically, this takes the form of a back and forth between a user, who provides prompts or instructions, and an assistant, who in turn provides responses.</p><p id="8d77" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A scenario that has received comparatively less attention is one in which an assistant is a semi-active participant in a conversation between two or more users. Examples of such interactions are conversations between groups of friends planning activities together — with the assistant providing recommendations when applicable and staying silent otherwise — or customer support chats, with the assistant providing suggestions to the customer service representative. In these cases, the assistant is not expected to respond at every turn: It would be awkward if it regularly barged in during casual chit-chat between friends.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/2ae3c2938854108e952881ce1da19d1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGqFYf20cH-mBz-FJrgOXA.jpeg"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">(Image credit: DALL-E 3 with post-processing by the author to remove extra fingers)</figcaption></figure><p id="a421" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this series I’ll go through the steps needed to build a lightweight assistant for this purpose using open-source LLMs. In this context “lightweight” means a model that requires 16GB and 8GB of GPU RAM for training and inference respectively, and that it can efficiently run on a CPU if needed. For this purpose, I will be using Llama-2-7b-hf-chat, Zephyr-7b-beta, and OpenChat-3.5-0106, which all fit this description.</p><h1 id="4c47" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">ChatGPT-3.5-Turbo Baseline</h1><p id="8020" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">To get a feeling for the task we’ll first implement it using ChatGPT. This will give us a reference point from a strong model and will give us an estimate of the task’s difficulty.</p><p id="810e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s think about some of the unique aspects of our use case:</p><ul class=""><li id="0ac3" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oz pa pb bk">We don’t want the assistant to be overzealous: It should only chime in if asked directly or if it has some interesting trivia to add. To this end the assistant needs the possibility to remain silent.</li><li id="5ffe" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne oz pa pb bk">There are multiple human users in the conversation. To make sense of it, we need to indicate which user is the speaker for each chat message.</li></ul><p id="e709" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the first aspect we need to define the mechanism for when the assistant chooses to remain silent. To achieve this, we’ll instruct the model to return “(silence)” as its response. Such a prediction can then be filtered during post-processing. An alternative is to ask the model to return an empty prediction, but anecdotally this seems not to be working reliably with some models (they are not used to staying silent!).</p><p id="dcbc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the second aspect, OpenAI’s API conveniently lets us provide the name of the participant for each message in the conversation (curiously this functionality is not exposed in the <a class="af ph" href="https://platform.openai.com/playground" rel="noopener ugc nofollow" target="_blank">Playground</a>). This is unfortunately not true for the common open-source models (where we will need a workaround), but for ChatGPT we should be fine.</p><p id="7914" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This leaves one more crucial decision: The prompt. For our use case I’m deliberately picking something short and precise (it can always be adjusted if the tone of the responses ends up being off):</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="46e4" class="pm nz fq pj b bg pn po l pp pq">You are an assistant in a group conversation between multiple users.<br/>Your task is to help with relevant information or when directly asked.<br/>Do not be overzealous. If you do not have anything important to say,<br/>respond with "(silence)".</span></pre><p id="53ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We now have everything we need, let’s give it a try. Using a chat loop as implemented in <a class="af ph" href="https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link" rel="noopener ugc nofollow" target="_blank">this notebook</a>, we get the following conversation:</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="0c2c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The initial results are encouraging if not perfect: The assistant occasionally chooses to remain silent (adhering to the format from the instructions) or chimes in with helpful information, but it also sometimes responds with unnecessary chit-chat. Changing the prompt to:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="bd95" class="pm nz fq pj b bg pn po l pp pq">You are an assistant in a group conversation between multiple users.<br/>Your task is to help with relevant information or when you are directly<br/>addressed as "assistant". Do not be overzealous, remember that most of<br/>the time the users will be speaking to each other, not to you. If you<br/>do not have anything important to say, respond with "(silence)".</span></pre><p id="3555" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">and inserting this reminder system message after every user message:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="1f3b" class="pm nz fq pj b bg pn po l pp pq">Remember that the users are most likely to be speaking to each other,<br/>not to you. If you do not have anything important to say, respond with<br/>"(silence)".</span></pre><p id="8337" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">does not seem to make a big difference, as seen in this conversation:</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="202d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It’s likely that the model’s performance can be improved significantly with more work on the prompt, but for now this is sufficient for our purposes: We have a baseline to compare against and we also get an indication that the problem is tractable, if not trivial.</p><h2 id="5c88" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Open-Source Models and Finetuning</h2><p id="004a" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">We’ve seen that despite some hiccups, ChatGPT-3.5-Turbo is able to act as a semi-active participant in a group conversation. The same is unfortunately not true for common open-source models in the 7B parameter class, which end up responding at every turn. Fortunately, the great thing about open-source LLMs is that we can adapt them to our task via finetuning.</p><p id="f937" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is worth pointing out that finetuning is not applicable to every situation. For example, if you want to teach a model new facts, finetuning will not be the right tool (a better approach is Retrieval Augmented Generation). However, if you want to alter the tone or format of the responses (as we do here), finetuning is just the thing you need.</p><h2 id="e522" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Dataset Generation</h2><p id="2068" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">A critical thing to decide for finetuning is the dataset. We’ll need to provide a set of good examples of multi-user conversations where an assistant largely remains silent, but occasionally chimes in with helpful information. To quickly bootstrap such a set, I enrolled the help of Mixtral-8x7B-Instruct-v0.1, hosted on replicate.com. Specifically, I generated 50 synthetic conversations using this prompt (along with some variations in the topic of discussion and participant names, see <a class="af ph" href="https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link" rel="noopener ugc nofollow" target="_blank">this notebook</a> for details):</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="4241" class="pm nz fq pj b bg pn po l pp pq">Generate a conversation representing a chat between two users.<br/>The users are Cynthia and Fred and they are discussing potential<br/>Christmas gifts for friends. An assistant chimes in when it can fill<br/>in trivia, otherwise it remains silent. The conversation should have<br/>between 10 and 12 turns. Return the conversation in a JSON format,<br/>like this:<br/><br/>[<br/>  {<br/>    "role": "user",<br/>    "name": "Alice",<br/>    "content": "Hi Grace! How are you?"<br/>  },<br/>  {<br/>    "role": "user",<br/>    "name": "Grace",<br/>    "content": "I'm good, how about you?"<br/>  },<br/>  {<br/>    "role": "user",<br/>    "name": "Alice",<br/>    "content": "Doing fine as well. I've been reading a book by the author of the Da Vinci code. Sorry, forgot his name"<br/>  },<br/>  {<br/>    "role": "assistant",<br/>    "content": "That’s Dan Brown! He also authored a few other books, for example \"Angels &amp; Demons\" and \"Inferno\"."<br/>  }<br/>]</span></pre><p id="cb92" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Obviously, the result is not a high quality, curated dataset, so using it for a production model is not recommended. I will discuss some ways to improve the dataset’s quality, as well as approaches for evaluating the resultant model in a subsequent article. However, the dataset is good enough for our purpose right now, that is to validate that a small model can be adapted for the purpose of a multi-user chat assistant.</p><p id="90ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The dataset generation notebook is available here, and the generated dataset was uploaded to <a class="af ph" href="https://huggingface.co/jjezabek/multi_user_chat_synthetic" rel="noopener ugc nofollow" target="_blank">this HuggingFace repository</a>. Below is an example generated dialog:</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><h2 id="69b0" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">A Note About Chat Templates</h2><p id="8840" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">When using a pretrained chat model, it is a good idea to ensure that the format of your input matches the one that the model had been trained with. This has become a bit easier with HuggingFace in September 2023 with the introduction of the <em class="ql">apply_chat_template</em> method of the tokenizer. This method takes care of formatting the various user, system and assistant prompts and responses into the required format expected by the model.</p><p id="afde" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Unfortunately, not all models have been updated to have a chat template, so I recommend inspecting the output from <em class="ql">apply_chat_template</em> for each model and comparing it to the model’s documentation.</p><p id="a251" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the context of finetuning (as opposed to just using on off-the-shelf model for inference) we don’t necessarily have to follow a prescribed format. In fact, for non-chat models defining your own chat template is a necessity. However, for chat models sticking with the existing chat template is likely to make the finetuning task easier, resulting in fewer training steps and a smaller possibility of unwanted side effects (think catastrophic forgetting).</p><p id="7040" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For the models we’ve chosen, Zephyr, Llama-7b-chat, and OpenChat-3.5, we are in luck: All of them have their chat templates defined correctly and <em class="ql">apply_chat_template</em> works as expected.</p><h1 id="ad90" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Finetuning</h1><p id="2f75" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">We are now ready to kick off the finetuning. As mentioned before, the goal is to fit the training into 16GB of GPU memory, allowing it to run on a single T4 GPU (no need to hunt for the ultra-rare Pokémon… err, I mean A100s). To achieve this, we’ll use 4-bit quantization and LoRA. If you’re unfamiliar with these terms, I highly recommend <a class="af ph" href="https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora" rel="noopener ugc nofollow" target="_blank">this article</a> as an introduction. This section will go through the main steps needed for finetuning, the complete training notebook can be accessed <a class="af ph" href="https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="0f12" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Before starting training, we need to slightly massage the synthetic dataset created earlier:</p><ul class=""><li id="576b" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oz pa pb bk">We need to add information about who the speaker is in each user turn. Remember the helpful <em class="ql">name</em> field in OpenAI’s API that allowed us to differentiate between various human speakers? It’s sadly not present in Zephyr’s, Llama’s and OpenChat’s chat templates. As a workaround we will just prepend “{name}: ” at the start of each line.</li><li id="ef58" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne oz pa pb bk">We also need to add assistant lines saying “(silence)” every time the assistant chooses not to respond in a turn. In addition, we will also prepend “(response)” before each assistant line. This is not strictly necessary for the basic chat case but will allow us to cajole the model into answering even if it preferred to remain silent (this will come handy during evaluation but can also be a product feature).</li><li id="d8ab" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne oz pa pb bk">Finally, we also need to apply the chat template.</li></ul><p id="d730" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The dataset preprocessing is implemented as follows:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="ebf2" class="pm nz fq pj b bg pn po l pp pq">from transformers import AutoTokenizer<br/><br/>tokenizer = AutoTokenizer.from_pretrained(HF_BASE_MODEL_NAME, use_fast=False)</span></pre><pre class="qm pi pj pk bp pl bb bk"><span id="0054" class="pm nz fq pj b bg pn po l pp pq">from datasets import Dataset<br/>from huggingface_hub import hf_hub_download<br/>import json<br/><br/>def build_dataset():<br/>    local_filename = hf_hub_download(<br/>        repo_id=HF_DATASET_NAME,<br/>        filename=HF_DATA_FILE_NAME<br/>    )<br/>    with open(local_filename) as f:<br/>        conversations = f.readlines()<br/>        result = []<br/>        for conversation in conversations:<br/>            lines = json.loads(conversation)<br/>            transformed_lines = []<br/><br/>            idx = 0<br/>            while idx &lt; len(lines):<br/>                assert lines[idx]['role'] == 'user'<br/>                transformed_lines.append({<br/>                    'role': 'user',<br/>                    'content': f"{lines[idx]['name']}: {lines[idx]['content']}",<br/>                })<br/><br/>                idx += 1<br/><br/>                if idx == len(lines) or lines[idx]['role'] != 'assistant':<br/>                    # Insert artificial (silence) response<br/>                    transformed_lines.append({<br/>                        'role': 'assistant',<br/>                        'content': '(silence)',<br/>                    })<br/>                else:<br/>                    transformed_lines.append({<br/>                        'role': 'assistant',<br/>                        'content': f"(response) {lines[idx]['content']}",<br/>                    })<br/>                    idx += 1<br/><br/>            result_row = {<br/>                'text': tokenizer.apply_chat_template(tokenize=False, conversation=transformed_lines)<br/>            }<br/>            result.append(result_row)<br/><br/>    return result<br/><br/>dataset = Dataset.from_list(build_dataset())</span></pre><p id="3262" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that no system prompt is included. The reason is that we’re finetuning a model for this one specific task, so providing the instructions to the model is redundant: It learns what it is supposed to do from its training. This has the nice side effect of both shorter training and slightly quicker inference.</p><p id="19a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Having finished preparing the dataset, we now load the quantized model:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="6ac2" class="pm nz fq pj b bg pn po l pp pq">import torch<br/>from transformers import AutoModelForCausalLM<br/><br/>torch_compute_type = torch.bfloat16 if USE_BFLOAT16 else torch.float16<br/><br/>model = AutoModelForCausalLM.from_pretrained(<br/>    active_config['base_model_name'],<br/>    torch_dtype=torch_compute_type,<br/>    bnb_4bit_quant_type='nf4',<br/>    bnb_4bit_compute_dtype=torch_compute_type,<br/>    load_in_4bit=True,<br/>    device_map={'':0},<br/>    trust_remote_code=True,<br/>    use_cache=True<br/>)</span></pre><p id="3eeb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We then define the adapter model (i.e. the low rank “diff” from the base model):</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="8f16" class="pm nz fq pj b bg pn po l pp pq">from peft import LoraConfig, get_peft_model<br/><br/>peft_config = LoraConfig(<br/>        lora_alpha=16,<br/>        lora_dropout=0.1,<br/>        r=64,<br/>        bias="none",<br/>        task_type="CAUSAL_LM",<br/>)<br/><br/># Note: This is needed for Zephyr, otherwise we get this:<br/>#       RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn<br/>model.enable_input_require_grads()<br/>peft_model = get_peft_model(model, peft_config)</span></pre><p id="8a50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">and instantiate the trainer and the training arguments:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="6c89" class="pm nz fq pj b bg pn po l pp pq">from transformers import TrainingArguments<br/><br/>output_dir = "peft_model"<br/><br/># These arguments (LR, gradient norm, etc.) seem to be fairly frequently<br/># used for QLoRA. Default arguments work too, but require about 50% more<br/># epochs. Also tried optim='lion_32bit' out of curiosity, the result was<br/># pretty much the same as the default (AdamW), but each epoch was 30-40%<br/># slower.<br/>training_args = TrainingArguments(<br/>    output_dir=output_dir,<br/>    num_train_epochs=TRAIN_EPOCHS,<br/>    per_device_train_batch_size=4,<br/>    gradient_accumulation_steps=2,<br/>    gradient_checkpointing=True,<br/>    logging_steps=1,<br/>    bf16=USE_BFLOAT16,<br/>    #optim='lion_32bit',<br/>    learning_rate=2e-4,<br/>    max_grad_norm=0.3,<br/>    warmup_ratio=0.03,<br/>    lr_scheduler_type="constant",<br/>)</span></pre><p id="ab77" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The settings used above are fairly standard (and I encourage you to tweak them as needed). The ones that really matter are the number of epochs, the learning rate, and the batch size. The above is a particular configuration that worked for me and might be a good starting point but is obviously not a substitute for a real hyperparameter search.</p><p id="3920" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We are now ready to instantiate the trainer and kick off the training:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="6082" class="pm nz fq pj b bg pn po l pp pq">from trl import SFTTrainer<br/><br/>max_seq_length = 1024<br/><br/>trainer = SFTTrainer(<br/>    model=peft_model,<br/>    train_dataset=dataset,<br/>    peft_config=peft_config,<br/>    max_seq_length=max_seq_length,<br/>    tokenizer=tokenizer,<br/>    args=training_args,<br/>    dataset_text_field='text',<br/>)</span></pre><pre class="qm pi pj pk bp pl bb bk"><span id="29ba" class="pm nz fq pj b bg pn po l pp pq">trainer.train()</span></pre><p id="19f9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That was quick, just 8 minutes on a T4! Let’s test how it does by creating a conversational pipeline and a loop, using the same <a class="af ph" href="https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link" rel="noopener ugc nofollow" target="_blank">notebook</a> as for the OpenAI API case. Here is an example conversation using a model finetuned from OpenChat-3.5–0106:</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="a33d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is pretty encouraging: The model follows our format requirements and seems to make reasonable decisions on when to chime in and when to remain silent.</p><p id="3043" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So — are we done? One thing to note about the training is that the model is taught to predict all of the tokens in each sample, including the user messages and any special tokens. The following section will show how this can be suppressed.</p><h2 id="a1a6" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Training on Completions Only</h2><p id="1dff" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">First things first: Why do we even care about not teaching the model to predict the user messages? One argument can be made on the grounds of privacy: If real conversations are used as training data, a model could possibly be persuaded by an end user to leak some of the user messages (for what it’s worth, assistant responses can contain sensitive information as well). A second argument is that trying to predict user messages is unnecessary, and as a result wasteful. This can mean that you will need to train for a longer time to get good results, and hence risk unwanted side effects (again, this is chiefly catastrophic forgetting).</p><p id="1768" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Depending on your use case both of these arguments might be moot, and the model might do well with the training procedure described above. If, however, it’s not, or if you are just curious, I encourage you to keep reading.</p><p id="06d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">HuggingFace’s <em class="ql">trl </em>library provides us with a tool to solve this particular problem, implemented as <em class="ql">DataCollatorForCompletionsOnlyLM</em>. This collator changes the labels for the tokens representing user messages to an “ignore” label, meaning the models are not trained to predict them. The user messages are of course still used as context for predicting assistant messages.</p><p id="8c5e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ql">DataCollatorForCompletionsOnlyLM</em> requires us to pass two strings that it can use to find the start of the user messages (the <em class="ql">instruction_template</em> parameter) and the assistant messages (<em class="ql">response_template</em>). We can find them by inspecting the output of <em class="ql">apply_chat_template</em>: In the case of Zephyr, they are “&lt;|user|&gt;” and “&lt;|assistant|&gt;”, for Llama they are “[INST]” and “[/INST]”. Let’s try it out:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="e5e6" class="pm nz fq pj b bg pn po l pp pq">trainer.data_collator = DataCollatorForCompletionOnlyLM(<br/>    response_template="&lt;|assistant|&gt;",<br/>    instruction_template="&lt;|user|&gt;",<br/>    tokenizer=tokenizer<br/>)<br/><br/>trainer.train()<br/><br/>### Output:<br/># UserWarning: Could not find response key `&lt;|assistant|&gt;` in the following instance: [...] This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.</span></pre><p id="4d24" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Uh oh, this looks bad. Essentially the trainer cannot find our template fragments and as a result ignores all our samples. The reason for this is explained in <a class="af ph" href="https://huggingface.co/docs/trl/sft_trainer#using-tokenids-directly-for-responsetemplate" rel="noopener ugc nofollow" target="_blank">this article</a>: Depending on the preceding context, a string like “&lt;|user|&gt;” can have different tokenized representations. Fortunately, <em class="ql">DataCollatorForCompletionsOnlyLM </em>allows us to pass the tokenized versions of these delimiter strings instead of the literal ones. In order to find these tokenized versions, we can inspect the tokenized output of a chat template:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="5130" class="pm nz fq pj b bg pn po l pp pq">conversation = [<br/>  { 'role': 'user', 'content': "hi!" },<br/>  { 'role': 'assistant', 'content': "Hello!" }<br/>]<br/><br/>for token in tokenizer.apply_chat_template(conversation):<br/>    print(f"Token Id: {token}, Value: '{tokenizer.decode([token])}'")<br/><br/>### Output<br/># Token Id: 523, Value: '&lt;'<br/># Token Id: 28766, Value: '|'<br/># Token Id: 1838, Value: 'user'<br/># Token Id: 28766, Value: '|'<br/># Token Id: 28767, Value: '&gt;'<br/># Token Id: 13, Value: '<br/># '<br/># Token Id: 5365, Value: 'hi'<br/># Token Id: 28808, Value: '!'<br/># Token Id: 2, Value: '&lt;/s&gt;'<br/># Token Id: 28705, Value: ''<br/># Token Id: 13, Value: '<br/># '<br/># Token Id: 28789, Value: '&lt;'<br/># Token Id: 28766, Value: '|'<br/># Token Id: 489, Value: 'ass'<br/># Token Id: 11143, Value: 'istant'<br/># Token Id: 28766, Value: '|'<br/># Token Id: 28767, Value: '&gt;'<br/># Token Id: 13, Value: '<br/># '<br/># Token Id: 16230, Value: 'Hello'<br/># Token Id: 28808, Value: '!'<br/># Token Id: 2, Value: '&lt;/s&gt;'<br/># Token Id: 28705, Value: ''<br/># Token Id: 13, Value: '<br/># '</span></pre><p id="7938" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">From the output we can infer that “&lt;|assistant|&gt;” is tokenized as [28789, 28766, 489, 11143, 28766, 28767], and “&lt;|user|&gt;” is tokenized as [28789, 28766, 1838, 28766, 28767]. I have included the tokenized sequences for a few common models in the table below.</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="5f43" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">With this in hand, we can now retry training using the updated data collator:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="66f8" class="pm nz fq pj b bg pn po l pp pq">response_template = [28789, 28766, 489, 11143, 28766, 28767]<br/>instruction_template = [28789, 28766, 1838, 28766, 28767]<br/><br/>trainer.data_collator = DataCollatorForCompletionOnlyLM(<br/>    response_template=response_template,<br/>    instruction_template=instruction_template,<br/>    tokenizer=tokenizer<br/>)<br/><br/>trainer.train()</span></pre><p id="7632" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This gets rid of the warning and the training loss starts decreasing. We can now wait for the model training to finish and upload the model to HuggingFace Hub.</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="1bc5" class="pm nz fq pj b bg pn po l pp pq">peft_model.push_to_hub(active_config['finetuned_model_name'])<br/>tokenizer.push_to_hub(active_config['finetuned_model_name'])</span></pre><h2 id="c82d" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Smoke Testing</h2><p id="6636" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">Let’s now see how the model is doing in practice by running <a class="af ph" href="https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link" rel="noopener ugc nofollow" target="_blank">this notebook</a> (which can be executed locally using a consumer grade 8GB GPU). Here is an example conversation, again for a model finetuned from OpenChat-3.5–0106:</p><figure class="ni nj nk nl nm nn"><div class="pr io l ed"><div class="ps pt l"/></div></figure><p id="5585" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So — are we done now? This depends on the goal: We do have a model that I like to call “syntactically competent”, meaning that it follows our defined format and is able to decide when to talk and when to remain silent. If the goal is a toy assistant, this might be sufficient. However, for any serious production use, there is still a fair amount of work to do, which I’ll discuss in subsequent articles.</p><h2 id="0494" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Follow-ups</h2><p id="5848" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">Let’s list some of the things that are worth consideration as follow-up steps:</p><ul class=""><li id="8d40" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne oz pa pb bk"><strong class="ml fr">High quality training set:</strong> So far, we have only used a synthetic training set generated by Mixtral. This set does not have too much variation and may contain falsehoods. It was useful for bootstrapping but is insufficient for production use.</li><li id="d472" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne oz pa pb bk"><strong class="ml fr">Evaluation:</strong> So far, we’ve only done a few smoke tests, but we don’t have a good grasp of how the model is performing: Is it responding truthfully, is it doing a good job in determining when to chime in? We also don’t know how much the finetuned model diverged from the base one. In a follow-up article I’ll show how to shed some light on these questions.</li><li id="67f6" class="mj mk fq ml b go pc mn mo gr pd mq mr ms pe mu mv mw pf my mz na pg nc nd ne oz pa pb bk"><strong class="ml fr">Context:</strong> We cannot expect a model with just 7B parameters to be knowledgeable on every topic. In fact, for practical purposes, we may want to constrain the model to particular topics relevant to our product. To this end, we may want to provide contextual information to our model that is relevant to the users’ questions and condition the model to only answer based on this information. This approach is known as Retrieval Augmented Generation (RAG), and I’ll show how it can be applied in our multi-user setting.</li></ul><h1 id="560e" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Resources and Artifacts</h1><p id="aae7" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">The notebooks used for training and evaluation are available on Colab: <a class="af ph" href="https://colab.research.google.com/drive/1JWAK3ecaO4EjUqIk1LBIfGQ8zV4oWmDk?usp=drive_link" rel="noopener ugc nofollow" target="_blank">Dataset generation</a>, <a class="af ph" href="https://colab.research.google.com/drive/1MDXvw8ie3XnpR0OujFhwLgTU0XIeepSv?usp=drive_link" rel="noopener ugc nofollow" target="_blank">training</a> and <a class="af ph" href="https://colab.research.google.com/drive/18Lsn3ws5H3jCEJ-loLmuL4_195BOEEoa?usp=drive_link" rel="noopener ugc nofollow" target="_blank">inference</a>.</p><p id="9f3e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The synthetic dataset is available <a class="af ph" href="https://huggingface.co/jjezabek/multi_user_chat_synthetic" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="1ad9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, the models are available on HuggingFace, finetuned from <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-completions-only" rel="noopener ugc nofollow" target="_blank">Zephyr</a>, <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-completions-only" rel="noopener ugc nofollow" target="_blank">Llama-2</a> and <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-completions-only" rel="noopener ugc nofollow" target="_blank">OpenChat-3.5</a>. If you are interested in the models trained on whole conversations (as opposed to completions only), they are available as well, finetuned from <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-zephyr-7b-beta-full-conversations" rel="noopener ugc nofollow" target="_blank">Zephyr</a>, <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-llama-2-7b-chat-full-conversations" rel="noopener ugc nofollow" target="_blank">Llama-2</a> and <a class="af ph" href="https://huggingface.co/jjezabek/multi-user-chat-openchat-3.5-0106-full-conversations" rel="noopener ugc nofollow" target="_blank">OpenChat-3.5</a>.</p><h1 id="9daf" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Troubleshooting</h1><p id="827e" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">Below I’m listing some pitfalls that I’ve encountered frequently during finetuning, these might come handy when finetuning other models.</p><h2 id="07c5" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Pad Token</h2><p id="6df2" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">I’ve seen the pad token set to the EOS token in multiple tutorials (and also by default in the Zephyr model). This doesn’t play well with HuggingFace’s data collators though: <a class="af ph" href="https://github.com/huggingface/transformers/blob/976189a6df796a2ff442dd81b022626c840d8c27/src/transformers/data/data_collator.py#L752" rel="noopener ugc nofollow" target="_blank">this line</a> in <em class="ql">DataCollatorForLanguageModeling </em>means that models are not trained to predict pad tokens. If the pad and EOS tokens are the same, you might end up with a model that continues generating tokens without stopping. My recommendation is to set the pad token to the UNK token if available (and distinct from EOS). Alternatively, you can use the tokenizer’s <em class="ql">add_token </em>method to add it to the vocabulary.<br/>In short: <strong class="ml fr">Make sure the pad token is not the same as the EOS token.</strong> Recent versions of HuggingFace started adding this warning, which adds visibility to the issue:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="0ef0" class="pm nz fq pj b bg pn po l pp pq">UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.</span></pre><h2 id="df8d" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">Loss Falling to 0.0 During Training</h2><p id="dbae" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">When using half precision floats (that is torch.float16), I’ve seen situations where the loss goes to 0.0 after a few steps and remains there. Specifically, this happens with our training notebook with the Llama-2 model. There are reports online of similar issues (for example <a class="af ph" href="https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da?permalink_comment_id=4634125#gistcomment-4634125" rel="noopener ugc nofollow" target="_blank">here</a>), curiously they were resolved at that time by setting the tokenizer’s <em class="ql">padding_side</em> to “right”. In our case the padding is already on the right-hand side, so that fix does not apply.</p><p id="e50e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The workaround is to use a different type for training: Either torch.bfloat16 (which is unavailable on older instances like T4 and V100) or torch.float32 (which results in a performance hit at training time, but otherwise works fine).</p><h2 id="c9ac" class="pu nz fq bf oa pv pw px od py pz qa og ms qb qc qd mw qe qf qg na qh qi qj qk bk">“RuntimeError: element 0 of tensors does not require grad…”</h2><p id="d6fd" class="pw-post-body-paragraph mj mk fq ml b go ou mn mo gr ov mq mr ms ow mu mv mw ox my mz na oy nc nd ne fj bk">Depending on the model, you might come across this error:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="ca48" class="pm nz fq pj b bg pn po l pp pq">RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</span></pre><p id="0d46" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The simple fix is to add this line after instantiating the model:</p><pre class="ni nj nk nl nm pi pj pk bp pl bb bk"><span id="da23" class="pm nz fq pj b bg pn po l pp pq">model.enable_input_require_grads()</span></pre></div></div></div></div>    
</body>
</html>