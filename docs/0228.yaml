- en: 'RLAIF: Reinforcement Learning from AI Feedback'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLAIF：来自AI反馈的强化学习
- en: 原文：[https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23](https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23](https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23)
- en: Making alignment via RLHF more scalable by automating human feedback…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过自动化人类反馈使RLHF的对齐过程更具可扩展性…
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    ·18 min read·Jan 23, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    ·18 分钟阅读 ·2024年1月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e6700ce9041c1d6c1e5ca6dbf2d86e49.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6700ce9041c1d6c1e5ca6dbf2d86e49.png)'
- en: (Photo by [Rock’n Roll Monkey](https://unsplash.com/@rocknrollmonkey?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/gray-and-white-robot-illustration-FTfjMijq-Ws?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：[Rock’n Roll Monkey](https://unsplash.com/@rocknrollmonkey?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    在 [Unsplash](https://unsplash.com/photos/gray-and-white-robot-illustration-FTfjMijq-Ws?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: Beyond using larger models and datasets for pretraining, the drastic increase
    in large language model (LLM) quality has been due to advancements in the alignment
    process, which is largely being fueled by finetuning techniques like supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). RLHF
    in particular is an interesting technique, as it allows us to directly finetune
    a language model based on human-provided preferences. Put simply, we can just
    teach the model to produce outputs that humans prefer, which is a flexible and
    powerful framework. However, it requires that a large amount of human preference
    labels be collected, which can be expensive and time consuming. Within this overview,
    we will explore recent research that aims to automate the collection of human
    preferences for RLHF using AI, forming a new technique known as reinforcement
    learning from AI feedback (RLAIF).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用更大模型和数据集进行预训练外，大型语言模型（LLM）质量的显著提升还得益于对齐过程的进展，这主要是通过细化技术如监督式微调（SFT）和来自人类反馈的强化学习（RLHF）推动的。特别是RLHF是一种有趣的技术，它使我们能够基于人类提供的偏好直接微调语言模型。简而言之，我们可以直接教导模型生成符合人类偏好的输出，这是一个灵活且强大的框架。然而，这要求收集大量人类偏好标签，而这通常既昂贵又耗时。在本综述中，我们将探讨旨在利用AI自动化收集RLHF所需人类偏好的最新研究，形成一种新的技术，称为来自AI反馈的强化学习（RLAIF）。
- en: Training a Language Model
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练语言模型
- en: The language model training process progresses in several phases; see above.
    First, we pretrain the model over a large corpus of unlabeled textual data, which
    is the most expensive part of training. After pretraining, we perform a three-part
    alignment process…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的训练过程分为几个阶段；见上文。首先，我们在大量未标注的文本数据上进行预训练，这是训练中最昂贵的部分。预训练后，我们执行一个三阶段的对齐过程…
