- en: 'RLAIF: Reinforcement Learning from AI Feedback'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23](https://towardsdatascience.com/rlaif-reinforcement-learning-from-ai-feedback-d7dbdae8f093?source=collection_archive---------8-----------------------#2024-01-23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Making alignment via RLHF more scalable by automating human feedback…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page---byline--d7dbdae8f093--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d7dbdae8f093--------------------------------)
    ·18 min read·Jan 23, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6700ce9041c1d6c1e5ca6dbf2d86e49.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Rock’n Roll Monkey](https://unsplash.com/@rocknrollmonkey?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/gray-and-white-robot-illustration-FTfjMijq-Ws?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Beyond using larger models and datasets for pretraining, the drastic increase
    in large language model (LLM) quality has been due to advancements in the alignment
    process, which is largely being fueled by finetuning techniques like supervised
    fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). RLHF
    in particular is an interesting technique, as it allows us to directly finetune
    a language model based on human-provided preferences. Put simply, we can just
    teach the model to produce outputs that humans prefer, which is a flexible and
    powerful framework. However, it requires that a large amount of human preference
    labels be collected, which can be expensive and time consuming. Within this overview,
    we will explore recent research that aims to automate the collection of human
    preferences for RLHF using AI, forming a new technique known as reinforcement
    learning from AI feedback (RLAIF).
  prefs: []
  type: TYPE_NORMAL
- en: Training a Language Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The language model training process progresses in several phases; see above.
    First, we pretrain the model over a large corpus of unlabeled textual data, which
    is the most expensive part of training. After pretraining, we perform a three-part
    alignment process…
  prefs: []
  type: TYPE_NORMAL
