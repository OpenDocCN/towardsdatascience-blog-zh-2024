["```py\nclient = OpenAI(\n    base_url=\"https://integrate.api.nvidia.com/v1\",\n    api_key=os.environ[\"NVIDIA_API_KEY\"]\n)\nMODEL = \"meta/llama-3.1-405b-instruct\"\n```", "```py\nn_subtopics = 5\n\nTOPIC_GENERATION_PROMPT_TEMPLATE = \"\"\"\\\nI want to create a synthetic dataset of natural language and Git commands. Based on this context, give me {n_subtopics} subtopics\nto cover what needs to be covered when working with Git. \n\nThe list must be without numbers, and without any description of the subtopics. The subtopics should be separated by a comma. There must be no other text than the list.\n\"\"\"\n\ndef generate_subtopics(client, n_subtopics):\n    prompt = TOPIC_GENERATION_PROMPT_TEMPLATE.format(n_subtopics=n_subtopics)\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"user\",\n             \"content\": prompt}\n        ],\n        temperature=0.2,\n        top_p=0.7,\n    )\n    return response\n\nresponses = generate_subtopics(client, n_subtopics=n_subtopics)\nprint(responses.choices[0].message.content)\n```", "```py\nn_instructions = 100\n\nINSTRUCTION_PROMPT_TEMPLATE = \"\"\"\\\nThe objective is to create a dataset of user instructions in natural language that should be returned by Git commands.\nGiven a topic in Git, generate {n_instructions} possible concise instructions that could be given to an AI assitant about that topic.\nWrite some of these instructions as if given by someone with limited knowledge of Git terminologies and knowledge, \nlike a beginner programmer. Your response should be in a list format.\n\nThe topic is: {sub_topic}\nThe list must be without numbers. The questions/instructions should be separated by a newline character. There must be no other text than the list.\n\"\"\"\nsubtopic_list = responses.choices[0].message.content.split(\",\")\ndef generate_instructions(client, sub_topic, n_instructions):\n    print(f\"Generating Instructions for {sub_topic}.\")\n    prompt = INSTRUCTION_PROMPT_TEMPLATE.format(sub_topic=sub_topic, n_instructions=n_instructions)\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"user\",\n             \"content\": prompt}\n        ],\n        temperature=0.2,\n        top_p=0.7,\n    )\n    return response.choices[0].message.content\n\ndef instructions_generator(client, subtopic_list, n_instructions):\n    instruction_list = [generate_instructions(client, subtopic, n_instructions) for subtopic in subtopic_list]\n    return instruction_list\n\ninstruction_list = instructions_generator(client, subtopic_list, n_instructions)\n\ninstruction_list_formatted = []\nfor instruction_set in instruction_list:\n    instruction_list_formatted.extend([instruction.strip() for instruction in instruction_set.split(\"\\n\") if instruction])\nprint(instruction_list_formatted)\n```", "```py\n'Make a branch that I can merge back into the main branch',\n'I want to make a branch that is based on an older version of the code',\n'Can you show me a log of all commits that have been made to the repository this year?',\n```", "```py\nRESPONSE_PROMPT_TEMPLATE = \"\"\"\\\nGiven an question/instruction related to Git, generate a response that could be given. \nKeep the response on-topic, informative, concise.\n\nThe user prompt is: {instruction}\n\"\"\"\ndef generate_responses(client, instruction):\n    prompt = RESPONSE_PROMPT_TEMPLATE.format(instruction=instruction)\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages=[\n            {\"role\": \"user\",\n             \"content\": prompt}\n        ],\n        temperature=0.2,\n        top_p=0.7,\n        max_tokens=1024,\n    )\n    return response.choices[0].message.content\n\ndef response_generator(client, instruction_list):\n    response_list = [generate_responses(client, instruction) for instruction in instruction_list]\n    return response_list\n\ninstruction_response_list = response_generator(client, instruction_list_formatted)\ninstruction_response_pair_list = []\nfor instruction, response in zip(instruction_list_formatted, instruction_response_list):\n    instruction_response_pair_list.append(\n        {\n            \"instruction\": instruction,\n            \"responses\": response,\n        }\n    )\n```", "```py\n{\"instruction\": \"Can you make a branch for me and name it \\\"new-feature\\\"\", \n\"responses\": \"To create a new branch named \\\"new-feature\\\", you can use the following Git command:\\n\\n`git branch new-feature`\\n\\nThis will create a new branch with the specified name. If you want to switch to the new branch immediately, you can use:\\n\\n`git checkout -b new-feature`\\n\\nThis will create the branch and switch to it in one step.\"}\n```", "```py\ndef get_scores_from_response(score_response_template):\n    logprobs = score_response_template.choices[0].logprobs.content\n    score_dict = {}\n    for score in logprobs:\n        score_dict[score.token] = score.logprob\n    return score_dict\n\ndef get_response_and_scores(client, model, question, response_content):\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": question\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": response_content\n        }\n    ]\n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n    )\n    scores = get_scores_from_response(response)\n    return scores\n```", "```py\nhelpfulness_THRESHOLD = 3\nverbosity_THRESHOLD = 2.5\nsynthetic_data = [data for i, data in enumerate(synthetic_data) \n                  if not (score_list[i][\"helpfulness\"] < helpfulness_THRESHOLD or \n                          score_list[i][\"verbosity\"] > verbosity_THRESHOLD)]\n```", "```py\nfrom huggingface_hub import login\nlogin()\n```", "```py\nwith open(f'synthetic_data_filtered.jsonl', 'r') as f:\n    data = [json.loads(line) for line in f]\ndataset = Dataset.from_list(data)\ndataset_dict = DatasetDict({\"train\": dataset})\ndataset_dict.push_to_hub(\"hesamsheikh/git-prompt\")\n```"]