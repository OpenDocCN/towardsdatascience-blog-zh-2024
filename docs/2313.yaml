- en: Using Generative AI to Automatically Create a Video Talk from an Article
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=collection_archive---------2-----------------------#2024-09-22](https://towardsdatascience.com/using-generative-ai-to-automatically-create-a-video-talk-from-an-article-6381c44c5fe0?source=collection_archive---------2-----------------------#2024-09-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using Gemini + Text to Speech + MoviePy to create a video, and what this says
    about what GenAI is becoming rapidly useful for
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lakshmanok.medium.com/?source=post_page---byline--6381c44c5fe0--------------------------------)[![Lak
    Lakshmanan](../Images/9faaaf72d600f592cbaf3e9089cbb913.png)](https://lakshmanok.medium.com/?source=post_page---byline--6381c44c5fe0--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6381c44c5fe0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6381c44c5fe0--------------------------------)
    [Lak Lakshmanan](https://lakshmanok.medium.com/?source=post_page---byline--6381c44c5fe0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6381c44c5fe0--------------------------------)
    ·10 min read·Sep 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Like most everyone, I was flabbergasted by [NotebookLM and its ability to generate
    a podcast](https://blog.google/technology/ai/notebooklm-audio-overviews/) from
    a set of documents. And then, I got to thinking: “how do they do that, and where
    can I get some of that magic?” How easy would it be to replicate?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Goal: Create a video talk from an article'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I don’t want to create a podcast, but I’ve often wished I could generate slides
    and a video talk from my blog posts —some people prefer paging through slides,
    and others prefer to watch videos, and this would be a good way to meet them where
    they are. In this article, I’ll show you how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [full code for this article](https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb)
    is on GitHub — in case you want to follow along with me. And the goal is to create
    this video from [this article](https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc):'
  prefs: []
  type: TYPE_NORMAL
- en: Video created automatically using the code described in this article. Video
    generated by author.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Initialize the LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am going to use Google Gemini Flash because (a) it is the least expensive
    frontier LLM today, (b) it’s multimodal in that it can read and understand images
    also, and (c) it supports controlled generation, meaning that we can make sure
    the output of the LLM matches a desired structure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that I’m using Google Generative AI and not Google Cloud Vertex AI. The
    two packages are different. The Google one supports Pydantic objects for controlled
    generation; the Vertex AI one only supports JSON for now.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Get a PDF of the article
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I used Python to download the article as a PDF, and upload it to a temporary
    storage location that Gemini can read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, something about medium prevents pdfkit from getting the images
    in the article (perhaps because they are webm and not png …). So, my slides are
    going to be based on just the text of the article and not the images.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Create lecture notes in JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, the data format I want is a set of slides each of which has a title, key
    points, and a set of lecture notes. The lecture as a whole has a title and an
    attribution also.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s tell Gemini what we want it to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The prompt is pretty straightforward — ask Gemini to read the article, extract
    key points and create lecture notes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, invoke the model, passing in the PDF file and asking it to populate the
    desired structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A few things to note about the code above:'
  prefs: []
  type: TYPE_NORMAL
- en: We pass in the prompt as the system prompt, so that we don’t need to keep sending
    in the prompt with new inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We specify the desired response type as JSON, and the schema to be a Pydantic
    object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We send the PDF file to the model and tell it generate a response. We’ll wait
    for it to complete (no need to stream)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result is JSON, so extract it into a Python object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, this is what the 3rd slide looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Convert to PowerPoint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use the Python package pptx to create a Presentation with notes and
    bullet points. The code to create a slide looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a PowerPoint presentation that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/532193d938ec39c67ae8e4b4c5ca81e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The PowerPoint file that was generated from the keypoints and lecture notes.
    Screenshot by author.
  prefs: []
  type: TYPE_NORMAL
- en: Not very fancy, but definitely a great starting point for editing if you are
    going to give a talk.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Read the notes aloud and save audio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Well, we were inspired by a podcast, so let’s see how to create just an audio
    of someone summarizing the article.
  prefs: []
  type: TYPE_NORMAL
- en: We already have the lecture notes, so let’s create audio files of each of the
    slides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the code to take some text, and have an AI voice read it out. We save
    the resulting audio into an mp3 file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: What’s happening in the code above?
  prefs: []
  type: TYPE_NORMAL
- en: We are using Google Cloud’s text to speech API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking it to use a standard US accent female voice. If you were doing a podcast,
    you’d pass in a “speaker map” here, one voice for each speaker.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then give it in the input text, ask it generate audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the audio as an mp3 file. Note that this has to match the audio encoding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, create audio by iterating through the slides, and passing in the lecture
    notes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a bunch of audio files. You can concatenate them if you wish
    using pydub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: But it turned out that I didn’t need to. The individual audio files, one for
    each slide, were what I needed to create a video. For a podcast, of course, you’d
    want a single mp3 or wav file.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Create images of the slides
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rather annoyingly, there’s no easy way to render PowerPoint slides as images
    using Python. You need a machine with Office software installed to do that — not
    the kind of thing that’s easily automatable. Maybe I should have used Google Slides
    … Anyway, a simple way to render images is to use the Python Image Library (PIL):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting image is not great, but it is serviceable (you can tell no one
    pays me to write production code anymore):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/417e2c0e11b84caacc8069f9a4e922d8.png)'
  prefs: []
  type: TYPE_IMG
- en: The images used along with the audio clips look like this. Image generated by
    author.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Create a Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have a set of audio files and a set of image files, we can use
    a Python package moviepy to create a video clip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can now write it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'End result? We have four artifacts, all created automatically from the article.pdf:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'There’s:'
  prefs: []
  type: TYPE_NORMAL
- en: a JSON file with keypoints, lecture notes, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A PowerPoint file that you can modify. The slides have the key points, and the
    notes section of the slides has the “lecture notes”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An audio file consisting of an AI voice reading out the lecture notes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mp4 movie (that I uploaded to YouTube) of the audio + images. This is the
    video talk that I set out to create.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretty cool, eh?
  prefs: []
  type: TYPE_NORMAL
- en: 8\. What this says about the future of software
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are all, as a community, probing around to find what this really cool technology
    (generative AI) can be used for. Obviously, you can use it to create content,
    but the content that it creates is good for brainstorming, but not to use as-is.
    Three years of improvements in the tech have not solved the problem that GenAI
    generates blah content, and not-ready-to-use code.
  prefs: []
  type: TYPE_NORMAL
- en: That brings us to some of the ancillary capabilities that GenAI has opened up.
    And these turn out to be extremely useful. There are four capabilities of GenAI
    that this post illustrates.
  prefs: []
  type: TYPE_NORMAL
- en: '**(1) Translating unstructured data to structured data**'
  prefs: []
  type: TYPE_NORMAL
- en: The Attention paper was written to solve the translation problem, and it turns
    out transformer-based models are really good at translation. We keep discovering
    use cases of this. But not just [Japanese to English](https://mse238blog.stanford.edu/2017/08/jchoi8/machine-learning-transforms-google-translate-overnight/),
    but also [Java 11 to Java 17](https://digiday.com/media/how-amazons-genai-tool-for-developers-is-saving-4500-years-of-work-260-million-annually/),
    of [text to SQL](https://paperswithcode.com/task/text-to-sql), of text to speech,
    between database dialects, …, and now of articles to audio-scripts. This, it turns
    out is the stepping point of using GenAI to create podcasts, lectures, videos,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: All I had to do was to prompt the LLM to construct a series of slide contents
    (keypoints, title, etc.) from the article, and it did. It even returned the data
    to me in structured format, conducive to using it from a computer program. Specifically,
    *GenAI is really good at translating unstructured data to structured data*.
  prefs: []
  type: TYPE_NORMAL
- en: '**(2) Code search and coding assistance are now dramatically better**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing that GenAI turns out to be really good at is at adapting code
    samples dynamically. I don’t write code to create presentations or text-to-speech
    or moviepy everyday. Two years ago, I’d have been using Google search and getting
    Stack Overflow pages and adapting the code by hand. Now, Google search is giving
    me ready-to-incorporate code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/318fc9e541190e93ace8486d8dcd05f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Search returning code samples, adapated to my specific query. Screenshot
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, had I been using a Python IDE (rather than a Jupyter notebook), I
    could have avoided the search step completely — I could have written a comment
    and gotten the code generated for me. This is hugely helpful, and speeds up development
    using general purpose APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '**(3) GenAI web services are robust and easy-to-consume**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s not lose track of the fact that I used the Google Cloud Text-to-Speech
    service to turn my audio script into actual audio files. Text-to-speech is itself
    a generative AI model (and another example of the translation superpower). The
    Google TTS service which was [introduced in 2018](https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-text-to-speech-powered-by-deepmind-wavenet-technology)
    (and presumably improved since then) was one of the first generative AI services
    in production and made available through an API.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I used two generative AI models — TTS and Gemini — that are
    made available as web services. All I had to do was to call their APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '**(4) It’s easier than ever to provide end-user customizability**'
  prefs: []
  type: TYPE_NORMAL
- en: I didn’t do this, but you can squint a little and see where things are headed.
    If I’d wrapped up the presentation creation, audio creation, and movie creation
    code in services, I could have had a prompt create the function call to invoke
    these services as well. And put a request-handling agent that would allow you
    to use text to change the look-and-feel of the slides or the voice of the person
    reading the video.
  prefs: []
  type: TYPE_NORMAL
- en: It becomes extremely easy to add open-ended customizability to the software
    you build.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inspired by the NotebookLM podcast feature, I set out to build an application
    that would convert my articles to video talks. The key step is to prompt an LLM
    to produce slide contents from the article, another GenAI model to convert the
    audio script into audio files, and use existing Python APIs to put them together
    into a video.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article illustrates four capabilities that GenAI is unlocking: translation
    of all kinds, coding assistance, robust web services, and end-user customizability.'
  prefs: []
  type: TYPE_NORMAL
- en: I loved being able to easily and quickly create video lectures from my articles.
    But I’m even more excited about the potential that we keep discovering in this
    new tool we have in our hands.
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Full code for this article: [https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb](https://github.com/lakshmanok/lakblogs/blob/main/genai_seminar/create_lecture.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The source article that I converted to a video: [https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc](https://lakshmanok.medium.com/what-goes-into-bronze-silver-and-gold-layers-of-a-medallion-data-architecture-4b6fdfb405fc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The resulting video: [https://youtu.be/jKzmj8-1Y9Q](https://youtu.be/jKzmj8-1Y9Q)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Turns out [Sascha Heyer wrote up how to use GenAI to generate a podcast](https://medium.com/google-cloud/building-a-dynamic-podcast-generator-inspired-by-googles-notebooklm-and-illuminate-e585cfcd0af1),
    which is the exact Notebook LM usecase. His approach is somewhat similar to mine,
    except that there is no video, just audio. In a cool twist, he uses his own voice
    as one of the podcast speakers!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, here’s the video talk of this article created using the technique
    shown in this video. Ideally, we are pulling out code snippets and images from
    the article, but this is a start …
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
