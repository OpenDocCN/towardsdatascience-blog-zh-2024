<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Object Detection Basics — A Comprehensive Beginner’s Guide (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Object Detection Basics — A Comprehensive Beginner’s Guide (Part 1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05">https://towardsdatascience.com/object-detection-basics-a-comprehensive-beginners-guide-part-1-f57380c89b78?source=collection_archive---------6-----------------------#2024-02-05</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><div/><div><h2 id="e0da" class="pw-subtitle-paragraph go fq fr bf b gp gq gr gs gt gu gv gw gx gy gz ha hb hc hd cq dx">Learn the basics of this advanced computer vision task of object detection in an easy to understand multi-part beginner’s guide</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Raghav Bali" class="l ep by dd de cx" src="../Images/49fea68f38f59d0bc39dab484b55684f.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*6nRZK0-KCmkqu5I3auzK3w.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@Rghv_Bali?source=post_page---byline--f57380c89b78--------------------------------" rel="noopener follow">Raghav Bali</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f57380c89b78--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn lg lh ab q ee li lj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count le lf">3</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ih lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mn bh"><figure class="mo mp mq mr ms mn bh paragraph-image"><img src="../Images/8f95d39d6f6a1014031f57234f642646.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/0*4Vq2m6HJ1OMOW77g"/><figcaption class="mu mv mw mx my mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@javigabbo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Javier García</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cb"><div class="ci bh ew ex ey ez"><p id="25d1" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Driving a car nowadays with the latest drive assist technologies for lane detection, blind-spots, traffic signals and so on is pretty common. If we take a step back for a minute to appreciate what is happening behind the scenes, the Data Scientist in us soon realises that the system is not just <em class="ny">classifying</em> objects but also <em class="ny">locating</em> them in the scene (in real-time).</p><p id="de2c" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Such capabilities are prime examples of an <strong class="ne fs">object detection</strong> system in action. Drive assist technologies, industrial robots and security systems all make use of object detection models to detect objects of interest. <strong class="ne fs">Object detection</strong> is an advanced computer vision task which involves both <em class="ny">localisation</em> [of objects] as well as <em class="ny">classification</em>.</p><p id="ee12" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">In this article, we will dive deeper into the details of the object detection task. We will learn about various concepts associated with it to help us understand novel architectures (covered in subsequent articles). We will cover key aspects and concepts required to understand object detection models from a Transfer Learning standpoint.</p><h1 id="65c5" class="nz oa fr bf ob oc od gr oe of og gu oh oi oj ok ol om on oo op oq or os ot ou bk">Key Concepts and Building Blocks</h1><p id="8764" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">Object detection consists of two main sub-tasks, <em class="ny">localization</em> and <em class="ny">classification</em>. Classification of identified objects is straightforward to understand. But how do we define localization of objects? Let us cover some key concepts:</p><h2 id="f2bb" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Bounding Boxes</h2><p id="2f07" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">For the task of object detection, we identify a given object’s location using a rectangular box. This regular box is termed as a <em class="ny">bounding box</em> and used for localization of objects. Typically, the top left corner of the input image is set as origin or (0,0). A rectangular bounding box is defined with the help of its x and y coordinates for the top-left and bottom right vertices. Let us understand this visually. Figure 1(a) depicts a sample image with its origin set at its top left corner.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div class="mx my pr"><img src="../Images/0580171bb1c23766c169aa7f91f7a9a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*xCxEVI1sEvE43p6Ty0Xm5Q.png"/></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 1: (a) A sample image with different objects, (b) bounding boxes for each of the objects with top-left and bottom-right vertices annotated,(c.)alternate way of identifying a bounding box is to use its top-left coordinates along with width and height parameters. Source: Author</figcaption></figure><p id="b076" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Figure 1(b) shows each of the identified objects with their corresponding bounding boxes. It is important to note that a bounding box is annotated with its top-left and bottom-right coordinates which are relative to the image’s origin. With 4 values, we can identify a bounding box uniquely. An alternate method to identify a bounding box is to use top-left coordinates along with its width and height values. Figure 1(c) shows this alternate way of identifying a bounding box. Different solutions may use different methods and it is mostly a matter of preference of one over the other.</p><p id="0587" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Object detection models require bounding box coordinates for each object per training sample apart from class label. Similarly, an object detection model generates <em class="ny">bounding box coordinates</em> along with <em class="ny">class labels</em> per identified object during inference stage.</p><h2 id="0e56" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Anchor Boxes</h2><p id="f28d" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">Every object detection model scans through a large number of possible regions to identify/locate objects for any given image. During the course of training, the model learns to determine which of the scanned regions are of interest and adjust the coordinates of these regions to match the ground truth bounding boxes. Different models may generate these regions of interest differently. Yet, the most popular and widely used method is based on <em class="ny">anchor boxes</em>. For every pixel in the given image, multiple bounding boxes of different sizes and aspect ratios (ratio of width to height) are generated. These bounding boxes are termed as anchor boxes. Figure 2 illustrates different anchor boxes for particular pixel in the given image.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div class="mx my ps"><img src="../Images/ee0dfbb28dcda77b5f25eff7b479d027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*ESy4o-6C7tDgEuXR0td4Mw.png"/></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 2: Different anchor boxes for a specific pixel (highlighted in red) for the given image. Source: Author</figcaption></figure><p id="4d6f" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Anchor box dimensions are controlled using two parameters, <em class="ny">scale</em> denoted as s 𝜖 (0,1] and <em class="ny">aspect ratio</em> denoted as r &gt;0. As shown in figure 2, for an image of height and width h ⨉ w and specific values of s and r, multiple anchor boxes can be generated. Typically, we use the following formulae to get dimensions of the anchor boxes:</p><blockquote class="pt"><p id="c013" class="pu pv fr bf pw px py pz qa qb qc nx dx"><strong class="al">wₐ=w.s√r</strong></p><p id="c15c" class="pu pv fr bf pw px py pz qa qb qc nx dx"><strong class="al">hₐ = h.s / √r</strong></p></blockquote><p id="7ad7" class="pw-post-body-paragraph nc nd fr ne b gp qd ng nh gs qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx fk bk">Where wₐ<strong class="ne fs"> </strong>and hₐ<strong class="ne fs"> </strong>are the width and height of the anchor box respectively. Number and dimensions of anchor boxes are either predefined or picked up by the model during the course of training itself. To put things in perspective, a model generates a number of anchor boxes per pixel and learns to adjust/match them with ground truth bounding box as the training progresses.</p><p id="7e29" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Bounding boxes and anchor boxes are key concepts to understand the overall object detection task. Before we get into the specifics of how such architectures work, let us first understand the way we evaluate the performance of such models. The following are some of the important evaluation metrics used:</p><h2 id="fe06" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Intersection over union (IOU)</h2><p id="1dc1" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">An object detection model typically generates a number of anchor boxes which are then adjusted to match the ground truth bounding box. But how do we know when the match has happened or how well the match is?</p><p id="d49e" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk"><em class="ny">Jaccard Index</em> is a measure used to determine the similarity between two sets. In case of object detection, Jaccard Index is also termed as Intersection Over Union or IOU. It is given as:</p><blockquote class="pt"><p id="ed56" class="pu pv fr bf pw px py pz qa qb qc nx dx"><strong class="al">IOU = | Bₜ ∩ Bₚ | / | Bₜ ∪ Bₚ |</strong></p></blockquote><p id="fe31" class="pw-post-body-paragraph nc nd fr ne b gp qd ng nh gs qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx fk bk">Where Bₜ is the ground truth bounding box and Bₚ is the predicted bounding box. In simple terms it is a score between 0 and 1 determined as the ratio of area of overlap and area of union between predicted and ground truth bounding box. The higher the overlap, the better the score. A score close to 1 depicts near perfect match. Figure 3 showcases different scenarios of overlaps between predicted and ground truth bounding boxes for a sample image.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="mx my qi"><img src="../Images/691e0c67a0710229f85cb479bfc965a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGirS3kwnNBcbNlBwxMkrQ.png"/></div></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 3: Intersection Over Union (IOU) is a measure of match between the predicted and ground-truth bounding box. The higher the overlap, the better is the score. Source: Author</figcaption></figure><p id="ced2" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Depending upon the problem statement and complexity of the dataset, different thresholds for IOU are set to determine which predicted bounding boxes should be considered. For instance, an object detection challenge based on <a class="af nb" href="https://arxiv.org/abs/1405.0312v3" rel="noopener ugc nofollow" target="_blank">MS-COCO</a> uses an IOU threshold of 0.5 to consider a predicted bounding box as true positive.</p><h2 id="5d80" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Mean Average Precision (MAP)</h2><p id="95e0" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">Precision and Recall are typical metrics used to understand performance of classifiers in machine learning context. The following formulae define these metrics:</p><blockquote class="pt"><p id="b9f2" class="pu pv fr bf pw px py pz qa qb qc nx dx"><strong class="al">Precision = TP / (TP + FP)</strong></p><p id="b2a3" class="pu pv fr bf pw px py pz qa qb qc nx dx">Recall = TP/ (TP + FN)</p></blockquote><p id="dac4" class="pw-post-body-paragraph nc nd fr ne b gp qd ng nh gs qe nj nk nl qf nn no np qg nr ns nt qh nv nw nx fk bk">Where, <em class="ny">TP, FP and FN</em> stand for <em class="ny">True Positive, False Positive </em>and <em class="ny">False Negative</em> outcomes respectively. Precision and Recall are typically used together to generate Precision-Recall Curve to get a robust quantification of performance. This is required due to the opposing nature of precision and recall, i.e. as a model’s recall increases its precision starts decreasing. <em class="ny">PR curves </em>are used to calculate <em class="ny">F1 score</em>, <em class="ny">Area Under the Curve (AUC)</em> or <em class="ny">average precision (AP)</em> metrics. Average Precision is calculated as the average of precision at different threshold values for recall. Figure 4(a) shows a typical PR curve and figure 4(b) depicts how AP is calculated.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="mx my qn"><img src="../Images/b4b6e7d79fc860599b356c1a5d74ebbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjG833vhS896L6H09ZbYjA.png"/></div></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 4: a) A typical PR-curve shows model’s precision at different recall values. This is a downward sloping graph due to opposing nature of precision and recall metrics; (b) PR-Curve is used to calculate aggregated/combined scores such as F1 score, Area Under the Curve (AUC) and Average Precision (AP); (c.) mean Average Precision (mAP) is a robust combined metric to understand model performance across all classes at different thresholds. Each colored line depicts a different PR curve based on specific IOU threshold for each class. Source: Author</figcaption></figure><p id="1658" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Figure 4(c) depicts how average precision metric is extended to the object detection task. As shown, we calculate PR-Curve at different thresholds of IOU (this is done for each class). We then take a mean across all average precision values (for each class) to get the final mAP metric. This combined metric is a robust quantification of a given model’s performance. By narrowing down performance to just one quantifiable metric makes it easy to compare different model’s on the same test dataset.</p><p id="de39" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Another metric used to benchmark object detection models is <strong class="ne fs">frames per second (FPS)</strong>. This metric points to the number of input images or frames the model can analyze for objects per second. This is an important metric for real-time use-cases such as security video surveillance, face detection, etc.</p><p id="e9a5" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Equipped with these concepts, we are now ready to understand the general framework for object detection next.</p><h1 id="fdf6" class="nz oa fr bf ob oc od gr oe of og gu oh oi oj ok ol om on oo op oq or os ot ou bk">Object Detection Framework</h1><p id="454e" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">Object detection is an important and active area of research. Over the years, a number of different yet effective architectures have been developed and used in real-world setting. The task of object detection requires all such architectures to tackle a list of sub-tasks. Let us develop an understanding of the general framework to tackle object detection before we get to the details of how specific models handle them. The general framework comprises of the following steps:</p><ul class=""><li id="c867" class="nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qo qp qq bk">Region Proposal Network</li><li id="b350" class="nc nd fr ne b gp qr ng nh gs qs nj nk nl qt nn no np qu nr ns nt qv nv nw nx qo qp qq bk">Localization and Class Predictions</li><li id="9340" class="nc nd fr ne b gp qr ng nh gs qs nj nk nl qt nn no np qu nr ns nt qv nv nw nx qo qp qq bk">Output Optimizations</li></ul><p id="782d" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Let us now go through each of these steps in some detail.</p><h2 id="b38e" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Regional Proposal</h2><p id="91b2" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">As the name suggests, the first and foremost step in the object detection framework is to propose <em class="ny">regions of interest (ROI)</em>. ROIs are the regions of the input image for which the model believes there is a high likelihood of an object’s presence. The likelihood of an object’s presence or absence is defined using a score called objectness score. Regions which have objectness score greater than a certain threshold are passed onto the next stage while others are reject.</p><p id="f9aa" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">For example, take a look at figure 5 for different ROIs proposed by the model. It is important to note that a large number of ROIs are generated at this step. Based on the objectness score threshold, the model classifies ROIs as foreground or background and only passes foreground regions for further analysis.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="mx my qw"><img src="../Images/c2cc1fcfb109d88d81dd4324de71c252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9bmyXhLdomPOmHpMw92og.png"/></div></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 5: Regional Proposal is the first step in object detection framework. Regions of Interest are highlighted as red rectangular boxes. The model marks regions with high likelihood of an image (high objectness score) as foreground regions and rest as background regions. Source: Author</figcaption></figure><p id="b07a" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">There are a number of different ways of generating regions of interest. Earlier models used to make use of selective search and related algorithms to generate ROIs while newer and more complex models make use of deep learning models to do so. We will cover these when we discuss specific architectures in the upcoming articles.</p><h2 id="d6f1" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Localization And Class Predictions</h2><p id="df1f" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">Object detection models are different from the classification models we typically work with. An object detection model generates two outputs for every foreground region from the previous step:</p><ul class=""><li id="6a06" class="nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx qo qp qq bk"><strong class="ne fs">Object Class</strong>: This is the typical classification objective to assign a class label to every proposed foreground region. Typically, pre-trained networks are used to extract features from the proposed region and then use those features to predict the class. State-of-the-art models such as the ones trained on ImageNet or MS-COCO with a large number of classes are widely adapted/transfer learnt. It is important to note that we generate a class label for every proposed region and not just a single label for the whole image (as compared to a typical classification task)</li><li id="8b70" class="nc nd fr ne b gp qr ng nh gs qs nj nk nl qt nn no np qu nr ns nt qv nv nw nx qo qp qq bk"><strong class="ne fs">Bounding Box Coordinates</strong>: A bounding box is defined a tuple with 4 values for x, y, width and height. At this stage the model generates a tuple for every proposed foreground region as well (along with the object class).</li></ul><h2 id="be86" class="pa oa fr bf ob pb pc pd oe pe pf pg oh nl ph pi pj np pk pl pm nt pn po pp pq bk">Output Optimization</h2><p id="9567" class="pw-post-body-paragraph nc nd fr ne b gp ov ng nh gs ow nj nk nl ox nn no np oy nr ns nt oz nv nw nx fk bk">As mentioned earlier, an object detection model proposes a large number of ROIs in step one followed by bounding box and class predictions in step two. While there is some level of filtering of ROIs in step one (foreground vs background regions based on objectness score), there are still a large number of regions used for predictions in step two. Generating predictions for such a large number of proposed regions ensures good coverage for various objects in the image. Yet, there are a number of regions with good amount of overlap for the same region. For example, look at the 6 bounding boxes predicted for the same object in figure 6(a). This potentially can lead to difficulty in getting the exact count of different objects in the input image.</p><figure class="mo mp mq mr ms mn mx my paragraph-image"><div role="button" tabindex="0" class="qj qk ed ql bh qm"><div class="mx my qx"><img src="../Images/595c3830c4ee85b07a92cc6afacc8f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNLdcOsIbXrTr9F4k3cmbg.png"/></div></div><figcaption class="mu mv mw mx my mz na bf b bg z dx">Figure 6 (a)Object detection model generating 6 bounding boxes with good overlap for the same object. (b) Output optimized using NMS. Source: Author</figcaption></figure><p id="558a" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">Hence, there is a third step in this framework which concerns the optimization of the output. This optimization step ensures there is only one bounding box and class prediction per object in the input image. There are different ways of performing this optimization. By far, the most popular method is called <strong class="ne fs">Non-Maximum Suppression (NMS)</strong>. As the name suggests, NMS analyzes all bounding boxes for each object to find the one with maximum probability and suppress the rest of them (see figure 6(b) for optimized output after applying NMS).</p><p id="1a70" class="pw-post-body-paragraph nc nd fr ne b gp nf ng nh gs ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fk bk">This concludes a high-level understanding of a general object detection framework. We discussed about the three major steps involved in localization and classification of objects in a given image. In this next article we will use this understanding to understand specific implementations and their key contributions.</p></div></div></div></div>    
</body>
</html>