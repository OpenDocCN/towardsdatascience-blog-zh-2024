<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Deep Dive into Self-Attention by Hand✍︎</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Deep Dive into Self-Attention by Hand✍︎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22">https://towardsdatascience.com/deep-dive-into-self-attention-by-hand-%EF%B8%8E-f02876e49857?source=collection_archive---------0-----------------------#2024-04-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0753" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore the intricacies of the attention mechanism responsible for fueling the transformers</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Srijanie Dey, PhD" class="l ep by dd de cx" src="../Images/2b3292a3b22d712d91d0bfc14df64446.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:88:88/0*KYs4FkQ1LOfJ0P4Y"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@srijanie.dey?source=post_page---byline--f02876e49857--------------------------------" rel="noopener follow">Srijanie Dey, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f02876e49857--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="e12a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Attention! Attention!</p><p id="4a5d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Because ‘Attention is All You Need’.</p><p id="9fcd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">No, I am not saying that, the Transformer is.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/fa02dd175a9c6c0c241a9a16281d8d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DfknMjKdMQP94OQj8t_iA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author (Robtimus Prime seeking attention. As per my son, bright rainbow colors work better for attention and hence the color scheme.)</figcaption></figure><p id="2d21" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As of today, the world has been swept over by the power of transformers. Not the likes of ‘Robtimus Prime’ but the ones that constitute neural networks. And that power is because of the concept of ‘<strong class="ml fr">attention</strong>’. So, what does attention in the context of transformers really mean? Let’s try to find out some answers here:</p><p id="85fe" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">First and foremost:</p><h2 id="f729" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">What are transformers?</strong></h2><p id="2afe" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Transformers are neural networks that specialize in learning context from the data. Quite similar to us trying to find the meaning of <strong class="ml fr"><em class="oy">‘attention and context’ </em></strong>in terms of transformers.</p><h2 id="59cb" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">How do transformers learn context from the data?</strong></h2><p id="41fd" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">By using the attention mechanism.</p><h2 id="fcb4" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">What is the attention mechanism?</strong></h2><p id="d963" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The attention mechanism helps the model scan all parts of a sequence at each step and determine which elements need to be focused on. The attention mechanism was proposed as an alternative to the ‘strict/hard’ solution of fixed-length vectors in the encoder-decoder architecture and provide a ‘soft’ solution focusing only on the relevant parts.</p><h2 id="a987" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">What is self-attention?</strong></h2><p id="6e45" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The attention mechanism worked to improve the performance of Recurrence Neural Networks (RNNs), with the effect seeping into Convolutional Neural Networks (CNNs). However, with the introduction of the transformer architecture in the year 2017, the need for RNNs and CNNs was quietly obliterated. And the central reason for it was the self-attention mechanism.</p><p id="845a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The self-attention mechanism was special in the sense that it was built to inculcate the context of the input sequence in order to enhance the attention mechanism. This idea became transformational as it was able to capture the complex nuances of a language.</p><blockquote class="oz pa pb"><p id="04ba" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As an example:</p><p id="706d" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When I ask my 4-year old what transformers are, his answer only contains the words robots and cars. Because that is the only context he has. But for me, transformers also mean neural networks as this second context is available to the slightly more experienced mind of mine. And that is how different contexts provide different solutions and so tend to be very important.</p></blockquote><h2 id="7bab" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">The word ‘self’ refers to the fact that the attention mechanism examines the same input sequence that it is processing.</strong></h2><p id="7d8c" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">There are many variations of how self-attention is performed. But the <strong class="ml fr">scaled dot-product</strong> mechanism has been one of the most popular ones. This was the one introduced in the original transformer architecture paper in 2017 — “<a class="af pc" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a>”.</p><h2 id="5478" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">Where and how does self-attention feature in transformers?</strong></h2><p id="c404" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">I like to see the transformer architecture as a combination of two shells — the outer shell and the inner shell.</p><ol class=""><li id="f6e4" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pd pe pf bk">The outer shell is a combination of the attention-weighting mechanism and the feed forward layer about which I talk in detail in this <a class="af pc" href="https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813" rel="noopener">article</a>.</li><li id="0587" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">The inner shell consists of the self-attention mechanism and is part of the attention-weighting feature.</li></ol></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4f20" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, without further delay, let us dive into the details behind the self-attention mechanism and unravel the workings behind it. The <strong class="ml fr">Query-Key module </strong>and the <strong class="ml fr">SoftMax</strong> function play a crucial role in this technique.</p><p id="2e44" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This discussion is based on Prof. Tom Yeh’s wonderful AI by Hand Series on <a class="af pc" href="https://lnkd.in/gDW8Um4W" rel="noopener ugc nofollow" target="_blank">Self-Attention</a>. (All the images below, unless otherwise noted, are by Prof. Tom Yeh from the above-mentioned LinkedIn post, which I have edited with his permission.)</p><p id="c0af" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So here we go:</p><h1 id="972e" class="pt nz fq bf oa pu pv gq oe pw px gt oi py pz qa qb qc qd qe qf qg qh qi qj qk bk"><strong class="al">Self-Attention</strong></h1><p id="6611" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">To build some context here, here is a pointer to how we process the<strong class="ml fr"> ‘Attention-Weighting’ </strong>in the transformer<strong class="ml fr"> </strong>outer shell.</p><h2 id="88be" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">Attention weight matrix (A)</strong></h2><p id="1a77" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The attention weight matrix <strong class="ml fr">A </strong>is obtained by feeding the input features into the Query-Key (QK) module. This matrix tries to find the most relevant parts in the input sequence. Self-Attention comes into play while creating the Attention weight matrix <strong class="ml fr">A</strong> using the QK-module.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ql"><img src="../Images/98c0a5d3d06cbc56f250062940a59e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYNNNiaZac_ZNGFVUn4aag.gif"/></div></div></figure><h2 id="615c" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">How does the QK-module work?</strong></h2><p id="7028" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Let us look at The different components of Self-Attention: <strong class="ml fr">Query (Q), Key (K) </strong>and <strong class="ml fr">Value (V)</strong>.</p><p id="586f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I love using the spotlight analogy here as it helps me visualize the model throwing light on each element of the sequence and trying to find the most relevant parts. Taking this analogy a bit further, let us use it to understand the different components of Self-Attention.</p><p id="6bbb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine a big stage getting ready for the world’s largest Macbeth production. The audience outside is teeming with excitement.</p><ul class=""><li id="63c1" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qm pe pf bk"><em class="oy">The lead actor walks onto the stage, the spotlight shines on him and he asks in his booming voice “Should I seize the crown?”. The audience whispers in hushed tones and wonders which path this question will lead to. Thus, Macbeth himself represents the role of </em><strong class="ml fr"><em class="oy">Query (Q)</em></strong><em class="oy"> as he asks pivotal questions and drives the story forward.</em></li><li id="d720" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne qm pe pf bk"><em class="oy">Based on Macbeth’s query, the spotlight shifts to other crucial characters that hold information to the answer. The influence of other crucial characters in the story, like Lady Macbeth, triggers Macbeth’s own ambitions and actions. These other characters can be seen as the </em><strong class="ml fr"><em class="oy">Key (K) </em></strong><em class="oy">as they unravel different facets of the story based on the particulars they know.</em></li><li id="ec44" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne qm pe pf bk"><em class="oy">Finally, the extended characters — family, friends, supporter, naysayers provide enough motivation and information to Macbeth by their actions and perspectives. These can be seen as </em><strong class="ml fr"><em class="oy">Value (V).</em></strong><em class="oy"> The </em><strong class="ml fr"><em class="oy">Value (V)</em></strong><em class="oy"> pushes Macbeth towards his decisions and shapes the fate of the story.</em></li></ul><p id="16ba" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And with that is created one of the world’s finest performances, that remains etched in the minds of the awestruck audience for the years to come.</p><p id="cdca" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now that we have witnessed the role of <strong class="ml fr">Q</strong>, <strong class="ml fr">K</strong>, <strong class="ml fr">V</strong> in the fantastical world of performing arts, let’s return to planet matrices and learn the mathematical nitty-gritty behind the <strong class="ml fr">QK-module</strong>. This is the roadmap that we will follow:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/de5f0f180195583b9ef5aed6a8b04bbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SDpBMRQYC_R5qfkShE4_dQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Roadmap for the Self-Attention mechanism</figcaption></figure><p id="6e1d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And so the process begins.</p><h2 id="a4af" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">We are given:</strong></h2><p id="3de0" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">A set of 4-feature vectors (Dimension 6)</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qn"><img src="../Images/a6ad182544df69a2550ac9be45b5015f.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*iFUncxZxq5FXxqndV3F6nA.png"/></div></figure><h2 id="0204" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">Our goal :</strong></h2><p id="2609" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Transform the given features into <strong class="ml fr">Attention Weighted Features</strong>.</p><p id="1898" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[1] <strong class="ml fr">Create Query, Key, Value Matrices</strong></p><p id="c77c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To do so, we multiply the features with linear transformation matrices W_Q, W_K, and W_V, to obtain query vectors (q1,q2,q3,q4), key vectors (k1,k2,k3,k4), and value vectors (v1,v2,v3,v4) respectively as shown below:</p><p id="eff7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To get <strong class="ml fr">Q</strong>, multiply W_Q with X:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qo"><img src="../Images/b6bb9e18f73411d19d946034f15605c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*ViaalDDKfL-sgU6Rl6t7TQ.gif"/></div></figure><p id="2a60" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To get <strong class="ml fr">K</strong>, multiply W_K with X:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qp"><img src="../Images/f535191e8ee035b3d596470749b19577.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*420E3ooyeVYDzjCNeOJEQA.gif"/></div></figure><p id="d0e5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Similarly, to get <strong class="ml fr">V</strong>, multiply W_V with X.</p><h2 id="7f70" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">To be noted:</strong></h2><ol class=""><li id="1a38" class="mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne pd pe pf bk">As can be seen from the calculation above, we use the same set of features for both queries and keys. And that is how the idea of <strong class="ml fr">“self”</strong> comes into play here, i.e. the model uses the same set of features to create its query vector as well as the key vector.</li><li id="4686" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">The <strong class="ml fr">query vector</strong> represents the current word (or token) for which we want to compute attention scores relative to other words in the sequence.</li><li id="ea72" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">The <strong class="ml fr">key vector </strong>represents the other words (or tokens) in the input sequence and we compute the attention score for each of them with respect to the current word.</li></ol><p id="ec01" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[2] <strong class="ml fr">Matrix Multiplication</strong></p><p id="24cd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next step is to multiply the transpose of <strong class="ml fr">K</strong> with <strong class="ml fr">Q</strong> i.e. <strong class="ml fr">K</strong>^T . <strong class="ml fr">Q</strong>.</p><p id="9006" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The idea here is to calculate the dot product between every pair of query and key vectors. Calculating the dot product gives us an estimate of the matching score between every “key-query” pair, by using the idea of <strong class="ml fr">Cosine Similarity </strong>between the two vectors. This is the <strong class="ml fr"><em class="oy">‘dot-product’ </em></strong>part of the scaled dot-product attention.</p><blockquote class="oz pa pb"><p id="a335" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Cosine-Similarity</strong></p><p id="df04" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It roughly measures if two vectors are pointing in the same direction thus implying the two vectors are similar.</p><p id="33a4" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Remember cos(0°) = 1, cos(90°) = 0 , cos(180°) =-1</strong></p><p id="980d" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">- If the dot product between the two vectors is approximately 1, it implies we are looking at an almost zero angle between the two vectors meaning they are very close to each other.</p><p id="50cc" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">- If the dot product between the two vectors is approximately 0, it implies we are looking at vectors that are orthogonal to each other and not very similar.</p><p id="0ebd" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">- If the dot product between the two vectors is approximately -1, it implies we are looking at an almost an 180° angle between the two vectors meaning they are opposites.</p></blockquote><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qq"><img src="../Images/977171591ba823b623324c65070e09d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*-j29g-rIZJ5Am0abu1WE0A.gif"/></div></figure><p id="d22e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[3]<strong class="ml fr"> Scale</strong></p><p id="6eab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The next step is to scale/normalize each element by the square root of the dimension ‘<em class="oy">d_k</em>’. In our case the number is 3. Scaling down helps to keep the impact of the dimension on the matching score in check.</p><p id="34ea" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">How does it do so? As per the original Transformer paper and going back to Probability 101, if two independent and identically distributed (i.i.d) variables <em class="oy">q</em> and <em class="oy">k</em> with mean 0 and variance 1 with dimension d are multiplied, the result is a new random variable with mean remaining 0 but variance changing to <em class="oy">d_k</em>.</p><p id="564e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now imagine how the matching score would look if our dimension is increased to 32, 64, 128 or even 4960 for that matter. The larger dimension would make the variance higher and push the values into regions ‘unknown’.</p><p id="faf2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To keep the calculation simple here, since <em class="oy">sqrt </em>[3] is approximately 1.73205, we replace it with [ <em class="oy">floor</em>(□/2) ].</p><blockquote class="oz pa pb"><p id="ad29" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Floor Function</strong></p><p id="aa55" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The floor function takes a real number as an argument and returns the largest integer less than or equal to that real number.</p><p id="4af9" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Eg : floor(1.5) = 1, floor(2.9) = 2, floor (2.01) = 2, floor(0.5) = 0.</p><p id="3fd9" class="mj mk oy ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The opposite of the floor function is the ceiling function.</p></blockquote><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qr"><img src="../Images/4995dd56bc12525e18ac38b916ef58f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*sal9Yyz3kY1gGJzecU2vwA.gif"/></div></figure><p id="c6df" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This the ‘scaled’ part of the scaled dot-product attention.</p><p id="52e6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[4] <strong class="ml fr">Softmax</strong></p><p id="c8d5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are three parts to this step:</p><ol class=""><li id="4916" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pd pe pf bk">Raise e to the power of the number in each cell (To make things easy, we use 3 to the power of the number in each cell.)</li><li id="88c0" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">Sum these new values across each column.</li><li id="32e5" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">For each column, divide each element by its respective sum (Normalize). The purpose of normalizing each column is to have numbers sum up to 1. In other words, each column then becomes a <strong class="ml fr">probability distribution</strong> of attention, which gives us our <strong class="ml fr">Attention Weight Matrix (A).</strong></li></ol><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qs"><img src="../Images/023e56eda75ef16ae51abfb8df4dae56.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*kWBYoZd0kej9xMpDayfJmg.gif"/></div></figure><h2 id="46f1" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">This Attention Weight Matrix is what we had obtained after passing our feature matrix through the QK-module in Step 2 in the Transformers section.</strong></h2><p id="6ea3" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk"><em class="oy">(Remark: The first column in the Attention Weight Matrix has a typo as the current elements don’t add up to 1. Please double-check. We are allowed these errors because we are human.)</em></p><p id="7180" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Softmax step is important as it assigns probabilities to the score obtained in the previous steps and thus helps the model decide how much importance (higher/lower attention weights) needs to be given to each word given the current query. As is to be expected, higher attention weights signify greater relevance allowing the model to capture dependencies more accurately.</p><p id="fedc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Once again, the scaling in the previous step becomes important here. Without the scaling, the values of the resultant matrix gets pushed out into regions that are not processed well by the Softmax function and may result in vanishing gradients.</p><p id="122c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">[5]<strong class="ml fr"> Matrix Multiplication</strong></p><p id="770b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally we multiply the value vectors (<strong class="ml fr">V</strong>s) with the Attention Weight Matrix (<strong class="ml fr">A</strong>). These value vectors are important as they contain the information associated with each word in the sequence.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qt"><img src="../Images/65df264c09946b1ed85a2343598b375f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ifh2bcLkqR77OyzDLhsqCQ.gif"/></div></div></figure><p id="cb76" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And the result of the final multiplication in this step are the <strong class="ml fr">attention weighted features Z</strong>s which are the ultimate solution of the self-attention mechanism. These attention-weighted features essentially contain a <strong class="ml fr">weighted representation</strong> <strong class="ml fr">of the features</strong> assigning higher weights for features with higher relevance as per the context.</p><p id="4ee8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now with this information available, we continue to the next step in the transformer architecture where the feed-forward layer processes this information further.</p><p id="e989" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">And this brings us to the end of the brilliant self-attention technique!</p><p id="f5d0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Reviewing all the key points based on the ideas we talked about above:</p><ol class=""><li id="7880" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pd pe pf bk">Attention mechanism was the result of an effort to better the performance of RNNs, <strong class="ml fr">addressing the issue of fixed-length vector representations </strong>in the encoder-decoder architecture. The flexibility of soft-length vectors with a focus on the relevant parts of a sequence was the core strength behind attention.</li><li id="77b3" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">Self-attention was introduced as a way to inculcate the idea of context into the model. The self-attention mechanism <strong class="ml fr">evaluates the same input sequence that it processes</strong>, hence the use of the word ‘self’.</li><li id="cefb" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">There are many variants to the <strong class="ml fr">self-attention mechanism</strong> and efforts are ongoing to make it more efficient. However, scaled dot-product attention is one of the most popular ones and a crucial reason why the transformer architecture was deemed to be so powerful.</li><li id="1f4d" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">Scaled dot-product self-attention mechanism comprises the <strong class="ml fr">Query-Key module (QK-module)</strong> along with the<strong class="ml fr"> Softmax function</strong>. The QK module is responsible for extracting the relevance of each element of the input sequence by calculating the attention scores and the Softmax function complements it by assigning probability to the attention scores.</li><li id="66f2" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">Once the attention-scores are calculated, <strong class="ml fr">they are multiplied with the value vector to obtain the attention-weighted features</strong> which are then passed on to the feed-forward layer.</li></ol><h2 id="ade6" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">Multi-Head Attention</h2><p id="e9cd" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">To cater to a varied and overall representation of the sequence, multiple copies of the <strong class="ml fr">self-attention mechanism are implemented in parallel</strong> which are then concatenated to produce the final attention-weighted values. This is called the Multi-Head Attention.</p><h2 id="ff20" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">Transformer in a Nutshell</h2><p id="dc54" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">This is how the inner-shell of the transformer architecture works. And bringing it together with the outer shell, here is a summary of the Transformer mechanism:</p><ol class=""><li id="9a81" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pd pe pf bk">The two big ideas in the Transformer architecture here are <strong class="ml fr">attention-weighting and the feed-forward layer (FFN)</strong>. Both of them combined together allow the Transformer to analyze the input sequence from two directions. <strong class="ml fr">Attention</strong> looks at the sequence based on <strong class="ml fr">positions</strong> and the <strong class="ml fr">FFN </strong>does it based on the <strong class="ml fr">dimensions</strong> of the feature matrix.</li><li id="f43f" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">The part that powers the attention mechanism is the <strong class="ml fr">scaled dot-product Attention</strong> which consists of the<strong class="ml fr"> QK-module </strong>and outputs the attention weighted features.</li></ol><h2 id="5a09" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">‘Attention Is really All You Need’</strong></h2><p id="ef98" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Transformers have been here for only a few years and the field of AI has already seen tremendous progress based on it. And the effort is still ongoing. When the authors of the paper used that title for their paper, they were not kidding.</p><p id="25ee" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is interesting to see once again how a fundamental idea — the ‘dot product’ coupled with certain embellishments can turn out to be so powerful!</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/a50dd9ea40a103d373f2c0faa0b7b37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_T-chnGS-lknA0RhbGwjw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Image by author</figcaption></figure><p id="e67b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">P.S. If you would like to work through this exercise on your own, here are the blank templates for you to use.</p><p id="c9bd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af pc" href="https://drive.google.com/file/d/1_wpS7-Mq6HiuCVe4ozmPXO3LydcJMtok/view?usp=drive_link" rel="noopener ugc nofollow" target="_blank">Blank Template for hand-exercise</a></p><p id="c577" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now go have some fun with the exercise while paying attention to your <strong class="ml fr">Robtimus Prime</strong>!</p></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="f311" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">Related Work:</strong></h2><p id="d1cc" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Here are the other articles in the Deep Dive by Hand Series:</p><ul class=""><li id="fc90" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne qm pe pf bk"><a class="af pc" href="https://medium.com/towards-data-science/deep-dive-into-vector-databases-by-hand-e9ab71f54f80" rel="noopener">Deep Dive into Vector Databases by Hand</a> ✍ that explores what exactly happens behind-the-scenes in Vector Databases.</li><li id="180b" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne qm pe pf bk"><a class="af pc" href="https://medium.com/towards-data-science/deep-dive-into-soras-diffusion-transformer-dit-by-hand-︎-1e4d84ec865d" rel="noopener">Deep Dive into Sora’s Diffusion Transformer (DiT) by Hand</a> ✍ that explores the secret behind Sora’s state-of-the-art videos.</li><li id="f6dd" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne qm pe pf bk"><a class="af pc" href="https://medium.com/towards-data-science/deep-dive-into-transformers-by-hand-︎-68b8be4bd813" rel="noopener">Deep Dive into Transformers by Hand </a>✍ that explores the power behind the power of transformers.</li></ul><h2 id="9ddb" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk"><strong class="al">References:</strong></h2><ol class=""><li id="f132" class="mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne pd pe pf bk">Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “<a class="af pc" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">Attention is all you need.</a>” <em class="oy">Advances in neural information processing systems</em> 30 (2017).</li><li id="b8ea" class="mj mk fq ml b go pg mn mo gr ph mq mr ms pi mu mv mw pj my mz na pk nc nd ne pd pe pf bk">Bahdanau, Dzmitry, Kyunghyun Cho and Yoshua Bengio. “<a class="af pc" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">Neural Machine Translation by Jointly Learning to Align and Translate</a>.” <em class="oy">CoRR</em> abs/1409.0473 (2014).</li></ol></div></div></div></div>    
</body>
</html>