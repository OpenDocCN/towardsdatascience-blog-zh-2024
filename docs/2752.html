<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>From Parallel Computing Principles to Programming for CPU and GPU Architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>From Parallel Computing Principles to Programming for CPU and GPU Architectures</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-parallel-computing-principles-to-programming-for-cpu-and-gpu-architectures-dd06e1f30586?source=collection_archive---------5-----------------------#2024-11-12">https://towardsdatascience.com/from-parallel-computing-principles-to-programming-for-cpu-and-gpu-architectures-dd06e1f30586?source=collection_archive---------5-----------------------#2024-11-12</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="677a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">For early ML Engineers and Data Scientists, to understand memory fundamentals, parallel execution, and how code is written for CPU and GPU.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@shreyashukla04?source=post_page---byline--dd06e1f30586--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shreya Shukla" class="l ep by dd de cx" src="../Images/202aca58677a445ec618494f1151d2d7.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*zn2xDpcnGgnmJpDvnKAW3w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--dd06e1f30586--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@shreyashukla04?source=post_page---byline--dd06e1f30586--------------------------------" rel="noopener follow">Shreya Shukla</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--dd06e1f30586--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">19 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 12, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/655d3adf6d073a78237e2072eedbb398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mmGagEU0PoNCGycZ"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@olav_ahrens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Olav Ahrens Røtne</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d25b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This article aims to explain the fundamentals of parallel computing. We start with the basics, including understanding shared vs. distributed architectures and communication within these systems. We will explore GPU architecture and how coding elements (using C++ Kokkos) help map architectural principles to code implementation. Finally, we will measure performance metrics (speedup) using the runtime data obtained from running the Kokkos code on both CPU and GPU architectures for vector-matrix multiplication, one of the most common operations in the machine learning domain.</p><p id="2e22" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The central theme of the article is exploring and answering questions. It may seem like a lengthy journey, but it will be worth it. Let’s get started!</p><h1 id="bb3e" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Fundamentals of System Architecture</h1><h2 id="7491" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">I get that parallel computing saves time by running multiple operations at once. But I’ve heard that system time is different from human time or wall-clock time. How is it different?</h2><p id="60b2" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">The smallest unit of time in computing is called a <strong class="nf fr">clock tick</strong>. It represents the minimum time required to perform an operation, such as fetching data, executing computations, or during communication. A <strong class="nf fr">clock tick </strong>technically refers to the change of state necessary for an instruction.<strong class="nf fr"> </strong>The state can be processor state, data state, memory state, or control signals.<strong class="nf fr"> </strong>In one clock tick, a complete instruction, part of an instruction, or multiple instructions may be executed.</p><p id="70dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">CPU allows for a limited number of state changes per second</strong>. For example, a CPU with 3GHz clock speed allows for 3 billion changes of state per second. There is a limit to the allowable clock speed because each clock tick generates heat, and excessive speed can damage the CPU chip due to the heat generated.</p><p id="7629" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Therefore, we want to utilize the available capacity by using parallel computing methodologies. The purpose is to hide <strong class="nf fr">memory latency</strong> (the time it takes for the first data to arrive from memory), increase <strong class="nf fr">memory bandwidth </strong>(the amount of data transferred per unit of time), and enhance <strong class="nf fr">compute throughput</strong> (the tasks performed in a clock tick).</p><p id="7d26" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To compare <strong class="nf fr">performance</strong>, such as when calculating efficiency of a parallel program, we use wall-clock time instead of clock ticks, since it includes all real-time overheads like memory latency and communication delays, that cannot be directly translated to clock ticks.</p><h2 id="1fe1" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">What does the architecture of a basic system look like?</h2><p id="8310" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">A system can consist of a single processor, a node, or even a cluster. Some of the <strong class="nf fr">physical</strong> building blocks of a system are —</p><ol class=""><li id="9c39" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Node</strong> — A physical computer unit that has several processor chips. Multiple nodes combine to form a <strong class="nf fr">cluster</strong>.</li><li id="bc20" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Processor Chips </strong>— Chips contain multiple processing elements called cores.</li><li id="08b7" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Core</strong> — Each core is capable of running an independent <strong class="nf fr">thread</strong> of execution.</li></ol><p id="1c3d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In set terms, a node can have a one-to-many relationship with processor chips, and each processor chip can have a one-to-many relationship with cores. The image below gives a visual description of a node with processors and cores.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/c153ab76de53b118f94f98245679085f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCT5M5lJU1hPwn0zS2Xiig.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Modern node with four eight-core processors that share a common memory pool. Ref: <a class="af nc" href="https://cvw.cac.cornell.edu/parallel/hpc/nodes" rel="noopener ugc nofollow" target="_blank">Cornell Virtual Workshop</a></figcaption></figure><p id="27ec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The <strong class="nf fr">non-physical </strong>components of a system include threads and processes —</p><ol class=""><li id="e381" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Thread</strong> — Thread is a <strong class="nf fr">sequence of CPU instructions</strong> that operating system treats as a single unit for scheduling and execution purposes.</li><li id="5254" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Process </strong>— In computing, a process is a <strong class="nf fr">coherent unit</strong> of resource allocation, including memory, file handlers, ports and devices. A single process may manage resources for several threads. <a class="af nc" href="https://cvw.cac.cornell.edu/parallel/terminology/threads-processes" rel="noopener ugc nofollow" target="_blank">Threads can be modelled as components of a process.</a></li></ol><h2 id="bbbe" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">So, do threads run on cores on the same system, or can they be spread across different systems for a single program? And in either case, how do they communicate? How’s memory handled for these threads ? Do they share it, or do they each get their own separate memory?</h2><p id="5b6c" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">A single program can execute across multiple cores on the same or different systems/ nodes. The design of the system and the program determines whether it aligns with the desired execution strategy.</p><p id="6752" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When designing a system, three key aspects must be considered: <strong class="nf fr">execution </strong>(how threads run), <strong class="nf fr">memory access</strong> (how memory is allocated to these threads), and <strong class="nf fr">communication</strong> (how threads communicate, especially when they need to update the same data). It’s important to note that these aspects are mostly interdependent.</p><p id="7a2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Execution</strong></p><p id="efc7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Serial execution<em class="qa"> </em></strong><em class="qa">— </em>This uses a single thread of execution to work on a single data item at any time.</p><p id="3433" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Parallel execution </strong>— In this, more than one thing happens simultaneously. In computing, this can be —</p><ol class=""><li id="4f29" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">One worker </strong>— A single thread of execution operating on multiple data items simultaneously (vector instructions in a CPU). Imagine a single person sorting a deck of cards by suit. With four suits to categorize, the individual must go through the entire deck to organize the cards for each suit.</li><li id="d4aa" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Working Together</strong> — Multiple threads of execution in a single process. It is equivalent to multiple people working together to sort a single deck of cards by suit.</li><li id="8708" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Working Independently</strong> — Multiple processes can work on the same problem, utilizing either the same node or multiple nodes. In this scenario, each person would be sorting their deck of cards.</li><li id="fb56" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk">Any combination of the above.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/6d28c89a540999a37bf692c49d4ab184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_mxYaX9oCVZ1agibGg7CvQ.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Working Together: Two workers need to insert cards from the same suit. Worker A is holding the partial results<br/>for the clubs suit, so Worker B is temporarily blocked. Ref: <a class="af nc" href="https://cvw.cac.cornell.edu/parallel/intro/working-together" rel="noopener ugc nofollow" target="_blank">Cornell Virtual Workshop</a></figcaption></figure><p id="23ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Memory Access</strong></p><ol class=""><li id="2aca" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Shared Memory </strong>— When a program runs on multiple cores (<strong class="nf fr">a single multithreaded process</strong>) on the same system, each thread within the process has access to memory in the same virtual address space.</li><li id="79a2" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Distributed Memory </strong>— A distributed memory design is employed when a program utilizes <strong class="nf fr">multiple processes</strong> (whether on a single node or across different nodes). In this architecture, each process owns a portion of the data, and other processes must send messages to the owner to update their respective parts. Even when multiple processes run on a single node, each has its own virtual memory space. Therefore, such processes should use distributed memory programming for communication.</li><li id="c343" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Hybrid Strategy </strong>— <strong class="nf fr">Multithreaded processes</strong> that can run on the same or different nodes, designed to use multiple cores on a single node through shared memory programming. At the same time, they can employ distributed memory strategies to coordinate with processes on other nodes. Imagine multiple people or threads working in multiple cubicles in the image above. Workers in the same cubicle communicate using shared memory programming, while those in different cubicles interact through distributed memory programming.</li></ol><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/55c24c2dcde05cd6ce2f376d807bdda8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lspL31BaeZGB8CTI4O5AKg.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">In a distributed memory design, parallel workers are assigned to different cubicles (processes). Ref: <a class="af nc" href="https://cvw.cac.cornell.edu/parallel/terminology/tasks" rel="noopener ugc nofollow" target="_blank">Cornell Virtual Workshop</a></figcaption></figure><p id="8b40" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Communication</strong></p><p id="4e20" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The communication mechanism depends on the memory architecture. In <strong class="nf fr">shared memory architectures</strong>, application programming interfaces like <strong class="nf fr">OpenMP </strong>(Open Multi-Processing)<strong class="nf fr"> </strong>enable communication between threads that share memory and data. On the other hand, <strong class="nf fr">MPI </strong>(Message Passing Interface) can be used for communication between processes running on the same or different nodes in <strong class="nf fr">distributed memory architectures</strong>.</p><h1 id="332b" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Parallelization Strategy and Performance</h1><h2 id="9df1" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">How can we tell if our parallelization strategy is working effectively?</h2><p id="b367" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">There are several methods, but here, we discuss efficiency and speedup. In parallel computing, efficiency refers to the proportion of available resources that are actively utilized during a computation. It is determined by comparing the actual resource utilization against the peak performance, i.e., optimal resource utilization.</p><p id="b8d5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Actual processor utilization </strong>refers to the number of floating point operations (FLOP) performed over a specific period.</p><p id="c7b0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Peak performance</strong> assumes that each processor core executes the maximum possible FLOPs during every clock cycle.</p><p id="3be5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Efficiency for parallel code</strong> is the ratio of actual floating-point operations per second (FLOPS) to the peak possible performance.</p><p id="4ccf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Speedup </strong>is used to assess efficiency and is measured as:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qc"><img src="../Images/917b99538054a2358cb3c512063c8ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*nqn0qZIR1DiE0OcMd5l-JA.png"/></div></figure><p id="b69a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://cvw.cac.cornell.edu/parallel/efficiency/about-efficiency" rel="noopener ugc nofollow" target="_blank">Speedup cannot be greater than the number of parallel resources</a> when programs are limited by computing speed of the processors.</p><p id="d626" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using speedup, parallel efficiency is measured as :</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/5979f6439a7de8b14daa2056e1bdd6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*NzwvyEFj7F4JeqebOiMVew.png"/></div></figure><blockquote class="qe qf qg"><p id="f12b" class="nd ne qa nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://cvw.cac.cornell.edu/parallel/efficiency/about-efficiency" rel="noopener ugc nofollow" target="_blank"><em class="fq">Hence, a worthy goal for optimization of a parallel program is to bring its speedup as close as possible to the number of cores.</em></a></p></blockquote><p id="87a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Suppose the serial execution of code took 300 seconds. After parallelizing the tasks using 50 cores, the overall wall-clock<strong class="nf fr"> </strong>time for parallel execution was 6 seconds. In this case, the speedup can be calculated as the wall-clock time for serial execution divided by the wall-clock time for parallel execution, resulting in a speedup of 300s/6s = 50. We get parallel efficiency by dividing the speedup by the number of cores, 50/50 ​= 1. This is an example of the best-case scenario: the workload is perfectly parallelized, and all cores are utilized efficiently.</p><h2 id="931b" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Will adding more computing units constantly improve performance if the data size or number of tasks increases?</h2><p id="4208" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">Only sometimes. In parallel computing, we have two types of scaling based on the problem size or the number of parallel tasks.</p><p id="be6c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Strong Scaling</strong> — Increasing the number of parallel tasks while keeping the problem size constant. However, even as we increase the number of computational units (cores, processors, or nodes) to process more tasks in parallel, there is an overhead associated with communication between these units or the host program, such as the time spent sending and receiving data.</p><blockquote class="qe qf qg"><p id="9b20" class="nd ne qa nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">S<a class="af nc" href="https://cvw.cac.cornell.edu/parallel/efficiency/scaling" rel="noopener ugc nofollow" target="_blank"><em class="fq">trong scaling performance is affected by the ratio of time communicating to the time spent computing. The larger the ratio, worse the strong scaling behaviour will be.</em></a></p></blockquote><p id="d4ac" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Ideally, the execution time decreases as the number of parallel tasks increases. However, if the code doesn’t get faster with strong scaling, it could indicate that we’re using too many tasks for the amount of work being done.</p><p id="5238" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Weak Scaling — </strong>In this, problem size increases as the number of tasks increase, so computation per task remains constant. If your program has good weak scaling performance, you can run a problem twice as large on twice as many nodes in the same wall-clock time.</p><h2 id="395b" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">There are restrictions around what we can parallelize since some operations can’t be parallelized. Is that right?</h2><p id="f06d" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">Yes, parallelizing certain sequential operations can be quite challenging. Parallelizing depends on multiple instruction streams and/or multiple data streams.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qh"><img src="../Images/826d67454a004c65b755cbb6b327bc25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVd6CHtrR8QIw2vUoYumuA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Different types of parallel computing architectures. Ref: <a class="af nc" href="https://cvw.cac.cornell.edu/parallel/hpc/taxonomy-parallel-computers" rel="noopener ugc nofollow" target="_blank">Cornell Virtual Workshop</a></figcaption></figure><p id="569c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To understand what can be parallelized, let’s look at SIMD in CPUs, which is achieved using vectorization.</p><blockquote class="qe qf qg"><p id="d77b" class="nd ne qa nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="fq">Vectorization is a programming technique in which operations are applied to entire arrays at once rather than processing individual elements one by one. It is achieved using the vector unit in processors, which includes vector registers and vector instructions.</em></p></blockquote><p id="e63a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Consider a scenario where we iterate over an array and perform multiple operations on a single element within a for loop. When the data is independent, writing vectorizable code becomes straightforward; see the example below:</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="1545" class="qm oa fq qj b bg qn qo l qp qq">do i, n<br/>  a(i) = b(i) + c(i)<br/>  d(i) = e(i) + f(i)<br/>end do</span></pre><p id="c40c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this loop, each iteration is independent — meaning <code class="cx qr qs qt qj b">a(i)</code> is processed independently of <code class="cx qr qs qt qj b">a(i+1)</code> and so on. Therefore, this code is vectorizable, that will allow multiple elements of array <code class="cx qr qs qt qj b">a</code> to be computed in parallel using elements from <code class="cx qr qs qt qj b">b</code> and <code class="cx qr qs qt qj b">c</code>, as demonstrated below:</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="de89" class="qm oa fq qj b bg qn qo l qp qq">b:  | b(i) | b(i+1) | b(i+2) | b(i+3) | ... |<br/>c:  | c(i) | c(i+1) | c(i+2) | c(i+3) | ... |<br/>------------------------------------------------<br/>           Vectorized Addition (SIMD)<br/><br/>Vector Register 1 (loaded with b values):<br/>      | b(i)   | b(i+1) | b(i+2) | b(i+3) | ... |<br/><br/>Vector Register 2 (loaded with c values):<br/>      | c(i)   | c(i+1) | c(i+2) | c(i+3) | ... |<br/><br/>------------------------------------------------<br/>Result in Vector Register 3:<br/>      | a(i)   | a(i+1) | a(i+2) | a(i+3) | ... |</span></pre><blockquote class="qe qf qg"><p id="58e6" class="nd ne qa nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="fq">Modern compilers are generally capable of analyzing such loops and transforming them into sequences of vector operations. </em><a class="af nc" href="https://cvw.cac.cornell.edu/vector/coding/data-dependencies" rel="noopener ugc nofollow" target="_blank"><em class="fq">Problem arises when an operation in one iteration depends upon the result of a previous iteration. In this case, automatic vectorization might lead to incorrect results. This situation is known as a data dependency.</em></a></p></blockquote><p id="81d1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Data dependencies commonly encountered in scientific code are -</p><p id="05fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Read After Write (RAW) </strong>— <em class="qa">Not Vectorizable</em></p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="21a9" class="qm oa fq qj b bg qn qo l qp qq">do i, n<br/>  a(i) = a(i-1) +b(i)</span></pre><p id="b8b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Write After Read (WAR) </strong>— <em class="qa">Vectorizable</em></p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="1a4a" class="qm oa fq qj b bg qn qo l qp qq">do i, n<br/>  a(i) = a(i+1) +b(i)</span></pre><p id="4625" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Write After Write (WAW)</strong> — <em class="qa">Not Vectorizable</em></p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="eb69" class="qm oa fq qj b bg qn qo l qp qq">do i, n<br/>  a(i%2) = a(i+1) +b(i)</span></pre><p id="e18d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Read After Read (RAR) </strong>— <em class="qa">Vectorizable</em></p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="4ec4" class="qm oa fq qj b bg qn qo l qp qq">do i, n<br/>  a(i) = b(i%2) + c(i)</span></pre><p id="5054" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Adhering to certain standard rules for vectorization — such as ensuring independent assignments in loop iterations, avoiding random data access, and preventing dependencies between iterations — can help write vectorizable code.</p><h1 id="9b96" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">GPU architecture and cross-architectural code</h1><h2 id="65c0" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">When data increases, it makes sense to parallelize as many parallelizable operations as possible to create scalable solutions, but that means we need bigger systems with lots of cores. Is that why we use GPUs? How are they different from CPUs, and what leads to their high throughput?</h2><p id="1d8b" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">YES!</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/7b996318622b4ea4b0acd28a9998e877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TMELFXYisIQC9hlacy6iIQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Comparing the relative capabilities of the basic elements of CPU and GPU architectures. Ref:<a class="af nc" href="https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/design" rel="noopener ugc nofollow" target="_blank"> Cornell Virtual Workshop</a></figcaption></figure><p id="f187" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">GPUs (Graphics processing units) </strong>have many more processor units (green) and higher aggregate <em class="qa">memory bandwidth</em> (the amount of data transferred per unit of time) as compared to CPUs, which, on the other hand, have more sophisticated instruction processing and faster clock speed. As seen above, CPUs have more cache memory than GPUs. However, CPUs have fewer arithmetic logic units (ALUs) and floating point units (FPUs) than GPUs. Considering these points, <strong class="nf fr">using CPUs for complex workflow and GPUs for computationally intensive tasks is intuitive.</strong></p><p id="2cf2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/performance" rel="noopener ugc nofollow" target="_blank">GPUs are designed to produce high computational throughput</a> using their massively parallel architecture. Their computational potential can be measured in billions of floating point operations per second (GFLOPS). GPU hardware comes in the form of standard graphic cards (NVIDIA quad), High-end accelerator cards (NVIDIA Tesla), etc.</p><p id="a88b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Two key properties of the graphics pipeline that enable parallelization and, thus, high throughput are —</p><ol class=""><li id="5422" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Independence of Objects — </strong>A typical graphics scene consists of many independent objects; each object can be processed separately without dependencies on the others.</li><li id="9fb1" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Uniform Processing Steps — </strong>The sequence of processing steps is the same for all objects.</li></ol><h2 id="5d6e" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">So, multiple cores of GPUs work on different data at the same time, executing computations in parallel like a SIMD (Single Instruction Multiple Data) architecture. How are tasks divided between cores? Does each core run a single thread like in the CPU?</h2><p id="9f1f" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">In a GPU, <strong class="nf fr">Streaming Multiprocessors (SMs)</strong> are similar to cores in a CPU. Cores in GPUs are similar to vector lanes in CPUs. SMs are the hardware units that house cores.</p><p id="0ea9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When a function or computation, referred as a <strong class="nf fr">kernel, </strong>is executed on the GPU, it is often broken down into <strong class="nf fr">thread blocks</strong>. These thread blocks contain <strong class="nf fr">multiple threads; </strong>each SM can manage many threads across its cores. If there are more thread blocks than SMs, multiple thread blocks can be assigned to a single SM. Also, multiple threads can run on a single core.</p><p id="12ba" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://cvw.cac.cornell.edu/gpu-architecture/gpu-characteristics/simt_warp" rel="noopener ugc nofollow" target="_blank">Each SM further divides the <strong class="nf fr">thread blocks</strong> into groups called <strong class="nf fr">warps</strong>, with each warp consisting of <strong class="nf fr">32 threads</strong>.</a> These threads execute the same stream of instructions on different data elements, following a <strong class="nf fr">Single Instruction, Multiple Data (SIMD)</strong> model. The warp size is set to 32 because, in NVIDIA’s architecture, CUDA cores are grouped into sets of 32. This enables all threads in a warp to be processed together in parallel by the 32 CUDA cores, achieving high efficiency and optimized resource utilization.</p><p id="aa5f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In <strong class="nf fr">SIMD (Single Instruction, Multiple Data)</strong>, a single instruction acts uniformly on all data elements, with each data element processed in exactly the same way. <strong class="nf fr">SIMT (Single Instruction, Multiple Threads)</strong>, which is commonly used in GPUs, relaxes this restriction. In SIMT, threads can be activated or deactivated so that instruction and data are processed in active threads; however, the local data remains unchanged on inactive threads.</p><h2 id="60a6" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">I want to understand how we can code to use different architectures. Can similar code work for both CPU and GPU architectures? What parameters and methods can we use to ensure that the code efficiently utilizes the underlying hardware architecture, whether it’s CPUs or GPUs?</h2><p id="8745" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">Code is generally written in high-level languages like C or C++ and must be converted into binary code by a compiler since machines cannot directly process high-level instructions. While both GPUs and CPUs can execute the same kernel, as we will see in the example code, we need to use directives or parameters to run the code on a specific architecture to compile and generate an instruction set for that architecture. This approach allows us to use architecture-specific capabilities. To ensure compatibility, we can specify the appropriate flags for the compiler to produce binary code optimized for the desired architecture, whether it is a CPU or a GPU.</p><p id="7cfe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Various coding frameworks, such as SYCL, CUDA, and Kokkos, are used to write kernels or functions for different architectures. In this article, we will use examples from Kokkos.</p><p id="a474" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">A bit about Kokkos — </strong>An open-source C++ programming model for performance portability for writing Kernels: it is implemented as a template library on top of CUDA, OpenMP, and other backends and aims to be descriptive, in the sense that we define <em class="qa">what</em> we want to do rather than prescriptive (how we want to do it). Kokkos Core provides a programming model for parallel algorithms that uses many-core chips and shares memory across those cores.</p><p id="9e25" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A <strong class="nf fr">kernel</strong> has three components —</p><ol class=""><li id="9e6b" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Pattern — </strong>Structure of the computation: for, scan, reduction, task-graph</li><li id="df51" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Execution policy — </strong>How computations are executed: static scheduling, dynamic scheduling, thread teams.</li><li id="94df" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Computational Body — </strong>Code which performs each unit of work. e.g., loop body</li></ol><p id="45f1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Pattern and policy drive computational body. In the example below, used just for illustration, ‘for<strong class="nf fr">’</strong> is the <em class="qa">pattern</em><strong class="nf fr">, </strong>the <em class="qa">condition</em> to control the pattern (element=0; element&lt;n; ++element) is the <em class="qa">policy</em>, and the <em class="qa">computational body</em> is the code executed within the pattern</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="f7aa" class="qm oa fq qj b bg qn qo l qp qq">for (element=0; element&lt;n; ++element){<br/>  total = 0;<br/>  for(qp = 0; qp &lt; numQPs; ++qp){<br/>    total += dot(left[element][qp], right[element][qp]);<br/>  } <br/>  elementValues[element] = total;<br/>}</span></pre><p id="195d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Kokkos framework allows developers to define parameters and methods based on three key factors: where the code will run <strong class="nf fr">(Execution Space)</strong>, what memory resources will be utilized <strong class="nf fr">(Memory Space)</strong>, and how data will be structured and managed <strong class="nf fr">(Data Structure and Data management</strong>).</p><p id="a2fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We primarily discuss how to write the Kokkos kernel for the <strong class="nf fr">vector-matrix product</strong> to understand how these factors are implemented for different architectures.</p><p id="8090" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">But before that, let’s discuss the building blocks of the kernel we want to write.</p><p id="4d2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Memory Space —</strong></p><p id="0ff2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Kokkos provides a range of memory space options that enable users to control memory management and data placement on different computing platforms. Some commonly used memory spaces are —</p><ol class=""><li id="8a67" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">HostSpace — </strong>This memory space represents the CPU’s main memory. It is used for computations on the CPU and is typically the default memory space when working on a CPU-based system.</li><li id="b5cb" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">CudaSpace — </strong>CudaSpace is used for NVIDIA GPUs with CUDA. It provides memory allocation and management for GPU devices, allowing for efficient data transfer and computation.</li><li id="576e" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">CudaUVMSpac — </strong>For Unified Virtual Memory (UVM) systems, such as those on some NVIDIA GPUs, CudaUVMSpace enables the allocation of memory accessible from both the CPU and GPU without explicit data transfers. Cuda runtime automatically handles data movement at a performance hit.</li></ol><p id="076e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It is also essential to discuss <strong class="nf fr">memory layout</strong>, which refers to the organization and arrangement of data in memory. Kokkos provides several memory layout options to help users optimize their data storage for various computations. Some commonly used memory layouts are —</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qv"><img src="../Images/1049e781fcb83e45fec8455b8c008e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*3bh5KUgypepnKvY6sCrRKg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Row-major vs Column-major iteration of a matrix. Ref: <a class="af nc" href="https://en.wikipedia.org/wiki/Row-_and_column-major_order" rel="noopener ugc nofollow" target="_blank">Wikipedia</a></figcaption></figure><ol class=""><li id="94af" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">LayoutRight (also known as Row-Major) </strong>is the default memory layout for multi-dimensional arrays in C and C++. In LayoutRight, the rightmost index varies most rapidly in memory. If no layout is chosen, the default layout for HostSpace is LayoutRight.</li><li id="71b9" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">LayoutLeft (also known as Column-Major) — </strong>In LayoutLeft, the leftmost index varies most rapidly in memory. If no layout is chosen, the default layout for CudaSpace is LayoutLeft.</li></ol><p id="86e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the programmatic implementation below, we defined memory space and layout as macros based on the compiler flag ENABLE_CUDA, which will be True if we want to run our code on GPU and False for CPU.</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="5c0c" class="qm oa fq qj b bg qn qo l qp qq">    // ENABLE_CUDA is a compile time argument with default value true<br/>    #define ENABLE_CUDA true<br/>    <br/>    // If CUDA is enabled, run the kernel on the CUDA (GPU) architecture<br/>    #if defined(ENABLE_CUDA) &amp;&amp; ENABLE_CUDA<br/>        #define MemSpace Kokkos::CudaSpace<br/>        #define Layout Kokkos::LayoutLeft<br/>    #else<br/>    // Define default values or behavior when ENABLE_CUDA is not set or is false<br/>        #define MemSpace Kokkos::HostSpace<br/>        #define Layout Kokkos::LayoutRight<br/>    #endif</span></pre><p id="b6bc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Data Structure and Data Management —</strong></p><p id="84b8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Kokkos Views <em class="qa">— </em></strong>In Kokkos, a “view” is a <strong class="nf fr">fundamental data structure</strong> representing one-dimensional and multi-dimensional arrays, which can be used to store and access data efficiently. Kokkos views provide a high-level abstraction for managing data and is designed to work seamlessly with different execution spaces and memory layouts.</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="86e0" class="qm oa fq qj b bg qn qo l qp qq">    // View for a 2d array of data type double<br/>    Kokkos::View&lt;double**&gt; myView("myView", numRows, numCols);<br/>    // Access Views<br/>    myView(i, j) = 42.0; <br/>    double value = myView(i, j);</span></pre><p id="0a2e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Kokkos Mirroring technique for data management </strong>— Mirrors are views of equivalent arrays residing in possible different memory spaces, which is when we need data in both CPU and GPU architecture. This technique is helpful for scenarios like reading data from a file on the CPU and subsequently processing it on the GPU. Kokkos’ mirroring creates a mirrored view of the data, allowing seamless sharing between the CPU and GPU execution spaces and facilitating data transfer and synchronization.</p><p id="0048" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To create a mirrored copy of the primary data, we can use Kokkos’<em class="qa"> create_mirror_view()</em> function. This function generates a mirror view in a specified execution space (e.g., GPU) with the same data type and dimensions as the primary view.</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="58ac" class="qm oa fq qj b bg qn qo l qp qq">    // Intended Computation -<br/>    // &lt;y, A*x&gt; = y^T * A * x<br/>    // Here:<br/>    // y and x are vectors.<br/>    // A is a matrix.<br/>    <br/>    // Allocate y, x vectors and Matrix A on device<br/>    typedef Kokkos::View&lt;double*, Layout, MemSpace&gt;   ViewVectorType;<br/>    typedef Kokkos::View&lt;double**, Layout, MemSpace&gt;  ViewMatrixType;<br/>    <br/>    // N and M are number of rows and columns<br/>    ViewVectorType y( "y", N );<br/>    ViewVectorType x( "x", M );<br/>    ViewMatrixType A( "A", N, M );<br/>    <br/>    // Create host mirrors of device views<br/>    ViewVectorType::HostMirror h_y = Kokkos::create_mirror_view( y );<br/>    ViewVectorType::HostMirror h_x = Kokkos::create_mirror_view( x );<br/>    ViewMatrixType::HostMirror h_A = Kokkos::create_mirror_view( A );<br/><br/>    // Initialize y vector on host.<br/>    for ( int i = 0; i &lt; N; ++i ) {<br/>        h_y( i ) = 1;<br/>    }<br/><br/>    // Initialize x vector on host.<br/>    for ( int i = 0; i &lt; M; ++i ) {<br/>        h_x( i ) = 1;<br/>    }<br/><br/>    // Initialize A matrix on host.<br/>    for ( int j = 0; j &lt; N; ++j ) {<br/>        for ( int i = 0; i &lt; M; ++i ) {<br/>            h_A( j, i ) = 1;<br/>        }<br/>    }<br/><br/>    // Deep copy host views to device views.<br/>    Kokkos::deep_copy( y, h_y );<br/>    Kokkos::deep_copy( x, h_x );<br/>    Kokkos::deep_copy( A, h_A );</span></pre><p id="0fca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Execution Space —</strong></p><p id="ae77" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In Kokkos, the <strong class="nf fr">execution space</strong> refers to the specific computing environment or hardware platform where parallel operations and computations are executed. Kokkos abstracts the execution space, enabling code to be written in a descriptive manner while adapting to various hardware platforms.</p><p id="4bee" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We discuss two primary execution spaces —</p><ol class=""><li id="49f7" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pr ps pt bk"><strong class="nf fr">Serial</strong>: The Serial execution space is a primary and portable option suitable for single-threaded CPU execution. It is often used for debugging, testing, and as a baseline for performance comparisons.</li><li id="5b0f" class="nd ne fq nf b go pu nh ni gr pv nk nl nm pw no np nq px ns nt nu py nw nx ny pr ps pt bk"><strong class="nf fr">Cuda</strong>: The Cuda execution space is used for NVIDIA GPUs and relies on CUDA technology for parallel processing. It enables efficient GPU acceleration and management of GPU memory.</li></ol><p id="6d11" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Either the ExecSpace can be defined, or it can be determined dynamically based on the Memory space as below:</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="a647" class="qm oa fq qj b bg qn qo l qp qq"><br/>// Execution space determined based on MemorySpace<br/>using ExecSpace = MemSpace::execution_space;</span></pre><h2 id="8d46" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">How can we use these building blocks to write an actual kernel? Can we use it to compare performance between different architectures?</h2><p id="3f91" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">For the purpose of writing a kernel and performance comparison, we use following computation:</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="4cee" class="qm oa fq qj b bg qn qo l qp qq">&lt;y, A*x&gt; = y^T * (A * x)<br/><br/>Here:<br/><br/>y and x are vectors.<br/><br/>A is a matrix.<br/><br/>&lt;y, A*x&gt; represents the inner product or dot product of vectors y <br/>and the result of the matrix-vector multiplication A*x.<br/><br/>y^T denotes the transpose of vector y.<br/><br/>* denotes matrix-vector multiplication.</span></pre><p id="1a21" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The kernel for this operation in Kokkos —</p><pre class="mm mn mo mp mq qi qj qk bp ql bb bk"><span id="05d2" class="qm oa fq qj b bg qn qo l qp qq">    // Use a RangePolicy.<br/>    typedef Kokkos::RangePolicy&lt;ExecSpace&gt;  range_policy;<br/><br/>    // The below code is run for multiple iterations across different <br/>    // architectures for time comparison<br/>    Kokkos::parallel_reduce( "yAx", range_policy( 0, N ),<br/>                KOKKOS_LAMBDA ( int j, double &amp;update ) {<br/>                    double temp2 = 0;<br/><br/>                    for ( int i = 0; i &lt; M; ++i ) {<br/>                        temp2 += A( j, i ) * x( i );<br/>                    }<br/>                    update += y( j ) * temp2;<br/>            }, result );</span></pre><p id="4e88" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the above kernel, <code class="cx qr qs qt qj b">parallel_reduce</code> serves as the pattern, <code class="cx qr qs qt qj b">range_policy</code> defines the policy, and the actual operations constitute the computational body.</p><p id="16e0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I executed this kernel on a TACC Frontera node which has an NVIDIA Quadro RTX 5000 GPU. The experiments were performed with varying values of <strong class="nf fr">N</strong>, which refers to the lengths of the vectors <strong class="nf fr">y</strong> and <strong class="nf fr">x</strong>, and the number of rows in matrix <strong class="nf fr">A</strong>. Computation was performed 100 times to get notable results, and the execution time of the kernel was recorded for both Serial (Host) and CUDA execution spaces. I used <code class="cx qr qs qt qj b"><strong class="nf fr">ENABLE_CUDA</strong></code> compiler flag to switch between execution environments: <strong class="nf fr">True</strong> for GPU/CUDA execution space and <strong class="nf fr">False</strong> for CPU/serial execution space. The results of these experiments are presented below, with the corresponding speedup.</p><figure class="mm mn mo mp mq mr"><div class="qw io l ed"><div class="qx qy l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Data on kernel execution runtime data and speedup for CPU vs GPU architecture Ref: Table by author</figcaption></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/29171354662a8b3bac463f48c99db523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wJAOH1-RgGlhuI9oHe0u_g.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Speedup trend for varying data size (GPU vs CPU) Ref: Image by author</figcaption></figure><p id="b35e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We notice that the speedup increases significantly with the size of N, indicating that the CUDA implementation becomes increasingly advantageous for larger problem sizes.</p><p id="cc53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That’s all for now! I hope this article has been helpful in getting started on the right foot in exploring the domain of computing. Understanding the basics of the GPU architecture is crucial, and this article introduces one way of writing cross-architectural code that I experimented with. However, there are several methods and technologies worth exploring.</p><p id="e008" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While I’m not a field expert, this article reflects my learning journey from my brief experience working at TACC in Austin, TX. I welcome feedback and discussions, and I would be happy to assist if you have any questions or want to learn more. Please refer to the excellent resources below for further learning. Happy computing!</p></div></div></div><div class="ab cb ra rb rc rd" role="separator"><span class="re by bm rf rg rh"/><span class="re by bm rf rg rh"/><span class="re by bm rf rg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4d9c" class="ov oa fq bf ob ow ox oy oe oz pa pb oh nm pc pd pe nq pf pg ph nu pi pj pk pl bk">Acknowledgments</h2><p id="eb04" class="pw-post-body-paragraph nd ne fq nf b go pm nh ni gr pn nk nl nm po no np nq pp ns nt nu pq nw nx ny fj bk">This article draws from three primary sources. The first is the graduate-level course <strong class="nf fr">SDS394: Scientific and Technical Computing at UT Austin</strong>, which provided essential background knowledge on single-core multithreaded systems. The second is the <a class="af nc" href="https://cvw.cac.cornell.edu/parallel" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Cornell Virtual Workshop: Parallel Programming Concepts and High Performance Computing</strong></a>, an excellent resource for learning about parallel computing. The Kokkos code implementation is primarily based on materials available at <a class="af nc" href="https://github.com/kokkos/kokkos-tutorials" rel="noopener ugc nofollow" target="_blank"><strong class="nf fr">Kokkos Tutorials on GitHub</strong></a>. These are all amazing resources for anyone interested in learning more about parallel computing.</p><p id="d4da" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">— — — — — — — — — —</p><p id="c77d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The development of C++ Kokkos kernels for performance comparison across different architectures was part of a project supported by the <strong class="nf fr">Intel OneAPI Center of Excellence</strong> and the <strong class="nf fr">TACC STAR Scholars program</strong>, funded through generous contributions from TACC industry partners, including <strong class="nf fr">Intel, Shell, Exxon,</strong> and <strong class="nf fr">Chevron</strong>.</p><p id="2ac7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">— — — — — — — — — —</p><p id="1b02" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">References/Resources:</p><div class="ri rj rk rl rm rn"><a href="https://github.com/VictorEijkhout/TheArtofHPC_pdfs/tree/main?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">GitHub - VictorEijkhout/TheArtofHPC_pdfs: All pdfs of Victor Eijkhout's Art of HPC books and…</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">All pdfs of Victor Eijkhout's Art of HPC books and courses - VictorEijkhout/TheArtofHPC_pdfs</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">github.com</p></div></div><div class="rw l"><div class="rx l ry rz sa rw sb lr rn"/></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://docs.tacc.utexas.edu/hpc/frontera/?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">Frontera - TACC HPC Documentation</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">Last update: October 24, 2024 Important: Please note TACC's new SU charge policy. Frontera is funded by the National…</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">docs.tacc.utexas.edu</p></div></div><div class="rw l"><div class="sc l ry rz sa rw sb lr rn"/></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://cvw.cac.cornell.edu/parallel?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">Cornell Virtual Workshop: Parallel Programming Concepts and High Performance Computing</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">This roadmap explains parallel programming concepts, how parallel programming relates to high performance computing…</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">cvw.cac.cornell.edu</p></div></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://cvw.cac.cornell.edu/gpu-architecture?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">Cornell Virtual Workshop: Understanding GPU Architecture</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">This roadmap is intended for those who are relatively new to GPUs or who would like to learn more about computer technology that goes into them….</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">cvw.cac.cornell.edu</p></div></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://cvw.cac.cornell.edu/vector?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">Cornell Virtual Workshop: Vectorization</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">This roadmap describes the vectorization process as it relates to computing hardwares, compilers, and coding practices…</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">cvw.cac.cornell.edu</p></div></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://github.com/kokkos/kokkos-tutorials?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">GitHub — kokkos/kokkos-tutorials: Tutorials for the Kokkos C++ Performance Portability Programming…</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">Tutorials for the Kokkos C++ Performance Portability Programming Ecosystem — kokkos/kokkos-tutorials</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">github.com</p></div></div><div class="rw l"><div class="sd l ry rz sa rw sb lr rn"/></div></div></a></div><div class="ri rj rk rl rm rn"><a href="https://github.com/kokkos/kokkos-tutorials/wiki/Kokkos-Lecture-Series?source=post_page-----dd06e1f30586--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="ro ab ig"><div class="rp ab co cb rq rr"><h2 class="bf fr hw z io rs iq ir rt it iv fp bk">Kokkos Lecture Series</h2><div class="ru l"><h3 class="bf b hw z io rs iq ir rt it iv dx">Tutorials for the Kokkos C++ Performance Portability Programming Ecosystem - Kokkos Lecture Series ·…</h3></div><div class="rv l"><p class="bf b dy z io rs iq ir rt it iv dx">github.com</p></div></div><div class="rw l"><div class="se l ry rz sa rw sb lr rn"/></div></div></a></div></div></div></div></div>    
</body>
</html>