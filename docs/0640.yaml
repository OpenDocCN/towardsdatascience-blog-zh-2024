- en: How to Improve LLMs with RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac?source=collection_archive---------0-----------------------#2024-03-09](https://towardsdatascience.com/how-to-improve-llms-with-rag-abdc132f76ac?source=collection_archive---------0-----------------------#2024-03-09)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A beginner-friendly introduction w/ Python code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page---byline--abdc132f76ac--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page---byline--abdc132f76ac--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--abdc132f76ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--abdc132f76ac--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page---byline--abdc132f76ac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--abdc132f76ac--------------------------------)
    ·13 min read·Mar 9, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This article is part of a [larger series](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c)
    on using large language models in practice. In the [previous post](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32),
    we fine-tuned Mistral-7b-Instruct to respond to YouTube comments using QLoRA.
    Although the fine-tuned model successfully captured my style when responding to
    viewer feedback, its responses to technical questions didn’t match my explanations.
    Here, I’ll discuss how we can improve LLM performance using retrieval augmented
    generation (i.e. RAG).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/324d0e1a0754979b87fd7e0109c25717.png)'
  prefs: []
  type: TYPE_IMG
- en: The original RAG system. Image from Canva.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated an impressive ability to store
    and deploy vast knowledge in response to user queries. While this has enabled
    the creation of powerful AI systems like ChatGPT, compressing world knowledge
    in this way has **two key limitations**.
  prefs: []
  type: TYPE_NORMAL
- en: '**First**, an LLM’s knowledge is static, i.e., not updated as new information
    becomes available. **Second**, LLMs may have an insufficient “understanding” of
    niche and specialized information that was not prominent in their training data.
    These limitations can result in undesirable (and even fictional) model responses
    to user queries.'
  prefs: []
  type: TYPE_NORMAL
- en: One way we can mitigate these limitations is to **augment a model via a specialized
    and mutable knowledge base**, e.g., customer FAQs, software documentation, or
    product catalogs. This enables the creation of more…
  prefs: []
  type: TYPE_NORMAL
