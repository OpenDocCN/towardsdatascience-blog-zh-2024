- en: The LLM Triangle Principles to Architect Reliable AI Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16](https://towardsdatascience.com/the-llm-triangle-principles-to-architect-reliable-ai-apps-d3753dd8542e?source=collection_archive---------0-----------------------#2024-07-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Software design principles for thoughtfully designing reliable, high-performing
    LLM applications. A framework to bridge the gap between potential and production-grade
    performance.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[![Almog
    Baku](../Images/3ac36986f6ca0ba56c8edced6ec7dd07.png)](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    [Almog Baku](https://medium.com/@almogbaku?source=post_page---byline--d3753dd8542e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d3753dd8542e--------------------------------)
    ·16 min read·Jul 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) hold immense potential, but developing reliable
    production-grade applications remains challenging. After building dozens of LLM
    systems, I’ve distilled the formula for success into 3+1 fundamental principles
    that any team can apply.
  prefs: []
  type: TYPE_NORMAL
- en: “LLM-Native apps are 10% sophisticated model, and 90% experimenting data-driven
    engineering work.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building production-ready LLM applications requires *careful engineering practices*.
    When users cannot interact *directly* with the LLM, the prompt must be meticulously
    composed to cover all nuances, as *iterative user feedback may be unavailable*.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the LLM Triangle Principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLM Triangle Principles encapsulate the essential guidelines for building
    effective LLM-native apps. They provide a solid conceptual framework, guide developers
    in constructing robust and reliable LLM-native applications, and offer direction
    and support.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a29d388a95ba8611ab9ea94a2a9665d.png)'
  prefs: []
  type: TYPE_IMG
- en: An optimal LLM Usage is achieved by optimizing the three prominent principles
    through the lens of the SOP. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The Key Apices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LLM Triangle Principles introduces four programming principles to help you
    design and build LLM-Native apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first principle is the *Standard Operating Procedure (****SOP****).* The
    SOP guides the three apices of our triangle: ***Model***, ***Engineering Techniques*,**
    and ***Contextual Data***.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing the three apices principles *through the lens* of the **SOP is the
    key** **to ensuring a high-performing** LLM-native app.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Standard Operating Procedure (SOP)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[**S**tandard **O**perating **P**rocedure (SOP)](https://en.wikipedia.org/wiki/Standard_operating_procedure)
    is a well-known terminology in the industrial world. It’s a set of step-by-step
    instructions compiled by large organizations to help their workers carry out routine
    operations while maintaining high-quality and similar results each time. This
    practically turns inexperienced or low-skilled workers into experts by writing
    detailed instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: The LLM Triangle Principles borrow the SOP paradigm and encourage you to **consider
    the model as an inexperienced/unskilled worker**. We can ensure higher-quality
    results by “teaching” the model how an expert would perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c52e5439c5a35d0aa068834dbcf132b.png)'
  prefs: []
  type: TYPE_IMG
- en: The SOP ***guiding*** principle. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: “Without an SOP, even the most powerful LLM will fail to deliver consistently
    high-quality results.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When thinking about the **SOP *guiding* principle**, we should identify what
    techniques will help us implement the SOP most effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Cognitive modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create an SOP, we need to take our best-performing workers (domain experts),
    model how they think and work to achieve the same results, and write down everything
    they do.
  prefs: []
  type: TYPE_NORMAL
- en: After editing and formalizing it, we’ll have detailed instructions to help every
    inexperienced or low-skilled worker succeed and yield excellent work.
  prefs: []
  type: TYPE_NORMAL
- en: Like humans, it’s essential to *reduce the cognitive load* of the task by simplifying
    or splitting it. Following a simple step-by-step instruction is more straightforward
    than a lengthy, complex procedure.
  prefs: []
  type: TYPE_NORMAL
- en: During this process, we identify the hidden [*implicit cognition*](https://en.wikipedia.org/wiki/Implicit_cognition)
    *“jumps”* — the small, unconscious steps experts take that significantly impact
    the outcome. These subtle, unconscious, often unspoken assumptions or decisions
    can substantially affect the final result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e8abc91b025fdeea3cfe961d6f6111e.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of an “implicit cognition jump.” (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we want to model an SQL analyst. We’ll start by interviewing
    them and ask them a few questions, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: What do you do when you are asked to analyze a business problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you make sure your solution meets the request?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <reflecting the process as we understand to the interviewee>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this accurately capture your process? <getting corrections>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4f537cd288b3ad06ed31c92d586b5af4.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of the cognitive process that the analyst does and how to model it.
    (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The implicit cognition process takes many shapes and forms; a typical example
    is a “domain-specific definition.” For example, “bestseller” might be a prominent
    term for our domain expert, but not for everyone else.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66542e24426dcd8ad82a02786a45e3fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Expanding the implicit cognition process in our SQL analyst example. (Image
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we’ll have a full SOP “recipe” that allows us to emulate our top-performing
    analyst.
  prefs: []
  type: TYPE_NORMAL
- en: When mapping out these complex processes, it can be helpful to visualize them
    as a graph. This is especially helpful when the process is nuanced and involves
    many steps, conditions, and splits.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d292b6c118ddeba744f0a0967b1a39d6.png)'
  prefs: []
  type: TYPE_IMG
- en: The “SQL Analyst SOP” includes all the required technical steps, visualized
    as a graph. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Our final solution should mimic the steps defined in the SOP. In this stage,
    try to ignore the implementation—later, you can implement it across one or many
    steps/chains throughout our solution.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the rest of the principles, the cognitive modeling (SOP writing) is the
    *only standalone process*. It’s highly recommended that you model your process
    before writing code. That being said, while implementing it, you might go back
    and change it based on new insights or understandings you gained.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the importance of creating a well-defined SOP, that guides
    our *business understanding* of the problem, let’s explore how we can effectively
    implement it using various engineering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Engineering Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Engineering Techniques](https://www.promptingguide.ai/) help you practically
    implement your SOP and get the most out of the model. When thinking about the
    **Engineering Techniques principle**, we should consider what tools(techniques)
    in our toolbox can help us implement and shape our SOP and assist the model in
    communicating well with us.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6a821a075378b6d2e1cdd1d807de0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: The Engineering Techniques principle. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Some engineering techniques are only implemented in the prompt layer, while
    many require a software layer to be effective, and some combine both layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ddc3397969ad3a8ea0111ff3c09cae12.png)'
  prefs: []
  type: TYPE_IMG
- en: Engineering Techniques Layers. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'While many small nuances and techniques are discovered daily, I’ll cover two
    primary techniques: workflow/chains and agents.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. LLM-Native architectures (aka flow engineering or chains)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The LLM-Native Architecture describes the agentic flow your app is going through
    to yield the task’s result.
  prefs: []
  type: TYPE_NORMAL
- en: Each step in our flow is a standalone process that must occur to achieve our
    task. Some steps will be performed simply by deterministic code; for some, we
    will use an LLM (agent).
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we can reflect on the Standard Operating Procedure (SOP) we drew
    and think:'
  prefs: []
  type: TYPE_NORMAL
- en: Which SOP steps should we glue together to the same agent? And what steps should
    we split as different agents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What SOP steps should be executed in a standalone manner (but they might be
    fed with information from previous steps)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What SOP steps can we perform in a deterministic code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/45e4b1ee6d0a0a2ace59885dbeedee4e.png)'
  prefs: []
  type: TYPE_IMG
- en: An LLM-Native Architecture example for “Wikipedia writer” based on a given SOP.
    (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Before navigating to the next step in our architecture/graph, we should define
    its key properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs and outputs** — What is the signature of this step? What is required
    before we can take an action? (this can also serve as an output format for an
    agent)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality assurances—**What makes the response “good enough”? Are there cases
    that require human intervention in the loop? What kinds of assertions can we configure?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomous level** — How much control do we need over the result’s quality?
    What range of use cases can this stage handle? In other words, how much can we
    trust the model to work independently at this point?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Triggers** — What is the next step? What defines the next step?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-functional** — What’s the required latency? Do we need special business
    monitoring here?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover control** — What kind of failures(systematic and agentic) can occur?
    What are our fallbacks?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State management** — Do we need a special state management mechanism? How
    do we retrieve/save states (define the indexing key)? Do we need persistence storage?
    What are the different usages of this state(e.g., cache, logging, etc.)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.2\. What are agents?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An LLM agent is a standalone component of an LLM-Native architecture that involves
    calling an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: It’s an instance of LLM usage with the prompt containing the context. Not all
    agents are equal — Some will use “tools,” some won’t; some might be used “just
    once” in the flow, while others can be called recursively or multiple times, carrying
    the previous input and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Agents with tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some LLM agents can use “tools” — predefined functions for tasks like calculations
    or web searches. The agent outputs instructions specifying the tool and input,
    which the application executes, returning the result to the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the concept, let’s look at a simple prompt implementation for
    tool calling. This can work even with models not natively trained to call tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to distinguish between agents with tools (hence *autonomous agents*)
    and agents whose output can lead to performing an action.
  prefs: []
  type: TYPE_NORMAL
- en: “Autonomous agents are agents that have the ability to generate a way to accomplish
    the task.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Autonomous agents are *given the right* to **decide** if they should act and
    with what action. In contrast, a (nonautonomous) agent simply “processes” our
    request(e.g., classification), and based on this process, our deterministic code
    performs an action, and the model has zero control over that.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0e9e53e6f52e3df3279712dc420d130.png)'
  prefs: []
  type: TYPE_IMG
- en: An autonomous agent VS agent that triggers an action. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: As we increase the agent’s autonomy in planning and executing tasks, we enhance
    its decision-making capabilities but potentially reduce control over output quality.
    Although this might look like a magical solution to make it more “smart” or “advanced,”
    it comes with the cost of losing control over the quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f127e24bc282742d3b4399a1ba9ba57.png)'
  prefs: []
  type: TYPE_IMG
- en: The tradeoffs of an autonomous agent. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Beware the allure of fully autonomous agents. While their architecture might
    look appealing and simpler, using it for everything (or as the initial PoC) might
    be very deceiving from the “real production” cases. Autonomous agents are hard
    to debug and unpredictable(response with unstable quality), which makes them unusable
    for production.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, agents (without implicit guidance) are not very good at planning
    complex processes and usually skip essential steps. For example, in our “Wikipedia
    writer” use-case, they’ll just start writing and skip the systematic process.
    This makes agents (and autonomous agents especially) only as good as the model,
    or more accurately — only as good as the data they were trained on relative to
    your task.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of giving the agent (or a swarm of agents) the liberty to do everything
    end-to-end, try to hedge their task to a specific region of your flow/SOP that
    requires this kind of agility or creativity. This can yield higher-quality results
    because you can enjoy both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: 'An excellent example is [AlphaCodium](https://www.codium.ai/blog/alphacodium-state-of-the-art-code-generation-for-code-contests/):
    By combining a structured flow with different agents (including a novel agent
    that iteratively writes and tests code), they increased GPT-4 accuracy (pass@5)
    on CodeContests from 19% to 44%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e388535058a331dbe448aefe597b45f5.png)'
  prefs: []
  type: TYPE_IMG
- en: AlphaCodium’s LLM Architecture. (Image by the curtesy [Codium.ai](https://www.codium.ai/))
  prefs: []
  type: TYPE_NORMAL
- en: 'While engineering techniques lay the groundwork for implementing our SOP and
    optimizing LLM-native applications, we must also carefully consider another critical
    component of the LLM Triangle: the model itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we choose is a critical component of our project’s success—a large
    one (such as GPT-4 or Claude Opus) might yield better results but be quite costly
    at scale, while a smaller model might be less “smart” but help with the budget.
    When thinking about the **Model principle**, we should aim to identify our constraints
    and goals and what kind of model can help us fulfill them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2141352ce97abf24254c35db07b7d46.png)'
  prefs: []
  type: TYPE_IMG
- en: The Model principle. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: “Not all LLMs are created equal. Match the model to the mission.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The truth is that we don’t always need the largest model; it depends on the
    task. To find the right match, we must have an [experimental process](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    and try multiple variations of our solution.
  prefs: []
  type: TYPE_NORMAL
- en: It helps to look at our “inexperienced worker” analogy — a very “smart” worker
    with many academic credentials probably will succeed in some tasks easily. Still,
    they might be overqualified for the job, and hiring a “cheaper” candidate will
    be much more cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'When considering a model, we should define and compare solutions based on the
    tradeoffs we are willing to take:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task Complexity** — Simpler tasks (such as summarization) are easier to complete
    with smaller models, while reasoning usually requires larger models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference infrastructure** — Should it run on the cloud or edge devices?
    The model size might impact a small phone, but it can be tolerated for cloud-serving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pricing** — What price can we tolerate? Is it cost-effective considering
    the business impact and predicated usage?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency** — As the model grows larger, the latency grows as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Labeled data** — Do we have data we can use immediately to enrich the model
    with examples or relevant information that is not trained upon?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many cases, until you have the “in-house expertise,” it helps to pay a little
    extra for an experienced worker — the same applies to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have *labeled data*, start with a stronger (larger) model, *collect
    data*, and then utilize it to empower a model using a few-shot or fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Fine-tuning a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a few aspects that you must consider before resorting to fine-tune
    a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy** — Your data might include pieces of private information that must
    be kept from the model. You must anonymize your data to avoid legal liabilities
    if your data contains private information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Laws, Compliance, and Data Rights** — Some legal questions can be raised
    when training a model. For example, the OpenAI terms-of-use policy prevents you
    from training a model without OpenAI using generated responses. Another typical
    example is complying with the GDPR’s laws, which require a “right for revocation,”
    where a user can require the company to remove information from the system. This
    raises legal questions about whether the model should be retrained or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Updating latency —** The latency or data cutoff is much higher when training
    a model. Unlike embedding the new information via the context (see “4\. Contextual
    Data” section below), which provides immediate latency, training the model is
    a long process that takes time. Due to that, models are retrained less often.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Development and operation —** Implementing a reproducible, scalable, and
    monitored fine-tuning pipeline is essential while continuously evaluating the
    results’ performance. This complex process requires constant maintenance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost —** Retraining is considered expensive due to its complexity and the
    highly intensive resources(GPUs) required per training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability of LLMs to act as *in-context learners* and the fact that the newer
    models support a much larger context window simplify our implementation dramatically
    and can provide excellent results even without fine-tuning. Due to the complexity
    of fine-tuning, using it as a last resort or skipping it entirely is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, fine-tuning models for specific tasks (e.g., structured JSON output)
    or domain-specific language can be highly efficient. A small, task-specific model
    can be highly effective and much cheaper in inference than large LLMs. Choose
    your solution wisely, and assess all the relevant considerations before escalating
    to LLM training.
  prefs: []
  type: TYPE_NORMAL
- en: “Even the most powerful model requires relevant and well-structured contextual
    data to shine.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. Contextual Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***LLMs are in-context learners.*** That means that by providing task-specific
    information, the LLM agent can help us to perform it *without* special training
    or fine-tuning. This enables us to “teach” new knowledge or skills easily. When
    thinking about the **Contextual Data principle**, we should aim to organize and
    model the available data and how to compose it within our prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/879503f2a13a6e3cc811e6bb8ba4ecc5.png)'
  prefs: []
  type: TYPE_IMG
- en: The Contextual Data principle. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'To compose our context, we include the relevant (contextual) information within
    the prompt we send to the LLM. There are two kinds of contexts we can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Embedded contexts** — embedded information pieces provided as part of the
    prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Attachment contexts** — A list of information pieces glues by the beginning/end
    of the prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Contexts are usually implemented using a “prompt template” (such as [jinja2](https://jinja.palletsprojects.com/en/3.1.x/)
    or [mustache](https://mustache.github.io/) or simply native [formatting literal
    strings](https://docs.python.org/3/reference/lexical_analysis.html#formatted-string-literals));
    this way, we can compose them elegantly while keeping the essence of our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 4.1\. Few-shot learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Few-shot learning is a powerful way to “teach” LLMs by example without requiring
    extensive fine-tuning. Providing a few representative examples in the prompt can
    guide the model in understanding the desired format, style, or task.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we want the LLM to generate email responses, we could include
    a few examples of well-written responses in the prompt. This helps the model learn
    the preferred structure and tone.
  prefs: []
  type: TYPE_NORMAL
- en: We can use diverse examples to help the model catch different corner cases or
    nuances and learn from them. Therefore, it’s essential to include a variety of
    examples that cover a range of scenarios your application might encounter.
  prefs: []
  type: TYPE_NORMAL
- en: As your application grows, you may consider implementing “[Dynamic few-shot](https://arxiv.org/abs/1804.09458),”
    which involves programmatically selecting the most relevant examples for each
    input. While it increases your implementation complexity, it ensures the model
    receives the most appropriate guidance for each case, significantly improving
    performance across a wide range of tasks without costly fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Retrieval Augmented Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Retrieval Augmented Generation (RAG)](https://www.promptingguide.ai/techniques/rag)
    is a technique for retrieving relevant documents for additional context before
    generating a response. It’s like giving the LLM a quick peek at specific reference
    material to help inform its answer. This keeps responses current and factual without
    needing to retrain the model.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, on a support chatbot application, RAG could pull relevant help-desk
    wiki pages to inform the LLM’s answers.
  prefs: []
  type: TYPE_NORMAL
- en: This approach helps LLMs *stay current* and *reduces hallucinations* by grounding
    responses in retrieved facts. RAG is particularly handy for tasks that require
    updated or specialized knowledge without retraining the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we are building a support chat for our product. In that
    case, we can use RAG to retrieve a relevant document from our helpdesk wiki, then
    provide it to an LLM agent and ask it to compose an answer based on the question
    and provide a document.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three key pieces to look at while implementing RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval mechanism** — While the traditional implementation of RAG involves
    retrieving a relevant document using a vector similarity search, sometimes it’s
    better or cheaper to use simpler methods such as keyword-based search (like [BM-25](https://en.wikipedia.org/wiki/Okapi_BM25)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indexed data structure —**Indexing the entire document naively, without preprocessing,
    may limit the effectiveness of the retrieval process. Sometimes, we want to add
    a data preparation step, such as preparing a list of questions and answers based
    on the document.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata—**Storing relevant metadata allows for more efficient referencing
    and filtering of information (e.g., narrowing down wiki pages to only those related
    to the user’s specific product inquiry). This extra data layer streamlines the
    retrieval process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.3\. Providing relevant context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The context information relevant to your agent can vary. Although it may seem
    beneficial, providing the model (like the “unskilled worker”) with too much information
    can be overwhelming and irrelevant to the task. Theoretically, this causes the
    model to learn irrelevant information (or token connections), which can lead to
    confusion and [hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)).
  prefs: []
  type: TYPE_NORMAL
- en: When Gemini 1.5 was released and introduced as an LLM that could process up
    to 10M tokens, some practitioners questioned whether the context was still an
    issue. While it’s a fantastic accomplishment, especially for some use cases (such
    as chat with PDFs), it’s still limited, especially when reasoning over various
    documents.
  prefs: []
  type: TYPE_NORMAL
- en: Compacting the prompt and providing the LLM agent with only relevant information
    is crucial. This reduces the processing power the model invests in irrelevant
    tokens, improves the quality, optimizes the latency, and reduces the cost.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tricks to improve the relevancy of the provided context, most
    of which relate to how you store and catalog your data.
  prefs: []
  type: TYPE_NORMAL
- en: For RAG applications, it’s handy to add a data preparation that shapes the information
    you store (e.g., questions and answers based on the document, then providing the
    LLM agent only with the answer; this way, the agent gets a summarized and shorter
    context), and use re-ranking algorithms on top of the retrieved documents to refine
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: “Data fuels the engine of LLM-native applications. A strategic design of contextual
    data unlocks their true potential.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion and Implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLM Triangle Principles provide a structured approach to developing high-quality
    LLM-native applications, addressing the gap between LLMs’ enormous potential and
    real-world implementation challenges. Developers can create more reliable and
    effective LLM-powered solutions by focusing on 3+1 key principles—the **Model**,
    **Engineering Techniques**, and **Contextual Data**—all guided by a well-defined
    **SOP**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6559fd3272a486ef7e1bd7e6edd445e5.png)'
  prefs: []
  type: TYPE_IMG
- en: The LLM Triangle Principles. (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Key takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Start with a clear SOP**: Model your expert’s cognitive process to create
    a step-by-step guide for your LLM application. Use it as a guide while thinking
    of the other principles.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Choose the right model**: Balance capabilities with cost, and consider starting
    with larger models before potentially moving to smaller, fine-tuned ones.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Leverage engineering techniques**: Implement LLM-native architectures and
    use agents strategically to optimize performance and maintain control. Experiment
    with different prompt techniques to find the most effective prompt for your case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Provide relevant context**: Use in-context learning, including RAG, when
    appropriate, but be cautious of overwhelming the model with irrelevant information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate and experiment**: Finding the right solution often requires testing
    and refining your work. I recommend reading and implementing the [“Building LLM
    Apps: A Clear Step-By-Step Guide”](/building-llm-apps-a-clear-step-by-step-guide-1fe1e6ef60fd)
    tips for a detailed LLM-Native development process guide.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By applying the LLM Triangle Principles, organizations can move beyond a simple
    proof-of-concept and develop robust, production-ready LLM applications that truly
    harness the power of this transformative technology.
  prefs: []
  type: TYPE_NORMAL
- en: If you find this whitepaper helpful, please give it a few **claps** 👏 on Medium
    and **share** it with your fellow AI enthusiasts. Your support means the world
    to me! 🌍
  prefs: []
  type: TYPE_NORMAL
- en: Let’s keep the conversation going — feel free to reach out via [email](mailto:almog.baku@gmail.com)
    or [connect on LinkedIn](https://www.linkedin.com/in/almogbaku/) 🤝
  prefs: []
  type: TYPE_NORMAL
- en: Special thanks to [Gal Peretz](https://medium.com/u/532f8dc01db8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Gad Benram](https://medium.com/u/b45fa95a7293?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Liron Izhaki Allerhand](https://medium.com/u/251cd1007ce8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Itamar Friedman](https://medium.com/u/bcd07dca5f93?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Lee Twito](https://medium.com/u/241b56ab4bf?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Ofir Ziv](https://medium.com/u/2db20f6d91e8?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Philip Tannor](https://medium.com/u/5c5d2a69bcdb?source=post_page---user_mention--d3753dd8542e--------------------------------),
    [Yair Livne](https://medium.com/u/6f8924605cf6?source=post_page---user_mention--d3753dd8542e--------------------------------)
    and [Shai Alon](https://medium.com/u/fa1f5e83a0c8?source=post_page---user_mention--d3753dd8542e--------------------------------)
    for insights, feedback, and editing notes.
  prefs: []
  type: TYPE_NORMAL
