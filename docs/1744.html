<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Detecting Clouds with AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Detecting Clouds with AI</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17">https://towardsdatascience.com/detecting-clouds-with-ai-b553e6576af6?source=collection_archive---------3-----------------------#2024-07-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="dda5" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">From Random Forest to YOLO: Comparing different algorithms for cloud segmentation in satellite Images.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dr. Carmen Adriana Martínez Barbosa" class="l ep by dd de cx" src="../Images/caad66f044af1131e17dc28ea2f48863.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*NFhDEeUmBdi6gegYQbCMZQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://anamabo3.medium.com/?source=post_page---byline--b553e6576af6--------------------------------" rel="noopener follow">Dr. Carmen Adriana Martínez Barbosa</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b553e6576af6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="cbf5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><em class="ne">Written by: Carmen Martínez-Barbosa and José Arturo Celis-Gil</em></p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/ed5cf045e14d5d3a1a0af6ef2b47a0b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3pVGGzj4cW45aA5_bQbCnw.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Clouds on a green field full of flowers painted in Van Gogh's style. Image created by the authors using DALL.E.</figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8829" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Satellite imagery has revolutionized our world. Thanks to it, humanity can track, in real-time, changes in water, air, land, vegetation, and the footprint effects that we are producing around the globe. The applications that offer this kind of information are endless. For instance, they have been used <a class="af og" href="https://link.springer.com/article/10.1007/s10661-023-10989-1" rel="noopener ugc nofollow" target="_blank">to assess the impact of land use on river water quality</a>. Satellite images have also been used to <a class="af og" href="https://cdn.techscience.cn/files/iasc/2023/TSP_IASC-37-2/TSP_IASC_39057/TSP_IASC_39057.pdf" rel="noopener ugc nofollow" target="_blank">monitor wildlife</a> and observe <a class="af og" href="https://www.sciencedirect.com/science/article/abs/pii/S2210670723002640" rel="noopener ugc nofollow" target="_blank">the growth of the urban population</a>, among other things.</p><p id="398e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">According to the <a class="af og" href="https://www.ucsusa.org/about/history" rel="noopener ugc nofollow" target="_blank">Union of Concerned Scientists</a> (UCS), approximately <a class="af og" href="https://www.geospatialworld.net/prime/business-and-industry-trends/how-many-satellites-orbiting-earth/" rel="noopener ugc nofollow" target="_blank">one thousand Earth observation satellites are orbiting our planet</a>. However, one of the most known is <em class="ne">Sentinel-2. </em>Developed by the European Space Agency (ESA), <em class="ne">Sentinel-2</em> is an earth observation mission from the <a class="af og" href="https://en.wikipedia.org/wiki/Copernicus_Programme" rel="noopener ugc nofollow" target="_blank">Copernicus Programme</a> that acquires imagery at high spatial resolution (10 m to 60 m) over land and coastal waters. The data obtained by<em class="ne"> Sentinel-2</em> are multi-spectral images with 13 bands that run across the visible, near-infrared, and short-wave infrared parts of the electromagnetic spectrum.</p><p id="2012" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The imagery produced by <em class="ne">Sentinel-2 </em>and other Earth observation satellites is essential to developing the applications described above. However, using satellite images might be hampered by the presence of clouds. According to <a class="af og" href="https://arxiv.org/pdf/2112.15483.pdf#:~:text=In%20today%27s%20world%2C%20Satellite%20images,visibility%20of%20these%20image%20scenes." rel="noopener ugc nofollow" target="_blank">Rutvik Chauhan et al., </a>roughly half of the Earth's surface is covered in opaque clouds, with an additional 20% being blocked by cirrus or thin clouds. The situation worsens as clouds can cover a region of interest for several months. Therefore, cloud removal is indispensable for preprocessing satellite data.</p><p id="a855" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this blog, we use and compare different algorithms for segmenting clouds in <em class="ne">Sentinel-2 </em>satellite images. We explore various methods, from the classical Random Forest to the state-of-the-art computer vision algorithm YOLO. You can find all the code for this project in<a class="af og" href="https://github.com/JoseCelis/cloud-I" rel="noopener ugc nofollow" target="_blank"> <strong class="mk fr">this GitHub repository</strong></a>.</p><p id="3210" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Without further ado, let's get started!</p><p id="8fc7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Disclaimer: </strong><a class="af og" href="https://registry.opendata.aws/sentinel-2/#:~:text=License,European%20and%20International%20user%20community." rel="noopener ugc nofollow" target="_blank"><em class="ne">Sentinel</em> data is free and open to the broad Regional, National, European, and International user community.</a><strong class="mk fr"> </strong>You can access the data through the Copernicus Open Access Hub, Google Earth Engine, or the Python package sentinelhub. In this blog, we use the last option.</p><h1 id="27b7" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">SentinelHub in a nutshell</h1><p id="15db" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">S<a class="af og" href="https://github.com/sentinel-hub/sentinelhub-py" rel="noopener ugc nofollow" target="_blank"><em class="ne">entinelhub</em></a><em class="ne"> </em>is a Python package that supports many utilities for downloading, analyzing, and processing satellite imagery, including <em class="ne">Sentinel-2</em> data. This package offers excellent documentation and examples that facilitate its usage, making it quite prominent when developing end-to-end geo-data science solutions in Python.</p><p id="7fe6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To use <em class="ne">Sentinelhub, </em>you must<em class="ne"> </em>create an account in the <a class="af og" href="https://services.sentinel-hub.com/auth/realms/main/protocol/openid-connect/auth?client_id=30cf1d69-af7e-4f3a-997d-0643d660a478&amp;redirect_uri=https%3A%2F%2Fapps.sentinel-hub.com%2Fdashboard%2F&amp;state=cd274940-99ee-4c57-8418-f82540051357&amp;response_mode=fragment&amp;response_type=code&amp;scope=openid&amp;nonce=fa1f4d93-8730-49d7-beab-dbe2fc822833&amp;code_challenge=tP9ehp6dDZnaVjnvnJi2DhSAAn0sAkZqvMmAFo1atJ0&amp;code_challenge_method=S256" rel="noopener ugc nofollow" target="_blank">Sentinel Hub dashboard</a>. Once you log in, go to your dashboard's "User Settings" tab and create an OAuth client. This client allows you to connect to Sentinehub via API. The steps to get an OAuth client are clearly explained in <a class="af og" href="https://docs.sentinel-hub.com/api/latest/api/overview/authentication/#registering-oauth-client" rel="noopener ugc nofollow" target="_blank"><em class="ne">Sentinelhub's</em> official documentation</a>.</p><p id="5a83" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Once you have your credentials, <strong class="mk fr">save them in a secure place.</strong> They will not be shown again; you must create new ones if you lose them.</p><p id="faf9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You are now ready to download<em class="ne"> Sentinel-2 </em>images and cloud probabilities!</p><h1 id="9b83" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Getting the data</h1><p id="f146" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">In our <a class="af og" href="https://github.com/JoseCelis/cloud-I" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>, you can find the script <code class="cx pi pj pk pl b">src/import_image.py</code>that downloads both <em class="ne">Sentinel-2 </em>images and cloud probabilities using your OAuth credentials<em class="ne">. </em>We include the file <code class="cx pi pj pk pl b">settings/coordinates.yaml</code> that contains a collection of bounding boxes with their respective date and coordinate reference system (CRS). Feel free to use this file to download the data; however, we encourage you to use your own coordinates set.</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Example of coordinates to download data using <em class="pp">Sentinelhub</em>.</figcaption></figure><p id="9731" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We download all 13 bands of the images in Digital Numbers (DN). For our purposes, we only use optical (RGB) bands.</p><h1 id="36a9" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Is it necessary to preprocess the data?</h1><p id="199f" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">The raw images’ DN distribution in the RGB bands is usually skewed, having outliers or noise. Therefore, you must preprocess these data before training any machine learning model.</p></div></div><div class="nn"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="ni nj nk nl nm nn pw px paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pv"><img src="../Images/dc04035237d59eb6039f8635e9d0621f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*4TG7ZbP-niZ2SCoxtm4Z4A.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Example of a raw image, its DN distribution, and cloud probabilities. Image made by the authors.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d851" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The steps we follow to preprocess the raw images are the following:</p><ul class=""><li id="e533" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd py pz qa bk">Usage of a <code class="cx pi pj pk pl b">log1p</code> transformation: This helps reduce the skewness of the DN distributions.</li><li id="56a1" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk">Usage of a <code class="cx pi pj pk pl b">min-max</code>scaling transformation: We do this to normalize the RGB bands.</li><li id="07e2" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk">Convert DN to pixel values: We multiply the normalized RGB bands by 255 and convert the result to UINT8.</li></ul><p id="e53d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The implementation of these steps can be made in a single function in Python:</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Preprocessing of raw Sentinel images. You can see the code in the script src/preprocess.py in our <a class="af og" href="https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>.</figcaption></figure><p id="6502" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The images are cleaned. Now, it’s time to convert the cloud probabilities to masks.</p><p id="1655" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">One of the great advantages of using <em class="ne">Sentinelhub</em> is that the cloud probabilities come with pixel values on a grayscale. Therefore, every pixel value divided by 255 represents the probability of having a cloud in that pixel. By doing this, we go from values in the range [0, 255] to [0, 1]. Now, to create a mask, we need classes and not probabilities. Thus, we set a threshold of 0.4 to decide whether a pixel has a cloud.</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Convert cloud probabilities into a mask. The code is in the script src/preprocess.py in our <a class="af og" href="https://github.com/JoseCelis/cloud-I/blob/main/src/preprocess.py" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>.</figcaption></figure><p id="2eb7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The preprocessing described above enhances the brightness and contrast of the datasets; it is also necessary to get meaningful results when training different models.</p></div></div><div class="nn"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="ni nj nk nl nm nn pw px paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng pv"><img src="../Images/b333e345ae04a5ae3378de2802dab50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Tv_ku6NzXnWXt7uAoMqGQg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Example image after being preprocessed, its pixel value distribution, and the resulting cloud mask. Image made by the authors.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2d1c" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">Some warnings to consider</h2><p id="2f91" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">In some cases, the resulting mask doesn't fit the clouds of the corresponding image, as shown in the following picture:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qx"><img src="../Images/0455330f5602184cdee5c2ec35453735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOHp1qfCsXu2R6QvsTYAHQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Example of a faulty mask. Note how regions without clouds are marked as such.</figcaption></figure><p id="f9e6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This can be due to multiple reasons: one is the cloud detection model used in <em class="ne">Sentinelhub, </em>which returns false positives.<em class="ne"> </em>Another reason could be the fixed threshold value used during our preprocessing. To resolve this issue<em class="ne">,</em> we propose either creating new masks or discarding the image-mask pairs. We chose the second option. <a class="af og" href="https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">In this link</strong></a><strong class="mk fr">, we share a selection of preprocessed images and masks. Feel free to use them in case you want to experiment with the algorithms explained in this blog.</strong></p><h1 id="0b3b" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Before modeling, let’s establish a proper metric to evaluate the models’ prediction performance.</h1><p id="df69" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">Several metrics are used to evaluate an instance segmentation model. One of them is the Intersection over Union (IoU). This metric measures the amount of overlap between two segmentation masks. The IoU can have values from 0 to 1. An IoU=0 means no overlap between the predicted and the real segmentation mask. An IoU=1 indicates a perfect prediction.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qy"><img src="../Images/51c00917fb3595a00ad8d8be82b6b3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7u0pUf-AFTOneqgw1zWeHA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Definition of IoU. Image made by the authors.</figcaption></figure><p id="826f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">We measure the IoU on one test image to evaluate our models.</strong> Our implementation of the IoU is as follows:</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">IoU implementation using TensorFlow.</figcaption></figure><h1 id="1f1e" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Finally, Segmenting clouds in the images</h1><p id="1747" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">We are now ready to segment the clouds in the preprocessed satellite images. We use several algorithms, including classical methods like Random Forests and ANNs. We also use common object segmentation architectures such as U-NET and SegNet. Finally, we experiment with one of the state-of-the-art computer vision algorithms: YOLO.</p><h2 id="9aa3" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">Random Forest</h2><p id="4a6f" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">We want to explore how well classical methods segment clouds in Satellite images. For this experiment, we use a Random Forest. As known, a Random Forest is a set of decision trees, each trained on a different random subset of the data.</p><p id="f62e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We must convert the images to tabular data to train the Random Forest algorithm. In the following code snippet, we show how to do so:</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Conversion from image to tabular data and training of a Random Forest model.</figcaption></figure><p id="7558" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Note:</strong> You can train the models using the preprocessed images and masks by running the script <code class="cx pi pj pk pl b">src/model.py</code> in your terminal:</p><pre class="ni nj nk nl nm qz pl ra bp rb bb bk"><span id="7a1c" class="rc oi fq pl b bg rd re l rf rg">&gt; python src/model.py --model_name={model_name}</span></pre><p id="2f23" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Where:</p><ul class=""><li id="a021" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd py pz qa bk"><code class="cx pi pj pk pl b">--model_name=rf </code>trains a Random Forest.</li><li id="43e8" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk"><code class="cx pi pj pk pl b">--model_name=ann</code> trains an ANN.</li><li id="0b5f" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk"><code class="cx pi pj pk pl b">--model_name=unet</code> trains a U-NET model.</li><li id="0d29" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk"><code class="cx pi pj pk pl b">--model_name=segnet</code> trains a SegNet model.</li><li id="519a" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk"><code class="cx pi pj pk pl b">--model_name=yolo</code> trains YOLO.</li></ul><p id="006a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The prediction over a test image using Random Forest gives the following result:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rh"><img src="../Images/f0c0a0ec9515af44b9ec5c02bbaa5faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUaHX4uCl3GcgN70kAI9zg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Cloud predictions using Random Forest. Image created by the authors.</figcaption></figure><p id="0e0b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Surprisingly, Random Forest does a good job of segmenting the clouds in this image. However, its prediction is by pixel, meaning this model does not recognize the clouds’ edges during training.</p><h2 id="9be6" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">ANN</h2><p id="dc49" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">Artificial Neural Networks are powerful tools that mimic the brain's structure to learn from data and make predictions. We use a simple architecture with one hidden dense layer. Our aim was not to optimize the ANN's architecture but to explore the capabilities of dense layers to segment clouds in Satellite images.</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">ANN implementation in Keras.</figcaption></figure><p id="9337" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As we did for Random Forest, we converted the images to tabular data to train the ANN.</p><p id="093b" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The model predictions on the test image are as follows:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng ri"><img src="../Images/f586d4c840bcd6b1aaf945097ae814bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0YwEWbxEn5JL9IQRVs_XA.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Cloud predictions using an ANN. Image created by the authors.</figcaption></figure><p id="c257" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Although this model's IoU is worse than that of the Random Forest, the ANN does not classify coast pixels as clouds. This fact might be due to the simplicity of its architecture.</p><h2 id="5918" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">U-NET</h2><p id="045b" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">It's a convolutional Neural Network developed in 2015 by Olaf Ronneberger et al. (See the original paper <a class="af og" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">here</a>). This architecture is an encoder-decoder-based model. The encoder captures an image's essential features and patterns, like edges, colors, and textures. The decoder helps to create a detailed map of the different objects or areas in the image. In the U-NET architecture, each convolutional encoder layer is connected to its counterpart in the decoder layers. This is called <strong class="mk fr">skip connection</strong>.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rj"><img src="../Images/8cf56c2734133ce93abc442a2f79af06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOODVC7bPlheH7veaRD8-Q.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">The architecture of UNET. Image taken from <a class="af og" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">Olaf Ronneberger et al. 2015</a>.</figcaption></figure><p id="907a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">U-Net is often preferred for tasks requiring high accuracy and detail, such as medical imaging.</p><p id="ba44" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Our implementation of the U-NET architecture is in the following code snippet:</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">U-NET implementation in Keras.</figcaption></figure><p id="ae5d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The complete implementation of the U-NET model can be found in the script <code class="cx pi pj pk pl b">src/model_class.py</code> in our <a class="af og" href="https://github.com/JoseCelis/cloud-I/blob/main/src/model_class.py" rel="noopener ugc nofollow" target="_blank">GitHub repository</a>. For training, we use a batch size of 10 and 100 epochs. The results of the U-NET model on the test image are the following:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rk"><img src="../Images/130c15d5508b834e578ff72c82392beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2auTn0aMWeOLvUji2T9n0A.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Cloud predictions using U-NET. Image created by the authors.</figcaption></figure><p id="587e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This is the best IoU measurement obtained.</p><h2 id="94f9" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">SegNet</h2><p id="95c3" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">It's another encoder-decoder-based model developed in 2017 by <a class="af og" href="https://arxiv.org/abs/1511.00561v3" rel="noopener ugc nofollow" target="_blank">Vijay Badrinarayanan et al.</a> SegNet is more memory-efficient due to its use of max-pooling indices for upsampling. This architecture is suitable for applications where memory efficiency and speed are crucial, like real-time video processing.</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng rl"><img src="../Images/221f14ffd45bdc9139c0722de0351ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*6OvUE0-Cs4-2zJQ5axtWiQ.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">SegNet architecture. Image taken from <a class="af og" href="https://www.researchgate.net/publication/350109636_Hybrid_Deep_Learning_Models_with_Sparse_Enhancement_Technique_for_Detection_of_Newly_Grown_Tree_Leaves" rel="noopener ugc nofollow" target="_blank">Shih-Yu Chen et al. (2021).</a></figcaption></figure><p id="efe6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This architecture differs from U-NET in that U-NET uses skip connections to retain fine details, while SegNet does not.</p><p id="a6e1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Like the other models, SegNet can be trained by running the script <code class="cx pi pj pk pl b">src/model.py.</code> Once more, we use a batch size of 10 and 100 epochs for training. The resulting cloud segmentation on the test image is shown below:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rm"><img src="../Images/6b8b643ab0e4e8ac9f063ab832e136e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBwAempNxw3gmab1EF6g1w.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Cloud predictions using SegNet. Image created by the authors.</figcaption></figure><p id="f209" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Not as good as U-NET!</p><h2 id="9af9" class="qg oi fq bf oj qh qi qj om qk ql qm op mr qn qo qp mv qq qr qs mz qt qu qv qw bk">YOLO</h2><p id="67f9" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">You Only Look Once (YOLO) is a fast and efficient object detection algorithm developed in 2015 by <a class="af og" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">Joseph Redmon et al.</a> The beauty of this algorithm is that it treats object detection as a regression problem instead of a classification task by spatially separating bounding boxes and associating probabilities to each of the detected images using a single convolutional neural network (CNN).</p><p id="9754" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">YOLO's advantage is that it supports multiple computer vision tasks, including image segmentation. We use a YOLO segmentation model through <a class="af og" href="https://docs.ultralytics.com/" rel="noopener ugc nofollow" target="_blank">the Ultralytics Framework</a>. The training is quite simple, as shown in the snippet below:</p><figure class="ni nj nk nl nm nn"><div class="pm io l ed"><div class="pn po l"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Training of YOLO using the Ultralytics Framework.</figcaption></figure><p id="eec3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You just need to set up a <em class="ne">dataset</em>.<em class="ne">yaml</em> file which contains the paths of the images and labels. More information on how to run a YOLO model for segmentation is found <a class="af og" href="https://docs.ultralytics.com/tasks/segment/#models" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="7ee3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Note:</strong> Cloud contours are needed instead of masks to train the YOLO model for segmentation. You can find the labels in <a class="af og" href="https://drive.google.com/drive/folders/1rBMHZC-CZCvAfkz1Qg0cOjWcnvHucQN8?usp=sharing" rel="noopener ugc nofollow" target="_blank">this data link</a>.</p><p id="abe4" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The results of the cloud segmentation on the test image are the following:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng rn"><img src="../Images/3d0439ef2a35972c02d1644dc67939af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRme07dlfuMuEGl7tMIQWQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">Cloud predictions using YOLO. Image created by the authors.</figcaption></figure><p id="c575" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Ugh, this is an ugly result!</p><p id="4334" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">While YOLO is a powerful tool for many segmentation tasks, it may perform poorly on images with significant blurring because blurring reduces the contrast between the object and the background. Additionally, YOLO can have difficulty segmenting each object in pictures with many overlapping objects. Since clouds can be blurred objects without well-defined edges and often overlap with others, YOLO is not an appropriate model for segmenting clouds in Satellite images.</p><p id="2c0d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">We shared the trained models explained above </strong><a class="af og" href="https://drive.google.com/drive/folders/1OySSazRiUWkjhokVk6_WycdvRFMy2hFa?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="mk fr">in this link.</strong></a><strong class="mk fr"> We did not include Random Forest due to the file size (it's 6 GB!).</strong></p><h1 id="74e1" class="oh oi fq bf oj ok ol gq om on oo gt op oq or os ot ou ov ow ox oy oz pa pb pc bk">Take away messages</h1><p id="d590" class="pw-post-body-paragraph mi mj fq mk b go pd mm mn gr pe mp mq mr pf mt mu mv pg mx my mz ph nb nc nd fj bk">We explore how to segment clouds in <em class="ne">Sentinel-2 </em>satellite images using different ML methods. Here are some learnings from this experiment:</p><ul class=""><li id="5396" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd py pz qa bk">The data obtained using the Python package <em class="ne">sentinelhub </em>is not ready for model training. You must preprocess and perhaps adapt these data to a proper format depending on the selected model (for instance, convert the images to tabular data when training Random Forest or ANNs).</li><li id="197a" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk">The best model is U-NET, followed by Random Forest and SegNet. It's not surprising that U-NET and SegNet are on this list. Both architectures were developed for segmentation tasks. However, Random Forest performs surprisingly well. This shows how ML methods can also work in image segmentation.</li><li id="920e" class="mi mj fq mk b go qb mm mn gr qc mp mq mr qd mt mu mv qe mx my mz qf nb nc nd py pz qa bk">The worst models were ANN and YOLO. Due to its simplicity of architecture, we expected ANN not to give good results. Regarding YOLO, segmenting clouds in images is not a suitable task for this algorithm despite being the state-of-the-art method in computer vision. This experiment overall shows that we, as data scientists, must always look for the algorithm that best fits our data.</li></ul><p id="c005" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We hope you enjoyed this post. Once more, thanks for reading!</p><p id="af41" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You can contact us via LinkedIn at:</p><p id="adcf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af og" href="https://www.linkedin.com/in/jose-celis-gil/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/jose-celis-gil/</a></p><p id="ad88" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><a class="af og" href="https://www.linkedin.com/in/camartinezbarbosa/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/camartinezbarbosa/</a></p></div></div></div></div>    
</body>
</html>