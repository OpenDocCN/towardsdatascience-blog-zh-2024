- en: How to Train an Instance Segmentation Model with No Training Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29](https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: All you need is a bit of computing power
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    ·8 min read·Jan 29, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3c91d0c17e4991f49a48ae9d09eb9f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Izzy Park](https://unsplash.com/@blue_jean?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Did you know that for most common types of things, you don’t necessarily need
    data anymore to train object detection or even instance segmentation models?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get real on a given example. Let’s assume you have been given the task
    to build an instance segmentation model for the following classes:'
  prefs: []
  type: TYPE_NORMAL
- en: Lion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zebra
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tiger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arguably, data would be easy to find for such classes: plenty of images of
    those animals are available on the internet. But if we need to build a commercially
    viable product for instance segmentation, we still need two things:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure we have collected images with commercial use license
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these tasks can be very time consuming and/or cost some significant
    amount of money.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore another path: the use of free, available models. To do so, we’ll
    use a 2-step process to generate both the data and the associated labels:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll generate images with [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion),
    a very powerful, Generative AI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we’ll generate and curate the labels with Meta’s [Segment Anything Model](https://segment-anything.com/)
    (SAM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Note that, at the date of publication of this article, images generated with
    Stable Diffusion are kind of in a grey area, and can be used for commercial use.
    But the regulation may change in the future.*'
  prefs: []
  type: TYPE_NORMAL
- en: All the codes used in this post are available [in this repository](https://github.com/vincent-vdb/medium_posts).
  prefs: []
  type: TYPE_NORMAL
- en: Generating data using Stable Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I generated the data with Stable Diffusion. Before actually generating the data,
    let’s quickly give a few information about stable diffusion and how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: How to use Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For that, I used the following repository: [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is very complete and frequently updated, allowing to use a lot of tools
    and plugins. It is very easy to install, on any distribution, by following the
    instructions in the readme. You can also find some very useful tutorials on how
    to use effectively Stable Diffusion:'
  prefs: []
  type: TYPE_NORMAL
- en: For beginners, I would suggest [this tutorial](https://stable-diffusion-art.com/beginners-guide/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more advanced usage, I would suggest [this tutorial](https://stable-diffusion-art.com/prompt-guide/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without going into the details of how the stable diffusion model is trained
    and works (there are plenty of good resources for that), it’s good to know that
    actually there is more than one model.
  prefs: []
  type: TYPE_NORMAL
- en: There are several “official” versions of the model released by Stability AI,
    such as Stable Diffusion 1.5, 2.1 or XL. These official models can be easily downloaded
    on the [HuggingFace of Stability AI](https://huggingface.co/stabilityai).
  prefs: []
  type: TYPE_NORMAL
- en: But since Stable Diffusion is open source, anyone can train their own model.
    There is a huge number of available models on the website [Civitai](https://civitai.com/models/),
    sometimes trained for specific purposes, such as fantasy images, punk images or
    realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our need, I will use two models including one specifically trained for realistic
    image generation, since I want to generate realistic images of animals.
  prefs: []
  type: TYPE_NORMAL
- en: 'The used models and hyperparameters are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models: [JuggernautXL](https://civitai.com/models/133005/juggernaut-xl) and
    [Realistic Vision V6.0 B1](https://civitai.com/models/4201/realistic-vision-v60-b1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sampling: Euler a, 20 iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CFG Scale: 2 (the lower the value, the more randomness in the produced output)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Negative prompt: “*bad quality, bad anatomy, worst quality, low quality, low
    resolution, blur, blurry, ugly, wrong proportions, watermark, image artifacts,
    lowres, ugly, jpeg artifacts, deformed, noisy image, deformation, digital art,
    unrealistic, drawing, painting*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prompt: “*a realistic picture of a lion sitting on the grass*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To automate image generation with different settings, I used a specific feature
    script called X/Y/Z plot with prompt S/R for each axis.
  prefs: []
  type: TYPE_NORMAL
- en: The “prompt S/R” means search and replace, allowing to search for a string in
    the original prompt and replace it with other strings. Using X/Y/Z plot and prompt
    S/R on each axis, it allows to generate images for any combination of the possible
    given values (just like a hyperparameter grid search).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the parameters I used on each axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '*lion, zebra, tiger, horse*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*sitting, sleeping, standing, running, walking*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*on the grass, in the wild, in the city, in the jungle, from the back, from
    side view*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this, I can easily generate in one go images of the following prompt “a
    realistic picture of a <animal> <action> <location>” with all the values proposed
    in the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'All in all, it would generate images for 4 animals, 5 actions and 6 locations:
    so 120 possibilities. Adding to that, I used a batch count of 2 and 2 different
    models, increasing the generated images to 480 to create my dataset (120 for each
    animal class). Below are some examples of the generated images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54cbcd00b8d086177b3e4aaf3705e836.png)'
  prefs: []
  type: TYPE_IMG
- en: Samples of the generated images using Stable Diffusion. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, most of the pictures are realistic enough. We will now get the
    instance masks, so that we can then train a segmentation model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get the labels, we will use SAM model to generate masks, and we will then
    manually filter out masks that are not good enough, as well as unrealistic images
    (often called hallucinations).
  prefs: []
  type: TYPE_NORMAL
- en: Generating the raw masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To generate the raw masks, let’s use SAM model. The SAM model requires input
    prompts (not a textual prompt): either a bounding box or a few point locations.
    This allows the model to generate the mask from this input prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will do the most simple input prompt: the center point. Indeed,
    in most images generated by Stable Diffusion, the main object is centered, allowing
    us to efficiently use SAM with always the same input prompt and absolutely no
    labeling. To do so, we use the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: Function to generate the masks using SAM. Full code available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function will first instantiate a SAM predictor, given a model type and
    a checkpoint (to download [here](https://github.com/facebookresearch/segment-anything#model-checkpoints)).
    It will then loop over the images in the input folder and do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the mask thanks to SAM, with both the options *multimask_output* set
    to *True* and *False*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply closing to the mask before writing it as an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: We use both options *multimask_output* set to *True* and *False* because no
    option gives consistently superior results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We apply closing to the masks, because raw masks sometimes have a few holes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few examples of images with their masks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4495d22c8066eb188e3a74e1a7fc8508.png)'
  prefs: []
  type: TYPE_IMG
- en: A few images with the generated SAM masks displayed as a yellowish overlay.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, once selected, the masks are quite accurate and it took virtually
    no time to label.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not all the masks were correctly computed in the previous subsection. Indeed,
    sometimes the object was not centered, thus the mask prediction was off. Sometimes,
    for some reason, the mask is just wrong and would need more input prompts to make
    it work.
  prefs: []
  type: TYPE_NORMAL
- en: 'One quick workaround is to simply either select the best mask between the 2
    computed ones, or simply remove the image from the dataset if no mask was good
    enough. Let’s do that with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Function allowing to select the best mask, or just reject the image. Full code
    available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code loops over all the generated images with Stable Diffusion and does
    the following for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the two generated SAM masks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the image twice, one with each masks as an overlay, side by side
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Waits for a keyboard event to make the selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The expected keyboard events are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Left arrow of the keyboard to select the left mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Right arrow to select the left mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Down arrow to discard this image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running this script may take some time, since you have to go through all the
    images. Assuming 1 second per image, it would take about 10 minutes for 600 images.
    This is still much faster than actually labeling images with masks, that usually
    takes at least 30 second per mask for high quality masks. Moreover, this allows
    to effectively filter out any unrealistic image.
  prefs: []
  type: TYPE_NORMAL
- en: Running this script on the generated 480 images took me less than 5 minutes.
    I selected the masks and filtered unrealistic images, so that I ended up with
    412 masks. Next step is to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before training the YOLO segmentation model, we need to create the dataset properly.
    Let’s go through these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Function to create the dataset. Full code available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code loops through all the image and does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select the train or validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert the masks to polygons for YOLO expected input label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Copy the image and the label in the right folders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One tricky part in this code is in the mask to polygon conversion, done by the
    *mask2yolo* function. This makes use of [shapely](https://pypi.org/project/shapely/)
    and [rasterio](https://pypi.org/project/rasterio/) libraries to make this conversion
    efficiently. Of course, you can find the fully working in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, you would end up with the following structure in your *datasets*
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cd3f4d34f280df4c7fe5b85b946013b.png)'
  prefs: []
  type: TYPE_IMG
- en: Folder structure after creating the dataset. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the expected structure to train a model using the YOLOv8 library: it’s
    finally time to train the model!'
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can now train the model. Let’s use a YOLOv8 nano segmentation model. Training
    a model is just two lines of code with the [Ultralytics library](https://pypi.org/project/ultralytics/),
    as we can see in the following gist:'
  prefs: []
  type: TYPE_NORMAL
- en: Function to train a YOLO segmentation model. Full code available in the repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'After 15 epochs of training on the previously prepared dataset, the results
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1df48de7b65b8e0337edb9843d485dc.png)'
  prefs: []
  type: TYPE_IMG
- en: Results generated by YOLOv8 library after 15 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the metrics are quite high with a mAP50–95 close to 1, suggesting
    good performances. Of course, the dataset diversity being quite limited, those
    good performances are mostly likely caused by overfitting in some extent.
  prefs: []
  type: TYPE_NORMAL
- en: For a more realistic evaluation, next we’ll evaluate the model on a few real
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model on real data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From [Unsplash](https://unsplash.com/), I got a few images from each class
    and tested the model on this data. The results are right below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb175cb46b3fc0e02c2113040b890191.png)'
  prefs: []
  type: TYPE_IMG
- en: Segmentation and class prediction results on real images from Unsplash.
  prefs: []
  type: TYPE_NORMAL
- en: 'On these 8 real images, the model performed quite well: the animal class is
    successfully predicted, and the mask seems quite accurate. Of course, to evaluate
    properly this model, we would need a proper labeled dataset images and segmentation
    masks of each class.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With absolutely no images and no labels, we could train a segmentation model
    for 4 classes: horse, lion, tiger and zebra. To do so, we leveraged three amazing
    tools:'
  prefs: []
  type: TYPE_NORMAL
- en: Stable diffusion to generate realistic images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM to compute the accurate masks of the objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLOv8 to efficiently train an instance segmentation model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While we couldn’t properly evaluate the trained model because we lack a labeled
    test dataset, it seems promising on a few images. Do not take this post as self-sufficient
    way to train any instance segmentation, but more as a method to speed up and boost
    the performances in your next projects. From my own experience, the use of synthetic
    data and tools like SAM can greatly improve your productivity in building production-grade
    computer vision models.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all the code to do this on your own is fully available in [this repository](https://github.com/vincent-vdb/medium_posts/tree/main),
    and will hopefully help you in your next computer vision project!
  prefs: []
  type: TYPE_NORMAL
