- en: How to Train an Instance Segmentation Model with No Training Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在没有训练数据的情况下训练实例分割模型
- en: 原文：[https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29](https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29](https://towardsdatascience.com/how-to-train-an-instance-segmentation-model-with-no-training-data-190dc020bf73?source=collection_archive---------10-----------------------#2024-01-29)
- en: All you need is a bit of computing power
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你只需要一点计算能力
- en: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[![Vincent
    Vandenbussche](../Images/b2febfc63ca0efbda0af5501f6080ab7.png)](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    [Vincent Vandenbussche](https://medium.com/@vincent.vandenbussche?source=post_page---byline--190dc020bf73--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    ·8 min read·Jan 29, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--190dc020bf73--------------------------------)
    ·8分钟阅读·2024年1月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f3c91d0c17e4991f49a48ae9d09eb9f5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3c91d0c17e4991f49a48ae9d09eb9f5.png)'
- en: Photo by [Izzy Park](https://unsplash.com/@blue_jean?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Izzy Park](https://unsplash.com/@blue_jean?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Did you know that for most common types of things, you don’t necessarily need
    data anymore to train object detection or even instance segmentation models?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗，对于大多数常见的物品类型，你不再一定需要数据来训练物体检测甚至是实例分割模型？
- en: 'Let’s get real on a given example. Let’s assume you have been given the task
    to build an instance segmentation model for the following classes:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个实际的例子为例。假设你被要求为以下类别构建一个实例分割模型：
- en: Lion
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狮子
- en: Horse
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马
- en: Zebra
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斑马
- en: Tiger
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 老虎
- en: 'Arguably, data would be easy to find for such classes: plenty of images of
    those animals are available on the internet. But if we need to build a commercially
    viable product for instance segmentation, we still need two things:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，类似的类别数据很容易找到：互联网上有大量这些动物的图像。然而，如果我们需要为实例分割构建一个具有商业可行性的产品，我们仍然需要两样东西：
- en: Make sure we have collected images with commercial use license
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们收集到的图像具有商业使用许可
- en: Label the data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注数据
- en: Both of these tasks can be very time consuming and/or cost some significant
    amount of money.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这两项任务可能非常耗时和/或需要花费相当大的金额。
- en: 'Let’s explore another path: the use of free, available models. To do so, we’ll
    use a 2-step process to generate both the data and the associated labels:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索另一条路径：使用免费且可用的模型。为此，我们将采用一个两步过程来生成数据及其相关标签：
- en: First, we’ll generate images with [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion),
    a very powerful, Generative AI model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将使用[稳定扩散](https://en.wikipedia.org/wiki/Stable_Diffusion)，一个非常强大的生成型AI模型，来生成图像
- en: Then, we’ll generate and curate the labels with Meta’s [Segment Anything Model](https://segment-anything.com/)
    (SAM)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将使用Meta的[Segment Anything Model](https://segment-anything.com/)（SAM）生成并策划标签
- en: '*Note that, at the date of publication of this article, images generated with
    Stable Diffusion are kind of in a grey area, and can be used for commercial use.
    But the regulation may change in the future.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，在本文发布之时，使用稳定扩散生成的图像处于灰色地带，且可以用于商业用途。但相关规定未来可能会发生变化。*'
- en: All the codes used in this post are available [in this repository](https://github.com/vincent-vdb/medium_posts).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码都可以在[这个仓库](https://github.com/vincent-vdb/medium_posts)中找到。
- en: Generating data using Stable Diffusion
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Stable Diffusion 生成数据
- en: I generated the data with Stable Diffusion. Before actually generating the data,
    let’s quickly give a few information about stable diffusion and how to use it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用 Stable Diffusion 生成了数据。在实际生成数据之前，先简要介绍一下 Stable Diffusion 以及如何使用它。
- en: How to use Stable Diffusion
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 Stable Diffusion
- en: 'For that, I used the following repository: [https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我使用了以下仓库：[https://github.com/AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui)
- en: 'It is very complete and frequently updated, allowing to use a lot of tools
    and plugins. It is very easy to install, on any distribution, by following the
    instructions in the readme. You can also find some very useful tutorials on how
    to use effectively Stable Diffusion:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它功能非常完整且经常更新，允许使用大量工具和插件。按照 readme 中的说明，任何发行版都可以非常容易地安装。你还可以找到一些非常有用的教程，教你如何有效地使用
    Stable Diffusion：
- en: For beginners, I would suggest [this tutorial](https://stable-diffusion-art.com/beginners-guide/)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于初学者，我建议查看[这个教程](https://stable-diffusion-art.com/beginners-guide/)
- en: For more advanced usage, I would suggest [this tutorial](https://stable-diffusion-art.com/prompt-guide/)
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更高级的用法，我建议查看[这个教程](https://stable-diffusion-art.com/prompt-guide/)
- en: Without going into the details of how the stable diffusion model is trained
    and works (there are plenty of good resources for that), it’s good to know that
    actually there is more than one model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入讨论 Stable Diffusion 模型的训练和工作原理（这方面有很多优秀的资源），但值得知道的是，实际上有不止一个模型。
- en: There are several “official” versions of the model released by Stability AI,
    such as Stable Diffusion 1.5, 2.1 or XL. These official models can be easily downloaded
    on the [HuggingFace of Stability AI](https://huggingface.co/stabilityai).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个由 Stability AI 发布的“官方”版本模型，例如 Stable Diffusion 1.5、2.1 或 XL。这些官方模型可以轻松地在[Stability
    AI 的 HuggingFace](https://huggingface.co/stabilityai)上下载。
- en: But since Stable Diffusion is open source, anyone can train their own model.
    There is a huge number of available models on the website [Civitai](https://civitai.com/models/),
    sometimes trained for specific purposes, such as fantasy images, punk images or
    realistic images.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于 Stable Diffusion 是开源的，任何人都可以训练自己的模型。在网站[Civitai](https://civitai.com/models/)上有大量可用的模型，有时是为特定目的训练的，比如幻想图像、朋克风格图像或真实图像。
- en: Generating the data
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成数据
- en: For our need, I will use two models including one specifically trained for realistic
    image generation, since I want to generate realistic images of animals.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的需求，我将使用两个模型，其中一个是专门为真实图像生成训练的，因为我想生成真实的动物图像。
- en: 'The used models and hyperparameters are the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的模型和超参数如下：
- en: 'Models: [JuggernautXL](https://civitai.com/models/133005/juggernaut-xl) and
    [Realistic Vision V6.0 B1](https://civitai.com/models/4201/realistic-vision-v60-b1)'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：[JuggernautXL](https://civitai.com/models/133005/juggernaut-xl)和[Realistic
    Vision V6.0 B1](https://civitai.com/models/4201/realistic-vision-v60-b1)
- en: 'Sampling: Euler a, 20 iterations'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样：Euler a，20次迭代
- en: 'CFG Scale: 2 (the lower the value, the more randomness in the produced output)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CFG 规模：2（数值越低，产生的输出越随机）
- en: 'Negative prompt: “*bad quality, bad anatomy, worst quality, low quality, low
    resolution, blur, blurry, ugly, wrong proportions, watermark, image artifacts,
    lowres, ugly, jpeg artifacts, deformed, noisy image, deformation, digital art,
    unrealistic, drawing, painting*”'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负面提示：“*差质量、差解剖学、最差质量、低质量、低分辨率、模糊、模糊不清、丑陋、比例错误、水印、图像伪影、低分辨率、丑陋、JPEG伪影、变形、噪点图像、形变、数字艺术、不真实、绘画、油画*”
- en: 'Prompt: “*a realistic picture of a lion sitting on the grass*”'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示：“*一只坐在草地上的狮子的真实照片*”
- en: To automate image generation with different settings, I used a specific feature
    script called X/Y/Z plot with prompt S/R for each axis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自动化生成不同设置的图像，我使用了一个名为 X/Y/Z 图与提示 S/R 的特性脚本，适用于每个轴。
- en: The “prompt S/R” means search and replace, allowing to search for a string in
    the original prompt and replace it with other strings. Using X/Y/Z plot and prompt
    S/R on each axis, it allows to generate images for any combination of the possible
    given values (just like a hyperparameter grid search).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “提示 S/R”指的是搜索和替换，允许在原始提示中搜索一个字符串并将其替换为其他字符串。通过在每个轴上使用 X/Y/Z 图和提示 S/R，可以生成任何可能给定值组合的图像（就像超参数网格搜索一样）。
- en: 'Here are the parameters I used on each axis:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我在每个轴上使用的参数：
- en: '*lion, zebra, tiger, horse*'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*狮子，斑马，老虎，马*'
- en: '*sitting, sleeping, standing, running, walking*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*坐着，睡觉，站立，跑步，走路*'
- en: '*on the grass, in the wild, in the city, in the jungle, from the back, from
    side view*'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在草地上，在野外，在城市中，在丛林里，从背后，从侧面看*'
- en: Using this, I can easily generate in one go images of the following prompt “a
    realistic picture of a <animal> <action> <location>” with all the values proposed
    in the parameters.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方法，我可以一次性轻松生成如下提示的图像：“一张真实的<动物> <动作> <位置>”的图片，所有参数值都由此生成。
- en: 'All in all, it would generate images for 4 animals, 5 actions and 6 locations:
    so 120 possibilities. Adding to that, I used a batch count of 2 and 2 different
    models, increasing the generated images to 480 to create my dataset (120 for each
    animal class). Below are some examples of the generated images.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，它会为4种动物、5个动作和6个位置生成图像：所以一共有120种可能性。再加上我使用了批量计数为2和2个不同的模型，使得生成的图像数量增加到480，从而创建了我的数据集（每个动物类别120个）。下面是一些生成图像的示例。
- en: '![](../Images/54cbcd00b8d086177b3e4aaf3705e836.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54cbcd00b8d086177b3e4aaf3705e836.png)'
- en: Samples of the generated images using Stable Diffusion. Image by author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Stable Diffusion生成的图像示例。图像由作者提供。
- en: As we can see, most of the pictures are realistic enough. We will now get the
    instance masks, so that we can then train a segmentation model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，大多数图片都足够真实。接下来，我们将获取实例遮罩，以便训练一个分割模型。
- en: Getting the labels
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取标签
- en: To get the labels, we will use SAM model to generate masks, and we will then
    manually filter out masks that are not good enough, as well as unrealistic images
    (often called hallucinations).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取标签，我们将使用SAM模型生成遮罩，然后手动过滤掉不够好的遮罩，以及不现实的图像（通常称为幻觉）。
- en: Generating the raw masks
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成原始遮罩
- en: 'To generate the raw masks, let’s use SAM model. The SAM model requires input
    prompts (not a textual prompt): either a bounding box or a few point locations.
    This allows the model to generate the mask from this input prompt.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成原始遮罩，我们将使用SAM模型。SAM模型需要输入提示（不是文本提示）：可以是一个边界框或一些点位。这允许模型根据这些输入提示生成遮罩。
- en: 'In our case, we will do the most simple input prompt: the center point. Indeed,
    in most images generated by Stable Diffusion, the main object is centered, allowing
    us to efficiently use SAM with always the same input prompt and absolutely no
    labeling. To do so, we use the following function:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们将使用最简单的输入提示：中心点。事实上，在Stable Diffusion生成的大多数图像中，主要对象都是居中的，这使得我们可以高效地使用SAM，始终使用相同的输入提示，并且完全不需要标注。为此，我们使用以下功能：
- en: Function to generate the masks using SAM. Full code available in the repository.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SAM生成遮罩的功能。完整代码可以在代码库中找到。
- en: 'This function will first instantiate a SAM predictor, given a model type and
    a checkpoint (to download [here](https://github.com/facebookresearch/segment-anything#model-checkpoints)).
    It will then loop over the images in the input folder and do the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能将首先实例化一个SAM预测器，给定模型类型和检查点（可以在[这里](https://github.com/facebookresearch/segment-anything#model-checkpoints)下载）。然后，它将遍历输入文件夹中的图像，进行以下操作：
- en: Load the image
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载图像
- en: Compute the mask thanks to SAM, with both the options *multimask_output* set
    to *True* and *False*
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用SAM计算遮罩，使用*multimask_output*设置为*True*和*False*的两种选项。
- en: Apply closing to the mask before writing it as an image
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将遮罩写成图像之前，先对其应用闭运算。
- en: 'A few things to note:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意几点：
- en: We use both options *multimask_output* set to *True* and *False* because no
    option gives consistently superior results
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们同时使用*multimask_output*设置为*True*和*False*的两种选项，因为没有哪种选项能 consistently 提供更优的结果。
- en: We apply closing to the masks, because raw masks sometimes have a few holes
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对遮罩应用闭运算，因为原始遮罩有时会有一些小孔。
- en: 'Here are a few examples of images with their masks:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些带有遮罩的图像示例：
- en: '![](../Images/4495d22c8066eb188e3a74e1a7fc8508.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4495d22c8066eb188e3a74e1a7fc8508.png)'
- en: A few images with the generated SAM masks displayed as a yellowish overlay.
    Image by author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一些图像，显示了生成的SAM遮罩，作为黄色叠加层。图像由作者提供。
- en: As we can see, once selected, the masks are quite accurate and it took virtually
    no time to label.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，一旦选择了，遮罩非常准确，几乎不需要时间来标注。
- en: Selecting the masks
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择遮罩
- en: Not all the masks were correctly computed in the previous subsection. Indeed,
    sometimes the object was not centered, thus the mask prediction was off. Sometimes,
    for some reason, the mask is just wrong and would need more input prompts to make
    it work.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有的遮罩在上一小节中都正确计算出来。事实上，有时对象并没有居中，因此遮罩预测不准确。有时，出于某种原因，遮罩就是错误的，需要更多的输入提示来使其生效。
- en: 'One quick workaround is to simply either select the best mask between the 2
    computed ones, or simply remove the image from the dataset if no mask was good
    enough. Let’s do that with the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的解决方法是直接选择两个计算出的掩码中最好的一个，或者如果没有合适的掩码，直接将图片从数据集中删除。我们可以通过以下代码来实现：
- en: Function allowing to select the best mask, or just reject the image. Full code
    available in the repository.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用于选择最佳掩码，或直接拒绝图片的函数。完整代码在代码库中可用。
- en: 'This code loops over all the generated images with Stable Diffusion and does
    the following for each image:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码会遍历所有使用Stable Diffusion生成的图片，并对每张图片执行以下操作：
- en: Load the two generated SAM masks
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载两个生成的SAM掩码
- en: Display the image twice, one with each masks as an overlay, side by side
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图片显示两次，每张图片分别叠加一个掩码，并并排显示
- en: Waits for a keyboard event to make the selection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等待键盘事件以进行选择
- en: 'The expected keyboard events are the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的键盘事件如下：
- en: Left arrow of the keyboard to select the left mask
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用键盘的左箭头选择左侧掩码
- en: Right arrow to select the left mask
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用右箭头选择左侧掩码
- en: Down arrow to discard this image
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向下箭头以丢弃这张图片
- en: Running this script may take some time, since you have to go through all the
    images. Assuming 1 second per image, it would take about 10 minutes for 600 images.
    This is still much faster than actually labeling images with masks, that usually
    takes at least 30 second per mask for high quality masks. Moreover, this allows
    to effectively filter out any unrealistic image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个脚本可能需要一些时间，因为你必须处理所有图片。假设每张图片处理时间为1秒，对于600张图片，大约需要10分钟。这仍然比实际为每张图片标注掩码要快得多，后者通常需要至少30秒才能为每个高质量掩码标注。此外，这还能够有效地筛选出任何不真实的图片。
- en: Running this script on the generated 480 images took me less than 5 minutes.
    I selected the masks and filtered unrealistic images, so that I ended up with
    412 masks. Next step is to train the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个脚本处理生成的480张图片，我只用了不到5分钟。我选择了合适的掩码并过滤了不真实的图片，最终得到了412张掩码。下一步是训练模型。
- en: Training the model
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Before training the YOLO segmentation model, we need to create the dataset properly.
    Let’s go through these steps.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练YOLO分割模型之前，我们需要正确创建数据集。让我们逐步完成这些步骤。
- en: Creating the dataset
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: Function to create the dataset. Full code available in the repository.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建数据集的函数。完整代码在代码库中可用。
- en: 'This code loops through all the image and does the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码遍历所有图片，并执行以下操作：
- en: Randomly select the train or validation set
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择训练集或验证集
- en: Convert the masks to polygons for YOLO expected input label
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将掩码转换为多边形，以适应YOLO期望的输入标签
- en: Copy the image and the label in the right folders
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图片和标签复制到正确的文件夹中
- en: One tricky part in this code is in the mask to polygon conversion, done by the
    *mask2yolo* function. This makes use of [shapely](https://pypi.org/project/shapely/)
    and [rasterio](https://pypi.org/project/rasterio/) libraries to make this conversion
    efficiently. Of course, you can find the fully working in the repository.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中的一个难点是在掩码到多边形的转换部分，这由*mask2yolo*函数完成。它利用了[shapely](https://pypi.org/project/shapely/)和[rasterio](https://pypi.org/project/rasterio/)库来高效地进行转换。当然，你可以在代码库中找到完整的实现。
- en: 'In the end, you would end up with the following structure in your *datasets*
    folder:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将在*datasets*文件夹中看到以下结构：
- en: '![](../Images/5cd3f4d34f280df4c7fe5b85b946013b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cd3f4d34f280df4c7fe5b85b946013b.png)'
- en: Folder structure after creating the dataset. Image by author.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集后的文件夹结构。图片来源：作者。
- en: 'This is the expected structure to train a model using the YOLOv8 library: it’s
    finally time to train the model!'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用YOLOv8库训练模型的预期结构：终于可以开始训练模型了！
- en: Training the model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We can now train the model. Let’s use a YOLOv8 nano segmentation model. Training
    a model is just two lines of code with the [Ultralytics library](https://pypi.org/project/ultralytics/),
    as we can see in the following gist:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练模型了。让我们使用一个YOLOv8 nano分割模型。训练模型只需要两行代码，使用[Ultralytics库](https://pypi.org/project/ultralytics/)，正如我们在下面的代码片段中看到的：
- en: Function to train a YOLO segmentation model. Full code available in the repository.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练YOLO分割模型的函数。完整代码在代码库中可用。
- en: 'After 15 epochs of training on the previously prepared dataset, the results
    are the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前准备好的数据集上训练了15个周期后，结果如下：
- en: '![](../Images/c1df48de7b65b8e0337edb9843d485dc.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1df48de7b65b8e0337edb9843d485dc.png)'
- en: Results generated by YOLOv8 library after 15 epochs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 经过15个周期后，由YOLOv8库生成的结果。
- en: As we can see, the metrics are quite high with a mAP50–95 close to 1, suggesting
    good performances. Of course, the dataset diversity being quite limited, those
    good performances are mostly likely caused by overfitting in some extent.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，指标相当高，mAP50–95接近1，表明性能良好。当然，由于数据集多样性相对有限，这些良好的表现很可能是由于某种程度上的过拟合所导致。
- en: For a more realistic evaluation, next we’ll evaluate the model on a few real
    images.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更为真实的评估，接下来我们将在几张真实图像上评估模型。
- en: Evaluating the model on real data
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在真实数据上评估模型
- en: 'From [Unsplash](https://unsplash.com/), I got a few images from each class
    and tested the model on this data. The results are right below:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我从[Unsplash](https://unsplash.com/)上获取了每个类别的一些图像，并在这些数据上测试了模型。结果如下：
- en: '![](../Images/fb175cb46b3fc0e02c2113040b890191.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb175cb46b3fc0e02c2113040b890191.png)'
- en: Segmentation and class prediction results on real images from Unsplash.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Unsplash的真实图像上的分割和类别预测结果。
- en: 'On these 8 real images, the model performed quite well: the animal class is
    successfully predicted, and the mask seems quite accurate. Of course, to evaluate
    properly this model, we would need a proper labeled dataset images and segmentation
    masks of each class.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这8张真实图像上，模型表现得相当不错：动物类别成功预测，且遮罩似乎相当准确。当然，为了正确评估该模型，我们需要一个合适的标注数据集图像和每个类别的分割遮罩。
- en: Conclusion
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'With absolutely no images and no labels, we could train a segmentation model
    for 4 classes: horse, lion, tiger and zebra. To do so, we leveraged three amazing
    tools:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全没有图像和标签的情况下，我们可以为四个类别训练一个分割模型：马、狮子、老虎和斑马。为此，我们利用了三个令人惊叹的工具：
- en: Stable diffusion to generate realistic images
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用稳定扩散生成逼真的图像
- en: SAM to compute the accurate masks of the objects
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SAM计算物体的准确遮罩
- en: YOLOv8 to efficiently train an instance segmentation model
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv8高效训练实例分割模型
- en: While we couldn’t properly evaluate the trained model because we lack a labeled
    test dataset, it seems promising on a few images. Do not take this post as self-sufficient
    way to train any instance segmentation, but more as a method to speed up and boost
    the performances in your next projects. From my own experience, the use of synthetic
    data and tools like SAM can greatly improve your productivity in building production-grade
    computer vision models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们因为缺乏标注测试数据集而无法正确评估训练好的模型，但在一些图像上它看起来很有前景。请不要将这篇文章当作训练任何实例分割的独立方法，而应视为一种加速和提升你下一个项目性能的方法。根据我的经验，使用合成数据和像SAM这样的工具可以大大提高你在构建生产级计算机视觉模型时的生产力。
- en: Of course, all the code to do this on your own is fully available in [this repository](https://github.com/vincent-vdb/medium_posts/tree/main),
    and will hopefully help you in your next computer vision project!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，所有的代码都可以在[这个仓库](https://github.com/vincent-vdb/medium_posts/tree/main)中找到，并希望能帮助你在下一个计算机视觉项目中！
