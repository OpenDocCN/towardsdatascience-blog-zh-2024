- en: Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Llama 3.2-Vision多模态LLM与您的图片进行对话
- en: 原文：[https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05](https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05](https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05)
- en: Learn how to build Llama 3.2-Vision locally in a chat-like mode, and explore
    its Multimodal skills on a Colab notebook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何在本地以类似聊天的方式构建Llama 3.2-Vision，并在Colab笔记本中探索其多模态技能
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[![Lihi
    Gur Arie, 博士](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    [Lihi Gur Arie, 博士](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    ·7 min read·Dec 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    ·阅读时长：7分钟·2024年12月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea91d922aefd2e7597a663d07051b804.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea91d922aefd2e7597a663d07051b804.png)'
- en: Annotated image by author. Original image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注释的图像。原始图像来自[Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/)。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The integration of vision capabilities with Large Language Models (LLMs) is
    revolutionizing the computer vision field through multimodal LLMs (MLLM). These
    models combine text and visual inputs, showing impressive abilities in image understanding
    and reasoning. While these models were previously accessible only via APIs, recent
    open source options now allow for local execution, making them more appealing
    for production environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将视觉能力与大型语言模型（LLM）相结合，通过多模态LLM（MLLM）正在彻底改变计算机视觉领域。这些模型结合了文本和视觉输入，展现了在图像理解和推理方面的出色能力。尽管这些模型之前只能通过API访问，但最近的开源选项现在允许本地执行，使它们在生产环境中更具吸引力。
- en: In this tutorial, we will learn how to chat with our images using the open source
    Llama 3.2-Vision model, and you’ll be amazed by its OCR, image understanding,
    and reasoning capabilities. All the code is conveniently provided in a handy Colab
    notebook.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将学习如何使用开源的Llama 3.2-Vision模型与图像进行对话，您将会惊叹于它的OCR、图像理解和推理能力。所有代码都方便地提供在一个实用的Colab笔记本中。
- en: '*If you don’t have a paid Medium account, you can read for free* [*here*](/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675)*.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您没有付费的Medium账号，您可以免费阅读* [*这里*](/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675)*。*'
- en: '**Llama 3.2-Vision**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Llama 3.2-Vision**'
- en: '**Background**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**背景**'
- en: 'Llama, short for “Large Language Model Meta AI” is a series of advanced LLMs
    developed by Meta. Their latest, Llama 3.2, was introduced with advanced vision
    capabilities. The vision variant comes in two sizes: 11B and 90B parameters, enabling
    inference on edge devices. With a context window of up to 128k tokens and support
    for high resolution images up to 1120x1120 pixels, Llama 3.2 can process complex
    visual and textual information.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Llama，缩写为“Large Language Model Meta AI”（大型语言模型元AI），是一系列由Meta开发的先进LLM。它们的最新版本Llama
    3.2，推出了先进的视觉能力。视觉变体有两种参数大小：11B和90B，能够在边缘设备上进行推理。Llama 3.2拥有多达128k个token的上下文窗口，并支持分辨率高达1120x1120像素的图像，能够处理复杂的视觉和文本信息。
- en: '**Architecture**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**架构**'
- en: The Llama series of models are decoder-only Transformers. Llama 3.2-Vision is
    built on top of a pre-trained Llama 3.1 text-only model. It utilizes a standard,
    dense auto-regressive Transformer architecture, that does not deviate significantly
    from its predecessors, Llama and Llama 2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 系列模型是仅解码器的 Transformer 模型。Llama 3.2-Vision 构建在预训练的 Llama 3.1 仅文本模型之上。它使用标准的密集自回归
    Transformer 架构，与其前身 Llama 和 Llama 2 相比没有显著偏差。
- en: To support visual tasks, Llama 3.2 extracts image representation vectors using
    a pre-trained vision encoder (ViT-H/14), and integrates these representations
    into the frozen language model using a vision adapter. The adapter consists of
    a series of cross-attention layers that allow the model to focus on specific parts
    of the image that correspond to the text being processed [1].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持视觉任务，Llama 3.2 使用预训练的视觉编码器（ViT-H/14）提取图像表示向量，并通过视觉适配器将这些表示集成到冻结的语言模型中。该适配器由一系列交叉注意力层组成，允许模型专注于与正在处理的文本相对应的图像特定部分
    [1]。
- en: The adapter is trained on text-image pairs to align image representations with
    language representations. During adapter training, the parameters of the image
    encoder are updated, while the language model parameters remain frozen to preserve
    existing language capabilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该适配器在文本-图像对上进行训练，以将图像表示与语言表示对齐。在适配器训练过程中，图像编码器的参数会更新，而语言模型的参数保持不变，以保留现有的语言能力。
- en: '![](../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png)'
- en: Llama 3.2-Vision architecture. The vision module (green) is integrated into
    the fixed language model (pink). Image was created by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.2-Vision 架构。视觉模块（绿色）与固定的语言模型（粉色）集成。此图由作者创建。
- en: This design allows Llama 3.2 to excel in multimodal tasks while maintaining
    its strong text-only performance. The resulting model demonstrates impressive
    capabilities in tasks that require both image and language understanding, and
    allowing users to interactively communicate with their visual inputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计使得 Llama 3.2 能在多模态任务中表现出色，同时保持其强大的仅文本性能。最终的模型展示了在需要图像和语言理解的任务中令人印象深刻的能力，并允许用户与其视觉输入进行互动。
- en: Lets code!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始编码吧！
- en: With our understanding of Llama 3.2’s architecture in place, we can dive into
    the practical implementation. But first, we need do some preparations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解了 Llama 3.2 的架构之后，我们可以深入到实际的实现中。但首先，我们需要做一些准备工作。
- en: '**Preparations**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**准备工作**'
- en: 'Before running Llama 3.2 — Vision 11B on Google Colab, we need to make some
    preparations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Colab 上运行 Llama 3.2 — Vision 11B 之前，我们需要做一些准备工作：
- en: 'GPU setup:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU 设置：
- en: A high-end GPU with at least 22GB VRAM is recommended for efficient inference
    [2].
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐使用至少 22GB VRAM 的高端 GPU 以实现高效推理 [2]。
- en: 'For Google Colab users: Navigate to ‘Runtime’ > ‘Change runtime type’ > ‘A100
    GPU’. Note that high-end GPU’s may not be available for free Colab users.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Google Colab 用户：导航至 ‘运行时’ > ‘更改运行时类型’ > ‘A100 GPU’。请注意，高端 GPU 可能对免费 Colab
    用户不可用。
- en: '2\. Model Permissions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 模型权限：
- en: Request Access to Llama 3.2 Models [here](https://www.llama.com/llama-downloads/).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求访问 Llama 3.2 模型 [这里](https://www.llama.com/llama-downloads/)。
- en: '3\. Hugging Face Setup:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Hugging Face 设置：
- en: Create a Hugging Face account if you don’t have on already [here](https://huggingface.co/join).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你还没有 Hugging Face 账户，请 [这里](https://huggingface.co/join) 创建一个。
- en: Generate an access token from your Hugging Face account if you don’t have one,
    [here](https://huggingface.co/settings/tokens).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你还没有访问令牌，请从你的 Hugging Face 账户生成一个，[这里](https://huggingface.co/settings/tokens)。
- en: For Google Colab users, set up the Hugging Face token as a secret environmental
    variable named ‘HF_TOKEN’ in google Colab Secrets.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Google Colab 用户，请将 Hugging Face 令牌设置为名为‘HF_TOKEN’的秘密环境变量，并添加到 Google Colab
    Secrets 中。
- en: 4\. Install the required libraries.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 安装所需的库。
- en: '**Loading The** **Model**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载模型**'
- en: Once we’ve set up the environment and acquired the necessary permissions, we
    will use the Hugging Face Transformers library to instantiate the model and its
    associated processor. The processor is responsible for preparing inputs for the
    model and formatting its outputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置好环境并获得必要的权限，我们将使用 Hugging Face Transformers 库来实例化模型及其相关的处理器。处理器负责为模型准备输入并格式化输出。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Expected Chat Template**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期的聊天模板**'
- en: 'Chat templates maintain context through conversation history by storing exchanges
    between the “user” (us) and the “assistant” (the AI model). The conversation history
    is structured as a list of dictionaries called `messages`, where each dictionary
    represents a single conversational turn, including both user and model responses.
    User turns can include image-text or text-only inputs, with `{"type": "image"}`
    indicating an image input.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '聊天模板通过存储“用户”（我们）与“助手”（AI 模型）之间的对话交换，保持上下文。对话历史以一个字典列表的形式结构化，称为`messages`，每个字典代表一个单独的对话轮次，包括用户和模型的回应。用户的轮次可以包含图像-文本或仅文本输入，`{"type":
    "image"}`表示图像输入。'
- en: 'For example, after a few chat iterations, the `messages` list might look like
    this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在经过几轮聊天后，`messages`列表可能如下所示：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This list of messages is later passed to the `apply_chat_template()` method
    to convert the conversation into a single tokenizable string in the format that
    the model expects.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这个消息列表稍后会传递给`apply_chat_template()`方法，将对话转换成模型期望的格式，以便作为一个单一的可标记字符串。
- en: '**Main function**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要功能**'
- en: For this tutorial I provided a `chat_with_mllm` function that enables dynamic
    conversation with the Llama 3.2 MLLM. This function handles image loading, pre-processes
    both images and the text inputs, generates model responses, and manages the conversation
    history to enable chat-mode interactions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我提供了一个`chat_with_mllm`函数，使得可以与Llama 3.2 MLLM进行动态对话。该函数处理图像加载，预处理图像和文本输入，生成模型回应，并管理对话历史，以支持聊天模式的交互。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Chat with Llama
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Llama聊天
- en: '**Butterfly Image Example**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**蝴蝶图像示例**'
- en: In our our first example, we’ll chat with Llama 3.2 about an image of a hatching
    butterfly. Since Llama 3.2-Vision does not support prompting with system prompts
    when using images, we will append instructions directly to the user prompt to
    guide the model’s responses. By setting `do_sample=True` and `temperature=0.2`
    , we enable slight randomness while maintaining response coherence. For fixed
    answer, you can set `do_sample==False` . The `messages` parameter, which holds
    the chat history, is initially empty, as in the `images` parameter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个例子中，我们将与Llama 3.2聊一个孵化蝴蝶的图像。由于Llama 3.2-Vision在使用图像时不支持系统提示的输入，我们将直接把指令附加到用户提示中，以指导模型的回答。通过设置`do_sample=True`和`temperature=0.2`，我们在保持回应连贯性的同时引入了轻微的随机性。如果需要固定答案，可以设置`do_sample==False`。`messages`参数用于存储聊天历史，最初为空，与`images`参数一样。
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/547d3652bb89e47a8039ddfee8ff7c8b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/547d3652bb89e47a8039ddfee8ff7c8b.png)'
- en: Image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/)提供。
- en: As we can see, the output is accurate and concise, demonstrating that the model
    effectively understood the image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，输出准确简洁，展示了模型有效理解图像的能力。
- en: 'For the next chat iteration, we’ll pass a new prompt along with the chat history
    (`messages`) and the image file (`images`). The new prompt is designed to assess
    the reasoning ability of Llama 3.2:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于下一个聊天轮次，我们将传递一个新的提示以及聊天历史（`messages`）和图像文件（`images`）。新提示旨在评估Llama 3.2的推理能力：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We continued this chat in the provided Colab notebook and obtained the following
    conversation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在提供的Colab笔记本中继续这个对话，并得到了以下对话：
- en: '![](../Images/a72d924ad8dd280b870dadf9ab7c7224.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72d924ad8dd280b870dadf9ab7c7224.png)'
- en: Image by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The conversation highlights the model’s image understanding ability by accurately
    describing the scene. It also demonstrates its reasoning skills by logically connecting
    information to correctly conclude what will happen to the chrysalis and explaining
    why some are brown while others are green.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对话突出了模型的图像理解能力，准确描述了场景。它还展示了其推理能力，通过逻辑地连接信息，正确地得出蛹将会发生什么，并解释了为什么有些蛹是棕色的，而有些是绿色的。
- en: '**2\. Meme Image Example**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 表情包图像示例**'
- en: In this example, I will show the model a meme I created myself, to assess Llama’s
    OCR capabilities and determine whether it understands my sense of humor.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我将向模型展示我自己制作的一个表情包，评估Llama的OCR能力，并判断它是否能理解我的幽默感。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the input meme:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输入的表情包：
- en: '![](../Images/e196e4c1075cc3f596b4cfa4160b6658.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e196e4c1075cc3f596b4cfa4160b6658.png)'
- en: Meme by author. Original bear image by [Hans-Jurgen Mager](https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表情包由作者制作。原始熊的图像由[Hans-Jurgen Mager](https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA)提供。
- en: 'And this is the model’s response:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型的回应：
- en: '![](../Images/92233ad5cc21df183820c16f4e063aae.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92233ad5cc21df183820c16f4e063aae.png)'
- en: Image by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As we can see, the model demonstrates great OCR abilities, and understands the
    meaning of the text in the image. As for its sense of humor — what do you think,
    did it get it? Did you get it? Maybe I should work on my sense of humor too!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，模型展示了出色的OCR能力，能够理解图像中文字的含义。至于它的幽默感——你怎么看，它理解了吗？你明白了吗？也许我也该提升一下我的幽默感！
- en: Concluding Remarks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: In this tutorial, we learned how to build the Llama 3.2-Vision model locally
    and manage conversation history for chat-like interactions, enhancing user engagement.
    We explored Llama 3.2’s zero-shot abilities and were impressed by its scene understanding,
    reasoning and OCR skills.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何在本地构建Llama 3.2-Vision模型，并管理聊天式互动的对话历史，从而增强用户参与度。我们探索了Llama 3.2的零-shot能力，并对其场景理解、推理和OCR技能印象深刻。
- en: Advanced techniques can be applied to Llama 3.2, such as fine-tuning on unique
    data, or using retrieval-augmented generation (RAG) to ground predictions and
    reduce hallucinations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 高级技术可以应用于Llama 3.2，例如在独特数据上进行微调，或使用检索增强生成（RAG）来支持预测并减少幻觉。
- en: Overall, this tutorial provides insight into the rapidly evolving field of Multimodal
    LLMs and their powerful capabilities for various applications.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，本教程提供了对多模态LLM快速发展的领域以及它们在各种应用中的强大能力的洞察。
- en: Thank you for reading!
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Congratulations on making it all the way here. Click 👍x50 to show your appreciation
    and raise the algorithm self esteem 🤓
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你一直看到这里。点击👍x50表示感谢，并提升算法的自尊心🤓
- en: '**Want to learn more?**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多吗？**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**探索**](https://medium.com/@lihigurarie)我写的其他文章'
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**订阅**](https://medium.com/@lihigurarie/subscribe)以便在我发布文章时收到通知'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)上关注我
- en: 'Full Code as Colab notebook:'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码作为Colab笔记本：
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Colab笔记本上的代码：[link](https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa)'
- en: '[1] [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Llama 3模型群体](https://arxiv.org/pdf/2407.21783)'
- en: '[2] [Llama 3.2 11B Vision Requirements](https://llamaimodel.com/requirements-3-2/)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Llama 3.2 11B Vision要求](https://llamaimodel.com/requirements-3-2/)'
