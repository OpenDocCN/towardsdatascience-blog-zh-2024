- en: Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Llama 3.2-Visionå¤šæ¨¡æ€LLMä¸æ‚¨çš„å›¾ç‰‡è¿›è¡Œå¯¹è¯
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05](https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05](https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05)
- en: Learn how to build Llama 3.2-Vision locally in a chat-like mode, and explore
    its Multimodal skills on a Colab notebook
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ å¦‚ä½•åœ¨æœ¬åœ°ä»¥ç±»ä¼¼èŠå¤©çš„æ–¹å¼æ„å»ºLlama 3.2-Visionï¼Œå¹¶åœ¨Colabç¬”è®°æœ¬ä¸­æ¢ç´¢å…¶å¤šæ¨¡æ€æŠ€èƒ½
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[![Lihi
    Gur Arie, åšå£«](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    [Lihi Gur Arie, åšå£«](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    Â·7 min readÂ·Dec 5, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    Â·é˜…è¯»æ—¶é•¿ï¼š7åˆ†é’ŸÂ·2024å¹´12æœˆ5æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea91d922aefd2e7597a663d07051b804.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea91d922aefd2e7597a663d07051b804.png)'
- en: Annotated image by author. Original image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æ³¨é‡Šçš„å›¾åƒã€‚åŸå§‹å›¾åƒæ¥è‡ª[Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/)ã€‚
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: The integration of vision capabilities with Large Language Models (LLMs) is
    revolutionizing the computer vision field through multimodal LLMs (MLLM). These
    models combine text and visual inputs, showing impressive abilities in image understanding
    and reasoning. While these models were previously accessible only via APIs, recent
    open source options now allow for local execution, making them more appealing
    for production environments.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è§†è§‰èƒ½åŠ›ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›¸ç»“åˆï¼Œé€šè¿‡å¤šæ¨¡æ€LLMï¼ˆMLLMï¼‰æ­£åœ¨å½»åº•æ”¹å˜è®¡ç®—æœºè§†è§‰é¢†åŸŸã€‚è¿™äº›æ¨¡å‹ç»“åˆäº†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ï¼Œå±•ç°äº†åœ¨å›¾åƒç†è§£å’Œæ¨ç†æ–¹é¢çš„å‡ºè‰²èƒ½åŠ›ã€‚å°½ç®¡è¿™äº›æ¨¡å‹ä¹‹å‰åªèƒ½é€šè¿‡APIè®¿é—®ï¼Œä½†æœ€è¿‘çš„å¼€æºé€‰é¡¹ç°åœ¨å…è®¸æœ¬åœ°æ‰§è¡Œï¼Œä½¿å®ƒä»¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›´å…·å¸å¼•åŠ›ã€‚
- en: In this tutorial, we will learn how to chat with our images using the open source
    Llama 3.2-Vision model, and youâ€™ll be amazed by its OCR, image understanding,
    and reasoning capabilities. All the code is conveniently provided in a handy Colab
    notebook.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨å¼€æºçš„Llama 3.2-Visionæ¨¡å‹ä¸å›¾åƒè¿›è¡Œå¯¹è¯ï¼Œæ‚¨å°†ä¼šæƒŠå¹äºå®ƒçš„OCRã€å›¾åƒç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ‰€æœ‰ä»£ç éƒ½æ–¹ä¾¿åœ°æä¾›åœ¨ä¸€ä¸ªå®ç”¨çš„Colabç¬”è®°æœ¬ä¸­ã€‚
- en: '*If you donâ€™t have a paid Medium account, you can read for free* [*here*](/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675)*.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœæ‚¨æ²¡æœ‰ä»˜è´¹çš„Mediumè´¦å·ï¼Œæ‚¨å¯ä»¥å…è´¹é˜…è¯»* [*è¿™é‡Œ*](/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675)*ã€‚*'
- en: '**Llama 3.2-Vision**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Llama 3.2-Vision**'
- en: '**Background**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**èƒŒæ™¯**'
- en: 'Llama, short for â€œLarge Language Model Meta AIâ€ is a series of advanced LLMs
    developed by Meta. Their latest, Llama 3.2, was introduced with advanced vision
    capabilities. The vision variant comes in two sizes: 11B and 90B parameters, enabling
    inference on edge devices. With a context window of up to 128k tokens and support
    for high resolution images up to 1120x1120 pixels, Llama 3.2 can process complex
    visual and textual information.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Llamaï¼Œç¼©å†™ä¸ºâ€œLarge Language Model Meta AIâ€ï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹å…ƒAIï¼‰ï¼Œæ˜¯ä¸€ç³»åˆ—ç”±Metaå¼€å‘çš„å…ˆè¿›LLMã€‚å®ƒä»¬çš„æœ€æ–°ç‰ˆæœ¬Llama
    3.2ï¼Œæ¨å‡ºäº†å…ˆè¿›çš„è§†è§‰èƒ½åŠ›ã€‚è§†è§‰å˜ä½“æœ‰ä¸¤ç§å‚æ•°å¤§å°ï¼š11Bå’Œ90Bï¼Œèƒ½å¤Ÿåœ¨è¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œæ¨ç†ã€‚Llama 3.2æ‹¥æœ‰å¤šè¾¾128kä¸ªtokençš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¹¶æ”¯æŒåˆ†è¾¨ç‡é«˜è¾¾1120x1120åƒç´ çš„å›¾åƒï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚
- en: '**Architecture**'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¶æ„**'
- en: The Llama series of models are decoder-only Transformers. Llama 3.2-Vision is
    built on top of a pre-trained Llama 3.1 text-only model. It utilizes a standard,
    dense auto-regressive Transformer architecture, that does not deviate significantly
    from its predecessors, Llama and Llama 2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Llama ç³»åˆ—æ¨¡å‹æ˜¯ä»…è§£ç å™¨çš„ Transformer æ¨¡å‹ã€‚Llama 3.2-Vision æ„å»ºåœ¨é¢„è®­ç»ƒçš„ Llama 3.1 ä»…æ–‡æœ¬æ¨¡å‹ä¹‹ä¸Šã€‚å®ƒä½¿ç”¨æ ‡å‡†çš„å¯†é›†è‡ªå›å½’
    Transformer æ¶æ„ï¼Œä¸å…¶å‰èº« Llama å’Œ Llama 2 ç›¸æ¯”æ²¡æœ‰æ˜¾è‘—åå·®ã€‚
- en: To support visual tasks, Llama 3.2 extracts image representation vectors using
    a pre-trained vision encoder (ViT-H/14), and integrates these representations
    into the frozen language model using a vision adapter. The adapter consists of
    a series of cross-attention layers that allow the model to focus on specific parts
    of the image that correspond to the text being processed [1].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ”¯æŒè§†è§‰ä»»åŠ¡ï¼ŒLlama 3.2 ä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆViT-H/14ï¼‰æå–å›¾åƒè¡¨ç¤ºå‘é‡ï¼Œå¹¶é€šè¿‡è§†è§‰é€‚é…å™¨å°†è¿™äº›è¡¨ç¤ºé›†æˆåˆ°å†»ç»“çš„è¯­è¨€æ¨¡å‹ä¸­ã€‚è¯¥é€‚é…å™¨ç”±ä¸€ç³»åˆ—äº¤å‰æ³¨æ„åŠ›å±‚ç»„æˆï¼Œå…è®¸æ¨¡å‹ä¸“æ³¨äºä¸æ­£åœ¨å¤„ç†çš„æ–‡æœ¬ç›¸å¯¹åº”çš„å›¾åƒç‰¹å®šéƒ¨åˆ†
    [1]ã€‚
- en: The adapter is trained on text-image pairs to align image representations with
    language representations. During adapter training, the parameters of the image
    encoder are updated, while the language model parameters remain frozen to preserve
    existing language capabilities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥é€‚é…å™¨åœ¨æ–‡æœ¬-å›¾åƒå¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥å°†å›¾åƒè¡¨ç¤ºä¸è¯­è¨€è¡¨ç¤ºå¯¹é½ã€‚åœ¨é€‚é…å™¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå›¾åƒç¼–ç å™¨çš„å‚æ•°ä¼šæ›´æ–°ï¼Œè€Œè¯­è¨€æ¨¡å‹çš„å‚æ•°ä¿æŒä¸å˜ï¼Œä»¥ä¿ç•™ç°æœ‰çš„è¯­è¨€èƒ½åŠ›ã€‚
- en: '![](../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png)'
- en: Llama 3.2-Vision architecture. The vision module (green) is integrated into
    the fixed language model (pink). Image was created by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3.2-Vision æ¶æ„ã€‚è§†è§‰æ¨¡å—ï¼ˆç»¿è‰²ï¼‰ä¸å›ºå®šçš„è¯­è¨€æ¨¡å‹ï¼ˆç²‰è‰²ï¼‰é›†æˆã€‚æ­¤å›¾ç”±ä½œè€…åˆ›å»ºã€‚
- en: This design allows Llama 3.2 to excel in multimodal tasks while maintaining
    its strong text-only performance. The resulting model demonstrates impressive
    capabilities in tasks that require both image and language understanding, and
    allowing users to interactively communicate with their visual inputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è®¾è®¡ä½¿å¾— Llama 3.2 èƒ½åœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼ŒåŒæ—¶ä¿æŒå…¶å¼ºå¤§çš„ä»…æ–‡æœ¬æ€§èƒ½ã€‚æœ€ç»ˆçš„æ¨¡å‹å±•ç¤ºäº†åœ¨éœ€è¦å›¾åƒå’Œè¯­è¨€ç†è§£çš„ä»»åŠ¡ä¸­ä»¤äººå°è±¡æ·±åˆ»çš„èƒ½åŠ›ï¼Œå¹¶å…è®¸ç”¨æˆ·ä¸å…¶è§†è§‰è¾“å…¥è¿›è¡Œäº’åŠ¨ã€‚
- en: Lets code!
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼€å§‹ç¼–ç å§ï¼
- en: With our understanding of Llama 3.2â€™s architecture in place, we can dive into
    the practical implementation. But first, we need do some preparations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç†è§£äº† Llama 3.2 çš„æ¶æ„ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥åˆ°å®é™…çš„å®ç°ä¸­ã€‚ä½†é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åšä¸€äº›å‡†å¤‡å·¥ä½œã€‚
- en: '**Preparations**'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**å‡†å¤‡å·¥ä½œ**'
- en: 'Before running Llama 3.2 â€” Vision 11B on Google Colab, we need to make some
    preparations:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Google Colab ä¸Šè¿è¡Œ Llama 3.2 â€” Vision 11B ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦åšä¸€äº›å‡†å¤‡å·¥ä½œï¼š
- en: 'GPU setup:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU è®¾ç½®ï¼š
- en: A high-end GPU with at least 22GB VRAM is recommended for efficient inference
    [2].
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨èä½¿ç”¨è‡³å°‘ 22GB VRAM çš„é«˜ç«¯ GPU ä»¥å®ç°é«˜æ•ˆæ¨ç† [2]ã€‚
- en: 'For Google Colab users: Navigate to â€˜Runtimeâ€™ > â€˜Change runtime typeâ€™ > â€˜A100
    GPUâ€™. Note that high-end GPUâ€™s may not be available for free Colab users.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº Google Colab ç”¨æˆ·ï¼šå¯¼èˆªè‡³ â€˜è¿è¡Œæ—¶â€™ > â€˜æ›´æ”¹è¿è¡Œæ—¶ç±»å‹â€™ > â€˜A100 GPUâ€™ã€‚è¯·æ³¨æ„ï¼Œé«˜ç«¯ GPU å¯èƒ½å¯¹å…è´¹ Colab
    ç”¨æˆ·ä¸å¯ç”¨ã€‚
- en: '2\. Model Permissions:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. æ¨¡å‹æƒé™ï¼š
- en: Request Access to Llama 3.2 Models [here](https://www.llama.com/llama-downloads/).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ±‚è®¿é—® Llama 3.2 æ¨¡å‹ [è¿™é‡Œ](https://www.llama.com/llama-downloads/)ã€‚
- en: '3\. Hugging Face Setup:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Hugging Face è®¾ç½®ï¼š
- en: Create a Hugging Face account if you donâ€™t have on already [here](https://huggingface.co/join).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰ Hugging Face è´¦æˆ·ï¼Œè¯· [è¿™é‡Œ](https://huggingface.co/join) åˆ›å»ºä¸€ä¸ªã€‚
- en: Generate an access token from your Hugging Face account if you donâ€™t have one,
    [here](https://huggingface.co/settings/tokens).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜æ²¡æœ‰è®¿é—®ä»¤ç‰Œï¼Œè¯·ä»ä½ çš„ Hugging Face è´¦æˆ·ç”Ÿæˆä¸€ä¸ªï¼Œ[è¿™é‡Œ](https://huggingface.co/settings/tokens)ã€‚
- en: For Google Colab users, set up the Hugging Face token as a secret environmental
    variable named â€˜HF_TOKENâ€™ in google Colab Secrets.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº Google Colab ç”¨æˆ·ï¼Œè¯·å°† Hugging Face ä»¤ç‰Œè®¾ç½®ä¸ºåä¸ºâ€˜HF_TOKENâ€™çš„ç§˜å¯†ç¯å¢ƒå˜é‡ï¼Œå¹¶æ·»åŠ åˆ° Google Colab
    Secrets ä¸­ã€‚
- en: 4\. Install the required libraries.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. å®‰è£…æ‰€éœ€çš„åº“ã€‚
- en: '**Loading The** **Model**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŠ è½½æ¨¡å‹**'
- en: Once weâ€™ve set up the environment and acquired the necessary permissions, we
    will use the Hugging Face Transformers library to instantiate the model and its
    associated processor. The processor is responsible for preparing inputs for the
    model and formatting its outputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è®¾ç½®å¥½ç¯å¢ƒå¹¶è·å¾—å¿…è¦çš„æƒé™ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face Transformers åº“æ¥å®ä¾‹åŒ–æ¨¡å‹åŠå…¶ç›¸å…³çš„å¤„ç†å™¨ã€‚å¤„ç†å™¨è´Ÿè´£ä¸ºæ¨¡å‹å‡†å¤‡è¾“å…¥å¹¶æ ¼å¼åŒ–è¾“å‡ºã€‚
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Expected Chat Template**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**é¢„æœŸçš„èŠå¤©æ¨¡æ¿**'
- en: 'Chat templates maintain context through conversation history by storing exchanges
    between the â€œuserâ€ (us) and the â€œassistantâ€ (the AI model). The conversation history
    is structured as a list of dictionaries called `messages`, where each dictionary
    represents a single conversational turn, including both user and model responses.
    User turns can include image-text or text-only inputs, with `{"type": "image"}`
    indicating an image input.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'èŠå¤©æ¨¡æ¿é€šè¿‡å­˜å‚¨â€œç”¨æˆ·â€ï¼ˆæˆ‘ä»¬ï¼‰ä¸â€œåŠ©æ‰‹â€ï¼ˆAI æ¨¡å‹ï¼‰ä¹‹é—´çš„å¯¹è¯äº¤æ¢ï¼Œä¿æŒä¸Šä¸‹æ–‡ã€‚å¯¹è¯å†å²ä»¥ä¸€ä¸ªå­—å…¸åˆ—è¡¨çš„å½¢å¼ç»“æ„åŒ–ï¼Œç§°ä¸º`messages`ï¼Œæ¯ä¸ªå­—å…¸ä»£è¡¨ä¸€ä¸ªå•ç‹¬çš„å¯¹è¯è½®æ¬¡ï¼ŒåŒ…æ‹¬ç”¨æˆ·å’Œæ¨¡å‹çš„å›åº”ã€‚ç”¨æˆ·çš„è½®æ¬¡å¯ä»¥åŒ…å«å›¾åƒ-æ–‡æœ¬æˆ–ä»…æ–‡æœ¬è¾“å…¥ï¼Œ`{"type":
    "image"}`è¡¨ç¤ºå›¾åƒè¾“å…¥ã€‚'
- en: 'For example, after a few chat iterations, the `messages` list might look like
    this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ç»è¿‡å‡ è½®èŠå¤©åï¼Œ`messages`åˆ—è¡¨å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This list of messages is later passed to the `apply_chat_template()` method
    to convert the conversation into a single tokenizable string in the format that
    the model expects.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¶ˆæ¯åˆ—è¡¨ç¨åä¼šä¼ é€’ç»™`apply_chat_template()`æ–¹æ³•ï¼Œå°†å¯¹è¯è½¬æ¢æˆæ¨¡å‹æœŸæœ›çš„æ ¼å¼ï¼Œä»¥ä¾¿ä½œä¸ºä¸€ä¸ªå•ä¸€çš„å¯æ ‡è®°å­—ç¬¦ä¸²ã€‚
- en: '**Main function**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸»è¦åŠŸèƒ½**'
- en: For this tutorial I provided a `chat_with_mllm` function that enables dynamic
    conversation with the Llama 3.2 MLLM. This function handles image loading, pre-processes
    both images and the text inputs, generates model responses, and manages the conversation
    history to enable chat-mode interactions.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘æä¾›äº†ä¸€ä¸ª`chat_with_mllm`å‡½æ•°ï¼Œä½¿å¾—å¯ä»¥ä¸Llama 3.2 MLLMè¿›è¡ŒåŠ¨æ€å¯¹è¯ã€‚è¯¥å‡½æ•°å¤„ç†å›¾åƒåŠ è½½ï¼Œé¢„å¤„ç†å›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œç”Ÿæˆæ¨¡å‹å›åº”ï¼Œå¹¶ç®¡ç†å¯¹è¯å†å²ï¼Œä»¥æ”¯æŒèŠå¤©æ¨¡å¼çš„äº¤äº’ã€‚
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Chat with Llama
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸LlamaèŠå¤©
- en: '**Butterfly Image Example**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è´è¶å›¾åƒç¤ºä¾‹**'
- en: In our our first example, weâ€™ll chat with Llama 3.2 about an image of a hatching
    butterfly. Since Llama 3.2-Vision does not support prompting with system prompts
    when using images, we will append instructions directly to the user prompt to
    guide the modelâ€™s responses. By setting `do_sample=True` and `temperature=0.2`
    , we enable slight randomness while maintaining response coherence. For fixed
    answer, you can set `do_sample==False` . The `messages` parameter, which holds
    the chat history, is initially empty, as in the `images` parameter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä¸Llama 3.2èŠä¸€ä¸ªå­µåŒ–è´è¶çš„å›¾åƒã€‚ç”±äºLlama 3.2-Visionåœ¨ä½¿ç”¨å›¾åƒæ—¶ä¸æ”¯æŒç³»ç»Ÿæç¤ºçš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†ç›´æ¥æŠŠæŒ‡ä»¤é™„åŠ åˆ°ç”¨æˆ·æç¤ºä¸­ï¼Œä»¥æŒ‡å¯¼æ¨¡å‹çš„å›ç­”ã€‚é€šè¿‡è®¾ç½®`do_sample=True`å’Œ`temperature=0.2`ï¼Œæˆ‘ä»¬åœ¨ä¿æŒå›åº”è¿è´¯æ€§çš„åŒæ—¶å¼•å…¥äº†è½»å¾®çš„éšæœºæ€§ã€‚å¦‚æœéœ€è¦å›ºå®šç­”æ¡ˆï¼Œå¯ä»¥è®¾ç½®`do_sample==False`ã€‚`messages`å‚æ•°ç”¨äºå­˜å‚¨èŠå¤©å†å²ï¼Œæœ€åˆä¸ºç©ºï¼Œä¸`images`å‚æ•°ä¸€æ ·ã€‚
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/547d3652bb89e47a8039ddfee8ff7c8b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/547d3652bb89e47a8039ddfee8ff7c8b.png)'
- en: Image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/)æä¾›ã€‚
- en: As we can see, the output is accurate and concise, demonstrating that the model
    effectively understood the image.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¾“å‡ºå‡†ç¡®ç®€æ´ï¼Œå±•ç¤ºäº†æ¨¡å‹æœ‰æ•ˆç†è§£å›¾åƒçš„èƒ½åŠ›ã€‚
- en: 'For the next chat iteration, weâ€™ll pass a new prompt along with the chat history
    (`messages`) and the image file (`images`). The new prompt is designed to assess
    the reasoning ability of Llama 3.2:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸‹ä¸€ä¸ªèŠå¤©è½®æ¬¡ï¼Œæˆ‘ä»¬å°†ä¼ é€’ä¸€ä¸ªæ–°çš„æç¤ºä»¥åŠèŠå¤©å†å²ï¼ˆ`messages`ï¼‰å’Œå›¾åƒæ–‡ä»¶ï¼ˆ`images`ï¼‰ã€‚æ–°æç¤ºæ—¨åœ¨è¯„ä¼°Llama 3.2çš„æ¨ç†èƒ½åŠ›ï¼š
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We continued this chat in the provided Colab notebook and obtained the following
    conversation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æä¾›çš„Colabç¬”è®°æœ¬ä¸­ç»§ç»­è¿™ä¸ªå¯¹è¯ï¼Œå¹¶å¾—åˆ°äº†ä»¥ä¸‹å¯¹è¯ï¼š
- en: '![](../Images/a72d924ad8dd280b870dadf9ab7c7224.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a72d924ad8dd280b870dadf9ab7c7224.png)'
- en: Image by Author
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: The conversation highlights the modelâ€™s image understanding ability by accurately
    describing the scene. It also demonstrates its reasoning skills by logically connecting
    information to correctly conclude what will happen to the chrysalis and explaining
    why some are brown while others are green.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹è¯çªå‡ºäº†æ¨¡å‹çš„å›¾åƒç†è§£èƒ½åŠ›ï¼Œå‡†ç¡®æè¿°äº†åœºæ™¯ã€‚å®ƒè¿˜å±•ç¤ºäº†å…¶æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡é€»è¾‘åœ°è¿æ¥ä¿¡æ¯ï¼Œæ­£ç¡®åœ°å¾—å‡ºè›¹å°†ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆæœ‰äº›è›¹æ˜¯æ£•è‰²çš„ï¼Œè€Œæœ‰äº›æ˜¯ç»¿è‰²çš„ã€‚
- en: '**2\. Meme Image Example**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. è¡¨æƒ…åŒ…å›¾åƒç¤ºä¾‹**'
- en: In this example, I will show the model a meme I created myself, to assess Llamaâ€™s
    OCR capabilities and determine whether it understands my sense of humor.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘å°†å‘æ¨¡å‹å±•ç¤ºæˆ‘è‡ªå·±åˆ¶ä½œçš„ä¸€ä¸ªè¡¨æƒ…åŒ…ï¼Œè¯„ä¼°Llamaçš„OCRèƒ½åŠ›ï¼Œå¹¶åˆ¤æ–­å®ƒæ˜¯å¦èƒ½ç†è§£æˆ‘çš„å¹½é»˜æ„Ÿã€‚
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the input meme:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¾“å…¥çš„è¡¨æƒ…åŒ…ï¼š
- en: '![](../Images/e196e4c1075cc3f596b4cfa4160b6658.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e196e4c1075cc3f596b4cfa4160b6658.png)'
- en: Meme by author. Original bear image by [Hans-Jurgen Mager](https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æƒ…åŒ…ç”±ä½œè€…åˆ¶ä½œã€‚åŸå§‹ç†Šçš„å›¾åƒç”±[Hans-Jurgen Mager](https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA)æä¾›ã€‚
- en: 'And this is the modelâ€™s response:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¨¡å‹çš„å›åº”ï¼š
- en: '![](../Images/92233ad5cc21df183820c16f4e063aae.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92233ad5cc21df183820c16f4e063aae.png)'
- en: Image by Author
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: As we can see, the model demonstrates great OCR abilities, and understands the
    meaning of the text in the image. As for its sense of humor â€” what do you think,
    did it get it? Did you get it? Maybe I should work on my sense of humor too!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹å±•ç¤ºäº†å‡ºè‰²çš„OCRèƒ½åŠ›ï¼Œèƒ½å¤Ÿç†è§£å›¾åƒä¸­æ–‡å­—çš„å«ä¹‰ã€‚è‡³äºå®ƒçš„å¹½é»˜æ„Ÿâ€”â€”ä½ æ€ä¹ˆçœ‹ï¼Œå®ƒç†è§£äº†å—ï¼Ÿä½ æ˜ç™½äº†å—ï¼Ÿä¹Ÿè®¸æˆ‘ä¹Ÿè¯¥æå‡ä¸€ä¸‹æˆ‘çš„å¹½é»˜æ„Ÿï¼
- en: Concluding Remarks
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è¯­
- en: In this tutorial, we learned how to build the Llama 3.2-Vision model locally
    and manage conversation history for chat-like interactions, enhancing user engagement.
    We explored Llama 3.2â€™s zero-shot abilities and were impressed by its scene understanding,
    reasoning and OCR skills.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•åœ¨æœ¬åœ°æ„å»ºLlama 3.2-Visionæ¨¡å‹ï¼Œå¹¶ç®¡ç†èŠå¤©å¼äº’åŠ¨çš„å¯¹è¯å†å²ï¼Œä»è€Œå¢å¼ºç”¨æˆ·å‚ä¸åº¦ã€‚æˆ‘ä»¬æ¢ç´¢äº†Llama 3.2çš„é›¶-shotèƒ½åŠ›ï¼Œå¹¶å¯¹å…¶åœºæ™¯ç†è§£ã€æ¨ç†å’ŒOCRæŠ€èƒ½å°è±¡æ·±åˆ»ã€‚
- en: Advanced techniques can be applied to Llama 3.2, such as fine-tuning on unique
    data, or using retrieval-augmented generation (RAG) to ground predictions and
    reduce hallucinations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜çº§æŠ€æœ¯å¯ä»¥åº”ç”¨äºLlama 3.2ï¼Œä¾‹å¦‚åœ¨ç‹¬ç‰¹æ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œæˆ–ä½¿ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ¥æ”¯æŒé¢„æµ‹å¹¶å‡å°‘å¹»è§‰ã€‚
- en: Overall, this tutorial provides insight into the rapidly evolving field of Multimodal
    LLMs and their powerful capabilities for various applications.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæœ¬æ•™ç¨‹æä¾›äº†å¯¹å¤šæ¨¡æ€LLMå¿«é€Ÿå‘å±•çš„é¢†åŸŸä»¥åŠå®ƒä»¬åœ¨å„ç§åº”ç”¨ä¸­çš„å¼ºå¤§èƒ½åŠ›çš„æ´å¯Ÿã€‚
- en: Thank you for reading!
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼
- en: Congratulations on making it all the way here. Click ğŸ‘x50 to show your appreciation
    and raise the algorithm self esteem ğŸ¤“
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ ä¸€ç›´çœ‹åˆ°è¿™é‡Œã€‚ç‚¹å‡»ğŸ‘x50è¡¨ç¤ºæ„Ÿè°¢ï¼Œå¹¶æå‡ç®—æ³•çš„è‡ªå°Šå¿ƒğŸ¤“
- en: '**Want to learn more?**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒ³äº†è§£æ›´å¤šå—ï¼Ÿ**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles Iâ€™ve written'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ¢ç´¢**](https://medium.com/@lihigurarie)æˆ‘å†™çš„å…¶ä»–æ–‡ç« '
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è®¢é˜…**](https://medium.com/@lihigurarie/subscribe)ä»¥ä¾¿åœ¨æˆ‘å‘å¸ƒæ–‡ç« æ—¶æ”¶åˆ°é€šçŸ¥'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨[**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)ä¸Šå…³æ³¨æˆ‘
- en: 'Full Code as Colab notebook:'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œæ•´ä»£ç ä½œä¸ºColabç¬”è®°æœ¬ï¼š
- en: References
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Colabç¬”è®°æœ¬ä¸Šçš„ä»£ç ï¼š[link](https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa)'
- en: '[1] [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Llama 3æ¨¡å‹ç¾¤ä½“](https://arxiv.org/pdf/2407.21783)'
- en: '[2] [Llama 3.2 11B Vision Requirements](https://llamaimodel.com/requirements-3-2/)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Llama 3.2 11B Visionè¦æ±‚](https://llamaimodel.com/requirements-3-2/)'
