- en: Chat with Your Images Using Llama 3.2-Vision Multimodal LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05](https://towardsdatascience.com/chat-with-your-images-using-multimodal-llms-60af003e8bfa?source=collection_archive---------3-----------------------#2024-12-05)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to build Llama 3.2-Vision locally in a chat-like mode, and explore
    its Multimodal skills on a Colab notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--60af003e8bfa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60af003e8bfa--------------------------------)
    ·7 min read·Dec 5, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea91d922aefd2e7597a663d07051b804.png)'
  prefs: []
  type: TYPE_IMG
- en: Annotated image by author. Original image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The integration of vision capabilities with Large Language Models (LLMs) is
    revolutionizing the computer vision field through multimodal LLMs (MLLM). These
    models combine text and visual inputs, showing impressive abilities in image understanding
    and reasoning. While these models were previously accessible only via APIs, recent
    open source options now allow for local execution, making them more appealing
    for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will learn how to chat with our images using the open source
    Llama 3.2-Vision model, and you’ll be amazed by its OCR, image understanding,
    and reasoning capabilities. All the code is conveniently provided in a handy Colab
    notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you don’t have a paid Medium account, you can read for free* [*here*](/chat-with-your-images-using-multimodal-llms-60af003e8bfa?sk=7c8e28ff04e5109ca98e185fef120675)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Llama 3.2-Vision**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Background**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama, short for “Large Language Model Meta AI” is a series of advanced LLMs
    developed by Meta. Their latest, Llama 3.2, was introduced with advanced vision
    capabilities. The vision variant comes in two sizes: 11B and 90B parameters, enabling
    inference on edge devices. With a context window of up to 128k tokens and support
    for high resolution images up to 1120x1120 pixels, Llama 3.2 can process complex
    visual and textual information.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Architecture**'
  prefs: []
  type: TYPE_NORMAL
- en: The Llama series of models are decoder-only Transformers. Llama 3.2-Vision is
    built on top of a pre-trained Llama 3.1 text-only model. It utilizes a standard,
    dense auto-regressive Transformer architecture, that does not deviate significantly
    from its predecessors, Llama and Llama 2.
  prefs: []
  type: TYPE_NORMAL
- en: To support visual tasks, Llama 3.2 extracts image representation vectors using
    a pre-trained vision encoder (ViT-H/14), and integrates these representations
    into the frozen language model using a vision adapter. The adapter consists of
    a series of cross-attention layers that allow the model to focus on specific parts
    of the image that correspond to the text being processed [1].
  prefs: []
  type: TYPE_NORMAL
- en: The adapter is trained on text-image pairs to align image representations with
    language representations. During adapter training, the parameters of the image
    encoder are updated, while the language model parameters remain frozen to preserve
    existing language capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ce66fc5c8775374b6c44cf1d2fffe0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Llama 3.2-Vision architecture. The vision module (green) is integrated into
    the fixed language model (pink). Image was created by author.
  prefs: []
  type: TYPE_NORMAL
- en: This design allows Llama 3.2 to excel in multimodal tasks while maintaining
    its strong text-only performance. The resulting model demonstrates impressive
    capabilities in tasks that require both image and language understanding, and
    allowing users to interactively communicate with their visual inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Lets code!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With our understanding of Llama 3.2’s architecture in place, we can dive into
    the practical implementation. But first, we need do some preparations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Preparations**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before running Llama 3.2 — Vision 11B on Google Colab, we need to make some
    preparations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'GPU setup:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A high-end GPU with at least 22GB VRAM is recommended for efficient inference
    [2].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Google Colab users: Navigate to ‘Runtime’ > ‘Change runtime type’ > ‘A100
    GPU’. Note that high-end GPU’s may not be available for free Colab users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2\. Model Permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: Request Access to Llama 3.2 Models [here](https://www.llama.com/llama-downloads/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3\. Hugging Face Setup:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a Hugging Face account if you don’t have on already [here](https://huggingface.co/join).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate an access token from your Hugging Face account if you don’t have one,
    [here](https://huggingface.co/settings/tokens).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Google Colab users, set up the Hugging Face token as a secret environmental
    variable named ‘HF_TOKEN’ in google Colab Secrets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4\. Install the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Loading The** **Model**'
  prefs: []
  type: TYPE_NORMAL
- en: Once we’ve set up the environment and acquired the necessary permissions, we
    will use the Hugging Face Transformers library to instantiate the model and its
    associated processor. The processor is responsible for preparing inputs for the
    model and formatting its outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Expected Chat Template**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chat templates maintain context through conversation history by storing exchanges
    between the “user” (us) and the “assistant” (the AI model). The conversation history
    is structured as a list of dictionaries called `messages`, where each dictionary
    represents a single conversational turn, including both user and model responses.
    User turns can include image-text or text-only inputs, with `{"type": "image"}`
    indicating an image input.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, after a few chat iterations, the `messages` list might look like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This list of messages is later passed to the `apply_chat_template()` method
    to convert the conversation into a single tokenizable string in the format that
    the model expects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Main function**'
  prefs: []
  type: TYPE_NORMAL
- en: For this tutorial I provided a `chat_with_mllm` function that enables dynamic
    conversation with the Llama 3.2 MLLM. This function handles image loading, pre-processes
    both images and the text inputs, generates model responses, and manages the conversation
    history to enable chat-mode interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Chat with Llama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Butterfly Image Example**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our our first example, we’ll chat with Llama 3.2 about an image of a hatching
    butterfly. Since Llama 3.2-Vision does not support prompting with system prompts
    when using images, we will append instructions directly to the user prompt to
    guide the model’s responses. By setting `do_sample=True` and `temperature=0.2`
    , we enable slight randomness while maintaining response coherence. For fixed
    answer, you can set `do_sample==False` . The `messages` parameter, which holds
    the chat history, is initially empty, as in the `images` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/547d3652bb89e47a8039ddfee8ff7c8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Pixabay](https://www.pexels.com/photo/brown-and-white-swallowtail-butterfly-under-white-green-and-brown-cocoon-in-shallow-focus-lens-63643/).
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the output is accurate and concise, demonstrating that the model
    effectively understood the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next chat iteration, we’ll pass a new prompt along with the chat history
    (`messages`) and the image file (`images`). The new prompt is designed to assess
    the reasoning ability of Llama 3.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We continued this chat in the provided Colab notebook and obtained the following
    conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a72d924ad8dd280b870dadf9ab7c7224.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The conversation highlights the model’s image understanding ability by accurately
    describing the scene. It also demonstrates its reasoning skills by logically connecting
    information to correctly conclude what will happen to the chrysalis and explaining
    why some are brown while others are green.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Meme Image Example**'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, I will show the model a meme I created myself, to assess Llama’s
    OCR capabilities and determine whether it understands my sense of humor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the input meme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e196e4c1075cc3f596b4cfa4160b6658.png)'
  prefs: []
  type: TYPE_IMG
- en: Meme by author. Original bear image by [Hans-Jurgen Mager](https://unsplash.com/photos/polar-bear-on-snow-covered-ground-during-daytime-C9Ay328wHgA).
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is the model’s response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92233ad5cc21df183820c16f4e063aae.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the model demonstrates great OCR abilities, and understands the
    meaning of the text in the image. As for its sense of humor — what do you think,
    did it get it? Did you get it? Maybe I should work on my sense of humor too!
  prefs: []
  type: TYPE_NORMAL
- en: Concluding Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we learned how to build the Llama 3.2-Vision model locally
    and manage conversation history for chat-like interactions, enhancing user engagement.
    We explored Llama 3.2’s zero-shot abilities and were impressed by its scene understanding,
    reasoning and OCR skills.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced techniques can be applied to Llama 3.2, such as fine-tuning on unique
    data, or using retrieval-augmented generation (RAG) to ground predictions and
    reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this tutorial provides insight into the rapidly evolving field of Multimodal
    LLMs and their powerful capabilities for various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations on making it all the way here. Click 👍x50 to show your appreciation
    and raise the algorithm self esteem 🤓
  prefs: []
  type: TYPE_NORMAL
- en: '**Want to learn more?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full Code as Colab notebook:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/0e87500813c29bb4c4a6a990795c3aaa)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Llama 3.2 11B Vision Requirements](https://llamaimodel.com/requirements-3-2/)'
  prefs: []
  type: TYPE_NORMAL
