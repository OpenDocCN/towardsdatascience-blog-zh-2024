- en: Linear Discriminant Analysis (LDA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12](https://towardsdatascience.com/linear-discriminant-analysis-lda-598d8e90f8b9?source=collection_archive---------3-----------------------#2024-10-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover how LDA helps identify critical data features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[![Ingo
    Nowitzky](../Images/00d3560055109732b871c001d2b51ab5.png)](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    [Ingo Nowitzky](https://medium.com/@ingo.nowitzky?source=post_page---byline--598d8e90f8b9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--598d8e90f8b9--------------------------------)
    ·12 min read·Oct 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a54046c9d6695be140657cea1a5f7f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification of LDA within AI and ML Methods | image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**This article aims to explore Linear Discriminant Analysis (LDA), focusing
    on its core ideas, its mathematical implementation in code, and a practical example
    from manufacturing.'
  prefs: []
  type: TYPE_NORMAL
- en: I hope you’re on board. Let’s get started!**
  prefs: []
  type: TYPE_NORMAL
- en: 'Who works with **industrial data** in practice will be familiar with this situation:
    The datasets usually have many features, and it is often unclear which of the
    features are important and which are less. “Important” is a relative term in this
    context. Often, the goal is to differentiate the datasets from each other, i.e.,
    to classify them. A very typical task is to distinguish good parts from bad parts
    and to identify the causes (aka features) that lead to the failure of the parts.'
  prefs: []
  type: TYPE_NORMAL
- en: A commonly used method is the well-known Principal Component Analysis (PCA).
    While PCA belongs to the **un**supervised methods, the less widespread LDA is
    a supervised method and thus learns from **labeled data**. Therefore, it is particularly
    suited for explaining failure patterns from large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Goal and Principle of LDA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of LDA is to linearly combine the features of the data so that the
    labels of the datasets are best separated from each other, and the number of new
    features is reduced to a predefined count. In AI jargon, this is typically referred
    to as a **projection to a lower-dimensional space**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd8246e1f49c04fee04de426915216e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Principle of LDA | image modified from [Raschka/Mirjalili, 2019](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/images/05_06.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Excursus: What is dimensionality and what is dimensionality reduction?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0445b3df24a1a26401d95b2d0687883c.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensions and graphical representation | image by author
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality refers to the number of features in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'With just one measurement (or feature), such as the tool temperature from an
    injection molding machine, we can represent it on a number line. Two features,
    like temperature and tool pressure, are still manageable: we can easily plot the
    data on an x-y chart. With three features — temperature, tool pressure, and injection
    pressure — things get more interesting, but we can still plot the data in a 3D
    x-y-z chart. However, when we add more features, such as viscosity, electrical
    conductivity, and others, the complexity increases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08dee7410ef3334e826510e464e4477e.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensionality reduction | image by author
  prefs: []
  type: TYPE_NORMAL
- en: In practice, datasets often contain hundreds or even thousands of features.
    This presents a challenge because many machine learning algorithms perform poorly
    as datasets grow too large. Additionally, the amount of data required increases
    exponentially with the number of dimensions to achieve statistical significance.
    This phenomenon is known as the “curse of dimensionality.” These factors make
    it essential to determine which features are relevant and to eliminate the less
    meaningful ones early in the data science process.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How does LDA work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of Linear Discriminant Analysis (LDA) can be broken down into five
    key steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1:** Compute the *d*-dimensional mean vectors for each of the *k* classes
    separately from the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that LDA is a supervised machine learning technique, meaning we can
    utilize the known labels. In the first step, we calculate the mean vectors *mean_c*
    for all samples belonging to a specific class *c*. To do this, we filter the feature
    matrix by class label and compute the mean for each of the *d* features. As a
    result, we obtain *k* mean vectors (one for each of the *k* classes), each with
    a length of *d* (corresponding to the *d* features).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5ac7e6ee3fbe5483857a12a459e805e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Label vector Y and feature matrix X | image by author
  prefs: []
  type: TYPE_NORMAL
- en: Mean vector for class c
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2:** Compute the scatter matrices (between-class scatter matrix and
    within-class scatter matrix).'
  prefs: []
  type: TYPE_NORMAL
- en: The within-class scatter matrix measures the variation among samples within
    the same class. To find a subspace with optimal separability, we aim to minimize
    the values in this matrix. In contrast, the between-class scatter matrix measures
    the variation between different classes. For optimal separability, we aim to maximize
    the values in this matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, within-class scatter looks at how compact each class is, whereas
    between-class scatter examines how far apart different classes are.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb10ec259899c64b81ce85f1100ba03a.png)'
  prefs: []
  type: TYPE_IMG
- en: Within-class and between-class scatter matrices | image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the **within-class scatter** matrix *S_W*. It is calculated
    as the sum of the scatter matrices *S_c* for each individual class:'
  prefs: []
  type: TYPE_NORMAL
- en: Within-class scatter matrix S_W
  prefs: []
  type: TYPE_NORMAL
- en: 'The **between-class scatter** matrix *S_B* is derived from the differences
    between the class means *mean_c* and the overall mean of the entire dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Between-class scatter matrix S_B
  prefs: []
  type: TYPE_NORMAL
- en: where *mean* refers to the mean vector calculated over all samples, regardless
    of their class labels.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:** Calculate the eigenvectors and eigenvalues for the ratio of *S_W*​
    and *S_B*​.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, for optimal class separability, we aim to maximize *S_B​* and
    minimize *S_W*​. We can achieve both by maximizing the ratio *S_B/S_W*​. In linear
    algebra terms, this ratio corresponds to the scatter matrix *S_W⁻¹ S_B*​, which
    is maximized in the subspace spanned by the eigenvectors with the highest eigenvalues.
    The eigenvectors define the directions of this subspace, while the eigenvalues
    represent the magnitude of the distortion. We will select the *m* eigenvectors
    associated with the highest eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: Calculation formula for eigenvectors and eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43e9d50886343e62245ff02b8f47d566.png)'
  prefs: []
  type: TYPE_IMG
- en: Subspace spanned by eigenvectors | image by author
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 4:** Sort the eigenvectors in descending order of their corresponding
    eigenvalues, and select the *m* eigenvectors with the largest eigenvalues to form
    a *d × m-*dimensional transformation matrix *W*.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, our goal is not only to project the data into a subspace that enhances
    class separability but also to reduce dimensionality. The eigenvectors will define
    the axes of our new feature subspace. To decide which eigenvectors to discard
    for the lower-dimensional subspace, we need to examine their corresponding eigenvalues.
    In simple terms, the eigenvectors with the smallest eigenvalues contribute the
    least to class separation, and these are the ones we want to drop. The typical
    approach is to rank the eigenvalues in descending order and select the top *m*
    eigenvectors. *m* is a freely chosen parameter. The larger *m*, the less information
    is lost during the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'After sorting the eigenpairs by decreasing eigenvalues and selecting the top
    *m* pairs, the next step is to construct the *d × m*-dimensional transformation
    matrix *W*. This is done by stacking the *m* selected eigenvectors horizontally,
    resulting in the matrix *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformation matrix W
  prefs: []
  type: TYPE_NORMAL
- en: The first column of *W* represents the eigenvector corresponding to the highest
    eigenvalue, the second column represents the eigenvector corresponding to the
    second highest eigenvalue, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 5:** Use *W* to project the samples onto the new subspace.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final step, we use the *d × m-*dimensional transformation matrix *W*,
    which we composed from the top *m* selected eigenvectors, to project our samples
    onto the new subspace:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformed feature matrix Z
  prefs: []
  type: TYPE_NORMAL
- en: 'where *X* is the initial *n × d*-dimensional feature matrix representing our
    samples, and *Z* is the newly transformed *n × m*-dimensional feature matrix in
    the new subspace. This means that the selected eigenvectors serve as the “recipes”
    for transforming the original features into the new features (the **Linear Discriminants**):
    The eigenvector with the highest eigenvalue provides the transformation recipe
    for **LD1**, the eigenvector with the second highest eigenvalue corresponds to
    **LD2**, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ab651ba543314aeae2afbdab253c687.png)'
  prefs: []
  type: TYPE_IMG
- en: Projection of X onto the linear discriminants LD
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementing Linear Discriminant Analysis (LDA) from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate the theory and mathematics in action, we will program our own
    LDA from scratch using only numpy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Applying LDA to an Industrial Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see LDA in action, we will apply it to a typical task in the production environment.
    We have data from a simple manufacturing line with only 7 stations. Each of these
    stations sends a data point (yes, I know, only one data point is highly unrealistic).
    Unfortunately, our line produces a significant number of defective parts, and
    we want to find out which stations are responsible for this.
  prefs: []
  type: TYPE_NORMAL
- en: First, we load the data and take an initial look.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c8c63823ac648330d6223f37f1ecc5be.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we study the distribution of the data using the `.describe()` method from
    Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3e772f56c494366b54ef78ece6386e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We see that we have 20,000 data points, and the measurements range from -5
    to +150\. Hence, we note for later that we need to normalize the dataset: the
    different magnitudes of the numerical values would otherwise negatively affect
    the LDA.'
  prefs: []
  type: TYPE_NORMAL
- en: How many good parts and how many bad parts do we have?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ea0d6a2c9c981587a36f6beeb5ca4898.png)'
  prefs: []
  type: TYPE_IMG
- en: We have 19,031 good parts and 969 defective parts. The fact that the dataset
    is so imbalanced is an issue for further analysis. Therefore, we select all defective
    parts and an equal number of randomly chosen good parts for the further processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/872c6b4b1dd3882783e57eb19d3c52a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s apply our LDA from scratch to the balanced dataset. We use the `StandardScaler`
    from `sklearn` to normalize the measurements for each feature to have a mean of
    0 and a standard deviation of 1\. We choose only one linear discriminant axis
    (*m=1*) onto which we project the data. This helps us clearly see which features
    are most relevant in distinguishing good from bad parts, and we visualize the
    projected data in a histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bc19ff379454908643a58e3ad24e81a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature matrix projected to one LD (m=1)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89ff843406588e3be91c2f2fb0098c46.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature importance = How much do the stations contribute to class separation?
  prefs: []
  type: TYPE_NORMAL
- en: The histogram shows that we can separate the good parts from the defective parts
    very well, with only a small overlap. This is already a positive result and indicates
    that our LDA was successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'The “LDA Coefficients” from the table “Feature Contributions to LDA Components”
    represent the eigenvector from the first (and only, since *m=1*) column of our
    transformation matrix *W*. They indicate the direction and magnitude with which
    the normalized measurements from the stations are projected onto the linear discriminant
    axis. The values in the table are sorted in descending order. We need to read
    the table from both the top and the bottom simultaneously because the absolute
    value of the coefficient indicates the significance of each station in separating
    the classes and, consequently, its contribution to the production of defective
    parts. The sign indicates whether a lower or higher measurement increases the
    likelihood of defective parts. Let’s take a closer look at our example:'
  prefs: []
  type: TYPE_NORMAL
- en: The largest absolute value is from Station 4, with a coefficient of -0.672\.
    This means that Station 4 has the strongest influence on part failure. Due to
    the negative sign, higher positive measurements are projected towards a negative
    linear discriminant (LD). The histogram shows that a negative LD is associated
    with good (green) parts. Conversely, **low and negative measurements at this station
    increase the likelihood of part failure**.
  prefs: []
  type: TYPE_NORMAL
- en: The second highest absolute value is from Station 2, with a coefficient of 0.557\.
    Therefore, this station is the second most significant contributor to part failures.
    The positive sign indicates that high positive measurements are projected towards
    the positive LD. From the histogram, we know that a high positive LD value is
    associated with a high likelihood of failure. In other words, **high measurements
    at Station 2 lead to part failures**.
  prefs: []
  type: TYPE_NORMAL
- en: The third highest coefficient comes from Station 7, with a value of -0.486\.
    This makes Station 7 the third largest contributor to part failures. The negative
    sign again indicates that high positive values at this station lead to a negative
    LD (which corresponds to good parts). Conversely, **low and negative values at
    this station lead to part failures**.
  prefs: []
  type: TYPE_NORMAL
- en: All other LDA coefficients are an order of magnitude smaller than the three
    mentioned, **the associated stations therefore have no influence on part failure**.
  prefs: []
  type: TYPE_NORMAL
- en: Are the results of our LDA analysis correct? As you may have already guessed,
    the production dataset is synthetically generated. I labeled all parts as defective
    where the measurement at Station 2 was greater than 0.5, the value at Station
    4 was less than -2.5, and the value at Station 7 was less than 3\. **It turns
    out that the LDA hit the mark perfectly!**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis (LDA) not only reduces the complexity of datasets
    but also highlights the key features that drive class separation, making it highly
    effective for identifying failure causes in production systems. It is a straightforward
    yet powerful method with practical applications and is readily available in libraries
    like `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve optimal results, it is crucial to balance the dataset (ensure a similar
    number of samples in each class) and normalize it (mean of 0 and standard deviation
    of 1).
  prefs: []
  type: TYPE_NORMAL
- en: '**The next time you work with a large dataset containing class labels and numerous
    features, why not give LDA a try?**'
  prefs: []
  type: TYPE_NORMAL
