<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Human Pose Tracking with MediaPipe in 2D and 3D: Rerun Showcase</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Human Pose Tracking with MediaPipe in 2D and 3D: Rerun Showcase</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=collection_archive---------1-----------------------#2024-03-11">https://towardsdatascience.com/human-pose-tracking-with-mediapipe-rerun-showcase-125053cfe64f?source=collection_archive---------1-----------------------#2024-03-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4534" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How to easily visualise MediaPipe’s human pose tracking with Rerun</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://andreasnaoum.medium.com/?source=post_page---byline--125053cfe64f--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Andreas Naoum" class="l ep by dd de cx" src="../Images/e14d545f270170877e0af31572275e17.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*r7Wj9h7zMnu0bE3G9ucXpQ.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--125053cfe64f--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://andreasnaoum.medium.com/?source=post_page---byline--125053cfe64f--------------------------------" rel="noopener follow">Andreas Naoum</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--125053cfe64f--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/f8588c83a6d99bada222874b26a1f256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNCowb__raZvv9NeIc2oOQ.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Human Pose Tracking | Image by Author</figcaption></figure><h1 id="83b6" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Overview</h1><p id="c8df" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We explore a use case that leverages the power of <a class="af ou" href="https://developers.google.com/mediapipe" rel="noopener ugc nofollow" target="_blank">MediaPipe</a> for tracking human poses in both 2D and 3D. What makes this exploration even more fascinating is the visualisation aspect powered by the open-source visualisation tool <a class="af ou" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank">Rerun</a>, which provides a holistic view of human poses in action.</p><p id="c2aa" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">In this blog post, you’ll be guided to use MediaPipe to track human poses in 2D and 3D, and explore the visualisation capabilities of Rerun.</p><h1 id="dfca" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Human Pose Tracking</h1><p id="90a8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Human pose tracking is a task in computer vision that focuses on identifying key body locations, analysing posture, and categorising movements. At the heart of this technology is a pre-trained machine-learning model to assess the visual input and recognise landmarks on the body in both image coordinates and 3D world coordinates. The use cases and applications of this technology include but are not limited to Human-Computer Interaction, Sports Analysis, Gaming, Virtual Reality, Augmented Reality, Health, etc.</p><p id="87dc" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">It would be good to have a perfect model, but unfortunately, the current models are still imperfect. Although datasets could have a variety of body types, the human body differs among individuals. The uniqueness of each individual’s body poses a challenge, particularly for those with non-standard arm and leg dimensions, which may result in lower accuracy when using this technology. When considering the integration of this technology into systems, it is crucial to acknowledge the possibility of inaccuracies. Hopefully, ongoing efforts within the scientific community will pave the way for the development of more robust models.</p><p id="e4d8" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Beyond lack of accuracy, ethical and legal considerations emerge from utilising this technology. For instance, capturing human body poses in public spaces could potentially invade privacy rights if individuals have not given their consent. It’s crucial to take into account any ethical and legal concerns before implementing this technology in real-world scenarios.</p><h1 id="7272" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Prerequisites &amp; Setup</h1><p id="5d67" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Begin by installing the required libraries:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="332a" class="pe nd fq pb b bg pf pg l ph pi"># Install the required Python packages <br/>pip install mediapipe<br/>pip install numpy<br/>pip install opencv-python&lt;4.6<br/>pip install requests&gt;=2.31,&lt;3<br/>pip install rerun-sdk<br/><br/># or just use the requirements file<br/>pip install -r examples/python/human_pose_tracking/requirements.txt</span></pre><h1 id="f2ec" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Track Human Pose using MediaPipe</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pj"><img src="../Images/d027aae91cc4189161358f87f836a4f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ruim9pk0ZwNuBJYnUnmFaQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image via <a class="af ou" href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker" rel="noopener ugc nofollow" target="_blank">Pose Landmark Detection Guide</a> by <a class="af ou" href="https://about.google/brand-resource-center/" rel="noopener ugc nofollow" target="_blank">Google</a> [1]</figcaption></figure><p id="aa3f" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk"><a class="af ou" href="https://mediapipe-studio.webapps.google.com/home" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr">MediaPipe Python</strong></a> is a handy tool for developers looking to integrate on-device ML solutions for computer vision and machine learning.</p><p id="1dfe" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">In the code below, <a class="af ou" href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker" rel="noopener ugc nofollow" target="_blank">MediaPipe pose landmark detection</a> was utilised for detecting landmarks of human bodies in an image. This model can detect body pose landmarks as both image coordinates and 3D world coordinates. Once you have successfully run the ML model, you can use the image coordinates and the 3D world coordinates to visualise the output.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="4562" class="pe nd fq pb b bg pf pg l ph pi">import mediapipe as mp<br/>import numpy as np<br/>from typing import Any<br/>import numpy.typing as npt<br/>import cv2<br/><br/><br/>"""<br/>    Read 2D landmark positions from Mediapipe Pose results.<br/><br/>    Args:<br/>        results (Any): Mediapipe Pose results.<br/>        image_width (int): Width of the input image.<br/>        image_height (int): Height of the input image.<br/><br/>    Returns:<br/>        np.array | None: Array of 2D landmark positions or None if no landmarks are detected.<br/>"""<br/>def read_landmark_positions_2d(<br/>        results: Any,<br/>        image_width: int,<br/>        image_height: int,<br/>) -&gt; npt.NDArray[np.float32] | None:<br/>    if results.pose_landmarks is None:<br/>        return None<br/>    else:<br/>        # Extract normalized landmark positions and scale them to image dimensions<br/>        normalized_landmarks = [results.pose_landmarks.landmark[lm] for lm in mp.solutions.pose.PoseLandmark]<br/>        return np.array([(image_width * lm.x, image_height * lm.y) for lm in normalized_landmarks])<br/><br/><br/>"""<br/>    Read 3D landmark positions from Mediapipe Pose results.<br/><br/>    Args:<br/>        results (Any): Mediapipe Pose results.<br/><br/>    Returns:<br/>        np.array | None: Array of 3D landmark positions or None if no landmarks are detected.<br/>"""<br/>def read_landmark_positions_3d(<br/>        results: Any,<br/>) -&gt; npt.NDArray[np.float32] | None:<br/>    if results.pose_landmarks is None:<br/>        return None<br/>    else:<br/>        # Extract 3D landmark positions<br/>        landmarks = [results.pose_world_landmarks.landmark[lm] for lm in mp.solutions.pose.PoseLandmark]<br/>        return np.array([(lm.x, lm.y, lm.z) for lm in landmarks])<br/><br/><br/>"""<br/>    Track and analyze pose from an input image.<br/><br/>    Args:<br/>        image_path (str): Path to the input image.<br/>"""<br/>def track_pose(image_path: str) -&gt; None:<br/>    # Read the image, convert color to RGB<br/>    image = cv2.imread(image_path)<br/>    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)<br/><br/>    # Create a Pose model instance<br/>    pose_detector = mp.solutions.pose.Pose(static_image_mode=True)<br/><br/>    # Process the image to obtain pose landmarks<br/>    results = pose_detector.process(image)<br/>    h, w, _ = image.shape<br/><br/>    # Read 2D and 3D landmark positions<br/>    landmark_positions_2d = read_landmark_positions_2d(results, w, h)<br/>    landmark_positions_3d = read_landmark_positions_3d(results)</span></pre><h1 id="2ef2" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Visualise the output of MediaPipe using Rerun</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pk"><img src="../Images/0ffbd838ee50a8f7b64114417b1e978b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLTeddhSrEeqoeQSaBEbqA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Rerun Viewer | Image via <a class="af ou" href="https://www.rerun.io/docs/reference/viewer/overview" rel="noopener ugc nofollow" target="_blank">Rerun Docs</a> [2]</figcaption></figure><p id="f727" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk"><a class="af ou" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank">Rerun</a> serves as a visualisation tool for multi-modal data. Through the <a class="af ou" href="https://www.rerun.io/docs/reference/viewer/overview" rel="noopener ugc nofollow" target="_blank">Rerun Viewer</a>, you can build layouts, customise visualisations and interact with your data. The rest part of this section details how you can log and present data using the Rerun SDK to visualise it within the Rerun Viewer</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pl"><img src="../Images/bba414c2f083ce237872945da1c80150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*44fk0Hlvq7-b7SQksf6P1A.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Pose Landmarker Model | Image via <a class="af ou" href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker" rel="noopener ugc nofollow" target="_blank">Pose Landmark Detection Guide</a> by <a class="af ou" href="https://about.google/brand-resource-center/" rel="noopener ugc nofollow" target="_blank">Google</a> [1]</figcaption></figure><p id="b308" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">In both 2D and 3D points, specifying connections between points is essential. Defining these connections automatically renders lines between them. Using the information provided by MediaPipe, you can get the pose points connections from the <code class="cx pm pn po pb b">POSE_CONNECTIONS</code> set and then set them as keypoint connections using <a class="af ou" href="https://www.rerun.io/docs/concepts/annotation-context" rel="noopener ugc nofollow" target="_blank">Annotation Context</a>.</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="f88a" class="pe nd fq pb b bg pf pg l ph pi">rr.log(<br/>        "/",<br/>        rr.AnnotationContext(<br/>            rr.ClassDescription(<br/>                info=rr.AnnotationInfo(id=0, label="Person"),<br/>                keypoint_annotations=[rr.AnnotationInfo(id=lm.value, label=lm.name) for lm in mp_pose.PoseLandmark],<br/>                keypoint_connections=mp_pose.POSE_CONNECTIONS,<br/>            )<br/>        ),<br/>        timeless=True,<br/>    )</span></pre><h2 id="07be" class="pp nd fq bf ne pq pr ps nh pt pu pv nk oh pw px py ol pz qa qb op qc qd qe qf bk">Image Coordinates — 2D Positions</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/20e233978eee38de893191d9b35b30f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrA_fG-LmqXMeA26Mwe2cA.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualising Human Pose as 2D Points | Image by Author</figcaption></figure><p id="3a2b" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Visualising the body pose landmarks on the video appears to be a good choice. To achieve that, you need to follow the rerun documentation for Entities and Components. <a class="af ou" href="https://www.rerun.io/docs/concepts/entity-path" rel="noopener ugc nofollow" target="_blank">The Entity Path Hierarchy</a> page describes how to log multiple Components on the same Entity. For example, you can create the ‘video’ entity and include the components ‘video/rgb’ for the video and ‘video/pose’ for the body pose. If you’re aiming to use that for a video, you need the concept of <a class="af ou" href="https://www.rerun.io/docs/concepts/timelines" rel="noopener ugc nofollow" target="_blank">Timelines</a>. Each frame can be associated with the appropriate data.</p><p id="a3d0" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Here is a function that can visualise the 2D points on the video:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="0e91" class="pe nd fq pb b bg pf pg l ph pi">def track_pose_2d(video_path: str) -&gt; None:<br/>    mp_pose = mp.solutions.pose  <br/>  <br/>    with closing(VideoSource(video_path)) as video_source, mp_pose.Pose() as pose:<br/>        for idx, bgr_frame in enumerate(video_source.stream_bgr()):<br/>            if max_frame_count is not None and idx &gt;= max_frame_count:<br/>                break<br/><br/>            rgb = cv2.cvtColor(bgr_frame.data, cv2.COLOR_BGR2RGB)<br/><br/>            # Associate frame with the data<br/>            rr.set_time_seconds("time", bgr_frame.time)<br/>            rr.set_time_sequence("frame_idx", bgr_frame.idx)<br/><br/>            # Present the video<br/>            rr.log("video/rgb", rr.Image(rgb).compress(jpeg_quality=75))<br/><br/>            # Get the prediction results<br/>            results = pose.process(rgb)<br/>            h, w, _ = rgb.shape<br/><br/>            # Log 2d points to 'video' entity<br/>            landmark_positions_2d = read_landmark_positions_2d(results, w, h)<br/>            if landmark_positions_2d is not None:<br/>                rr.log(<br/>                    "video/pose/points",<br/>                    rr.Points2D(landmark_positions_2d, class_ids=0, keypoint_ids=mp_pose.PoseLandmark),<br/>                )</span></pre><h2 id="97e4" class="pp nd fq bf ne pq pr ps nh pt pu pv nk oh pw px py ol pz qa qb op qc qd qe qf bk">3D World Coordinates — 3D Points</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/fc0f27b3622dd6cffc6cbae4a70cb5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nD0SFxuZoi2IZMAbp4YVpw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualising Human Pose as 3D Points | Image by Author</figcaption></figure><p id="0bdd" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Why settle on 2D points when you have 3D Points? Create a new entity, name it “Person”, and log the 3D points. It’s done! You just created a 3D presentation of the human body pose.</p><p id="491f" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">Here is how to do it:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="a6a8" class="pe nd fq pb b bg pf pg l ph pi">def track_pose_3d(video_path: str, *, segment: bool, max_frame_count: int | None) -&gt; None:<br/>    mp_pose = mp.solutions.pose  <br/><br/>    rr.log("person", rr.ViewCoordinates.RIGHT_HAND_Y_DOWN, timeless=True)<br/>  <br/>    with closing(VideoSource(video_path)) as video_source, mp_pose.Pose() as pose:<br/>        for idx, bgr_frame in enumerate(video_source.stream_bgr()):<br/>            if max_frame_count is not None and idx &gt;= max_frame_count:<br/>                break<br/><br/>            rgb = cv2.cvtColor(bgr_frame.data, cv2.COLOR_BGR2RGB)<br/><br/>            # Associate frame with the data<br/>            rr.set_time_seconds("time", bgr_frame.time)<br/>            rr.set_time_sequence("frame_idx", bgr_frame.idx)<br/><br/>            # Present the video<br/>            rr.log("video/rgb", rr.Image(rgb).compress(jpeg_quality=75))<br/><br/>            # Get the prediction results<br/>            results = pose.process(rgb)<br/>            h, w, _ = rgb.shape<br/><br/>            # New entity "Person" for the 3D presentation<br/>            landmark_positions_3d = read_landmark_positions_3d(results)<br/>            if landmark_positions_3d is not None:<br/>                rr.log(<br/>                    "person/pose/points",<br/>                    rr.Points3D(landmark_positions_3d, class_ids=0, keypoint_ids=mp_pose.PoseLandmark),<br/>                )</span></pre><h1 id="d092" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Source Code</h1><p id="f936" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The tutorial focuses on the main parts of the Human Pose Tracking example. For those who prefer a hands-on approach, the full source code for this example is available on <a class="af ou" href="https://github.com/rerun-io/rerun/tree/latest/examples/python/human_pose_tracking" rel="noopener ugc nofollow" target="_blank">GitHub</a>. Feel free to explore, modify, and understand the inner workings of the implementation.</p><h1 id="5524" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Tips &amp; Suggestions</h1><h2 id="110a" class="pp nd fq bf ne pq pr ps nh pt pu pv nk oh pw px py ol pz qa qb op qc qd qe qf bk">1. Compress the image for efficiency</h2><p id="49bf" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">You can boost the overall procedure speed by compressing the logged images:</p><pre class="mm mn mo mp mq pa pb pc bp pd bb bk"><span id="0c9e" class="pe nd fq pb b bg pf pg l ph pi">rr.log(<br/>  "video", <br/>  rr.Image(img).compress(jpeg_quality=75)<br/>)</span></pre><h2 id="63ae" class="pp nd fq bf ne pq pr ps nh pt pu pv nk oh pw px py ol pz qa qb op qc qd qe qf bk">2. Limit Memory Use</h2><p id="9905" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">If you’re logging more data than can be fitted into your RAM, it will start dropping the old data. The default limit is 75% of your system RAM. If you want to increase that you could use the command line argument — memory-limit. More information about memory limits can be found on Rerun’s <a class="af ou" href="https://www.rerun.io/docs/howto/limit-ram" rel="noopener ugc nofollow" target="_blank">How To Limit Memory Use</a> page.</p><h2 id="c312" class="pp nd fq bf ne pq pr ps nh pt pu pv nk oh pw px py ol pz qa qb op qc qd qe qf bk">3. Customise Visualisations for your needs</h2><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/98f7548f01e6161352b1194f993aaf09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duw1MBryxcQIvtdNoh4cpg.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Customise Rerun Viewer | Image by Author</figcaption></figure><h1 id="3bc3" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Beyond Human Pose Tracking</h1><p id="4b42" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk"><em class="qh">If you found this article useful and insightful, there’s more!</em></p><p id="5024" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk"><em class="qh">Similar articles:</em></p><div class="qi qj qk ql qm qn"><a href="https://ai.gopubby.com/real-time-face-and-face-landmark-detection-with-mediapipe-rerun-showcase-40481baa1763?source=post_page-----125053cfe64f--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qo ab ig"><div class="qp ab co cb qq qr"><h2 class="bf fr hw z io qs iq ir qt it iv fp bk">Real-Time Face and Face Landmark Detection with MediaPipe: Rerun Showcase</h2><div class="qu l"><h3 class="bf b hw z io qs iq ir qt it iv dx">How to easily visualise MediaPipe’s face and face landmark detection in 2D and 3D with Rerun</h3></div><div class="qv l"><p class="bf b dy z io qs iq ir qt it iv dx">ai.gopubby.com</p></div></div><div class="qw l"><div class="qx l qy qz ra qw rb lr qn"/></div></div></a></div><div class="qi qj qk ql qm qn"><a rel="noopener follow" target="_blank" href="/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831?source=post_page-----125053cfe64f--------------------------------"><div class="qo ab ig"><div class="qp ab co cb qq qr"><h2 class="bf fr hw z io qs iq ir qt it iv fp bk">Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase</h2><div class="qu l"><h3 class="bf b hw z io qs iq ir qt it iv dx">How to visualise MediaPipe’s Hand Tracking and Gesture Recognition with Rerun</h3></div><div class="qv l"><p class="bf b dy z io qs iq ir qt it iv dx">towardsdatascience.com</p></div></div><div class="qw l"><div class="rc l qy qz ra qw rb lr qn"/></div></div></a></div><div class="qi qj qk ql qm"><div role="button" tabindex="0" class="ab bx cp ke io rd re bp rf lr ao"><div class="rg l"><div class="ab q"><div class="l ed"><img alt="Andreas Naoum" class="l ep by rh ri cx" src="../Images/5af57533919b28694a1f6a0caf923221.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*r7Wj9h7zMnu0bE3G9ucXpQ.png"/><div class="el by l rh ri em n ay tu"/></div><div class="rj l ig"><p class="bf b dy z io ip iq ir is it iu iv dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://andreasnaoum.medium.com/?source=post_page-----125053cfe64f--------------------------------" rel="noopener follow" target="_top">Andreas Naoum</a></p></div></div><div class="cq rm hk l"><h2 class="bf fr ws hx io wt iq ir qt it iv fp bk">Multimodal Data Visualizations</h2></div><div class="ab q"><div class="l ig"><a class="bf b dy z bk wu vu vv vw vx le vy vz uf id wa wb wc uj uk ul ep bm um my" href="https://andreasnaoum.medium.com/list/multimodal-data-visualizations-48083691fa4b?source=post_page-----125053cfe64f--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wv l ig"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="rv dz rw io ab rx ig ed"><div class="ed rp bx rq rr"><div class="dz l"><img alt="" class="dz" src="../Images/afd598e10c2627e4d16424dd5903e02f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*OMagxCtoVgx1pZzB9FDdlA.gif"/></div></div><div class="ed rp bx kf rs rt"><div class="dz l"><img alt="" class="dz" src="../Images/6ce441267a7e6f182643f87d802ee4ac.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*ojNcLB13lhHu0mjZisPwPw.gif"/></div></div><div class="ed bx hs ru rt"><div class="dz l"><img alt="" class="dz" src="../Images/431a1a3f42709b335dddbdfeecffb9ef.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*rNqwPWfDslScJ58OVOhP-Q.gif"/></div></div></div></div></div><p id="94c8" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk"><em class="qh">I regularly share tutorials on visualisation for computer vision and robotics. Follow me for future updates!</em></p><p id="9ff2" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk"><em class="qh">Also, you can find me on </em><a class="af ou" href="http://www.linkedin.com/in/andreas-naoum" rel="noopener ugc nofollow" target="_blank"><strong class="oa fr"><em class="qh">LinkedIn</em></strong></a><strong class="oa fr"><em class="qh">.</em></strong></p><h1 id="9c41" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Sources</h1><p id="7af4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[1] <a class="af ou" href="https://developers.google.com/mediapipe/solutions/vision/pose_landmarker" rel="noopener ugc nofollow" target="_blank">Pose Landmark Detection Guide</a> by <a class="af ou" href="https://about.google/brand-resource-center/" rel="noopener ugc nofollow" target="_blank">Google</a>, Portions of this page are reproduced from work created and <a class="af ou" href="https://developers.google.com/readme/policies" rel="noopener ugc nofollow" target="_blank">shared by Google</a> and used according to terms described in the <a class="af ou" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons 4.0 Attribution License</a>.</p><p id="8883" class="pw-post-body-paragraph ny nz fq oa b go ov oc od gr ow of og oh ox oj ok ol oy on oo op oz or os ot fj bk">[2] <a class="af ou" href="https://www.rerun.io/docs/reference/viewer/overview" rel="noopener ugc nofollow" target="_blank">Rerun Docs</a> by <a class="af ou" href="https://www.rerun.io" rel="noopener ugc nofollow" target="_blank">Rerun</a> under <a class="af ou" href="https://github.com/rerun-io/rerun/blob/main/LICENSE-MIT" rel="noopener ugc nofollow" target="_blank">MIT license</a></p></div></div></div></div>    
</body>
</html>