- en: 'Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œè§£é‡Šï¼šå¸¦æœ‰è¿·ä½ äºŒç»´æ•°æ®é›†çš„è§†è§‰æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25](https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25](https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25)
- en: CLASSIFICATION ALGORITHM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç®—æ³•
- en: Dissecting the math (with visuals) of a tiny neural network
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è§£å‰–ä¸€ä¸ªå°å‹ç¥ç»ç½‘ç»œçš„æ•°å­¦ï¼ˆå¸¦è§†è§‰å±•ç¤ºï¼‰
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    Â·13 min readÂ·Oct 25, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    Â·13åˆ†é’Ÿé˜…è¯»Â·2024å¹´10æœˆ25æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
- en: '`â›³ï¸ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: Â· [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    Â· [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    Â· [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    Â· [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    â–¶ [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '`â›³ï¸ æ›´å¤š [åˆ†ç±»ç®—æ³•](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)ï¼Œè§£é‡Šï¼šÂ·
    [è™šæ‹Ÿåˆ†ç±»å™¨](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    Â· [Kè¿‘é‚»åˆ†ç±»å™¨](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    Â· [é«˜æ–¯æœ´ç´ è´å¶æ–¯](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [å†³ç­–æ ‘åˆ†ç±»å™¨](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [é€»è¾‘å›å½’](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    Â· [æ”¯æŒå‘é‡åˆ†ç±»å™¨](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    â–¶ [å¤šå±‚æ„ŸçŸ¥å™¨](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
- en: Ever feel like neural networks are showing up everywhere? Theyâ€™re in the news,
    in your phone, even in your social media feed. But letâ€™s be honest â€” most of us
    have no clue how they actually work. All that fancy math and strange terms like
    â€œbackpropagationâ€?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å¦è§‰å¾—ç¥ç»ç½‘ç»œæ— å¤„ä¸åœ¨ï¼Ÿå®ƒä»¬å‡ºç°åœ¨æ–°é—»ä¸­ã€æ‰‹æœºé‡Œï¼Œç”šè‡³å‡ºç°åœ¨ä½ çš„ç¤¾äº¤åª’ä½“åŠ¨æ€ä¸­ã€‚ä½†è¯´å®è¯â€”â€”æˆ‘ä»¬å¤§å¤šæ•°äººæ ¹æœ¬ä¸æ¸…æ¥šå®ƒä»¬æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚é‚£äº›å¤æ‚çš„æ•°å­¦å’Œåƒâ€œåå‘ä¼ æ’­â€è¿™æ ·çš„æœ¯è¯­å‘¢ï¼Ÿ
- en: 'Hereâ€™s a thought: what if we made things super simple? Letâ€™s explore a Multilayer
    Perceptron (MLP) â€” **the most basic type of neural network** â€” to classify a simple
    2D dataset using a small network, working with just a handful of data points.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ€è€ƒï¼šå¦‚æœæˆ‘ä»¬å°†äº‹æƒ…ç®€åŒ–å‘¢ï¼Ÿè®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰â€”â€”**æœ€åŸºæœ¬ç±»å‹çš„ç¥ç»ç½‘ç»œ**â€”â€”æ¥ä½¿ç”¨ä¸€ä¸ªå°å‹ç½‘ç»œåˆ†ç±»ä¸€ä¸ªç®€å•çš„äºŒç»´æ•°æ®é›†ï¼Œåªä½¿ç”¨å°‘é‡çš„æ•°æ®ç‚¹ã€‚
- en: Through clear visuals and step-by-step explanations, youâ€™ll see the math come
    to life, watching exactly how numbers and equations flow through the network and
    how learning really happens!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¸…æ™°çš„è§†è§‰æ•ˆæœå’Œé€æ­¥çš„è§£é‡Šï¼Œä½ å°†çœ‹åˆ°æ•°å­¦å˜å¾—ç”ŸåŠ¨ï¼Œè§‚å¯Ÿæ•°å­—å’Œæ–¹ç¨‹å¼å¦‚ä½•åœ¨ç½‘ç»œä¸­æµåŠ¨ï¼Œä»¥åŠå­¦ä¹ æ˜¯å¦‚ä½•çœŸæ­£å‘ç”Ÿçš„ï¼
- en: '![](../Images/68c633860f446dbc91b04b1a420fcef7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68c633860f446dbc91b04b1a420fcef7.png)'
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è§†è§‰æ•ˆæœï¼šä½œè€…ä½¿ç”¨Canva Proåˆ›å»ºã€‚ä¼˜åŒ–ä¸ºæ‰‹æœºç«¯æ˜¾ç¤ºï¼›åœ¨æ¡Œé¢ç«¯å¯èƒ½æ˜¾ç¤ºè¿‡å¤§ã€‚
- en: Definition
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: A Multilayer Perceptron (MLP) is a type of neural network that uses layers of
    connected nodes to learn patterns. It gets its name from having multiple layers
    â€” typically an input layer, one or more middle (hidden) layers, and an output
    layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œç±»å‹ï¼Œå®ƒä½¿ç”¨è¿æ¥çš„èŠ‚ç‚¹å±‚æ¥å­¦ä¹ æ¨¡å¼ã€‚å®ƒä¹‹æ‰€ä»¥å¾—åï¼Œæ˜¯å› ä¸ºå®ƒæœ‰å¤šä¸ªå±‚â€”â€”é€šå¸¸åŒ…æ‹¬ä¸€ä¸ªè¾“å…¥å±‚ã€ä¸€ä¸ªæˆ–å¤šä¸ªä¸­é—´ï¼ˆéšè—ï¼‰å±‚å’Œä¸€ä¸ªè¾“å‡ºå±‚ã€‚
- en: Each node connects to all nodes in the next layer. When the network learns,
    it adjusts the strength of these connections based on training examples. For instance,
    if certain connections lead to correct predictions, they become stronger. If they
    lead to mistakes, they become weaker.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªèŠ‚ç‚¹éƒ½ä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰èŠ‚ç‚¹ç›¸è¿ã€‚å½“ç½‘ç»œå­¦ä¹ æ—¶ï¼Œå®ƒä¼šæ ¹æ®è®­ç»ƒç¤ºä¾‹è°ƒæ•´è¿™äº›è¿æ¥çš„å¼ºåº¦ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæŸäº›è¿æ¥å¯¼è‡´æ­£ç¡®çš„é¢„æµ‹ï¼Œå®ƒä»¬ä¼šå˜å¾—æ›´å¼ºã€‚å¦‚æœå®ƒä»¬å¯¼è‡´é”™è¯¯é¢„æµ‹ï¼Œå®ƒä»¬ä¼šå˜å¾—æ›´å¼±ã€‚
- en: This way of learning through examples helps the network recognize patterns and
    make predictions about new situations it hasnâ€™t seen before.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§ä¾‹å­å­¦ä¹ çš„æ–¹å¼ï¼Œå¸®åŠ©ç½‘ç»œè¯†åˆ«æ¨¡å¼ï¼Œå¹¶å¯¹å®ƒä»æœªè§è¿‡çš„æ–°æƒ…å†µåšå‡ºé¢„æµ‹ã€‚
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
- en: MLPs are considered fundamental in the field of neural networks and deep learning
    because they can handle complex problems that simpler methods struggle with.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰è¢«è®¤ä¸ºæ˜¯ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„åŸºç¡€ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¤„ç†ä¸€äº›ç®€å•æ–¹æ³•æ— æ³•è§£å†³çš„å¤æ‚é—®é¢˜ã€‚
- en: ğŸ“Š Dataset Used
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†
- en: 'To understand how MLPs work, letâ€™s start with a simple example: a mini 2D dataset
    with just a few samples. Weâ€™ll use [the same dataset](https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    from our previous article to keep things manageable.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPsï¼‰çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹ï¼šä¸€ä¸ªåªæœ‰å‡ ä¸ªæ ·æœ¬çš„è¿·ä½ äºŒç»´æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨[ç›¸åŒçš„æ•°æ®é›†](https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)æ¥ä¿æŒç®€æ´ï¼Œè¿™ä¸ªæ•°æ®é›†æ¥è‡ªæˆ‘ä»¬ä¹‹å‰çš„æ–‡ç« ã€‚
- en: '![](../Images/7d0cc55573cd44b994e5a8fcff182285.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d0cc55573cd44b994e5a8fcff182285.png)'
- en: 'Columns: Temperature (0â€“3), Humidity (0â€“3), Play Golf (Yes/No). The training
    dataset has 2 dimensions and 8 samples.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—ï¼šæ¸©åº¦ï¼ˆ0-3ï¼‰ã€æ¹¿åº¦ï¼ˆ0-3ï¼‰ã€æ‰“é«˜å°”å¤«ï¼ˆæ˜¯/å¦ï¼‰ã€‚è®­ç»ƒæ•°æ®é›†æœ‰2ä¸ªç»´åº¦å’Œ8ä¸ªæ ·æœ¬ã€‚
- en: Rather than jumping straight into training, letâ€™s try to understand the key
    pieces that make up a neural network and how they work together.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç›´æ¥è¿›å…¥è®­ç»ƒä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè¯•ç€ç†è§£æ„æˆç¥ç»ç½‘ç»œçš„å…³é”®éƒ¨åˆ†åŠå…¶å¦‚ä½•ååŒå·¥ä½œã€‚
- en: 'Step 0: Network Structure'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 0ï¼šç½‘ç»œç»“æ„
- en: 'First, letâ€™s look at the parts of our network:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ç½‘ç»œçš„å„ä¸ªéƒ¨åˆ†ï¼š
- en: Node (Neuron)
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰
- en: We begin with the basic structure of a neural network. This structure is composed
    of many individual units called nodes or neurons.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„å¼€å§‹ã€‚è¿™ä¸ªç»“æ„ç”±è®¸å¤šä¸ªå•ç‹¬çš„å•ä½ç»„æˆï¼Œç§°ä¸ºèŠ‚ç‚¹æˆ–ç¥ç»å…ƒã€‚
- en: '![](../Images/77265c27db24a49170b0d38542b64000.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77265c27db24a49170b0d38542b64000.png)'
- en: This neural network has 8 nodes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¥ç»ç½‘ç»œæœ‰8ä¸ªèŠ‚ç‚¹ã€‚
- en: 'These nodes are organized into groups called layers to work together:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›èŠ‚ç‚¹è¢«ç»„ç»‡æˆä¸€ä¸ªä¸ªå±‚æ¬¡ç»“æ„æ¥ååŒå·¥ä½œï¼š
- en: Input layer
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚
- en: The input layer is where we start. It takes in our raw data, and the number
    of nodes here matches how many features we have.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚æ˜¯æˆ‘ä»¬å¼€å§‹çš„åœ°æ–¹ã€‚å®ƒæ¥æ”¶æˆ‘ä»¬çš„åŸå§‹æ•°æ®ï¼ŒèŠ‚ç‚¹çš„æ•°é‡ä¸ç‰¹å¾çš„æ•°é‡ç›¸åŒ¹é…ã€‚
- en: '![](../Images/f6b53d8cb9333fa129a63e062bced4a6.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6b53d8cb9333fa129a63e062bced4a6.png)'
- en: The input layer has 2 nodes, one for each feature.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å±‚æœ‰2ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªç‰¹å¾å¯¹åº”ä¸€ä¸ªèŠ‚ç‚¹ã€‚
- en: Hidden layer
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšè—å±‚
- en: Next come the hidden layers. We can have one or more of these layers, and we
    can choose how many nodes each one has. Often, we use fewer nodes in each layer
    as we go deeper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯éšè—å±‚ã€‚æˆ‘ä»¬å¯ä»¥æœ‰ä¸€ä¸ªæˆ–å¤šä¸ªè¿™æ ·çš„å±‚ï¼Œä¸”å¯ä»¥é€‰æ‹©æ¯å±‚çš„èŠ‚ç‚¹æ•°é‡ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬åœ¨å±‚æ•°è¶Šæ·±æ—¶ï¼Œæ¯å±‚çš„èŠ‚ç‚¹æ•°é‡ä¼šè¶Šæ¥è¶Šå°‘ã€‚
- en: '![](../Images/889ce747a48b89418498002342df446c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/889ce747a48b89418498002342df446c.png)'
- en: This neural network has 2 hidden layers with 3 and 2 nodes, respectively.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¥ç»ç½‘ç»œæœ‰2ä¸ªéšè—å±‚ï¼Œåˆ†åˆ«åŒ…å«3ä¸ªèŠ‚ç‚¹å’Œ2ä¸ªèŠ‚ç‚¹ã€‚
- en: Output layer
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾“å‡ºå±‚
- en: 'The last layer gives us our final answer. The number of nodes in our output
    layer depends on our task: for binary classification or regression, we might have
    just one output node, while for multi-class problems, weâ€™d have one node per class.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€å±‚ç»™å‡ºäº†æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆã€‚è¾“å‡ºå±‚ä¸­èŠ‚ç‚¹çš„æ•°é‡å–å†³äºæˆ‘ä»¬çš„ä»»åŠ¡ï¼šå¯¹äºäºŒåˆ†ç±»æˆ–å›å½’é—®é¢˜ï¼Œæˆ‘ä»¬å¯èƒ½åªæœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œè€Œå¯¹äºå¤šç±»é—®é¢˜ï¼Œæˆ‘ä»¬åˆ™ä¸ºæ¯ä¸ªç±»åˆ«è®¾ç½®ä¸€ä¸ªèŠ‚ç‚¹ã€‚
- en: '![](../Images/501dd95ec39e9361695c1dfe3776c883.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/501dd95ec39e9361695c1dfe3776c883.png)'
- en: This neural network has output layer with only 1 node (because binary).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚åªæœ‰1ä¸ªèŠ‚ç‚¹ï¼ˆå› ä¸ºæ˜¯äºŒåˆ†ç±»ï¼‰ã€‚
- en: Weights
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒé‡
- en: 'The nodes connect to each other using weights â€” numbers that control how much
    each piece of information matters. Each connection between nodes has its own weight.
    This means we need lots of weights: every node in one layer connects to every
    node in the next layer.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ä¹‹é—´é€šè¿‡æƒé‡ç›¸äº’è¿æ¥â€”â€”æƒé‡æ˜¯æ§åˆ¶æ¯ä¸ªä¿¡æ¯ç‰‡æ®µé‡è¦æ€§çš„æ•°å­—ã€‚æ¯ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥éƒ½æœ‰è‡ªå·±çš„æƒé‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¤§é‡çš„æƒé‡ï¼šä¸€å±‚ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½è¿æ¥åˆ°ä¸‹ä¸€å±‚çš„æ¯ä¸ªèŠ‚ç‚¹ã€‚
- en: '![](../Images/943ab96a3ec27296e97af58df6ac6f3b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/943ab96a3ec27296e97af58df6ac6f3b.png)'
- en: This neural network has total of 14 weights.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¥ç»ç½‘ç»œæ€»å…±æœ‰14ä¸ªæƒé‡ã€‚
- en: Biases
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åç½®
- en: Along with weights, each node also has a bias â€” an extra number that helps it
    make better decisions. While weights control connections between nodes, biases
    help each node adjust its output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†æƒé‡ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¿˜æ‹¥æœ‰ä¸€ä¸ªåç½®â€”â€”ä¸€ä¸ªé¢å¤–çš„æ•°å­—ï¼Œå¸®åŠ©å®ƒåšå‡ºæ›´å¥½çš„å†³ç­–ã€‚æƒé‡æ§åˆ¶èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥ï¼Œè€Œåç½®åˆ™å¸®åŠ©æ¯ä¸ªèŠ‚ç‚¹è°ƒæ•´å…¶è¾“å‡ºã€‚
- en: '![](../Images/ad9281828475d8a5feccaf9fb504b653.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad9281828475d8a5feccaf9fb504b653.png)'
- en: This neural network has 6 bias values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç¥ç»ç½‘ç»œæœ‰6ä¸ªåç½®å€¼ã€‚
- en: The Neural Network
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œ
- en: 'In summary, we will use and train this neural network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“æ¥è¯´ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¹¶è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œï¼š
- en: '![](../Images/91aa2d74c3115b07db4df4ad2928bd43.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91aa2d74c3115b07db4df4ad2928bd43.png)'
- en: 'Our network consists of 4 layers: 1 input layer (2 nodes), 2 hidden layers
    (3 nodes & 2 nodes), and 1 output layer (1 node). This creates a 2â€“3â€“2â€“1 architecture.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç½‘ç»œç”±4å±‚ç»„æˆï¼š1ä¸ªè¾“å…¥å±‚ï¼ˆ2ä¸ªèŠ‚ç‚¹ï¼‰ã€2ä¸ªéšè—å±‚ï¼ˆ3ä¸ªèŠ‚ç‚¹å’Œ2ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œä»¥åŠ1ä¸ªè¾“å‡ºå±‚ï¼ˆ1ä¸ªèŠ‚ç‚¹ï¼‰ã€‚è¿™å½¢æˆäº†ä¸€ä¸ª2â€“3â€“2â€“1çš„æ¶æ„ã€‚
- en: 'Letâ€™s look at this new diagram that shows our network from top to bottom. Iâ€™ve
    updated it to make the math easier to follow: information starts at the top nodes
    and flows down through the layers until it reaches the final answer at the bottom.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ–°å›¾ç¤ºï¼Œå®ƒå±•ç¤ºäº†æˆ‘ä»¬çš„ç½‘ç»œä»ä¸Šåˆ°ä¸‹çš„ç»“æ„ã€‚æˆ‘å·²ç»æ›´æ–°äº†å®ƒï¼Œä»¥ä¾¿æ›´å®¹æ˜“ç†è§£æ•°å­¦è¿‡ç¨‹ï¼šä¿¡æ¯ä»é¡¶éƒ¨çš„èŠ‚ç‚¹å¼€å§‹ï¼Œæ²¿ç€å„å±‚å‘ä¸‹æµåŠ¨ï¼Œç›´åˆ°åˆ°è¾¾åº•éƒ¨çš„æœ€ç»ˆç­”æ¡ˆã€‚
- en: '![](../Images/86f3b59ab8e38a31139b12deeff4036b.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86f3b59ab8e38a31139b12deeff4036b.png)'
- en: Now that we understand how our network is built, letâ€™s see how information moves
    through it. This is called the forward pass.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç†è§£äº†ç½‘ç»œçš„æ„å»ºæ–¹å¼ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹ä¿¡æ¯æ˜¯å¦‚ä½•åœ¨å…¶ä¸­æµåŠ¨çš„ã€‚è¿™å«åšå‰å‘ä¼ æ’­ï¼ˆforward passï¼‰ã€‚
- en: 'Step 1: Forward Pass'
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šå‰å‘ä¼ æ’­
- en: 'Letâ€™s see how our network turns input into output, step by step:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€æ­¥äº†è§£ç½‘ç»œæ˜¯å¦‚ä½•å°†è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºçš„ï¼š
- en: Weight initialization
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒé‡åˆå§‹åŒ–
- en: Before our network can start learning, we need to give each weight a starting
    value. We choose small random numbers between -1 and 1\. Starting with random
    numbers helps our network learn without any early preferences or patterns.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç½‘ç»œå¼€å§‹å­¦ä¹ ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæƒé‡è®¾ç½®ä¸€ä¸ªåˆå§‹å€¼ã€‚æˆ‘ä»¬é€‰æ‹©åœ¨-1åˆ°1ä¹‹é—´çš„å°éšæœºæ•°ã€‚ä½¿ç”¨éšæœºæ•°å¼€å§‹æœ‰åŠ©äºæˆ‘ä»¬çš„ç½‘ç»œåœ¨æ²¡æœ‰ä»»ä½•æ—©æœŸåå¥½æˆ–æ¨¡å¼çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚
- en: '![](../Images/acad0344344b5bb71231385d3d1816b5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acad0344344b5bb71231385d3d1816b5.png)'
- en: All the weights are chosen randomly from a [-0.5, 0.5] range.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æƒé‡éƒ½æ˜¯ä»[-0.5, 0.5]èŒƒå›´å†…éšæœºé€‰æ‹©çš„ã€‚
- en: Weighted sum
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ æƒå’Œ
- en: Each node processes incoming data in two steps. First, it multiplies each input
    by its weight and adds all these numbers together. Then it adds one more number
    â€” the bias â€” to complete the calculation. The bias is essentially **a weight with
    a constant input of 1**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªèŠ‚ç‚¹ä»¥ä¸¤æ­¥å¤„ç†ä¼ å…¥çš„æ•°æ®ã€‚é¦–å…ˆï¼Œå®ƒå°†æ¯ä¸ªè¾“å…¥ä¹˜ä»¥å…¶æƒé‡ï¼Œå¹¶å°†æ‰€æœ‰è¿™äº›æ•°å­—ç›¸åŠ ã€‚ç„¶åï¼Œå®ƒå†åŠ ä¸Šä¸€ä¸ªæ•°å­—â€”â€”åç½®â€”â€”ä»¥å®Œæˆè®¡ç®—ã€‚åç½®æœ¬è´¨ä¸Šæ˜¯**ä¸€ä¸ªæ’å®šè¾“å…¥ä¸º1çš„æƒé‡**ã€‚
- en: '![](../Images/f8d6f7809ee3fa23613d041108562246.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8d6f7809ee3fa23613d041108562246.png)'
- en: Activation function
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‡½æ•°
- en: Each node takes its weighted sum and runs it through an **activation function**
    to produce its output. The activation function helps our network learn complicated
    patterns by introducing non-linear behavior.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªèŠ‚ç‚¹å°†å…¶åŠ æƒå’Œé€šè¿‡**æ¿€æ´»å‡½æ•°**è¿›è¡Œå¤„ç†ï¼Œä»¥ç”Ÿæˆè¾“å‡ºã€‚æ¿€æ´»å‡½æ•°é€šè¿‡å¼•å…¥éçº¿æ€§è¡Œä¸ºï¼Œå¸®åŠ©æˆ‘ä»¬çš„ç½‘ç»œå­¦ä¹ å¤æ‚çš„æ¨¡å¼ã€‚
- en: 'In our hidden layers, we use the ReLU function (Rectified Linear Unit). ReLU
    is straightforward: if a number is positive, it stays the same; if itâ€™s negative,
    it becomes zero.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„éšè—å±‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ ReLU å‡½æ•°ï¼ˆæ•´æµçº¿æ€§å•å…ƒï¼‰ã€‚ReLU å¾ˆç®€å•ï¼šå¦‚æœä¸€ä¸ªæ•°æ˜¯æ­£æ•°ï¼Œå®ƒä¿æŒä¸å˜ï¼›å¦‚æœæ˜¯è´Ÿæ•°ï¼Œå®ƒå˜ä¸ºé›¶ã€‚
- en: '![](../Images/6a74e04f64fc33016b7d0d03042d1b62.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a74e04f64fc33016b7d0d03042d1b62.png)'
- en: Layer-by-layer computation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å±‚çº§è®¡ç®—
- en: This two-step process (weighted sums and activation) happens in every layer,
    one after another. Each layerâ€™s calculations help transform our input data step
    by step into our final prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¸¤æ­¥è¿‡ç¨‹ï¼ˆåŠ æƒæ±‚å’Œå’Œæ¿€æ´»ï¼‰åœ¨æ¯ä¸€å±‚ä¸­ä¾æ¬¡è¿›è¡Œã€‚æ¯ä¸€å±‚çš„è®¡ç®—å¸®åŠ©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥åœ°å°†è¾“å…¥æ•°æ®è½¬åŒ–ä¸ºæœ€ç»ˆçš„é¢„æµ‹å€¼ã€‚
- en: '![](../Images/62d3a696fdde86aeed6858154a42ef3c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62d3a696fdde86aeed6858154a42ef3c.png)'
- en: Output generation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¾“å‡ºç”Ÿæˆ
- en: The last layer creates our networkâ€™s final answer. For our yes/no classification
    task, we use a special activation function called **sigmoid** in this layer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€å±‚ç»™å‡ºäº†ç½‘ç»œçš„æœ€ç»ˆç­”æ¡ˆã€‚å¯¹äºæˆ‘ä»¬çš„æ˜¯/å¦åˆ†ç±»ä»»åŠ¡ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸€å±‚ä½¿ç”¨ä¸€ç§ç‰¹æ®Šçš„æ¿€æ´»å‡½æ•°ï¼Œå«åš **sigmoid**ã€‚
- en: 'The sigmoid function turns any number into a value between 0 and 1\. This makes
    it perfect for yes/no decisions, as we can treat the output like a probability:
    closer to 1 means more likely â€˜yesâ€™, closer to 0 means more likely â€˜noâ€™.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid å‡½æ•°å°†ä»»ä½•æ•°å€¼è½¬åŒ–ä¸º0åˆ°1ä¹‹é—´çš„å€¼ã€‚è¿™ä½¿å¾—å®ƒéå¸¸é€‚åˆç”¨äºæ˜¯/å¦çš„å†³ç­–ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å°†è¾“å‡ºè§†ä¸ºä¸€ç§æ¦‚ç‡ï¼šè¶Šæ¥è¿‘1æ„å‘³ç€è¶Šå¯èƒ½æ˜¯â€œæ˜¯â€ï¼Œè¶Šæ¥è¿‘0æ„å‘³ç€è¶Šå¯èƒ½æ˜¯â€œå¦â€ã€‚
- en: '![](../Images/0661018ebd5f06045168823edd7e748a.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0661018ebd5f06045168823edd7e748a.png)'
- en: This process of forward pass turns our input into a prediction between 0 and
    1\. But how good are these predictions? Next, weâ€™ll measure how close our predictions
    are to the correct answers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‰å‘ä¼ æ’­è¿‡ç¨‹å°†æˆ‘ä»¬çš„è¾“å…¥è½¬åŒ–ä¸ºä¸€ä¸ª0åˆ°1ä¹‹é—´çš„é¢„æµ‹å€¼ã€‚ä½†æ˜¯è¿™äº›é¢„æµ‹æœ‰å¤šå‡†ç¡®å‘¢ï¼Ÿæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¡¡é‡æˆ‘ä»¬çš„é¢„æµ‹ä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„æ¥è¿‘ç¨‹åº¦ã€‚
- en: 'Step 2: Loss Calculation'
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šæŸå¤±è®¡ç®—
- en: Loss function
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°
- en: To check how well our network is doing, we measure the difference between its
    predictions and the correct answers. For binary classification, we use a method
    called **binary cross-entropy** that shows us how far off our predictions are
    from the true values.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬çš„ç½‘ç»œè¡¨ç°å¦‚ä½•ï¼Œæˆ‘ä»¬è¡¡é‡å®ƒçš„é¢„æµ‹ä¸æ­£ç¡®ç­”æ¡ˆä¹‹é—´çš„å·®å¼‚ã€‚å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§å«åš **äºŒå…ƒäº¤å‰ç†µ** çš„æ–¹æ³•ï¼Œå®ƒèƒ½å‘Šè¯‰æˆ‘ä»¬é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„åå·®ã€‚
- en: '![](../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png)'
- en: Math Notation in Neural Network
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œä¸­çš„æ•°å­¦ç¬¦å·
- en: 'To improve our networkâ€™s performance, weâ€™ll need to use some math symbols.
    Letâ€™s define what each symbol means before we continue:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜æˆ‘ä»¬ç½‘ç»œçš„æ€§èƒ½ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ä¸€äº›æ•°å­¦ç¬¦å·ã€‚åœ¨ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸‹æ¯ä¸ªç¬¦å·çš„å«ä¹‰ï¼š
- en: '**Weights and Bias**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒé‡å’Œåç½®**'
- en: Weights are represented as matrices and biases as vectors (or 1-dimensional
    matrices). The bracket notation `[1]` indicates the layer number.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡è¡¨ç¤ºä¸ºçŸ©é˜µï¼Œåç½®è¡¨ç¤ºä¸ºå‘é‡ï¼ˆæˆ–ä¸€ç»´çŸ©é˜µï¼‰ã€‚æ‹¬å·è¡¨ç¤ºæ³•`[1]`è¡¨ç¤ºå±‚çš„ç¼–å·ã€‚
- en: '![](../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png)'
- en: '**Input, Output, Weighted Sum, and Value after Activation**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥ã€è¾“å‡ºã€åŠ æƒå’Œä»¥åŠæ¿€æ´»åçš„å€¼**'
- en: The values within nodes can be represented as vectors, forming a consistent
    mathematical framework.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: èŠ‚ç‚¹ä¸­çš„å€¼å¯ä»¥è¡¨ç¤ºä¸ºå‘é‡ï¼Œä»è€Œå½¢æˆä¸€è‡´çš„æ•°å­¦æ¡†æ¶ã€‚
- en: '![](../Images/f18f52954b17091054a53c2ec7e40f8c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f18f52954b17091054a53c2ec7e40f8c.png)'
- en: '**All Together** These math symbols help us write exactly what our network
    does:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ€»ç»“** è¿™äº›æ•°å­¦ç¬¦å·å¸®åŠ©æˆ‘ä»¬ç²¾ç¡®åœ°æè¿°ç½‘ç»œçš„è¿ä½œè¿‡ç¨‹ï¼š'
- en: '![](../Images/637013282fdcc206f32a296b0ce5d284.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/637013282fdcc206f32a296b0ce5d284.png)'
- en: 'Letâ€™s look at a diagram that shows all the math happening in our network. Each
    layer has:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªå›¾ç¤ºï¼Œå±•ç¤ºæˆ‘ä»¬ç½‘ç»œä¸­æ‰€æœ‰æ•°å­¦è¿ç®—çš„è¿‡ç¨‹ã€‚æ¯ä¸€å±‚éƒ½æœ‰ï¼š
- en: Weights (*W*) and biases (*b*) that connect layers
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿æ¥å±‚çš„æƒé‡ï¼ˆ*W*ï¼‰å’Œåç½®ï¼ˆ*b*ï¼‰
- en: Values before activation (*z*)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»å‰çš„å€¼ï¼ˆ*z*ï¼‰
- en: Values after activation (*a*)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¿€æ´»åçš„å€¼ï¼ˆ*a*ï¼‰
- en: Final prediction (*Å·*) and loss (*L*) at the end
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€ç»ˆé¢„æµ‹å€¼ï¼ˆ*Å·*ï¼‰å’ŒæŸå¤±å€¼ï¼ˆ*L*ï¼‰åœ¨æœ«å°¾
- en: '![](../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png)'
- en: 'Letâ€™s see exactly what happens at each layer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸€å±‚åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼š
- en: '*First hidden layer*:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬ä¸€ä¸ªéšè—å±‚*ï¼š'
- en: '*Â·* Takes our input *x*, multiplies it by weights *W*[1], adds bias *b*[1]
    to get *z*[1]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å–è¾“å…¥ *x*ï¼Œä¸æƒé‡ *W*[1] ç›¸ä¹˜ï¼ŒåŠ å…¥åç½® *b*[1] å¾—åˆ° *z*[1]'
- en: '*Â·* Applies ReLU to *z*[1] to get output *a*[1]'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å¯¹ *z*[1] åº”ç”¨ ReLU å‡½æ•°å¾—åˆ°è¾“å‡º *a*[1]'
- en: '*Second hidden layer*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬äºŒä¸ªéšè—å±‚*ï¼š'
- en: '*Â·* Takes *a*[1], multiplies by weights *W*[2], adds bias *b*[2] to get *z*[2]'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å– *a*[1]ï¼Œä¸æƒé‡ *W*[2] ç›¸ä¹˜ï¼ŒåŠ å…¥åç½® *b*[2] å¾—åˆ° *z*[2]'
- en: '*Â·* Applies ReLU to *z*[2] to get output *a*[2]'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å¯¹ *z*[2] åº”ç”¨ ReLU å‡½æ•°å¾—åˆ°è¾“å‡º *a*[2]'
- en: '*Output layer*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¾“å‡ºå±‚*ï¼š'
- en: '*Â·* Takes *a*[2], multiplies by weights *W*[3], adds bias *b*[3] to get *z*[3]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å– *a*[2]ï¼Œä¸æƒé‡ *W*[3] ç›¸ä¹˜ï¼ŒåŠ å…¥åç½® *b*[3] å¾—åˆ° *z*[3]'
- en: '*Â·* Applies sigmoid to *z*[3] to get our final prediction *Å·*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å¯¹ *z*[3] åº”ç”¨ sigmoid æ¥è·å¾—æˆ‘ä»¬çš„æœ€ç»ˆé¢„æµ‹ *Å·*'
- en: '![](../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png)'
- en: Now that we see all the math in our network, how do we improve these numbers
    to get better predictions? This is where **backpropagation** comes in â€” it shows
    us how to adjust our weights and biases to make fewer mistakes.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çœ‹åˆ°ç½‘ç»œä¸­çš„æ‰€æœ‰æ•°å­¦å…¬å¼ï¼Œå¦‚ä½•æ”¹è¿›è¿™äº›æ•°å­—ä»¥è·å¾—æ›´å¥½çš„é¢„æµ‹ï¼Ÿè¿™æ—¶**åå‘ä¼ æ’­**å°±æ´¾ä¸Šç”¨åœºäº†â€”â€”å®ƒå‘æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•è°ƒæ•´æˆ‘ä»¬çš„æƒé‡å’Œåå·®ï¼Œä»¥å‡å°‘é”™è¯¯ã€‚
- en: 'Step 3: Backpropagation'
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬3æ­¥ï¼šåå‘ä¼ æ’­
- en: 'Before we see how to improve our network, letâ€™s quickly review some math tools
    weâ€™ll need:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬äº†è§£å¦‚ä½•æ”¹è¿›ç½‘ç»œä¹‹å‰ï¼Œè®©æˆ‘ä»¬å¿«é€Ÿå›é¡¾ä¸€ä¸‹æˆ‘ä»¬éœ€è¦çš„ä¸€äº›æ•°å­¦å·¥å…·ï¼š
- en: Derivative
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¯¼æ•°
- en: 'To optimize our neural network, we use **gradients** â€” a concept closely related
    to derivatives. Letâ€™s review some fundamental derivative rules:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¼˜åŒ–æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ä½¿ç”¨**æ¢¯åº¦**â€”â€”è¿™æ˜¯ä¸€ä¸ªä¸å¯¼æ•°å¯†åˆ‡ç›¸å…³çš„æ¦‚å¿µã€‚è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åŸºæœ¬çš„å¯¼æ•°è§„åˆ™ï¼š
- en: '![](../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png)'
- en: Partial Derivative
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åå¯¼æ•°
- en: 'Letâ€™s clarify the distinction between regular and partial derivatives:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¾„æ¸…å¸¸è§„å¯¼æ•°å’Œåå¯¼æ•°ä¹‹é—´çš„åŒºåˆ«ï¼š
- en: '***Regular Derivative*:** *Â·* Used when a function has only one variable'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¸¸è§„å¯¼æ•°*ï¼š* *Â·* ç”¨äºå‡½æ•°åªæœ‰ä¸€ä¸ªå˜é‡çš„æƒ…å†µ'
- en: '*Â·* Shows how much the function changes when its only variable changes'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* æ˜¾ç¤ºå½“å”¯ä¸€å˜é‡å˜åŒ–æ—¶å‡½æ•°çš„å˜åŒ–é‡'
- en: '*Â·* Written as d*f*/d*x*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å†™ä½œ d*f*/d*x*'
- en: '***Partial Derivative***:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '***åå¯¼æ•°***ï¼š'
- en: '*Â·* Used when a function has more than one variable'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* ç”¨äºå‡½æ•°æœ‰å¤šä¸ªå˜é‡çš„æƒ…å†µ'
- en: '*Â·* Shows how much the function changes when one variable changes, while **keeping
    the other variables the same (as constant).**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* æ˜¾ç¤ºå½“ä¸€ä¸ªå˜é‡å˜åŒ–æ—¶å‡½æ•°çš„å˜åŒ–é‡ï¼ŒåŒæ—¶**ä¿æŒå…¶ä»–å˜é‡ä¸å˜ï¼ˆè§†ä¸ºå¸¸æ•°ï¼‰**ã€‚'
- en: '*Â·* Written as âˆ‚*f*/*âˆ‚*x'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* å†™ä½œ âˆ‚*f*/*âˆ‚*x'
- en: '![](../Images/2f60c208ee4d23e387354ed4a346fc2d.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f60c208ee4d23e387354ed4a346fc2d.png)'
- en: Some examples of partial derivatives
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›åå¯¼æ•°çš„ä¾‹å­
- en: Gradient Calculation and Backpropagation
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è®¡ç®—ä¸åå‘ä¼ æ’­
- en: Returning to our neural network, we need to determine **how to adjust each weight
    and bias** to minimize the error. We can do this using a method called backpropagation,
    which shows us how changing each value affects our networkâ€™s errors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å›åˆ°æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬éœ€è¦ç¡®å®š**å¦‚ä½•è°ƒæ•´æ¯ä¸ªæƒé‡å’Œåå·®**ä»¥æœ€å°åŒ–è¯¯å·®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ç§å«åšåå‘ä¼ æ’­çš„æ–¹æ³•æ¥å®ç°ï¼Œå®ƒå‘æˆ‘ä»¬å±•ç¤ºäº†æ”¹å˜æ¯ä¸ªå€¼å¦‚ä½•å½±å“ç½‘ç»œçš„è¯¯å·®ã€‚
- en: Since backpropagation works backwards through our network, letâ€™s flip our diagram
    upside down to see how this works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåå‘ä¼ æ’­æ˜¯é€šè¿‡ç½‘ç»œåå‘è¿›è¡Œçš„ï¼Œæˆ‘ä»¬å¯ä»¥å°†å›¾ç¤ºç¿»è½¬è¿‡æ¥ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚
- en: '![](../Images/9f371a97af93a091b3afdbb51105776b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f371a97af93a091b3afdbb51105776b.png)'
- en: Matrix Rules for Networks
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç½‘ç»œçš„çŸ©é˜µè§„åˆ™
- en: 'Since our network uses matrices (groups of weights and biases), we need special
    rules to calculate how changes affect our results. Here are two key matrix rules.
    For vectors **v, u** (size 1 Ã— *n*)and matrices **W, X** (size *n* Ã— *n*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„ç½‘ç»œä½¿ç”¨çŸ©é˜µï¼ˆæƒé‡å’Œåå·®çš„ç»„åˆï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ç‰¹æ®Šçš„è§„åˆ™æ¥è®¡ç®—å˜åŒ–å¦‚ä½•å½±å“æˆ‘ä»¬çš„ç»“æœã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªå…³é”®çš„çŸ©é˜µè§„åˆ™ã€‚å¯¹äºå‘é‡**v, u**ï¼ˆå¤§å°ä¸º1
    Ã— *n*ï¼‰å’ŒçŸ©é˜µ**W, X**ï¼ˆå¤§å°ä¸º*n* Ã— *n*ï¼‰ï¼š
- en: '*Sum Rule*:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ±‚å’Œè§„åˆ™*ï¼š'
- en: âˆ‚(**W** + **X**)/âˆ‚**W** = **I** (Identity matrix, size *n* Ã— *n*)
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: âˆ‚(**W** + **X**)/âˆ‚**W** = **I**ï¼ˆå•ä½çŸ©é˜µï¼Œå¤§å°ä¸º*n* Ã— *n*ï¼‰
- en: âˆ‚(**u** + **v**)/âˆ‚**v** = **I** (Identity matrix, size *n* Ã— *n*)
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: âˆ‚(**u** + **v**)/âˆ‚**v** = **I**ï¼ˆå•ä½çŸ©é˜µï¼Œå¤§å°ä¸º*n* Ã— *n*ï¼‰
- en: '*Matrix-Vector Product Rule*:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*çŸ©é˜µ-å‘é‡ä¹˜ç§¯è§„åˆ™*ï¼š'
- en: âˆ‚(**vW**)/âˆ‚**W** = **v**áµ€
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: âˆ‚(**vW**)/âˆ‚**W** = **v**áµ€
- en: âˆ‚(**vW**)/âˆ‚**v** = **W**áµ€
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: âˆ‚(**vW**)/âˆ‚**v** = **W**áµ€
- en: 'Using these rules, we obtain:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›è§„åˆ™ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '![](../Images/21a2c76b825f478f0251171b7b979348.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21a2c76b825f478f0251171b7b979348.png)'
- en: '**Activation Function Derivatives** *Derivatives of ReLU*For vectors **a**
    and **z** (size 1 Ã— *n*), where **a** = ReLU(**z**):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¿€æ´»å‡½æ•°çš„å¯¼æ•°** *ReLUçš„å¯¼æ•°*å¯¹äºå‘é‡**a**å’Œ**z**ï¼ˆå¤§å°ä¸º1 Ã— *n*ï¼‰ï¼Œå…¶ä¸­**a** = ReLU(**z**)ï¼š'
- en: âˆ‚**a**/âˆ‚**z** = diag(**z** > 0)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ‚**a**/âˆ‚**z** = diag(**z** > 0)
- en: 'Creates a diagonal matrix that shows: 1 if input was positive, 0 if input was
    zero or negative.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œè¡¨ç¤ºï¼šå¦‚æœè¾“å…¥ä¸ºæ­£ï¼Œåˆ™ä¸º1ï¼›å¦‚æœè¾“å…¥ä¸ºé›¶æˆ–è´Ÿï¼Œåˆ™ä¸º0ã€‚
- en: '*Derivatives of Sigmoid*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sigmoid çš„å¯¼æ•°*'
- en: 'For **a** = Ïƒ(**z**), where Ïƒ is the sigmoid function:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº**a** = Ïƒ(**z**)ï¼Œå…¶ä¸­ Ïƒ æ˜¯ sigmoid å‡½æ•°ï¼š
- en: âˆ‚**a**/âˆ‚**z** = **a** âŠ™ (1 **- a**)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ‚**a**/âˆ‚**z** = **a** âŠ™ (1 **- a**)
- en: This multiplies elements directly (âŠ™ means multiply each position).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç›´æ¥ä¹˜ä»¥å…ƒç´ ï¼ˆâŠ™è¡¨ç¤ºé€ä½ç½®ç›¸ä¹˜ï¼‰ã€‚
- en: '![](../Images/406536d3c65893436cf88c0b469d845e.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/406536d3c65893436cf88c0b469d845e.png)'
- en: '**Binary Cross-Entropy Loss Derivative**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**äºŒè¿›åˆ¶äº¤å‰ç†µæŸå¤±å‡½æ•°çš„å¯¼æ•°**'
- en: 'For a single example with loss *L* = -[*y* log(Å·) + (1-*y*) log(1-*Å·*)]:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸€ä¸ªå•ç‹¬çš„ä¾‹å­ï¼ŒæŸå¤±å‡½æ•°ä¸º *L* = -[*y* log(Å·) + (1-*y*) log(1-*Å·*)]ï¼š
- en: âˆ‚*L*/âˆ‚*Å·* = -(*y*-*Å·*) / [*Å·*(1-*Å·*)]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: âˆ‚*L*/âˆ‚*Å·* = -(*y*-*Å·*) / [*Å·*(1-*Å·*)]
- en: '![](../Images/145aab17537a82e33342299758155d04.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/145aab17537a82e33342299758155d04.png)'
- en: 'Up to this point, we can summarized all the partial derivatives as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰çš„åå¯¼æ•°æ€»ç»“å¦‚ä¸‹ï¼š
- en: '![](../Images/2ecaacf0d1a77fe1296855018554dd45.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ecaacf0d1a77fe1296855018554dd45.png)'
- en: 'The following image shows all the partial derivatives that weâ€™ve obtained so
    far:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾å±•ç¤ºäº†æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢å¾—åˆ°çš„æ‰€æœ‰åå¯¼æ•°ï¼š
- en: '![](../Images/fbe67994cd024b69583a9413901bca26.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbe67994cd024b69583a9413901bca26.png)'
- en: Chain Rule
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é“¾å¼æ³•åˆ™
- en: 'In our network, changes flow through multiple steps: a weight affects its layerâ€™s
    output, which affects the next layer, and so on until the final error. The chain
    rule tells us to **multiply these step-by-step changes together** to find how
    each weight and bias affects the final error.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç½‘ç»œä¸­ï¼Œå˜åŒ–é€šè¿‡å¤šä¸ªæ­¥éª¤è¿›è¡Œä¼ æ’­ï¼šä¸€ä¸ªæƒé‡å½±å“å®ƒæ‰€åœ¨å±‚çš„è¾“å‡ºï¼Œè¿™åˆå½±å“ä¸‹ä¸€å±‚ï¼Œä¾æ­¤ç±»æ¨ï¼Œç›´åˆ°æœ€ç»ˆè¯¯å·®ã€‚é“¾å¼æ³•åˆ™å‘Šè¯‰æˆ‘ä»¬**å°†è¿™äº›é€æ­¥å˜åŒ–ç›¸ä¹˜**ï¼Œä»¥æ‰¾å‡ºæ¯ä¸ªæƒé‡å’Œåç½®å¦‚ä½•å½±å“æœ€ç»ˆçš„è¯¯å·®ã€‚
- en: '![](../Images/1f8a5d158f28ec911745e2a4e658dc00.png)![](../Images/1f941aa47b9eaad7e064eb594ea454ce.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f8a5d158f28ec911745e2a4e658dc00.png)![](../Images/1f941aa47b9eaad7e064eb594ea454ce.png)'
- en: Error Calculation
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¯¯å·®è®¡ç®—
- en: Rather than directly computing weight and bias derivatives, we first calculate
    layer errors âˆ‚*L*/âˆ‚*zË¡* (the gradient with respect to pre-activation outputs).
    This makes it easier to then calculate how we should adjust the weights and biases
    in earlier layers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¹¶ä¸æ˜¯ç›´æ¥è®¡ç®—æƒé‡å’Œåç½®çš„å¯¼æ•°ï¼Œè€Œæ˜¯é¦–å…ˆè®¡ç®—å±‚è¯¯å·®âˆ‚*L*/âˆ‚*zË¡*ï¼ˆç›¸å¯¹äºé¢„æ¿€æ´»è¾“å‡ºçš„æ¢¯åº¦ï¼‰ã€‚è¿™ä½¿å¾—åç»­è®¡ç®—æˆ‘ä»¬åº”è¯¥å¦‚ä½•è°ƒæ•´æ—©æœŸå±‚çš„æƒé‡å’Œåç½®å˜å¾—æ›´å®¹æ˜“ã€‚
- en: '![](../Images/6632ead66c14bdace2390adf44c9fd3a.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6632ead66c14bdace2390adf44c9fd3a.png)'
- en: Weight gradients and bias gradients
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒé‡æ¢¯åº¦å’Œåç½®æ¢¯åº¦
- en: 'Using these layer errors and the chain rule, we can express the weight and
    bias gradients as:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›å±‚è¯¯å·®å’Œé“¾å¼æ³•åˆ™ï¼Œæˆ‘ä»¬å¯ä»¥å°†æƒé‡å’Œåç½®çš„æ¢¯åº¦è¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/2d83604e28f9ecfdd5471d925723a364.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d83604e28f9ecfdd5471d925723a364.png)'
- en: The gradients show us how each value in our network affects our networkâ€™s error.
    We then make small changes to these values to help our network make better predictions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦å‘æˆ‘ä»¬å±•ç¤ºäº†ç½‘ç»œä¸­æ¯ä¸ªå€¼å¦‚ä½•å½±å“ç½‘ç»œçš„è¯¯å·®ã€‚æˆ‘ä»¬éšåå¯¹è¿™äº›å€¼è¿›è¡Œå°å¹…è°ƒæ•´ï¼Œä»¥å¸®åŠ©ç½‘ç»œåšå‡ºæ›´å¥½çš„é¢„æµ‹ã€‚
- en: 'Step 4: Weight Update'
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 4ï¼šæƒé‡æ›´æ–°
- en: Updating weights
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ›´æ–°æƒé‡
- en: Once we know how each weight and bias affects the error (the gradients), we
    improve our network by adjusting these values in the opposite direction of their
    gradients. This reduces the networkâ€™s error step by step.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬çŸ¥é“æ¯ä¸ªæƒé‡å’Œåç½®å¦‚ä½•å½±å“è¯¯å·®ï¼ˆå³æ¢¯åº¦ï¼‰ï¼Œæˆ‘ä»¬å°±é€šè¿‡å°†è¿™äº›å€¼è°ƒæ•´åˆ°ä¸æ¢¯åº¦ç›¸åçš„æ–¹å‘æ¥æ”¹è¿›æˆ‘ä»¬çš„ç½‘ç»œã€‚è¿™ä¸€æ­¥æ­¥åœ°å‡å°‘äº†ç½‘ç»œçš„è¯¯å·®ã€‚
- en: '![](../Images/56bd580089de99dc7ce5af8f7a7915d5.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56bd580089de99dc7ce5af8f7a7915d5.png)'
- en: Learning Rate and Optimization
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­¦ä¹ ç‡ä¸ä¼˜åŒ–
- en: 'Instead of making big changes all at once, we make small, careful adjustments.
    We use a number called the learning rate (*Î·*) to control how much we change each
    value:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¹¶ä¸æ˜¯ä¸€æ¬¡æ€§åšå‡ºå¤§çš„å˜åŒ–ï¼Œè€Œæ˜¯è¿›è¡Œå°è€Œè°¨æ…çš„è°ƒæ•´ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå«åšå­¦ä¹ ç‡ï¼ˆ*Î·*ï¼‰çš„æ•°å€¼æ¥æ§åˆ¶æ¯æ¬¡è°ƒæ•´çš„å¹…åº¦ï¼š
- en: 'If *Î·* is too big: The changes are too large and we might make things worse'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœ*Î·*å¤ªå¤§ï¼šå˜åŒ–è¿‡å¤§ï¼Œå¯èƒ½ä¼šå¯¼è‡´æƒ…å†µå˜å¾—æ›´ç³Ÿ
- en: 'If *Î·* is too small: The changes are tiny and it takes too long to improve'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦‚æœ*Î·*å¤ªå°ï¼šå˜åŒ–å¾ˆå°ï¼Œæ”¹è¿›éœ€è¦å¾ˆé•¿æ—¶é—´
- en: 'This way of making small, controlled changes is called **Stochastic Gradient
    Descent (SGD)**. We can write it as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é€šè¿‡å°å¹…ã€å¯æ§çš„è°ƒæ•´æ¥ä¼˜åŒ–çš„æ–¹å¼è¢«ç§°ä¸º**éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰**ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶å†™ä½œï¼š
- en: '![](../Images/525bd2513df13222570baac0537d0393.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/525bd2513df13222570baac0537d0393.png)'
- en: The value of Î· (learning rate) is usually chosen to be small, typically ranging
    from 0.1 to 0.0001, to ensure stable learning.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Î·ï¼ˆå­¦ä¹ ç‡ï¼‰çš„å€¼é€šå¸¸é€‰æ‹©è¾ƒå°ï¼Œé€šå¸¸åœ¨ 0.1 åˆ° 0.0001 ä¹‹é—´ï¼Œä»¥ç¡®ä¿å­¦ä¹ çš„ç¨³å®šæ€§ã€‚
- en: We just saw how our network learns from **one example.** The network repeats
    all these steps for each example in our dataset, getting better with each round
    of practice
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšæ‰çœ‹åˆ°äº†ç½‘ç»œå¦‚ä½•é€šè¿‡**ä¸€ä¸ªç¤ºä¾‹**è¿›è¡Œå­¦ä¹ ã€‚ç½‘ç»œå¯¹æ•°æ®é›†ä¸­æ¯ä¸ªç¤ºä¾‹é‡å¤è¿™äº›æ­¥éª¤ï¼Œåœ¨æ¯è½®å®è·µä¸­é€æ­¥æ”¹è¿›ã€‚
- en: Summary of Steps
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­¥éª¤æ€»ç»“
- en: 'Here are all the steps we covered to train our network on a single example:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æˆ‘ä»¬ä¸ºè®­ç»ƒç½‘ç»œåœ¨å•ä¸ªç¤ºä¾‹ä¸Šæ‰€æ¶‰åŠçš„æ‰€æœ‰æ­¥éª¤ï¼š
- en: '![](../Images/cf84eac8a9e5bace7343937bcfc86018.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf84eac8a9e5bace7343937bcfc86018.png)'
- en: Scaling to Full Datasets
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©å±•åˆ°å®Œæ•´çš„æ•°æ®é›†
- en: Epoch
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿­ä»£
- en: Our network repeats these four steps â€” forward pass, loss calculation, backpropagation,
    and weight updates â€” for every example in our dataset. Going through all examples
    once is called **an epoch**.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç½‘ç»œä¼šå¯¹æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹é‡å¤è¿™å››ä¸ªæ­¥éª¤â€”â€”å‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€åå‘ä¼ æ’­å’Œæƒé‡æ›´æ–°ã€‚éå†æ‰€æœ‰ç¤ºä¾‹ä¸€æ¬¡ç§°ä¸º**ä¸€æ¬¡è¿­ä»£**ã€‚
- en: '![](../Images/2d454eb9ee57344d96212fd863f3b201.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d454eb9ee57344d96212fd863f3b201.png)'
- en: The network usually needs to see all examples many times to get good at its
    task, even up to 1000 times. Each time through helps it learn the patterns better.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œé€šå¸¸éœ€è¦å¤šæ¬¡çœ‹åˆ°æ‰€æœ‰ä¾‹å­ï¼Œæ‰èƒ½ç†Ÿç»ƒæŒæ¡ä»»åŠ¡ï¼Œç”šè‡³å¤šè¾¾1000æ¬¡ã€‚æ¯ä¸€æ¬¡è®­ç»ƒå¸®åŠ©å®ƒæ›´å¥½åœ°å­¦ä¹ æ¨¡å¼ã€‚
- en: '![](../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png)'
- en: Batch
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‰¹æ¬¡
- en: 'Instead of learning from one example at a time, our network learns from small
    groups of examples (called **batches**) at once. This has several benefits:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç½‘ç»œä¸æ˜¯ä¸€æ¬¡ä»ä¸€ä¸ªä¾‹å­ä¸­å­¦ä¹ ï¼Œè€Œæ˜¯ä¸€æ¬¡ä»ä¸€å°ç»„ä¾‹å­ï¼ˆç§°ä¸º**æ‰¹æ¬¡**ï¼‰ä¸­å­¦ä¹ ã€‚è¿™æœ‰å‡ ä¸ªå¥½å¤„ï¼š
- en: Works faster
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿è¡Œæ›´å¿«
- en: Learns better patterns
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å­¦ä¹ æ›´å¥½çš„æ¨¡å¼
- en: Makes steadier improvements
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¨³å®šåœ°æ”¹å–„
- en: When working with batches, the network looks at all examples in the group before
    making changes. This gives better results than changing values after each single
    example.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†æ‰¹æ¬¡æ—¶ï¼Œç½‘ç»œä¼šå…ˆæŸ¥çœ‹ç»„å†…çš„æ‰€æœ‰ä¾‹å­ï¼Œç„¶åå†åšå‡ºæ”¹å˜ã€‚è¿™æ¯”æ¯çœ‹ä¸€ä¸ªä¾‹å­å°±æ”¹å˜ä¸€æ¬¡å€¼èƒ½å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚
- en: '![](../Images/d12014b56111808330a0933a6d0f9811.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d12014b56111808330a0933a6d0f9811.png)'
- en: Testing Step
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ‹è¯•æ­¥éª¤
- en: Preparing Fully-trained Neural Network
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡å®Œå…¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œ
- en: After training is done, our network is ready to make predictions on new examples
    it hasnâ€™t seen before. It uses the same steps as training, but **only needs to
    move forward** through the network to make predictions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬çš„ç½‘ç»œå‡†å¤‡å¯¹å®ƒä»æœªè§è¿‡çš„æ–°ä¾‹å­è¿›è¡Œé¢„æµ‹ã€‚å®ƒä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„æ­¥éª¤ï¼Œä½†**åªéœ€å‘å‰ä¼ æ’­**é€šè¿‡ç½‘ç»œè¿›è¡Œé¢„æµ‹ã€‚
- en: Making Predictions
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿›è¡Œé¢„æµ‹
- en: 'When processing new data:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æ–°æ•°æ®æ—¶ï¼š
- en: 1\. Input layer takes in the new values
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. è¾“å…¥å±‚æ¥æ”¶æ–°å€¼
- en: '2\. At each layer:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. åœ¨æ¯ä¸€å±‚ï¼š
- en: '*Â·* Multiplies by weights and adds biases'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* é€šè¿‡æƒé‡è¿›è¡Œä¹˜æ³•è¿ç®—å¹¶åŠ ä¸Šåå·®'
- en: '*Â·* Applies the activation function'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*Â·* åº”ç”¨æ¿€æ´»å‡½æ•°'
- en: 3\. Output layer generates predictions (e.g., probabilities between 0 and 1
    for binary classification)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. è¾“å‡ºå±‚ç”Ÿæˆé¢„æµ‹ï¼ˆä¾‹å¦‚ï¼ŒäºŒåˆ†ç±»çš„æ¦‚ç‡å€¼åœ¨0åˆ°1ä¹‹é—´ï¼‰
- en: '![](../Images/aa9a06f87d503e7635666859984dd39e.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa9a06f87d503e7635666859984dd39e.png)'
- en: The prediction for ID 9 is 1 (YES).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ID 9çš„é¢„æµ‹ç»“æœæ˜¯1ï¼ˆæ˜¯ï¼‰ã€‚
- en: Deterministic Nature of Neural Network
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„ç¡®å®šæ€§ç‰¹å¾
- en: When our network sees the same input twice, it will give the same answer both
    times (as long as we havenâ€™t changed its weights and biases). The networkâ€™s ability
    to handle new examples comes from its training, not from any randomness in making
    predictions.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬çš„ç½‘ç»œä¸¤æ¬¡çœ‹åˆ°ç›¸åŒçš„è¾“å…¥æ—¶ï¼Œå®ƒä¼šä¸¤æ¬¡ç»™å‡ºç›¸åŒçš„ç­”æ¡ˆï¼ˆå‰ææ˜¯æˆ‘ä»¬æ²¡æœ‰æ”¹å˜å®ƒçš„æƒé‡å’Œåå·®ï¼‰ã€‚ç½‘ç»œå¤„ç†æ–°ä¾‹å­çš„èƒ½åŠ›æ¥è‡ªäºå®ƒçš„è®­ç»ƒï¼Œè€Œä¸æ˜¯åœ¨é¢„æµ‹æ—¶çš„ä»»ä½•éšæœºæ€§ã€‚
- en: Final Remarks
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€åçš„å¤‡æ³¨
- en: 'As our network practices with the examples again and again, it gets better
    at its task. It makes fewer mistakes over time, and its predictions get more accurate.
    This is how neural networks learn: look at examples, find mistakes, make small
    improvements, and repeat!'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€ç½‘ç»œä¸€æ¬¡åˆä¸€æ¬¡åœ°ç»ƒä¹ è¿™äº›ä¾‹å­ï¼Œå®ƒä¼šåœ¨ä»»åŠ¡ä¸Šå˜å¾—æ›´å¥½ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒçŠ¯çš„é”™è¯¯è¶Šæ¥è¶Šå°‘ï¼Œé¢„æµ‹ä¹Ÿå˜å¾—æ›´åŠ å‡†ç¡®ã€‚è¿™å°±æ˜¯ç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹ çš„è¿‡ç¨‹ï¼šæŸ¥çœ‹ä¾‹å­ï¼Œæ‰¾åˆ°é”™è¯¯ï¼Œåšå‡ºå°çš„æ”¹è¿›ï¼Œå¹¶ä¸æ–­é‡å¤ï¼
- en: ğŸŒŸ **Multilayer Perceptron Classifier Code Summary**
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒŸ **å¤šå±‚æ„ŸçŸ¥æœºåˆ†ç±»å™¨ä»£ç æ€»ç»“**
- en: Now letâ€™s see our neural network in action. Hereâ€™s some Python code that builds
    the network weâ€™ve been talking about, using the same structure and rules we just
    learned.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è¿ä½œçš„ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›Pythonä»£ç ï¼Œæ„å»ºäº†æˆ‘ä»¬ä¸€ç›´åœ¨è®¨è®ºçš„ç½‘ç»œï¼Œä½¿ç”¨çš„æ˜¯æˆ‘ä»¬åˆšåˆšå­¦ä¹ çš„ç›¸åŒç»“æ„å’Œè§„åˆ™ã€‚
- en: '[PRE0]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Want to Learn More?
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒ³äº†è§£æ›´å¤šï¼Ÿ
- en: Check out scikit-learnâ€™s official documentation of [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    for more details and how to use it
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹scikit-learnçš„[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)å®˜æ–¹æ–‡æ¡£ï¼Œäº†è§£æ›´å¤šè¯¦æƒ…åŠå¦‚ä½•ä½¿ç”¨
- en: This article uses Python 3.7 and scikit-learn 1.5, but the core ideas work with
    other versions too
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä½¿ç”¨çš„æ˜¯Python 3.7å’Œscikit-learn 1.5ï¼Œä½†æ ¸å¿ƒç†å¿µä¹Ÿé€‚ç”¨äºå…¶ä»–ç‰ˆæœ¬ã€‚
- en: Image Attribution
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾ç‰‡å½’å±
- en: All diagrams and technical illustrations in this article were created by the
    author using licensed design elements from Canva Pro under their commercial license
    terms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡ä¸­çš„æ‰€æœ‰å›¾è¡¨å’ŒæŠ€æœ¯æ’å›¾éƒ½æ˜¯ä½œè€…ä½¿ç”¨Canva Proçš„å•†ä¸šè®¸å¯è¯æ¡æ¬¾ä¸‹æˆæƒè®¾è®¡å…ƒç´ åˆ¶ä½œçš„ã€‚
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Classification Algorithms
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç®—æ³•
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------)8ä¸ªæ•…äº‹![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Regression Algorithms
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›å½’ç®—æ³•
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------)5ä¸ªæ•…äº‹![ä¸€ä¸ªç»‘ç€è¾«å­ã€æˆ´ç€ç²‰è‰²å¸½å­çš„å¡é€šå¨ƒå¨ƒã€‚è¿™åªâ€œå‚€å„¡â€å¨ƒå¨ƒï¼Œå‡­å€Ÿå…¶ç®€å•çš„è®¾è®¡å’Œå¿ƒå½¢å›¾æ¡ˆçš„è¡¬è¡«ï¼Œå½¢è±¡åœ°ä»£è¡¨äº†æœºå™¨å­¦ä¹ ä¸­çš„å‚€å„¡å›å½’å™¨æ¦‚å¿µã€‚å°±åƒè¿™ä¸ªç©å…·èˆ¬çš„äººç‰©æ˜¯ä¸€ä¸ªç®€åŒ–çš„ã€é™æ€çš„äººç‰©ä»£è¡¨ä¸€æ ·ï¼Œå‚€å„¡å›å½’å™¨æ˜¯ç”¨ä½œæ›´å¤æ‚åˆ†æçš„åŸºç¡€æ¨¡å‹ã€‚](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
- en: Ensemble Learning
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ 
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------)4ä¸ªæ•…äº‹![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
