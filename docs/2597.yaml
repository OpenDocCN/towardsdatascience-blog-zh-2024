- en: 'Multilayer Perceptron, Explained: A Visual Guide with Mini 2D Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25](https://towardsdatascience.com/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c?source=collection_archive---------1-----------------------#2024-10-25)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: CLASSIFICATION ALGORITHM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dissecting the math (with visuals) of a tiny neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Samy
    Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--0ae8100c5d1c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--0ae8100c5d1c--------------------------------)
    Â·13 min readÂ·Oct 25, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '`â›³ï¸ More [CLASSIFICATION ALGORITHM](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c),
    explained: Â· [Dummy Classifier](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)
    Â· [K Nearest Neighbor Classifier](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)
    Â· [Bernoulli Naive Bayes](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6)
    Â· [Gaussian Naive Bayes](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c)
    Â· [Decision Tree Classifier](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e)
    Â· [Logistic Regression](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505)
    Â· [Support Vector Classifier](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    â–¶ [Multilayer Perceptron](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`'
  prefs: []
  type: TYPE_NORMAL
- en: Ever feel like neural networks are showing up everywhere? Theyâ€™re in the news,
    in your phone, even in your social media feed. But letâ€™s be honest â€” most of us
    have no clue how they actually work. All that fancy math and strange terms like
    â€œbackpropagationâ€?
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s a thought: what if we made things super simple? Letâ€™s explore a Multilayer
    Perceptron (MLP) â€” **the most basic type of neural network** â€” to classify a simple
    2D dataset using a small network, working with just a handful of data points.'
  prefs: []
  type: TYPE_NORMAL
- en: Through clear visuals and step-by-step explanations, youâ€™ll see the math come
    to life, watching exactly how numbers and equations flow through the network and
    how learning really happens!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68c633860f446dbc91b04b1a420fcef7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'All visuals: Author-created using Canva Pro. Optimized for mobile; may appear
    oversized on desktop.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Multilayer Perceptron (MLP) is a type of neural network that uses layers of
    connected nodes to learn patterns. It gets its name from having multiple layers
    â€” typically an input layer, one or more middle (hidden) layers, and an output
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Each node connects to all nodes in the next layer. When the network learns,
    it adjusts the strength of these connections based on training examples. For instance,
    if certain connections lead to correct predictions, they become stronger. If they
    lead to mistakes, they become weaker.
  prefs: []
  type: TYPE_NORMAL
- en: This way of learning through examples helps the network recognize patterns and
    make predictions about new situations it hasnâ€™t seen before.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14e597fceb0af5b4a1ac957a8f8434fc.png)'
  prefs: []
  type: TYPE_IMG
- en: MLPs are considered fundamental in the field of neural networks and deep learning
    because they can handle complex problems that simpler methods struggle with.
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ“Š Dataset Used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how MLPs work, letâ€™s start with a simple example: a mini 2D dataset
    with just a few samples. Weâ€™ll use [the same dataset](https://medium.com/towards-data-science/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9)
    from our previous article to keep things manageable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d0cc55573cd44b994e5a8fcff182285.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Columns: Temperature (0â€“3), Humidity (0â€“3), Play Golf (Yes/No). The training
    dataset has 2 dimensions and 8 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than jumping straight into training, letâ€™s try to understand the key
    pieces that make up a neural network and how they work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 0: Network Structure'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, letâ€™s look at the parts of our network:'
  prefs: []
  type: TYPE_NORMAL
- en: Node (Neuron)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin with the basic structure of a neural network. This structure is composed
    of many individual units called nodes or neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77265c27db24a49170b0d38542b64000.png)'
  prefs: []
  type: TYPE_IMG
- en: This neural network has 8 nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'These nodes are organized into groups called layers to work together:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The input layer is where we start. It takes in our raw data, and the number
    of nodes here matches how many features we have.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6b53d8cb9333fa129a63e062bced4a6.png)'
  prefs: []
  type: TYPE_IMG
- en: The input layer has 2 nodes, one for each feature.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next come the hidden layers. We can have one or more of these layers, and we
    can choose how many nodes each one has. Often, we use fewer nodes in each layer
    as we go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/889ce747a48b89418498002342df446c.png)'
  prefs: []
  type: TYPE_IMG
- en: This neural network has 2 hidden layers with 3 and 2 nodes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last layer gives us our final answer. The number of nodes in our output
    layer depends on our task: for binary classification or regression, we might have
    just one output node, while for multi-class problems, weâ€™d have one node per class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/501dd95ec39e9361695c1dfe3776c883.png)'
  prefs: []
  type: TYPE_IMG
- en: This neural network has output layer with only 1 node (because binary).
  prefs: []
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The nodes connect to each other using weights â€” numbers that control how much
    each piece of information matters. Each connection between nodes has its own weight.
    This means we need lots of weights: every node in one layer connects to every
    node in the next layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/943ab96a3ec27296e97af58df6ac6f3b.png)'
  prefs: []
  type: TYPE_IMG
- en: This neural network has total of 14 weights.
  prefs: []
  type: TYPE_NORMAL
- en: Biases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Along with weights, each node also has a bias â€” an extra number that helps it
    make better decisions. While weights control connections between nodes, biases
    help each node adjust its output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad9281828475d8a5feccaf9fb504b653.png)'
  prefs: []
  type: TYPE_IMG
- en: This neural network has 6 bias values.
  prefs: []
  type: TYPE_NORMAL
- en: The Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In summary, we will use and train this neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91aa2d74c3115b07db4df4ad2928bd43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our network consists of 4 layers: 1 input layer (2 nodes), 2 hidden layers
    (3 nodes & 2 nodes), and 1 output layer (1 node). This creates a 2â€“3â€“2â€“1 architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s look at this new diagram that shows our network from top to bottom. Iâ€™ve
    updated it to make the math easier to follow: information starts at the top nodes
    and flows down through the layers until it reaches the final answer at the bottom.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86f3b59ab8e38a31139b12deeff4036b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand how our network is built, letâ€™s see how information moves
    through it. This is called the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Forward Pass'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Letâ€™s see how our network turns input into output, step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before our network can start learning, we need to give each weight a starting
    value. We choose small random numbers between -1 and 1\. Starting with random
    numbers helps our network learn without any early preferences or patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acad0344344b5bb71231385d3d1816b5.png)'
  prefs: []
  type: TYPE_IMG
- en: All the weights are chosen randomly from a [-0.5, 0.5] range.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted sum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each node processes incoming data in two steps. First, it multiplies each input
    by its weight and adds all these numbers together. Then it adds one more number
    â€” the bias â€” to complete the calculation. The bias is essentially **a weight with
    a constant input of 1**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8d6f7809ee3fa23613d041108562246.png)'
  prefs: []
  type: TYPE_IMG
- en: Activation function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each node takes its weighted sum and runs it through an **activation function**
    to produce its output. The activation function helps our network learn complicated
    patterns by introducing non-linear behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our hidden layers, we use the ReLU function (Rectified Linear Unit). ReLU
    is straightforward: if a number is positive, it stays the same; if itâ€™s negative,
    it becomes zero.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6a74e04f64fc33016b7d0d03042d1b62.png)'
  prefs: []
  type: TYPE_IMG
- en: Layer-by-layer computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This two-step process (weighted sums and activation) happens in every layer,
    one after another. Each layerâ€™s calculations help transform our input data step
    by step into our final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62d3a696fdde86aeed6858154a42ef3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Output generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last layer creates our networkâ€™s final answer. For our yes/no classification
    task, we use a special activation function called **sigmoid** in this layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid function turns any number into a value between 0 and 1\. This makes
    it perfect for yes/no decisions, as we can treat the output like a probability:
    closer to 1 means more likely â€˜yesâ€™, closer to 0 means more likely â€˜noâ€™.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0661018ebd5f06045168823edd7e748a.png)'
  prefs: []
  type: TYPE_IMG
- en: This process of forward pass turns our input into a prediction between 0 and
    1\. But how good are these predictions? Next, weâ€™ll measure how close our predictions
    are to the correct answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Loss Calculation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To check how well our network is doing, we measure the difference between its
    predictions and the correct answers. For binary classification, we use a method
    called **binary cross-entropy** that shows us how far off our predictions are
    from the true values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff06e056294f4c2bbd55e6a76b6db2ff.png)'
  prefs: []
  type: TYPE_IMG
- en: Math Notation in Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To improve our networkâ€™s performance, weâ€™ll need to use some math symbols.
    Letâ€™s define what each symbol means before we continue:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Weights and Bias**'
  prefs: []
  type: TYPE_NORMAL
- en: Weights are represented as matrices and biases as vectors (or 1-dimensional
    matrices). The bracket notation `[1]` indicates the layer number.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a70ec04d0c5d1a4e659e1fc32628bc96.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Input, Output, Weighted Sum, and Value after Activation**'
  prefs: []
  type: TYPE_NORMAL
- en: The values within nodes can be represented as vectors, forming a consistent
    mathematical framework.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f18f52954b17091054a53c2ec7e40f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**All Together** These math symbols help us write exactly what our network
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/637013282fdcc206f32a296b0ce5d284.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s look at a diagram that shows all the math happening in our network. Each
    layer has:'
  prefs: []
  type: TYPE_NORMAL
- en: Weights (*W*) and biases (*b*) that connect layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values before activation (*z*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values after activation (*a*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final prediction (*Å·*) and loss (*L*) at the end
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fc645b406cf6eb07ad93cd55ff59f9ab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s see exactly what happens at each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*First hidden layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Takes our input *x*, multiplies it by weights *W*[1], adds bias *b*[1]
    to get *z*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Applies ReLU to *z*[1] to get output *a*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Second hidden layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Takes *a*[1], multiplies by weights *W*[2], adds bias *b*[2] to get *z*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Applies ReLU to *z*[2] to get output *a*[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Output layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Takes *a*[2], multiplies by weights *W*[3], adds bias *b*[3] to get *z*[3]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Applies sigmoid to *z*[3] to get our final prediction *Å·*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c4c3f3539c1ab04fa6f9a66c47f140b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we see all the math in our network, how do we improve these numbers
    to get better predictions? This is where **backpropagation** comes in â€” it shows
    us how to adjust our weights and biases to make fewer mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Backpropagation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we see how to improve our network, letâ€™s quickly review some math tools
    weâ€™ll need:'
  prefs: []
  type: TYPE_NORMAL
- en: Derivative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To optimize our neural network, we use **gradients** â€” a concept closely related
    to derivatives. Letâ€™s review some fundamental derivative rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ea522923ac7ce3ac21bd2a8e5c18c63.png)'
  prefs: []
  type: TYPE_IMG
- en: Partial Derivative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Letâ€™s clarify the distinction between regular and partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Regular Derivative*:** *Â·* Used when a function has only one variable'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Shows how much the function changes when its only variable changes'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Written as d*f*/d*x*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Partial Derivative***:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Used when a function has more than one variable'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Shows how much the function changes when one variable changes, while **keeping
    the other variables the same (as constant).**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Written as âˆ‚*f*/*âˆ‚*x'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f60c208ee4d23e387354ed4a346fc2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Some examples of partial derivatives
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Calculation and Backpropagation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Returning to our neural network, we need to determine **how to adjust each weight
    and bias** to minimize the error. We can do this using a method called backpropagation,
    which shows us how changing each value affects our networkâ€™s errors.
  prefs: []
  type: TYPE_NORMAL
- en: Since backpropagation works backwards through our network, letâ€™s flip our diagram
    upside down to see how this works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f371a97af93a091b3afdbb51105776b.png)'
  prefs: []
  type: TYPE_IMG
- en: Matrix Rules for Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since our network uses matrices (groups of weights and biases), we need special
    rules to calculate how changes affect our results. Here are two key matrix rules.
    For vectors **v, u** (size 1 Ã— *n*)and matrices **W, X** (size *n* Ã— *n*):'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sum Rule*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: âˆ‚(**W** + **X**)/âˆ‚**W** = **I** (Identity matrix, size *n* Ã— *n*)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: âˆ‚(**u** + **v**)/âˆ‚**v** = **I** (Identity matrix, size *n* Ã— *n*)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Matrix-Vector Product Rule*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: âˆ‚(**vW**)/âˆ‚**W** = **v**áµ€
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: âˆ‚(**vW**)/âˆ‚**v** = **W**áµ€
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using these rules, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21a2c76b825f478f0251171b7b979348.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Activation Function Derivatives** *Derivatives of ReLU*For vectors **a**
    and **z** (size 1 Ã— *n*), where **a** = ReLU(**z**):'
  prefs: []
  type: TYPE_NORMAL
- en: âˆ‚**a**/âˆ‚**z** = diag(**z** > 0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Creates a diagonal matrix that shows: 1 if input was positive, 0 if input was
    zero or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Derivatives of Sigmoid*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For **a** = Ïƒ(**z**), where Ïƒ is the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: âˆ‚**a**/âˆ‚**z** = **a** âŠ™ (1 **- a**)
  prefs: []
  type: TYPE_NORMAL
- en: This multiplies elements directly (âŠ™ means multiply each position).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/406536d3c65893436cf88c0b469d845e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Binary Cross-Entropy Loss Derivative**'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a single example with loss *L* = -[*y* log(Å·) + (1-*y*) log(1-*Å·*)]:'
  prefs: []
  type: TYPE_NORMAL
- en: âˆ‚*L*/âˆ‚*Å·* = -(*y*-*Å·*) / [*Å·*(1-*Å·*)]
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/145aab17537a82e33342299758155d04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Up to this point, we can summarized all the partial derivatives as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ecaacf0d1a77fe1296855018554dd45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image shows all the partial derivatives that weâ€™ve obtained so
    far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbe67994cd024b69583a9413901bca26.png)'
  prefs: []
  type: TYPE_IMG
- en: Chain Rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our network, changes flow through multiple steps: a weight affects its layerâ€™s
    output, which affects the next layer, and so on until the final error. The chain
    rule tells us to **multiply these step-by-step changes together** to find how
    each weight and bias affects the final error.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f8a5d158f28ec911745e2a4e658dc00.png)![](../Images/1f941aa47b9eaad7e064eb594ea454ce.png)'
  prefs: []
  type: TYPE_IMG
- en: Error Calculation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than directly computing weight and bias derivatives, we first calculate
    layer errors âˆ‚*L*/âˆ‚*zË¡* (the gradient with respect to pre-activation outputs).
    This makes it easier to then calculate how we should adjust the weights and biases
    in earlier layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6632ead66c14bdace2390adf44c9fd3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Weight gradients and bias gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using these layer errors and the chain rule, we can express the weight and
    bias gradients as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d83604e28f9ecfdd5471d925723a364.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradients show us how each value in our network affects our networkâ€™s error.
    We then make small changes to these values to help our network make better predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Weight Update'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Updating weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we know how each weight and bias affects the error (the gradients), we
    improve our network by adjusting these values in the opposite direction of their
    gradients. This reduces the networkâ€™s error step by step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56bd580089de99dc7ce5af8f7a7915d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Learning Rate and Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of making big changes all at once, we make small, careful adjustments.
    We use a number called the learning rate (*Î·*) to control how much we change each
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *Î·* is too big: The changes are too large and we might make things worse'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If *Î·* is too small: The changes are tiny and it takes too long to improve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This way of making small, controlled changes is called **Stochastic Gradient
    Descent (SGD)**. We can write it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/525bd2513df13222570baac0537d0393.png)'
  prefs: []
  type: TYPE_IMG
- en: The value of Î· (learning rate) is usually chosen to be small, typically ranging
    from 0.1 to 0.0001, to ensure stable learning.
  prefs: []
  type: TYPE_NORMAL
- en: We just saw how our network learns from **one example.** The network repeats
    all these steps for each example in our dataset, getting better with each round
    of practice
  prefs: []
  type: TYPE_NORMAL
- en: Summary of Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are all the steps we covered to train our network on a single example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf84eac8a9e5bace7343937bcfc86018.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling to Full Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Epoch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our network repeats these four steps â€” forward pass, loss calculation, backpropagation,
    and weight updates â€” for every example in our dataset. Going through all examples
    once is called **an epoch**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d454eb9ee57344d96212fd863f3b201.png)'
  prefs: []
  type: TYPE_IMG
- en: The network usually needs to see all examples many times to get good at its
    task, even up to 1000 times. Each time through helps it learn the patterns better.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd6f40a7e518f1a85a5e58d93fec6dae.png)'
  prefs: []
  type: TYPE_IMG
- en: Batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of learning from one example at a time, our network learns from small
    groups of examples (called **batches**) at once. This has several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Works faster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learns better patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makes steadier improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When working with batches, the network looks at all examples in the group before
    making changes. This gives better results than changing values after each single
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d12014b56111808330a0933a6d0f9811.png)'
  prefs: []
  type: TYPE_IMG
- en: Testing Step
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing Fully-trained Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training is done, our network is ready to make predictions on new examples
    it hasnâ€™t seen before. It uses the same steps as training, but **only needs to
    move forward** through the network to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Making Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When processing new data:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Input layer takes in the new values
  prefs: []
  type: TYPE_NORMAL
- en: '2\. At each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Multiplies by weights and adds biases'
  prefs: []
  type: TYPE_NORMAL
- en: '*Â·* Applies the activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Output layer generates predictions (e.g., probabilities between 0 and 1
    for binary classification)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa9a06f87d503e7635666859984dd39e.png)'
  prefs: []
  type: TYPE_IMG
- en: The prediction for ID 9 is 1 (YES).
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic Nature of Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When our network sees the same input twice, it will give the same answer both
    times (as long as we havenâ€™t changed its weights and biases). The networkâ€™s ability
    to handle new examples comes from its training, not from any randomness in making
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As our network practices with the examples again and again, it gets better
    at its task. It makes fewer mistakes over time, and its predictions get more accurate.
    This is how neural networks learn: look at examples, find mistakes, make small
    improvements, and repeat!'
  prefs: []
  type: TYPE_NORMAL
- en: ğŸŒŸ **Multilayer Perceptron Classifier Code Summary**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now letâ€™s see our neural network in action. Hereâ€™s some Python code that builds
    the network weâ€™ve been talking about, using the same structure and rules we just
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Want to Learn More?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Check out scikit-learnâ€™s official documentation of [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
    for more details and how to use it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This article uses Python 3.7 and scikit-learn 1.5, but the core ideas work with
    other versions too
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image Attribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All diagrams and technical illustrations in this article were created by the
    author using licensed design elements from Canva Pro under their commercial license
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Classification Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----0ae8100c5d1c--------------------------------)8
    stories![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Regression Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----0ae8100c5d1c--------------------------------)5
    stories![A cartoon doll with pigtails and a pink hat. This â€œdummyâ€ doll, with
    its basic design and heart-adorned shirt, visually represents the concept of a
    dummy regressor in machine. Just as this toy-like figure is a simplified, static
    representation of a person, a dummy regressor is a basic models serve as baselines
    for more sophisticated analyses.](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy
    Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----0ae8100c5d1c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----0ae8100c5d1c--------------------------------)4
    stories![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)'
  prefs: []
  type: TYPE_NORMAL
