- en: Multi-GPU Fine-tuning for Llama 3.1 70B with FSDP and QLoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08](https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What you can do with only 2x24 GB GPUs and a lot of CPU RAM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    ·8 min read·Aug 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d64c5415cadb5ca8010b4aa2d818a1f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated with DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning large language models (LLMs) with up to 35B parameters is relatively
    easy and cheap since it can be done with a single consumer GPU. Fine-tuning larger
    models with a single consumer GPU is, in theory, not impossible as we can offload
    parts of the model to the CPU memory. However, it would be extremely slow, even
    with high-end CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple GPUs is the only alternative to keep fine-tuning fast enough.
    A configuration with 2x24 GB GPUs opens a lot of possibilities. 48 GB of GPU memory
    is enough to fine-tune 70B models such as Llama 3 70B and Qwen2 72B.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I explain how to fine-tune 70B LLMs using only two GPUs thanks
    to FSDP and [QLoRA](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b).
  prefs: []
  type: TYPE_NORMAL
- en: I first explain what is FSDP and then we will see how to modify a standard QLoRA
    fine-tuning code to run it on multiple GPUs. For the experiments and demonstrations,
    I use Llama 3.1 70B but it would work similarly for other LLMs. For the hardware,
    I relied on 2 RTX 3090 GPUs provided by [RunPod (referral link)](https://runpod.io/?ref=1ip9lvtj).
    Using 2 RTX 4090 GPUs would be faster but more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also made a notebook implementing the code described in this article. It’s
    available here:'
  prefs: []
  type: TYPE_NORMAL
