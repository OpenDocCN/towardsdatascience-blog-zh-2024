- en: Multi-GPU Fine-tuning for Llama 3.1 70B with FSDP and QLoRA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多GPU微调Llama 3.1 70B模型，采用FSDP和QLoRA
- en: 原文：[https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08](https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08](https://towardsdatascience.com/multi-gpu-fine-tuning-for-llama-3-1-70b-with-fsdp-and-qlora-67a8a5b4f0d6?source=collection_archive---------1-----------------------#2024-08-08)
- en: What you can do with only 2x24 GB GPUs and a lot of CPU RAM
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用仅有的2块24 GB GPU和大量CPU内存，你可以做什么
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--67a8a5b4f0d6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    ·8 min read·Aug 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--67a8a5b4f0d6--------------------------------)
    ·8分钟阅读·2024年8月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d64c5415cadb5ca8010b4aa2d818a1f8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d64c5415cadb5ca8010b4aa2d818a1f8.png)'
- en: Generated with DALL-E
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DALL-E生成
- en: Fine-tuning large language models (LLMs) with up to 35B parameters is relatively
    easy and cheap since it can be done with a single consumer GPU. Fine-tuning larger
    models with a single consumer GPU is, in theory, not impossible as we can offload
    parts of the model to the CPU memory. However, it would be extremely slow, even
    with high-end CPUs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 微调最多35B参数的大型语言模型（LLMs）相对简单且廉价，因为只需要一块消费级GPU即可完成。理论上，使用单块消费级GPU微调更大的模型并非不可能，因为我们可以将模型的部分内容转移到CPU内存中。然而，即使使用高端CPU，这个过程将会极其缓慢。
- en: Using multiple GPUs is the only alternative to keep fine-tuning fast enough.
    A configuration with 2x24 GB GPUs opens a lot of possibilities. 48 GB of GPU memory
    is enough to fine-tune 70B models such as Llama 3 70B and Qwen2 72B.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU是保持微调速度足够快的唯一替代方案。配置为2块24 GB GPU的系统提供了很多可能性。48 GB的GPU内存足以微调70B模型，如Llama
    3 70B和Qwen2 72B。
- en: In this article, I explain how to fine-tune 70B LLMs using only two GPUs thanks
    to FSDP and [QLoRA](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将解释如何仅使用两块GPU，通过FSDP和[QLoRA](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)微调70B参数的LLM。
- en: I first explain what is FSDP and then we will see how to modify a standard QLoRA
    fine-tuning code to run it on multiple GPUs. For the experiments and demonstrations,
    I use Llama 3.1 70B but it would work similarly for other LLMs. For the hardware,
    I relied on 2 RTX 3090 GPUs provided by [RunPod (referral link)](https://runpod.io/?ref=1ip9lvtj).
    Using 2 RTX 4090 GPUs would be faster but more expensive.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先解释什么是FSDP，然后我们将看到如何修改标准的QLoRA微调代码，以便在多个GPU上运行。在实验和演示中，我使用了Llama 3.1 70B，但对其他LLM也能起到类似的效果。硬件方面，我依赖于[RunPod
    (推荐链接)](https://runpod.io/?ref=1ip9lvtj)提供的2块RTX 3090 GPU。使用2块RTX 4090 GPU会更快，但成本也更高。
- en: 'I also made a notebook implementing the code described in this article. It’s
    available here:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我还制作了一个笔记本，实施了本文中描述的代码。它可以在这里获取：
