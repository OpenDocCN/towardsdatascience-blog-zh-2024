["```py\nIf      A > 10     Then: y = class Y \nElseif  B < 19     Then: y = class X\nElseif  C * D > 44 Then: y = class Y\nElse                     y = class Z\n```", "```py\nA > 10\n| - LEAF: y = class Y\n| - B > 19\n    | (subtree related to C*D omitted)\n    | - LEAF: y = class X\n```", "```py\nThe higher A is, the more likely y is to be class X and less likely to be Z\nregardless of B, C, and D\n\nThe higher B is, the more likely y is to be class Y and less likely to be X, \nregardless of A, C, and D\n\nThe lower C is, the more likely y is to be class Z and less likely to be X, \nregardless of A, B, and D \n```", "```py\nThe higher A is, the more likely y is to be class X and less likely to be Z\nregardless of B, C, and D\n```", "```py\nThe higher A is, the more likely y is to be class X, \nregardless of B, C and D\n\nThe higher B is, up to 100.0, the more likely y is class Y, \nregardless of A, C and D \n\nThe higher B is, where B is 100.0 or more, the more likely y is to be class Z, \nregardless of A, C and D\n\nThe higher C * D is, the more likely y is class X, \nregardless of A and B.\n```", "```py\nif B > 100:\n  calculate each of and take the average estimate:\n  if A <= vs > 50: calculate the probabilities of X, Y, and Z in both cases\n  if B <= vs > 150: calculate the probabilities of X, Y, and Z in both cases\n  if C <= vs > 60: calculate the probabilities of X, Y, and Z in both cases\n  if D <= vs > 200: calculate the probabilities of X, Y, and Z in both cases\nelse (B <= 100):\n  calculate each of and take the average estimate:\n  if A <= vs > 50: calculate the probabilities of X, Y and Z in both cases\n  if B <= vs > 50: calculate the probabilities of X, Y and Z in both cases\n  if C <= vs > 60: calculate the probabilities of X, Y and Z in both cases\n  if D <= vs > 200: calculate the probabilities of X, Y and Z in both cases\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom AdditiveDecisionTree import AdditiveDecisionTreeClasssifier\n\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nadt = AdditiveDecisionTreeClasssifier()\nadt.fit(X_train, y_train)\ny_pred_test = adt.predict(X_test)\n```", "```py\nexp_arr = adt.get_explanations(X[:5], y[:5])\nfor exp in exp_arr: \n    print(\"\\n\")\n    print(exp)\n```", "```py\nInitial distribution of classes: [0, 1]: [159, 267]\n\nPrediction for row 0: 0 -- Correct\nPath: [0, 2, 6]\n\nmean concave points is greater than 0.04891999997198582 \n  (has value: 0.1471) --> (Class distribution: [146, 20]\n\nAND worst area is greater than 785.7999877929688 \n  (has value: 2019.0) --> (Class distribution: [133, 3]\n\nwhere the majority class is: 0\n```", "```py\nInitial distribution of classes: [0, 1]: [159, 267]\n\nPrediction for row 0: 1 -- Correct\nPath: [0, 1, 3]\n\nmean concave points is less than 0.04891999997198582 \n  (has value: 0.04781) --> (Class distribution: [13, 247]\n\nAND worst radius is less than 17.589999198913574 \n  (has value: 15.11) --> (Class distribution: [7, 245]\n\nAND  vote based on: \n  1: mean texture is less than 21.574999809265137 \n    (with value of 14.36)  --> (class distribution: [1, 209])\n  2: area error is less than 42.19000053405762 \n    (with value of 23.56)  --> (class distribution: [4, 243])\nThe class with the most votes is 1\n```"]