- en: Building Transformer Models for Proteins From Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/building-transformer-models-for-proteins-from-scratch-60884eab5cc8?source=collection_archive---------4-----------------------#2024-05-07](https://towardsdatascience.com/building-transformer-models-for-proteins-from-scratch-60884eab5cc8?source=collection_archive---------4-----------------------#2024-05-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Practical Guide to Building and Evaluating Protein Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@yuan_tian?source=post_page---byline--60884eab5cc8--------------------------------)[![Yuan
    Tian](../Images/7ed0754ea76b2fedf57fcc75e8967c53.png)](https://medium.com/@yuan_tian?source=post_page---byline--60884eab5cc8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--60884eab5cc8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--60884eab5cc8--------------------------------)
    [Yuan Tian](https://medium.com/@yuan_tian?source=post_page---byline--60884eab5cc8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--60884eab5cc8--------------------------------)
    ·13 min read·May 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on our understanding of protein science foundations from the [**previous
    article**](https://medium.com/@yuan_tian/a-primer-to-protein-science-1b6778ae995e),
    we’re now ready to explore the exciting intersection of AI/ML and protein science.
    This article focuses on transformer-based language models, the technology powering
    advanced chatbots like ChatGPT. I won’t spend too much space on the in-depth explanations
    of transformers. For that, I highly recommend the blog post “[The Illustrated
    Transformer](https://jalammar.github.io/illustrated-transformer/)” by Jay Alammar,
    which provides a detailed breakdown and beautiful illustrations. Instead, we’ll
    focus on the practical implementation of transformers for protein analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we will build a basic protein transformer model to predict the
    antigen specificity of antibody sequences. This project will enhance our understanding
    of transformer implementation and their potential applications within protein
    science.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers, introduced in the groundbreaking paper “[Attention Is All You
    Need](https://arxiv.org/abs/1706.03762),” are neural networks with encoder and
    decoder components. Models like BERT (Bidirectional Encoder Representations from
    Transformers) leverage the encoder for understanding language and excel at downstream
    tasks like classification. Here, we’ll implement and train an encoder-based model
    to classify antibodies as HIV-1 or SARS-CoV-2 specific (**Figure 1**).
  prefs: []
  type: TYPE_NORMAL
