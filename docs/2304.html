<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Implementing GraphReader with Neo4j and LangGraph</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Implementing GraphReader with Neo4j and LangGraph</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-graphreader-with-neo4j-and-langgraph-e4c73826a8b7?source=collection_archive---------1-----------------------#2024-09-21">https://towardsdatascience.com/implementing-graphreader-with-neo4j-and-langgraph-e4c73826a8b7?source=collection_archive---------1-----------------------#2024-09-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="0594" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Elevating RAG accuracy and performance by structuring long documents into explorable graphs and implementing graph-based agent systems</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://bratanic-tomaz.medium.com/?source=post_page---byline--e4c73826a8b7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tomaz Bratanic" class="l ep by dd de cx" src="../Images/d5821aa70918fcb3fc1ff0013497b3d5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*SnWQP0l4Vg9577WAErbjfw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e4c73826a8b7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://bratanic-tomaz.medium.com/?source=post_page---byline--e4c73826a8b7--------------------------------" rel="noopener follow">Tomaz Bratanic</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e4c73826a8b7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">23 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/fc66a01ca45698bc7ef7f7bcc2dbeed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-1rHYV4HkwWyRDgf"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">An AI agent traversing the graph as imagined by ChatGPT</figcaption></figure><p id="9e85" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Large Language Models (LLMs) are great at traditional NLP tasks like summarization and sentiment analysis but the stronger models also demonstrate promising reasoning abilities. LLM reasoning is often understood as the ability to tackle complex problems by formulating a plan, executing it, and assessing progress at each step. Based on this evaluation, they can adapt by revising the plan or taking alternative actions. The rise of agents is becoming an increasingly compelling approach to answering complex questions in RAG applications.</p><p id="7f63" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this blog post, we’ll explore the implementation of the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">GraphReader agent</a>. This agent is designed to retrieve information from a structured knowledge graph that follows a predefined schema. Unlike the typical graphs you might see in presentations, this one is closer to a document or <strong class="ne fr">lexical graph</strong>, containing documents, their chunks, and relevant metadata in the form of atomic facts.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk nz"><img src="../Images/71b960bfb63594c2d88dfcb91c5ccafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aws90bkQ8PPNYFFIbXdMpA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Generated knowledge graph following the GraphReader implementation. Image by author.</figcaption></figure><p id="3177" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The image above illustrates a knowledge graph, beginning at the top with a document node labeled <em class="oa">Joan of Arc</em>. This document is broken down into text chunks, represented by numbered circular nodes (0, 1, 2, 3), which are connected sequentially through <em class="oa">NEXT</em> relationships, indicating the order in which the chunks appear in the document. Below the text chunks, the graph further breaks down into atomic facts, where specific statements about the content are represented. Finally, at the bottom level of the graph, we see the key elements, represented as circular nodes with topics like <em class="oa">historical icons</em>, <em class="oa">Dane</em>, <em class="oa">French nation</em>, and <em class="oa">France</em>. These elements act as metadata, linking the facts to the broader themes and concepts relevant to the document.</p><p id="17a3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once we have constructed the knowledge graph, we will follow the implementation provided in the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">GraphReader paper</a>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ob"><img src="../Images/fc1f216a8c0999ddf602d0b3abc21211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PokbiJwPnZ6Fndfo9tPwbg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GraphReader agent implementation. Image from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="e693" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The agent exploration process involves initializing the agent with a rational plan and selecting initial nodes to start the search in a graph. The agent explores these nodes by first gathering atomic facts, then reading relevant text chunks, and updating its notebook. The agent can decide to explore more chunks, neighboring nodes, or terminate based on gathered information. When the agent decided to terminate, the answer reasoning step is executed to generate the final answer.</p><p id="ee26" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In this blog post, we will implement the GraphReader paper using <a class="af ny" href="https://neo4j.com/" rel="noopener ugc nofollow" target="_blank">Neo4j</a> as the storage layer and <a class="af ny" href="https://www.langchain.com/" rel="noopener ugc nofollow" target="_blank">LangChain</a> in combination with <a class="af ny" href="https://langchain-ai.github.io/langgraph/" rel="noopener ugc nofollow" target="_blank">LangGraph</a> to define the agent and its flow.</p><p id="c132" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code is available on <a class="af ny" href="https://github.com/tomasonjo/blogs/tree/master/graphreader" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p><h1 id="c0d4" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Environment Setup</h1><p id="9081" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">You need to setup a Neo4j to follow along with the examples in this blog post. The easiest way is to start a free instance on <a class="af ny" href="https://neo4j.com/cloud/platform/aura-graph-database/" rel="noopener ugc nofollow" target="_blank">Neo4j Aura</a>, which offers cloud instances of Neo4j database. Alternatively, you can also setup a local instance of the Neo4j database by downloading the <a class="af ny" href="https://neo4j.com/download/" rel="noopener ugc nofollow" target="_blank">Neo4j Desktop</a> application and creating a local database instance.</p><p id="f1a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following code will instantiate a LangChain wrapper to connect to Neo4j Database.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="594b" class="ph od fq pe b bg pi pj l pk pl">os.environ["NEO4J_URI"] = "bolt://localhost:7687"<br/>os.environ["NEO4J_USERNAME"] = "neo4j"<br/>os.environ["NEO4J_PASSWORD"] = "password"<br/><br/>graph = Neo4jGraph(refresh_schema=False)<br/><br/>graph.query("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE")<br/>graph.query("CREATE CONSTRAINT IF NOT EXISTS FOR (c:AtomicFact) REQUIRE c.id IS UNIQUE")<br/>graph.query("CREATE CONSTRAINT IF NOT EXISTS FOR (c:KeyElement) REQUIRE c.id IS UNIQUE")</span></pre><p id="80e7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, we have also added <a class="af ny" href="https://neo4j.com/docs/cypher-manual/current/constraints/" rel="noopener ugc nofollow" target="_blank">constraints</a> for the node types we will be using. The constraints ensure faster import and retrieval performance.</p><p id="02e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, you will require an OpenAI api key that you pass in the following code:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="3c7e" class="ph od fq pe b bg pi pj l pk pl">os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")</span></pre><h1 id="19ce" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Graph construction</h1><p id="c731" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">We will be using the <a class="af ny" href="https://en.wikipedia.org/wiki/Joan_of_Arc" rel="noopener ugc nofollow" target="_blank">Joan of Arc</a> Wikipedia page in this example. We will use LangChain built-in utility to retrieve the text.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="04ee" class="ph od fq pe b bg pi pj l pk pl">wikipedia = WikipediaQueryRun(<br/>    api_wrapper=WikipediaAPIWrapper(doc_content_chars_max=10000)<br/>)<br/>text = wikipedia.run("Joan of Arc")</span></pre><p id="aea9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As mentioned before, the GraphReader agent expects knowledge graph that contains chunks, related atomic facts, and key elements.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pm"><img src="../Images/ab7091ef9ac06f48d9e80d51b203a06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZU6kh8gAMkQjUiUTgaNFPQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">GraphReader knowledge graph construction. Image from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="9c36" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">First, the document is split into chunks. In the paper they maintained paragraph structure while chunking. However, that is hard to do in a generic way. Therefore, we will use naive chunking here.</p><p id="6d48" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next, each chunk is processed by the LLM to identify <strong class="ne fr">atomic facts</strong>, which are the smallest, indivisible units of information that capture core details. For instance, from the sentence “The CEO of Neo4j, which is in Sweden, is Emil Eifrem” an atomic fact could be broken down into something like “The CEO of Neo4j is Emil Eifrem.” and “Neo4j is in Sweden.” Each atomic fact is focused on one clear, standalone piece of information.</p><p id="2b04" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">From these atomic facts, <strong class="ne fr">key elements</strong> are identified. For the first fact, “The CEO of Neo4j is Emil Eifrem,” the key elements would be “CEO,” “Neo4j,” and “Emil Eifrem.” For the second fact, “Neo4j is in Sweden,” the key elements would be “Neo4j” and “Sweden.” These key elements are the essential nouns and proper names that capture the core meaning of each atomic fact.</p><p id="7064" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The prompt used to extract the graph are provided in the appendix of the paper.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pn"><img src="../Images/84336de7edc06097874bf0b875122f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2K7VoON6thak0TeQq2svw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for key element and atomic fact extraction. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="735f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The authors used prompt-based extraction, where you instruct the LLM what it should output and then implement a function that parses the information in a structured manner. My preference for extracting structured information is to use the <code class="cx po pp pq pe b">with_structured_output</code> method in LangChain, which utilizes the tools feature to extract structured information. This way, we can skip defining a custom parsing function.</p><p id="0fa1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is the prompt that we can use for extraction.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="743c" class="ph od fq pe b bg pi pj l pk pl">construction_system = """<br/>You are now an intelligent assistant tasked with meticulously extracting both key elements and<br/>atomic facts from a long text.<br/>1. Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,<br/>actions), and adjectives (e.g., states, feelings) that are pivotal to the text’s narrative.<br/>2. Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include<br/>propositions, theories, existences, concepts, and implicit elements like logic, causality, event<br/>sequences, interpersonal relationships, timelines, etc.<br/>Requirements:<br/>#####<br/>1. Ensure that all identified key elements are reflected within the corresponding atomic facts.<br/>2. You should extract key elements and atomic facts comprehensively, especially those that are<br/>important and potentially query-worthy and do not leave out details.<br/>3. Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He,<br/>She to actual names).<br/>4. Ensure that the key elements and atomic facts you extract are presented in the same language as<br/>the original text (e.g., English or Chinese).<br/>"""<br/><br/>construction_human = """Use the given format to extract information from the <br/>following input: {input}"""<br/><br/>construction_prompt = ChatPromptTemplate.from_messages(<br/>    [<br/>        (<br/>            "system",<br/>            construction_system,<br/>        ),<br/>        (<br/>            "human",<br/>            (<br/>                "Use the given format to extract information from the "<br/>                "following input: {input}"<br/>            ),<br/>        ),<br/>    ]<br/>)</span></pre><p id="212c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We have put the instruction in the system prompt, and then in the user message we provide relevant text chunks that need to be processed.</p><p id="d1f9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To define the desired output, we can use the Pydantic object definition.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="c378" class="ph od fq pe b bg pi pj l pk pl">class AtomicFact(BaseModel):<br/>    key_elements: List[str] = Field(description="""The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g.,<br/>actions), and adjectives (e.g., states, feelings) that are pivotal to the atomic fact's narrative.""")<br/>    atomic_fact: str = Field(description="""The smallest, indivisible facts, presented as concise sentences. These include<br/>propositions, theories, existences, concepts, and implicit elements like logic, causality, event<br/>sequences, interpersonal relationships, timelines, etc.""")<br/><br/>class Extraction(BaseModel):<br/>    atomic_facts: List[AtomicFact] = Field(description="List of atomic facts")</span></pre><p id="402b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We want to extract a list of atomic facts, where each atomic fact contains a string field with the fact, and a list of present key elements. It is important to add description to each element to get the best results.</p><p id="5fe9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we can combine it all in a chain.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="c4de" class="ph od fq pe b bg pi pj l pk pl">model = ChatOpenAI(model="gpt-4o-2024-08-06", temperature=0.1)<br/>structured_llm = model.with_structured_output(Extraction)<br/><br/>construction_chain = construction_prompt | structured_llm</span></pre><p id="ffdc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To put it all together, we’ll create a function that takes a single document, chunks it, extracts atomic facts and key elements, and stores the results into Neo4j.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="7d50" class="ph od fq pe b bg pi pj l pk pl">async def process_document(text, document_name, chunk_size=2000, chunk_overlap=200):<br/>    start = datetime.now()<br/>    print(f"Started extraction at: {start}")<br/>    text_splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)<br/>    texts = text_splitter.split_text(text)<br/>    print(f"Total text chunks: {len(texts)}")<br/>    tasks = [<br/>        asyncio.create_task(construction_chain.ainvoke({"input":chunk_text}))<br/>        for index, chunk_text in enumerate(texts)<br/>    ]<br/>    results = await asyncio.gather(*tasks)<br/>    print(f"Finished LLM extraction after: {datetime.now() - start}")<br/>    docs = [el.dict() for el in results]<br/>    for index, doc in enumerate(docs):<br/>        doc['chunk_id'] = encode_md5(texts[index])<br/>        doc['chunk_text'] = texts[index]<br/>        doc['index'] = index<br/>        for af in doc["atomic_facts"]:<br/>            af["id"] = encode_md5(af["atomic_fact"])<br/>    # Import chunks/atomic facts/key elements<br/>    graph.query(import_query, <br/>            params={"data": docs, "document_name": document_name})<br/>    # Create next relationships between chunks<br/>    graph.query("""MATCH (c:Chunk) WHERE c.document_name = $document_name<br/>WITH c ORDER BY c.index WITH collect(c) AS nodes<br/>UNWIND range(0, size(nodes) -2) AS index<br/>WITH nodes[index] AS start, nodes[index + 1] AS end<br/>MERGE (start)-[:NEXT]-&gt;(end)<br/>""",<br/>           params={"document_name":document_name})<br/>    print(f"Finished import at: {datetime.now() - start}")</span></pre><p id="ffe7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At a high level, this code processes a document by breaking it into chunks, extracting information from each chunk using an AI model, and storing the results in a graph database. Here’s a summary:</p><ol class=""><li id="04bf" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pr ps pt bk">It splits the document text into chunks of a specified size, allowing for some overlap. The chunk size of 2000 tokens is used by the authors in the paper.</li><li id="90b2" class="nc nd fq ne b go pu ng nh gr pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">For each chunk, it asynchronously sends the text to an LLM for extraction of atomic facts and key elements.</li><li id="2ea8" class="nc nd fq ne b go pu ng nh gr pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">Each chunk and fact is given a unique identifier using an <em class="oa">md5</em> encoding function.</li><li id="f57a" class="nc nd fq ne b go pu ng nh gr pv nj nk nl pw nn no np px nr ns nt py nv nw nx pr ps pt bk">The processed data is imported into a graph database, with relationships established between consecutive chunks.</li></ol><p id="d4f0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can now run this function on our Joan of Arc text.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="09e4" class="ph od fq pe b bg pi pj l pk pl">await process_document(text, "Joan of Arc", chunk_size=500, chunk_overlap=100)</span></pre><p id="f0f0" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We used a smaller chunk size because it’s a small document, and we want to have a couple of chunks for demonstration purposes. If you explore the graph in Neo4j Browser, you should see a similar visualization.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/c7d1df64f44e006c2f1a15fe931a071c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTypf1s6rKLFBajeKixeQQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Visualization of the generated graph. Image by author.</figcaption></figure><p id="e38a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">At the center of the structure is the document node (blue), which branches out to chunk nodes (pink). These chunk nodes, in turn, are linked to atomic facts (orange), each of which connects to key elements (green).</p><p id="2916" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s examine the constructed graph a bit. We’ll start of by examining the token count distribution of atomic facts.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="3631" class="ph od fq pe b bg pi pj l pk pl">def num_tokens_from_string(string: str) -&gt; int:<br/>    """Returns the number of tokens in a text string."""<br/>    encoding = tiktoken.encoding_for_model("gpt-4")<br/>    num_tokens = len(encoding.encode(string))<br/>    return num_tokens<br/><br/><br/>atomic_facts = graph.query("MATCH (a:AtomicFact) RETURN a.text AS text")<br/>df = pd.DataFrame.from_records(<br/>    [{"tokens": num_tokens_from_string(el["text"])} for el in atomic_facts]<br/>)<br/><br/>sns.histplot(df["tokens"])</span></pre><p id="ae8f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qa"><img src="../Images/f0a07fc6b51842c42a43c28cada39f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dk7tph4tor2B2idLtpaIDg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Distribution of token count for atomic facts. Image by author.</figcaption></figure><p id="e6eb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Atomic facts are relatively short, with the longest being only about 50 tokens. Let’s examine a couple to get a better idea.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="16fa" class="ph od fq pe b bg pi pj l pk pl">graph.query("""MATCH (a:AtomicFact) <br/>RETURN a.text AS text<br/>ORDER BY size(text) ASC LIMIT 3<br/>UNION ALL<br/>MATCH (a:AtomicFact) <br/>RETURN a.text AS text<br/>ORDER BY size(text) DESC LIMIT 3""")</span></pre><p id="747a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/54adffdbe7154d75edeaaf6756f05ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LtNhNjUoo96kDc-ke5mtqQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Atomic facts</figcaption></figure><p id="e005" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Some of the shortest facts lack context. For example, the original score and screenplay don’t directly mention which. Therefore, if we processed multiple documents, these atomic facts might be less helpful. This lack of context might be solved with additional prompt engineering.</p><p id="ea42" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s also examine the most frequent keywords.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="af2d" class="ph od fq pe b bg pi pj l pk pl">data = graph.query("""<br/>MATCH (a:KeyElement) <br/>RETURN a.id AS key, <br/>       count{(a)&lt;-[:HAS_KEY_ELEMENT]-()} AS connections<br/>ORDER BY connections DESC LIMIT 5""")<br/>df = pd.DataFrame.from_records(data)<br/>sns.barplot(df, x='key', y='connections')</span></pre><p id="b300" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/addf379cd500eae452e4e83e3a21be08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4J5sfG_BEq5ucD261VRUEg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Top five most mentioned key elements. Image by author.</figcaption></figure><p id="06dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Unsurprisingly, Joan of Arc is the most mentioned keyword or element. Following are broad keywords like film, English, and France. I suspect that if we parsed many documents, broad keywords would end up having a lot of connections, which might lead to some downstream problems that aren’t dealt with in the original implementation. Another minor problem is the non-determinism of the extraction, as the results will be slight different on every run.</p><p id="ccd8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Additionally, the authors employ key element normalization as described in <a class="af ny" href="https://arxiv.org/pdf/2308.07074" rel="noopener ugc nofollow" target="_blank">Lu et al. (2023)</a>, specifically using frequency filtering, rule, semantic, and association aggregation. In this implementation, we skipped this step.</p><h1 id="1d50" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">GraphReader Agent</h1><p id="b04d" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">We’re ready to implement GraphReader, a graph-based agent system. The agent starts with a couple of predefined steps, followed by the steps in which it can traverse the graph autonomously, meaning the agent decides the following steps and how to traverse the graph.</p><p id="7101" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here is the LangGraph visualization of the agent we will implement.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/b24f71a840150b0590f2177624343f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*brUZD1mkh9tDg8AwMEmxTw.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Agent workflow implementation in LangGraph. Image by author.</figcaption></figure><p id="1a8b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The process begins with a rational planning stage, after which the agent makes an initial selection of nodes (key elements) to work with. Next, the agent checks atomic facts linked to the selected key elements. Since all these steps are predefined, they are visualized with a full line.</p><p id="a927" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Depending on the outcome of the atomic fact check, the flow proceeds to either read relevant text chunks or explore the neighbors of the initial key elements in search of more relevant information. Here, the next step is conditional and based on the results of an LLM and is, therefore, visualized with a dotted line.</p><p id="bfd9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the chunk check stage, the LLM reads and evaluates whether the information gathered from the current text chunk is sufficient. Based on this evaluation, the LLM has a few options. It can decide to read additional text chunks if the information seems incomplete or unclear. Alternatively, the LLM may choose to explore neighboring key elements, looking for more context or related information that the initial selection might not have captured. If, however, the LLM determines that enough relevant information has been gathered, it will proceed directly to the answer reasoning step. At this point, the LLM generates the final answer based on the collected information.</p><p id="ba31" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Throughout this process, the agent dynamically navigates the flow based on the outcomes of the conditional checks, making decisions on whether to repeat steps or continue forward depending on the specific situation. This provides flexibility in handling different inputs while maintaining a structured progression through the steps.</p><p id="e81f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, we’ll go over the steps and implement them using LangGraph abstraction. You can learn more about LangGraph through <a class="af ny" href="https://academy.langchain.com/courses/intro-to-langgraph" rel="noopener ugc nofollow" target="_blank">LangChain’s academy course</a>.</p><h2 id="cd03" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">LangGraph state</h2><p id="309c" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">To build a LangGraph implementation, we start by defining a state passed along the steps in the flow.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="2a95" class="ph od fq pe b bg pi pj l pk pl">class InputState(TypedDict):<br/>    question: str<br/><br/>class OutputState(TypedDict):<br/>    answer: str<br/>    analysis: str<br/>    previous_actions: List[str]<br/><br/>class OverallState(TypedDict):<br/>    question: str<br/>    rational_plan: str<br/>    notebook: str<br/>    previous_actions: Annotated[List[str], add]<br/>    check_atomic_facts_queue: List[str]<br/>    check_chunks_queue: List[str]<br/>    neighbor_check_queue: List[str]<br/>    chosen_action: str</span></pre><p id="2356" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For more advanced use cases, multiple separate states can be used. In our implementation, we have separate input and output states, which define the input and output of the LangGraph, and a separate overall state, which is passed between steps.</p><p id="07ba" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">By default, the state is overwritten when returned from a node. However, you can define other operations. For example, with the <code class="cx po pp pq pe b">previous_actions</code> we define that the state is appended or added instead of overwritten.</p><p id="dd83" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The agent begins by maintaining a notebook to record supporting facts, which are eventually used to derive the final answer. Other states will be explained as we go along.</p><p id="5018" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s move on to defining the nodes in the LangGraph.</p><h2 id="5b9f" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Rational plan</h2><p id="09e0" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">In the rational plan step, the agent breaks the question into smaller steps, identifies the key information required, and creates a logical plan. The logical plan allows the agent to handle complex multi-step questions.</p><p id="de50" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">While the code is unavailable, all the prompts are in the appendix, so we can easily copy them.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qv"><img src="../Images/e33bbce0019256b2aad5cbb955cecfd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7vuWRCcIHrffm2b0oCbdpw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for rational plan. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="8c80" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The authors don’t explicitly state whether the prompt is provided in the system or user message. For the most part, I have decided to put the instructions as a system message.</p><p id="b0de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The following code shows how to construct a chain using the above rational plan as the system message.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="4f3f" class="ph od fq pe b bg pi pj l pk pl">rational_plan_system = """As an intelligent assistant, your primary objective is to answer the question by gathering<br/>supporting facts from a given article. To facilitate this objective, the first step is to make<br/>a rational plan based on the question. This plan should outline the step-by-step process to<br/>resolve the question and specify the key information required to formulate a comprehensive answer.<br/>Example:<br/>#####<br/>User: Who had a longer tennis career, Danny or Alice?<br/>Assistant: In order to answer this question, we first need to find the length of Danny’s<br/>and Alice’s tennis careers, such as the start and retirement of their careers, and then compare the<br/>two.<br/>#####<br/>Please strictly follow the above format. Let’s begin."""<br/><br/>rational_prompt = ChatPromptTemplate.from_messages(<br/>    [<br/>        (<br/>            "system",<br/>            rational_plan_system,<br/>        ),<br/>        (<br/>            "human",<br/>            (<br/>                "{question}"<br/>            ),<br/>        ),<br/>    ]<br/>)<br/><br/>rational_chain = rational_prompt | model | StrOutputParser()</span></pre><p id="e190" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now, we can use this chain to define a rational plan node. A node in LangGraph is a function that takes the state as input and updates it as output.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="e1a8" class="ph od fq pe b bg pi pj l pk pl">def rational_plan_node(state: InputState) -&gt; OverallState:<br/>    rational_plan = rational_chain.invoke({"question": state.get("question")})<br/>    print("-" * 20)<br/>    print(f"Step: rational_plan")<br/>    print(f"Rational plan: {rational_plan}")<br/>    return {<br/>        "rational_plan": rational_plan,<br/>        "previous_actions": ["rational_plan"],<br/>    }</span></pre><p id="5de3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The function starts by invoking the LLM chain, which produces the rational plan. We do a little printing for debugging and then update the state as the function’s output. I like the simplicity of this approach.</p><h2 id="443f" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Initial node selection</h2><p id="4d61" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">In the next step, we select the initial nodes based on the question and rational plan. The prompt is the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qw"><img src="../Images/e1462c0d6a9382ff40e6b394531bd9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_9RpDLTpI38WOTuDqiqYw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for initial node selection. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="de57" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The prompt starts by giving the LLM some context about the overall agent system, followed by the task instructions. The idea is to have the LLM select the top 10 most relevant nodes and score them. The authors simply put all the key elements from the database in the prompt for an LLM to select from. However, I think that approach doesn’t really scale. Therefore, we will create and use a vector index to retrieve a list of input nodes for the prompt.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="9e8a" class="ph od fq pe b bg pi pj l pk pl">neo4j_vector = Neo4jVector.from_existing_graph(<br/>    embedding=embeddings,<br/>    index_name="keyelements",<br/>    node_label="KeyElement",<br/>    text_node_properties=["id"],<br/>    embedding_node_property="embedding",<br/>    retrieval_query="RETURN node.id AS text, score, {} AS metadata"<br/>)<br/><br/>def get_potential_nodes(question: str) -&gt; List[str]:<br/>    data = neo4j_vector.similarity_search(question, k=50)<br/>    return [el.page_content for el in data]</span></pre><p id="70f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The <code class="cx po pp pq pe b">from_existing_graph</code> method pulls the defined <code class="cx po pp pq pe b">text_node_properties</code> from the graph and calculates embeddings where they are missing. Here, we simply embed the <code class="cx po pp pq pe b">id</code> property of <strong class="ne fr">KeyElement</strong> nodes.</p><p id="7376" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s define the chain. We’ll first copy the prompt.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="3621" class="ph od fq pe b bg pi pj l pk pl">initial_node_system = """<br/>As an intelligent assistant, your primary objective is to answer questions based on information<br/>contained within a text. To facilitate this objective, a graph has been created from the text,<br/>comprising the following elements:<br/>1. Text Chunks: Chunks of the original text.<br/>2. Atomic Facts: Smallest, indivisible truths extracted from text chunks.<br/>3. Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic<br/>facts derived from different text chunks.<br/>Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the<br/>rational plan, and a list of node key elements. These initial nodes are crucial because they are the<br/>starting point for searching for relevant information.<br/>Requirements:<br/>#####<br/>1. Once you have selected a starting node, assess its relevance to the potential answer by assigning<br/>a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,<br/>whereas a score of 0 suggests minimal relevance.<br/>2. Present each chosen starting node in a separate line, accompanied by its relevance score. Format<br/>each line as follows: Node: [Key Element of Node], Score: [Relevance Score].<br/>3. Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.<br/>4. In the user’s input, each line constitutes a node. When selecting the starting node, please make<br/>your choice from those provided, and refrain from fabricating your own. The nodes you output<br/>must correspond exactly to the nodes given by the user, with identical wording.<br/>Finally, I emphasize again that you need to select the starting node from the given Nodes, and<br/>it must be consistent with the words of the node you selected. Please strictly follow the above<br/>format. Let’s begin.<br/>"""<br/><br/>initial_node_prompt = ChatPromptTemplate.from_messages(<br/>    [<br/>        (<br/>            "system",<br/>            initial_node_system,<br/>        ),<br/>        (<br/>            "human",<br/>            (<br/>                """Question: {question}<br/>Plan: {rational_plan}<br/>Nodes: {nodes}"""<br/>            ),<br/>        ),<br/>    ]<br/>)</span></pre><p id="c9a4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Again, we put most of the instructions as the system message. Since we have multiple inputs, we can define them in the human message. However, we need a more structured output this time. Instead of writing a parsing function that takes in text and outputs a JSON, we can simply use the <code class="cx po pp pq pe b">use_structured_output</code>method to define the desired output structure.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="d775" class="ph od fq pe b bg pi pj l pk pl">class Node(BaseModel):<br/>    key_element: str = Field(description="""Key element or name of a relevant node""")<br/>    score: int = Field(description="""Relevance to the potential answer by assigning<br/>a score between 0 and 100. A score of 100 implies a high likelihood of relevance to the answer,<br/>whereas a score of 0 suggests minimal relevance.""")<br/><br/>class InitialNodes(BaseModel):<br/>    initial_nodes: List[Node] = Field(description="List of relevant nodes to the question and plan")<br/><br/>initial_nodes_chain = initial_node_prompt | model.with_structured_output(InitialNodes)</span></pre><p id="dddc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We want to output a list of nodes containing the key element and the score. We can easily define the output using a Pydantic model. Additionally, it is vital to add descriptions to each of the field, so we can guide the LLM as much as possible.</p><p id="3915" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The last thing in this step is to define the node as a function.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="9276" class="ph od fq pe b bg pi pj l pk pl">def initial_node_selection(state: OverallState) -&gt; OverallState:<br/>    potential_nodes = get_potential_nodes(state.get("question"))<br/>    initial_nodes = initial_nodes_chain.invoke(<br/>        {<br/>            "question": state.get("question"),<br/>            "rational_plan": state.get("rational_plan"),<br/>            "nodes": potential_nodes,<br/>        }<br/>    )<br/>    # paper uses 5 initial nodes<br/>    check_atomic_facts_queue = [<br/>        el.key_element<br/>        for el in sorted(<br/>            initial_nodes.initial_nodes,<br/>            key=lambda node: node.score,<br/>            reverse=True,<br/>        )<br/>    ][:5]<br/>    return {<br/>        "check_atomic_facts_queue": check_atomic_facts_queue,<br/>        "previous_actions": ["initial_node_selection"],<br/>    }</span></pre><p id="f0d1" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the initial node selection, we start by getting a list of potential nodes using the vector similarity search based on the input. An option is to use rational plan instead. The LLM is prompted to output the 10 most relevant nodes. However, the authors say that we should use only 5 initial nodes. Therefore, we simply order the nodes by their score and take the top 5 ones. We then update the <code class="cx po pp pq pe b">check_atomic_facts_queue</code> with the selected initial key elements.</p><h2 id="bf56" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Atomic fact check</h2><p id="91a4" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">In this step, we take the initial key elements and inspect the linked atomic facts. The prompt is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qx"><img src="../Images/b0bf14319eb92e8c735c357ce95cf88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i97x_dlD83wS0zsr_8KGbg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for exploring atomic facts. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="b10e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All prompts start by giving the LLM some context, followed by task instructions. The LLM is instructed to read the atomic facts and decide whether to read the linked text chunks or if the atomic facts are irrelevant, search for more information by exploring the neighbors. The last bit of the prompt is the output instructions. We will use the structured output method again to avoid manually parsing and structuring the output.</p><p id="17f2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since chains are very similar in their implementation, different only by prompts, we’ll avoid showing every definition in this blog post. However, we’ll look at the LangGraph node definitions to better understand the flow.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="6490" class="ph od fq pe b bg pi pj l pk pl">def atomic_fact_check(state: OverallState) -&gt; OverallState:<br/>    atomic_facts = get_atomic_facts(state.get("check_atomic_facts_queue"))<br/>    print("-" * 20)<br/>    print(f"Step: atomic_fact_check")<br/>    print(<br/>        f"Reading atomic facts about: {state.get('check_atomic_facts_queue')}"<br/>    )<br/>    atomic_facts_results = atomic_fact_chain.invoke(<br/>        {<br/>            "question": state.get("question"),<br/>            "rational_plan": state.get("rational_plan"),<br/>            "notebook": state.get("notebook"),<br/>            "previous_actions": state.get("previous_actions"),<br/>            "atomic_facts": atomic_facts,<br/>        }<br/>    )<br/><br/>    notebook = atomic_facts_results.updated_notebook<br/>    print(<br/>        f"Rational for next action after atomic check: {atomic_facts_results.rational_next_action}"<br/>    )<br/>    chosen_action = parse_function(atomic_facts_results.chosen_action)<br/>    print(f"Chosen action: {chosen_action}")<br/>    response = {<br/>        "notebook": notebook,<br/>        "chosen_action": chosen_action.get("function_name"),<br/>        "check_atomic_facts_queue": [],<br/>        "previous_actions": [<br/>            f"atomic_fact_check({state.get('check_atomic_facts_queue')})"<br/>        ],<br/>    }<br/>    if chosen_action.get("function_name") == "stop_and_read_neighbor":<br/>        neighbors = get_neighbors_by_key_element(<br/>            state.get("check_atomic_facts_queue")<br/>        )<br/>        response["neighbor_check_queue"] = neighbors<br/>    elif chosen_action.get("function_name") == "read_chunk":<br/>        response["check_chunks_queue"] = chosen_action.get("arguments")[0]<br/>    return response</span></pre><p id="e267" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The atomic fact check node starts by invoking the LLM to evaluate the atomic facts of the selected nodes. Since we are using the <code class="cx po pp pq pe b">use_structured_output</code> we can parse the updated notebook and the chosen action output in a straightforward manner. If the selected action is to get additional information by inspecting the neighbors, we use a function to find those neighbors and append them to the <code class="cx po pp pq pe b">check_atomic_facts_queue</code>. Otherwise, we append the selected chunks to the <code class="cx po pp pq pe b">check_chunks_queue</code>. We update the overall state by updating the notebook, queues, and the chosen action.</p><h2 id="8a08" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Text chunk check</h2><p id="b784" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">As you might imagine by the name of the LangGraph node, in this step, the LLM reads the selected text chunk and decides the best next step based on the provided information. The prompt is the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/fad9ce6c214faf445fa5e129744e7154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D0gQd9IjlnU_ktmlPd2crA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for exploring chunks. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="1548" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The LLM is instructed to read the text chunk and decide on the best approach. My gut feeling is that sometimes relevant information is at the start or the end of a text chunk, and parts of the information might be missing due to the chunking process. Therefore, the authors decided to give the LLM the option to read a previous or next chunk. If the LLM decides it has enough information, it can hop on to the final step. Otherwise, it has the option to search for more details using the <code class="cx po pp pq pe b">search_more</code>function.</p><p id="ac28" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Again, we’ll just look at the LangGraph node function.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="5782" class="ph od fq pe b bg pi pj l pk pl">def chunk_check(state: OverallState) -&gt; OverallState:<br/>    check_chunks_queue = state.get("check_chunks_queue")<br/>    chunk_id = check_chunks_queue.pop()<br/>    print("-" * 20)<br/>    print(f"Step: read chunk({chunk_id})")<br/><br/>    chunks_text = get_chunk(chunk_id)<br/>    read_chunk_results = chunk_read_chain.invoke(<br/>        {<br/>            "question": state.get("question"),<br/>            "rational_plan": state.get("rational_plan"),<br/>            "notebook": state.get("notebook"),<br/>            "previous_actions": state.get("previous_actions"),<br/>            "chunk": chunks_text,<br/>        }<br/>    )<br/><br/>    notebook = read_chunk_results.updated_notebook<br/>    print(<br/>        f"Rational for next action after reading chunks: {read_chunk_results.rational_next_move}"<br/>    )<br/>    chosen_action = parse_function(read_chunk_results.chosen_action)<br/>    print(f"Chosen action: {chosen_action}")<br/>    response = {<br/>        "notebook": notebook,<br/>        "chosen_action": chosen_action.get("function_name"),<br/>        "previous_actions": [f"read_chunks({chunk_id})"],<br/>    }<br/>    if chosen_action.get("function_name") == "read_subsequent_chunk":<br/>        subsequent_id = get_subsequent_chunk_id(chunk_id)<br/>        check_chunks_queue.append(subsequent_id)<br/>    elif chosen_action.get("function_name") == "read_previous_chunk":<br/>        previous_id = get_previous_chunk_id(chunk_id)<br/>        check_chunks_queue.append(previous_id)<br/>    elif chosen_action.get("function_name") == "search_more":<br/>        # Go over to next chunk<br/>        # Else explore neighbors<br/>        if not check_chunks_queue:<br/>            response["chosen_action"] = "search_neighbor"<br/>            # Get neighbors/use vector similarity<br/>            print(f"Neighbor rational: {read_chunk_results.rational_next_move}")<br/>            neighbors = get_potential_nodes(<br/>                read_chunk_results.rational_next_move<br/>            )<br/>            response["neighbor_check_queue"] = neighbors<br/><br/>    response["check_chunks_queue"] = check_chunks_queue<br/>    return response</span></pre><p id="5a05" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We start by popping a chunk ID from the queue and retrieving its text from the graph. Using the retrieved text and additional information from the overall state of the LangGraph system, we invoke the LLM chain. If the LLM decides it wants to read previous or subsequent chunks, we append their IDs to the queue. On the other hand, if the LLM chooses to search for more information, we have two options. If there are any other chunks to read in the queue, we move to reading them. Otherwise, we can use the vector search to get more relevant key elements and repeat the process by reading their atomic facts and so on.</p><p id="4ed7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The paper is slightly dubious about the <code class="cx po pp pq pe b">search_more</code> function. On the one hand, it states that the <code class="cx po pp pq pe b">search_more</code> function can only read other chunks in the queue. On the other hand, in their example in the appendix, the function clearly explores the neighbors.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qz"><img src="../Images/6f7ac4164fc9cba89c57eb2e6ac9d0b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRlXK18Ig9lPXPQipXJzzQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Example action history. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="3f08" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">To clarify, I emailed the authors, and they confirmed that the <code class="cx po pp pq pe b">search_more</code>function first tries to go through additional chunks in the queue. If none are present, it moves on to exploring the neighbors. Since how to explore the neighbors isn’t explicitly defined, we again use the vector similarity search to find potential nodes.</p><h2 id="942f" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Neighbor selection</h2><p id="3ee1" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">When the LLM decides to explore the neighbors, we have helper functions to find potential key elements to explore. However, we don’t explore all of them. Instead, an LLM decides which of them is worth exploring, if any. The prompt is the following:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ra"><img src="../Images/f9aadd49fa519cb33b6f7a786876a749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0KAjbzW7sFYEwe5potaMjQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for exploring neighbors. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="29cd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Based on the provided potential neighbors, the LLM can decide which to explore. If none are worth exploring, it can decide to terminate the flow and move on to the answer reasoning step.</p><p id="0afa" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code is:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="b1cf" class="ph od fq pe b bg pi pj l pk pl">def neighbor_select(state: OverallState) -&gt; OverallState:<br/>    print("-" * 20)<br/>    print(f"Step: neighbor select")<br/>    print(f"Possible candidates: {state.get('neighbor_check_queue')}")<br/>    neighbor_select_results = neighbor_select_chain.invoke(<br/>        {<br/>            "question": state.get("question"),<br/>            "rational_plan": state.get("rational_plan"),<br/>            "notebook": state.get("notebook"),<br/>            "nodes": state.get("neighbor_check_queue"),<br/>            "previous_actions": state.get("previous_actions"),<br/>        }<br/>    )<br/>    print(<br/>        f"Rational for next action after selecting neighbor: {neighbor_select_results.rational_next_move}"<br/>    )<br/>    chosen_action = parse_function(neighbor_select_results.chosen_action)<br/>    print(f"Chosen action: {chosen_action}")<br/>    # Empty neighbor select queue<br/>    response = {<br/>        "chosen_action": chosen_action.get("function_name"),<br/>        "neighbor_check_queue": [],<br/>        "previous_actions": [<br/>            f"neighbor_select({chosen_action.get('arguments', [''])[0] if chosen_action.get('arguments', ['']) else ''})"<br/>        ],<br/>    }<br/>    if chosen_action.get("function_name") == "read_neighbor_node":<br/>        response["check_atomic_facts_queue"] = [<br/>            chosen_action.get("arguments")[0]<br/>        ]<br/>    return response</span></pre><p id="0e1b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Here, we execute the LLM chain and parse results. If the chosen action is to explore any neighbors, we add them to the <code class="cx po pp pq pe b">check_atomic_facts_queue</code> .</p><h2 id="df0b" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Answer reasoning</h2><p id="fec4" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The last step in our flow is to ask the LLM to construct the final answer based on the collected information in the notebook. The prompt is:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qy"><img src="../Images/52228770f9c551cbed8ed9778f3e14f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODMm3uj20jL6Al-WQqDwVw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The prompt for answer reasoning. Taken from the <a class="af ny" href="https://arxiv.org/abs/2406.14550" rel="noopener ugc nofollow" target="_blank">paper</a> with authors’ permission.</figcaption></figure><p id="1fcb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This node implementation is fairly straightforward as you can see by the code:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="9cd9" class="ph od fq pe b bg pi pj l pk pl">def answer_reasoning(state: OverallState) -&gt; OutputState:<br/>    print("-" * 20)<br/>    print("Step: Answer Reasoning")<br/>    final_answer = answer_reasoning_chain.invoke(<br/>        {"question": state.get("question"), "notebook": state.get("notebook")}<br/>    )<br/>    return {<br/>        "answer": final_answer.final_answer,<br/>        "analysis": final_answer.analyze,<br/>        "previous_actions": ["answer_reasoning"],<br/>    }</span></pre><p id="51e6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We simply input the original question and the notebook with the collected information to the chain and ask it to formulate the final answer and provide the explanation in the analysis part.</p><h2 id="2cee" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">LangGraph flow definition</h2><p id="150b" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">The only thing left is to define the LangGraph flow and how it should traverse between the nodes. I am quite fond of the simple approach the LangChain team has chosen.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="bf41" class="ph od fq pe b bg pi pj l pk pl">langgraph = StateGraph(OverallState, input=InputState, output=OutputState)<br/>langgraph.add_node(rational_plan_node)<br/>langgraph.add_node(initial_node_selection)<br/>langgraph.add_node(atomic_fact_check)<br/>langgraph.add_node(chunk_check)<br/>langgraph.add_node(answer_reasoning)<br/>langgraph.add_node(neighbor_select)<br/><br/>langgraph.add_edge(START, "rational_plan_node")<br/>langgraph.add_edge("rational_plan_node", "initial_node_selection")<br/>langgraph.add_edge("initial_node_selection", "atomic_fact_check")<br/>langgraph.add_conditional_edges(<br/>    "atomic_fact_check",<br/>    atomic_fact_condition,<br/>)<br/>langgraph.add_conditional_edges(<br/>    "chunk_check",<br/>    chunk_condition,<br/>)<br/>langgraph.add_conditional_edges(<br/>    "neighbor_select",<br/>    neighbor_condition,<br/>)<br/>langgraph.add_edge("answer_reasoning", END)<br/><br/>langgraph = langgraph.compile()</span></pre><p id="a525" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We begin by defining the state graph object, where we can define the information passed along in the LangGraph. Each node is simply added with the <code class="cx po pp pq pe b">add_node</code> method. Normal edges, where one step always follows the other, can be added with a <code class="cx po pp pq pe b">add_edge</code> method. On the other hand, if the traversals is dependent on previous actions, we can use the <code class="cx po pp pq pe b">add_conditional_edge</code> and pass in the function that selects the next node. For example, the <code class="cx po pp pq pe b">atomic_fact_condition</code> looks like this:</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="7a40" class="ph od fq pe b bg pi pj l pk pl">def atomic_fact_condition(<br/>    state: OverallState,<br/>) -&gt; Literal["neighbor_select", "chunk_check"]:<br/>    if state.get("chosen_action") == "stop_and_read_neighbor":<br/>        return "neighbor_select"<br/>    elif state.get("chosen_action") == "read_chunk":<br/>        return "chunk_check"</span></pre><p id="9c60" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see, it’s about as simple as it gets to define the conditional edge.</p><h2 id="fdbd" class="qe od fq bf oe qf qg qh oh qi qj qk ok nl ql qm qn np qo qp qq nt qr qs qt qu bk">Evaluation</h2><p id="3600" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">Finally we can test our implementation on a couple of questions. Let’s begin with a simple one.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="9875" class="ph od fq pe b bg pi pj l pk pl">langgraph.invoke({"question":"Did Joan of Arc lose any battles?"})</span></pre><p id="aeeb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rb"><img src="../Images/19397837792c7ab4322659d6ef31586e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNKNrPqv7gxux5Cx0GXnmw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><p id="e4f3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The agent begins by forming a rational plan to identify the battles Joan of Arc participated in during her military career and to determine whether any were lost. After setting this plan, it moves to an atomic fact check about key battles such as the Siege of Orléans, the Siege of Paris, and La Charité. Rather than expanding the graph, the agent directly confirms the facts it needs. It reads text chunks that provide further details on Joan of Arc’s unsuccessful campaigns, particularly the failed Siege of Paris and La Charité. Since this information answers the question about whether Joan lost any battles, the agent stops here without expanding its exploration further. The process concludes with a final answer, confirming that Joan did indeed lose some battles, notably at Paris and La Charité, based on the evidence gathered.</p><p id="dade" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s now throw it a curveball.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="d8ed" class="ph od fq pe b bg pi pj l pk pl">langgraph.invoke({"question":"What is the weather in Spain?"})</span></pre><p id="d80c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rc"><img src="../Images/00214edcb3f1a075f88bcac01ca2a835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nT2nLlPdX1_rBzWIhTpC6A.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><p id="7080" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After the rational plan, the agent selected the initial key elements to explore. However, the issue is that none of these key elements exists in the database, and the LLM simply hallucinated them. Maybe some prompt engineering could solve hallucinations, but I haven’t tried. One thing to note is that it’s not that terrible, as these key elements don’t exist in the database, so we can’t pull any relevant information. Since the agent didn’t get any relevant data, it searched for more information. However, none of the neighbors are relevant either, so the process is stopped, letting the user know that the information is unavailable.</p><p id="2038" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s try a multi-hop question.</p><pre class="mm mn mo mp mq pd pe pf bp pg bb bk"><span id="9dd6" class="ph od fq pe b bg pi pj l pk pl">langgraph.invoke(<br/>  {"question":"Did Joan of Arc visit any cities in early life where she won battles later?"})</span></pre><p id="2c50" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><em class="oa">Results</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rd"><img src="../Images/1694c03f212a1965a9d92ae832f1a378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qPZsiURXPF52WjiKAvt9Og.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author.</figcaption></figure><p id="8498" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It’s a bit too much to copy the whole flow, so I copied only the answer part. The flow for this questions is quite non-deterministic and very dependent on the model being used. It’s kind of funny, but as I was testing the newer the model, the worse it performed. So the GPT-4 was the best (also used in this example), followed by GPT-4-turbo, and the last place goes to GPT-4o.</p><h1 id="897c" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Summary</h1><p id="1ccd" class="pw-post-body-paragraph nc nd fq ne b go oy ng nh gr oz nj nk nl pa nn no np pb nr ns nt pc nv nw nx fj bk">I’m very excited about GraphReader and similar approaches, specifically because I think such an approach to (Graph)RAG can be pretty generic and applied to any domain. Additionally, you can avoid the whole graph modeling part as the graph schema is static, allowing the graph agent to traverse it using predefined functions.</p><p id="18c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We discussed some issues with this implementation along the way. For example, the graph construction on many documents might result in broad key elements ending up as supernodes, and sometimes, the atomic facts don’t contain the full context.</p><p id="fb27" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The retriever part is super reliant on extracted and selected key elements. In the original implementation, they put all the key elements in the prompt to choose from. However, I doubt that that approach scales well. Perhaps we also need an additional function to allow the agent to search for more information in other ways than just to explore the neighbor key elements.</p><p id="3335" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Lastly, the agent system is highly dependent on the performance of the LLM. Based on my testing, the best model from OpenAI is the original GPT-4, which is funny as it’s the oldest. I haven’t tested the o1, though.</p><p id="ca0f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All in all, I am excited to explore more of these document graphs implementations, where metadata is extracted from text chunk and used to navigate the information better. Let me know if you have any ideas how to improve this implementation or have any other you like.</p><p id="3a33" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As always, the code is available on <a class="af ny" href="https://github.com/tomasonjo/blogs/tree/master/graphreader" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></div></div></div></div>    
</body>
</html>