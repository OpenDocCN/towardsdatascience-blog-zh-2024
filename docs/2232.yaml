- en: Transformer? Diffusion? Transfusion!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12](https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A gentle introduction to the latest multi-modal transfusion model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    ·6 min read·Sep 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, Meta and Waymo released their latest paper — [*Transfusion: Predict
    the Next Token and Diffuse Images with One Multi-Modal Model*](https://www.arxiv.org/pdf/2408.11039)*,*
    which integrates the popular transformer model with the diffusion model for multi-modal
    training and prediction purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like Meta’s [previous work](https://arxiv.org/pdf/2405.09818), the Transfusion
    model is based on the [Llama architecture](https://arxiv.org/abs/2302.13971) with
    early fusion, which takes both the text token sequence and the image token sequence
    and uses a single transformer model to generate the prediction. But different
    from previous art, the Transfusion model addresses the image tokens differently:'
  prefs: []
  type: TYPE_NORMAL
- en: The image token sequence is generated by a pre-trained Variational Auto-Encoder
    part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The transformer attention for the image sequence is bi-directional rather than
    causal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/56e8aa62586af6fcaf8b4b136e95f299.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transfusion model architecture with pre-training tasks. The text pretraining
    is the next word prediction task. The image is pretraining is a denoising diffusion
    task. Image source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss the following in detail. We’ll first review the basics, like auto-regressive
    and diffusion models, then dive into the Transfusion architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Auto-regressive Models**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowadays, large language models (LLMs) are primarily based on transformer architectures,
    which were proposed in the [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    paper in 2017\. The transformer architecture contains two parts: the encoder and
    the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f406a6310d469c83c6397b89d15ec541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transformer architecture. Left — Encoder; Right — Decoder. Image source: [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Masked Language Models like BERT use the encoder part pre-trained with randomly
    bidirectional masked token prediction tasks (and next sentence prediction). For
    auto-regressive models like the latest LLMs, the decoder part is usually trained
    on the next token prediction task, where the LM loss is minimized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c05b3937ed1c7f83c91ecda23c4f01e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  prefs: []
  type: TYPE_NORMAL
- en: In the equation above, \theta is the model parameter set, and y_i is the token
    at index i in a sequence of length n. y<i are all the tokens before y_i.
  prefs: []
  type: TYPE_NORMAL
- en: '**Diffusion Models**'
  prefs: []
  type: TYPE_NORMAL
- en: What is the diffusion model? It is a series of deep learning models commonly
    used in computer vision (especially for medical image analysis) for image generation/denoising
    and other purposes. One of the most well-known diffusion models is the DDPM, which
    is from the [Denoising diffusion probabilistic models](https://arxiv.org/pdf/2006.11239)
    paper published in 2020\. The model is a parameterized Markov chain containing
    a backward and forward transition, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b22249fb84cdffa38178102bb5a7d39f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The diffusion model is a bi-directional Markov chain. Image source: [https://arxiv.org/pdf/2006.11239](https://arxiv.org/pdf/2006.11239)'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is a Markov chain? It’s a statistical process in which the current step
    only relies on the previous step, and the reverse is vice versa. By assuming a
    Markov process, the model can start with a clean image by iteratively adding Gaussian
    noise in the forward process (right -> left in the figure above) and iteratively
    “learn” the noise by using a Unet-based architecture in the reverse process (left
    -> right in the figure above). That’s why we can sometimes see the diffusion model
    as a generative model (when used from left to right) and sometimes as a denoising
    model (when used from right to left). The DDPM loss is given below, where the
    theta is the model parameter set, \epsilon is the known noise, and the \epsilon_theta
    is the noise estimated by a deep learning model (usually a UNet):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7de90ea23694edffed94c90c0409641f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diffusion Model in the Latent Space**'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of diffusion was further extended to the latent space in the [CVPR’22
    paper](https://arxiv.org/pdf/2112.10752), where the images are first “compressed”
    onto the latent space by using the encoder part of a pre-trained [Variational
    Auto Encoder](https://arxiv.org/pdf/1312.6114) (VAE). Then, the diffusion and
    reverse processes are performed on the latent space and mapped back to pixel space
    using the decoder part of the VAE. This could largely improve the learning speed
    and efficiency, as most calculations are performed in a lower dimensional space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9fad25cf2eddb54eeae02027cedab2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Latent diffusion model architecture. The \Epsilon and D are encoders and decoders
    individually. Image source: [https://arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752)'
  prefs: []
  type: TYPE_NORMAL
- en: '**VAE-based Image Transfusion**'
  prefs: []
  type: TYPE_NORMAL
- en: The core part of the Transfusion model is the fusion between the diffusion and
    the transformer for input images. First, an image is divided into a sequence of
    8*8 patches; each patch is passed into a pre-trained VAE encoder to “compress”
    into an 8-element latent vector representation. Then, noise is added to the latent
    representation and further processed by a linear layer/U-Net encoder to generate
    the “noisy” x_t. Third, the transformer model processes the sequence of noisy
    latent representations. Last, the outputs are reversely processed by another linear/U-Net
    decoder before using a VAE decoder to generate the “true” x_0 image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56fad8e93f71ed2b84dcbf2d20796ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Diffusion module part for image input. The noise is added to VAE-encoded embedding.
    Image souce: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  prefs: []
  type: TYPE_NORMAL
- en: In the actual implementation, the beginning of the image (BOI) token and the
    end of the image (EOI) token are padded to both sides of the image representation
    sequence before concatenating the text tokens. Self-attention for image training
    is bi-directional attention, while self-attention for text tokens is causal. At
    the training stage, the loss for the image sequence is DDPM loss, while the rest
    of the text tokens use the LM loss.
  prefs: []
  type: TYPE_NORMAL
- en: So why bother? Why do we need such a complicated procedure for processing image
    patch tokens? The paper explains that the token space for text and images is different.
    **While the text tokens are discrete, the image tokens/patches are naturally continuous**.
    In the previous art, image tokens need to be “discretized” before fusing into
    the transformer model, while integrating the diffusion model directly could resolve
    this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compare with state-of-the-art**'
  prefs: []
  type: TYPE_NORMAL
- en: The primary multi-modal model the paper compares to is the [Chameleon model](https://arxiv.org/abs/2405.09818),
    which Meta proposed earlier this year. Here, we compare the difference between
    architecture and training set size between the Chameleon-7B and Transfusion-7B.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64436cc8da1edbde103d95a46a0605f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Arechitecture and training difference between Chameleon 7B and Transfusion 7B.
    Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The paper lists the performance comparison over the Llama2 pre-training suite
    accuracy, COCO zero-shot Frechet Inception Distance (FID) and GenEval benchmark.
    We can see that the Transfusion performs much better than Chameleon on the image-related
    benchmarks (COCO and Gen) while losing very little margin compared to Chameleon,
    with the same amount of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e202a86699418ec3765bc3b34f706bdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Comments.**'
  prefs: []
  type: TYPE_NORMAL
- en: Although the idea of the paper is super interesting, the “Diffusion” part of
    the Transfusion is hardly an actual Diffusion, as there are only two timestamps
    in the Markov process. Besides, the pre-trained VAE makes the model no longer
    strictly end-to-end. Also, the VAE + Linear/UNet + Transformer Encoder + Linear/UNet
    + VAE design looks so complicated, which makes the audience can’t help but ask,
    is there a more elegant way to implement this idea? Besides, I previously wrote
    about the [latest publication from Apple](https://medium.com/towards-data-science/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1)
    on the generalization benefits of using autoregressive modelling on images, so
    it might be interesting to give a second thought to the “MIM + autoregressive”
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: If you find this post interesting and would like to discuss it, you’re welcome
    to leave a comment, and I’m happy to further the discussion there :)
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhou et al., Transfusion: Predict the Next Token and Diffuse Images with One
    Multi-Modal Model. arXiv 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al., Llama: Open and efficient foundation language models. arXiv
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al., High-resolution image synthesis with latent diffusion models.
    CVPR 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al., Denoising diffusion probabilistic models. NeurIPS 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, Attention is all you need. NeurIPS 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma, Auto-encoding variational bayes. arXiv preprint 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
