- en: Transformer? Diffusion? Transfusion!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器？扩散？输血！
- en: 原文：[https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12](https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12](https://towardsdatascience.com/transformer-diffusion-transfusion-d18d219f2a12?source=collection_archive---------4-----------------------#2024-09-12)
- en: A gentle introduction to the latest multi-modal transfusion model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对最新的多模态输血模型的温和介绍
- en: '[](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Mengliu
    Zhao](../Images/0b950a0785fa065db3319ed5be4a91de.png)](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    [Mengliu Zhao](https://mengliuz.medium.com/?source=post_page---byline--d18d219f2a12--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    ·6 min read·Sep 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d18d219f2a12--------------------------------)
    ·阅读时间6分钟·2024年9月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Recently, Meta and Waymo released their latest paper — [*Transfusion: Predict
    the Next Token and Diffuse Images with One Multi-Modal Model*](https://www.arxiv.org/pdf/2408.11039)*,*
    which integrates the popular transformer model with the diffusion model for multi-modal
    training and prediction purposes.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Meta和Waymo发布了他们的最新论文——[*Transfusion: 用一个多模态模型预测下一个令牌并扩散图像*](https://www.arxiv.org/pdf/2408.11039)*，*该论文将流行的变换器模型与扩散模型结合，用于多模态训练和预测。'
- en: 'Like Meta’s [previous work](https://arxiv.org/pdf/2405.09818), the Transfusion
    model is based on the [Llama architecture](https://arxiv.org/abs/2302.13971) with
    early fusion, which takes both the text token sequence and the image token sequence
    and uses a single transformer model to generate the prediction. But different
    from previous art, the Transfusion model addresses the image tokens differently:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与Meta的[先前工作](https://arxiv.org/pdf/2405.09818)类似，Transfusion模型基于[Llama架构](https://arxiv.org/abs/2302.13971)，采用早期融合，既处理文本令牌序列，也处理图像令牌序列，并使用单一的变换器模型来生成预测。但与以往的工作不同，Transfusion模型对图像令牌的处理方式有所不同：
- en: The image token sequence is generated by a pre-trained Variational Auto-Encoder
    part.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像令牌序列由预训练的变分自编码器部分生成。
- en: The transformer attention for the image sequence is bi-directional rather than
    causal.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像序列的变换器注意力是双向的，而非因果的。
- en: '![](../Images/56e8aa62586af6fcaf8b4b136e95f299.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56e8aa62586af6fcaf8b4b136e95f299.png)'
- en: 'Transfusion model architecture with pre-training tasks. The text pretraining
    is the next word prediction task. The image is pretraining is a denoising diffusion
    task. Image source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Transfusion模型架构与预训练任务。文本预训练是下一个词预测任务。图像预训练是去噪扩散任务。图像来源：[https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)
- en: Let’s discuss the following in detail. We’ll first review the basics, like auto-regressive
    and diffusion models, then dive into the Transfusion architecture.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论以下内容。我们将首先回顾基础知识，例如自回归和扩散模型，然后深入研究Transfusion架构。
- en: '**Auto-regressive Models**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**自回归模型**'
- en: 'Nowadays, large language models (LLMs) are primarily based on transformer architectures,
    which were proposed in the [Attention is All You Need](https://arxiv.org/abs/1706.03762)
    paper in 2017\. The transformer architecture contains two parts: the encoder and
    the decoder.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大型语言模型（LLMs）主要基于变换器架构，该架构在2017年的[《Attention is All You Need》](https://arxiv.org/abs/1706.03762)论文中提出。变换器架构包含两部分：编码器和解码器。
- en: '![](../Images/f406a6310d469c83c6397b89d15ec541.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f406a6310d469c83c6397b89d15ec541.png)'
- en: 'Transformer architecture. Left — Encoder; Right — Decoder. Image source: [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构。左—编码器；右—解码器。图像来源：[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)
- en: 'Masked Language Models like BERT use the encoder part pre-trained with randomly
    bidirectional masked token prediction tasks (and next sentence prediction). For
    auto-regressive models like the latest LLMs, the decoder part is usually trained
    on the next token prediction task, where the LM loss is minimized:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 像BERT这样的掩码语言模型使用预训练的编码器部分，通过随机双向掩码令牌预测任务（以及下一句预测）进行训练。而像最新的LLM这样的自回归模型，解码器部分通常通过下一令牌预测任务进行训练，其中语言模型的损失被最小化：
- en: '![](../Images/1c05b3937ed1c7f83c91ecda23c4f01e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c05b3937ed1c7f83c91ecda23c4f01e.png)'
- en: 'Equation source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 方程来源：[https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)
- en: In the equation above, \theta is the model parameter set, and y_i is the token
    at index i in a sequence of length n. y<i are all the tokens before y_i.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的方程中，θ是模型参数集，y_i是长度为n的序列中索引为i的令牌，y<i是所有位于y_i之前的令牌。
- en: '**Diffusion Models**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**扩散模型**'
- en: What is the diffusion model? It is a series of deep learning models commonly
    used in computer vision (especially for medical image analysis) for image generation/denoising
    and other purposes. One of the most well-known diffusion models is the DDPM, which
    is from the [Denoising diffusion probabilistic models](https://arxiv.org/pdf/2006.11239)
    paper published in 2020\. The model is a parameterized Markov chain containing
    a backward and forward transition, as shown below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是什么？它是一系列常用于计算机视觉（尤其是医学图像分析）中的深度学习模型，主要用于图像生成/去噪等任务。最著名的扩散模型之一是DDPM，它来自于2020年发表的[去噪扩散概率模型](https://arxiv.org/pdf/2006.11239)论文。该模型是一个参数化的马尔可夫链，包含了前向和反向转移，如下所示。
- en: '![](../Images/b22249fb84cdffa38178102bb5a7d39f.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b22249fb84cdffa38178102bb5a7d39f.png)'
- en: 'The diffusion model is a bi-directional Markov chain. Image source: [https://arxiv.org/pdf/2006.11239](https://arxiv.org/pdf/2006.11239)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型是一个双向马尔可夫链。图像来源：[https://arxiv.org/pdf/2006.11239](https://arxiv.org/pdf/2006.11239)
- en: 'What is a Markov chain? It’s a statistical process in which the current step
    only relies on the previous step, and the reverse is vice versa. By assuming a
    Markov process, the model can start with a clean image by iteratively adding Gaussian
    noise in the forward process (right -> left in the figure above) and iteratively
    “learn” the noise by using a Unet-based architecture in the reverse process (left
    -> right in the figure above). That’s why we can sometimes see the diffusion model
    as a generative model (when used from left to right) and sometimes as a denoising
    model (when used from right to left). The DDPM loss is given below, where the
    theta is the model parameter set, \epsilon is the known noise, and the \epsilon_theta
    is the noise estimated by a deep learning model (usually a UNet):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫链是什么？它是一种统计过程，其中当前步骤仅依赖于前一步，反之亦然。通过假设一个马尔可夫过程，模型可以在前向过程中（图中从右到左）通过迭代添加高斯噪声来开始于一个干净的图像，并通过在反向过程中（图中从左到右）使用基于UNet的架构迭代“学习”噪声。这就是为什么我们有时可以将扩散模型看作生成模型（从左到右使用时），有时又可以看作去噪模型（从右到左使用时）。下面给出了DDPM损失，其中θ是模型参数集，\epsilon是已知的噪声，而\epsilon_theta是由深度学习模型（通常是UNet）估计的噪声：
- en: '![](../Images/7de90ea23694edffed94c90c0409641f.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7de90ea23694edffed94c90c0409641f.png)'
- en: 'Equation source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 方程来源：[https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)
- en: '**Diffusion Model in the Latent Space**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在空间中的扩散模型**'
- en: The idea of diffusion was further extended to the latent space in the [CVPR’22
    paper](https://arxiv.org/pdf/2112.10752), where the images are first “compressed”
    onto the latent space by using the encoder part of a pre-trained [Variational
    Auto Encoder](https://arxiv.org/pdf/1312.6114) (VAE). Then, the diffusion and
    reverse processes are performed on the latent space and mapped back to pixel space
    using the decoder part of the VAE. This could largely improve the learning speed
    and efficiency, as most calculations are performed in a lower dimensional space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散的思想进一步扩展到潜在空间中，在[CVPR’22论文](https://arxiv.org/pdf/2112.10752)中，图像首先通过使用预训练[变分自编码器](https://arxiv.org/pdf/1312.6114)（VAE）的编码器部分“压缩”到潜在空间。然后，扩散和反向过程在潜在空间中进行，并通过VAE的解码器部分将其映射回像素空间。这可以大大提高学习速度和效率，因为大部分计算是在较低维度的空间中进行的。
- en: '![](../Images/a9fad25cf2eddb54eeae02027cedab2e.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9fad25cf2eddb54eeae02027cedab2e.png)'
- en: 'Latent diffusion model architecture. The \Epsilon and D are encoders and decoders
    individually. Image source: [https://arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型架构。 \Epsilon 和 D 分别是编码器和解码器。图像来源：[https://arxiv.org/pdf/2112.10752](https://arxiv.org/pdf/2112.10752)
- en: '**VAE-based Image Transfusion**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于 VAE 的图像输送**'
- en: The core part of the Transfusion model is the fusion between the diffusion and
    the transformer for input images. First, an image is divided into a sequence of
    8*8 patches; each patch is passed into a pre-trained VAE encoder to “compress”
    into an 8-element latent vector representation. Then, noise is added to the latent
    representation and further processed by a linear layer/U-Net encoder to generate
    the “noisy” x_t. Third, the transformer model processes the sequence of noisy
    latent representations. Last, the outputs are reversely processed by another linear/U-Net
    decoder before using a VAE decoder to generate the “true” x_0 image.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Transfusion 模型的核心部分是扩散模型与变压器模型在输入图像上的融合。首先，图像被划分为一个 8*8 的补丁序列；每个补丁被送入一个预训练的
    VAE 编码器进行“压缩”，以得到一个 8 元素的潜在向量表示。然后，噪声被添加到潜在表示中，并通过一个线性层/U-Net 编码器进一步处理，生成“带噪声”的
    x_t。接着，变压器模型处理这一序列的带噪声潜在表示。最后，输出通过另一个线性/U-Net 解码器反向处理，然后使用 VAE 解码器生成“真实”的 x_0
    图像。
- en: '![](../Images/56fad8e93f71ed2b84dcbf2d20796ffb.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56fad8e93f71ed2b84dcbf2d20796ffb.png)'
- en: 'Diffusion module part for image input. The noise is added to VAE-encoded embedding.
    Image souce: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图像输入的扩散模块部分。噪声被添加到 VAE 编码的嵌入中。图像来源：[https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)
- en: In the actual implementation, the beginning of the image (BOI) token and the
    end of the image (EOI) token are padded to both sides of the image representation
    sequence before concatenating the text tokens. Self-attention for image training
    is bi-directional attention, while self-attention for text tokens is causal. At
    the training stage, the loss for the image sequence is DDPM loss, while the rest
    of the text tokens use the LM loss.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中，图像的开始（BOI）令牌和结束（EOI）令牌会被填充到图像表示序列的两侧，然后再与文本令牌进行拼接。图像训练的自注意力是双向注意力，而文本令牌的自注意力是因果的。在训练阶段，图像序列的损失是
    DDPM 损失，而其余的文本令牌使用的是语言模型（LM）损失。
- en: So why bother? Why do we need such a complicated procedure for processing image
    patch tokens? The paper explains that the token space for text and images is different.
    **While the text tokens are discrete, the image tokens/patches are naturally continuous**.
    In the previous art, image tokens need to be “discretized” before fusing into
    the transformer model, while integrating the diffusion model directly could resolve
    this issue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 那为什么要费劲呢？为什么我们需要如此复杂的处理程序来处理图像补丁令牌？论文解释说，文本和图像的令牌空间是不同的。**虽然文本令牌是离散的，但图像令牌/补丁自然是连续的**。在以往的技术中，图像令牌需要在融合到变压器模型之前进行“离散化”，而直接整合扩散模型可以解决这一问题。
- en: '**Compare with state-of-the-art**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**与最先进技术比较**'
- en: The primary multi-modal model the paper compares to is the [Chameleon model](https://arxiv.org/abs/2405.09818),
    which Meta proposed earlier this year. Here, we compare the difference between
    architecture and training set size between the Chameleon-7B and Transfusion-7B.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 论文比较的主要多模态模型是[Chameleon 模型](https://arxiv.org/abs/2405.09818)，这是 Meta 今年早些时候提出的。在这里，我们比较了
    Chameleon-7B 和 Transfusion-7B 在架构和训练集大小上的差异。
- en: '![](../Images/64436cc8da1edbde103d95a46a0605f9.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64436cc8da1edbde103d95a46a0605f9.png)'
- en: Arechitecture and training difference between Chameleon 7B and Transfusion 7B.
    Image by author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Chameleon 7B 和 Transfusion 7B 之间的架构与训练差异。图片由作者提供。
- en: The paper lists the performance comparison over the Llama2 pre-training suite
    accuracy, COCO zero-shot Frechet Inception Distance (FID) and GenEval benchmark.
    We can see that the Transfusion performs much better than Chameleon on the image-related
    benchmarks (COCO and Gen) while losing very little margin compared to Chameleon,
    with the same amount of parameters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 论文列出了在 Llama2 预训练套件精度、COCO 零样本 Frechet Inception 距离（FID）和 GenEval 基准测试上的性能比较。我们可以看到，Transfusion
    在与图像相关的基准（COCO 和 Gen）上表现远好于 Chameleon，而在与 Chameleon 相比时，仅略微有所失分，且参数量相同。
- en: '![](../Images/e202a86699418ec3765bc3b34f706bdd.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e202a86699418ec3765bc3b34f706bdd.png)'
- en: 'Image source: [https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[https://www.arxiv.org/pdf/2408.11039](https://www.arxiv.org/pdf/2408.11039)
- en: '**Further Comments.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步的评论。**'
- en: Although the idea of the paper is super interesting, the “Diffusion” part of
    the Transfusion is hardly an actual Diffusion, as there are only two timestamps
    in the Markov process. Besides, the pre-trained VAE makes the model no longer
    strictly end-to-end. Also, the VAE + Linear/UNet + Transformer Encoder + Linear/UNet
    + VAE design looks so complicated, which makes the audience can’t help but ask,
    is there a more elegant way to implement this idea? Besides, I previously wrote
    about the [latest publication from Apple](https://medium.com/towards-data-science/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1)
    on the generalization benefits of using autoregressive modelling on images, so
    it might be interesting to give a second thought to the “MIM + autoregressive”
    approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管论文的思路非常有趣，但“Transfusion”中的“扩散”部分实际上并不是真正的扩散，因为在马尔可夫过程中只有两个时间戳。此外，预训练的 VAE
    使得模型不再是严格的端到端。并且，VAE + 线性/UNet + Transformer 编码器 + 线性/UNet + VAE 设计看起来非常复杂，这让观众不禁问，是否有更优雅的方式来实现这个想法？此外，我之前写过关于[苹果公司最新出版的论文](https://medium.com/towards-data-science/from-masked-image-modeling-to-autoregressive-image-modeling-d9a3cadf72a1)，探讨了在图像上使用自回归建模的泛化效益，因此或许值得再次思考“MIM
    + 自回归”方法。
- en: If you find this post interesting and would like to discuss it, you’re welcome
    to leave a comment, and I’m happy to further the discussion there :)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有趣并且想讨论，欢迎留言，我很乐意在那儿进一步展开讨论 :)
- en: '**References**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: 'Zhou et al., Transfusion: Predict the Next Token and Diffuse Images with One
    Multi-Modal Model. arXiv 2024.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人，《Transfusion：用一个多模态模型预测下一个令牌并扩散图像》，arXiv 2024。
- en: 'Team C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
    2024.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C. Team，《Chameleon：混合模态早期融合基础模型》，arXiv 预印本 2024。
- en: 'Touvron et al., Llama: Open and efficient foundation language models. arXiv
    2023.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人，《Llama：开放且高效的基础语言模型》，arXiv 2023。
- en: Rombach et al., High-resolution image synthesis with latent diffusion models.
    CVPR 2022.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等人，《使用潜在扩散模型进行高分辨率图像合成》，CVPR 2022。
- en: Ho et al., Denoising diffusion probabilistic models. NeurIPS 2020.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人，《去噪扩散概率模型》，NeurIPS 2020。
- en: Vaswani, Attention is all you need. NeurIPS 2017.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani，《Attention is all you need》，NeurIPS 2017。
- en: Kingma, Auto-encoding variational bayes. arXiv preprint 2013.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma，《自编码变分贝叶斯》，arXiv 预印本 2013。
