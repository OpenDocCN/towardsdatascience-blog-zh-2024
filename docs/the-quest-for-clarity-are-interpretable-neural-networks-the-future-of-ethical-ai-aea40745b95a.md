# 清晰性的追求：可解释的神经网络是伦理人工智能的未来吗？

> 原文：[https://towardsdatascience.com/the-quest-for-clarity-are-interpretable-neural-networks-the-future-of-ethical-ai-aea40745b95a?source=collection_archive---------9-----------------------#2024-04-23](https://towardsdatascience.com/the-quest-for-clarity-are-interpretable-neural-networks-the-future-of-ethical-ai-aea40745b95a?source=collection_archive---------9-----------------------#2024-04-23)

## 机械可解释性能否克服事后解释的局限性？

[](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)[![Andy Spezzatti](../Images/f1bb10d48cd83582e6af91f6d14fed6e.png)](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------) [Andy Spezzatti](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------) ·阅读时间：10分钟·2024年4月23日

--

![](../Images/31bd9e11e02a8d71f4089090ff24e7bf.png)

图片由作者通过[Midjourney](https://www.midjourney.com/home)生成

开发符合伦理标准的人工智能（AI）系统面临重要挑战。尽管有许多构建可信AI的指南，但它们往往只提供宽泛的、高层次的指引，难以具体应用并验证合规性。

透明度和解释AI决策的能力至关重要，特别是随着AI应用在各个行业的普及。近期的研究进展提高了我们理解和预测AI行为的能力，这是实现其伦理采用和更广泛接受的关键一步。

## 为什么这很重要？

现代AI模型，尤其是深度学习中的模型，通常被称为“黑箱”，因为它们复杂的算法即使对于开发者而言也很难理解。这种缺乏透明度与需要解释和验证决策的领域中的问责需求相冲突。此外，像欧盟的《通用数据保护条例》（GDPR）等法律现在要求自动化系统提供更高的透明度，法律要求个人…
