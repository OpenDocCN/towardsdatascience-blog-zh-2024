# 清晰性的追求：可解释的神经网络是伦理人工智能的未来吗？

> 原文：[`towardsdatascience.com/the-quest-for-clarity-are-interpretable-neural-networks-the-future-of-ethical-ai-aea40745b95a?source=collection_archive---------9-----------------------#2024-04-23`](https://towardsdatascience.com/the-quest-for-clarity-are-interpretable-neural-networks-the-future-of-ethical-ai-aea40745b95a?source=collection_archive---------9-----------------------#2024-04-23)

## 机械可解释性能否克服事后解释的局限性？

[](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)![Andy Spezzatti](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------) [Andy Spezzatti](https://medium.com/@andy_spezzatti?source=post_page---byline--aea40745b95a--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aea40745b95a--------------------------------) ·阅读时间：10 分钟·2024 年 4 月 23 日

--

![](img/31bd9e11e02a8d71f4089090ff24e7bf.png)

图片由作者通过[Midjourney](https://www.midjourney.com/home)生成

开发符合伦理标准的人工智能（AI）系统面临重要挑战。尽管有许多构建可信 AI 的指南，但它们往往只提供宽泛的、高层次的指引，难以具体应用并验证合规性。

透明度和解释 AI 决策的能力至关重要，特别是随着 AI 应用在各个行业的普及。近期的研究进展提高了我们理解和预测 AI 行为的能力，这是实现其伦理采用和更广泛接受的关键一步。

## 为什么这很重要？

现代 AI 模型，尤其是深度学习中的模型，通常被称为“黑箱”，因为它们复杂的算法即使对于开发者而言也很难理解。这种缺乏透明度与需要解释和验证决策的领域中的问责需求相冲突。此外，像欧盟的《通用数据保护条例》（GDPR）等法律现在要求自动化系统提供更高的透明度，法律要求个人…
