# ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼Œè¯¦è§£ï¼šé€‚åˆåˆå­¦è€…çš„å¯è§†åŒ–æŒ‡å—åŠä»£ç ç¤ºä¾‹

> åŸæ–‡ï¼š[https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24](https://towardsdatascience.com/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6?source=collection_archive---------2-----------------------#2024-08-24)

## åˆ†ç±»ç®—æ³•

## é€šè¿‡æ˜¯/å¦æ¦‚ç‡è§£é”é¢„æµ‹èƒ½åŠ›

[](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)[![Samy Baladram](../Images/715cb7af97c57601966c5d2f9edd0066.png)](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------) [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--aec39771ddd6--------------------------------)

Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aec39771ddd6--------------------------------) Â·é˜…è¯»æ—¶é—´9åˆ†é’ŸÂ·2024å¹´8æœˆ24æ—¥

--

![](../Images/03ca097360665649a9247ebe64f4ed22.png)

`â›³ï¸ æ›´å¤šåˆ†ç±»ç®—æ³•ï¼Œè¯¦è§£ï¼š Â· [è™šæ‹Ÿåˆ†ç±»å™¨](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e) Â· [Kè¿‘é‚»åˆ†ç±»å™¨](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1) â–¶ [ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯](/bernoulli-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-aec39771ddd6) Â· [é«˜æ–¯æœ´ç´ è´å¶æ–¯](/gaussian-naive-bayes-explained-a-visual-guide-with-code-examples-for-beginners-04949cef383c) Â· [å†³ç­–æ ‘åˆ†ç±»å™¨](/decision-tree-classifier-explained-a-visual-guide-with-code-examples-for-beginners-7c863f06a71e) Â· [é€»è¾‘å›å½’](/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505) Â· [æ”¯æŒå‘é‡åˆ†ç±»å™¨](/support-vector-classifier-explained-a-visual-guide-with-mini-2d-dataset-62e831e7b9e9) Â· [å¤šå±‚æ„ŸçŸ¥æœº](/multilayer-perceptron-explained-a-visual-guide-with-mini-2d-dataset-0ae8100c5d1c)`

ä¸[è™šæ‹Ÿåˆ†ç±»å™¨](/dummy-classifier-explained-a-visual-guide-with-code-examples-for-beginners-009ff95fc86e)æˆ–[KNNåŸºäºç›¸ä¼¼åº¦æ¨ç†](/k-nearest-neighbor-classifier-explained-a-visual-guide-with-code-examples-for-beginners-a3d85cad00e1)çš„åŸºçº¿æ–¹æ³•ä¸åŒï¼Œæœ´ç´ è´å¶æ–¯åˆ©ç”¨æ¦‚ç‡ç†è®ºã€‚å®ƒå°†æ¯ä¸ªâ€œçº¿ç´¢â€ï¼ˆæˆ–ç‰¹å¾ï¼‰çš„ä¸ªä½“æ¦‚ç‡ç»“åˆèµ·æ¥ï¼Œåšå‡ºæœ€ç»ˆé¢„æµ‹ã€‚è¿™ç§ç›´æ¥è€Œå¼ºå¤§çš„æ–¹æ³•åœ¨è®¸å¤šæœºå™¨å­¦ä¹ åº”ç”¨ä¸­è¢«è¯æ˜æ˜¯æ— ä»·çš„ã€‚

![](../Images/30ab2ece7ab19eccfe1d56b891244792.png)

æ‰€æœ‰è§†è§‰å†…å®¹ï¼šä½œè€…ä½¿ç”¨Canva Proåˆ›å»ºã€‚å·²ä¼˜åŒ–ç§»åŠ¨è®¾å¤‡æ˜¾ç¤ºï¼›åœ¨æ¡Œé¢ä¸Šå¯èƒ½æ˜¾ç¤ºè¿‡å¤§ã€‚

# å®šä¹‰

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§ä½¿ç”¨æ¦‚ç‡å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»çš„æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒåŸºäº[è´å¶æ–¯å®šç†](https://en.wikipedia.org/wiki/Bayes%27_theorem)ï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—æ¡ä»¶æ¦‚ç‡çš„å…¬å¼ã€‚ â€œæœ´ç´ â€éƒ¨åˆ†æŒ‡çš„æ˜¯å®ƒçš„ä¸€ä¸ªå…³é”®å‡è®¾ï¼šå®ƒå‡è®¾æ‰€æœ‰ç‰¹å¾å½¼æ­¤ç‹¬ç«‹ï¼Œå³ä½¿å®ƒä»¬åœ¨ç°å®ä¸­å¯èƒ½ä¸æ˜¯ã€‚å°½ç®¡è¿™ç§ç®€åŒ–é€šå¸¸ä¸ç°å®ï¼Œä½†å®ƒå¤§å¤§å‡å°‘äº†è®¡ç®—å¤æ‚æ€§ï¼Œå¹¶ä¸”åœ¨è®¸å¤šå®é™…åœºæ™¯ä¸­è¡¨ç°è‰¯å¥½ã€‚

![](../Images/f7d3cf3a614af1aa7768d0a939f589b7.png)

æœ´ç´ è´å¶æ–¯æ–¹æ³•æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ç§ç®€å•ç®—æ³•ï¼ŒåŸºäºæ¦‚ç‡ä½œä¸ºå…¶åŸºç¡€ã€‚

# æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨çš„ä¸»è¦ç±»å‹

æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æœ‰ä¸‰ç§ä¸»è¦ç±»å‹ã€‚è¿™äº›ç±»å‹ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºå®ƒä»¬å¯¹ç‰¹å¾åˆ†å¸ƒçš„å‡è®¾ï¼š

1.  **ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯**ï¼šé€‚ç”¨äºäºŒè¿›åˆ¶/å¸ƒå°”ç‰¹å¾ã€‚å®ƒå‡è®¾æ¯ä¸ªç‰¹å¾æ˜¯äºŒè¿›åˆ¶å€¼ï¼ˆ0/1ï¼‰çš„å˜é‡ã€‚

1.  **å¤šé¡¹å¼æœ´ç´ è´å¶æ–¯**ï¼šé€šå¸¸ç”¨äºç¦»æ•£è®¡æ•°ã€‚å®ƒå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œå…¶ä¸­ç‰¹å¾å¯èƒ½æ˜¯å•è¯è®¡æ•°ã€‚

1.  **é«˜æ–¯æœ´ç´ è´å¶æ–¯**ï¼šå‡è®¾è¿ç»­ç‰¹å¾æœä»æ­£æ€åˆ†å¸ƒã€‚

![](../Images/649ef29e85be0e8574c9d60724d5f436.png)

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯å‡è®¾äºŒè¿›åˆ¶æ•°æ®ï¼Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯å¤„ç†ç¦»æ•£è®¡æ•°ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯å¤„ç†å‡è®¾æ­£æ€åˆ†å¸ƒçš„è¿ç»­æ•°æ®ã€‚

é‡ç‚¹æ˜¯ä»æœ€ç®€å•çš„æœ´ç´ è´å¶æ–¯å¼€å§‹ï¼Œè¿™å°±æ˜¯ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ã€‚å…¶åç§°ä¸­çš„â€œä¼¯åŠªåˆ©â€æ¥è‡ªäºå‡è®¾æ¯ä¸ªç‰¹å¾éƒ½æ˜¯äºŒè¿›åˆ¶å€¼çš„å‡è®¾ã€‚

# ä½¿ç”¨çš„æ•°æ®é›†

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªäººå·¥é«˜å°”å¤«æ•°æ®é›†ï¼ˆçµæ„Ÿæ¥æºäº[1]ï¼‰ä½œä¸ºç¤ºä¾‹ã€‚è¯¥æ•°æ®é›†é¢„æµ‹ä¸€ä¸ªäººæ˜¯å¦ä¼šæ ¹æ®å¤©æ°”æ¡ä»¶æ‰“é«˜å°”å¤«ã€‚

![](../Images/26a4306804458312bd32df0299f49b7b.png)

åˆ—ï¼šâ€˜Outlookâ€™ï¼Œâ€˜Temperatureâ€™ï¼ˆåæ°åº¦ï¼‰ï¼Œâ€˜Humidityâ€™ï¼ˆç™¾åˆ†æ¯”ï¼‰ï¼Œâ€˜Windâ€™å’Œâ€˜Playâ€™ï¼ˆç›®æ ‡ç‰¹å¾ï¼‰

```py
# IMPORTING DATASET #
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pandas as pd
import numpy as np

dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', 'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', 'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)

# ONE-HOT ENCODE 'Outlook' COLUMN
df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)

# CONVERT 'Windy' (bool) and 'Play' (binary) COLUMNS TO BINARY INDICATORS
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Set feature matrix X and target vector y
X, y = df.drop(columns='Play'), df['Play']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

print(pd.concat([X_train, y_train], axis=1), end='\n\n')
print(pd.concat([X_test, y_test], axis=1))
```

æˆ‘ä»¬å°†ç¨ä½œè°ƒæ•´ï¼Œä½¿ç”¨ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ï¼Œé€šè¿‡å°†ç‰¹å¾è½¬æ¢ä¸ºäºŒè¿›åˆ¶æ ¼å¼ã€‚

![](../Images/61eb5e8d4ce645769beb07e6f3802a4c.png)

ç”±äºæ‰€æœ‰æ•°æ®å¿…é¡»æ˜¯0å’Œ1æ ¼å¼ï¼Œå› æ­¤â€˜Outlookâ€™è¢«è¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œè€ŒTemperatureè¢«åˆ†ä¸ºâ‰¤80å’Œ>80ã€‚åŒæ ·ï¼ŒHumidityè¢«åˆ†ä¸ºâ‰¤75å’Œ>75ã€‚

```py
# One-hot encode the categorized columns and drop them after, but do it separately for training and test sets
# Define categories for 'Temperature' and 'Humidity' for training set
X_train['Temperature'] = pd.cut(X_train['Temperature'], bins=[0, 80, 100], labels=['Warm', 'Hot'])
X_train['Humidity'] = pd.cut(X_train['Humidity'], bins=[0, 75, 100], labels=['Dry', 'Humid'])

# Similarly, define for the test set
X_test['Temperature'] = pd.cut(X_test['Temperature'], bins=[0, 80, 100], labels=['Warm', 'Hot'])
X_test['Humidity'] = pd.cut(X_test['Humidity'], bins=[0, 75, 100], labels=['Dry', 'Humid'])

# One-hot encode the categorized columns
one_hot_columns_train = pd.get_dummies(X_train[['Temperature', 'Humidity']], drop_first=True, dtype=int)
one_hot_columns_test = pd.get_dummies(X_test[['Temperature', 'Humidity']], drop_first=True, dtype=int)

# Drop the categorized columns from training and test sets
X_train = X_train.drop(['Temperature', 'Humidity'], axis=1)
X_test = X_test.drop(['Temperature', 'Humidity'], axis=1)

# Concatenate the one-hot encoded columns with the original DataFrames
X_train = pd.concat([one_hot_columns_train, X_train], axis=1)
X_test = pd.concat([one_hot_columns_test, X_test], axis=1)

print(pd.concat([X_train, y_train], axis=1), '\n')
print(pd.concat([X_test, y_test], axis=1))
```

# ä¸»è¦æœºåˆ¶

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ç®—æ³•é€‚ç”¨äºæ¯ä¸ªç‰¹å¾å€¼ä¸º 0 æˆ– 1 çš„æ•°æ®ã€‚

1.  è®¡ç®—è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚

1.  å¯¹äºæ¯ä¸ªç‰¹å¾å’Œç±»åˆ«ï¼Œè®¡ç®—åœ¨ç»™å®šç±»åˆ«ä¸‹ç‰¹å¾ä¸º 1 å’Œ 0 çš„æ¦‚ç‡ã€‚

1.  å¯¹äºä¸€ä¸ªæ–°å®ä¾‹ï¼šå¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œå°†å…¶æ¦‚ç‡ä¸è¯¥ç±»åˆ«ä¸‹æ¯ä¸ªç‰¹å¾å€¼ï¼ˆ0 æˆ– 1ï¼‰çš„æ¦‚ç‡ç›¸ä¹˜ã€‚

1.  é¢„æµ‹å…·æœ‰æœ€é«˜ç»“æœæ¦‚ç‡çš„ç±»åˆ«ã€‚

![](../Images/84714052b549a7af7c0d539e9ad80a40.png)

å¯¹äºæˆ‘ä»¬çš„é«˜å°”å¤«æ•°æ®é›†ï¼Œä¼¯åŠªåˆ© NB åˆ†ç±»å™¨æŸ¥çœ‹æ¯ä¸ªç‰¹å¾åœ¨æ¯ä¸ªç±»åˆ«ï¼ˆYES å’Œ NOï¼‰ä¸‹å‘ç”Ÿçš„æ¦‚ç‡ï¼Œç„¶åæ ¹æ®å“ªä¸ªç±»åˆ«çš„æ¦‚ç‡æ›´é«˜æ¥åšå‡ºå†³ç­–ã€‚

# è®­ç»ƒæ­¥éª¤

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬ä»è®­ç»ƒæ•°æ®ä¸­è®¡ç®—æ¦‚ç‡ï¼š

1.  **ç±»åˆ«æ¦‚ç‡è®¡ç®—**ï¼šå¯¹äºæ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—å…¶æ¦‚ç‡ï¼šï¼ˆè¯¥ç±»åˆ«ä¸­å®ä¾‹çš„æ•°é‡ï¼‰/ï¼ˆæ‰€æœ‰å®ä¾‹çš„æ€»æ•°ï¼‰ã€‚

![](../Images/9b5948e82009cdf2e32a27b5175ef059.png)

åœ¨æˆ‘ä»¬çš„é«˜å°”å¤«ç¤ºä¾‹ä¸­ï¼Œç®—æ³•å°†è®¡ç®—æ€»ä½“ä¸Šæ‰“é«˜å°”å¤«çš„é¢‘ç‡ã€‚

```py
from fractions import Fraction

def calc_target_prob(attr):
    total_counts = attr.value_counts().sum()
    prob_series = attr.value_counts().apply(lambda x: Fraction(x, total_counts).limit_denominator())
    return prob_series

print(calc_target_prob(y_train))
```

2. **ç‰¹å¾æ¦‚ç‡è®¡ç®—**ï¼šå¯¹äºæ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—ï¼š

+   ï¼ˆç‰¹å¾ä¸º 0 çš„å®ä¾‹æ•°é‡ï¼‰/ï¼ˆè¯¥ç±»åˆ«ä¸­å®ä¾‹çš„æ•°é‡ï¼‰

+   ï¼ˆç‰¹å¾ä¸º 1 çš„å®ä¾‹æ•°é‡ï¼‰/ï¼ˆè¯¥ç±»åˆ«ä¸­å®ä¾‹çš„æ•°é‡ï¼‰

![](../Images/2b11338b2b4b82915b66915a59a02c86.png)

å¯¹äºæ¯ç§å¤©æ°”æ¡ä»¶ï¼ˆä¾‹å¦‚ï¼Œæ™´å¤©ï¼‰ï¼Œè®¡ç®—åœ¨æ™´å¤©æ—¶æ‰“é«˜å°”å¤«çš„é¢‘ç‡ï¼Œä»¥åŠæ™´å¤©æ—¶ä¸æ‰“é«˜å°”å¤«çš„é¢‘ç‡ã€‚

```py
from fractions import Fraction

def sort_attr_label(attr, lbl):
    return (pd.concat([attr, lbl], axis=1)
            .sort_values([attr.name, lbl.name])
            .reset_index()
            .rename(columns={'index': 'ID'})
            .set_index('ID'))

def calc_feature_prob(attr, lbl):
    total_classes = lbl.value_counts()
    counts = pd.crosstab(attr, lbl)
    prob_df = counts.apply(lambda x: [Fraction(c, total_classes[x.name]).limit_denominator() for c in x])

    return prob_df

print(sort_attr_label(y_train, X_train['sunny']))
print(calc_feature_prob(X_train['sunny'], y_train))
```

![](../Images/3f877762f5383987bfcc7aebba944cee.png)

åŒæ ·çš„è¿‡ç¨‹åº”ç”¨äºæ‰€æœ‰å…¶ä»–ç‰¹å¾ã€‚

```py
for col in X_train.columns:
  print(calc_feature_prob(X_train[col], y_train), "\n")
```

3. **å¹³æ»‘ï¼ˆå¯é€‰ï¼‰**ï¼šåœ¨æ¯æ¬¡æ¦‚ç‡è®¡ç®—æ—¶ï¼Œå‘åˆ†å­å’Œåˆ†æ¯ä¸­æ·»åŠ ä¸€ä¸ªå°å€¼ï¼ˆé€šå¸¸ä¸º 1ï¼‰ï¼Œä»¥é¿å…å‡ºç°é›¶æ¦‚ç‡ã€‚

![](../Images/12828bae35f3666b1a11714d940b86dd.png)

æˆ‘ä»¬å¯¹æ‰€æœ‰åˆ†å­åŠ  1ï¼Œå¯¹æ‰€æœ‰åˆ†æ¯åŠ  2ï¼Œä»¥ä¿æŒæ€»ç±»åˆ«æ¦‚ç‡ä¸º 1ã€‚

```py
# In sklearn, all processes above is summarized in this 'fit' method:
from sklearn.naive_bayes import BernoulliNB
nb_clf = BernoulliNB(alpha=1)
nb_clf.fit(X_train, y_train)
```

4. **å­˜å‚¨ç»“æœ**ï¼šä¿å­˜æ‰€æœ‰è®¡ç®—å‡ºçš„æ¦‚ç‡ï¼Œä»¥ä¾¿åœ¨åˆ†ç±»è¿‡ç¨‹ä¸­ä½¿ç”¨ã€‚

![](../Images/4f252c018473e393221152c14c8237de.png)

å¹³æ»‘å·²ç»åº”ç”¨åˆ°æ‰€æœ‰ç‰¹å¾æ¦‚ç‡ä¸­ã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›è¡¨æ ¼è¿›è¡Œé¢„æµ‹ã€‚

# åˆ†ç±»æ­¥éª¤

ç»™å®šä¸€ä¸ªæ–°å®ä¾‹ï¼Œå…¶ç‰¹å¾å€¼ä¸º 0 æˆ– 1ï¼š

1.  **æ¦‚ç‡æ”¶é›†**ï¼šå¯¹äºæ¯ä¸ªå¯èƒ½çš„ç±»åˆ«ï¼š

+   ä»è¯¥ç±»åˆ«å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆç±»åˆ«æ¦‚ç‡ï¼‰å¼€å§‹ã€‚

+   å¯¹äºæ–°å®ä¾‹ä¸­çš„æ¯ä¸ªç‰¹å¾ï¼Œæ”¶é›†è¯¥ç‰¹å¾åœ¨æ­¤ç±»åˆ«ä¸‹ä¸º 0/1 çš„æ¦‚ç‡ã€‚

![](../Images/e75f17e0d471205439b795c7edd459e1.png)

å¯¹äº ID 14ï¼Œæˆ‘ä»¬é€‰æ‹©æ¯ä¸ªç‰¹å¾ï¼ˆæ— è®ºæ˜¯ 0 è¿˜æ˜¯ 1ï¼‰å‘ç”Ÿçš„æ¦‚ç‡ã€‚

2. **å¾—åˆ†è®¡ç®—ä¸é¢„æµ‹**ï¼šå¯¹äºæ¯ä¸ªç±»åˆ«ï¼š

+   å°†æ‰€æœ‰æ”¶é›†åˆ°çš„æ¦‚ç‡ç›¸ä¹˜ã€‚

+   ç»“æœæ˜¯è¯¥ç±»åˆ«çš„å¾—åˆ†ã€‚

+   å¾—åˆ†æœ€é«˜çš„ç±»åˆ«å°±æ˜¯é¢„æµ‹ç»“æœã€‚

![](../Images/27aa29b9b8151107a827f3c9a27df791.png)

åœ¨å°†ç±»åˆ«æ¦‚ç‡ä¸æ‰€æœ‰ç‰¹å¾æ¦‚ç‡ç›¸ä¹˜åï¼Œé€‰æ‹©å¾—åˆ†æ›´é«˜çš„ç±»åˆ«ã€‚

```py
y_pred = nb_clf.predict(X_test)
print(y_pred)
```

# è¯„ä¼°æ­¥éª¤

![](../Images/3902cfe7e903f14c43f4a61c2785b3f9.png)

è¿™ä¸ªç®€å•çš„æ¦‚ç‡æ¨¡å‹åœ¨è¿™ä¸ªç®€å•æ•°æ®é›†ä¸Šè¡¨ç°å‡ºäº†å¾ˆå¥½çš„å‡†ç¡®æ€§ã€‚

```py
# Evaluate the classifier
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

# å…³é”®å‚æ•°

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯æœ‰ä¸€äº›é‡è¦å‚æ•°ï¼š

1.  **Alpha (Î±)**ï¼šè¿™æ˜¯å¹³æ»‘å‚æ•°ã€‚å®ƒä¸ºæ¯ä¸ªç‰¹å¾æ·»åŠ ä¸€ä¸ªå°çš„è®¡æ•°ï¼Œä»¥é˜²æ­¢é›¶æ¦‚ç‡ã€‚é»˜è®¤é€šå¸¸ä¸º1.0ï¼ˆæ‹‰æ™®æ‹‰æ–¯å¹³æ»‘ï¼‰ï¼Œå¦‚å‰æ‰€ç¤ºã€‚

1.  **Binarize**ï¼šå¦‚æœæ‚¨çš„ç‰¹å¾å°šæœªæ˜¯äºŒå…ƒçš„ï¼Œæ­¤é˜ˆå€¼ä¼šå°†å…¶è½¬æ¢ã€‚ä»»ä½•é«˜äºæ­¤é˜ˆå€¼çš„å€¼å˜ä¸º1ï¼Œä½äºè¯¥é˜ˆå€¼çš„å€¼å˜ä¸º0ã€‚

![](../Images/ec7dd643acab9b9101b133d2d24b6fb2.png)

å¯¹äºscikit-learnä¸­çš„BernoulliNBï¼Œæ•°å€¼ç‰¹å¾é€šå¸¸æ˜¯æ ‡å‡†åŒ–çš„ï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨äºŒå€¼åŒ–çš„ã€‚æ¨¡å‹ä¼šå°†è¿™äº›æ ‡å‡†åŒ–çš„å€¼è½¬åŒ–ä¸ºäºŒè¿›åˆ¶ï¼Œé€šå¸¸ä½¿ç”¨0ï¼ˆå‡å€¼ï¼‰ä½œä¸ºé˜ˆå€¼ã€‚

3. **Fit Prior**ï¼šæ˜¯å¦å­¦ä¹ ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ï¼Œæˆ–å‡è®¾å‡åŒ€å…ˆéªŒï¼ˆ50/50ï¼‰ã€‚

![](../Images/84092e5c0f5d406baed165d4f14aab41.png)

å¯¹äºæˆ‘ä»¬çš„é«˜å°”å¤«æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯èƒ½ä»é»˜è®¤çš„Î±=1.0å¼€å§‹ï¼Œä¸è¿›è¡ŒäºŒå€¼åŒ–ï¼ˆå› ä¸ºæˆ‘ä»¬çš„ç‰¹å¾å·²ç»æ˜¯äºŒå€¼çš„ï¼‰ï¼Œå¹¶è®¾ç½®fit_prior=Trueã€‚

# ä¼˜ç¼ºç‚¹

å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä»»ä½•ç®—æ³•ä¸€æ ·ï¼Œä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ä¹Ÿæœ‰å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚

# ä¼˜ç‚¹ï¼š

1.  **ç®€å•æ€§**ï¼šå®¹æ˜“å®ç°å’Œç†è§£ã€‚

1.  **æ•ˆç‡**ï¼šè®­ç»ƒå’Œé¢„æµ‹é€Ÿåº¦å¿«ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡ç‰¹å¾ç©ºé—´ã€‚

1.  **å°æ•°æ®é›†çš„è¡¨ç°**ï¼šå³ä½¿åœ¨è®­ç»ƒæ•°æ®æœ‰é™çš„æƒ…å†µä¸‹ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚

1.  **å¤„ç†é«˜ç»´æ•°æ®**ï¼šåœ¨ç‰¹å¾ç»´åº¦è¾ƒå¤šæ—¶è¡¨ç°è‰¯å¥½ï¼Œå°¤å…¶é€‚ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚

# ç¼ºç‚¹ï¼š

1.  **ç‹¬ç«‹æ€§å‡è®¾**ï¼šå‡è®¾æ‰€æœ‰ç‰¹å¾æ˜¯ç‹¬ç«‹çš„ï¼Œä½†åœ¨å®é™…æ•°æ®ä¸­ï¼Œè¿™é€šå¸¸ä¸æˆç«‹ã€‚

1.  **ä»…é™äºŒå…ƒç‰¹å¾**ï¼šåœ¨å…¶çº¯ç²¹å½¢å¼ä¸‹ï¼Œä»…é€‚ç”¨äºäºŒå…ƒæ•°æ®ã€‚

1.  **å¯¹è¾“å…¥æ•°æ®çš„æ•æ„Ÿæ€§**ï¼šå¯¹ç‰¹å¾çš„äºŒå€¼åŒ–æ–¹å¼å¯èƒ½è¾ƒä¸ºæ•æ„Ÿã€‚

1.  **é›¶é¢‘é—®é¢˜**ï¼šå¦‚æœæ²¡æœ‰å¹³æ»‘å¤„ç†ï¼Œé›¶æ¦‚ç‡ä¼šå¼ºçƒˆå½±å“é¢„æµ‹ç»“æœã€‚

# æœ€åçš„å¤‡æ³¨

ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„äºŒå…ƒåˆ†ç±»æœºå™¨å­¦ä¹ ç®—æ³•ã€‚å®ƒåœ¨æ–‡æœ¬åˆ†æå’Œåƒåœ¾é‚®ä»¶æ£€æµ‹ä¸­è¡¨ç°ä¼˜ç§€ï¼Œè¿™äº›åœºæ™¯ä¸­çš„ç‰¹å¾é€šå¸¸æ˜¯äºŒå…ƒçš„ã€‚å› å…¶é€Ÿåº¦å’Œæ•ˆç‡è‘—ç§°ï¼Œè¿™ä¸€æ¦‚ç‡æ¨¡å‹åœ¨å°æ•°æ®é›†å’Œé«˜ç»´ç©ºé—´ä¸­è¡¨ç°å‡ºè‰²ã€‚

å°½ç®¡å‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œä½†å®ƒçš„å‡†ç¡®æ€§å¸¸å¸¸èƒ½ä¸æ›´å¤æ‚çš„æ¨¡å‹ç›¸åª²ç¾ã€‚ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ä½œä¸ºä¸€ä¸ªä¼˜ç§€çš„åŸºå‡†æ¨¡å‹å’Œå®æ—¶åˆ†ç±»å·¥å…·ã€‚

# ğŸŒŸ ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ç®€åŒ–ç‰ˆ

```py
# Import needed libraries
import pandas as pd
from sklearn.naive_bayes import BernoulliNB
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

# Load the dataset
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)

# Prepare data for model
df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Split data into training and testing sets
X, y = df.drop(columns='Play'), df['Play']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Scale numerical features (for automatic binarization)
scaler = StandardScaler()
float_cols = X_train.select_dtypes(include=['float64']).columns
X_train[float_cols] = scaler.fit_transform(X_train[float_cols])
X_test[float_cols] = scaler.transform(X_test[float_cols])

# Train the model
nb_clf = BernoulliNB()
nb_clf.fit(X_train, y_train)

# Make predictions
y_pred = nb_clf.predict(X_test)

# Check accuracy
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

## è¿›ä¸€æ­¥é˜…è¯»

å¯¹äº[ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)åˆ†ç±»å™¨åŠå…¶åœ¨scikit-learnä¸­çš„å®ç°ï¼Œè¯»è€…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼Œæ–‡æ¡£ä¸­æä¾›äº†å…³äºå…¶ä½¿ç”¨å’Œå‚æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚

## æŠ€æœ¯ç¯å¢ƒ

æœ¬æ–‡ä½¿ç”¨ Python 3.7 å’Œ scikit-learn 1.5ã€‚è™½ç„¶è®¨è®ºçš„æ¦‚å¿µé€šå¸¸é€‚ç”¨ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œç•¥æœ‰å·®å¼‚ã€‚

## å…³äºæ’å›¾

é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œï¼Œå¹¶ç»“åˆäº†æ¥è‡ª Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚

![](../Images/3eeb239271faf81969b84660399a1fb4.png)

è¦æŸ¥çœ‹ Bernoulli Naive Bayes çš„ç®€æ´è§†è§‰æ€»ç»“ï¼Œè¯·æŸ¥çœ‹[Instagramä¸Šçš„é…å¥—å¸–å­](https://www.instagram.com/p/C_CUwtAyVI3/)ã€‚

## å‚è€ƒæ–‡çŒ®

[1] T. M. Mitchell, [æœºå™¨å­¦ä¹ ](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html) (1997), McGraw-Hill Science/Engineering/Math, ç¬¬59é¡µ

ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:

![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)

## åˆ†ç±»ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----aec39771ddd6--------------------------------)8ç¯‡æ•…äº‹![](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](../Images/6ea70d9d2d9456e0c221388dbb253be8.png)![](../Images/7221f0777228e7bcf08c1adb44a8eb76.png)

ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:

![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)

## å›å½’ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----aec39771ddd6--------------------------------)5ç¯‡æ•…äº‹![ä¸€ä¸ªå¸¦ç€åŒé©¬å°¾å’Œç²‰è‰²å¸½å­çš„å¡é€šå¨ƒå¨ƒã€‚è¿™ä¸ªâ€œå‡äººâ€å¨ƒå¨ƒï¼Œå‡­å€Ÿå…¶ç®€å•çš„è®¾è®¡å’Œå¿ƒå½¢è£…é¥°çš„è¡£æœï¼Œç›´è§‚åœ°å±•ç¤ºäº†æœºå™¨å­¦ä¹ ä¸­â€œå‡å›å½’å™¨â€çš„æ¦‚å¿µã€‚å°±åƒè¿™ä¸ªç©å…·äººç‰©æ˜¯ä¸€ä¸ªç®€åŒ–çš„ã€é™æ€çš„äººç‰©è¡¨ç¤ºä¸€æ ·ï¼Œå‡å›å½’å™¨æ˜¯ä½œä¸ºåŸºçº¿çš„åŸºæœ¬æ¨¡å‹ï¼Œç”¨äºæ›´å¤æ‚çš„åˆ†æã€‚](../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png)![](../Images/44e6d84e61c895757ff31e27943ee597.png)![](../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy Baladram](../Images/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----aec39771ddd6--------------------------------)

## é›†æˆå­¦ä¹ 

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----aec39771ddd6--------------------------------)4ç¯‡æ•…äº‹![](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](../Images/22a5d43568e70222eb89fd36789a9333.png)![](../Images/8ea1a2f29053080a5feffc709f5b8669.png)
