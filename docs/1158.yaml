- en: The struggle of Artificially Imitated Intelligence in specialist domains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-struggle-of-artificially-imitated-intelligence-in-specialist-domains-6e63a4e0ebfc?source=collection_archive---------4-----------------------#2024-05-08](https://towardsdatascience.com/the-struggle-of-artificially-imitated-intelligence-in-specialist-domains-6e63a4e0ebfc?source=collection_archive---------4-----------------------#2024-05-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And why the path to real intelligence goes through ontologies and knowledge
    graphs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@konstantin.vasilev.phd?source=post_page---byline--6e63a4e0ebfc--------------------------------)[![Konstantin
    Vasilev](../Images/2694c3cc9ebb075977d6aa1868bc31fc.png)](https://medium.com/@konstantin.vasilev.phd?source=post_page---byline--6e63a4e0ebfc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--6e63a4e0ebfc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--6e63a4e0ebfc--------------------------------)
    [Konstantin Vasilev](https://medium.com/@konstantin.vasilev.phd?source=post_page---byline--6e63a4e0ebfc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--6e63a4e0ebfc--------------------------------)
    ·15 min read·May 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Those who follow me, might remember a similar [AI rant](https://www.linkedin.com/posts/konstantin-vasilev_ai-ml-chatgpt-activity-7018125021319450624-J8sq?utm_source=share&utm_medium=member_desktop)
    from a year ago, under the pseudonym “Grumpy Risk Manager”. Now I’m back, grumpier
    than ever, with specific examples but also ideas for solutions!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7f6e3d27957d8ee3f07ad6a49f780f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: author collage'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) like ChatGPT are impressive in their ability to
    discuss generic topics in natural language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, they struggle in specialist domains such as medicine, finance and law.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is due to lack of real understanding and focus on imitation rather than
    intelligence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are at the peak of their hype. They are considered “intelligent” due to
    their ability to answer and discuss generic topics in natural language.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, once you dive into a specialist/complex domains such as medicine,
    finance, law, it is easy to observe logical inconsistencies, plain mistakes and
    the so called “hallucinations”. To put it simply, the LLM behaves like a pupil
    with a very rich dictionary who tries to pretend that they’ve studied for the
    exam and know all the answers, but they actually don’t! They just pretend to be
    intelligent due to the vast information at their disposal, but their ability to
    reason using this information is very limited. I would even go a step further
    and say that:'
  prefs: []
  type: TYPE_NORMAL
- en: The so-called Artificial Intelligence (AI) is very often Artificial Imitation
    of Intelligence (AII). This is particularly bad in specialist domains like medicine
    or finance, since a mistake there can lead to human harm and financial losses.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let me give you a real example from the domain in which I’ve spent the last
    10 years — financial risk. Good evidence of it being “specialist” is the amount
    of contextual information that has to be provided to the average person in order
    to understand the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: Banks are subject to regulatory Capital requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Capital can be considered a buffer which absorbs financial losses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The requirements to hold Capital, ensures that banks have sufficient capability
    to absorb losses reducing the likelihood of bankruptcy and financial crisis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The rules for setting the requirements in 1\. are based on risk-proportionality
    principles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**→** the riskier the business that banks undertake'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**→** higher risk-weights'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**→** higher capital requirements'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**→** larger loss buffer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**→** stable bank'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The degree of riskiness in 4\. is often measured in the form of credit rating
    of the firms with which the bank does business.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Credit ratings come from different agencies and in different formats.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In order to standardise the ratings, regulators have created mapping rules from
    every rating format to the standardised Credit Quality Step (CQS) in the range
    of 1 to 6.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the regulatory rules for determining the risk-weights in 4\. are based
    on the CQS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rules in 8\. for European banks are set in the Capital Requirements Regulation
    (CRR).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The topic in the 9 statements above seems complex and it really is, there are
    dozens of additional complications and cases that exist, but which I’ve avoided
    on purpose, as they are not even necessary for illustrating the struggle of AII
    with such topics. Furthermore, the complexity doesn’t arise from any of the individual
    9 rules itself, but rather from their combination, there are a lot of concepts
    whose definition is based on several other concepts giving rise to a semantic
    net/graph of relationships connecting the concepts and the rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s focus only on the core concepts and rules in 4, which can be summarised
    as follows: **rating → CQS → risk-weight → capital requirement**. If we also look
    at an example [CRR article 122](https://www.eba.europa.eu/regulation-and-policy/single-rulebook/interactive-single-rulebook/16060),
    we can see a mapping table **CQS → risk-weight** which should be used for corporate
    exposures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/431f996bf908c5eae9e04dd8c578e027.png)'
  prefs: []
  type: TYPE_IMG
- en: Capital Requirements Regulation (CRR 122)
  prefs: []
  type: TYPE_NORMAL
- en: This mapping rule is very simple, it can be understood easily by the average
    person with a bit of context. It is also known by many banking specialists and
    most of them don’t memorise the exact rule but simply know the information of
    the 9 statements above and know where to look in order to see the exact mapping.
  prefs: []
  type: TYPE_NORMAL
- en: Artificially Imitated Intelligence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s formulate a question and see whether AII can deal with it, here is
    what I asked ChatGPT 3.5:'
  prefs: []
  type: TYPE_NORMAL
- en: According to CRR, what is the risk-weight of an exposure to corporate which
    has credit quality step of 5?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'And here is the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Capital Requirements Regulation (CRR), the risk-weight of an exposure
    to a corporate entity depends on its credit quality step. **In the context of
    credit quality steps, a credit quality step of 5 typically represents the highest
    credit quality, indicating a very low risk of default**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the standardised approach outlined in the CRR, exposures to corporates are
    assigned risk weights based on their credit ratings or credit quality steps. The
    risk weights for exposures to corporates range from 20% to 150%, depending on
    the credit quality step and whether the exposure is short-term or long-term.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Since a credit quality step of 5 signifies a very low risk, the corresponding
    risk weight assigned to such an exposure would be one of the lowest. In this case,
    for a corporate with a credit quality step of 5, the risk weight would likely
    be 20%** or similar, indicating a low capital requirement for banks holding such
    exposures.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Can you spot the factual mistakes, highlighted in **bold**?
  prefs: []
  type: TYPE_NORMAL
- en: First very big mistake is that the AII completely misunderstood and inverted
    the **CQS → risk-weight** relationship, high CQS of 5 doesn’t represent low risk
    but exactly the opposite, it is one of the highest risks! **This is actually the
    biggest mistake one could possibly make when answering this question!**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even after the mistake in 1, I would expect the AII to be able to look-up a
    table in an article and conclude that below a CQS of 5, there is a risk-weight
    of 150%. But no, the AII confidently claims 20% risk-weight, due to low risk…
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Although undeserved, I still gave the benefit of doubt to the AII, by asking
    the same question but clarifying the exact CRR article: 122\. Shameless, but confident,
    the AII now responded that the risk-weight should be 100%, still claiming that
    CQS of 5 is good credit quality **→** another obvious mistake.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feeling safe for my job and that the financial industry still needs me, I started
    thinking about solutions, which ironically could make my job unsafe in the future…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why ontologies and knowledge graphs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Enter ontologies, a form of knowledge representation of a particular domain.
    One good way of thinking about it, is in terms of ordering the different ways
    of representing knowledge from least to more sophisticated:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data dictionary:** table with field names and metadata attributes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Taxonomy:** table/s with added nesting of data types and sub-types in terms
    of relationships (e.g. Pigeon <is a type of> Bird)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ontology:** Multidimensional taxonomies with more than one type of relationships
    (e.g. Birds <eat> Seeds) “the unholy marriage of a taxonomy with object oriented
    programming” ([Kurt Cagle, 2017](https://www.linkedin.com/pulse/data-dictionaries-arent-ontologies-kurt-cagle/?trackingId=NvradWFCRsuQ8ucCGeqPPw%3D%3D))'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Why would one want to incorporate such complex relational structure in their
    data? Below are the benefits which will be later illustrated with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Uniform** representation of: **structure**, **data** and **logic**. In the
    example above, Bird is a class which is a template with generic properties = **structure**.
    In an ontology, we can also define many actual instances of individual Birds with
    their own properties = **data**. Finally, we can also add **logic** (e.g. If a
    Bird <eats> more than 5 Seeds, then <it is> not Hungry). This is essentially making
    the data “smart” by incorporating some of the logic as data itself, thus making
    it a reusable knowledge. It also makes information both human and machine readable
    which is particularly useful in ML.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Explainability and Lineage:** most frequent implementation of ontology is
    via Resource Description Framework (RDF) in the form of graphs. These graphs can
    then be queried in order to evaluate existing rules and instances or add new ones.
    Moreover, the chain of thought, through the graph nodes and edges can be traced,
    explaining the query results and avoiding the ML black box problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reasoning and Inference:** when new information is added, a semantic reasoner
    can evaluate the consequences on the graph. Moreover, new knowledge can be derived
    from existing one via “What if” questions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consistency:** any conflicting rules or instances that deviate from the generic
    class properties are automatically identified as an error by the reasoner and
    cannot become part of the graph. This is extremely valuable as it enforces agreement
    of knowledge in a given area, eliminating any subjective interpretations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interoperability and Scalability:** the reusable knowledge can focus on a
    particular specialist domain or connect different domains (see [FIBO](https://github.com/edmcouncil/fibo)
    in finance, [OntoMathPRO](https://ontomathpro.org/) in maths, [OGMS](https://www.ebi.ac.uk/ols4/ontologies/ogms)
    in medicine). Moreover, one could download a general industry ontology and extend
    it with private enterprise data in the form of instances and custom rules.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ontologies can be considered one of the earliest and purest forms of AI, long
    before large ML models became a thing and all based on the idea of making data
    smart via structuring. Here by AI, I mean real intelligence — the reason the ontology
    can explain the evaluated result of a given rule is because it has semantic understanding
    about how things work! The concept became popular first under the idea of Semantic
    Web in the early 2000s, representing the evolution of the internet of linked data
    (Web 3.0), from the internet of linked apps (Web 2.0) and the internet of linked
    pages (Web 1.0).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Graphs (KGs) are a bit more generic term for the storage of data in
    graph format, which may not necessarily follow ontological and semantic principles,
    while the latter are usually represented in the form of a KG. Nowadays, with the
    rise of LLMs, KGs are often seen as a good candidate for resolving their weaknesses
    in specialist domains, which in turn revives the concept of ontologies and their
    KG representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to very interesting convergence of paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: Ontologies aim to generate intelligence through making the data smart via structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LLMs aim to generate intelligence through leaving the data unstructured but
    making the model very large and structural: ChatGPT has around 175 billion parameters!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clearly the goal is the same, and the outcome of whether the data becomes part
    of the model or the model becomes part of the data becomes simply a matter of
    reference frame, inevitably leading to a form of **information singularity.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why use ontologies in banking?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Specialisation:** as shown above, LLMs struggle in specialist fields such
    as finance. This is particularly bad in a field in which mistakes are costly.
    In addition, value added from automating knowledge in specialist domains that
    have fewer qualified experts can be much higher than that of automation in generic
    domains (e.g. replacing banking expert vs support agent).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Audit trail:** when financial items are evaluated and aggregated in a financial
    statement, regulators and auditors expect to have continuous audit trail from
    all granular inputs and rules to the final aggregate result.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Explainability:** professionals rely on having a good understanding of the
    mechanisms under which a bank operates and impact of risk drivers on its portfolios
    and business decisions. Moreover, regulators explicitly require such understanding
    via regular “What if” exercises in the form of stress testing. This is one of
    the reasons ML has seen poor adoption in core banking — the so-called black box
    problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Objectivity and Standardisation:** lack of interpretation and subjectivity
    ensures level playing field in the industry, fair competition and effectiveness
    of the regulations in terms of ensuring financial stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now imagine a perfect world in which regulations such as the CRR are provided
    in the form of ontology rather than free text.
  prefs: []
  type: TYPE_NORMAL
- en: Each bank can import the ontology standard and extend it with its own private
    data and portfolio characteristics, and evaluate all regulatory rules.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Furthermore, the individual enterprise strategy can be also combined with the
    regulatory constraints in order to enable automated financial planning and optimised
    decision making.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the complex composite impacts of the big graph of rules and data can
    be disentangled in order to explain the final results and give insights into previously
    non-obvious relationships.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The below example aims to illustrate these ideas on a minimal effort, maximum
    impact basis!
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the search for solutions of the illustrated LLM weaknesses, I designed the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an ontology in the form of a knowledge graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the **structure** of entities, add individual **instances/data** and
    **logic** governing their interactions, following the CRR regulation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the knowledge graph to evaluate the risk-weight.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask the KG to explain how it reached this result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For creating the simple ontology, I used the [CogniPy](https://cognipy.org/index.html)
    library with the main benefits of:'
  prefs: []
  type: TYPE_NORMAL
- en: Using Controlled Natural Language (CNL) for both writing and querying the ontology,
    meaning no need to know specific graph query languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualisation of the materialised knowledge graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reasoners with ability to explain results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, let’s start by defining the structure of our ontology. This is similar
    to defining classes in objective oriented programming with different properties
    and constraints.
  prefs: []
  type: TYPE_NORMAL
- en: In the first CNL statement, we define the company class and its properties.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Several things to note is that class names are with small letter (company).
    Different relationships and properties are defined with dash-case, while data
    types are defined in the brackets. Gradually, this starts to look more and more
    like a fully fledged programming language based on plain English.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we illustrate another ability to denote the uniqueness of the company
    based on its id via generic class statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s add some data or instances of the company class, with instances starting
    with capital letter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here we add a data point with a specific company called Lamersoft, with assigned
    values to its properties. Of course, we are not limited to a single data point,
    we could have thousands or millions in the same ontology and they can be imported
    with or without the **structure** or the **logic** components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve added **data** to our **structure**, we can query the ontology
    for the first time to get all companies, which returns a DataFrame of instances
    matching the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/459dc93ed116228cec57669b95e9505c.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrame with query results
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also plot our knowledge graph, which shows the relationship between
    the Lamersoft instance and the general class company:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d49486c8659c942f3ef52d7ca14970eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Ontology graph
  prefs: []
  type: TYPE_NORMAL
- en: Logic
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, let’s add some simple rules implementing the CRR risk-weight regulations
    for corporates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The first rule defines what a corporate is, which usually is a company with
    large turnover above 50 million. The second rule implements part of the CRR mapping
    table **CQS → risk-weight** which was so hard to understand by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding the rules, we’ve completed our ontology and can plot the knowledge
    graph again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de5bb6782706d8277b078d70e65dc6f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Ontology graph with evaluated rules
  prefs: []
  type: TYPE_NORMAL
- en: 'Notably, 2 important deductions have been made automatically by the knowledge
    graph as soon as we’ve added the **logic** to the **structure** and **data**:'
  prefs: []
  type: TYPE_NORMAL
- en: Lamersoft has been identified as a corporate due to its turnover property and
    the corporate classification rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lamersoft’s risk-weight has been evaluated due to its CQS property and the CRR
    rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is all as a result of the magical automated consistency (no conflicts)
    of all information in the ontology. If we were to add any rule or instance that
    contradicts any of the existing information we would get an error from the reasoner
    and the knowledge graph would not be materialised.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can also play with the reasoner and ask why a given evaluation has been
    made or what is the chain of thought and audit trail leading to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of the output formatting, we can still clearly read that by the two
    expressions defining Lamersoft as a company and its specific turnover, it was
    concluded that it is a corporate because of the specific turnover condition. Unfortunately,
    the current library implementation doesn’t seem to support an explanation of the
    risk-weight result, which is food for the future ideas section.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, I deem the example successful as it managed to unite in a single
    scalable ontology, **structure**, **data** and **logic**, with minimal effort
    and resources, using natural English. Moreover, it was able to make evaluations
    of the rules and explain them with a complete audit trail.
  prefs: []
  type: TYPE_NORMAL
- en: 'One could say here, ok what have we achieved, it is just another programming
    language closer to natural English, and one could do the same things with Python
    classes, instances and assertions. And this is true, to the extent that any programming
    language is a communication protocol between human and machine. Also, we can clearly
    observe the trend of the programming syntaxes moving closer to the human language,
    from the Domain Driven Design (DDD) focusing on implementing the actual business
    concepts and interactions, to the LLM add-ons of Integrated Development Environments
    (IDEs) to generate code from natural language. This becomes a clear trend:'
  prefs: []
  type: TYPE_NORMAL
- en: The role of programmers as intermediators between the business and the technology
    is changing. Do we need code and business documentation, if the former can be
    generated directly from the natural language specification of the business problem,
    and the latter can be generated in the form of natural language definition of
    the logic by the explainer?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine a world in which all banking regulations are provided centrally by the
    regulator not in the form of text but in the form of an ontology or smart data,
    that includes all structure and logic. While individual banks import the central
    ontology and extend it with their own data, thus automatically evaluating all
    rules and requirements. This will remove any room for subjectivity and interpretation
    and ensure a complete audit trail of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond regulations, enterprises can develop their own ontologies in which they
    encode, automate and reuse the knowledge of their specialists or different calculation
    methodologies and governance processes. On an enterprise level, such ontology
    can add value for enforcing a common dictionary and understanding of the rules
    and reduce effort wasted on interpretations and disagreements which can be redirected
    to building more knowledge in the form of ontology. The same concept can be applied
    to any specialist area in which:'
  prefs: []
  type: TYPE_NORMAL
- en: Text association is not sufficient and LLMs struggle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data for effective ML training is not available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly-qualified specialists can be assisted by real artificial intelligence,
    reducing costs and risks of mistakes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If data is nowadays deemed as valuable as gold, I believe that the real diamond
    is structured data, that we can call knowledge. Such knowledge in the form of
    ontologies and knowledge graphs can also be traded between companies just like
    data is traded now for marketing purposes. **Who knows, maybe this will evolve
    into a pay-per-node business model, where expertise in the form of smart data
    can be sold as a product or service.**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then we can call intelligence our ability to accumulate knowledge and to query
    it for getting actionable insights. This can evolve into specialist AIs that tap
    into ontologies in order to gain expertise in a given field and reduce hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are already making an impact on company profits — [Klarna is expected to
    have $40 million improvement on profits](https://www.pymnts.com/buy-now-pay-later/2024/klarna-boosts-profits-with-chatgpt-bnpl-firms-tap-artificial-intelligence/)
    as a result of ChatGPT handling most of its customer service chats, reducing the
    costs for human agents.
  prefs: []
  type: TYPE_NORMAL
- en: Note however the exact area of application of the LLM! This is not the more
    specialised fields of financial/product planning or asset and liabilities management
    of a financial company such as Klarna. It is the general customer support service,
    which is the entry level position in many companies, which already uses a lot
    of standardised responses or procedures. The area in which it is easiest to apply
    AI but also in which the value added might not be the largest. In addition, the
    risk of LLM hallucination due to lack of real intelligence is still there. Especially
    in the financial services sector, any form of “financial advice” by the LLM can
    lead to legal and regulatory repercussions.
  prefs: []
  type: TYPE_NORMAL
- en: Future ideas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LLMs already utilise knowledge graphs in the so-called Retrieval-Augmented
    Generation (RAG). However, these graphs are generic concepts that might include
    any data structure and do not necessarily represent ontologies, which use by LLMs
    is relatively less explored. This gives me the following ideas for next article:'
  prefs: []
  type: TYPE_NORMAL
- en: Use plain English to query the ontology, avoiding reliance on particular CNL
    syntax — this can be done via NLP model that generates queries to the knowledge
    graph in which the ontology is stored — [chatting with KGs](https://www.youtube.com/watch?v=Kla1c_p5v0w).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a more robust way of generating the ontology — the [CogniPy](https://cognipy.org/index.html)
    library was useful for quick illustration, however, for extended use a more proven
    framework for ontology-oriented programming should be used like [Owlready2](https://owlready2.readthedocs.io/en/latest/index.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Point 1\. enables the general user to get information from the ontology without
    knowing any programming, however, point 2 implies that a software developer is
    needed for defining and writing to the ontology (which has its pros and cons).
    However, if we want to close the AI loop, then specialists should be able to define
    ontologies using natural language and without the need for developers. This will
    be harder to do, but similar examples already exist: [LLM with KG interface](https://www.youtube.com/watch?v=XMwa9q6wWkw&t=1s),
    [entity resolution](/an-introduction-to-entity-resolution-needs-and-challenges-97fba052dde5).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A proof of concept that achieves all 3 points above can claim the title of true
    AI, it should be able to develop knowledge in a smart data structure which is
    both human and machine readable, and query it via natural language to get actionable
    insights with complete transparency and audit trail.
  prefs: []
  type: TYPE_NORMAL
- en: Follow me for part 2!
  prefs: []
  type: TYPE_NORMAL
