<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Detecting Concept Shift: Impact on Machine Learning Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Detecting Concept Shift: Impact on Machine Learning Performance</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-concept-shift-impact-on-machine-learning-performance-16923261cda8?source=collection_archive---------9-----------------------#2024-01-16">https://towardsdatascience.com/detecting-concept-shift-impact-on-machine-learning-performance-16923261cda8?source=collection_archive---------9-----------------------#2024-01-16</a></blockquote><div><div class="em ff fg fh fi fj"/><div class="fk fl fm fn fo"><div class="ab cb"><div class="ci bh ew ex ey ez"><h2 id="3b90" class="fp fq fr bf b dy fs ft fu fv fw fx dx fy" aria-label="kicker paragraph">MLOps</h2><div/><div><h2 id="2af5" class="pw-subtitle-paragraph gt ga fr bf b gu gv gw gx gy gz ha hb hc hd he hf hg hh hi cq dx">When should I retrain my model?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><div><div class="bm" aria-hidden="false"><a href="https://michaloleszak.medium.com/?source=post_page---byline--16923261cda8--------------------------------" rel="noopener follow"><div class="l hp hq by hr hs"><div class="l ed"><img alt="Michał Oleszak" class="l ep by dd de cx" src="../Images/61b32e70cec4ba54612a8ca22e977176.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*fjA_nwCfsnZsxqOlVdKOCA.png"/><div class="ht by l dd de em n hu eo"/></div></div></a></div></div><div class="hv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--16923261cda8--------------------------------" rel="noopener follow"><div class="l hw hx by hr hy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ht by l br hz em n hu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ic id bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://michaloleszak.medium.com/?source=post_page---byline--16923261cda8--------------------------------" rel="noopener follow">Michał Oleszak</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ic id dx"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="bf b bg z dx"><div class="ab cn in io ip"><div class="iq ir ab"><div class="bf b bg z dx ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--16923261cda8--------------------------------" rel="noopener follow"><p class="bf b bg z iu iv iw ix iy iz ja jb bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 16, 2024</span></div></span></div></span></div></div></div><div class="ab cp je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w ea eb q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ed it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="bf b dy z dx"><span class="kq">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kp li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lg lh">1</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq mr"><img src="../Images/df5c3efdf2b6711c8bebfeb66348eb3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y7RUleuhqGpGXMvO"/></div></div></figure><p id="d0eb" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Have you heard of lifelong learning? You might be familiar with the story: with today’s rapid technology advancements, what we learned at school will not set us up for professional success for our whole career. To stay useful in the job market, one needs to learn how to learn continuously. In this aspect of life, AI is not so different from us humans. Machine learning models’ knowledge becomes obsolete, too, and they need to relearn stuff just like we do. But when does a model become obsolete?</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq nz"><img src="../Images/dc5b3128f07866fbee190bbda8a97d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*f5HSr-4IvOn726Ic.png"/></div></figure><h1 id="8f31" class="oa ob fr bf oc od oe gw of og oh gz oi oj ok ol om on oo op oq or os ot ou ov bk">What is concept shift, and can we detect it?</h1><p id="8660" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">The phenomenon responsible for ML models’ knowledge going stale is known as <em class="pb">concept shift</em>. However, before we dive into the details, let’s take a quick high-level overview of the broader problem: data shifts.</p><h2 id="2df9" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Data shifts primer</h2><p id="56bc" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">The world changes. Consumer behaviors and tastes evolve over time; your users might change their preferences as they grow older; data-collecting devices tend to break or malfunction in unexpected ways. Whatever industry you are working in, and whatever problem you’re solving with machine learning, you can be sure that at some point, the data your production model receives will be different from the data it has seen during training. As a consequence of this, machine learning models tend to deteriorate over time after being deployed to production.</p><h2 id="5511" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Types of data shift</h2><p id="cbdf" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">The changes in the world can translate to the changes in your data in different ways. To better understand this, it’s useful to introduce a bit of notation.</p><p id="0acc" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Machine learning models, in general, operate on two kinds of input data: features, <em class="pb">X</em>, and targets, <em class="pb">y</em>. The data shift in its most generic form is described as a change in the joint distribution of features and targets, <em class="pb">P(X, Y)</em>. There are four potential causes for <em class="pb">P(X, Y)</em> to change<em class="pb">.</em></p><p id="3c48" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">To list all four, we need to use the so-called product rule, a mathematical formula stating that P(X, Y) = P(Y, X) = P(X|Y)P(Y) = P(Y|X)P(X).</p><p id="ba8e" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">From there, it follows that the joint distribution of features and targets (which can be equivalently written as P(X, Y) or P(Y, X) can be decomposed in two alternative and equivalent ways:</p><ol class=""><li id="b647" class="nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny ps pt pu bk">P(X|Y) * P(Y)</li><li id="d34e" class="nd ne fr nf b gu pv nh ni gx pw nk nl nm px no np nq py ns nt nu pz nw nx ny ps pt pu bk">P(Y|X) * P(X)</li></ol><p id="86e2" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">This means that if any of the four elements above changes, P(X, Y) will also change, resulting in a data shift. The change of each of the four elements has its own name, its own causes, and its own solutions. Let’s take a look at them briefly.</p><p id="5dbc" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Side note: I said that each of the four elements can change, leading to a data shift. But of course, there is no rule forbidding multiple of the four elements to change at the same time. In fact, they often do, causing the resulting data shift to be a multifaceted and complex phenomenon. In this article, however, let’s assume only one of the four changes at any given time.</p><p id="18b4" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">So, back to the four types of data shift.</p><ul class=""><li id="29ca" class="nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa pt pu bk">If P(X) changes (and P(Y|X) remains unchanged), we are talking about <em class="pb">covariate shift</em>. The name makes a lot of sense once we realize that covariate is just another term for the feature or the independent variable in a model. Covariate shift is when the distribution of the model inputs changes.</li><li id="0c97" class="nd ne fr nf b gu pv nh ni gx pw nk nl nm px no np nq py ns nt nu pz nw nx ny qa pt pu bk">If P(Y) changes (but P(X|Y) remains unchanged), we are talking about a <em class="pb">label shift</em>. It means the output distribution changed, but for any given output, the input distribution stays the same.</li><li id="e1c9" class="nd ne fr nf b gu pv nh ni gx pw nk nl nm px no np nq py ns nt nu pz nw nx ny qa pt pu bk">If P(Y|X) changes (but P(X) remains unchanged), that’s the <em class="pb">concept shift</em>, the topic of this article. We will explore it in detail soon.</li><li id="9d2c" class="nd ne fr nf b gu pv nh ni gx pw nk nl nm px no np nq py ns nt nu pz nw nx ny qa pt pu bk">Finally, the situation in which P(X|Y) changes while P(Y) remains the same is known as <em class="pb">manifestation shift</em>. It means that the same target values manifest themselves differently in the input distribution. We won’t cover manifestation shifts here, leaving it for a separate article.</li></ul><p id="aa39" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Out of the four types of data shift, covariate shift and concept shift are the most widely discussed and are arguably the major concerns for most companies having ML models serving predictions in production. Let’s discuss detecting the two to see how concept shift detection introduces new challenges compared to covariate shift detection.</p><h2 id="1d7f" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Detecting data shifts</h2><p id="85bc" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Covariate shift is arguably easier to both understand and detect. Let’s revisit: it’s a situation in which P(X) changes. In other words, the distribution of the model’s input features at serving time is different from the one it has seen in training.</p><p id="8fc0" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In the vast majority of cases, one has access to both training features and serving features. It’s enough to compare their distributions: if they differ, a covariate shift has happened.</p><p id="903d" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Alright, that’s an oversimplification. In reality, there are two approaches to measuring covariate shift. We can look at it in a univariate way by checking if the distribution of one or more of the features has changed, or in a multivariate way where we focus on the joint distribution of all features.</p><p id="f46f" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In the univariate approach, one can <a class="af qb" rel="noopener" target="_blank" href="/how-to-detect-data-drift-with-hypothesis-testing-1a3be3f8e625">compare training and serving distributions using statistical tests and distance measures</a>, feature by feature. In the multivariate approach, <a class="af qb" rel="noopener" target="_blank" href="/detecting-covariate-shift-a-guide-to-the-multivariate-approach-c099bd1891b9">a more nuanced approach based on PCA is a good way to go</a>. But in either case, the task is to compare two observed quantities and decide whether they are truly different or not.</p><p id="d3a5" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">In the case of concept shift, the challenge of shift detection is more involved. Let’s revisit: concept shift is when P(Y|X) changes, that is, for given feature values, the target distribution changes.</p><p id="4f79" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The tricky part is in measuring and comparing P(Y|X), often referred to as the <em class="pb">concept. </em>It’s not a single quantity that can be easily calculated. It’s the true mapping, or relation, between inputs and outputs. We know it for the training data (to the best of our model’s ability), but how can we know when it changes in the real world? Let’s see!</p><h1 id="121d" class="oa ob fr bf oc od oe gw of og oh gz oi oj ok ol om on oo op oq or os ot ou ov bk">Concept shift detection in the wild</h1><p id="3d35" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Thanks for bearing with me through this rather lengthy introduction! Now that we know what concept shift is and why it’s challenging to detect, let’s discuss it in greater detail, following a practical example.</p><h2 id="ddb5" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Concept shift in time &amp; space</h2><p id="63bf" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Concept shift means that for specific inputs, the distribution of the output has changed (P(Y|X) has changed, remember?). This change can occur in either of the two dimensions: in time or space.</p><p id="1c76" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Concept shift in time means that the concept the model has learned during training has since then changed in the real world. In other words, the model’s knowledge is not up-to-date anymore.</p><p id="dacd" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Let me borrow an example from Chip Huyen’s fantastic book “<em class="pb">Designing Machine Learning Systems</em>”: imagine you’re building a model to predict housing prices in San Francisco. Before the coronavirus pandemic, a three-bedroom apartment might have cost $2m, but because of the virus, many people have left the city, and as a result of declining demand, the same apartment could now cost $1.5m. The feature distributions P(X) have not changed: the houses still have the same number of bedrooms, square footage, etc. It’s just that the same set of inputs now maps to a different output.</p><p id="5387" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Concept shift in space when a concept learned from data from a particular geography or a particular set of users is not relevant for different regions or user bases. For example, adding 50 square feet to a San Francisco apartment can result in a significant price increase. However, the same addition to a house in rural Wyoming, where the housing market is much less competitive, might not translate to an equally large price increase.</p><p id="1f5d" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Alright, so what we know so far is that concept shift might be a problem when either some time has passed since model deployment, or when the model starts serving different users or geographies. But how do we go about detecting it?</p><h2 id="85f5" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Detecting concept shift</h2><p id="fffe" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Imagine this: you train your San Francisco house pricing model on all available data and deploy it to production. Afterward, you collect the features that the model receives for inference and store them in daily batches.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qc"><img src="../Images/6bef8a3560f8092a63ddaee52ef3f3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3fYbmyw_ZWQ75NhC"/></div></div><figcaption class="qd qe qf mp mq qg qh bf b bg z dx">Training and serving data sets. Image by the author.</figcaption></figure><p id="bd11" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here, <em class="pb">X-serve-0 </em>are the features from the day of deployment, <em class="pb">X-serve-1 </em>are the features from the following day, and so on, while <em class="pb">y-serve-* </em>denotes the corresponding targets.</p><p id="0a7b" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">It’s day 0 today: the model trained on data up until yesterday is now in production. Are today’s data (<em class="pb">X-serve-0</em> and <em class="pb">y-serve-0</em>) subject to concept shift?</p><p id="25c0" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Let’s assume for a moment that this is a binary question. In practice, of course, concept shift can be large or small and impact model performance heavily or not very much. But for now, let’s say that concept shift has either happened on day 0 or not.</p><p id="1a57" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Here’s an idea: let’s train a model on day 0 data. If there was no concept shift, it should learn the same features-to-target mapping that our production model has learned. If concept shift occurred, the learned mapping will be different.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qc"><img src="../Images/1b3750236f177775215c26c9456b7fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vlrD4DXjBU86-0aS"/></div></div><figcaption class="qd qe qf mp mq qg qh bf b bg z dx">Concept shift detection mechanism. Image by the author.</figcaption></figure><p id="6d36" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Next, let’s use this day-0 model to make predictions for test data: we just feed it <em class="pb">X-test</em>. If the outputs are close to the test-set predictions from the production model<em class="pb">, </em>it means that our day-0 model has learned the same P(Y|X), or the same concept, as our production model. Therefore, we proclaim no concept shift. If the outputs are different, however, then concept shift must have happened.</p><blockquote class="qi"><p id="0415" class="qj qk fr bf ql qm qn qo qp qq qr ny dx">We can detect concept shift by training a model on serving data and comparing it to the production model.</p></blockquote><p id="eaa5" class="pw-post-body-paragraph nd ne fr nf b gu qs nh ni gx qt nk nl nm qu no np nq qv ns nt nu qw nw nx ny fk bk">We can repeat this process daily with every new batch of data we receive in serving to keep refreshing our knowledge of whether a concept shift has happened or not.</p><h2 id="a223" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Concept shift: detection vs. impact on performance</h2><p id="d9ac" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">This is all nice, but there is one caveat to it, which a watchful reader might have spotted already. The outputs from the <em class="pb">day-*</em> models will never be exactly the same as the ones from the production model: even in the absence of any shift, the sampling error (different sample of training data) will lead to slightly different results. How large differences do actually signal concept shift? Or, to rephrase this question more practically: when do we need to retrain the model?</p><p id="446d" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Indeed, not every difference should call for retraining, which could be a costly or complex procedure. As mentioned above, the difference might sometimes be the result of random sampling, in which case no retraining is necessary. On other occasions, the difference might actually be caused by the concept shift, but one that’s not impacting the model in a meaningful way. In this case, retraining is not needed either.</p><p id="cc0b" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The key observation to take away here is that one should only retrain the model when the concept shift is meaningfully impacting the model’s performance.</p><blockquote class="qi"><p id="f654" class="qj qk fr bf ql qm qn qo qp qq qr ny dx">One should only retrain the model when the concept shift is meaningfully impacting the model’s performance.</p></blockquote><p id="2139" class="pw-post-body-paragraph nd ne fr nf b gu qs nh ni gx qt nk nl nm qu no np nq qv ns nt nu qw nw nx ny fk bk">So how do we tell how much is the performance impacted by concept shift? Let’s flip this question: are there situations where concept shift occurs but does not hurt the model’s performance?</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq nz"><img src="../Images/dc5b3128f07866fbee190bbda8a97d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*f5HSr-4IvOn726Ic.png"/></div></figure><h1 id="8bed" class="oa ob fr bf oc od oe gw of og oh gz oi oj ok ol om on oo op oq or os ot ou ov bk">Harmless concept shift</h1><p id="cc50" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Imagine that your San Francisco house pricing model is now a classification model in which you are predicting whether a house costs more or less than $1m given its features. You have followed the steps described above to find large differences between the outputs of the production model and the current-day model.</p><h2 id="7173" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Unchanged predicted labels</h2><p id="8e22" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Here is the plot showing the differences in the probability of the house costing more than $1m from the two models for a subset of 10 data points.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq qx"><img src="../Images/baa635b782a882e366827284c4ea5be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*zrpQGOWH0fsEcgR4"/></div><figcaption class="qd qe qf mp mq qg qh bf b bg z dx">Concept shift is harmless if final predictions don’t change. Image by the author.</figcaption></figure><p id="7b68" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">There are three important observations to be made here. First, the two models predict completely different probabilities. The difference is large for each data point and can be as significant as close to 50 percentage points. We can be almost certain that a significant concept shift has occurred.</p><p id="98b0" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Second, there is no consistency in the two models’ relative outputs. Sometimes one produces a much higher probability than the other, sometimes the other way round.</p><p id="fb57" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Third, the concept shift we are experiencing is completely harmless for the model. Wait, what? That’s right! Although significant, the concept shift we’re dealing with will not impact the model performance at all!</p><blockquote class="qi"><p id="85e9" class="qj qk fr bf ql qm qn qo qp qq qr ny dx">Concept shift does not always impact model performance.</p></blockquote><p id="6495" class="pw-post-body-paragraph nd ne fr nf b gu qs nh ni gx qt nk nl nm qu no np nq qv ns nt nu qw nw nx ny fk bk">Recall we’re looking at a binary classification task. Given a customary decision threshold at 50%, for each data point, both models will yield the same prediction: data points 2, 3, 4, 5, and 8 correspond to positive predictions (price above $1m), and the remaining ones — to negative predictions. Performance metrics such as accuracy, precision, recall, or f1-score will be the same for both models (ROC AUC will be impacted, though, since it uses the model scores rather than just class assignments).</p><p id="94e1" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">I admit that this example is artificial and has been deliberately drafted to show what I’m trying to convey: that concept shift need not impact performance. But fair enough — in reality, one would rarely ever just use the predicted labels while disregarding certainty scores. Let’s look at another, arguably more realistic scenario in which concept shift will not hurt you.</p><h2 id="d71b" class="pc ob fr bf oc pd pe pf of pg ph pi oi nm pj pk pl nq pm pn po nu pp pq pr fx bk">Shift in sparse regions</h2><p id="3468" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">Model features constitute a multidimensional space, and each training example is a point in this space. If you only had two features, x1 and x2, you could plot each example as a point on a two-dimensional plane — the feature space. With three features, each example will be a point inside a cube. In the more common situations of using four features or more, our brains fail to imagine the scene, but still, each example is a point in the feature space.</p><p id="bdbf" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The training examples are not uniformly distributed across the feature space. Some areas within the feature space will be densely packed by data points, while elsewhere they will be quite sparse. Another way to think about it is that in your data, some combinations of feature values are frequent and others very rare.</p><p id="36e0" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Now, here’s the thing: concept shift might occur in any region within the feature space. If it happens to be in a sparse region, its impact on the model’s performance will be minor. This is because there is not much training nor serving data in this region. Thus, the model will hardly ever get to predict in this region. Any misclassifications caused by the concept shift in a sparse region will be rare events, not contributing much to the model’s overall performance.</p><blockquote class="qi"><p id="b0e6" class="qj qk fr bf ql qm qn qo qp qq qr ny dx">Misclassifications caused by the concept shift in a sparse region will be rare events, not contributing much to the model’s overall performance.</p></blockquote><p id="b77e" class="pw-post-body-paragraph nd ne fr nf b gu qs nh ni gx qt nk nl nm qu no np nq qv ns nt nu qw nw nx ny fk bk">The takeaway from the two stories above is that some concept shifts are harmless, and only a meaningfully negative impact on performance calls for model retraining. Once you have detected a concept shift, estimate its impact on your model first before taking unnecessary action!</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq nz"><img src="../Images/dc5b3128f07866fbee190bbda8a97d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*f5HSr-4IvOn726Ic.png"/></div></figure><h1 id="c991" class="oa ob fr bf oc od oe gw of og oh gz oi oj ok ol om on oo op oq or os ot ou ov bk">Tools for concept shift detection</h1><p id="896f" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">We could summarize our whole discussion up to this point as: don’t focus on the shift’s presence. Detect its impact on performance instead.</p><p id="3e02" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">However, this is not how people typically do it. A quick web search reveals that most approaches to concept shift detection (such as <a class="af qb" href="https://deepchecks.com/how-to-detect-concept-drift-with-machine-learning-monitoring/" rel="noopener ugc nofollow" target="_blank">this one from DeepChecks blog</a> or <a class="af qb" href="https://www.evidentlyai.com/ml-in-production/concept-drift#how-to-detect-concept-drift" rel="noopener ugc nofollow" target="_blank">this one from Evidently AI</a>) work indirectly: they are typically based on detecting the prediction drift, label drift, or data drift.</p><p id="ad30" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">The only tool I found that claims to be able to directly detect the magnitude of concept shift, and more importantly to quantify its impact on model performance as we have just discussed, is NannyML. I contacted the team and was told that besides being available as a standalone algorithm on <a class="af qb" href="https://aws.amazon.com/marketplace/pp/prodview-64nptz3lrs4gc" rel="noopener ugc nofollow" target="_blank">AWS </a>(which had appeared in my search), it is also available as an <a class="af qb" href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps/nannyml1682590100745.nannyml-managed?tab=Overview" rel="noopener ugc nofollow" target="_blank">Azure managed app</a>.</p><p id="610e" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">This approach follows the previously discussed workflow. Every day after deployment, a day-model is trained on serving data collected on this particular day. Next, we look at the predicted probabilities that our day-model produced for the training data and compare them with the ones from the production model. These differences let us estimate the shift’s impact on performance metrics such as ROC AUC, accuracy, and others.</p><p id="b57c" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">I used the free trial to see how to estimate the performance implications of a concept shift in practice for a classification task. And no, it won’t be about San Francisco housing again.</p><p id="9c95" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Consider flight cancellations. They are primarily driven by operational factors like weather conditions or airline-specific problems. We can use these features to quite reliably predict whether a given flight will be canceled or not.</p><p id="38cc" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Or at least that was the case until the end of the year 2019. With the onset of the COVID-19 pandemic, travel restrictions, lockdowns, and a sharp decrease in travel demand led to a significant increase in flight cancellations, fundamentally changing the relationship between factors such as weather and cancellations. For example, good weather did not guarantee fewer cancellations anymore.</p><p id="f5bf" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Let’s train a model to predict cancellations on data up to the year 2018, and treat years 2019 through 2023 as our serving data based on the <a class="af qb" href="https://www.transtats.bts.gov/homedrillchart.asp" rel="noopener ugc nofollow" target="_blank">data from the Bureau of Transportation Statistics</a>. Here’s what NannyML’s concept shift detection algorithm outputs.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mp mq qc"><img src="../Images/ac91e608bb1d4382a93e0e06171639a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*utYlGK8nwaY-2Fha"/></div></div><figcaption class="qd qe qf mp mq qg qh bf b bg z dx">NannyML’s concept shift detection. Image by the author.</figcaption></figure><p id="6d53" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">During the first year after deployment, 2019, no significant concept shift seems to have happened. Our thresholds for meaningful performance change were not crossed. The following year, however, as the pandemic broke out, our cancellation classifier lost 6 accuracy percentage points! Interestingly, the following year, things roughly got back to their pre-pandemic state.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq nz"><img src="../Images/dc5b3128f07866fbee190bbda8a97d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*f5HSr-4IvOn726Ic.png"/></div></figure><h1 id="e049" class="oa ob fr bf oc od oe gw of og oh gz oi oj ok ol om on oo op oq or os ot ou ov bk">Considerations &amp; Conclusion</h1><p id="a174" class="pw-post-body-paragraph nd ne fr nf b gu ow nh ni gx ox nk nl nm oy no np nq oz ns nt nu pa nw nx ny fk bk">A Concept shift is a change in the mapping between features and targets, while the features themselves remain unchanged. Think of it as: the same inputs, different outputs. It’s arguably harder to detect than its evil twin, covariate shift, in which the features’ distributions change.</p><p id="b51e" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">A clever way of detecting concept shift is to regularly train models on incoming serving data and compare the concept they learn to the concept learned by the production model. If they are different, concept shift must have happened. This approach has some limitations, though. It assumes that the targets for the serving data are available, which is not the case in many applications.</p><p id="ee97" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Finally, not all concept shift is bad. In some situations, however, it can negatively impact the performance of your models in production, and by extension, the business value delivered by these models. By following the approach outlined above, you can quantify your concept shift’s impact and ensure your ML models continue to provide value.</p><figure class="ms mt mu mv mw mx mp mq paragraph-image"><div class="mp mq nz"><img src="../Images/dc5b3128f07866fbee190bbda8a97d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*f5HSr-4IvOn726Ic.png"/></div></figure><p id="ebf0" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">Thanks for reading!</p><p id="0f18" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">If you liked this post, why don’t you <a class="af qb" href="https://michaloleszak.medium.com/subscribe" rel="noopener"><strong class="nf gb">subscribe for email updates</strong></a> on my new articles? By <a class="af qb" href="https://michaloleszak.medium.com/membership" rel="noopener"><strong class="nf gb">becoming a Medium member</strong></a>, you can support my writing and get unlimited access to all stories by other authors and yours truly. Need consulting? You can ask me anything or book me for a 1:1 <a class="af qb" href="https://topmate.io/michaloleszak" rel="noopener ugc nofollow" target="_blank"><strong class="nf gb">here</strong></a>.</p><p id="bbcb" class="pw-post-body-paragraph nd ne fr nf b gu ng nh ni gx nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fk bk">You can also try one of <a class="af qb" href="https://michaloleszak.github.io/blog/" rel="noopener ugc nofollow" target="_blank">my other articles</a>. Can’t choose? Pick one of these:</p><div class="qy qz ra rb rc rd"><a rel="noopener follow" target="_blank" href="/how-to-detect-data-drift-with-hypothesis-testing-1a3be3f8e625?source=post_page-----16923261cda8--------------------------------"><div class="re ab im"><div class="rf ab co cb rg rh"><h2 class="bf gb ic z iu ri iw ix rj iz jb ga bk">How to Detect Data Drift with Hypothesis Testing</h2><div class="rk l"><h3 class="bf b ic z iu ri iw ix rj iz jb dx">Hint: forget about the p-values</h3></div><div class="gr l"><p class="bf b dy z iu ri iw ix rj iz jb dx">towardsdatascience.com</p></div></div><div class="rl l"><div class="rm l rn ro rp rl rq lx rd"/></div></div></a></div><div class="qy qz ra rb rc rd"><a rel="noopener follow" target="_blank" href="/organizing-a-machine-learning-monorepo-with-pants-8e0570de0c4c?source=post_page-----16923261cda8--------------------------------"><div class="re ab im"><div class="rf ab co cb rg rh"><h2 class="bf gb ic z iu ri iw ix rj iz jb ga bk">Organizing a Machine Learning Monorepo with Pants</h2><div class="rk l"><h3 class="bf b ic z iu ri iw ix rj iz jb dx">Streamline your ML workflow management</h3></div><div class="gr l"><p class="bf b dy z iu ri iw ix rj iz jb dx">towardsdatascience.com</p></div></div><div class="rl l"><div class="rr l rn ro rp rl rq lx rd"/></div></div></a></div><div class="qy qz ra rb rc rd"><a rel="noopener follow" target="_blank" href="/self-supervised-learning-in-computer-vision-fd43719b1625?source=post_page-----16923261cda8--------------------------------"><div class="re ab im"><div class="rf ab co cb rg rh"><h2 class="bf gb ic z iu ri iw ix rj iz jb ga bk">Self-Supervised Learning in Computer Vision</h2><div class="rk l"><h3 class="bf b ic z iu ri iw ix rj iz jb dx">How to train models with only a few labeled examples</h3></div><div class="gr l"><p class="bf b dy z iu ri iw ix rj iz jb dx">towardsdatascience.com</p></div></div><div class="rl l"><div class="rs l rn ro rp rl rq lx rd"/></div></div></a></div></div></div></div></div>    
</body>
</html>