<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Advanced Retriever Techniques to Improve Your RAGs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Advanced Retriever Techniques to Improve Your RAGs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61?source=collection_archive---------0-----------------------#2024-04-17">https://towardsdatascience.com/advanced-retriever-techniques-to-improve-your-rags-1fac2b86dd61?source=collection_archive---------0-----------------------#2024-04-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="bae4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Master Advanced Information Retrieval: Cutting-edge Techniques to Optimize the Selection of Relevant Documents with <strong class="al">Langchain</strong> to Create Excellent RAGs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@damiangilgonzalez?source=post_page---byline--1fac2b86dd61--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Damian Gil" class="l ep by dd de cx" src="../Images/8b378c321ee21b0bd40faa14db7e9487.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*g7rD02yuhg7PQyT7BIs0Ag.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--1fac2b86dd61--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@damiangilgonzalez?source=post_page---byline--1fac2b86dd61--------------------------------" rel="noopener follow">Damian Gil</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--1fac2b86dd61--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">9</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="56e7" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Content Table</h1><p id="fb01" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk"><strong class="np fr">· </strong><a class="af oj" href="#a896" rel="noopener ugc nofollow"><strong class="np fr">Introduction</strong></a><strong class="np fr"><br/>· </strong><a class="af oj" href="#d7a4" rel="noopener ugc nofollow"><strong class="np fr">Vectore Store Creation</strong></a><strong class="np fr"><br/>· </strong><a class="af oj" href="#10a9" rel="noopener ugc nofollow"><strong class="np fr">Method: Naive Retriever</strong></a><strong class="np fr"><br/>· </strong><a class="af oj" href="#1f6d" rel="noopener ugc nofollow"><strong class="np fr">Method: Parent Document Retriever</strong></a><strong class="np fr"><br/>· </strong><a class="af oj" href="#b2c7" rel="noopener ugc nofollow"><strong class="np fr">Method: Self Query Retriever</strong></a><strong class="np fr"><br/></strong> ∘ <a class="af oj" href="#a483" rel="noopener ugc nofollow">Query Constructor</a><br/> ∘ <a class="af oj" href="#21ad" rel="noopener ugc nofollow">Query Translater</a><strong class="np fr"><br/>· </strong><a class="af oj" href="#8232" rel="noopener ugc nofollow"><strong class="np fr">Method: Contextual Compression Retriever (Reranking)</strong></a><strong class="np fr"><br/>· </strong><a class="af oj" href="#3c63" rel="noopener ugc nofollow"><strong class="np fr">Conclusion</strong></a></p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="a896" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Introduction</h1><p id="ae43" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk"><strong class="np fr"><em class="ok">Let’s briefly remember what the 3 acronyms that make up the word RAG mean:</em></strong></p><ul class=""><li id="7036" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Retrieval</strong>: The main objective of a RAG is to collect the most relevant documents/chunks regarding the query.</li><li id="33d9" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Augmented</strong>: Create a well-structured prompt so that when the call is made to the LLM, it knows perfectly what its purpose is, what the context is and how it should respond.</li><li id="bcc4" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Generation</strong>: This is where the LLM comes into play. When the model is given good context (provided by the “Retrieval” step) and has clear instructions (provided by the “Augmented” step), it will generate high-value responses for the user.</li></ul><p id="9939" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As we can see, the generation of the response to a user’s query (If we apply a RAG for the purpose of Q&amp;A), depends directly on how well we have built the “<em class="ok">Augmented</em>” and especially the “<em class="ok">Retrieval</em>”.</p><p id="a415" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><strong class="np fr">In this article we are going to focus exclusively on the “<em class="ok">Retrieval</em>” part</strong>. In this important process of returning the most relevant documents, the concept of vector store appears.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz pa"><img src="../Images/a581156c21a7c624ecde0048b05ffc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GplH0dB2VWe16swRh7_v5A.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Overview of the techniques shown in this article (Image by Author).</figcaption></figure><p id="7436" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">To create these retrievals, we will use the Langchain library.</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div class="oy oz pr"><img src="../Images/3182127071644529aa590a1862f565f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*lDZjBZ8_6zBKyvQaJmJONA.png"/></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Overview of the technologies used in this article (Image by Author).</figcaption></figure><p id="cafb" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The vectore store is nothing more than a vector database, which stores documents in vector format. This vector representation comes from the use of transformers. I’m not saying something you don’t know at the moment.</p><p id="f3d4" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">It is clear that the more robust and complete this vector store is, the better retriever we can run. We already know that the creation of this database is an art in itself. Depending on the size of the chunks or the embedding model we use, our RAG will be better or worse.</p><p id="0d51" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">I make a clarification here:</p><blockquote class="ps"><p id="8086" class="pt pu fq bf pv pw px py pz qa qb oi dx">In this post we are NOT going to discuss how to create this vector store.<br/>In this post we are going to discuss some of the techniques used to retrieve relevant documents.</p></blockquote><p id="d232" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">Since a picture is worth a thousand words, I suggest you take a look at the following:</p></div></div><div class="pg"><div class="ab cb"><div class="lm qh ln qi lo qj cf qk cg ql ci bh"><figure class="pb pc pd pe pf pg qn qo paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz qm"><img src="../Images/7a733d4eae52fcc8376c3438a97622fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*kppt1RbSswPKnwSEOFau2A.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">A RAG encompasses a series of well-defined steps. This post will only cover the retriever part (Image by Author).</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="3f12" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Therefore, I reiterate that in this post we are going to deeply study one of the many important steps in creating a good RAG tool. The “<em class="ok">Retrieve</em>” step is key since it directly improves the context that the LLM has when generating a response.</p><p id="bee9" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The methods we will study are:</p><ul class=""><li id="867a" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Naive Retriever</strong></li><li id="d909" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Parent Document Retriever</strong></li><li id="9b51" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Self-Query Retriever</strong></li><li id="6c06" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Contextual Compression Retriever (Reranking)</strong></li></ul><p id="a844" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">You can find the project with the notebooks <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/tree/main" rel="noopener ugc nofollow" target="_blank"><strong class="np fr">here</strong></a><strong class="np fr">. </strong>And you can also take a look at my github:</p><div class="qp qq qr qs qt qu"><a href="https://github.com/damiangilgonzalez1995?source=post_page-----1fac2b86dd61--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qv ab ig"><div class="qw ab co cb qx qy"><h2 class="bf fr hw z io qz iq ir ra it iv fp bk">damiangilgonzalez1995 - Overview</h2><div class="rb l"><h3 class="bf b hw z io qz iq ir ra it iv dx">Passionate about data, I transitioned from physics to data science. Worked at Telefonica, HP, and now CTO at…</h3></div><div class="rc l"><p class="bf b dy z io qz iq ir ra it iv dx">github.com</p></div></div><div class="rd l"><div class="re l rf rg rh rd ri lr qu"/></div></div></a></div></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="d7a4" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Vectore Store Creation</h1><p id="ab14" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">To expose these methods, a practical use case will be carried out to improve the explanation. Therefore, we are going to create a RAG about reviews of the John Wick movies.</p><p id="7a69" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">So that the reader can follow each step of this post, they can access the repository that I have created. In it you will find the code for each of the methods, in addition to the documents used to create the vector store. The jupyter notebook in charge of this task can be found in the git repository, and is the file called “<a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/0_create_vectore_db.ipynb" rel="noopener ugc nofollow" target="_blank">0_<strong class="np fr"><em class="ok">create_vectore_db.ipynb</em></strong></a>”.</p><p id="ccb1" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">In relation to the data source of our RAG, there are 4 csv’s each corresponding to the reviews obtained for each of the films in the John Wick saga. The files contain the following information:</p></div></div><div class="pg"><div class="ab cb"><div class="lm qh ln qi lo qj cf qk cg ql ci bh"><figure class="pb pc pd pe pf pg qn qo paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz rj"><img src="../Images/be5401cfc1f8f45719490ff05aa9528b.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*dYouqz7Qiu4I0sRCuqU67g.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Dataset of the project (Image by Author).</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e3ab" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As you can see, the “<strong class="np fr">Review</strong>” field will be the target of our retriever. The other fields being important to store as metadata:</p><ul class=""><li id="bcfc" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Movie_Title</strong></li><li id="74a2" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Review_Date</strong></li><li id="e539" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Review_Title</strong></li><li id="958d" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Review_Url</strong></li><li id="9b81" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Author</strong></li><li id="d150" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Rating</strong></li></ul><p id="8c73" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">To read and convert each row of our files into the “<em class="ok">Document</em>” format, we execute the following code:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="a6b9" class="ro ms fq rl b bg rp rq l rr rs">from langchain_community.document_loaders.csv_loader import CSVLoader<br/>from datetime import datetime, timedelta<br/><br/>documents = []<br/><br/>for i in range(1, 4):<br/>  loader = CSVLoader(<br/>    encoding="utf8",<br/>    file_path=f"data/john_wick_{i}.csv",<br/>    metadata_columns=["Review_Date", "Review_Title", "Review_Url", "Author", "Rating"]<br/>  )<br/><br/>  movie_docs = loader.load()<br/>  for doc in movie_docs:<br/><br/>    # We add metadate about the number of the movi<br/>    doc.metadata["Movie_Title"] = f"John Wick {i}"<br/><br/>    # convert "Rating" to an `int`, if no rating is provided - None<br/>    doc.metadata["Rating"] = int(doc.metadata["Rating"]) if doc.metadata["Rating"] else 5<br/><br/>  documents.extend(movie_docs)</span></pre><p id="c551" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">We already have our documents in “<em class="ok">Document</em>” format:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="160c" class="ro ms fq rl b bg rp rq l rr rs">print(documents[0])<br/><br/>Document(page_content=": 0\nReview: The best way I can describe John Wick is to picture Taken but instead of Liam Neeson it's Keanu Reeves and instead of his daughter it's his dog. That's essentially the plot of the movie. John Wick (Reeves) is out to seek revenge on the people who took something he loved from him. It's a beautifully simple premise for an action movie - when action movies get convoluted, they get bad i.e. A Good Day to Die Hard. John Wick gives the viewers what they want: Awesome action, stylish stunts, kinetic chaos, and a relatable hero to tie it all together. John Wick succeeds in its simplicity.", metadata={'source': 'data/john_wick_1.csv', 'row': 0, 'Review_Date': '6 May 2015', 'Review_Title': ' Kinetic, concise, and stylish; John Wick kicks ass.\n', 'Review_Url': '/review/rw3233896/?ref_=tt_urv', 'Author': 'lnvicta', 'Rating': 8, 'Movie_Title': 'John Wick 1', 'last_accessed_at': datetime.datetime(2024, 4, 8, 11, 49, 47, 92560)})</span></pre><p id="00d5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">We only have to create a vector database (<strong class="np fr">Vectore Store</strong>) locally. For this, I have used <strong class="np fr">Chroma</strong>. Also keep in mind that it is necessary to use an embedding model, which will transform our documents into vector format for storage. Everything mentioned can be seen in the following piece of code:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="c3d4" class="ro ms fq rl b bg rp rq l rr rs">from langchain_community.vectorstores import Chroma<br/>from langchain_openai import OpenAIEmbeddings<br/>import os<br/>from dotenv import load_dotenv<br/><br/>load_dotenv()<br/>os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_KEY')<br/><br/>embeddings = OpenAIEmbeddings(model="text-embedding-3-small")<br/><br/>db = Chroma.from_documents(documents=documents, embedding=embeddings, collection_name="doc_jonhWick", persist_directory="./jonhWick_db")</span></pre><p id="0a0a" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This will create a database on our premises called “<strong class="np fr"><em class="ok">JonhWick_db</em></strong>”. This will be the database that our RAG will use and from where our retriever will obtain the most relevant documents regarding the user’s queries.</p><p id="8ff6" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Now is the time to present the different methods for creating a retriever.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="10a9" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk"><strong class="al">Method: Naive Retriever</strong></h1><blockquote class="ps"><p id="3c77" class="pt pu fq bf pv pw px py pz qa qb oi dx">Code in <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/1_naive_retriever.ipynb" rel="noopener ugc nofollow" target="_blank">1_naive_retriever.ipynb</a> file.</p></blockquote><p id="7324" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">This method is the simplest, in fact its name indicates it. We use this adjective to identify this method for the simple reason that when entering the query into our database, we hope (naively) that it will return the most relevant documents/chunks.</p><p id="2adc" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Basically what happens is that we encode the user query with the same transformer with which we created the vector store. Once its vector representation is obtained, we calculate the similarity by calculating the cosine, the distance, etc.</p><blockquote class="rt ru rv"><p id="d96f" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">And we collect the top K documents closest/similar to the query.</p></blockquote><p id="6aab" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The flow of this type of retriever can be seen in the following image:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz rw"><img src="../Images/fdf9e144f28b71307dd78345689f6d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjPwddYG7vfSEM5iwTJwwg.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Simplified representation of a <strong class="bf mt">Naive retriever</strong> (Image by Author).</figcaption></figure><p id="29a0" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Keeping the scheme in mind, let’s see how all this looks in the code. We read the database:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="dffa" class="ro ms fq rl b bg rp rq l rr rs">from langchain_community.vectorstores import Chroma<br/>from langchain_openai import OpenAIEmbeddings<br/>import os<br/>from dotenv import load_dotenv<br/><br/><br/><br/>load_dotenv()<br/>os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_KEY')<br/><br/>embeddings = OpenAIEmbeddings(model="text-embedding-3-small")<br/><br/>vectordb= Chroma(persist_directory="./jonhWick_db", <br/>                  embedding_function=embeddings, <br/>                  collection_name="doc_jonhWick")pyth</span></pre><p id="72f5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">And we create our <strong class="np fr"><em class="ok">retriever</em></strong>. We can configure the similarity calculation method, in addition to other parameters.</p><blockquote class="rt ru rv"><p id="91d4" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><strong class="np fr">Retriever</strong></p></blockquote><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="bf54" class="ro ms fq rl b bg rp rq l rr rs"># Specifying top k<br/>naive_retriever = vectordb.as_retriever(search_kwargs={ "k" : 10})<br/><br/># Similarity score threshold retrieval<br/># naive_retriever = db.as_retriever(search_kwargs={"score_threshold": 0.8}, search_type="similarity_score_threshold")<br/><br/># Maximum marginal relevance retrieval<br/># naive_retriever = db.as_retriever(search_type="mmr")</span></pre><p id="57d9" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Actually, we have already created our “<strong class="np fr"><em class="ok">Naive Retriever</em></strong>”, but to see how it works, we will create the complete RAG that we remember is composed of the following components:</p><ul class=""><li id="d57f" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><em class="ok">R (Retrieval)</em>: <strong class="np fr">Done</strong></li><li id="0f16" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><em class="ok">A (Augmented)</em>: <strong class="np fr">Not yet</strong></li><li id="838b" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><em class="ok">G (Generation)</em>: <strong class="np fr">Not yet</strong></li></ul><blockquote class="rt ru rv"><p id="1235" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><strong class="np fr">Augmented &amp; Generation</strong></p></blockquote><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="2aec" class="ro ms fq rl b bg rp rq l rr rs">from langchain_core.prompts import ChatPromptTemplate<br/>from langchain_openai import ChatOpenAI<br/><br/># Augmented<br/>TEMPLATE = """\<br/>You are happy assistant. Use the context provided below to answer the question.<br/><br/>If you do not know the answer, or are unsure, say you don't know.<br/><br/>Query:<br/>{question}<br/><br/>Context:<br/>{context}<br/>"""<br/><br/>rag_prompt = ChatPromptTemplate.from_template(TEMPLATE)<br/><br/><br/># Generation<br/>chat_model = ChatOpenAI()</span></pre><p id="6d22" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">We already have the 3 components of our RAG. All that remains is to assemble them, and for this we will use the langchain chains to create a RAG.</p><p id="1f57" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">I don’t know if you know the language created by langchain for creating chains in a more efficient way. This language is known as <strong class="np fr">LCEL (LangChain Expression Language).</strong> If you are new to this way of creating chains in langchain, I leave you a very good tutorial here:</p><figure class="pb pc pd pe pf pg"><div class="rx io l ed"><div class="ry rz l"/></div></figure><p id="59d6" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Finally, we create our RAG using Langchain’s own chain creation language (<strong class="np fr"><em class="ok">LCEL</em></strong>):</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="ac15" class="ro ms fq rl b bg rp rq l rr rs">from langchain_core.runnables import RunnablePassthrough, RunnableParallel<br/>from operator import itemgetter<br/>from langchain_core.output_parsers import StrOutputParser<br/><br/>setup_and_retrieval = RunnableParallel({"question": RunnablePassthrough(), "context": naive_retriever })<br/>output_parser = StrOutputParser()<br/><br/><br/>naive_retrieval_chain = setup_and_retrieval <br/>                        | rag_prompt <br/>                        | chat_model <br/>                        | output_parser<br/><br/><br/>naive_retrieval_chain.invoke( "Did people generally like John Wick?")<br/><br/><br/># response: 'Yes, people generally liked John Wick.'<br/></span></pre><p id="3224" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This is the simplest way to create a chain for a RAG. In the Jupyter notebook you can find the same chain but more robust. Since I don’t want us to get lost on this topic now, I have only shown the simplest form. Also so that we understand what is happening in the code above, I have created this very clarifying diagram:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sa"><img src="../Images/8c020ccc0b269e852d9b957ec4899227.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YH4FkwquJCEAGf_XxKnylQ.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Creation of a RAG with Langchain and its LCEL language (Image by Author).</figcaption></figure><p id="46ea" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Great, we’re done creating our <strong class="np fr"><em class="ok">Naive RAG</em></strong>. Let’s move on to the next method.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1f6d" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Method: Parent Document Retriever</h1><blockquote class="ps"><p id="fddd" class="pt pu fq bf pv pw px py pz qa qb oi dx">Code in <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/2_parent_document_retriever.ipynb" rel="noopener ugc nofollow" target="_blank">2_parent_document_retriever.ipynb</a> file.</p></blockquote><p id="4832" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">Imagine that we have created a RAG to recognize possible diseases by introducing some of their symptoms in the consultation. In the event that we have a Naive RAG, we may collect a series of possible diseases that only coincide in one or two symptoms, leaving our tool in a bit of a bad place.</p><p id="3d6e" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This is an ideal case to use Parent Doc Retriever. And the type of technique consists of cutting large chunks (parent chunk) into even smaller pieces (child chunk). By having small chunks, the information they contain is more concentrated and therefore, its informative value is not diluted between paragraphs of text.</p><p id="500c" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">There is a small problem in all this:</p><ul class=""><li id="9be8" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk">If we want to be precise in searching for the most relevant documents, we need to break our documents into <strong class="np fr"><em class="ok">small chunks</em></strong>.</li><li id="c9d2" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">But it is also very important to provide good context to the LLM, which is achieved by providing <strong class="np fr"><em class="ok">larger chunks</em></strong>.</li></ul><p id="6876" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">What has been said can be seen in the following image:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sb"><img src="../Images/8413c96f449f1fa9ad87a95c0951e5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1ts55Pi4D5y--VoZoe9Og.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Representation of the balance between these two concepts/metrics (Image by Author).</figcaption></figure><p id="3f0e" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">It seems that there is no way out of the problem, since when we increase the precision, the context is reduced, and vice versa. This is when this method appears that will solve our lives.</p><p id="6e74" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The main idea is to further chop the large chunks (<strong class="np fr">Parent chunks/documents</strong>) into smaller chunks (<strong class="np fr">Child Chunks/documents</strong>). Once this is done, perform the search for the most relevant top K documents with the child chunks, and return the parents chunks to which the top K child document belongs.</p><p id="19ef" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">We already have the main idea, now let’s get it down to earth. The best way to explain it is step by step:</p><ol class=""><li id="fe20" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi sc or os bk">Obtain the documents and create the large chunks (<strong class="np fr">Parent chunks</strong>)</li><li id="132c" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi sc or os bk">Perform a split of each of the parent chunks for the growth of the <strong class="np fr">child chunks</strong>.</li><li id="00bb" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi sc or os bk">Save the child chunks (<em class="ok">Vector Representatio</em>n) in the <strong class="np fr">Vector Store</strong>.</li><li id="a114" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi sc or os bk">Save the <strong class="np fr"><em class="ok">parent chunks in memory</em></strong> (We do not need to create their vector representation).</li></ol><p id="e741" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">What has been said can be seen in the following image:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sd"><img src="../Images/c6ef2ddb4cb287519a2c1535eb7c79c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tLZ3wXgvaHjbLaeaDjR8kg.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Visual representation of how <strong class="bf mt">child chunks</strong> are created from <strong class="bf mt">parent chunks</strong>, and their storage. These are necessary steps to create a<strong class="bf mt"> parent document retriever </strong>(Image by Author).</figcaption></figure><p id="bb2a" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This may seem very complex to create, since we have to create a new database with the small chunks, save the parent chunks in memory. Additionally, know which parent chunk each child chunk belongs to. Thank goodness <strong class="np fr">Langchain </strong>exists and the way to build it is super simple.</p><p id="c2a4" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Surely you have come to the conclusion that it is necessary to create a new vector store for this method. Furthermore, in the case of reviews of the John Wick movies, such as the data source with CSV files, it is not necessary to perform the first split (parent chunks). This is because we can consider each row of our csv files to be a chunk in itself.</p><p id="e15b" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Overall, let’s visualize the following image that reflects how this method works:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz se"><img src="../Images/96d669987379cb97201196e3a60d3d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPjhr0dzCyGcQNuXUo1qJQ.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Visual representation of how a <strong class="bf mt">Parent Document Retriever</strong> works (Image by Author).</figcaption></figure><p id="e460" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Going to code it is represented as follows:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="f3cb" class="ro ms fq rl b bg rp rq l rr rs">from langchain.retrievers import ParentDocumentRetriever<br/>from langchain.storage import InMemoryStore<br/>from langchain_text_splitters import RecursiveCharacterTextSplitter<br/>from langchain_openai import OpenAIEmbeddings<br/>from langchain_community.vectorstores import Chroma<br/><br/># documents = Read csv files. Check jupyter notebook for more details<br/><br/>parent_docs = documents<br/><br/># Embedding Model<br/>embeddings = OpenAIEmbeddings(model="text-embedding-3-small")<br/><br/><br/># Splitters<br/>child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)<br/># We don't need a parent splitter because the data cames from CSV file, and each row is a parent doc.<br/># parent_splitter = RecursiveCharacterTextSplitter(chunk_size=800)<br/><br/># Stores<br/>store = InMemoryStore()<br/>vectorstore = Chroma(embedding_function=embeddings, collection_name="fullDoc", persist_directory="./JohnWick_db_parentsRD")<br/><br/><br/><br/>parent_document_retriever = ParentDocumentRetriever(<br/>    vectorstore=vectorstore,<br/>    docstore=store,<br/>    child_splitter=child_splitter,<br/>    # parent_splitter =parent_splitter<br/>)</span></pre><p id="ca92" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Something intuitive about what happens here is that the <strong class="np fr">number of chunks in the vector store (number of child chunks) should be much higher than the number of documents stored in memory (parent chunks)</strong>. With the following code we can check it:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="b09f" class="ro ms fq rl b bg rp rq l rr rs">print(f"Number of parent chunks  is: {len(list(store.yield_keys()))}")<br/><br/>print(f"Number of child chunks is: {len(parent_document_retriever.vectorstore.get()['ids'])}")<br/><br/>'''<br/>Number of parent chunks  is: 75<br/>Number of child chunks is: 3701<br/>'''</span></pre><p id="05d5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Great, we would already have our <strong class="np fr">Parent Document Retriever</strong>, we just need to create our RAG based on this retriever and that would be it. It would be done exactly the same as in the previous method. I attach the code for creating the chain in langchain. To see more details, take a look at the <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/2_parent_document_retriever.ipynb" rel="noopener ugc nofollow" target="_blank">jupyter notebook</a>.</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="a8f9" class="ro ms fq rl b bg rp rq l rr rs">setup_and_retrieval = RunnableParallel({"question": RunnablePassthrough(), "context": parent_document_retriever })<br/>output_parser = StrOutputParser()<br/><br/><br/>parent_retrieval_chain = setup_and_retrieval | rag_prompt | chat_model | output_parser</span></pre><p id="6ad5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Note that it is exactly the same as in the previous case, only with the small difference that in the <strong class="np fr"><em class="ok">“setup_and_retrieval”</em></strong> variable, we configure that we want to use our<strong class="np fr"><em class="ok"> “parent_document_retriever”</em></strong>, instead of the <strong class="np fr"><em class="ok">“naive_retriever”</em></strong>.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b2c7" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Method: Self Query Retriever</h1><blockquote class="ps"><p id="597a" class="pt pu fq bf pv pw px py pz qa qb oi dx">Code in <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/3_self_query_retriever.ipynb" rel="noopener ugc nofollow" target="_blank">3_self_query_retriever.ipynb</a> file.</p></blockquote><p id="8116" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">This is possibly one of the most optimal methods to improve the efficiency of our retriever.</p><blockquote class="ps"><p id="fbaa" class="pt pu fq bf pv pw px py pz qa qb oi dx">Its main feature is that it is capable of performing searches in the vector store, applying filters based on the metadata.</p></blockquote><p id="529d" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">We know that when we apply a “<strong class="np fr">Naive retrieval</strong>”, we are calculating the similarity of all the chunks of the vector database with the query. The more chunks the vector store has, the more similarity calculations will have to be done. Now, imagine being able to do a prior <strong class="np fr">filter based on the metadata</strong>, and after selecting the chunks that meet the conditions imposed in relation to the metadata, calculate similarities.<strong class="np fr"> This can drastically reduce computational and time cost.</strong></p><p id="0057" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s look at a use case to fully understand when to apply this type of retreival.</p><blockquote class="rt ru rv"><p id="6219" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s imagine that we have stored in our vector database a large number of experiences and leisure offers (Ex: surf classes, zip line, gastronomic route, etc.). The description of the experience is what we have encoded, using our embedding model. Additionally, each offer has 3 key values or metadata: Date, price and place.</p><p id="8a87" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s imagine that a user is looking for an experience of this style: An experience in nature, that is for the whole family and safe. Furthermore, the price must be less than $50 and the place is California.</p></blockquote><p id="5583" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Something is clear here</p><blockquote class="ps"><p id="ba6f" class="pt pu fq bf pv pw px py pz qa qb oi dx"><strong class="al">WE DO NOT WANT YOU TO RETURN US ACTIVITY/EXPERIENCES THAT DO NOT MEET THE PRICE OR PLACE THAT THE USER REQUESTS.</strong></p></blockquote><p id="7e7a" class="pw-post-body-paragraph nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi fj bk">Therefore, it does not make sense to calculate similarities with chunks/experiences that do not comply with the metadata filter.</p><p id="8182" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This case is ideal for applying <strong class="np fr"><em class="ok">Self Query Retriever</em></strong>. What this type of retriever allows us is to perform a first filter through the metadata, and then perform the similarity calculation between the chunks that meet the metadata requirements and the user input.</p><p id="9abc" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This technique can be summarized in two very specific steps:</p><ul class=""><li id="3c0f" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Query Constructor</strong></li><li id="22c2" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Query Translater</strong></li></ul></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="a483" class="sf ms fq bf mt sg sh si mw sj sk sl mz nw sm sn so oa sp sq sr oe ss st su sv bk"><strong class="al">Query Constructor</strong></h2><p id="59dd" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">The objective of the step called “<strong class="np fr"><em class="ok">Query Constructor</em></strong>” is <strong class="np fr">to create the appropriate query and filters according to the user input.</strong></p><p id="5495" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Who is in charge of applying the corresponding filters and how do you know what they are?</p><p id="8e15" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">For this we are going to use an LLM. This LLM will have to be able to decide which filters to apply and when. We will also have to explain beforehand what the metadata is and what each of them means. In short, the prompt must contain 3 key points:</p><ul class=""><li id="5802" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Context</strong>: Personality, how you should act, output format, etc.</li><li id="143c" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Metadata</strong>: Information about available metadata.</li><li id="d697" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Query</strong>: The user’s query/input/question.</li></ul><p id="6c6e" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The output generated by the LLM cannot be directly entered into the database. Therefore, the so-called “<strong class="np fr"><em class="ok">Query Translater</em></strong>” is needed.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="21ad" class="sf ms fq bf mt sg sh si mw sj sk sl mz nw sm sn so oa sp sq sr oe ss st su sv bk">Query Translater</h2><p id="5c25" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">This is a module in charge of <strong class="np fr">translating the output of the LLM (Query Constructor) into the appropriate format to perform the query.</strong> Depending on the vector database you use, you will have to use one or the other. In my case I used <strong class="np fr">Chroma db</strong>, therefore, I need a translator focused on this database. Luckily, Langchain has specific database translators for almost all of them.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="de1d" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As you may have already noticed, I am a big fan of diagrams. Let’s look at the following which provides quite a bit of clarity to the matter:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sw"><img src="../Images/4f316e099990fd95c123b5e3de6d15ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5AwjpE8h9kaYYViptmGBQA.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Visual representation of how a <strong class="bf mt">Self Query Retriever </strong>works (Image by Author).</figcaption></figure><p id="d07a" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Regarding the previous image, we see that everything begins with the user’s query. We create the prompt that contains the 3 key fields and is introduced to the LLM that generates a response with two key fields: “<strong class="np fr"><em class="ok">Query</em></strong>” and “<strong class="np fr"><em class="ok">Filter</em></strong>”. This is fed into the query translator which translates these two fields into the correct format needed by <strong class="np fr"><em class="ok">Chroma DB.</em></strong> Performs the query and returns the most relevant documents based on the user’s initial question.</p><p id="80f4" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Something to emphasize is that the query entered by the user does not have to be the same as the one entered into the database. In the diagram shown, it can be seen that the LLM, taking into account the <strong class="np fr">available metadata and the user’s question, detects that it can create a filter with the “Rating” metadata. It also creates a new query based on the user’s query.</strong></p><p id="73fe" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s look at all this in code. As I have explained, it is very important to provide the LLM with a detailed description of the metadata available in the vector store. This translates into the following piece of code:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="e7ca" class="ro ms fq rl b bg rp rq l rr rs">from langchain.chains.query_constructor.base import AttributeInfo<br/>from langchain.retrievers.self_query.base import SelfQueryRetriever<br/>from langchain_openai import ChatOpenAI<br/>from langchain.retrievers.self_query.chroma import ChromaTranslator<br/><br/><br/><br/>metadata_field_info = [<br/>    AttributeInfo(<br/>        name="Movie_Title",<br/>        description="The title of the movie",<br/>        type="string",<br/>    ),<br/>    AttributeInfo(<br/>        name="Review_Date",<br/>        description="The date of the review",<br/>        type="string",<br/>    ),<br/>    AttributeInfo(<br/>        name="Review_Title",<br/>        description="The title of the review",<br/>        type="string",<br/>    ),<br/>    AttributeInfo(<br/>        name="Review_Url",<br/>        description="The URL of the review",<br/>        type="string",<br/>    ),<br/>    AttributeInfo(<br/>        name="Author",<br/>        description="The author of the review",<br/>        type="string",<br/>    ),<br/>    AttributeInfo(<br/>        name="Rating",<br/>        description="A 1 to 10 rating for the movie",<br/>        type="integer",<br/>    )<br/>]</span></pre><p id="8843" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">To define our retrieval we must define the following points:</p><ul class=""><li id="edbc" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk">The LLM to use</li><li id="16d3" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">The embedding model to be used</li><li id="7e6e" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">The vector basis that is accessed</li><li id="46ba" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">A description of what information can be found in the<br/>documents of this vector base.</li><li id="049a" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">The metadata description</li><li id="4468" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk">The Query translator you want to use</li></ul><p id="578f" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s see what it looks like in code:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="8465" class="ro ms fq rl b bg rp rq l rr rs">document_content_desription = "A review of the Jonh Wick movie."<br/>embeddings = OpenAIEmbeddings(model="text-embedding-3-small")<br/>chat_model = ChatOpenAI()<br/><br/>self_query_retriever = SelfQueryRetriever.from_llm(<br/>    llm=ChatOpenAI(temperature=0),<br/>    vectorstore =vectordb,<br/>    document_contents = document_content_desription,<br/>    metadata_field_info =metadata_field_info,<br/>    verbose = True,<br/>    structured_query_translator = ChromaTranslator()<br/>)</span></pre><p id="6656" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s see with a very clear example how we have greatly improved our RAG by using this type of retriever. <strong class="np fr">First we use a naive retriever and then a self query retriever.</strong></p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="1f58" class="ro ms fq rl b bg rp rq l rr rs">Question = "Make a summary of the reviews that talk about John Wick 3 and have a score higher than 7"<br/>response = naive_retrieval_chain.invoke(Question)<br/>print(response)<br/><br/>'''<br/>I don't know the answer.<br/>'''<br/>------------------------------------------------------------------------<br/><br/>response = self_retrieval_chain.invoke(Question)<br/>print(response)<br/><br/>'''<br/>John Wick: Chapter 3 - Parabellum is quite literally <br/>about consequences, dealing with the fallout of John's...<br/>'''</span></pre><p id="6c80" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As we can see, there is a notable improvement.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8232" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Method: Contextual Compression Retriever (Reranking)</h1><blockquote class="ps"><p id="7e0b" class="pt pu fq bf pv pw px py pz qa qb oi dx">Code in <a class="af oj" href="https://github.com/damiangilgonzalez1995/AdvancedRetrievalRags/blob/main/4_contextual_compression_retriever(reranking).ipynb" rel="noopener ugc nofollow" target="_blank">4_contextual_compression_retriever(reranking).ipynb</a> file.</p></blockquote><ul class=""><li id="902c" class="nn no fq np b go qc nr ns gr qd nu nv nw qe ny nz oa qf oc od oe qg og oh oi oq or os bk"><strong class="np fr">Context Windows</strong>: The more documents we obtain from the vectore store,<strong class="np fr"> the more information the LLM</strong> will have to give a good answer.</li><li id="10a1" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Recall</strong>: The more documents are retrieved from the vector store, the probability of obtaining<strong class="np fr"> irrelevant chunks is greater and therefore, the recall increases</strong> (Not a good thing).</li></ul><p id="6e3f" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">There seems to be no solution for this problem. When we achieve better performance in one of the metrics, the other seems destined to worsen. <strong class="np fr"><em class="ok">Are we sure about that?</em></strong></p><p id="68a1" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">This is when this technique, compression retriever, is presented, focusing on the reranking technique. Let’s say that this technique consists of two very different steps:</p><ul class=""><li id="ed0a" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">Step 1</strong>: Get a good amount of relevant docs based on the input/question. Normally we set the most relevant K.</li><li id="f590" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">Step 2</strong>: Recalculate which of these documents are really relevant. discarding the other documents that are not really useful (<strong class="np fr">Compression</strong>).</li></ul><p id="043c" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">For the first step, what is known as <strong class="np fr">Bi-Encoder</strong> is used, which is nothing more than what we usually use to make a basic RAG. Vectorize our documents. vectorize the query and calculate the similarity with any metric of our choice.</p><p id="874a" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The second step is something different from what we are used to seeing. This recalculation/reranking is executed by the <strong class="np fr">reranking model</strong> or <strong class="np fr">cross-encoder.</strong></p><blockquote class="rt ru rv"><p id="e2ca" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><strong class="np fr">These models expect two documents/texts as input, returning a similarity score between the pair.</strong></p></blockquote><p id="0429" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">If one of these two inputs is the <strong class="np fr">query </strong>and the other is a <strong class="np fr">chunk</strong>, we can calculate the similarity between the two.</p><p id="d00d" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">These two methods can be displayed as follows:</p><figure class="pb pc pd pe pf pg oy oz paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sx"><img src="../Images/733f0ca8db289656a32f5b509bea13e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6XNQFxw0bbzxSTKz6EoNw.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Visual representation of the two methods presented in the post to calculate the similarity between texts (Image by Author).</figcaption></figure><p id="c0b2" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">You will have realized that the two methods in the end provide the same result, a metric that reflects the similarity between two texts. And this is totally true, but there is a key feature:</p><blockquote class="rt ru rv"><p id="92b9" class="nn no ok np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The result returned by the cross encoder is much more reliable than with the Bi-encoder</p></blockquote><p id="34f4" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Okay, it works better, then, because we don’t use it directly with all chunks, instead of just the top K chunks. Because it would be <strong class="np fr"><em class="ok">terribly expensive in time and money/computation</em></strong>. For this reason, we make a <strong class="np fr">first filter of the chunks closest in similarity to the query,</strong> <strong class="np fr">reducing the use of the reranking model to only K times.</strong></p><p id="80d6" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">A good question would be where to find the Cross-Encoder models? We are lucky that there are open source models that we can find in <a class="af oj" href="https://huggingface.co/cross-encoder" rel="noopener ugc nofollow" target="_blank">HuggingFace</a>, but for the practical case of this post we are going to use the model made available by the company <a class="af oj" href="https://cohere.com" rel="noopener ugc nofollow" target="_blank">Cohere</a>.</p><div class="qp qq qr qs qt qu"><a href="https://cohere.com/?source=post_page-----1fac2b86dd61--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qv ab ig"><div class="qw ab co cb qx qy"><h2 class="bf fr hw z io qz iq ir ra it iv fp bk">Cohere | The leading AI platform for enterprise</h2><div class="rb l"><h3 class="bf b hw z io qz iq ir ra it iv dx">Cohere provides industry-leading large language models (LLMs) and RAG capabilities tailored to meet the needs of…</h3></div><div class="rc l"><p class="bf b dy z io qz iq ir ra it iv dx">cohere.com</p></div></div><div class="rd l"><div class="sy l rf rg rh rd ri lr qu"/></div></div></a></div><p id="3fd1" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">To better understand the architecture of this method, let’s look at a visual example.</p></div></div><div class="pg"><div class="ab cb"><div class="lm qh ln qi lo qj cf qk cg ql ci bh"><figure class="pb pc pd pe pf pg qn qo paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz sz"><img src="../Images/f6e6821a5c1c17bfdc9eb5d3c0fadc87.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*x4GGBN7Y9NQJdXHYk_HJmg.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Visual representation of how a <strong class="bf mt">Contextual Compression Retriever (Reranking) </strong>works (Image by Author).</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fcba" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">The image shows the steps:</p><ul class=""><li id="63df" class="nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi oq or os bk"><strong class="np fr">1º)</strong> We obtain the query, which we encode in its vector form with a transformer and we introduce it into the vector base.</li><li id="9a7e" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">2º)</strong> Collect the documents <strong class="np fr">most similar to the query from our database</strong>. We can use any retriever method.</li><li id="5e8b" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">3º)</strong> Next we use the Cohere cross-encoder model. In the example in the image, this model will be used a total of 4 times. Remember that the <strong class="np fr">input of this model will be the query and a document/chunk, to collect the similarity of these two texts.</strong></li><li id="df10" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">4º)</strong> The 4 calls have been made to this model in the previous step and 4 new values (between 0 and 1) of the similarity between the query and each of the documents have been obtained. As can be seen, the chunk number 1 obtained in the previous steps, after the reranking, is now in 4th place.</li><li id="6605" class="nn no fq np b go ot nr ns gr ou nu nv nw ov ny nz oa ow oc od oe ox og oh oi oq or os bk"><strong class="np fr">5º)</strong> We add the first 3 chunks most relevant to the context.</li></ul><p id="b402" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Returning again to the computational cost and time, if the cross-encoders were applied directly, think that with each <strong class="np fr">new query, the similarity of the query with each of the documents should be calculated</strong>. Something that is not optimal at all.</p><p id="dd32" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">On the other hand, using <strong class="np fr">Bi-Encoders, the vector representation of the documents is the same for each new query.</strong></p><p id="9c6b" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">We then have a much superior method that is expensive to execute, and on the other hand, another method that works well but does not have a large computational cost with each new query. All this ends with the conclusion of unifying these two methods for a better RAG. And this is known as the <strong class="np fr"><em class="ok">Contextual Compression with reranking method</em></strong>.</p><p id="d350" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Let’s move on to the code part. Let’s remember that this method uses a retreiver, which in our case will be a Naive Retriever:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="e5c9" class="ro ms fq rl b bg rp rq l rr rs">naive_retriever = vectordb.as_retriever(search_kwargs={ "k" : 10})</span></pre><p id="cdd1" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Thanks to L<strong class="np fr">angchain </strong>and its integration with <strong class="np fr">Cohere</strong>, we only have to import the module that will execute the call to the Cohere cross-encoder model:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="2077" class="ro ms fq rl b bg rp rq l rr rs">from langchain_cohere import CohereRerank<br/><br/>os.environ["COHERE_API_KEY"] = "YOUR API KEY FROM COHERE"<br/><br/>compressor = CohereRerank(top_n=3)</span></pre><p id="9d35" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">Finally, we create our <strong class="np fr">Contextual Compression Retriever with Langchain</strong>:</p><pre class="pb pc pd pe pf rk rl rm bp rn bb bk"><span id="768a" class="ro ms fq rl b bg rp rq l rr rs">from langchain.retrievers.contextual_compression import ContextualCompressionRetriever<br/><br/>compression_retriever = ContextualCompressionRetriever(<br/>    base_compressor=compressor, <br/>    base_retriever=naive_retriever<br/>)</span></pre><p id="c4ea" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As simple as that. Let’s see a comparison between a <strong class="np fr"><em class="ok">Naive Retriever and a Reranking Retriever</em></strong>:</p></div></div><div class="pg"><div class="ab cb"><div class="lm qh ln qi lo qj cf qk cg ql ci bh"><figure class="pb pc pd pe pf pg qn qo paragraph-image"><div role="button" tabindex="0" class="ph pi ed pj bh pk"><div class="oy oz ta"><img src="../Images/70e6c3fa51c7f2afef9abf607072c3e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*DaodHxX3sXawjuDrdcD-eQ.png"/></div></div><figcaption class="pm pn po oy oz pp pq bf b bg z dx">Example of how the reranking method recalculates the similarity between the query and the chunks. This causes the most relevant documents returned by the first retriever (In our case, Naive retriever), to be completely reordered. The 3 best are collected as shown (Image by Author).</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="65b9" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">As we see, Naive returns us the top 10 chunks/documents. After performing the reranking and obtaining the 3 most relevant documents/chunks, there are noticeable changes. Notice how document <strong class="np fr">number 16</strong>, which is in <strong class="np fr">third position</strong> in relation to its relevance in the first retriever,<strong class="np fr"> becomes first position</strong> when performing the reranking.</p></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="3c63" class="mr ms fq bf mt mu mv gq mw mx my gt mz na nb nc nd ne nf ng nh ni nj nk nl nm bk">Conclusion</h1><p id="3fe9" class="pw-post-body-paragraph nn no fq np b go nq nr ns gr nt nu nv nw nx ny nz oa ob oc od oe of og oh oi fj bk">We have seen that depending on the characteristics of the case where we want to apply a RAG, we will want to use one method or another. Furthermore, there may be the case in which one does not know which retriever method to use. For this, there are different libraries to evaluate your rags.</p><p id="050c" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">There are several tools for this purpose. Some of those options that I personally recommend are the combination of <a class="af oj" href="https://docs.ragas.io/en/stable/" rel="noopener ugc nofollow" target="_blank">RAGAS </a>and <a class="af oj" href="https://www.langchain.com/langsmith" rel="noopener ugc nofollow" target="_blank">LangSmith</a>.</p><div class="qp qq qr qs qt qu"><a href="https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/?source=post_page-----1fac2b86dd61--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qv ab ig"><div class="qw ab co cb qx qy"><h2 class="bf fr hw z io qz iq ir ra it iv fp bk">Evaluating RAG pipelines with Ragas + LangSmith</h2><div class="rb l"><h3 class="bf b hw z io qz iq ir ra it iv dx">Editor's Note: This post was written in collaboration with the Ragas team. One of the things we think and talk about a…</h3></div><div class="rc l"><p class="bf b dy z io qz iq ir ra it iv dx">blog.langchain.dev</p></div></div><div class="rd l"><div class="tb l rf rg rh rd ri lr qu"/></div></div></a></div><p id="b9d5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk">I highly recommend following, learning and watching the videos of these people who are really what inspired me to make this article.</p><div class="qp qq qr qs qt qu"><a href="https://www.youtube.com/@AI-Makerspace?source=post_page-----1fac2b86dd61--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qv ab ig"><div class="qw ab co cb qx qy"><h2 class="bf fr hw z io qz iq ir ra it iv fp bk">AI Makerspace</h2><div class="rb l"><h3 class="bf b hw z io qz iq ir ra it iv dx">Learn how to build, ship, and share production Large Language Model applications with us!</h3></div><div class="rc l"><p class="bf b dy z io qz iq ir ra it iv dx">www.youtube.com</p></div></div></div></a></div></div></div></div><div class="ab cb mj mk ml mm" role="separator"><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp mq"/><span class="mn by bm mo mp"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="837a" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><em class="ok">Thank you for reading!</em></p><p id="a018" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><em class="ok">If you find my work useful, you can subscribe to </em><a class="af oj" href="https://medium.com/@damiangilgonzalez/subscribe" rel="noopener"><strong class="np fr"><em class="ok">get an email every time that I publish a new article</em></strong></a><strong class="np fr"><em class="ok">.</em></strong></p><p id="3bd5" class="pw-post-body-paragraph nn no fq np b go ol nr ns gr om nu nv nw on ny nz oa oo oc od oe op og oh oi fj bk"><em class="ok">If you’d like, </em><a class="af oj" href="https://www.linkedin.com/in/damiangilgonzalez/" rel="noopener ugc nofollow" target="_blank"><strong class="np fr"><em class="ok">follow</em></strong><em class="ok"> </em><strong class="np fr"><em class="ok">me on Linkedin</em></strong><em class="ok">!</em></a></p></div></div></div></div>    
</body>
</html>