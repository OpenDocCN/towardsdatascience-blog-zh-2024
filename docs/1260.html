<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Predicting the Unpredictable 🔮</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Predicting the Unpredictable 🔮</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19">https://towardsdatascience.com/predicting-the-unpredictable-905f634acc20?source=collection_archive---------5-----------------------#2024-05-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5f66" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">The Magic of Mixture Density Networks Explained</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Miguel Dias, PhD" class="l ep by dd de cx" src="../Images/7ad3bc036519adf1b0292c9ed6fcc2fc.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*peCGPPsZa-8l3gVnkraXRw.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://pandego.medium.com/?source=post_page---byline--905f634acc20--------------------------------" rel="noopener follow">Miguel Dias, PhD</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--905f634acc20--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">6 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="db30" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Tired of your neural networks making lame predictions? 🤦‍♂️ Wish they could predict more than just the average future? Enter Mixture Density Networks (MDNs), a supercharged approach that doesn’t just guess the future — it predicts a whole spectrum of possibilities!</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng nh"><img src="../Images/31e1fa24973449dd5e5ea7ba6966f249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOHI3ogfv7133bB2rlMMqQ.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">When trying to predict the future but all you see are Gaussian curves.</figcaption></figure><h2 id="868f" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">A Blast from the Past</h2><p id="3482" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Christopher M. Bishop’s 1994 paper, <a class="af oy" href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" rel="noopener ugc nofollow" target="_blank">Mixture Density Networks</a>¹, is where the magic began. It’s a classic! 📚 Bishop basically said, <em class="oz">“Why settle for one guess when you can have a whole bunch of them?”</em> And thus, MDNs were born.</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0716" class="pi nz fq bf oa pj pk gq oe pl pm gt oi pn po pp pq pr ps pt pu pv pw px py pz bk">MDNs: The Sorcerers of Uncertainty</h1><p id="2d28" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">MDNs take your boring old neural network and turn it into a prediction powerhouse. Why settle for one prediction when you can have an entire buffet of potential outcomes?</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><div class="nf ng qa"><img src="../Images/05b0ea62c9c551bd8bd15eca03512fe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKuoYsGWis22cOV7KpLjVg.png"/></div></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx">If life throws complex, unpredictable scenarios your way, MDNs are ready with a probability-laden safety net.</figcaption></figure><h2 id="b73c" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">The Core Idea</h2><p id="dc43" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">In a MDN, the probability density of the target variable <em class="oz">t</em> given the input <em class="oz">x</em> is represented as a linear combination of kernel functions, typically Gaussian functions, though not limited to. In math speak:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qb"><img src="../Images/135c8711ce8984fbadd7ed6a6e6c5ee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*5rdHysP5Aj7kn8jJ_eU_XA.png"/></div></figure><p id="4805" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Where 𝛼<em class="oz">ᵢ(x)</em> are the mixing coefficients, and who doesn’t love a good mix, am I right? 🎛️ These determine how much <em class="oz">weight</em> each component <em class="oz">𝜙ᵢ(t|x) — </em>each Gaussian in our case — holds in the model.</p><h2 id="ae74" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">Brewing the Gaussians ☕</h2><p id="c465" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Each Gaussian component <em class="oz">𝜙ᵢ(t|x)</em> has its own mean 𝜇<em class="oz">ᵢ(x)</em> and variance 𝜎<em class="oz">ᵢ</em>².</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qc"><img src="../Images/f3f072bcd7a0c7c0481ab76a685e180a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*O7nX4noteXyeTopkN23QmQ.png"/></div></figure><h2 id="7891" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">Mixing It Up 🎧 with Coefficients</h2><p id="1414" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The mixing coefficients 𝛼<em class="oz">ᵢ</em> are crucial as they balance the influence of each Gaussian component, governed by a <em class="oz">softmax</em> function to ensure they sum up to 1:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qd"><img src="../Images/4f6a1fbff7ec9316850ffc8fddfb86a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*MfOO4wV3H6WQHRPMGh6-yg.png"/></div></figure><h2 id="fe04" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">Magical Parameters ✨ Means &amp; Variances</h2><p id="de58" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Means 𝜇<em class="oz">ᵢ</em> and variances 𝜎<em class="oz">ᵢ</em>² define each Gaussian. And guess what? Variances have to be positive! We achieve this by using the exponential of the network outputs:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qe"><img src="../Images/c10f1456a9362c13dd43ca0ac6d9a875.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*e5GT1GJpzV96j8wbfE7_Gw.png"/></div></figure><h1 id="35a6" class="pi nz fq bf oa pj qf gq oe pl qg gt oi pn qh pp pq pr qi pt pu pv qj px py pz bk">Training Our Wizardry 🧙‍♀️</h1><p id="d823" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Alright, so how do we train this beast? Well, it’s all about maximizing the likelihood of our observed data. Fancy terms, I know. Let’s see it in action.</p><h2 id="2ab1" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">The Log-Likelihood Spell ✨</h2><p id="a901" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">The likelihood of our data under the MDN model is the product of the probabilities assigned to each data point. In math speak:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qk"><img src="../Images/7ddac70d340283eedb4ecb4861bd7d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*6Vpaz0buLtjNRPfmHAgjvQ.png"/></div></figure><p id="187d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This basically says, <em class="oz">“Hey, what’s the chance we got this data given our model?”</em>. But products can get messy, so we take the log (because math loves logs), which turns our product into a sum:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng ql"><img src="../Images/28e66784911b7c1d99047c77553d9e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*JAeMYmw6shAddN6npEubeA.png"/></div></figure><p id="4f1d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Now, here’s the kicker: we actually want to minimize the negative log likelihood because our optimization algorithms like to minimize things. So, plugging in the definition of <em class="oz">p(t|x)</em>, the error function we actually minimize is:</p><figure class="ni nj nk nl nm nn nf ng paragraph-image"><div class="nf ng qm"><img src="../Images/3306f22814c044a9477588c330faa1ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*2_8-txJ6jNFrqfYs14uzSw.png"/></div></figure><p id="1025" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This formula might look intimidating, but it’s just saying we sum up the log probabilities across all data points, then throw in a negative sign because minimization is our jam.</p><h1 id="d831" class="pi nz fq bf oa pj qf gq oe pl qg gt oi pn qh pp pq pr qi pt pu pv qj px py pz bk">From Math to Magic in Code 🧑‍💻</h1><p id="b10e" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Now here’s how to translate our wizardry into Python, and you can find the full code <a class="af oy" href="https://github.com/pandego/mdn-playground" rel="noopener ugc nofollow" target="_blank">here</a>:</p><div class="qn qo qp qq qr qs"><a href="https://github.com/pandego/mdn-playground?source=post_page-----905f634acc20--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="qt ab ig"><div class="qu ab co cb qv qw"><h2 class="bf fr hw z io qx iq ir qy it iv fp bk">GitHub — pandego/mdn-playground: A playground for Mixture Density Networks.</h2><div class="qz l"><h3 class="bf b hw z io qx iq ir qy it iv dx">A playground for Mixture Density Networks. Contribute to pandego/mdn-playground development by creating an account on…</h3></div><div class="ra l"><p class="bf b dy z io qx iq ir qy it iv dx">github.com</p></div></div><div class="rb l"><div class="rc l rd re rf rb rg lr qs"/></div></div></a></div><h2 id="541d" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">The Loss Function</h2><pre class="ni nj nk nl nm rh ri rj bp rk bb bk"><span id="b96c" class="rl nz fq ri b bg rm rn l ro rp">def mdn_loss(alpha, sigma, mu, target, eps=1e-8):<br/>    target = target.unsqueeze(1).expand_as(mu)<br/>    m = torch.distributions.Normal(loc=mu, scale=sigma)<br/>    log_prob = m.log_prob(target)<br/>    log_prob = log_prob.sum(dim=2)<br/>    log_alpha = torch.log(alpha + eps)  # Avoid log(0) disaster<br/>    loss = -torch.logsumexp(log_alpha + log_prob, dim=1)<br/>    return loss.mean()</span></pre><p id="e6aa" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s the breakdown:</p><ol class=""><li id="16fd" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">target = target.unsqueeze(1).expand_as(mu)</code>: Expand the target to match the shape of <code class="cx rt ru rv ri b">mu</code>.</li><li id="8c8f" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">m = torch.distributions.Normal(loc=mu, scale=sigma)</code>: Create a normal distribution.</li><li id="ef14" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">log_prob = m.log_prob(target)</code>: Calculate the log probability.</li><li id="b230" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">log_prob = log_prob.sum(dim=2)</code>: Sum log probabilities.</li><li id="9647" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">log_alpha = torch.log(alpha + eps)</code>: Calculate log of mixing coefficients.</li><li id="f2a8" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">loss = -torch.logsumexp(log_alpha + log_prob, dim=1)</code>: Combine and log-sum-exp the probabilities.</li><li id="ec8d" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">return loss.mean()</code>: Return the average loss.</li></ol><h2 id="1439" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">The Neural Network</h2><p id="c763" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Let’s create a neural network that’s all set to handle the wizardry:</p><pre class="ni nj nk nl nm rh ri rj bp rk bb bk"><span id="0c48" class="rl nz fq ri b bg rm rn l ro rp">class MDN(nn.Module):<br/>    def __init__(self, input_dim, output_dim, num_hidden, num_mixtures):<br/>        super(MDN, self).__init__()<br/>        self.hidden = nn.Sequential(<br/>            nn.Linear(input_dim, num_hidden),<br/>            nn.Tanh(),<br/>            nn.Linear(num_hidden, num_hidden),<br/>            nn.Tanh(),<br/>        )<br/>        self.z_alpha = nn.Linear(num_hidden, num_mixtures)<br/>        self.z_sigma = nn.Linear(num_hidden, num_mixtures * output_dim)<br/>        self.z_mu = nn.Linear(num_hidden, num_mixtures * output_dim)<br/>        self.num_mixtures = num_mixtures<br/>        self.output_dim = output_dim<br/><br/>    def forward(self, x):<br/>        hidden = self.hidden(x)<br/>        alpha = F.softmax(self.z_alpha(hidden), dim=-1)<br/>        sigma = torch.exp(self.z_sigma(hidden)).view(-1, self.num_mixtures, self.output_dim)<br/>        mu = self.z_mu(hidden).view(-1, self.num_mixtures, self.output_dim)<br/>        return alpha, sigma, mu</span></pre><p id="8e56" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Notice the <em class="oz">softmax</em> being applied to 𝛼<em class="oz">ᵢ </em><code class="cx rt ru rv ri b">alpha = F.softmax(self.z_alpha(hidden), dim=-1)</code>, so they sum up to 1, and the exponential to 𝜎<em class="oz">ᵢ</em> <code class="cx rt ru rv ri b">sigma = torch.exp(self.z_sigma(hidden)).view(-1, self.num_mixtures, self.output_dim)</code>, to ensure they remain positive, as explained earlier.</p><h2 id="d07b" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">The Prediction</h2><p id="f729" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Getting predictions from MDNs is a bit of a trick. Here’s how you sample from the mixture model:</p><pre class="ni nj nk nl nm rh ri rj bp rk bb bk"><span id="e282" class="rl nz fq ri b bg rm rn l ro rp">def get_sample_preds(alpha, sigma, mu, samples=10):<br/>    N, K, T = mu.shape<br/>    sampled_preds = torch.zeros(N, samples, T)<br/>    uniform_samples = torch.rand(N, samples)<br/>    cum_alpha = alpha.cumsum(dim=1)<br/>    for i, j in itertools.product(range(N), range(samples)):<br/>        u = uniform_samples[i, j]<br/>        k = torch.searchsorted(cum_alpha[i], u).item()<br/>        sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])<br/>    return sampled_preds</span></pre><p id="0d4e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here’s the breakdown:</p><ol class=""><li id="5386" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">N, K, T = mu.shape</code>: Get the number of data points, mixture components, and output dimensions.</li><li id="8ac9" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">sampled_preds = torch.zeros(N, samples, T)</code>: Initialize the tensor to store sampled predictions.</li><li id="4041" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">uniform_samples = torch.rand(N, samples)</code>: Generate uniform random numbers for sampling.</li><li id="0c8e" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">cum_alpha = alpha.cumsum(dim=1)</code>: Compute the cumulative sum of mixture weights.</li><li id="95df" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">for i, j in itertools.product(range(N), range(samples))</code>: Loop over each combination of data points and samples.</li><li id="5b18" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">u = uniform_samples[i, j]</code>: Get a random number for the current sample.</li><li id="c902" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">k = torch.searchsorted(cum_alpha[i], u).item()</code>: Find the mixture component index.</li><li id="bf14" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">sampled_preds[i, j] = torch.normal(mu[i, k], sigma[i, k])</code>: Sample from the selected Gaussian component.</li><li id="7733" class="mj mk fq ml b go rw mn mo gr rx mq mr ms ry mu mv mw rz my mz na sa nc nd ne rq rr rs bk"><code class="cx rt ru rv ri b">return sampled_preds</code>: Return the tensor of sampled predictions.</li></ol><h1 id="9f15" class="pi nz fq bf oa pj qf gq oe pl qg gt oi pn qh pp pq pr qi pt pu pv qj px py pz bk">Practical Example: Predicting the ‘Apparent’ 🌡️</h1><p id="4dea" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Let’s apply MDNs to predict <em class="oz">‘Apparent Temperature’</em> using a simple <a class="af oy" href="https://www.kaggle.com/datasets/muthuj7/weather-dataset" rel="noopener ugc nofollow" target="_blank">Weather Dataset</a>. I trained an MDN with a 50-hidden-layer network, and guess what? It rocks! 🎸</p><p id="74b3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Find the full code <a class="af oy" href="https://github.com/pandego/mdn-playground" rel="noopener ugc nofollow" target="_blank">here</a>. Here are some results:</p></div></div><div class="nn"><div class="ab cb"><div class="lm sb ln sc lo sd cf se cg sf ci bh"><div class="ni nj nk nl nm ab ke"><figure class="lb nn sg sh si sj sk paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><img src="../Images/fe6a1b24db3a4e4ea8af212f60e0b809.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*gy6r0W_47f0uuaIeO_Vf-g.png"/></div></figure><figure class="lb nn sl sh si sj sk paragraph-image"><div role="button" tabindex="0" class="no np ed nq bh nr"><img src="../Images/94d623b2b9d13803c0299c6920415c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*6dE9aYdAH59i_jkyc8w3Vg.png"/></div><figcaption class="nt nu nv nf ng nw nx bf b bg z dx sm ed sn so">Histogram <strong class="bf oa">(left)</strong> and Scatterplot <strong class="bf oa">(right)</strong> of ‘Apparent Temperature’, Measured vs Predictions (R² = .99 and MAE = .5).</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c304" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The results are pretty sweet, and with some hyper-parameter tuning and data preprocessing, for instance outliers removal and resampling, it could be even better!</p></div></div></div><div class="ab cb pa pb pc pd" role="separator"><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg ph"/><span class="pe by bm pf pg"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e250" class="pi nz fq bf oa pj pk gq oe pl pm gt oi pn po pp pq pr ps pt pu pv pw px py pz bk">The Future is Multimodal 🎆</h1><p id="9e5f" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">Consider a scenario where data exhibits a complex pattern, such as a dataset from financial markets or biometric readings. Linear regression would struggle here, capturing none of the underlying dynamics. Non-linear regression might contour to the data better but still falls short in quantifying the uncertainty or capturing multiple potential outcomes. MDNs leap beyond, offering a comprehensive model that anticipates various possibilities, each with its own likelihood!</p><p id="3518" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Embrace the Chaos!</p><p id="4980" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">These neural network wizards excel in predicting chaotic, complex scenarios where traditional models just fall flat. Stock market predictions, guessing the weather, or foreseeing the next viral meme 🦄 — MDNs have got you covered.</p><p id="f7ef" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">MDNs are Awesome!</p><p id="ec93" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But MDNs don’t just predict — they give you a range of possible futures. They’re your crystal ball 🔮 for understanding uncertainty, capturing intricate relationships, and providing a probabilistic peek into what lies ahead. For researchers, practitioners, or AI enthusiasts, MDNs are a fascinating frontier in the vast, wondrous realm of machine learning!</p><h2 id="1bc9" class="ny nz fq bf oa ob oc od oe of og oh oi ms oj ok ol mw om on oo na op oq or os bk">References</h2><p id="2577" class="pw-post-body-paragraph mj mk fq ml b go ot mn mo gr ou mq mr ms ov mu mv mw ow my mz na ox nc nd ne fj bk">[1] Christopher M. Bishop, <a class="af oy" href="https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf" rel="noopener ugc nofollow" target="_blank">Mixture Density Networks</a> (1994), Neural Computing Research Group Report.</p><p id="9447" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="oz">Unless otherwise noted, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>