- en: Fine-Tune Llama 3.2 for Powerful Performance on Targeted Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10](https://towardsdatascience.com/fine-tune-llama-3-2-for-powerful-performance-in-targeted-domains-8c4fccef93dd?source=collection_archive---------0-----------------------#2024-10-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how you can fine-tune Llama3.2, Meta’s most recent Large language model,
    to achieve powerful performance on targeted domains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Eivind
    Kjosbakken](../Images/5f91b74428e1202fc4a176a3dd1cb1c7.png)](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    [Eivind Kjosbakken](https://oieivind.medium.com/?source=post_page---byline--8c4fccef93dd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8c4fccef93dd--------------------------------)
    ·10 min read·Oct 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I discuss how to run [Llama 3.2](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)
    locally and fine-tune the model to increase its performance on specific tasks.
    Working with large language models has become a critical part of any data scientist’s
    or ML engineer’s job, and fine-tuning the large language models can lead to powerful
    improvements in the language models’ capabilities. This article will thus show
    you how you can fine-tune Llama3.2 to improve its performance within a targeted
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af3a47e9176f7611742d79620e2189fb.png)'
  prefs: []
  type: TYPE_IMG
- en: This article will show you how to work with and fine-tune Llama3.2 to better
    solve domain-specific problems. Image by ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My motivation for this article is that I want to spend more time working on
    large language models and figure out how to utilize them effectively. There are
    many options for utilizing large language models effectively, such as [prompt
    tuning](https://www.datacamp.com/tutorial/understanding-prompt-tuning), [RAG systems](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/),
    or [function calling](https://platform.openai.com/docs/guides/function-calling).
    However, fine-tuning a model is also a valid option, though it requires more effort
    than my three options. Fine-tuning large language models requires a solid GPU,
    training data, which may require a lot of manual work, and setting up the training
    script. Luckily, however, the [Unsloth library](https://unsloth.ai/) makes fine-tuning
    a lot simpler, which is…
  prefs: []
  type: TYPE_NORMAL
