<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Beyond Kleinberg’s Impossibility Theorem of Clustering: My Study Note of a Pragmatic Clustering Evaluation Framework</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Beyond Kleinberg’s Impossibility Theorem of Clustering: My Study Note of a Pragmatic Clustering Evaluation Framework</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beyond-kleinbergs-impossibility-theorem-of-clustering-a-pragmatic-clustering-evaluation-framework-3b25eccf37f2?source=collection_archive---------10-----------------------#2024-06-21">https://towardsdatascience.com/beyond-kleinbergs-impossibility-theorem-of-clustering-a-pragmatic-clustering-evaluation-framework-3b25eccf37f2?source=collection_archive---------10-----------------------#2024-06-21</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7188" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This article explores a pragmatic evaluation framework for clustering under the constraint of Kleinberg’s Impossibility Theorem of Clustering</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://deeporigami.medium.com/?source=post_page---byline--3b25eccf37f2--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Michio Suginoo" class="l ep by dd de cx" src="../Images/15e4a70d17d163889cc902bf4409931a.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*kx7NRQ9KN0OWP2BRK-obxA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3b25eccf37f2--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://deeporigami.medium.com/?source=post_page---byline--3b25eccf37f2--------------------------------" rel="noopener follow">Michio Suginoo</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3b25eccf37f2--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 21, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/94a20207398170b1d86652eeb8655152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9C0S9q7TplbtxwxPIIk94w.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Processed by the Author</figcaption></figure><p id="956c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In his paper “<strong class="nd fr">an Impossibility Theorem of Clustering</strong>” published in 2002, Jon Kleinberg articulated that there is no clustering model that can satisfy all three desirable axioms of clustering simultaneously: scale invariance, richness, and consistency. (Kleinberg, 2002)</p><p id="cbbe" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">What do those three axioms mean? Here is an interpretation of the three axioms.</p><ul class=""><li id="0dc6" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Scale invariant means: a clustering algorithm should generate the same results when all distances among datapoints are scaled by the factor of a constant.</li><li id="5826" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Richness means: the clustering algorithm should demonstrate high capability in generating all possible partitions of a given dataset.</li><li id="f648" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Consistency means: when we enhance a set of clusters by increasing the inter-cluster distances and decreasing intra-cluster distances, the clustering algorithm should generate the same results.</li></ul><p id="b796" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To cut a long story short, Kleinberg demonstrated that a mathematically satisfactory clustering algorithm is non-existent.</p><p id="371f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This might be a (the?) death sentence for clustering analysis to some theoretical fundamentalists.</p><p id="462a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Nevertheless, I encountered one academic paper challenging the validity of Kleinberg Impossibility Theorem. I would not step into that territory. But if you are interested in that, here you are: “<a class="af of" href="https://arxiv.org/pdf/1702.04577" rel="noopener ugc nofollow" target="_blank"><em class="og">On the Discrepancy Between Kleinberg’s Clustering Axioms and k-Means Clustering Algorithm Behavior</em></a>.”</p><p id="2619" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Whatever the ground-truth may be, ever since Kleinberg released his impossibility theorem, many methodologies for clustering evaluation have been proposed from the field of engineering (e.g. applied mathematics, information theory, etc.).</p><p id="8b71" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To pursue pragmatism in filling the gap between theoretical/scientific limitations and practical functionality is the domain of engineering.</p><p id="a0a0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As a matter of fact, it seems that there is no universally accepted scientific theory that explains why airplanes can fly. Here is <a class="af of" href="https://www.scientificamerican.com/video/no-one-can-explain-why-planes-stay-in-the-air/" rel="noopener ugc nofollow" target="_blank">an article about it</a>. In the absence of scientific theory, I have survived many flights, thanks to the art of engineering.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj oh"><img src="../Images/d5a9b6dbad132e9861dca3247d35aea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cCiuesH1lT_QFPjr"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af of" href="https://unsplash.com/@museumsvictoria?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Museums Victoria</a> on <a class="af of" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9411" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In appreciating the engineering spirit of pragmatism, I would need a reasonably good framework to fill the gap between Kleinberg’s Impossible Theorem and our daily practical applications for clustering analysis.</p><p id="96ab" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">If airplanes can fly without universally accepted scientific theory, we can do clustering!</p><p id="6314" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Maybe... why not!</p><p id="9666" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Easier said than done, under Kleinberg’s Impossibility Theorem:</p><ul class=""><li id="8614" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">How can we evaluate the results of clustering algorithms?</li><li id="c07e" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">How can we select algorithms best suited for a given objective?</li></ul><p id="151f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A comprehensive understanding of these sorts of very simple questions remains elusive, at least to me.</p><p id="28b7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this backdrop, I encountered a paper written by Palacio-Niño &amp; Berzal (2019), “<a class="af of" href="https://arxiv.org/abs/1905.05667" rel="noopener ugc nofollow" target="_blank"><em class="og">Evaluation Metrics for Unsupervised Learning Algorithms</em></a>”, in which they outlined a clustering validation framework in their attempt to better evaluate the quality of clustering performances under the mathematical limitations posed by “the Impossible Theorem”. Yes, they are quite conscious of Kleinberg’s Impossible Theorem in prescribing their framework.</p><p id="9a9e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To promote our pragmatic use of clustering algorithms, I thought that it would be constructive to share my study note about the pragmatic evaluation framework in this article.</p><p id="e367" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since this is my note, there are many modifications here and there for my own personal purposes deviating from the details of the paper written by Palacio-Niño &amp; Berzal. In addition, because this article rather intends to paint an overall structure of the proposed clustering validation framework, it does not intend to get into details. If you wish, please read <a class="af of" href="https://arxiv.org/abs/1905.05667" rel="noopener ugc nofollow" target="_blank">the full text of their original paper online</a> to fill the gap between my article and their original paper.</p><p id="04b4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">As the final precaution, I don’t profess that this is a comprehensive or a standard guide of clustering validation framework. But I hope that novices to clustering analysis find it useful as a guide to shape their own clustering validation framework.</p><p id="82ae" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">No more, no less.</p><p id="ba18" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s start.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6d94" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk"><strong class="al"><em class="pm">An Overall Framework of Cluster Validation</em></strong></h1><p id="c2dd" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">Here is the structure of their framework. We can see four stages of validation:</p><ul class=""><li id="02ac" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">preliminary evaluation,</li><li id="f830" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">internal validation,</li><li id="3c6e" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">external validation,</li><li id="9f2b" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">relative validation.</li></ul><p id="b681" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s check them one by one.</p><h2 id="d411" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk"><em class="pm">1. Preliminary evaluation:</em></h2><p id="3533" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">The objective of this process is to simply confirm the presence of clusters in the dataset.</p><p id="8597" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This process applies the framework of hypothesis testing to assess the presence of clustering tendency in the dataset. This process set the null hypothesis that the dataset is purely random so that there is no cluster tendency in the dataset.</p><p id="794c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since hypothesis testing can be treated as a standalone subject, I will put it aside from this article going forward.</p><h2 id="9f38" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk"><em class="pm">2. Internal Validation or Unsupervised Validation</em></h2><p id="aa21" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">The objective of the internal validation is to assess the quality of the clustering structure solely based on the given dataset without any external information about the ground-truth labels. In other words, when we do not have any advanced knowledge of the ground-truth labels, internal validation is a sole option.</p><p id="314a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A typical objective of clustering internal validation is to discover clusters that maximize intra-cluster similarities and minimize inter-cluster similarities. For this purpose, internal criteria are designed to measure intra-cluster similarities and inter-cluster dispersion. Simple!</p><p id="2e84" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">That said, there is a catch:</p><p id="1c89" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“<em class="og">good scores on an internal criterion do not necessarily translate into good effectiveness in an application.</em>” (Manning et al., 2008)</p><p id="704a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A better score on internal criterion do not necessarily guarantee a better effectiveness of the resulting model. Internal validation is not enough!</p><p id="7149" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">So, what would we do then?</p><p id="d8f8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This is the very reason why we do need external validations.</p><h2 id="ee6e" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk">3. External Validation or Supervised Validation</h2><p id="5a4e" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">In contrast to the internal validation, the external validation requires external class labels: ideally the ground-truth label, if not, potentially its representative surrogate. Because the very first reason why we use unsupervised clustering algorithm is because we do not have any idea about the labels to begin with, the idea of external validation appears absurd, paradoxical, or at least counterintuitive.</p><p id="dae9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Nevertheless, when we have external information about the class labels — e.g. a set of results from a benchmark model or a gold standard model — we can implement the external validation.</p><p id="98bd" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Because we use reference class labels for validation, the objectives of external validation would naturally converge to the general validation framework of supervised classification analysis.</p><p id="257f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In broader terms, this category includes the model selection and human judgement.</p><h2 id="1f01" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk">4. Relative Validation:</h2><p id="ab16" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">Next, relative validation.</p><p id="0bae" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is an illustration of relative validation.</p><p id="bacf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Particularly for the class of partitioning clustering (e.g. K-Means), the setting of the number of clusters is an important starting point to determine the configuration of algorithm, because it would materially affect the result of clustering.</p><p id="ecba" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In other words, for this class of clustering algorithms, the number of clusters is a hyperparameter of the algorithm. In this context, the number of clusters needs to be optimized from the perspective of algorithm’s parameters.</p><p id="ac3f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The problem here is the optimization needs to be simultaneously implemented together with other hyperparameters that determine the configuration of algorithm.</p><p id="dade" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It requires comparisons to understand how a set of hyperparameters settings can affect the algorithmic configuration. This type of relative validation is typically treated within the domain of parameter optimization. Since parameter optimization of machine learning algorithm is a big subject of machine learning training (model development), I will put it aside from this article going forward.</p><p id="a413" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">By now, we have a fair idea about the overall profile of their validation framework.</p><p id="b533" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Next, a relevant question is “What sort of metrics shall we use for each validation?”</p><p id="ec3e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In this context, I gathered some metrics as examples for the internal validation and the external validation in the next section.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e701" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">Metrics for Internal &amp; External Validations</h1><p id="a03d" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">Now, let’s focus on internal validation and external validation. Below, I will list some metrics of my choice with hyper-links where you can trace their definitions and formulas in details.</p><p id="13e8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Since I will not cover the formulas for these metrics, the readers are advised to trace the hyper-links provided below to find them out!</p><h2 id="3960" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk">A. Metrics used for Internal Validation</h2><p id="1516" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">The objective of the internal validation is to establish the quality of the clustering structure solely based on the given dataset.</p><p id="8678" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="og">Classification of Internal evaluation methods:</em></p><p id="172e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Internal validation methods can be categorized accordingly to the classes of clustering methodologies. A typical classification of clustering can be formulated as follows:</p><ul class=""><li id="fd07" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">Partitioning methods (e.g. K-means),</li><li id="3daf" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Hierarchical methods (e.g. agglomerative clustering),</li><li id="18cf" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Density base methods (e.g. DBSCAN), and</li><li id="e78c" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">the rest</li></ul><p id="8fd0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here, I cover the first two: partitioning clustering and hierarchical clustering.</p><p id="e299" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="og">a) Partitioning Methods: e.g. K-means</em></strong></p><p id="07b7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For partitioning methods, there are three basis of evaluation metrics: cohesion, separation, and their hybrid.</p><p id="52c3" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="og">Cohesion:</em></p><p id="0ac6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Cohesion evaluates the closeness of the inner-cluster data structure. The lower the value of cohesion metrics, the better quality the clusters are. An example of cohesion metrics is:</p><ul class=""><li id="0910" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">SSW: Sum of Squared Errors Within Cluster.</li></ul><p id="1745" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="og">Separation:</em></p><p id="d2ab" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Separation is an inter-cluster metrics and evaluates the dispersion of the inter-cluster data structure. The idea behind a separation metric is to maximize the distance between clusters. An example of cohesion metrics is:</p><ul class=""><li id="9c72" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">SSB: Sum of Squared Errors between Clusters.</li></ul><p id="c47d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><em class="og">Hybrid of both cohesion and separation:</em></p><p id="acb1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Hybrid type quantifies the level of separation and cohesion in a single metric. Here is a list of examples:</p><p id="d7fa" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">i) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" rel="noopener ugc nofollow" target="_blank">The silhouette coefficient</a>: in the range of [-1, 1]</p><p id="9c65" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This metrics is a relative measure of the inter-cluster distance with neighboring cluster.</p><p id="35a4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a general interpretation of the metric:</p><ul class=""><li id="aa52" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The best value: 1</li><li id="db32" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">The worst value: -1.</li><li id="0e6f" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Values near 0: overlapping clusters.</li><li id="becc" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Negative values: high possibility that a sample is assigned to a wrong cluster.</li></ul><p id="0ef8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric: <a class="af of" href="https://www.geeksforgeeks.org/silhouette-index-cluster-validity-index-set-2/?ref=ml_lbp" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/silhouette-index-cluster-validity-index-set-2/?ref=ml_lbp</a></p><p id="912c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">ii) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html" rel="noopener ugc nofollow" target="_blank">The Calisnki-Harabasz coefficient</a>:</p><p id="4f0c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Also known as the Variance Ratio Criterion, this metrics measures the ratio of the sum of inter-clusters dispersion and of intra-cluster dispersion for all clusters.</p><p id="4eb7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For a given assignment of clusters, the higher the value of the metric, the better the clustering result is: since a higher value indicates that the resulting clusters are compact and well-separated.</p><p id="0b37" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric: <a class="af of" href="https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp</a></p><p id="2419" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">iii) <a class="af of" href="https://github.com/jqmviegas/jqm_cvi/tree/master" rel="noopener ugc nofollow" target="_blank">Dann Index</a>:</p><p id="176f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For a given assignment of clusters, a higher Dunn index indicates better clustering.</p><p id="9a34" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric: <a class="af of" href="https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/dunn-index-and-db-index-cluster-validity-indices-set-1/?ref=ml_lbp</a></p><p id="0867" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">iv) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score" rel="noopener ugc nofollow" target="_blank">Davies Bouldin Score</a>:</p><p id="6b95" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The metric measures the ratio of intra-cluster similarity to inter-cluster similarity. Logically, a higher metric suggests a denser intra-cluster structure and a more separated inter-cluster structure, thus, a better clustering result.</p><p id="9f78" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric: <a class="af of" href="https://www.geeksforgeeks.org/davies-bouldin-index/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/davies-bouldin-index/</a></p><p id="f78a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="og">b) Hierarchical Methods: e.g. agglomerate clustering algorithm</em></strong></p><p id="46c4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">i) Human judgement based on visual representation of dendrogram.</p><p id="2ea9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Although Palacio-Niño &amp; Berzal did not include human judgement; it is one of the most useful tools for internal validation for hierarchical clustering based on dendrogram.</p><p id="1f35" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Instead, the co-authors listed the following two correlation coefficient metrics specialized in evaluating the results of a hierarchical clustering.</p><p id="84fb" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For both, their higher values indicate better results. Both take values in the range of [-1, 1].</p><p id="769f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">ii) <a class="af of" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.cophenet.html" rel="noopener ugc nofollow" target="_blank">The Cophenetic Correlation Coefficient</a> (CPCC): [-1, 1]</p><p id="f666" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It measures distance between observations in the hierarchical clustering defined by the linkage.</p><p id="00b2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">iii) Hubert Statistic: [-1, 1]</p><p id="eb78" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A higher Hubert value corresponds to a better clustering of data.</p><p id="057a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="og">c) Potential Category: Self-supervised learning</em></strong></p><p id="0e91" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Self-supervised learning can generate the feature representations which can be used for clustering. Self-supervised learnings have no explicit labels in the dataset but use the input data itself as labels for learning. Palacio-Niño &amp; Berzal did not include self-supervised framework, such as autoencoder and GANs, for their proposal in this section. Well, they are not clustering algorithm per se. Nevertheless, I will keep this particular domain pending for my note. Time will tell if any specialized metrics emerge from this particular domain.</p><p id="c649" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Before closing the section of internal validation, here is a caveat from Gere (2023).</p><p id="97b1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“<em class="og">Choosing the proper hierarchical clustering algorithm and number of clusters is always a key question … . In many cases, researchers do not publish any reason why it was chosen a given distance measure and linkage rule along with cluster numbers. The reason behind this could be that different cluster validation and comparison techniques give contradictory results in most cases. … The results of the validation methods deviate, suggesting that clustering depends heavily on the data set in question. Although Euclidean distance, Ward’s method seems a safe choice, testing, and validation of different clustering combinations is strongly suggested.</em>”</p><p id="d23f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Yes, it is a hard task.</p><p id="00b5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, let’s move on to external validation.</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="97c1" class="ps or fq bf os pt pu pv ov pw px py oy nk pz qa qb no qc qd qe ns qf qg qh qi bk">B. Metrics used for External Validation</h2><p id="d5d5" class="pw-post-body-paragraph nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw fj bk">Repeatedly, a better score on internal criterion do not necessarily guarantee a better effectiveness of the resulting model. (Manning et al., 2008) In this context, it would be imperative for us to explore external validation.</p><p id="3965" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">In contrast to the internal validation, the external validation requires external class labels. When we have such external information — the ground-truth labels as an idea option or their surrogates as a practical option such as the results from benchmark models — the external validation objective of clustering converges to that of supervised learning by design.</p><p id="9cd6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The co-authors listed three classes of external validation methodologies: matching sets, peer-to-peer correlation, and information theory.</p><p id="3b39" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">All of them, in one way or another, compare two sets of cluster results: the one obtained from the clustering algorithm under evaluation, call it <em class="og">C</em>; the other, call it <em class="og">P</em>, from an external reference — another benchmark algorithm or, if possible, the ground-truth classes.</p><ol class=""><li id="b7c3" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw qj ny nz bk"><strong class="nd fr"><em class="og">Matching Sets:</em></strong></li></ol><p id="c976" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This class of methods identifies the relationship between each predicted cluster in <strong class="nd fr"><em class="og">C</em></strong> and its corresponding external reference classes of <strong class="nd fr"><em class="og">P</em></strong>. Some of them are popular validation metrics for supervised classification. I will just list up some metrics in this category here. Please follow their hyper-links for further details.</p><p id="53ea" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">a) <a class="af of" href="https://stackoverflow.com/questions/34047540/python-clustering-purity-metric" rel="noopener ugc nofollow" target="_blank">classification accuracy</a>:</p><p id="9691" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">b) <a class="af of" href="https://stackoverflow.com/questions/34047540/python-clustering-purity-metric" rel="noopener ugc nofollow" target="_blank">purity</a>:</p><p id="8442" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">c) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#precision-score" rel="noopener ugc nofollow" target="_blank">precision score</a>:</p><p id="e713" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">d) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#recall-score" rel="noopener ugc nofollow" target="_blank">recall score</a>:</p><p id="ece8" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">e) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank">F-measure</a>:</p><p id="9efe" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="og">2. Peer-to-peer correlation:</em></strong></p><p id="4c3a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This class of metrics is a group of similarity measures between a pair of equivalent partitions resulted from two different methods, <strong class="nd fr"><em class="og">C</em></strong> and <strong class="nd fr"><em class="og">P</em></strong>. Logically, the higher the similarity, the better the clustering result: in the sense that the predicted cluster classes resembles to the reference class labels.</p><p id="c334" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">a) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html" rel="noopener ugc nofollow" target="_blank">Jaccard Score</a>: [0, 1]</p><p id="9d69" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It compares the external reference class labels and the predicted labels by measuring the overlap between these 2 sets: the ratio of the size of the intersection to that of the union of the two labels sets.</p><p id="a50b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A higher the metric, the more correlated these two sets are.</p><p id="7f80" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">b) <a class="af of" href="https://clusteringjl.readthedocs.io/en/latest/randindex.html" rel="noopener ugc nofollow" target="_blank">Rand Index</a>: [0, 1]</p><p id="284c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“<a class="af of" href="https://clusteringjl.readthedocs.io/en/latest/randindex.html" rel="noopener ugc nofollow" target="_blank"><em class="og">From a mathematical standpoint, Rand index is related to the accuracy, but is applicable even when class labels are not used.</em></a>”</p><p id="0ce2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is how to interpret the result of the metric.</p><ul class=""><li id="6e69" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk">The value, 0: No agreement between the two sets of clustering results, <strong class="nd fr"><em class="og">C</em></strong> and <strong class="nd fr"><em class="og">P</em></strong>.</li><li id="3473" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">The value, 1: Complete agreement between the two sets.</li></ul><p id="ce1f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric:</p><p id="8df5" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af of" href="https://www.geeksforgeeks.org/rand-index-in-machine-learning/?ref=ml_lbp" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/rand-index-in-machine-learning/?ref=ml_lbp</a></p><p id="d6c9" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">c) <a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" rel="noopener ugc nofollow" target="_blank">Folkes Mallows coefficient</a></p><p id="8fdf" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It measures “<a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fowlkes_mallows_score.html#sklearn.metrics.fowlkes_mallows_score" rel="noopener ugc nofollow" target="_blank"><em class="og">the geometric mean between of the precision and recall</em></a><em class="og">.</em>”</p><p id="ef20" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a use case example of the metric:</p><p id="e485" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><a class="af of" href="https://www.geeksforgeeks.org/ml-fowlkes-mallows-score/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/ml-fowlkes-mallows-score/</a></p><p id="015e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr"><em class="og">3. Information Theory:</em></strong></p><p id="536f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Now, we have another class of metrics from information theory. There are two basis for this class of metrics: entropy and mutual information.</p><p id="a6dc" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Entropy is“<em class="og">a reciprocal measure of purity that allows us to measure the degree of disorder in the clustering results.</em>”</p><p id="581f" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Mutual Information measures “<em class="og">the reduction in uncertainty about the clustering results given knowledge of the prior partition.</em>”</p><p id="f634" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">And we have the following metrics as examples.</p><ul class=""><li id="0411" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><a class="af of" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html#sklearn.metrics.adjusted_mutual_info_score" rel="noopener ugc nofollow" target="_blank">Adjusted Mutual Information Score</a></li><li id="f6cf" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk"><a class="af of" href="https://clusteringjl.readthedocs.io/en/latest/varinfo.html" rel="noopener ugc nofollow" target="_blank">Variation of Information</a></li><li id="de5e" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk"><a class="af of" href="https://clusteringjl.readthedocs.io/en/latest/vmeasure.html" rel="noopener ugc nofollow" target="_blank">V-Measure</a></li></ul><p id="7095" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">4. <strong class="nd fr"><em class="og">Model Selection Metrics:</em></strong></p><p id="de7d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">For external validation, I further would like to add model selection metrics below from another reference. (Karlsson et al., 2019)</p><ul class=""><li id="0005" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz bk"><a class="af of" href="https://www.statsmodels.org/stable/generated/statsmodels.tools.eval_measures.aic.html" rel="noopener ugc nofollow" target="_blank">Akaike’s information criterion (AIC)</a></li><li id="ed54" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk"><a class="af of" href="https://www.statsmodels.org/dev/generated/statsmodels.tools.eval_measures.hqic.html" rel="noopener ugc nofollow" target="_blank">the Hannan–Quinn information criterion (HQC)</a> and</li><li id="fd91" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk"><a class="af of" href="https://www.statsmodels.org/dev/generated/statsmodels.tools.eval_measures.bic.html" rel="noopener ugc nofollow" target="_blank">the Bayesian information criterion (BIC)</a></li></ul><p id="5187" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">We can use them to compare their values among multiple results. The result with the lowest values of these metrics is considered to be the best fit. Nevertheless, standalone these metrics cannot tell the quality of one single result.</p><p id="fe7b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Here is a precaution for the use of these model selection metrics. For any of those information criteria to be valid in evaluating models, it requires a certain set of preconditions: low multicollinearity, sufficient sample sizes, and good fitting of models with high R-squared metrics. When any of these conditions is not met, the reliability of these metrics could materially be impaired. (Karlsson et al., 2019)</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e22e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">That’s all for this article.</p><p id="2f88" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">I would not profess that what I covered here is comprehensive or even a gold standard. Actually, there are different approaches. For example, R has a clustering validation package called <a class="af of" href="https://cran.r-project.org/web/packages/clValid/vignettes/clValid.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="nd fr"><em class="og">clValid</em></strong></a>, which uses different approaches: from “internal”, “stability”, and “biological” modes. And I think that <strong class="nd fr"><em class="og">clValid</em></strong> is a wonderful tool.</p><p id="215b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Given that, I rather would hope that this article serves as a useful starting guide for novices to clustering analysis in shaping their own clustering evaluation framework.</p><p id="afe2" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Repeatedly, the intention of this article is to paint an overview of a potentially pragmatic evaluation framework for clustering under the theoretical constraints characterized by Kleinberg’s Impossible Theorem of Clustering.</p><p id="9754" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">At last, but not least, remember the following aphorism:</p><p id="c8c1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">“<a class="af of" href="https://www.lacan.upc.edu/admoreWeb/2018/05/all-models-are-wrong-but-some-are-useful-george-e-p-box/" rel="noopener ugc nofollow" target="_blank">All models are wrong, but some are useful</a>”</p><p id="d180" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">This aphorism should continue resonating in our mind when we deal with any model. By the way, the aphorism is often associated with the renowned statistician of history, George E. P. Box.</p><p id="7900" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Given the imperfect conditions that we live in, let’s promote together practical knowledge in the spirit of Aristotelian <a class="af of" href="https://en.wikipedia.org/wiki/Phronesis#:~:text=Phronesis%20(Ancient%20Greek%3A%20%CF%86%CF%81%CF%8C%CE%BD%CE%B7%CF%83%E1%BF%90%CF%82%2C,discussion%20in%20ancient%20Greek%20philosophy." rel="noopener ugc nofollow" target="_blank"><em class="og">Phronesis</em></a>.</p><p id="3b53" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Thanks for reading.</p><p id="41d7" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Michio Suginoo</p></div></div></div><div class="ab cb oi oj ok ol" role="separator"><span class="om by bm on oo op"/><span class="om by bm on oo op"/><span class="om by bm on oo"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="aab6" class="oq or fq bf os ot ou gq ov ow ox gt oy oz pa pb pc pd pe pf pg ph pi pj pk pl bk">References</h1><ul class=""><li id="dd27" class="nb nc fq nd b go pn nf ng gr po ni nj nk pp nm nn no pq nq nr ns pr nu nv nw nx ny nz bk"><em class="og">Davies_bouldin_score</em>. (n.d.). Scikit-Learn. Retrieved June 20, 2024, from <a class="af of" href="https://scikit-learn/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html</a></li><li id="22e3" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Gere, A. (2023). Recommendations for validating hierarchical clustering in consumer sensory projects. <em class="og">Current Research in Food Science</em>, <em class="og">6</em>, 100522. <a class="af of" href="https://doi.org/10.1016/j.crfs.2023.100522" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.crfs.2023.100522</a></li><li id="01a7" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Karlsson, P. S., Behrenz, L., &amp; Shukur, G. (2019). Performances of Model Selection Criteria When Variables are Ill Conditioned. <em class="og">Computational Economics</em>, <em class="og">54</em>(1), 77–98. <a class="af of" href="https://doi.org/10.1007/s10614-017-9682-8" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s10614-017-9682-8</a></li><li id="99d5" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Kleinberg, J. (2002). An Impossibility Theorem for Clustering. <em class="og">Advances in Neural Information Processing Systems</em>, <em class="og">15</em>.</li><li id="814a" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). <em class="og">Introduction to Information Retrieval/ Flat Clustering/ Evaluation of clustering</em>. <a class="af of" href="http://Https://Nlp.Stanford.Edu/IR-Book." rel="noopener ugc nofollow" target="_blank">Https://Nlp.Stanford.Edu/IR-Book.</a> <a class="af of" href="https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html</a></li><li id="1a44" class="nb nc fq nd b go oa nf ng gr ob ni nj nk oc nm nn no od nq nr ns oe nu nv nw nx ny nz bk">Palacio-Niño, J.-O., &amp; Berzal, F. (2019). <em class="og">Evaluation Metrics for Unsupervised Learning Algorithms</em> (arXiv:1905.05667). arXiv. <a class="af of" href="http://arxiv.org/abs/1905.05667" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1905.05667</a></li></ul></div></div></div></div>    
</body>
</html>