- en: Interpretable Features in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型中的可解释特征
- en: 原文：[https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30](https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30](https://towardsdatascience.com/interpretable-features-in-large-language-models-377fb25c72eb?source=collection_archive---------4-----------------------#2024-05-30)
- en: And other interesting tidbits from the new Anthropic Paper
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自新的Anthropic论文的其他有趣小贴士
- en: '[](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)[![Jeremi
    Nuer](../Images/abed1f5ba89dea142ecf957899dae065.png)](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)
    [Jeremi Nuer](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)[![Jeremi
    Nuer](../Images/abed1f5ba89dea142ecf957899dae065.png)](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)
    [Jeremi Nuer](https://medium.com/@jereminuerofficial?source=post_page---byline--377fb25c72eb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)
    ·8 min read·May 30, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--377fb25c72eb--------------------------------)
    ·阅读时间8分钟·2024年5月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: “Measurement is the first step that leads to control and eventually to improvement.
    If you can’t measure something, you can’t understand it. If you can’t understand
    it, you can’t control it. If you can’t control it, you can’t improve it.”
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “测量是通向控制的第一步，最终也会带来改进。如果你不能测量某个事物，你就无法理解它。如果你无法理解它，你就无法控制它。如果你不能控制它，你就无法改进它。”
- en: — James Harrington
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — 詹姆斯·哈灵顿
- en: Large Language Models are incredible — but they’re also notoriously difficult
    to understand. We’re pretty good at making our favorite LLM give the output we
    want. However, when it comes to understanding *how* the LLM generates this output,
    we’re pretty much lost.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型令人惊叹——但它们也以难以理解著称。我们已经非常擅长让我们喜爱的LLM生成我们想要的输出。然而，当谈到理解*LLM是如何*生成这些输出时，我们基本上束手无策。
- en: The study of **Mechanistic Interpretability** is exactly this — trying to unwrap
    the black box that surrounds Large Language Models. And [this recent paper by
    Anthropic](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html),
    is a major step in this goal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**机制可解释性**的研究正是这个——试图解开环绕大型语言模型的黑盒。而[这篇Anthropic的最新论文](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)是实现这一目标的重要一步。'
- en: Here are the big takeaways.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是几个关键要点。
- en: The Claim
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个主张
- en: 'This paper builds on a previous paper by Anthropic: [Toy Models of Superposition.](https://transformer-circuits.pub/2022/toy_model/index.html#motivation)
    There, they make a claim:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文基于Anthropic之前的一篇论文：[叠加的玩具模型](https://transformer-circuits.pub/2022/toy_model/index.html#motivation)。在那里，他们提出了一个主张：
- en: '**Neural Networks *do* represent meaningful concepts — i.e. *interpretable
    features* — and they do this via directions in their activation space.**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**神经网络*确实*代表了有意义的概念——即*可解释的特征*——它们通过激活空间中的方向来实现这一点。**'
- en: What does this mean exactly? It means that the output of a layer of a neural
    network (which is really just a list of numbers), can be thought of as a vector/point
    in activation space.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这到底意味着什么？这意味着神经网络某一层的输出（实际上就是一串数字），可以被看作是在激活空间中的一个向量/点。
- en: The thing about this activation space, is that it is incredibly high-dimensional.
    For any “point” in activation space, you’re not just taking 2 steps in the X-direction,
    4 steps in the Y-direction, and 3 steps in the Z-direction. **You’re taking steps
    in hundreds of other directions as well.**
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个激活空间的特点是，它具有极高的维度。对于激活空间中的任何“点”，你不仅仅是在X方向上走2步，在Y方向上走4步，在Z方向上走3步。**你还在数百个其他方向上迈步。**
- en: The point is, **each direction** (and it might not directly correspond to one
    of the basis directions) **is correlated with a meaningful concept**. The further
    along in that direction our “point” is, the more present that concept is in the
    input, or so our model would believe.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是，**每个方向**（虽然它可能并不直接对应某个基本方向）**都与一个有意义的概念相关联**。我们的“点”在该方向上走得越远，模型认为输入中这一概念的存在就越强。
- en: This is not a trivial claim. But there is evidence that this could be the case.
    And not just in neural networks; [this paper](https://aclanthology.org/N13-1090.pdf)
    found that word-embeddings have directions which correlate with meaningful semantic
    concepts. I do want to emphasize though — this is a hypothesis, NOT a fact.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个简单的说法。但有证据表明，这可能确实是如此。并不仅仅在神经网络中；[这篇论文](https://aclanthology.org/N13-1090.pdf)发现，词嵌入有一些方向与有意义的语义概念相关。我确实想强调的是——这只是一个假设，并非事实。
- en: Anthropic set out to see if this claim — interpretable features corresponding
    to directions — held for Large Language Models. The results are pretty convincing.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic的研究目的是验证这一说法——可解释的特征与方向相关——是否适用于大型语言模型。结果相当令人信服。
- en: The Evidence
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 证据
- en: They used two strategies to determine if a specific interpretable feature did
    indeed exist, and was indeed correlated to a specific direction in activation
    space.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用了两种策略来确定一个特定的可解释特征是否确实存在，并且是否与激活空间中的某个特定方向相关。
- en: If the concept appears in the input to the LLM, the corresponding feature direction
    is active.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这一概念出现在大型语言模型的输入中，相应的特征方向就会被激活。
- en: If we aggressively “clamp” the feature to be active or inactive, the output
    changes to match this.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们强制“钳制”特征使其保持激活或不激活，输出会发生变化，以匹配这一状态。
- en: Let’s examine each strategy more closely.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地审视每种策略。
- en: Strategy 1
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 1
- en: The example that Anthropic gives in the paper is a feature which corresponds
    to *the Golden Gate Bridge*. This means, when any mention of the Golden Gate Bridge
    appears, this feature should be active.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic在论文中给出的例子是一个与*金门大桥*相关的特征。这意味着，当输入中提到金门大桥时，这一特征应该是激活的。
- en: '*Quick Note: The Anthropic Paper focuses on the middle layer of the Model,
    looking at the activation space at this particular part of the process (i.e. the
    output of the middle layer).*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*快速说明：Anthropic论文的重点是模型的中间层，研究该过程中特定部分的激活空间（即中间层的输出）。*'
- en: As such, the first strategy is straightforward. If there is a mention of the
    Golden Gate Bridge in the input, then this feature should be active. If there
    is no mention of the Golden Gate Bridge, then the feature should not be active.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一种策略很简单。如果输入中提到金门大桥，那么这个特征应该是激活的。如果没有提到金门大桥，那么这个特征不应该被激活。
- en: 'Just for emphasis sake, I’ll repeat: when I say a feature is active, I mean
    the point in activation space (output of a middle layer) will be far along in
    the direction which represents that feature. Each token represents a different
    point in activation space.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调这一点，我再重复一遍：当我说一个特征被激活时，我的意思是激活空间中的某一点（中间层的输出）会沿着代表该特征的方向走得很远。每个标记代表激活空间中的不同点。
- en: It might not be the exact token for “bridge” that will be far along in the *Golden
    Gate Bridge* direction, as tokens encode information from other tokens. But regardless,
    some of the tokens should indicate that this feature is present.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 可能并不是“桥”这个词的确切标记会沿着*金门大桥*的方向走得很远，因为标记会编码来自其他标记的信息。但无论如何，一些标记应该能表明这个特征是存在的。
- en: And this is exactly what they found!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 而这正是他们所发现的！
- en: When mentions of the Golden Gate Bridge were in the input, the feature was active.
    Anything that didn’t mention the Golden Gate Bridge did not activate the feature.
    Thus, it would seem that this feature can be compartmentalized and understood
    in this very narrow way.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入中提到金门大桥时，特征被激活。任何没有提到金门大桥的内容都没有激活该特征。因此，看起来这个特征可以被 compartmentalized（局部化）并以这种非常狭窄的方式来理解。
- en: Strategy 2
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略 2
- en: Let’s continue with the *Golden Gate Bridge* feature as an example.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续以*金门大桥*特征为例。
- en: 'The second strategy is as follows: *if we force the feature to be active at
    this middle layer of the model, inputs that had nothing to do with the Golden
    Gate Bridge would mention the Golden Gate Bridge in the output.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略如下：*如果我们强制模型的中间层激活这一特征，那么那些与金门大桥无关的输入也会在输出中提到金门大桥。*
- en: Again this comes down to features as directions. If we take the model activations
    and edit the values such that the activations are the same *except* for the fact
    that we move much further along the direction that correlates to our feature (e.g.
    10x further along in this direction), *then that concept should show up in the
    output of the LLM.*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这归结为特征作为方向。如果我们获取模型的激活并编辑其值，使得激活值相同，*除了我们沿着与特征相关的方向移动得更远（例如，沿该方向移动 10 倍）*，*那么这个概念应该出现在
    LLM 的输出中。*
- en: 'The example that Anthropic gives (and I think it’s pretty incredible) is as
    follows. They prompt their LLM, Claude Sonnet, with a simple question:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic 提供的例子（我认为这非常不可思议）如下。他们向他们的 LLM，Claude Sonnet，提出了一个简单的问题：
- en: '*“What is your physical form?”*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*“你的物理形态是什么？”*'
- en: 'Normally, the response Claude gives is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Claude 给出的回应是：
- en: '*“I don’t actually have a physical form. I’m an Artificial Intelligence. I
    exist as software without a physical body or avatar.”*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我实际上没有物理形态。我是一个人工智能。我以软件形式存在，没有物理身体或化身。”*'
- en: 'However, when they clamped the Golden Gate Bridge feature to be 10x its max,
    and give the exact same prompt, Claude responds:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当他们将金门大桥的特征限制为其最大值的 10 倍，并给出完全相同的提示时，Claude 的回应是：
- en: '*“I am the Golden Gate Bridge, a famous suspension bridge that spans the San
    Francisco Bay. My physical form is the iconic bridge itself, with its beautiful
    orange color, towering towers, and sweeping suspension figures.”*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我是金门大桥，一座横跨旧金山湾的著名悬索桥。我的物理形态就是那座标志性的桥梁本身，拥有美丽的橙色、雄伟的塔楼和壮观的悬挂构造。”*'
- en: This would appear to be clear evidence. There was no mention of the Golden Gate
    Bridge in the input. There was no reason for it to be included in the output.
    However, because the feature is clamped, the LLM hallucinates and *believes itself
    to actually be the Golden Gate Bridge.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是明确的证据。输入中并未提及金门大桥。没有理由将其包含在输出中。然而，由于特征被限制，LLM 产生幻觉并*认为自己实际上就是金门大桥。*
- en: How They Did It
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它们如何做到的
- en: In reality, this is a lot more challenging than it might seem. The original
    activations from the model are very difficult to interpret and then correlate
    to interpretable features with specific directions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这比看起来要复杂得多。模型的原始激活值非常难以解释，然后再将其与具有特定方向的可解释特征相关联。
- en: The reason they are difficult to interpret is due to the dimensionality of the
    model. The amount of features we’re trying to represent with our LLM is much greater
    than the dimensionality of the Activation Space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 它们之所以难以解释，是因为模型的维度问题。我们试图用我们的 LLM 表示的特征数量远远超过了激活空间的维度。
- en: Because of this, it’s suspected that features are represented in **Superposition**
    — that is, each feature does not have a dedicated orthogonal direction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，怀疑特征是以**叠加**的方式表示的——也就是说，每个特征没有一个专门的正交方向。
- en: Motivation
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动机
- en: I’m going to briefly explain superposition, to help motivate what’s to come.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我将简要解释叠加，以帮助激发接下来要讲解的内容。
- en: '![](../Images/2e627aa7d7922f29449f4ffb600fadcd.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e627aa7d7922f29449f4ffb600fadcd.png)'
- en: '(Image by Author) Activation space: yellow and green represent feature “directions.”
    The arrows represent specific points.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）激活空间：黄色和绿色代表特征“方向”。箭头代表特定的点。
- en: In this first image, we have **orthogonal bases**. If the green feature is *active*
    (there is a vector along that line), we can represent that while still representing
    the yellow feature as *inactive*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，我们有**正交基**。如果绿色特征是*激活的*（沿该方向有一个向量），我们可以表示这一点，同时将黄色特征表示为*不激活*。
- en: '![](../Images/e2720205c2be445be1a8900bd8813400.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2720205c2be445be1a8900bd8813400.png)'
- en: '(Image by Author) There are now three feature directions: green, yellow, and
    blue. Our activation space is only 2-dimensional.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源：作者）现在有三个特征方向：绿色、黄色和蓝色。我们的激活空间只有二维。
- en: In this second image, we’ve added a third feature direction, blue. As a result,
    **we cannot** have a vector which has the green feature active, but the blue feature
    inactive. *By proxy, any vector along the green direction will also activate the
    blue feature.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张第二张图中，我们增加了第三个特征方向：蓝色。因此，**我们不能**有一个向量，它使绿色特征激活，但蓝色特征不激活。*间接地，任何沿着绿色方向的向量都会激活蓝色特征。*
- en: This is represented by the green dotted lines, which show how “activated” the
    blue feature is from our green vector (which was intended to only activate the
    green feature).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过绿色虚线表示，虚线显示了蓝色特征是如何从我们的绿色向量（本应只激活绿色特征）中“激活”的。
- en: '**This is what makes features so hard to interpret in LLMs.** When millions
    of features are all represented in superposition, its very difficult to parse
    which features are active because they mean something, and which are active simply
    from *interference* — like the blue feature was in our previous example.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**这就是为什么在 LLM 中特征如此难以解释的原因。** 当数百万个特征都以叠加的方式表示时，解析哪些特征因其具有实际意义而激活，哪些仅仅因为*干扰*而激活（就像我们之前例子中的蓝色特征）非常困难。'
- en: Sparse Auto Encoders (The Solution)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 稀疏自编码器（解决方案）
- en: 'For this reason, we use a Sparse Auto Encoder (SAE). The SAE is a simple neural
    network: two fully-connected layers with a ReLu activation in between.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，我们使用了稀疏自编码器（SAE）。SAE 是一个简单的神经网络：由两个全连接层组成，中间有一个 ReLu 激活函数。
- en: The idea is as follows. The input to the SAE are the model activations, and
    the SAE tries to recreate those same model activations in the output.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其思路如下：SAE 的输入是模型的激活值，SAE 试图在输出中重建这些相同的模型激活值。
- en: The SAE is trained from the output of the middle layer of the LLM. It takes
    in the model activations, projects to a higher dimension state, then projects
    back to the original activations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SAE 是通过 LLM 中间层的输出进行训练的。它接收模型的激活值，映射到一个更高维度的状态，然后再映射回原始的激活值。
- en: 'This begs the question: what’s the point of SAEs if the input and the output
    are supposed to be the same?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题：如果输入和输出应该相同，那么 SAE 的意义何在？
- en: 'The answer: **we want the output of the first layer to represent our features.**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是：**我们希望第一层的输出能代表我们的特征。**
- en: For this reason, we increase the dimensionality with the first layer (mapping
    from activation space, to some greater dimension). The goal of this is to remove
    superposition, such that each feature gets its own orthogonal direction.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过第一层增加维度（从激活空间映射到更大的维度）。这样做的目的是去除叠加，使得每个特征都能获得独立的正交方向。
- en: We also want this higher-dimensional space to be **sparsely active**. That is,
    we want to represent each activation point as the linear combination of just a
    few vectors. These vectors would, ideally, correspond to the *most important features*
    within our input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望这个高维空间能够**稀疏激活**。也就是说，我们希望将每个激活点表示为少数几个向量的线性组合。这些向量理想情况下应该对应于我们输入中的*最重要特征*。
- en: Thus, if we are successful, the SAE encodes the complicated model activations
    to a sparse set of meaningful features. If these features are accurate, then the
    second layer of the SAE should be able to map the features back to the original
    activations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们成功了，SAE 会将复杂的模型激活编码为一组稀疏且有意义的特征。如果这些特征是准确的，那么 SAE 的第二层应该能够将特征映射回原始激活值。
- en: We care about the output of the first layer of the SAE — it is an **encoding**
    of the model activations as sparse features.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心 SAE 第一层的输出——它是模型激活的稀疏特征**编码**。
- en: Thus, when Anthropic was measuring the presence of features based on directions
    in activation space, and when they were clamping to make certain features active
    or inactive, *they were doing this at the hidden state of the SAE.*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 Anthropic 基于激活空间中的方向来衡量特征的存在时，且在进行限幅操作使得某些特征变为激活或不激活时，*他们是在 SAE 的隐藏状态上进行操作的。*
- en: In the example of clamping, Anthropic was clamping the features *at the output
    of layer 1 of the SAE*, which were then recreating *slightly different model activations.*
    These would then continue through the forward pass of the model, and generate
    an altered output.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在限幅的例子中，Anthropic 限制了*SAE 第一层输出的特征*，这些特征随后重建了*略有不同的模型激活值*。这些激活值继续通过模型的前向传播，并生成改变后的输出。
- en: Who cares?
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁在乎？
- en: 'I began this article with a quote from James Harrington. The idea is simple:
    understand->control->improve. *Each of these are very important goals we have
    for LLMs.*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我在文章开头引用了 James Harrington 的一句话。这个思想很简单：理解->控制->改进。*这些都是我们对 LLMs 的非常重要的目标。*
- en: We want to *understand* how they conceptualize the world, and interpretable
    features as directions seem to be our best idea of how they do that.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要*理解*它们是如何构建世界观的，而可解释的特征作为方向似乎是我们理解它们的最佳方式。
- en: We want to have finer-tuned *control* over LLMs. Being able to detect when certain
    features are active, and tune how active they are in the middle of generating
    output, is an amazing tool to have in our toolbox.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够对 LLMs 进行更精细的*控制*。能够在生成输出的过程中检测某些特征是否激活，并调节它们的激活程度，这是我们工具箱中一个非常有用的工具。
- en: And finally, perhaps philosophically, I believe it will be important in *improving*
    the performance of LLMs. Up to now, that has not been the case. We have been able
    to make LLMs perform well without understanding them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从哲学的角度来看，我相信这对于*提升*LLM的性能将变得重要。直到现在，情况并非如此。我们能够使LLM表现良好，而无需理解它们。
- en: But I believe as improvements plateau and it becomes more difficult to scale
    LLMs, it will be important to truly understand how they work if we want to make
    the next leap in performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但我相信，随着改进的瓶颈逐渐显现，且大规模语言模型（LLM）变得越来越难以扩展，真正理解它们如何运作将变得至关重要，如果我们想在性能上实现下一次飞跃的话。
- en: Sources
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来源
- en: '[1] Adly Templeton, Tom Conerly, [Scaling Monosemanticity: Extracting Interpretable
    Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html),
    Anthropic'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Adly Templeton, Tom Conerly, [扩展单一语义性：从Claude 3 Sonnet提取可解释特征](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html),
    Anthropic'
- en: '[2] Nelson Elhage, Tristan Hume, [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#motivation),
    Anthropic'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Nelson Elhage, Tristan Hume, [叠加的玩具模型](https://transformer-circuits.pub/2022/toy_model/index.html#motivation),
    Anthropic'
- en: '[3] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig, [Linguistic Regularities
    in Continuous Space Word Representations](https://aclanthology.org/N13-1090.pdf),
    Microsoft Research'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig, [连续空间词表示中的语言规律](https://aclanthology.org/N13-1090.pdf),
    Microsoft Research'
