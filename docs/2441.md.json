["```py\nimport dspy\nfrom dsp.trackers.langfuse_tracker import LangfuseTracker\n\n# configure tracker \nlangfuse = LangfuseTracker()\n\n# instantiate openai client\nopenai = dspy.OpenAI(\n                      model='gpt-4o-mini', \n                      temperature=0.5, \n                      max_tokens=1500\n          )\n\n# dspy predict supercharged with automatic langfuse trackers \nopenai(\"What is DSPy?\")\n```", "```py\nimport os\nimport dspy\nfrom dsp.trackers.langfuse_tracker import LangfuseTracker\n\nconfig = {\n    'LANGFUSE_PUBLIC_KEY': 'XXXXXX',\n    'LANGFUSE_SECRET_KEY': 'XXXXXX',\n    'LANGFUSE_HOST': 'http://localhost:3000',\n    'OPENAI_API_KEY': 'XXXXXX',\n    'OPENAI_BASE_URL': 'XXXXXX',\n    'OPENAI_PROVIDER': 'XXXXXX',\n    'CHROMA_DB_PATH': './chromadb/',\n    'CHROMA_COLLECTION_NAME':\"supercharged_workshop_collection\",\n    'CHROMA_EMB_MODEL': 'all-MiniLM-L6-v2'\n}\n\n# setting config\nos.environ[\"LANGFUSE_PUBLIC_KEY\"] = config.get('LANGFUSE_PUBLIC_KEY')\nos.environ[\"LANGFUSE_SECRET_KEY\"] = config.get('LANGFUSE_SECRET_KEY')\nos.environ[\"LANGFUSE_HOST\"] = config.get('LANGFUSE_HOST')\nos.environ[\"OPENAI_API_KEY\"] = config.get('OPENAI_API_KEY')\n\n# setup Langfuse tracker\nlangfuse_tracker = LangfuseTracker(session_id='supercharger001')\n\n# instantiate language-model for DSPY\nllm_model = dspy.OpenAI(\n    api_key=config.get('OPENAI_API_KEY'),\n    model='gpt-4o-mini'\n)\n\n# instantiate chromadb client\nchroma_emb_fn = embedding_functions.\\\n                    SentenceTransformerEmbeddingFunction(\n                        model_name=config.get(\n                            'CHROMA_EMB_MODEL'\n                        )\n                    )\nclient = chromadb.HttpClient()\n\n# setup chromadb collection\ncollection = client.create_collection(\n    config.get('CHROMA_COLLECTION_NAME'),\n    embedding_function=chroma_emb_fn,\n    metadata={\"hnsw:space\": \"cosine\"}\n)\n```", "```py\n# Add to collection\ncollection.add(\n    documents=[v for _,v in nb_scraper.notebook_md_dict.items()], \n    ids=doc_ids, # must be unique for each doc\n)\n```", "```py\nretriever_model = ChromadbRM(\n    config.get('CHROMA_COLLECTION_NAME'),\n    config.get('CHROMA_DB_PATH'),\n    embedding_function=chroma_emb_fn,\n    client=client,\n    k=5\n)\n\n# Test Retrieval\nresults = retriever_model(\"RLHF\")\nfor result in results:\n    display(Markdown(f\"__Document__::{result.long_text[:100]}... \\n\"))\n    display(Markdown(f\">- __Document id__::{result.id} \\n>- __Document score__::{result.score}\"))\n```", "```py\nDocument::# Quick Overview of RLFH\n\nThe performance of Language Models until GPT-3 was kind of amazing as-is. ...\n\n- Document id::6_module_03_03_RLHF_phi2\n- Document score::0.6174977412306334\n\nDocument::# Getting Started : Text Representation Image\n\nThe NLP domain ...\n\n- Document id::2_module_01_02_getting_started\n- Document score::0.8062083377747705\n\nDocument::# Text Generation <a target=\"_blank\" href=\"https://colab.research.google.com/github/raghavbali/llm_w\" > ...\n\n- Document id::3_module_02_02_simple_text_generator\n- Document score::0.8826038964887366\n\nDocument::# Image DSPy: Beyond Prompting\n<img src= \"./assets/dspy_b\" > ...\n\n- Document id::12_module_04_05_dspy_demo\n- Document score::0.9200280698248913\n```", "```py\n# RAG Signature\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions with short factoid answers.\"\"\"\n\n    context = dspy.InputField(desc=\"may contain relevant facts\")\n    question = dspy.InputField()\n    answer = dspy.OutputField(desc=\"often less than 50 words\")\n\n# RAG Program\nclass RAG(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)\n\n# compile a RAG\n# note: we are not using any optimizers for this example\ncompiled_rag = RAG()\n```", "```py\nmy_questions = [\n    \"List the models covered in module03\",\n    \"Brief summary of module02\",\n    \"What is LLaMA?\"\n]\n\nfor question in my_questions:\n    # Get the prediction. This contains `pred.context` and `pred.answer`.\n    pred = compiled_rag(question)\n\n    display(Markdown(f\"__Question__: {question}\"))\n    display(Markdown(f\"__Predicted Answer__: _{pred.answer}_\"))\n    display(Markdown(\"__Retrieved Contexts (truncated):__\"))\n    for idx,cont in enumerate(pred.context):\n        print(f\"{idx+1}. {cont[:200]}...\" )\n        print()\n    display(Markdown('---'))\n```", "```py\n# get annotated dataset\nannotated_dataset = langfuse.get_dataset(\"llm_workshop_rag\")\n\n# ensure ollama is available in your environment\nollama_dspy = dspy.OllamaLocal(model='llama3.1',temperature=0.5)\n\n# get langfuse client from the dspy tracker object\nlangfuse =langfuse_tracker.langfuse\n\n# Set up the ollama as LM and RM\ndspy.settings.configure(lm=ollama_dspy,rm=retriever_model)\n\n# test rag using ollama\nollama_rag = RAG()\n\n# iterate through samples from the annotated dataset\nfor item in annotated_dataset.items:\n    question = item.input[0]['content'].split('Question: ')[-1].split('\\n')[0]\n    answer = item.expected_output['content'].split('Answer: ')[-1]\n    o_pred = ollama_rag(question)\n\n    # add observations to dataset related experiments\n    with item.observe(\n        run_name='ollama_experiment',\n        run_description='compare LLaMA3.1 RAG vs GPT4o-mini RAG ',\n        run_metadata={\"model\": \"llama3.1\"},\n    ) as trace_id:\n        langfuse.score(\n            name=\"visual-eval\",\n            # any float value\n            value=1.0,\n            comment=\"LLaMA3.1 is very verbose\",\n        )\n    # attach trace with new run\n    langfuse.trace(input=question,output=o_pred.answer,metadata={'model':'LLaMA3.1'})\n    display(Markdown(f\"__Question__: {question}\"))\n    display(Markdown(f\"__Predicted Answer (LLaMA 3.1)__: {o_pred.answer}\"))\n    display(Markdown(f\">__Annotated Answer (GPT-4o-mini)__: _{answer}_\"))\n```"]