- en: Tokens-to-Token Vision Transformers, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tokens-to-Token 视觉转换器解析
- en: 原文：[https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27](https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27](https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27)
- en: Vision Transformers Explained Series
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉转换器解析系列
- en: A Full Walk-Through of the Tokens-to-Token Vision Transformer, and Why It’s
    Better than the Original
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整解析 Tokens-to-Token 视觉转换器及其优于原始模型的原因
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    ·20 min read·Feb 27, 2024
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    ·20分钟阅读·2024年2月27日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*自2017年*《Attention is All You Need》*¹推出以来，transformer已经成为自然语言处理（NLP）领域的最先进技术。2021年，*《An
    Image is Worth 16x16 Words》*²成功地将transformer应用于计算机视觉任务。从那时起，许多基于transformer的架构被提出用于计算机视觉。*'
- en: '**In 2021*, Tokens-to-Token ViT: Training Vision Transformers from Scratch
    on ImageNet*³ outlined the Tokens-to-Token (T2T) ViT. This model aims to remove
    the heavy pretraining requirement present in the original ViT². This article walks
    through the T2T-ViT, including open-source code for T2T-ViT, as well as conceptual
    explanations of the components. All of the code uses the PyTorch Python package.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**2021年，*Tokens-to-Token ViT：从头开始在ImageNet上训练视觉转换器*³概述了Tokens-to-Token（T2T）ViT。该模型旨在消除原始ViT²中存在的重预训练要求。本文将详细解析T2T-ViT，包括T2T-ViT的开源代码，以及各个组件的概念性解释。所有代码均使用PyTorch
    Python包。**'
- en: '![](../Images/b75bcf1edb82ef945ae30bacbbc254a8.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b75bcf1edb82ef945ae30bacbbc254a8.png)'
- en: Photo by [Harrison Broadbent](https://unsplash.com/@harrisonbroadbent?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Harrison Broadbent](https://unsplash.com/@harrisonbroadbent?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是一个系列的一部分，深入探讨视觉转换器的内部工作原理。该系列中的每篇文章也提供了可以执行代码的 Jupyter Notebook。系列中的其他文章包括：
- en: '[Vision Transformers, Explained](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉转换器解析](/vision-transformers-explained-a9d07147e4c8)→ [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
- en: '[Attention for Vision Transformers, Explained](/attention-for-vision-transformers-explained-70f83984c673)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉转换器的注意力机制解析](/attention-for-vision-transformers-explained-70f83984c673)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
- en: '[Position Embeddings for Vision Transformers, Explained](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[视觉变换器的位置嵌入解析](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
- en: '[**Tokens-to-Token Vision Transformers, Explained**](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Tokens-to-Token视觉变换器解析**](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vision Transformers解析系列的GitHub仓库](https://github.com/lanl/vision_transformers_explained)'
- en: Table of Contents
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '[What is Tokens-to-Token ViT?](#16bf)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[什么是Tokens-to-Token ViT？](#16bf)'
- en: '[Tokens-to-Token (T2T) Module](#4ce4)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tokens-to-Token (T2T) 模块](#4ce4)'
- en: — [Soft Split](#1932)
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [软分割](#1932)
- en: — [Token Transformer](#2954)
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [Token Transformer](#2954)
- en: — [Neural Network Module](#fac6)
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [神经网络模块](#fac6)
- en: — [Image Reconstruction](#9ab4)
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [图像重建](#9ab4)
- en: — [All Together](#bc10)
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [全部内容](#bc10)
- en: '[ViT Backbone](#7cd7)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ViT骨干网络](#7cd7)'
- en: '[Complete Code](#3442)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[完整代码](#3442)'
- en: '[Conclusion](#302d)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[结论](#302d)'
- en: — [Citations](#d446)
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: — [引用](#d446)
- en: What is Tokens-to-Token ViT?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Tokens-to-Token ViT？
- en: The first vision transformers able to match the performance of CNNs on computer
    vision tasks required pre-training on large datasets and then transferring to
    the benchmark of interest². However, pre-training on such datasets is not always
    feasible. For one, the pre-training dataset that achieved the best results in
    *An Image is Worth 16x16 Words* (the JFT-300M dataset) is not publicly available².
    Furthermore, vistransformers designed for tasks other than traditional image classification
    may not have such large pre-training datasets available.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第一批能够在计算机视觉任务中匹配CNN性能的视觉变换器需要在大型数据集上进行预训练，然后再迁移到相关的基准任务上²。然而，在这些数据集上进行预训练并不总是可行的。首先，*An
    Image is Worth 16x16 Words*中所使用的预训练数据集（JFT-300M数据集）并未公开²。此外，为了执行其他任务而设计的视觉变换器可能无法获得如此庞大的预训练数据集。
- en: 'In 2021, *Tokens-to-Token ViT: Training Vision Transformers from Scratch on
    ImageNet*³ was published, presenting a methodology that would circumvent the heavy
    pre-training requirement of previous vistransformers. They achieved this by replacing
    the *patch tokenization* in the ViT model² with the a Tokens-to-Token (T2T) module.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在2021年，*Tokens-to-Token ViT: 从头开始在ImageNet上训练视觉变换器*³ 发表，提出了一种方法，可以避免之前视觉变换器在预训练方面的巨大需求。他们通过将ViT模型²中的*patch
    tokenization*替换为Tokens-to-Token (T2T)模块，成功实现了这一点。'
- en: '![](../Images/246ef8dc95b59a4579b2512487fd0f65.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/246ef8dc95b59a4579b2512487fd0f65.png)'
- en: T2T-ViT Model Diagram (image by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: T2T-ViT模型图（图来自作者）
- en: Since the T2T module is what makes the T2T-ViT model unique, it will be the
    focus of this article. For a deep dive into the ViT components see the [Vision
    Transformers article](/vision-transformers-explained-a9d07147e4c8). The code is
    based on the publicly available GitHub code for *Tokens-to-Token ViT³* with some
    modifications. Changes to the source code include, but are not limited to, modifying
    to allow for non-square input images and removing dropout layers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于T2T模块是T2T-ViT模型的独特之处，因此它将是本文的重点。有关ViT组件的深入解析，请参见[视觉变换器文章](/vision-transformers-explained-a9d07147e4c8)。该代码基于公开的GitHub代码
    *Tokens-to-Token ViT³*，并做了一些修改。源代码的更改包括但不限于，修改以支持非方形输入图像，并移除dropout层。
- en: Tokens-to-Token (T2T) Module
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tokens-to-Token (T2T) 模块
- en: The T2T module serves to process the input image into tokens that can be used
    in the ViT module. Instead of simply splitting the input image into patches that
    become tokens, the T2T module sequentially computes attention between tokens and
    aggregates them together to capture additional structure in the image and to reduce
    the overall token length. The T2T module diagram is shown below.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: T2T模块的作用是将输入图像处理成可以在ViT模块中使用的tokens。T2T模块不仅仅是简单地将输入图像拆分成patches并将其转化为tokens，而是通过序列化计算tokens之间的注意力，并将其聚合在一起，从而捕捉图像中的额外结构，并减少整体token的长度。T2T模块的示意图如下所示。
- en: '![](../Images/5526e55717581c71ec62c1f844891475.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5526e55717581c71ec62c1f844891475.png)'
- en: T2T Module Diagram (image by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: T2T 模块图（图像由作者提供）
- en: Soft Split
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Soft Split
- en: As the first layer in the T2T-ViT model, the soft split layer is what separates
    an image into a series of tokens. The soft split layers are shown as blue blocks
    in the T2T diagram. Unlike the *patch tokenization* in the original ViT (read
    more about that [here](/vision-transformers-explained-a9d07147e4c8)), the soft
    splits in the T2T-ViT create overlapping patches.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 T2T-ViT 模型中的第一层，soft split 层将图像分割成一系列的 tokens。T2T 图中的 soft split 层以蓝色块的形式展示。与原始
    ViT 中的 *patch tokenization*（更多内容请阅读[这里](/vision-transformers-explained-a9d07147e4c8)）不同，T2T-ViT
    中的 soft split 会创建重叠的 patch。
- en: Let’s look at an example of the soft split on this pixel art *Mountain at Dusk*
    by Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))⁴. The original artwork has
    been cropped and converted to a single channel image. This means that each pixel
    has a value between zero and one. Single channel images are typically displayed
    in grayscale; however, we’ll be displaying it in a purple color scheme because
    its easier to see.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个 soft split 在像素艺术作品 *黄昏中的山脉* 上的示例，作品由 Luis Zuno（[@ansimuz](http://twitter.com/ansimuz)）创作⁴。原始艺术作品已经被裁剪并转换为单通道图像。这意味着每个像素的值介于零和一之间。单通道图像通常以灰度显示；然而，为了更易于查看，我们将使用紫色配色方案进行展示。
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
- en: Code Output (image by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图像由作者提供）
- en: This image has size *H=60* and *W=100*. We’ll use a patch size — or equivalently
    *kernel* — of *k=20*. T2T-ViT sets the *stride* — a measure of overlap — at *s=ceil(k/2)*
    and the *padding* at *p=ceil(k/4)*. For our example, that means we’ll use *s=10*
    and *p=5*. The padding is all zero values, which appear as the darkest purple.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图像的尺寸为 *H=60* 和 *W=100*。我们将使用一个 patch 大小——也就是 *卷积核*——为 *k=20*。T2T-ViT 将 *步幅*
    设置为 *s=ceil(k/2)*，并将 *填充* 设置为 *p=ceil(k/4)*。对于我们的示例，这意味着我们将使用 *s=10* 和 *p=5*。填充区域的值全部为零，显示为最深的紫色。
- en: 'Before we can look at the patches created in the soft split, we have to know
    how many patches there will be. The soft splits are implemented as `torch.nn.Unfold`⁵
    layers. To calculate how many tokens the soft split will create, we use the following
    formula:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看 soft split 创建的 patch 之前，我们必须知道将会创建多少个 patch。soft split 层是通过 `torch.nn.Unfold`⁵
    层实现的。为了计算 soft split 会创建多少个 token，我们使用以下公式：
- en: where *h* is the original image height, *w* is the original image width, *k*
    is the kernel size, *s* is the stride size, and *p* is the padding size⁵. This
    formula assumes the kernel is square, and that the stride and padding are symmetric.
    Additionally, it assumes that dilation is 1.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *h* 是原始图像的高度，*w* 是原始图像的宽度，*k* 是卷积核大小，*s* 是步幅，*p* 是填充大小⁵。这个公式假设卷积核是方形的，并且步幅和填充是对称的。此外，还假设膨胀（dilation）为
    1。
- en: '***An aside about dilation:*** *PyTorch describes dilation as “control[ling]
    the spacing between the kernel points”⁵, and refers readers to the diagram* [*here*](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)*.
    A* `*dilation=1*` *value keeps the kernel as you would expect, all pixels touching.
    A user in* [*this forum*](https://discuss.pytorch.org/t/why-the-default-dilation-value-in-conv2d-is-1/5612)
    *suggests to think about it as “every* `*dilation*`*-th element is used.” In this
    case, every 1st element is used, meaning every element is used.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '***关于膨胀的补充说明：*** *PyTorch 将膨胀描述为“控制卷积核点之间的间距”⁵，并将读者引导至* [*这里*](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)*的图示。*
    `*dilation=1*` *的值保持卷积核不变，所有像素都会接触。论坛中的* [*这个用户*](https://discuss.pytorch.org/t/why-the-default-dilation-value-in-conv2d-is-1/5612)
    *建议将其理解为“每隔* `*dilation*`*个元素就会被使用。”在这种情况下，每个第1个元素都会被使用，也就是说每个元素都会被使用。*'
- en: 'The first term in the *num_tokens* equation describes how many tokens are along
    the height, while the second term describes how many tokens are along the width.
    We implement this in code below:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*num_tokens* 方程中的第一个项描述了高度方向上有多少个 token，而第二个项描述了宽度方向上有多少个 token。我们在下面的代码中实现了这一点：'
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the dimensions in the *Mountain at Dusk*⁴ example:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *黄昏中的山脉*⁴ 示例中的尺寸：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we can see how the soft split creates patches from the *Mountain at Dusk*⁴.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到 soft split 如何从 *黄昏中的山脉*⁴ 创建 patch。
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/3c15402b4c8bc793159dd0dda514c27a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c15402b4c8bc793159dd0dda514c27a.png)'
- en: Code Output (image by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图像由作者提供）
- en: We can see how the soft split results in overlapping patches. By counting the
    patches as they move across the image, we can see that there are 6 patches along
    the height and 10 patches along the width, exactly as predicted. By flattening
    these patches, we see the resulting tokens. Let’s flatten the first patch as an
    example.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，软分割导致了重叠的块。通过计算这些块在图像中移动的过程，我们可以看到图像的高度有6个块，宽度有10个块，正如预期的那样。通过将这些块展平，我们可以看到得到的令牌。让我们以第一个块为例进行展平。
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/38a3de588c621e12bfdbc3fe1db6b56c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38a3de588c621e12bfdbc3fe1db6b56c.png)'
- en: Code Output (image by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图片由作者提供）
- en: You can see where the padding shows up in the token!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到令牌中的填充部分！
- en: 'When passed to the next layer, all of the tokens are aggregated together in
    a matrix. That matrix looks like:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当传递到下一层时，所有的令牌会聚集在一起形成一个矩阵。该矩阵看起来像这样：
- en: '![](../Images/30691ccac86ef9fb2f06e9ef06133b33.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30691ccac86ef9fb2f06e9ef06133b33.png)'
- en: Token Matrix (image by author)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌矩阵（图片由作者提供）
- en: 'For *Mountain at Dusk*⁴ that would look like:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*《黄昏山景》*⁴来说，矩阵会是这样：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/9a6945b1ee2dfb7556d5318a67f1508a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a6945b1ee2dfb7556d5318a67f1508a.png)'
- en: Code Output (image by author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（图片由作者提供）
- en: You can see the large areas of padding in the top left and bottom right of the
    matrix, as well as in smaller segments throughout. Now, our tokens are ready to
    be passed along to the next step.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到矩阵左上角和右下角的大面积填充区域，以及其他小块的填充部分。现在，我们的令牌已经准备好，可以传递到下一步骤。
- en: Token Transformer
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 令牌转换器
- en: The next component of the T2T module is the Token Transformer, which is represented
    by the purple blocks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: T2T模块的下一个组件是令牌转换器，它由紫色块表示。
- en: '![](../Images/d0820b49b413302ff5c81dd37fe87513.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d0820b49b413302ff5c81dd37fe87513.png)'
- en: Token Transformer (image by author)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌转换器（图片由作者提供）
- en: 'The code for the Token Transformer class looks like:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Token Transformer类的代码如下：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The *chan, num_heads, qkv_bias,* and *qk_scale* parameters define the *Attention*
    module components. A deep dive into attention for vistransformers is best left
    for [another time](/attention-for-vision-transformers-explained-70f83984c673).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*chan, num_heads, qkv_bias,* 和 *qk_scale* 参数定义了*Attention*模块的组件。深入探讨VisTransformers中的注意力机制最好留到[另一个时间](/attention-for-vision-transformers-explained-70f83984c673)。'
- en: The *hidden_chan_mul* and *act_layer* parameters define the *Neural Network*
    module components. The activation layer can be any `torch.nn.modules.activation`⁶
    layer. The *norm_layer* can be chosen from any `torch.nn.modules.normalization`⁷
    layer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*hidden_chan_mul* 和 *act_layer* 参数定义了*神经网络*模块的组件。激活层可以是任何`torch.nn.modules.activation`⁶
    层。*norm_layer* 可以从任何`torch.nn.modules.normalization`⁷ 层中选择。'
- en: Let’s step through each blue block in the diagram. We’re using 7∗7=49 as our
    starting token size, since the fist soft split has a default kernel of 7x7.³ We’re
    using 64 channels because that’s also the default³. We’re using 100 tokens because
    it’s a nice number. We’re using a batch size of 13 because it’s prime and won’t
    be confused for any of the other parameters. We’re using 4 heads because it divides
    the channels; however, you won’t see the head dimension in the Token Transformer
    Module.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析图中的每一个蓝色块。我们使用7∗7=49作为初始令牌大小，因为第一次软分割的默认卷积核是7x7。³ 我们使用64个通道，因为那也是默认值。³
    我们使用100个令牌，因为这是一个不错的数字。我们使用13的批量大小，因为13是质数，不会与其他参数混淆。我们使用4个头，因为它可以将通道分割开；然而，你不会在令牌转换器模块中看到头维度。
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First, we pass the input through a norm layer, which does not change it’s shape.
    Next, it gets passed through the first *Attention* module, which changes the length
    of the tokens. Recall that a more in-depth explanation for Attention in VisTransformers
    can be found [here](/attention-for-vision-transformers-explained-70f83984c673).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入传递通过一个标准化层，它不会改变输入的形状。接着，它被传递到第一个*Attention*模块，该模块会改变令牌的长度。请回顾一下，关于VisTransformers中Attention的更深入解释可以在[这里](/attention-for-vision-transformers-explained-70f83984c673)找到。
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we must save the state for a split connection layer. In the actual class
    definition, this is done more efficiently in one line. However, for this walk
    through, we do it separately.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须保存分割连接层的状态。在实际的类定义中，这一步通常在一行中更高效地完成。然而，在本教程中，我们将其单独处理。
- en: Next, we can pass it through another norm layer and then the *Neural Network*
    module. The norm layer doesn’t change the shape of the input. The neural network
    is configured to also not change the shape.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将其传递通过另一个标准化层，然后进入*神经网络*模块。标准化层不会改变输入的形状。神经网络也配置为不会改变形状。
- en: The last step is the split connection, which also does not change the shape.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是分割连接，这也不会改变形状。
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s all for the Token Transformer Module.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是令牌变换器模块的全部内容。
- en: Neural Network Module
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络模块
- en: The neural network (NN) module is a sub-component of the token transformer module.
    The neural network module is very simple, consisting of a fully-connected layer,
    an activation layer, and another fully-connected layer. The activation layer can
    be any `torch.nn.modules.activation`⁶ layer, which is passed as input to the module.
    The NN module can be configured to change the shape of an input, or to maintain
    the same shape. We’re not going to step through this code, as NNs are common in
    machine learning, and not the focus of this article. However, the code for the
    NN module is presented below.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NN）模块是令牌变换器模块的一个子组件。神经网络模块非常简单，由一个全连接层、一个激活层和另一个全连接层组成。激活层可以是任何`torch.nn.modules.activation`⁶层，它作为输入传递给该模块。NN模块可以配置为改变输入的形状，或者保持相同的形状。我们不会逐步讲解这段代码，因为神经网络在机器学习中很常见，而且不是本文的重点。不过，下面会展示神经网络模块的代码。
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Image Reconstruction
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像重建
- en: 'The image reconstruction layers are also shown as blue blocks inside the T2T
    diagram. The shape of the input to the reconstruction layers looks like (batch,
    num_tokens, tokensize=channels). If we look at just one batch, that looks like
    this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图像重建层也作为蓝色块显示在T2T图示内。传入重建层的输入形状为(batch, num_tokens, tokensize=channels)。如果我们只查看一个批次，它看起来如下：
- en: '![](../Images/492257ac2cfdc81f71d3f1ca1e9abaf0.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/492257ac2cfdc81f71d3f1ca1e9abaf0.png)'
- en: Single Batch of Tokens (image by author)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 单批次令牌（图片由作者提供）
- en: 'The reconstruction layers reshape the tokens into a 2D image again, which looks
    like this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重建层将令牌重新塑形为二维图像，如下所示：
- en: '![](../Images/2ae4a553884203d9c8a41f94a9939294.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ae4a553884203d9c8a41f94a9939294.png)'
- en: Reconstructed Image (image by author)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 重建的图像（图片由作者提供）
- en: In each batch, there will be *tokensize = channel* number of reconstructed images.
    This is handled in the same way as if the image was in color, and had three color
    channels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个批次中，会有*tokensize = channel*数量的重建图像。这与图像是彩色的并且有三个颜色通道时的处理方式相同。
- en: 'The code for reconstruction isn’t wrapped in it’s own function. However, an
    example is shown below:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 重建的代码没有被封装到单独的函数中。然而，下面给出了一个示例：
- en: '[PRE17]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: where *W*, *H* are the width and height of the image, *B* is the batch size,
    and *C* is the channels.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*W*和*H*分别是图像的宽度和高度，*B*是批次大小，*C*是通道数。
- en: All Together
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 综合起来
- en: 'Now we’re ready to examine the whole T2T module put together! The model class
    for the T2T module looks like:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好一起查看整个T2T模块的实现了！T2T模块的模型类如下所示：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s walk through the forward pass. Since we already examined the components
    in more depth, this section will treat them as black boxes: we’ll just be looking
    at the input and outputs.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来一步步分析前向传递过程。由于我们已经更深入地检查了各个组件，本节将把它们视为黑盒：我们只会关注输入和输出。
- en: We’ll define an input to the network of shape 1x400x100 to represent a grayscale
    (one channel) rectangular image. We’re using 64 channels and 768 token length
    because those are the default values³. We’re using a batch size of 13 because
    it’s prime and won’t be confused for any of the other parameters.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个形状为1x400x100的网络输入，用来表示一张灰度（单通道）矩形图像。我们使用64个通道和768个令牌长度，因为这些是默认值³。我们使用13的批次大小，因为它是质数，不会与其他参数混淆。
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The input image is first passed through a soft split layer with *kernel* = 7,
    *stride* = 4, and *paddin*g = 2\. The length of the tokens will be the kernel
    size (7∗7=49) times the number of channels (= 1 for grayscale input). We can use
    the `count_tokens` function to calculate how many tokens there should be after
    the soft split.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像首先经过一个软分割层，*kernel* = 7，*stride* = 4，以及*paddin*g = 2。令牌的长度将是卷积核大小（7∗7=49）乘以通道数（灰度输入的通道数
    = 1）。我们可以使用`count_tokens`函数来计算软分割后应该有多少个令牌。
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, we pass through the first *Token Transformer*. This does not impact the
    batch size or number of tokens, but it changes the length of the tokens to be
    *channels* = 64.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过第一个*Token Transformer*。这不会影响批次大小或令牌数，但它会将令牌的长度更改为*channels* = 64。
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, we reconstruct the tokens back into a 2D image. The `count_tokens` function
    again can tell us the shape of the new image. It will have 64 channels, the same
    as the length of the tokens coming out of the *Token Transformer*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将令牌重建回二维图像。`count_tokens`函数再次可以告诉我们新图像的形状。它将具有64个通道，这与从*Token Transformer*出来的令牌长度相同。
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have a 2D image again, we go back to the soft split! The next code
    block goes through the second soft split, the second *Token Transformer*, and
    the second image reconstruction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们又得到一个2D图像，我们返回到软分割步骤！接下来的代码块完成了第二次软分割、第二次*Token Transformer*和第二次图像重建。
- en: '[PRE27]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: From this reconstructed image, we go through a final soft split. Recall that
    the output of the T2T module should be a list of tokens.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个重建的图像中，我们进行最终的软分割。回想一下，T2T模块的输出应该是一个token列表。
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The last layer in the T2T module is a linear layer to project the tokens to
    the desired output size. We specified that as *token_len*=768.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: T2T模块的最后一层是一个线性层，用于将tokens投射到期望的输出大小。我们将其设置为*token_len*=768。
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: And that concludes the T2T Module!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是T2T模块的全部内容！
- en: ViT Backbone
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ViT Backbone
- en: From the T2T module, the tokens proceed through a ViT backbone. This is identical
    to the backbone of the ViT model described in [2]. The [Vision Transformers article](/vision-transformers-explained-a9d07147e4c8)
    does an in-depth walk through of the ViT model and the ViT backbone. The code
    is reproduced below, but we won’t do a walk-through. Check that out [here](/vision-transformers-explained-a9d07147e4c8)
    and then come back!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从T2T模块开始，token通过ViT骨干网络（ViT Backbone）。这与[2]中描述的ViT模型的骨干网络相同。你可以在[视觉变换器文章](/vision-transformers-explained-a9d07147e4c8)中深入了解ViT模型和ViT骨干网络的实现。下面重现了代码，但我们不会详细讲解。可以在[这里](/vision-transformers-explained-a9d07147e4c8)查看文章内容，再回来继续阅读！
- en: '[PRE33]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Complete Code
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完整代码
- en: To create the complete T2T-ViT module, we use the *T2T* module and the *ViT
    Backbone*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建完整的T2T-ViT模块，我们使用了*T2T*模块和*ViT Backbone*。
- en: '[PRE34]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the *T2T-ViT Model*, the *img_size and softsplit_kernels* parameters define
    the soft splits in the T2T module. The *num_heads*, *token_chan, qkv_bias*, and
    *qk_scale* parameters define the *Attention* modules within the *Token Transformer*
    modules, which are themselves within the *T2T* module. The *T2T_hidden_chan_mul*
    and *act_layer* define the *NN* module within the *Token Transformer* module.
    The *token_len* defines the linear layers in the *T2T* module. The *norm_layer*
    defines the norms.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在*T2T-ViT模型*中，*img_size*和*softsplit_kernels*参数定义了T2T模块中的软分割。*num_heads*、*token_chan*、*qkv_bias*和*qk_scale*参数定义了*Token
    Transformer*模块中的*Attention*模块，这些模块本身位于*T2T*模块内。*T2T_hidden_chan_mul*和*act_layer*定义了*Token
    Transformer*模块中的*NN*模块。*token_len*定义了*T2T*模块中的线性层。*norm_layer*定义了规范化层。
- en: Similarly, the *num_heads*, *token_len, qkv_bias*, and *qk_scale* parameters
    define the *Attention* modules within the *Encoding Blocks*, which are themselves
    within the *ViT Backbone*. The *Encoding_hidden_chan_mul* and *act_layer* define
    the *NN* module within the *Encodin*g *Blocks*. The *depth* parameter defines
    how many *Encoding Blocks* are in the *ViT Backbone*. The *norm_layer* defines
    the norms. The *preds* parameter defines the prediction head in the *ViT Backbone*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*num_heads*、*token_len*、*qkv_bias*和*qk_scale*参数定义了*Encoding Blocks*中的*Attention*模块，这些模块本身位于*ViT
    Backbone*中。*Encoding_hidden_chan_mul*和*act_layer*定义了*Encoding Blocks*中的*NN*模块。*depth*参数定义了*ViT
    Backbone*中包含的*Encoding Blocks*的数量。*norm_layer*定义了规范化层。*preds*参数定义了*ViT Backbone*中的预测头。
- en: The *act_layer* can be any `torch.nn.modules.activation`⁶ layer, and the *norm_layer*
    can be any `torch.nn.modules.normalization`⁷ layer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*act_layer*可以是任何`torch.nn.modules.activation`⁶层，*norm_layer*可以是任何`torch.nn.modules.normalization`⁷层。'
- en: The *_init_weights* method sets custom initial weights for model training. This
    method could be deleted to initiate all learned weights and biases randomly. As
    implemented, the weights of linear layers are initialized as a truncated normal
    distribution; the biases of linear layers are initialized as zero; the weights
    of normalization layers are initialized as one; the biases of normalization layers
    are initialized as zero.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*_init_weights*方法为模型训练设置了自定义的初始权重。如果删除此方法，所有学习到的权重和偏置将会被随机初始化。当前实现中，线性层的权重初始化为截断的正态分布；线性层的偏置初始化为零；规范化层的权重初始化为一；规范化层的偏置初始化为零。'
- en: Conclusion
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Now, you can go forth and train T2T-ViT models with a deep understanding of
    their mechanics! The code in this article an be found in the [GitHub repository](https://github.com/lanl/vision_transformers_explained)
    for this series. The code from the T2T-ViT paper³ can be found [here](https://github.com/yitu-opensource/T2T-ViT).
    Happy transforming!
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以在深入理解其原理的基础上，开始训练T2T-ViT模型了！本文中的代码可以在[GitHub仓库](https://github.com/lanl/vision_transformers_explained)中找到。T2T-ViT论文³中的代码可以在[这里](https://github.com/yitu-opensource/T2T-ViT)找到。祝你变换愉快！
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本文已由洛斯阿拉莫斯国家实验室批准发布，编号为 LA-UR-23–33876。相关代码已获 BSD-3 开源许可证批准，编号 O#4693。
- en: Citations
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引用
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Vaswani 等人（2017）。*Attention Is All You Need*。[https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Dosovitskiy 等人（2020）。*一张图片等于 16x16 个词：用于大规模图像识别的 Transformers*。[https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
- en: '[3] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Yuan 等人（2021）。*Tokens-to-Token ViT：从头开始在 ImageNet 上训练 Vision Transformers*。[https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: → GitHub 代码：[https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)
- en: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *Mountain at Dusk Background.*
    License CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))。*黄昏山脉背景*。许可证 CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
- en: '[5] PyTorch. *Unfold.* [https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] PyTorch。*Unfold*。[https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
- en: '[6] PyTorch. *Non-linear Activation (weighted sum, nonlinearity).* [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] PyTorch。*非线性激活（加权和，非线性）*。[https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
- en: '[7] PyTorch. *Normalization Layers*. [https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] PyTorch。*归一化层*。[https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
