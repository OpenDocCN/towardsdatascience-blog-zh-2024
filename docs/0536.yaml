- en: Tokens-to-Token Vision Transformers, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27](https://towardsdatascience.com/tokens-to-token-vision-transformers-explained-2fa4e2002daa?source=collection_archive---------4-----------------------#2024-02-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vision Transformers Explained Series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Full Walk-Through of the Tokens-to-Token Vision Transformer, and Why It’s
    Better than the Original
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[![Skylar
    Jean Callis](../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png)](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    [Skylar Jean Callis](https://medium.com/@sjcallis?source=post_page---byline--2fa4e2002daa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2fa4e2002daa--------------------------------)
    ·20 min read·Feb 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Since their introduction in 2017 with* Attention is All You Need*¹, transformers
    have established themselves as the state of the art for natural language processing
    (NLP). In 2021,* An Image is Worth 16x16 Words*² successfully adapted transformers
    for computer vision tasks. Since then, numerous transformer-based architectures
    have been proposed for computer vision.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**In 2021*, Tokens-to-Token ViT: Training Vision Transformers from Scratch
    on ImageNet*³ outlined the Tokens-to-Token (T2T) ViT. This model aims to remove
    the heavy pretraining requirement present in the original ViT². This article walks
    through the T2T-ViT, including open-source code for T2T-ViT, as well as conceptual
    explanations of the components. All of the code uses the PyTorch Python package.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b75bcf1edb82ef945ae30bacbbc254a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Harrison Broadbent](https://unsplash.com/@harrisonbroadbent?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is part of a collection examining the internal workings of Vision
    Transformers in depth. Each of these articles is also available as a Jupyter Notebook
    with executable code. The other articles in the series are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Vision Transformers, Explained](/vision-transformers-explained-a9d07147e4c8)→
    [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Attention for Vision Transformers, Explained](/attention-for-vision-transformers-explained-70f83984c673)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Position Embeddings for Vision Transformers, Explained](/position-embeddings-for-vision-transformers-explained-a6f9add341d5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[**Tokens-to-Token Vision Transformers, Explained**](/tokens-to-token-vision-transformers-explained-2fa4e2002daa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: → [Jupyter Notebook](https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[GitHub Repository for Vision Transformers, Explained Series](https://github.com/lanl/vision_transformers_explained)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[What is Tokens-to-Token ViT?](#16bf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tokens-to-Token (T2T) Module](#4ce4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Soft Split](#1932)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Token Transformer](#2954)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Neural Network Module](#fac6)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [Image Reconstruction](#9ab4)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: — [All Together](#bc10)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[ViT Backbone](#7cd7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Complete Code](#3442)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Conclusion](#302d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — [Citations](#d446)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is Tokens-to-Token ViT?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first vision transformers able to match the performance of CNNs on computer
    vision tasks required pre-training on large datasets and then transferring to
    the benchmark of interest². However, pre-training on such datasets is not always
    feasible. For one, the pre-training dataset that achieved the best results in
    *An Image is Worth 16x16 Words* (the JFT-300M dataset) is not publicly available².
    Furthermore, vistransformers designed for tasks other than traditional image classification
    may not have such large pre-training datasets available.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2021, *Tokens-to-Token ViT: Training Vision Transformers from Scratch on
    ImageNet*³ was published, presenting a methodology that would circumvent the heavy
    pre-training requirement of previous vistransformers. They achieved this by replacing
    the *patch tokenization* in the ViT model² with the a Tokens-to-Token (T2T) module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/246ef8dc95b59a4579b2512487fd0f65.png)'
  prefs: []
  type: TYPE_IMG
- en: T2T-ViT Model Diagram (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Since the T2T module is what makes the T2T-ViT model unique, it will be the
    focus of this article. For a deep dive into the ViT components see the [Vision
    Transformers article](/vision-transformers-explained-a9d07147e4c8). The code is
    based on the publicly available GitHub code for *Tokens-to-Token ViT³* with some
    modifications. Changes to the source code include, but are not limited to, modifying
    to allow for non-square input images and removing dropout layers.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens-to-Token (T2T) Module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The T2T module serves to process the input image into tokens that can be used
    in the ViT module. Instead of simply splitting the input image into patches that
    become tokens, the T2T module sequentially computes attention between tokens and
    aggregates them together to capture additional structure in the image and to reduce
    the overall token length. The T2T module diagram is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5526e55717581c71ec62c1f844891475.png)'
  prefs: []
  type: TYPE_IMG
- en: T2T Module Diagram (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: Soft Split
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the first layer in the T2T-ViT model, the soft split layer is what separates
    an image into a series of tokens. The soft split layers are shown as blue blocks
    in the T2T diagram. Unlike the *patch tokenization* in the original ViT (read
    more about that [here](/vision-transformers-explained-a9d07147e4c8)), the soft
    splits in the T2T-ViT create overlapping patches.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of the soft split on this pixel art *Mountain at Dusk*
    by Luis Zuno ([@ansimuz](http://twitter.com/ansimuz))⁴. The original artwork has
    been cropped and converted to a single channel image. This means that each pixel
    has a value between zero and one. Single channel images are typically displayed
    in grayscale; however, we’ll be displaying it in a purple color scheme because
    its easier to see.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bac19092e0adc6376b2a9f26c9bcc604.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: This image has size *H=60* and *W=100*. We’ll use a patch size — or equivalently
    *kernel* — of *k=20*. T2T-ViT sets the *stride* — a measure of overlap — at *s=ceil(k/2)*
    and the *padding* at *p=ceil(k/4)*. For our example, that means we’ll use *s=10*
    and *p=5*. The padding is all zero values, which appear as the darkest purple.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can look at the patches created in the soft split, we have to know
    how many patches there will be. The soft splits are implemented as `torch.nn.Unfold`⁵
    layers. To calculate how many tokens the soft split will create, we use the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: where *h* is the original image height, *w* is the original image width, *k*
    is the kernel size, *s* is the stride size, and *p* is the padding size⁵. This
    formula assumes the kernel is square, and that the stride and padding are symmetric.
    Additionally, it assumes that dilation is 1.
  prefs: []
  type: TYPE_NORMAL
- en: '***An aside about dilation:*** *PyTorch describes dilation as “control[ling]
    the spacing between the kernel points”⁵, and refers readers to the diagram* [*here*](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md)*.
    A* `*dilation=1*` *value keeps the kernel as you would expect, all pixels touching.
    A user in* [*this forum*](https://discuss.pytorch.org/t/why-the-default-dilation-value-in-conv2d-is-1/5612)
    *suggests to think about it as “every* `*dilation*`*-th element is used.” In this
    case, every 1st element is used, meaning every element is used.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first term in the *num_tokens* equation describes how many tokens are along
    the height, while the second term describes how many tokens are along the width.
    We implement this in code below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the dimensions in the *Mountain at Dusk*⁴ example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can see how the soft split creates patches from the *Mountain at Dusk*⁴.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3c15402b4c8bc793159dd0dda514c27a.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: We can see how the soft split results in overlapping patches. By counting the
    patches as they move across the image, we can see that there are 6 patches along
    the height and 10 patches along the width, exactly as predicted. By flattening
    these patches, we see the resulting tokens. Let’s flatten the first patch as an
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/38a3de588c621e12bfdbc3fe1db6b56c.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: You can see where the padding shows up in the token!
  prefs: []
  type: TYPE_NORMAL
- en: 'When passed to the next layer, all of the tokens are aggregated together in
    a matrix. That matrix looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30691ccac86ef9fb2f06e9ef06133b33.png)'
  prefs: []
  type: TYPE_IMG
- en: Token Matrix (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'For *Mountain at Dusk*⁴ that would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a6945b1ee2dfb7556d5318a67f1508a.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: You can see the large areas of padding in the top left and bottom right of the
    matrix, as well as in smaller segments throughout. Now, our tokens are ready to
    be passed along to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Token Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next component of the T2T module is the Token Transformer, which is represented
    by the purple blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d0820b49b413302ff5c81dd37fe87513.png)'
  prefs: []
  type: TYPE_IMG
- en: Token Transformer (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for the Token Transformer class looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The *chan, num_heads, qkv_bias,* and *qk_scale* parameters define the *Attention*
    module components. A deep dive into attention for vistransformers is best left
    for [another time](/attention-for-vision-transformers-explained-70f83984c673).
  prefs: []
  type: TYPE_NORMAL
- en: The *hidden_chan_mul* and *act_layer* parameters define the *Neural Network*
    module components. The activation layer can be any `torch.nn.modules.activation`⁶
    layer. The *norm_layer* can be chosen from any `torch.nn.modules.normalization`⁷
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s step through each blue block in the diagram. We’re using 7∗7=49 as our
    starting token size, since the fist soft split has a default kernel of 7x7.³ We’re
    using 64 channels because that’s also the default³. We’re using 100 tokens because
    it’s a nice number. We’re using a batch size of 13 because it’s prime and won’t
    be confused for any of the other parameters. We’re using 4 heads because it divides
    the channels; however, you won’t see the head dimension in the Token Transformer
    Module.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: First, we pass the input through a norm layer, which does not change it’s shape.
    Next, it gets passed through the first *Attention* module, which changes the length
    of the tokens. Recall that a more in-depth explanation for Attention in VisTransformers
    can be found [here](/attention-for-vision-transformers-explained-70f83984c673).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now, we must save the state for a split connection layer. In the actual class
    definition, this is done more efficiently in one line. However, for this walk
    through, we do it separately.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can pass it through another norm layer and then the *Neural Network*
    module. The norm layer doesn’t change the shape of the input. The neural network
    is configured to also not change the shape.
  prefs: []
  type: TYPE_NORMAL
- en: The last step is the split connection, which also does not change the shape.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: That’s all for the Token Transformer Module.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Network Module
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The neural network (NN) module is a sub-component of the token transformer module.
    The neural network module is very simple, consisting of a fully-connected layer,
    an activation layer, and another fully-connected layer. The activation layer can
    be any `torch.nn.modules.activation`⁶ layer, which is passed as input to the module.
    The NN module can be configured to change the shape of an input, or to maintain
    the same shape. We’re not going to step through this code, as NNs are common in
    machine learning, and not the focus of this article. However, the code for the
    NN module is presented below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Image Reconstruction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The image reconstruction layers are also shown as blue blocks inside the T2T
    diagram. The shape of the input to the reconstruction layers looks like (batch,
    num_tokens, tokensize=channels). If we look at just one batch, that looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/492257ac2cfdc81f71d3f1ca1e9abaf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Single Batch of Tokens (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The reconstruction layers reshape the tokens into a 2D image again, which looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ae4a553884203d9c8a41f94a9939294.png)'
  prefs: []
  type: TYPE_IMG
- en: Reconstructed Image (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In each batch, there will be *tokensize = channel* number of reconstructed images.
    This is handled in the same way as if the image was in color, and had three color
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for reconstruction isn’t wrapped in it’s own function. However, an
    example is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: where *W*, *H* are the width and height of the image, *B* is the batch size,
    and *C* is the channels.
  prefs: []
  type: TYPE_NORMAL
- en: All Together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we’re ready to examine the whole T2T module put together! The model class
    for the T2T module looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s walk through the forward pass. Since we already examined the components
    in more depth, this section will treat them as black boxes: we’ll just be looking
    at the input and outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll define an input to the network of shape 1x400x100 to represent a grayscale
    (one channel) rectangular image. We’re using 64 channels and 768 token length
    because those are the default values³. We’re using a batch size of 13 because
    it’s prime and won’t be confused for any of the other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The input image is first passed through a soft split layer with *kernel* = 7,
    *stride* = 4, and *paddin*g = 2\. The length of the tokens will be the kernel
    size (7∗7=49) times the number of channels (= 1 for grayscale input). We can use
    the `count_tokens` function to calculate how many tokens there should be after
    the soft split.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Next, we pass through the first *Token Transformer*. This does not impact the
    batch size or number of tokens, but it changes the length of the tokens to be
    *channels* = 64.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Now, we reconstruct the tokens back into a 2D image. The `count_tokens` function
    again can tell us the shape of the new image. It will have 64 channels, the same
    as the length of the tokens coming out of the *Token Transformer*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a 2D image again, we go back to the soft split! The next code
    block goes through the second soft split, the second *Token Transformer*, and
    the second image reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: From this reconstructed image, we go through a final soft split. Recall that
    the output of the T2T module should be a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The last layer in the T2T module is a linear layer to project the tokens to
    the desired output size. We specified that as *token_len*=768.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: And that concludes the T2T Module!
  prefs: []
  type: TYPE_NORMAL
- en: ViT Backbone
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the T2T module, the tokens proceed through a ViT backbone. This is identical
    to the backbone of the ViT model described in [2]. The [Vision Transformers article](/vision-transformers-explained-a9d07147e4c8)
    does an in-depth walk through of the ViT model and the ViT backbone. The code
    is reproduced below, but we won’t do a walk-through. Check that out [here](/vision-transformers-explained-a9d07147e4c8)
    and then come back!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Complete Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create the complete T2T-ViT module, we use the *T2T* module and the *ViT
    Backbone*.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the *T2T-ViT Model*, the *img_size and softsplit_kernels* parameters define
    the soft splits in the T2T module. The *num_heads*, *token_chan, qkv_bias*, and
    *qk_scale* parameters define the *Attention* modules within the *Token Transformer*
    modules, which are themselves within the *T2T* module. The *T2T_hidden_chan_mul*
    and *act_layer* define the *NN* module within the *Token Transformer* module.
    The *token_len* defines the linear layers in the *T2T* module. The *norm_layer*
    defines the norms.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the *num_heads*, *token_len, qkv_bias*, and *qk_scale* parameters
    define the *Attention* modules within the *Encoding Blocks*, which are themselves
    within the *ViT Backbone*. The *Encoding_hidden_chan_mul* and *act_layer* define
    the *NN* module within the *Encodin*g *Blocks*. The *depth* parameter defines
    how many *Encoding Blocks* are in the *ViT Backbone*. The *norm_layer* defines
    the norms. The *preds* parameter defines the prediction head in the *ViT Backbone*.
  prefs: []
  type: TYPE_NORMAL
- en: The *act_layer* can be any `torch.nn.modules.activation`⁶ layer, and the *norm_layer*
    can be any `torch.nn.modules.normalization`⁷ layer.
  prefs: []
  type: TYPE_NORMAL
- en: The *_init_weights* method sets custom initial weights for model training. This
    method could be deleted to initiate all learned weights and biases randomly. As
    implemented, the weights of linear layers are initialized as a truncated normal
    distribution; the biases of linear layers are initialized as zero; the weights
    of normalization layers are initialized as one; the biases of normalization layers
    are initialized as zero.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you can go forth and train T2T-ViT models with a deep understanding of
    their mechanics! The code in this article an be found in the [GitHub repository](https://github.com/lanl/vision_transformers_explained)
    for this series. The code from the T2T-ViT paper³ can be found [here](https://github.com/yitu-opensource/T2T-ViT).
    Happy transforming!
  prefs: []
  type: TYPE_NORMAL
- en: This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876\.
    The associated code was approved for a BSD-3 open source license under O#4693.
  prefs: []
  type: TYPE_NORMAL
- en: Citations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Vaswani et al (2017). *Attention Is All You Need.* [https://doi.org/10.48550/arXiv.1706.03762](https://doi.org/10.48550/arXiv.1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Dosovitskiy et al (2020). *An Image is Worth 16x16 Words: Transformers
    for Image Recognition at Scale.* [https://doi.org/10.48550/arXiv.2010.11929](https://doi.org/10.48550/arXiv.2010.11929)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Yuan et al (2021). *Tokens-to-Token ViT: Training Vision Transformers from
    Scratch on ImageNet*. [https://doi.org/10.48550/arXiv.2101.11986](https://doi.org/10.48550/arXiv.2101.11986)'
  prefs: []
  type: TYPE_NORMAL
- en: '→ GitHub code: [https://github.com/yitu-opensource/T2T-ViT](https://github.com/yitu-opensource/T2T-ViT)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Luis Zuno ([@ansimuz](http://twitter.com/ansimuz)). *Mountain at Dusk Background.*
    License CC0: [https://opengameart.org/content/mountain-at-dusk-background](https://opengameart.org/content/mountain-at-dusk-background)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] PyTorch. *Unfold.* [https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold](https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] PyTorch. *Non-linear Activation (weighted sum, nonlinearity).* [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] PyTorch. *Normalization Layers*. [https://pytorch.org/docs/stable/nn.html#normalization-layers](https://pytorch.org/docs/stable/nn.html#normalization-layers)'
  prefs: []
  type: TYPE_NORMAL
