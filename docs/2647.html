<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Computer Use and AI Agents: A New Paradigm for Screen Interaction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Computer Use and AI Agents: A New Paradigm for Screen Interaction</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30">https://towardsdatascience.com/computer-use-and-ai-agents-a-new-paradigm-for-screen-interaction-b2dcbea0df5b?source=collection_archive---------6-----------------------#2024-10-30</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8262" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring the future of multimodal AI Agents and the Impact of Screen Interaction</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tula Masterman" class="l ep by dd de cx" src="../Images/c36b3740befd5dfdb8719dc6596f1a99.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*Fn6lzAzI489IDlnO-QI8_A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@tula.masterman?source=post_page---byline--b2dcbea0df5b--------------------------------" rel="noopener follow">Tula Masterman</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b2dcbea0df5b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 30, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">6</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/415e6388c247739dd845b959f936aec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mb3oHwfrL8TOHGuNrCRevg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image created by author using GPT4o</figcaption></figure><h1 id="3268" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk"><strong class="al">Introduction</strong>: The ever-evolving AI Agent Landscape</h1><p id="02ad" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Recent announcements from Anthropic, Microsoft, and Apple are changing the way we think about AI Agents. Today, the term “AI Agent” is oversaturated — nearly every AI-related announcement refers to agents, but their sophistication and utility vary greatly.</p><p id="1d2e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">At one end of the spectrum, we have advanced agents that leverage multiple loops for planning, tool execution, and goal evaluation, iterating until they complete a task. These agents might even create and use memories, learning from their past mistakes to drive future successes. Determining what makes an effective agent is a very active area of AI research. It involves understanding what attributes make a successful agent (e.g., how should the agent plan, how should it use memory, how many tools should it use, how should it keep track of it’s task) and the best approach to configure a team of agents.</p><p id="551e" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">On the other end of the spectrum, we find AI agents that execute single purpose tasks that require little if any reasoning. These agents are often more workflow focused. For example, an agent that consistently summarizes a document and stores the result. These agents are typically easier to implement because the use cases are narrowly defined, requiring less planning or coordination across multiple tools and fewer complex decisions.</p><p id="a984" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">With the latest announcements from Anthropic, Microsoft, and Apple, we’re <strong class="oa fr">witnessing a shift from text-based AI agents to multimodal agents</strong>. This opens up the potential to give an agent written or verbal instructions and allow it to seamlessly navigate your phone or computer to complete tasks. <strong class="oa fr">This has great potential to improve accessibility across devices, but also comes with significant risks.</strong> Anthropic’s computer use announcement highlights the risks of giving AI unfettered access to your screen, and provides risk mitigation tactics like running Claude in a dedicated virtual machine or container, limiting internet access to an allowlist of permitted domains, including human in the loop checks, and avoiding giving the model access to sensitive data. They note that no content submitted to the API will be used for training.</p><h1 id="2019" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Key Announcements from Anthropic, Microsoft, and Apple:</h1><h2 id="b7aa" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk"><strong class="al">Anthropic’s Claude 3.5 Sonnet: Giving AI the Power to Use Computers</strong></h2><ul class=""><li id="f898" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot pq pr ps bk"><strong class="oa fr">Overview</strong>: The goal of Computer Use is to give AI the ability to interact with a computer the same way a human would. Ideally Claude would be able to open and edit documents, click to various areas of the page, scroll and read pages, run and execute command line code, and more. Today, Claude can follow instructions from a human to move a cursor around the computer screen, click on relevant areas of the screen, and type into a virtual keyboard. Claude Scored 14.9% on the <a class="af pt" href="https://os-world.github.io/" rel="noopener ugc nofollow" target="_blank">OSWorld </a>benchmark, which is higher than other AI models on the same benchmark, but still significantly behind humans (humans typically score 70–75%).</li><li id="90cd" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How it works</strong>: Claude looks at user submitted screenshots and counts pixels to determine where it needs to move the cursor to complete the task. Researchers note that Claude was not given internet access during training for safety reasons, but that Claude was able to generalize from training tasks like using a calculator and text-editor to more complex tasks. It even retried tasks when it failed. Computer use includes three Anthropic defined tools: computer, text editor, and bash. The computer tool is used for screen navigation, text editor is used for viewing, creating, and editing text files, and bash is used to run bash shell commands.</li><li id="de04" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">Challenges</strong>: Despite it’s promising performance, there’s still a long way to go for Claude’s computer use abilities. Today it struggles with scrolling, overall reliability, and is vulnerable to prompt injections.</li><li id="9e48" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How to Use</strong>: Public beta available through the Anthropic API. Computer use can be combined with regular tool use.</li></ul><h2 id="9306" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk"><strong class="al">Microsoft’s OmniParser &amp; GPT-4V: Making Screens Understandable and Actionable for AI</strong></h2><ul class=""><li id="b6d7" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot pq pr ps bk"><strong class="oa fr">Overview</strong>: OmniParser is designed to parse screenshots of user interfaces and transform them into structured outputs. These outputs can be passed to a model like GPT-4V to generate actions based on the detected screen elements. OmniParser + GPT-4V were scored on a variety of benchmarks including <a class="af pt" href="https://microsoft.github.io/WindowsAgentArena/" rel="noopener ugc nofollow" target="_blank">Windows Agent Arena</a> which adapts the OSWorld benchmark to create Windows specific tasks. These tasks are designed to evaluate an agents ability to plan, understand the screen, and use tools, OmniParser &amp; GPT-4V scored ~20%.</li><li id="bbcc" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How it Works</strong>: OmniParser combines multiple fine-tuned models to understand screens. It uses a finetuned interactable icon/region detection model (<a class="af pt" href="https://yolov8.com/" rel="noopener ugc nofollow" target="_blank">YOLOv8</a>), a finetuned icon description model (<a class="af pt" href="https://arxiv.org/abs/2301.12597" rel="noopener ugc nofollow" target="_blank">BLIP-2</a> or <a class="af pt" href="https://arxiv.org/abs/2311.06242" rel="noopener ugc nofollow" target="_blank">Florence2</a>), and an OCR module. These models are used to detect icons and text and generate descriptions before sending this output to GPT-4V which decides how to use the output to interact with the screen.</li><li id="5bdc" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">Challenges</strong>: Today, when OmniParser detects repeated icons or text and passes them to GPT-4V, GPT-4V usually fails to click on the correct icon. Additionally, OmniParser is subject to OCR output so if the bounding box is off, the whole system might fail to click on the appropriate area for clickable links. There are also challenges with understanding certain icons since sometimes the same icon is used to describe different concepts (e.g., three dots for loading versus for a menu item).</li><li id="efcb" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How to Use</strong>: OmniParser is available on <a class="af pt" href="https://github.com/microsoft/OmniParser/" rel="noopener ugc nofollow" target="_blank">GitHub </a>&amp; <a class="af pt" href="https://huggingface.co/microsoft/OmniParser" rel="noopener ugc nofollow" target="_blank">HuggingFace</a> you will need to install the requirements and load the model from HuggingFace, next you can try running the demo notebooks to see how OmniParser breaks down images.</li></ul><h2 id="1d16" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Apple’s Ferret-UI: Bringing Multimodal Intelligence to Mobile UIs</h2><ul class=""><li id="751f" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot pq pr ps bk"><strong class="oa fr">Overview</strong>: Apple’s Ferret (Refer and Ground Anything Anywhere at Any Granularity) has been around since 2023, but recently Apple released Ferret-UI a MLLM (Multimodal Large Language Model) which can execute “referring, grounding, and reasoning tasks” on mobile UI screens. Referring tasks include actions like widget classification and icon recognition. Grounding tasks include tasks like find icon or find text. Ferret-UI can understand UIs and follow instructions to interact with the UI.</li><li id="c830" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How it Works</strong>: Ferret-UI is based on Ferret and adapted to work on finer grained images by training with “any resolution” so it can better understand mobile UIs. Each image is split into two sub-images which have their own features generated. The LLM uses the full image, both sub-images, regional features, and text embeddings to generate a response.</li><li id="704b" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">Challenges</strong>: Some of the results cited in the Ferret-UI paper demonstrate instances where Ferret predicts nearby text instead of the target text, predicts valid words when presented with a screen that has misspelled words, it also sometimes misclassifies UI attributes.</li><li id="ab6f" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><strong class="oa fr">How to Use</strong>: Apple made the data and code available on <a class="af pt" href="https://github.com/apple/ml-ferret/tree/main/ferretui" rel="noopener ugc nofollow" target="_blank">GitHub </a>for research use only. Apple released two Ferret-UI checkpoints, one built on Gemma-2b and one built on Llama-3–8B. The Ferret-UI models are subject to the licenses for Gemma and Llama while the dataset allows non-commercial use.</li></ul><h2 id="5265" class="oz nd fq bf ne pa pb pc nh pd pe pf nk oh pg ph pi ol pj pk pl op pm pn po pp bk">Summary: Three Approaches to AI Driven Screen Navigation</h2><p id="1aa2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In summary, each of these systems demonstrate a different approach to building multimodal agents that can interact with computers or mobile devices on our behalf.</p><p id="3d10" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Anthropic’s Claude 3.5 Sonnet focuses on general computer interaction where Claude counts pixels to appropriately navigate the screen. Microsoft’s OmniParser addresses specific challenges for breaking down user interfaces into structured outputs which are then sent to models like GPT-4V to determine actions. Apple’s Ferret-UI is tailored to mobile UI comprehension allowing it to identify icons, text, and widgets while also executing open-ended instructions related to the UI.</p><p id="a983" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Across each system, the <strong class="oa fr">workflow typically follows two key phases one for parsing the visual information and one for reasoning about how to interact with it</strong>. Parsing screens accurately is critical for properly planning how to interact with the screen and making sure the system reliably executes tasks.</p><h1 id="932c" class="nc nd fq bf ne nf ng gq nh ni nj gt nk nl nm nn no np nq nr ns nt nu nv nw nx bk">Conclusion: Building Smarter, Safer AI Agents</h1><p id="093a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In my opinion, the most exciting aspect of these developments is how <strong class="oa fr">multimodal capabilities and reasoning frameworks are starting to converge</strong>. While these tools offer <strong class="oa fr">promising capabilities</strong>, they still lag significantly behind human performance. There are also s<strong class="oa fr">ignificant AI safety concerns</strong> which need to be addressed when implementing any agentic system with screen access.</p><p id="72b8" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">One of the biggest benefits of agentic systems is their potential to overcome the cognitive limitations of individual models by breaking down tasks into specialized components. </strong>These systems can be built in many ways. In some cases, what appears to the user as <strong class="oa fr">a single agent</strong> may, behind the scenes, consist of <strong class="oa fr">a team of sub-agents</strong> — each <strong class="oa fr">managing distinct responsibilities</strong> like planning, screen interaction, or memory management. For example, a reasoning agent might coordinate with another agent that specializes in parsing screen data, while a separate agent curates memories to enhance future performance.</p><p id="38fe" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk">Alternatively, these capabilities might be <strong class="oa fr">combined within one robust agent</strong>. In this setup, the agent could have multiple internal planning modules— one focused on planning the screen interactions and another focused on managing the overall task. The best approach to structuring agents remains to be seen, but the goal remains the same: to create agents that perform reliably overtime, across multiple modalities, and adapt seamlessly to the user’s needs.</p><p id="6299" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><strong class="oa fr">References:</strong></p><ul class=""><li id="f754" class="ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot pq pr ps bk"><a class="af pt" href="https://www.anthropic.com/news/developing-computer-use" rel="noopener ugc nofollow" target="_blank">Developing Computer Use</a> (Anthropic)</li><li id="52aa" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://www.anthropic.com/news/3-5-models-and-computer-use" rel="noopener ugc nofollow" target="_blank">Introducing computer use, a new Claude 3.5 Sonnet, and Claude 3.5 Haiku</a> (Anthropic)</li><li id="1390" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://docs.anthropic.com/en/docs/build-with-claude/computer-use" rel="noopener ugc nofollow" target="_blank">Computer Use Documentation</a> (Anthropic)</li><li id="eee4" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://os-world.github.io/" rel="noopener ugc nofollow" target="_blank">OSWorld GitHub</a></li><li id="2f79" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://microsoft.github.io/WindowsAgentArena/" rel="noopener ugc nofollow" target="_blank">Windows Agent Arena</a></li><li id="137f" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://www.microsoft.com/en-us/research/articles/omniparser-for-pure-vision-based-gui-agent/" rel="noopener ugc nofollow" target="_blank">OmniParser for pure vision-based GUI agent</a></li><li id="9540" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://github.com/microsoft/OmniParser/" rel="noopener ugc nofollow" target="_blank">OmniParser GitHub</a></li><li id="fe36" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://huggingface.co/microsoft/OmniParser" rel="noopener ugc nofollow" target="_blank">OmniParser HuggingFace</a></li><li id="f3e4" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://arxiv.org/pdf/2408.00203" rel="noopener ugc nofollow" target="_blank">OmniParser ArXiv</a></li><li id="3fc8" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://github.com/apple/ml-ferret" rel="noopener ugc nofollow" target="_blank">Ferret GitHub</a></li><li id="dd7f" class="ny nz fq oa b go pu oc od gr pv of og oh pw oj ok ol px on oo op py or os ot pq pr ps bk"><a class="af pt" href="https://arxiv.org/pdf/2404.05719" rel="noopener ugc nofollow" target="_blank">Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs</a></li></ul><p id="23b5" class="pw-post-body-paragraph ny nz fq oa b go ou oc od gr ov of og oh ow oj ok ol ox on oo op oy or os ot fj bk"><em class="pz">Interested in discussing further or collaborating? Reach out on </em><a class="af pt" href="https://www.linkedin.com/in/tula-masterman/" rel="noopener ugc nofollow" target="_blank"><em class="pz">LinkedIn</em></a><em class="pz">!</em></p></div></div></div></div>    
</body>
</html>