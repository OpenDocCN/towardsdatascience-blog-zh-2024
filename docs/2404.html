<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Choose the Architecture for Your GenAI Application</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Choose the Architecture for Your GenAI Application</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03">https://towardsdatascience.com/how-to-choose-the-architecture-for-your-genai-application-6053e862c457?source=collection_archive---------1-----------------------#2024-10-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="51f2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A framework to select the simplest, fastest, cheapest architecture that will balance LLMs’ creativity and risk</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Lak Lakshmanan" class="l ep by dd de cx" src="../Images/9faaaf72d600f592cbaf3e9089cbb913.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/2*TveVoapl-TEk-jBTrbis8w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://lakshmanok.medium.com/?source=post_page---byline--6053e862c457--------------------------------" rel="noopener follow">Lak Lakshmanan</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--6053e862c457--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">12</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="bf9d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Look at any LLM tutorial and the suggested usage involves invoking the API, sending it a prompt, and using the response. Suppose you want the LLM to generate a thank-you note, you could do:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="2d7c" class="no np fq nl b bg nq nr l ns nt">import openai<br/>recipient_name = "John Doe"<br/>reason_for_thanks = "helping me with the project"<br/>tone = "professional"<br/>prompt = f"Write a thank you message to {recipient_name} for {reason_for_thanks}. Use a {tone} tone."<br/>response = openai.Completion.create("text-davinci-003", prompt=prompt, n=1)<br/>email_body = response.choices[0].text</span></pre><p id="fcfd" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While this is fine for PoCs, rolling to production with an architecture that treats an LLM as just another text-to-text (or text-to-image/audio/video) API results in an application that is under-engineered in terms of risk, cost, and latency.</p><p id="a2a5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The solution is not to go to the other extreme and over-engineer your application by fine-tuning the LLM and adding guardrails, etc. every time. The goal, as with any engineering project, is to find the right balance of complexity, fit-for-purpose, risk, cost, and latency for the specifics of each use case. In this article, I’ll describe a framework that will help you strike this balance.</p><h2 id="68e7" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">The framework of LLM application architectures</h2><p id="55ba" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Here’s a framework that I suggest you use to decide on the architecture for your GenAI application or agent. I’ll cover each of the eight alternatives shown in the Figure below in the sections that follow.</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="ot ou ov"><img src="../Images/1425ffc872beb43bfaeddb529c895e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y68Pw2IWj0aUpSG6ZfySPg.png"/></div></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">Choosing the right application architecture for your GenAI application. Diagram by author.</figcaption></figure><p id="f005" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The axes here (i.e., the decision criteria) are risk and creativity. For each use case where you are going to employ an LLM, start by identifying the creativity you need from the LLM and the amount of risk that the use case carries. This helps you narrow down the choice that strikes the right balance for you.</p><p id="bbd1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Note that whether or not to use Agentic Systems is a completely orthogonal decision to this — employ agentic systems when the task is too complex to be done by a single LLM call or if the task requires non-LLM capabilities. In such a situation, you’d break down the complex task into simpler tasks and orchestrate them in an agent framework. This article shows you how to build a GenAI application (or an agent) to perform one of those simple tasks.</p><h2 id="8f4a" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">Why the 1st decision criterion is creativity</h2><p id="cd47" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Why are creativity and risk the axes? LLMs are a non-deterministic technology and are more trouble than they are worth if you don’t really need all that much uniqueness in the content being created.</p><p id="afd2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For example, if you are generating a bunch of product catalog pages, how different do they really have to be? Your customers want accurate information on the products and may not really care that all SLR camera pages explain the benefits of SLR technology in the same way — in fact, some amount of standardization may be quite preferable for easy comparisons. This is a case where your creativity requirement on the LLM is quite low.</p><p id="d638" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It turns out that architectures that reduce the non-determinism also reduce the total number of calls to the LLM, and so also have the side-effect of reducing the overall cost of using the LLM. Since LLM calls are slower than the typical web service, this also has the nice side-effect of reducing the latency. That’s why the y-axis is creativity, and why we have cost and latency also on that axis.</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div class="ot ou ph"><img src="../Images/6cc4282e8145257ebf5242641daed464.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*0WhFPPRLeQKLajNiHzTCzw.png"/></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">Illustrative: use cases ordered by creativity. Diagram by author</figcaption></figure><p id="be25" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You could look at the illustrative use cases listed in the diagram above and argue whether they require low creativity or high. It really depends on your business problem. If you are a magazine or ad agency, even your informative content web pages (unlike the product catalog pages) may need to be creative.</p><h2 id="0138" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">Why the 2nd decision criterion is risk</h2><p id="15b0" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">LLMs have a tendency to hallucinate details and to reflect biases and toxicity in their training data. Given this, there are risks associated with directly sending LLM-generated content to end-users. Solving for this problem adds a lot of engineering complexity — you might have to introduce a human-in-the-loop to review content, or add guardrails to your application to validate that the generated content doesn’t violate policy.</p><p id="782c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If your use case allows end-users to send prompts to the model and the application takes actions on the backend (a common situation in many SaaS products) to generate a user-facing response, the risk associated with errors, hallucination, and toxicity is quite high.</p><p id="ef0c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The same use case (art generation) could carry different levels and kinds of risk depending on the context as shown in the figure below. For example, if you are generating background instrumental music to a movie, the risk associated might involve mistakenly reproducing copyrighted notes, whereas if you are generating ad images or videos broadcast to millions of users, you may be worried about toxicity. These different types of risk are associated with different levels of risk. As another example, if you are building an enterprise search application that returns document snippets from your corporate document store or technology documentation, the LLM-associated risks might be quite low. If your document store consists of medical textbooks, the risk associated with out-of-context content returned by a search application might be high.</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="ot ou pi"><img src="../Images/44c9c98ea17f42c4ebd891ad054e9d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGHa2oMZeValxu1aw2PNiA.png"/></div></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">Illustrative: use cases ordered by risk. Diagram by author</figcaption></figure><p id="8054" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As with the list of use cases ordered by creativity, you can quibble with the ordering of use cases by risk. But once you identify the risk associated with the use case and the creativity it requires, the suggested architecture is worth considering as a starting point. Then, if you understand the “why” behind each of these architectural patterns, you can select an architecture that balances your needs.</p><p id="298b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In the rest of this article, I’ll describe the architectures, starting from #1 in the diagram.</p><h2 id="aaed" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">1. Generate each time (for High Creativity, Low Risk tasks)</h2><p id="6243" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">This is the architectural pattern that serves as the default — invoke the API of the deployed LLM each time you want generated content. It’s the simplest, but it also involves making an LLM call each time.</p><p id="e638" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Typically, you’ll use a PromptTemplate and templatize the prompt that you send to the LLM based on run-time parameters. It’s a good idea to use a framework that allows you to swap out the LLM.</p><p id="b0b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For our example of sending an email based on the prompt, we could use langchain:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="5109" class="no np fq nl b bg nq nr l ns nt">prompt_template = PromptTemplate.from_template(<br/>    """<br/>    You are an AI executive assistant to {sender_name} who writes letters on behalf of the executive.<br/>    Write a 3-5 sentence thank you message to {recipient_name} for {reason_for_thanks}.<br/>    Extract the first name from {sender_name} and sign the message with just the first name.<br/>    """<br/>)<br/>...<br/>response = chain.invoke({<br/>    "recipient_name": "John Doe",<br/>    "reason_for_thanks": "speaking at our Data Conference",<br/>    "sender_name": "Jane Brown",<br/>})</span></pre><p id="c52c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Because you are calling the LLM each time, it’s appropriate only for tasks that require extremely high creativity (e.g., you want a different thank you note each time) and where you are not worried about the risk (e.g, if the end-user gets to read and edit the note before hitting “send”).</p><p id="c5e8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A common situation where this pattern is employed is for interactive applications (so it needs to respond to all kinds of prompts) meant for internal users (so low risk).</p><h2 id="375e" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">2. Response/Prompt caching (for Medium Creativity, Low Risk tasks)</h2><p id="fa0f" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">You probably don’t want to send the same thank you note again to the same person. You want it to be different each time.</p><p id="3929" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">But what if you are building a search engine on your past tickets, such as to assist internal customer support teams? In such cases, you do want repeat questions to generate the same answer each time.</p><p id="d09b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A way to drastically reduce cost and latency is to cache past prompts and responses. You can do such caching on the client side using langchain:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="af5d" class="no np fq nl b bg nq nr l ns nt">from langchain_core.caches import InMemoryCache<br/>from langchain_core.globals import set_llm_cache<br/><br/>set_llm_cache(InMemoryCache())<br/><br/>prompt_template = PromptTemplate.from_template(<br/>    """<br/>    What are the steps to put a freeze on my credit card account?<br/>    """<br/>)<br/>chain = prompt_template | model | parser</span></pre><p id="cb54" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">When I tried it, the cached response took 1/1000th of the time and avoided the LLM call completely.</p><p id="a795" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Caching is useful beyond client-side caching of exact text inputs and the corresponding responses (see Figure below). Anthropic supports “<a class="af pj" href="https://www.anthropic.com/news/prompt-caching" rel="noopener ugc nofollow" target="_blank">prompt caching</a>” whereby you can ask the model to cache part of a prompt (typically the system prompt and repetitive context) server-side, while continuing to send it new instructions in each subsequent query. Using prompt caching reduces cost and latency per query while not affecting the creativity. It is particularly helpful in RAG, document extraction, and few-shot prompting when the examples get large.</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="ot ou pk"><img src="../Images/daf299024bebc58df4684ce2e14e59cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g6OH8rp_oUknuFp_B1dA2Q.png"/></div></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">Response caching reduces the number of LLM calls; context caching reduces the number of tokens processed in each individual call. Together, they reduce the overall number of tokens and therefore the cost and latency. Diagram by author</figcaption></figure><p id="ae92" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Gemini separates out this functionality into <a class="af pj" href="https://ai.google.dev/gemini-api/docs/caching?lang=python" rel="noopener ugc nofollow" target="_blank">context caching</a> (which reduces the cost and latency) and <a class="af pj" href="https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions" rel="noopener ugc nofollow" target="_blank">system instructions</a> (which don’t reduce the token count, but do reduce latency). OpenAI recently announced support for prompt caching, with its implementation automatically caching the <a class="af pj" href="https://openai.com/index/api-prompt-caching/" rel="noopener ugc nofollow" target="_blank">longest prefix of a prompt</a> that was previously sent to the API, as long as the prompt is longer than 1024 tokens. Server-side caches like these do not reduce the capability of the model, only the latency and/or cost, as you will continue to potentially get different results to the same text prompt.</p><p id="73da" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The built-in caching methods require exact text match. However, it is possible to implement caching in a way that takes advantage of the nuances of your case. For example, you could rewrite prompts to canonical forms to increase the chances of a cache hit. Another common trick is to store the hundred most frequent questions, for any question that is close enough, you could rewrite the prompt to ask the stored question instead. In a multi-turn chatbot, you could get user confirmation on such semantic similarity. Semantic caching techniques like this will reduce the capability of the model somewhat, since you will get the same responses to even similar prompts.</p><h2 id="72c0" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">3. Pregenerated templates (for Medium Creativity, Low-Medium Risk tasks)</h2><p id="91a5" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Sometimes, you don’t really mind the same thank you note being generated to everyone in the same situation. Perhaps you are writing the thank you note to a customer who bought a product, and you don’t mind the same thank you note being generated to any customer who bought that product.</p><p id="9924" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At the same time, there is a higher risk associated with this use case because these communications are going out to end-users and there is no internal staff person able to edit each generated letter before sending it out.</p><p id="e7fc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In such cases, it can be helpful to pregenerate templated responses. For example, suppose you are a tour company and you offer 5 different packages. All you need is one thank you message for each of these packages. Maybe you want different messages for solo travelers vs. families vs. groups. You still need only 3x as many messages as you have packages.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="325b" class="no np fq nl b bg nq nr l ns nt">prompt_template = PromptTemplate.from_template(<br/>    """<br/>    Write a letter to a customer who has purchased a tour package.<br/>    The customer is traveling {group_type} and the tour is to {tour_destination}.<br/>    Sound excited to see them and explain some of the highlights of what they will see there<br/>    and some of the things they can do while there.<br/>    In the letter, use [CUSTOMER_NAME] to indicate the place to be replaced by their name<br/>    and [TOUR_GUIDE] to indicate the place to be replaced by the name of the tour guide.<br/>    """<br/>)<br/>chain = prompt_template | model | parser<br/>print(chain.invoke({<br/>    "group_type": "family",<br/>    "tour_destination": "Toledo, Spain",<br/>}))</span></pre><p id="ee66" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The result is messages like this for a given group-type and tour-destination:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="d950" class="no np fq nl b bg nq nr l ns nt">Dear [CUSTOMER_NAME],<br/><br/>We are thrilled to welcome you to Toledo on your upcoming tour! We can't wait to show you the beauty and history of this enchanting city.<br/><br/>Toledo, known as the "City of Three Cultures," boasts a fascinating blend of Christian, Muslim, and Jewish heritage.  You'll be mesmerized by the stunning architecture, from the imposing Alcázar fortress to the majestic Toledo Cathedral. <br/><br/>During your tour, you'll have the opportunity to:<br/><br/>* **Explore the historic Jewish Quarter:** Wander through the narrow streets lined with ancient synagogues and traditional houses. <br/>* **Visit the Monastery of San Juan de los Reyes:** Admire the exquisite Gothic architecture and stunning cloisters.<br/>* **Experience the panoramic views:** Take a scenic walk along the banks of the Tagus River and soak in the breathtaking views of the city.<br/>* **Delve into the art of Toledo:** Discover the works of El Greco, the renowned painter who captured the essence of this city in his art.<br/><br/>Our expert tour guide, [TOUR_GUIDE], will provide insightful commentary and share fascinating stories about Toledo's rich past. <br/><br/>We know you'll have a wonderful time exploring the city's treasures. Feel free to reach out if you have any questions before your arrival.<br/><br/>We look forward to welcoming you to Toledo!<br/><br/>Sincerely,<br/><br/>The [Tour Company Name] Team </span></pre><p id="53ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You can generate these messages, have a human vet them, and store them in your database.</p><p id="766e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As you can see, we asked the LLM to insert placeholders in the message that we can replace dynamically. Whenever you need to send out a response, retrieve the message from the database and replace the placeholders with actual data.</p><p id="b801" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using pregenerated templates turns a problem that would have required vetting hundreds of messages per day into one that requires vetting a few messages only when a new tour is added.</p><h2 id="77e4" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">4. Small Language Models (Low Risk, Low Creativity)</h2><p id="8ae7" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">Recent research shows that it is <a class="af pj" href="https://arxiv.org/abs/2401.11817" rel="noopener ugc nofollow" target="_blank">impossible to eliminate hallucination</a> in LLMs because it arises from a tension between learning all the computable functions we desire. A smaller LLM for a more targeted task has less risk of hallucinating than one that’s too large for the desired task. You might be using a frontier LLM for tasks that don’t require the power and world-knowledge that it brings.</p><p id="6656" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In use cases where you have a very simple task that doesn’t require much creativity and very low risk tolerance, you have the option of using a small language model (SLM). This does trade off accuracy — in a <a class="af pj" href="https://techcommunity.microsoft.com/t5/azure-for-isv-and-startups/evaluating-the-quality-of-ai-document-data-extraction-with-small/ba-p/4157719" rel="noopener ugc nofollow" target="_blank">June 2024 study, a Microsoft researcher</a> found that for extracting structured data from unstructured text corresponding to an invoice, their smaller text-based model (Phi-3 Mini 128K) could get 93% accuracy as compared to the 99% accuracy achievable by GPT-4o.</p><p id="0d7e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The team at LLMWare <a class="af pj" href="https://medium.com/@darrenoberst/best-small-language-models-for-accuracy-and-enterprise-use-cases-benchmark-results-cf71964759c8" rel="noopener">evaluates a wide range of SLMs</a>. At the time of writing (2024), they found that Phi-3 was the best, but that over time, smaller and smaller models were achieving this performance.</p><p id="483d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Representing these two studies pictorially, SLMs are increasingly achieving their accuracy with smaller and smaller sizes (so less and less hallucination) while LLMs have been focused on increasing task ability (so more and more hallucination). The difference in accuracy between these approaches for tasks like document extraction has stabilized (see Figure).</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div class="ot ou pl"><img src="../Images/3405095403aa19ecb80e6296e2b1022a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*qiGJ0zYTIM7xxCS87VUnmg.png"/></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">The trend is for SLMs to get the same accuracy with smaller and smaller models, and for LLMs to focus on more capabilities with larger and larger models. The accuracy differential on simple tasks has stabilized. Diagram by author.</figcaption></figure><p id="2497" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If this trend holds up, expect to be using SLMs and non-frontier LLMs for more and more enterprise tasks that require only low creativity and have a low tolerance for risk. Creating embeddings from documents, such as for knowledge retrieval and topic modeling, are use cases that tend to fit this profile. Use small language models for these tasks.</p><h2 id="c8e9" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">5. Assembled Reformat (Medium Risk, Low Creativity)</h2><p id="ad97" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">The underlying idea behind Assembled Reformat is to use pre-generation to reduce the risk on dynamic content, and use LLMs only for extraction and summarization, tasks that introduce only a low-level of risk even though they are done “live”.</p><p id="6abf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Suppose you are a manufacturer of machine parts and need to create a web page for each item in your product catalog. You are obviously concerned about accuracy. You don’t want to claim some item is heat-resistant when it’s not. You don’t want the LLM to hallucinate the tools required to install the part.</p><p id="5905" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">You probably have a database that describes the attributes of each part. A simple approach is to employ an LLM to generate content for each of the attributes. As with pre-generated templates (Pattern #3 above), make sure to have a human review them before storing the content in your content management system.</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="4a43" class="no np fq nl b bg nq nr l ns nt">prompt_template = PromptTemplate.from_template(<br/>    """<br/>    You are a content writer for a manufacturer of paper machines.<br/>    Write a one-paragraph description of a {part_name}, which is one of the parts of a paper machine.<br/>    Explain what the part is used for, and reasons that might need to replace the part.<br/>    """<br/>)<br/>chain = prompt_template | model | parser<br/>print(chain.invoke({<br/>    "part_name": "wet end",<br/>}))</span></pre><p id="5e50" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, simply appending all the text generated will result in something that’s not very pleasing to read. You could, instead, assemble all of this content into the context of the prompt, and ask the LLM to reformat the content into the desired website layout:</p><pre class="nf ng nh ni nj nk nl nm bp nn bb bk"><span id="e323" class="no np fq nl b bg nq nr l ns nt">class CatalogContent(BaseModel):<br/>    part_name: str = Field("Common name of part")<br/>    part_id: str = Field("unique part id in catalog")<br/>    part_description: str = Field("short description of part")<br/>    price: str = Field("price of part")<br/><br/>catalog_parser = JsonOutputParser(pydantic_object=CatalogContent)<br/><br/>prompt_template = PromptTemplate(<br/>    template="""<br/>    Extract the information needed and provide the output as JSON.<br/>    {database_info}<br/>    Part description follows:<br/>    {generated_description}<br/>    """,<br/>    input_variables=["generated_description", "database_info"],<br/>    partial_variables={"format_instructions": catalog_parser.get_format_instructions()},<br/>)<br/><br/>chain = prompt_template | model | catalog_parser</span></pre><p id="bd74" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you need to summarize reviews, or trade articles about the item, you can have this be done in a batch processing pipeline, and feed the summary into the context as well.</p><h2 id="c950" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">6. ML Selection of Template (Medium Creativity, Medium Risk)</h2><p id="d5ba" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">The assembled reformat approach works for web pages where the content is quite static (as in product catalog pages). However, if you are an e-commerce retailer, and you want to create personalized recommendations, the content is much more dynamic. You need higher creativity out of the LLM. Your risk tolerance in terms of accuracy is still about the same.</p><p id="f291" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What you can do in such cases is to continue to use pre-generated templates for each of your products, and then use machine learning to select which templates you will employ.</p><p id="23ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">For personalized recommendations, for example, you’d use a traditional recommendations engine to select which products will be shown to the user, and pull in the appropriate pre-generated content (images + text) for that product.</p><p id="2710" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This approach of combining pregeneration + ML can also be used if you are customizing your website for different customer journeys. You’ll pregenerate the landing pages and use a propensity model to choose what the next best action is.</p><p id="78a7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">7.Fine-tune (High Creativity, Medium Risk)</strong></p><p id="8a59" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If your creativity needs are high, there is no way to avoid using LLMs to generate the content you need. But, generating the content every time means that you can not scale human review.</p><p id="9adb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are two ways to address this conundrum. The simpler one, from an engineering complexity standpoint, is to teach the LLM to produce the kind of content that you want and not generate the kinds of content you don’t. This can be done through fine-tuning.</p><p id="ea8a" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">There are three methods to fine-tune a foundational model: adapter tuning, distillation, and human feedback. Each of these fine-tuning methods address different risks:</p><ul class=""><li id="375c" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pm pn po bk">Adapter tuning retains the full capability of the foundational model, but allows you to select for specific style (such as content that fits your company voice). The risk addressed here is brand risk.</li><li id="84f3" class="mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne pm pn po bk">Distillation approximates the capability of the foundational model, but on a limited set of tasks, and using a smaller model that can be deployed on premises or behind a firewall. The risk addressed here is of confidentiality.</li><li id="5a83" class="mj mk fq ml b go pp mn mo gr pq mq mr ms pr mu mv mw ps my mz na pt nc nd ne pm pn po bk">Human feedback either through RLHF or through DPO allows the model to start off with reasonable accuracy, but get better with human feedback. The risk addressed here is of fit-for-purpose.</li></ul><p id="df7e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Common use cases for fine-tuning include being able to create branded content, summaries of confidential information, and personalized content.</p><h2 id="4124" class="nu np fq bf nv nw nx ny nz oa ob oc od ms oe of og mw oh oi oj na ok ol om on bk">8. Guardrails (High Creativity, High Risk)</h2><p id="ec4c" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">What if you want the full spectrum of capabilities, and you have more than one type of risk to mitigate — perhaps you are worried about brand risk, leakage of confidential information, and/or interested in ongoing improvement through feedback?</p><p id="e99f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At that point, there is no alternative but to go whole hog and build guardrails. Guardrails may involve preprocessing the information going into the model, post-processing the output of the model, or iterating on the prompt based on error conditions.</p><p id="2db4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Pre-built guardrails (eg. Nvidia’s NeMo) exist for commonly needed functionality such as checking for jailbreak, masking sensitive data in the input, and self-check of facts.</p><figure class="nf ng nh ni nj ow ot ou paragraph-image"><div role="button" tabindex="0" class="ox oy ed oz bh pa"><div class="ot ou pu"><img src="../Images/07ef1f452e1c29a798262cd1c41af684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9T3ls8OevWznm0KAtn0Hiw.png"/></div></div><figcaption class="pc pd pe ot ou pf pg bf b bg z dx">Guardrails you may have to build. Diagram by author.</figcaption></figure><p id="f369" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, it’s likely that you’ll have to implement some of the guardrails yourself (see Figure above). An application that needs to be deployed alongside programmable guardrails is the most complex way that you could choose to implement a GenAI application. Make sure that this complexity is warranted before going down this route.</p><h1 id="1572" class="pv np fq bf nv pw px gq nz py pz gt od qa qb qc qd qe qf qg qh qi qj qk ql qm bk">Summary</h1><p id="8799" class="pw-post-body-paragraph mj mk fq ml b go oo mn mo gr op mq mr ms oq mu mv mw or my mz na os nc nd ne fj bk">I suggest you use a framework that balances creativity and risk to decide on the architecture for your GenAI application or agent. Creativity refers to the level of uniqueness required in the generated content. Risk relates to the impact if the LLM generates inaccurate, biased, or toxic content. Addressing high-risk scenarios necessitates engineering complexity, such as human review or guardrails.</p><p id="25d8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The framework consists of eight architectural patterns that address different combination of creativity and risk:</p><p id="4427" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">1. <strong class="ml fr">Generate Each Time:</strong> Invokes the LLM API for every content generation request, offering maximum creativity but with higher cost and latency. Suitable for interactive applications that don’t have much risk, such as internal tools..<br/>2. <strong class="ml fr">Response/Prompt Caching</strong>: For medium creativity, low-risk tasks. Caches past prompts and responses to reduce cost and latency. Useful when consistent answers are desirable, such as internal customer support search engines. Techniques like prompt caching, semantic caching, and context caching enhance efficiency without sacrificing creativity.<br/>3. <strong class="ml fr">Pregenerated Templates</strong>: Employs pre-generated, vetted templates for repetitive tasks, reducing the need for constant human review. Suitable for medium creativity, low-medium risk situations where standardized yet personalized content is required, such as customer communication in a tour company.<br/>4. <strong class="ml fr">Small Language Models (SLMs</strong>): Uses smaller models to reduce hallucination and cost as compared to larger LLMs. Ideal for low creativity, low-risk tasks like embedding creation for knowledge retrieval or topic modeling.<br/>5. <strong class="ml fr">Assembled Reformat: </strong>Uses LLMs for reformatting and summarization, with pre-generated content to ensure accuracy. Suitable for content like product catalogs where accuracy is paramount on some parts of the content, while creative writing is required on others.<br/>6. <strong class="ml fr">ML Selection of Template:</strong> Leverages machine learning to select appropriate pre-generated templates based on user context, balancing personalization with risk management. Suitable for personalized recommendations or dynamic website content. <br/>7. <strong class="ml fr">Fine-tune</strong>: Involves fine-tuning the LLM to generate desired content while minimizing undesired outputs, addressing risks related to one of brand voice, confidentiality, or accuracy. Adapter Tuning focuses on stylistic adjustments, distillation on specific tasks, and human feedback for ongoing improvement.<br/>8. <strong class="ml fr">Guardrails</strong>: High creativity, high-risk tasks require guardrails to mitigate multiple risks, including brand risk and confidentiality, through preprocessing, post-processing, and iterative prompting. Off-the-shelf guardrails address common concerns like jailbreaking and sensitive data masking while custom-built guardrails may be necessary for industry/application-specific requirements.</p><p id="e0e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">By using the above framework to architect GenAI applications, you will be able to balance complexity, fit-for-purpose, risk, cost, and latency for each use case.</p><p id="4eb4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="qn">(Periodic reminder: these posts are my personal views, not those of my employers, past or present.)</em></p></div></div></div></div>    
</body>
</html>