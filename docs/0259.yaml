- en: Deploying Large Language Models with SageMaker Asynchronous Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-large-language-models-with-sagemaker-asynchronous-inference-c00038b70b3e?source=collection_archive---------7-----------------------#2024-01-27](https://towardsdatascience.com/deploying-large-language-models-with-sagemaker-asynchronous-inference-c00038b70b3e?source=collection_archive---------7-----------------------#2024-01-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Queue Requests For Near Real-Time Based Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page---byline--c00038b70b3e--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page---byline--c00038b70b3e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c00038b70b3e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c00038b70b3e--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page---byline--c00038b70b3e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c00038b70b3e--------------------------------)
    ·10 min read·Jan 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fce977b001046234030458bd3e0ab4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/a-robot-holding-a-gun-next-to-a-pile-of-rolls-of-toilet-paper-YeoSV_3Up-k)
    by [**Gerard Siderius**](https://unsplash.com/@siderius_creativ)
  prefs: []
  type: TYPE_NORMAL
- en: LLMs continue to burst in popularity and so do the number of ways to host and
    deploy them for inference. The challenges with LLM hosting have been well documented
    particularly due to the size of the model and ensuring optimal usage of the hardware
    that they are deployed on. LLM use-cases also vary. Some may require real-time
    based response times, while others have a more near real-time based latency requirement.
  prefs: []
  type: TYPE_NORMAL
- en: For the latter and for more offline inference use-cases, [SageMaker Asynchronous
    Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html)
    serves as a great option. With Asynchronous Inference, as the name suggests we
    focus on a more near real-time based workload where the latency is not necessary
    super strict, but still requires an active endpoint that can be invoked and scaled
    as necessary. Specifically within LLMs these types of workloads are becoming more
    and more popular with use-cases such as Content Editing/Generation, Summarization,
    and more. All of these workloads don’t need sub-second responses, but still require
    a timely inference that they can invoke as needed as opposed to a fully offline
    nature such as that of a [SageMaker Batch Transform](/sagemaker-batch-transform-d94dbaf889f6).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we’ll take a look at how we can use the [HuggingFace Text](https://github.com/huggingface/text-generation-inference)…
  prefs: []
  type: TYPE_NORMAL
