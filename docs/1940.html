<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>KernelSHAP can be misleading with correlated predictors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>KernelSHAP can be misleading with correlated predictors</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09">https://towardsdatascience.com/kernelshap-can-be-misleading-with-correlated-predictors-9f64108f7cfb?source=collection_archive---------7-----------------------#2024-08-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f787" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A concrete case study</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Shuyang Xiang" class="l ep by dd de cx" src="../Images/36a5fd18fd9b7b88cb41094f09b83882.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Q-6F64L3h4jxYNYPiqHVaQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@vanillaxiangshuyang?source=post_page---byline--9f64108f7cfb--------------------------------" rel="noopener follow">Shuyang Xiang</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--9f64108f7cfb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Aug 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mi mj mk"><p id="4830" class="ml mm mn mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">“Like many other permutation-based interpretation methods, the Shapley value method suffers from inclusion of unrealistic data instances when features are correlated. To simulate that a feature value is missing from a coalition, we marginalize the feature. ..When features are dependent, then we might sample feature values that do not make sense for this instance. ”—<a class="af ni" href="https://christophm.github.io/interpretable-ml-book/shapley.html#disadvantages-13" rel="noopener ugc nofollow" target="_blank"> Interpretable-ML-Book</a>.</p></blockquote><p id="65d3" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">SHAP (SHapley Additive exPlanations) values are designed to fairly allocate the contribution of each feature to the prediction made by a machine learning model, based on the concept of Shapley values from cooperative game theory. The Shapley value framework has several desirable theoretical properties and can, in principle, handle any predictive model. However, SHAP values can potentially be misleading, especially when using the KernelSHAP method for approximation. When predictors are correlated, these approximations can be imprecise and even have the opposite sign.</p><p id="81af" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this blog post, I will demonstrate how the original SHAP values can differ significantly from approximations made by the <a class="af ni" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html" rel="noopener ugc nofollow" target="_blank">SHAP framework</a>, especially the KernalSHAP and discuss the reasons behind these discrepancies.</p><h1 id="14b2" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk"><strong class="al">Case Study: Churn Rate</strong></h1><p id="982f" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Consider a scenario where we aim to predict the churn rate of rental leases in an office building, based on two key factors: occupancy rate and the rate of reported problems.</p><p id="470e" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The occupancy rate significantly impacts the churn rate. For instance, if the occupancy rate is too low, tenants may leave due to the office being underutilized. Conversely, if the occupancy rate is too high, tenants might depart because of overcrowding, seeking better options elsewhere.</p><p id="18c8" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Furthermore, let’s assume that the rate of reported problems is highly correlated with the occupancy rate, specifically that the reported problem rate is the square of the occupancy rate.</p><p id="0a3f" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We define the churn rate function as follows:</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol om"><img src="../Images/456107a845a24d3ffa3fb61d9ede4aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAuKxLiSTaK3U5PV_W5WIQ.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Image by author: churn rate function</figcaption></figure><p id="8881" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This function with respect to the two variables can be represented by the following illustrations:</p><figure class="on oo op oq or os ok ol paragraph-image"><div class="ok ol pd"><img src="../Images/fac81573dd6ddaff630cd4e105c27f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yQtERFH2iNsOyLxsgueWjg.png"/></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Image by author: Churn on two variables</figcaption></figure><h1 id="3c4b" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Discrepancies between original SHAP and Kernel SHAP</h1><h2 id="76f0" class="pe nk fq bf nl pf pg ph no pi pj pk nr mv pl pm pn mz po pp pq nd pr ps pt pu bk">SHAP Values Computed Using Kernel SHAP</h2><p id="7a04" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">We will now use the following code to compute the SHAP values of the predictors:</p><pre class="on oo op oq or pv pw px bp py bb bk"><span id="65b0" class="pz nk fq pw b bg qa qb l qc qd"># Define the dataframe <br/>churn_df=pd.DataFrame(<br/>    {<br/>        "occupancy_rate":occupancy_rates,<br/>        "reported_problem_rate": reported_problem_rates,<br/>        "churn_rate":churn_rates,<br/>    }<br/>)<br/>X=churn_df.drop(["churn_rate"],axis=1)<br/>y=churn_df["churn_rate"]<br/><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)<br/># append one speical point<br/>X_test=pd.concat(objs=[X_test, pd.DataFrame({"occupancy_rate":[0.8], "reported_problem_rate":[0.64]})])<br/><br/># Define the prediction<br/>def predict_fn(data):<br/>    occupancy_rates = data[:, 0]<br/>    reported_problem_rates = data[:, 1]<br/><br/>    churn_rate= C_base +C_churn*(C_occ* occupancy_rates-reported_problem_rates-0.6)**2 +C_problem*reported_problem_rates<br/>    return churn_rate<br/><br/># Create the SHAP KernelExplainer using the correct prediction function<br/>background_data = shap.sample(X_train,100)<br/>explainer = shap.KernelExplainer(predict_fn, background_data)<br/>shap_values = explainer(X_test)</span></pre><p id="220d" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The code above performs the following tasks:</p><ol class=""><li id="e030" class="ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh qe qf qg bk">Data Preparation: A DataFrame named <code class="cx qh qi qj pw b">churn_df</code> is created with columns <code class="cx qh qi qj pw b">occupancy_rate</code>, <code class="cx qh qi qj pw b">reported_problem_rate</code>, and <code class="cx qh qi qj pw b">churn_rate</code>. Variables and target (<code class="cx qh qi qj pw b">churn_rate</code> ) are then created from and Data is split into training and testing sets, with 80% for training and 20% for testing. Note that a special data point with specific <code class="cx qh qi qj pw b">occupancy_rate</code> and <code class="cx qh qi qj pw b">reported_problem_rate</code> values is added to the test set <code class="cx qh qi qj pw b">X_test</code>.</li><li id="a782" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qe qf qg bk">Prediction Function Definition: A function <code class="cx qh qi qj pw b">predict_fn</code> is defined to calculate churn rate using a specific formula involving predefined constants.</li><li id="6d83" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qe qf qg bk">SHAP Analysis: A SHAP <code class="cx qh qi qj pw b">KernelExplainer</code> is initialized using the prediction function and <code class="cx qh qi qj pw b">background_data</code> samples from <code class="cx qh qi qj pw b">X_train. </code>SHAP values for <code class="cx qh qi qj pw b">X_test</code> are computed using the <code class="cx qh qi qj pw b">explainer</code>.</li></ol><p id="a4fd" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Below, you can see a summary SHAP bar plot, which represents the average SHAP values for <code class="cx qh qi qj pw b">X_test</code> :</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qp"><img src="../Images/4726e4abacf15f58d78cd540eb486327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICNZV8UC8N9cxaXRjHOoBA.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Image by author: average shap values</figcaption></figure><p id="1c3e" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In particular, we see that at the data point (0.8, 0.64), the SHAP values of the two features are 0.10 and -0.03, illustrated by the following force plot:</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qq"><img src="../Images/f3be60805793ee4b29f17cadb80ce818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsPHWMjiOg3p7jUDcgKUOw.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Image by author Force Plot of one data point</figcaption></figure><h2 id="28e8" class="pe nk fq bf nl pf pg ph no pi pj pk nr mv pl pm pn mz po pp pq nd pr ps pt pu bk">SHAP Values by orignal definition</h2><p id="b744" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Let’s take a step back and compute the exact SHAP values step by step according to their original definition. The general formula for SHAP values is given by:</p><figure class="on oo op oq or os ok ol paragraph-image"><div class="ok ol qr"><img src="../Images/506fd5f8560513877e5e88618a511382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*8-APOBVdBTjec41FW0K7BA.png"/></div></figure><p id="dbd5" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">where: S is a subset of all feature indices excluding i, |S| is the size of the subset S, M is the total number of features, f(XS​∪{xi​}) is the function evaluated with the features in S with xi present while f(XS) is the function evaluated with the feature in S with xi absent.</p><p id="8aa3" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Now, let’s calculate the SHAP values for two features: occupancy rate (denoted as x1​) and reported problem rate (denoted as x2​) at the data point (0.8, 0.64). Recall that x1​ and x2are related by x1 = x2².</p><p id="7be3" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We have the SHAP value for occupancy rate at the data point:</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qs"><img src="../Images/745d8cfe9265145b22870e14c1902dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfsF1k3im5C4QQyKFeY_zQ.png"/></div></div></figure><p id="17f0" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">and, similary, for the feature reported problem rate:</p><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qt"><img src="../Images/3ea90e76aec7a3ba8c1e59d143dc01cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0a3eULF_C4WNf_AzGKoS_g.png"/></div></div></figure><p id="ff9c" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">First, let’s compute the SHAP value for the occupancy rate at the data point:</p><ol class=""><li id="6b7f" class="ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh qe qf qg bk">The first term is the expectation of the model’s output when X1​ is fixed at 0.8 and X2​ is averaged over its distribution. Given the relationship x1 = x2², this expectation leads to the model’s output at the specific point (0.8, 0.64).</li><li id="4557" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qe qf qg bk">The second term is the unconditional expectation of the model’s output, where both X1 and X2 are averaged over their distributions. This can be computed by averaging the outputs over all data points in the background dataset.</li><li id="ac45" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qe qf qg bk">The third term is the model’s output at the specific point (0.8, 0.64).</li><li id="c969" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qe qf qg bk">The final term is the expectation of the model’s output when X1​ is averaged over its distribution, given that X2​ is fixed at the specific point 0.64. Again, due to the relationship x_1 = x_2²​, this expectation matches the model’s output at (0.8, 0.64), similar to the first step.</li></ol><p id="b4a1" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Thus, the SHAP values compute from the original definition for the two features occupancy rate and reported problem rate at the data point (0.8, 0.64) are -0.0375 and -0.0375, respectively, which is quite different from the values given by Kernel SHAP.</p><p id="5aa7" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Where comes discrepancies?</p><h1 id="02c7" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Cause of Discrepancies in SHAP Values</h1><p id="0c3f" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">As you may have noticed, the discrepancy between the two methods primarily arises from the second and fourth steps, where we need to compute the conditional expectation. This involves calculating the expectation of the model’s output when X1X_1X1​ is conditioned on 0.8.</p><ul class=""><li id="b153" class="ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh qu qf qg bk"><strong class="mo fr">Exact SHAP</strong>: When computing exact SHAP values, the dependencies between features (such as x1=x_2² in our example​) are explicitly accounted for. This ensures accurate calculations by considering how feature interactions impact the model’s output.</li><li id="d252" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk"><strong class="mo fr">Kernel SHAP</strong>: By default, Kernel SHAP assumes feature independence, which can lead to inaccurate SHAP values when features are actually dependent. According to the paper <a class="af ni" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"><em class="mn">A Unified Approach to Interpreting Model Predictions</em>,</a> this assumption is a simplification. In practice, features are often correlated, making it challenging to achieve accurate approximations when using Kernel SHAP.</li></ul><figure class="on oo op oq or os ok ol paragraph-image"><div role="button" tabindex="0" class="ot ou ed ov bh ow"><div class="ok ol qv"><img src="../Images/536458182b111c3c9659970702905c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LhIWV-Ir1jEmuo6wKK-bQ.png"/></div></div><figcaption class="oy oz pa ok ol pb pc bf b bg z dx">Screenshot from the paper</figcaption></figure><h1 id="eaf2" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Potential resolutions</h1><p id="4102" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">Unfortunately, computing SHAP values directly based on their original definition can be computationally expensive. Here are some alternative approaches to consider:</p><h2 id="cc90" class="pe nk fq bf nl pf pg ph no pi pj pk nr mv pl pm pn mz po pp pq nd pr ps pt pu bk">TreeSHAP</h2><ul class=""><li id="dfc2" class="ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh qu qf qg bk">Designed specifically for tree-based models like random forests and gradient boosting machines, TreeSHAP efficiently computes SHAP values while effectively managing feature dependencies.</li><li id="3645" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">This method is optimized for tree ensembles, making it faster and more scalable compared to traditional SHAP computations.</li><li id="8524" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">When using TreeSHAP within the SHAP framework, set the parameter <code class="cx qh qi qj pw b">feature_perturbation = "interventional"</code> to account for feature dependencies accurately.</li></ul><h2 id="050a" class="pe nk fq bf nl pf pg ph no pi pj pk nr mv pl pm pn mz po pp pq nd pr ps pt pu bk">Extending Kernel SHAP for Dependent Features</h2><ul class=""><li id="4910" class="ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh qu qf qg bk">To address feature dependencies,<!-- --> this paper<!-- --> involves extending Kernel SHAP. One method is to assume that the feature vector follows a multivariate Gaussian distribution. In this approach:</li><li id="66a9" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">Conditional distributions are modeled as multivariate Gaussian distributions.</li><li id="c34a" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">Samples are generated from these conditional Gaussian distributions using estimates from the training data.</li><li id="b716" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">The integral in the approximation is computed based on these samples.</li><li id="13a9" class="ml mm fq mo b go qk mq mr gr ql mt mu mv qm mx my mz qn nb nc nd qo nf ng nh qu qf qg bk">This method assumes a multivariate Gaussian distribution for features, which may not always be applicable in real-world scenarios where features can exhibit different dependency structures.</li></ul><h2 id="cf90" class="pe nk fq bf nl pf pg ph no pi pj pk nr mv pl pm pn mz po pp pq nd pr ps pt pu bk">Improving Kernel SHAP Accuracy</h2><ul class=""><li id="edfc" class="ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh qu qf qg bk"><strong class="mo fr">Description</strong>: Enhance the accuracy of Kernel SHAP by ensuring that the background dataset used for approximation is representative of the actual data distribution with independant features.</li></ul><p id="e8f1" class="pw-post-body-paragraph ml mm fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">By utilizing these methods, you can address the computational challenges associated with calculating SHAP values and enhance their accuracy in practical applications. However, it is important to note that no single solution is universally optimal for all scenarios.</p><h1 id="4391" class="nj nk fq bf nl nm nn gq no np nq gt nr ns nt nu nv nw nx ny nz oa ob oc od oe bk">Conclusion</h1><p id="b28a" class="pw-post-body-paragraph ml mm fq mo b go of mq mr gr og mt mu mv oh mx my mz oi nb nc nd oj nf ng nh fj bk">In this blog post, we’ve explored how SHAP values, despite their strong theoretical foundation and versatility across various predictive models, can suffer from accuracy issues when predictors are correlated, particularly when approximations like KernelSHAP are employed. Understanding these limitations is crucial for effectively interpreting SHAP values. By recognizing the potential discrepancies and selecting the most suitable approximation methods, we can achieve more accurate and reliable feature attribution in our models.</p></div></div></div></div>    
</body>
</html>