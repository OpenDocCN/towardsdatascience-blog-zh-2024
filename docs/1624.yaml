- en: Duplicate Detection with GenAI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GenAI进行重复检测
- en: 原文：[https://towardsdatascience.com/duplicate-detection-with-genai-ba2b4f7845e7?source=collection_archive---------1-----------------------#2024-07-01](https://towardsdatascience.com/duplicate-detection-with-genai-ba2b4f7845e7?source=collection_archive---------1-----------------------#2024-07-01)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/duplicate-detection-with-genai-ba2b4f7845e7?source=collection_archive---------1-----------------------#2024-07-01](https://towardsdatascience.com/duplicate-detection-with-genai-ba2b4f7845e7?source=collection_archive---------1-----------------------#2024-07-01)
- en: How using LLMs and GenAI techniques can improve de-duplication
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何利用LLM和GenAI技术改进去重
- en: '[](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)[![Ian
    Ormesher](../Images/a5b25ae4b6242d91b9752bf9719bcb0a.png)](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)
    [Ian Ormesher](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)[![Ian
    Ormesher](../Images/a5b25ae4b6242d91b9752bf9719bcb0a.png)](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)
    [Ian Ormesher](https://medium.com/@ianormy?source=post_page---byline--ba2b4f7845e7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)
    ·5 min read·Jul 1, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ba2b4f7845e7--------------------------------)
    ·阅读时间5分钟·2024年7月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6d3ebdec98b1edebbfc275bcd00775e1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d3ebdec98b1edebbfc275bcd00775e1.png)'
- en: 2D UMAP Musicbrainz 200K nearest neighbour plot
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2D UMAP Musicbrainz 200K最近邻图
- en: Customer data is often stored as records in Customer Relations Management systems
    (CRMs). Data which is manually entered into such systems by one of more users
    over time leads to data replication, partial duplication or fuzzy duplication.
    This in turn means that there no longer a single source of truth for customers,
    contacts, accounts, etc. Downstream business processes become increasing complex
    and contrived without a unique mapping between a record in a CRM and the target
    customer. Current methods to detect and de-duplicate records use traditional Natural
    Language Processing techniques known as Entity Matching. But it is possible to
    use the latest advancements in Large Language Models and Generative AI to vastly
    improve the identification and repair of duplicated records. On common benchmark
    datasets I found an improvement in the accuracy of data de-duplication rates from
    30 percent using NLP techniques to almost 60 percent using my proposed method.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 客户数据通常作为记录存储在客户关系管理系统（CRM）中。数据由一个或多个用户随着时间的推移手动输入到这些系统中，导致数据复制、部分重复或模糊重复。这意味着对于客户、联系人、账户等，已经不再有一个单一的真实数据来源。没有唯一的CRM记录与目标客户之间的映射，下游的业务流程会变得越来越复杂和难以操作。目前用于检测和去重记录的方法使用的是传统的自然语言处理技术，称为实体匹配（Entity
    Matching）。但可以利用大型语言模型（LLM）和生成式AI的最新进展，显著改善重复记录的识别和修复。在常见的基准数据集上，我发现我的方法使数据去重准确率从传统NLP技术的30%提升到接近60%。
- en: 'I want to explain the technique here in the hope that others will find it helpful
    and use it for their own de-duplication needs. It’s useful for other scenarios
    where you wish to identify duplicate records, not just for Customer data. I also
    wrote and published a research paper about this which you can view on Arxiv, if
    you want to know more in depth:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望在这里解释这一技术，期望其他人能发现它的帮助并用它来解决他们自己的去重需求。它对其他场景也很有用，比如你希望识别重复记录，而不仅仅是客户数据。我还撰写并发布了一篇关于这个话题的研究论文，如果你想深入了解，可以在Arxiv上查看：
- en: '[](https://arxiv.org/abs/2406.15483?source=post_page-----ba2b4f7845e7--------------------------------)
    [## Duplicate Detection with GenAI'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2406.15483?source=post_page-----ba2b4f7845e7--------------------------------)
    [## 使用GenAI进行重复检测'
- en: Customer data is often stored as records in Customer Relations Management systems
    (CRMs). Data which is manually…
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户数据通常作为记录存储在客户关系管理系统（CRM）中。通过手动输入的数据…
- en: arxiv.org](https://arxiv.org/abs/2406.15483?source=post_page-----ba2b4f7845e7--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2406.15483?source=post_page-----ba2b4f7845e7--------------------------------)'
- en: Traditional Approach
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统方法
- en: 'The task of identifying duplicate records is often done by pairwise record
    comparisons and is referred to as “Entity Matching” (EM). Typical steps of this
    process would be:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 识别重复记录的任务通常通过逐对记录比较来完成，称为“实体匹配”（EM）。这一过程的典型步骤如下：
- en: Data Preparation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Candidate Generation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选生成
- en: Blocking
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻塞
- en: Matching
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配
- en: Clustering
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Data Preparation
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: Data preparation is the cleaning of the data and involves such things as removing
    non-ASCII characters, capitalisation and tokenising the text. This is an important
    and necessary step for the NLP matching algorithms later in the process which
    don’t work well with different cases or non-ASCII characters.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备是对数据进行清理，包括去除非ASCII字符、大小写转换和分词等。这是一个重要且必要的步骤，为后续的NLP匹配算法提供支持，因为这些算法处理不同的大小写或非ASCII字符时效果不好。
- en: Candidate Generation
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 候选生成
- en: In the usual EM method, we would produce candidate records by combining all
    the records in the table with themselves to produce a cartesian product. You would
    remove all combinations which are of a row with itself. For a lot of the NLP matching
    algorithms comparing row A with row B is equivalent to comparing row B with row
    A. For those cases you can get away with keeping just one of those pairs. But
    even after this, you’re still left with a lot of candidate records. In order to
    reduce this number a technique called “blocking” is often used.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在常规的EM方法中，我们会通过将表中的所有记录与自身结合，产生一个笛卡尔积。然后会去除所有与自身的组合。对于许多NLP匹配算法来说，将行A与行B进行比较等同于将行B与行A进行比较。在这种情况下，可以只保留其中一对。但是，即便如此，仍然会剩下很多候选记录。为了减少这个数量，通常会使用一种叫做“阻塞”的技术。
- en: Blocking
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阻塞
- en: The idea of blocking is to eliminate those records that we know could not be
    duplicates of each other because they have different values for the “blocked”
    column. As an example, If we were considering customer records, a potential column
    to block on could be something like “City”. This is because we know that even
    if all the other details of the record are similar enough, they cannot be the
    same customer if they’re located in different cities. Once we have generated our
    candidate records, we then use blocking to eliminate those records that have different
    values for the blocked column.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 阻塞的思想是排除那些我们知道不可能是彼此重复的记录，因为它们在“阻塞”列上具有不同的值。例如，如果我们考虑的是客户记录，那么一个潜在的阻塞列可能是“城市”。这是因为我们知道即使记录的其他所有细节足够相似，如果它们位于不同的城市，也不能是同一个客户。生成候选记录后，我们使用阻塞来排除那些在阻塞列上有不同值的记录。
- en: Matching
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 匹配
- en: Following on from blocking we now examine all the candidate records and calculate
    traditional NLP similarity-based attribute value metrics with the fields from
    the two rows. Using these metrics, we can determine if we have a potential match
    or un-match.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在阻塞之后，我们现在检查所有候选记录，并使用来自两行的字段计算传统的基于相似性的NLP属性值度量。利用这些度量，我们可以确定是否存在潜在的匹配或不匹配。
- en: Clustering
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: Now that we have a list of candidate records that match, we can then group them
    into clusters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个匹配的候选记录列表，我们可以将它们分组到不同的簇中。
- en: Proposed Method
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提议的方法
- en: 'There are several steps to the proposed method, but the most important thing
    to note is that we no longer need to perform the “Data Preparation” or “Candidate
    Generation” step of the traditional methods. The new steps become:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的方法有几个步骤，但最重要的一点是，我们不再需要执行传统方法中的“数据准备”或“候选生成”步骤。新的步骤变为：
- en: Create Match Sentences
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建匹配句子
- en: Create Embedding Vectors of those Match Sentences
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建匹配句子的嵌入向量
- en: Clustering
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Create Match Sentences
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建匹配句子
- en: 'First a “Match Sentence” is created by concatenating the attributes we are
    interested in and separating them with spaces. As an example, let’s say we have
    a customer record which looks like this:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过将我们感兴趣的属性连接起来，并用空格分隔它们，创建一个“匹配句子”。举个例子，假设我们有一个客户记录，格式如下：
- en: 'We would create a “Match Sentence” by concatenating with spaces the name1,
    name2, name3, address and city attributes which would give us the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将name1、name2、name3、地址和城市属性用空格连接来创建一个“匹配句子”，得到如下内容：
- en: “John Hartley Smith 20 Main Street London”
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “John Hartley Smith 20 Main Street London”
- en: Create Embedding Vectors
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建嵌入向量
- en: Once our “Match Sentence” has been created it is then encoded into vector space
    using our chosen embedding model. This is achieved by using “[Sentence Transformers](https://huggingface.co/sentence-transformers/)”.
    The output of this encoding will be a floating-point vector of pre-defined dimensions.
    These dimensions relate to the embedding model that is used. I used the [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2/)
    embedding model which has a vector space of 768 dimensions. This embedding vector
    is then appended to the record. This is done for all the records.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的“匹配句子”被创建，它就会通过我们选择的嵌入模型编码到向量空间中。这是通过使用“[Sentence Transformers](https://huggingface.co/sentence-transformers/)”实现的。这个编码的输出将是一个具有预定义维度的浮点向量。这些维度与所使用的嵌入模型相关。我使用了
    [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2/)
    嵌入模型，它的向量空间有 768 个维度。这个嵌入向量随后会附加到记录中。这对于所有记录都执行此操作。
- en: Clustering
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类
- en: 'Once embedding vectors have been calculated for all the records, the next step
    is to create clusters of similar records. To do this I use the DBSCAN technique.
    DBSCAN works by first selecting a random record and finding records that are close
    to it using a distance metric. There are 2 different kinds of distance metrics
    that I’ve found to work:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为所有记录计算了嵌入向量，下一步就是创建相似记录的簇。为此，我使用了 DBSCAN 技术。DBSCAN 的工作原理是首先选择一个随机记录，并使用距离度量找到与其相近的记录。我发现有
    2 种不同的距离度量方法有效：
- en: L2 Norm distance
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 范数距离
- en: Cosine Similarity
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: For each of those metrics you choose an epsilon value as a threshold value.
    All records that are within the epsilon distance and have the same value for the
    “blocked” column are then added to this cluster. Once that cluster is complete
    another random record is selected from the unvisited records and a cluster then
    created around it. This then continues until all the records have been visited.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个度量标准，你选择一个 epsilon 值作为阈值。所有在 epsilon 距离内且“blocked”列值相同的记录将被加入到这个簇中。一旦这个簇完成，另一个未访问的随机记录将被选中，并围绕它创建一个新的簇。这个过程将一直持续，直到所有记录都被访问过。
- en: Experiments and Results
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验和结果
- en: I used this approach to identify duplicate records with customer data in my
    work. It produced some very nice matches. In order to be more objective, I also
    ran some experiments using a benchmark dataset called “Musicbrainz 200K”. It produced
    some quantifiable results that were an improvement over standard NLP techniques.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我在工作中使用这种方法识别客户数据中的重复记录，产生了一些非常不错的匹配。为了更加客观，我还使用了一个名为“Musicbrainz 200K”的基准数据集进行了实验。它产生了一些可量化的结果，相比标准的
    NLP 技术有所改进。
- en: Visualising Clustering
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化聚类
- en: 'I produced a nearest neighbour cluster map for the Musicbrainz 200K dataset
    which I then rendered in 2D using the UMAP reduction algorithm:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我为 Musicbrainz 200K 数据集生成了最近邻簇图，并使用 UMAP 降维算法将其渲染为 2D 图：
- en: '![](../Images/6d3ebdec98b1edebbfc275bcd00775e1.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d3ebdec98b1edebbfc275bcd00775e1.png)'
- en: 2D UMAP Musicbrainz 200K nearest neighbour plot
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2D UMAP Musicbrainz 200K 最近邻图
- en: Resources
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: 'I have created various notebooks that will help with trying the method out
    for yourselves:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了各种笔记本，帮助你们自己尝试这个方法：
- en: '[](https://github.com/ianormy/genai_duplicate_detection_paper?source=post_page-----ba2b4f7845e7--------------------------------)
    [## GitHub - ianormy/genai_duplicate_detection_paper: Resources and notebooks
    to accompany the…'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/ianormy/genai_duplicate_detection_paper?source=post_page-----ba2b4f7845e7--------------------------------)
    [## GitHub - ianormy/genai_duplicate_detection_paper: 伴随论文的资源和笔记本…'
- en: Resources and notebooks to accompany the Duplicate Detection using GenAI paper
    …
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 伴随《GenAI 重复检测》论文的资源和笔记本…
- en: github.com](https://github.com/ianormy/genai_duplicate_detection_paper?source=post_page-----ba2b4f7845e7--------------------------------)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/ianormy/genai_duplicate_detection_paper?source=post_page-----ba2b4f7845e7--------------------------------)'
- en: 'Duplicate Detection with GenAI paper: [[2406.15483] Duplicate Detection with
    GenAI](https://arxiv.org/abs/2406.15483)'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'GenAI 重复检测论文: [[2406.15483] Duplicate Detection with GenAI](https://arxiv.org/abs/2406.15483)'
- en: 'GitHub resources: [https://github.com/ianormy/genai_duplicate_detection_paper](https://github.com/ianormy/genai_duplicate_detection_paper)'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'GitHub 资源: [https://github.com/ianormy/genai_duplicate_detection_paper](https://github.com/ianormy/genai_duplicate_detection_paper)'
- en: 'all-mpnet-base-v2 embedding model: [https://huggingface.co/sentence-transformers/all-mpnet-base-v2/](https://huggingface.co/sentence-transformers/all-mpnet-base-v2/)'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'all-mpnet-base-v2 嵌入模型: [https://huggingface.co/sentence-transformers/all-mpnet-base-v2/](https://huggingface.co/sentence-transformers/all-mpnet-base-v2/)'
- en: 'Sentence Transformers: [https://huggingface.co/sentence-transformers/](https://huggingface.co/sentence-transformers/)'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 句子转换器：[https://huggingface.co/sentence-transformers/](https://huggingface.co/sentence-transformers/)
- en: 'UMAP python package: [https://pypi.org/project/umap-learn/](https://pypi.org/project/umap-learn/)'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: UMAP Python 包：[https://pypi.org/project/umap-learn/](https://pypi.org/project/umap-learn/)
- en: 'Benchmark datasets for entity resolution: [https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution/](https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution/)'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实体解析基准数据集：[https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution/](https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution/)
- en: 'Musicbrainz 200K dataset: [https://dbs.uni-leipzig.de/files/datasets/saeedi/musicbrainz-200-A01.csv.dapo](https://dbs.uni-leipzig.de/files/datasets/saeedi/musicbrainz-200-A01.csv.dapo)'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Musicbrainz 200K 数据集：[https://dbs.uni-leipzig.de/files/datasets/saeedi/musicbrainz-200-A01.csv.dapo](https://dbs.uni-leipzig.de/files/datasets/saeedi/musicbrainz-200-A01.csv.dapo)
