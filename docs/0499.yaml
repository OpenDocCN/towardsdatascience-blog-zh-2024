- en: A Visual Guide to Mamba and State Space Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mamba和状态空间模型的视觉指南
- en: 原文：[https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22](https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22](https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22)
- en: An alternative to Transformers for language modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言建模的Transformer替代方案
- en: '[](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    ·21 min read·Feb 22, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    ·阅读时长：21分钟·2024年2月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The Transformer architecture has been a major component in the success of Large
    Language Models (LLMs). It has been used for nearly all LLMs that are being used
    today, from open-source models like Mistral to closed-source models like ChatGPT.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构是大型语言模型（LLM）成功的重要组成部分。它已被几乎所有当前使用的LLM所采用，从像Mistral这样的开源模型到像ChatGPT这样的封闭源代码模型。
- en: To further improve LLMs, new architectures are developed that might even outperform
    the Transformer architecture. One of these methods is *Mamba*, a *State Space
    Model*.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提升LLM的性能，开发了新的架构，这些架构甚至可能超过Transformer架构。其中一种方法是*Mamba*，一种*状态空间模型*。
- en: '![](../Images/75c4b3b7479228a2c579c2ce79a120a9.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c4b3b7479228a2c579c2ce79a120a9.png)'
- en: The basic architecture of a State Space Model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间模型的基本架构。
- en: 'Mamba was proposed in the paper [Mamba: Linear-Time Sequence Modeling with
    Selective State Spaces](https://arxiv.org/abs/2312.00752). You can find its official
    implementation and model checkpoints in its [repository](https://github.com/state-spaces/mamba).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mamba是在论文[Mamba: 线性时间序列建模与选择性状态空间](https://arxiv.org/abs/2312.00752)中提出的。你可以在它的[代码库](https://github.com/state-spaces/mamba)中找到其官方实现和模型检查点。'
- en: In this post, I will introduce the field of State Space Models in the context
    of language modeling and explore concepts one by one to develop an intuition about
    the field. Then, we will cover how Mamba might challenge the Transformers architecture.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将介绍状态空间模型在语言建模中的应用，并逐步探索相关概念，以帮助你形成对该领域的直观理解。然后，我们将讨论Mamba如何挑战Transformer架构。
- en: As a visual guide, expect many visualizations to develop an intuition about
    Mamba and State Space Models!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为视觉指南，预计会有许多可视化内容，帮助你形成对Mamba和状态空间模型的直观理解！
- en: 'Part 1: The Problem with Transformers'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1部分：Transformer的问题
- en: To illustrate why Mamba is such an interesting architecture, let’s do a short
    re-cap of transformers first and explore one of its…
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明Mamba为何是一种如此有趣的架构，让我们首先简要回顾一下Transformer，并探索其中的一个…
