- en: A Visual Guide to Mamba and State Space Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22](https://towardsdatascience.com/a-visual-guide-to-mamba-and-state-space-models-8d0d3f7d3ea6?source=collection_archive---------4-----------------------#2024-02-22)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An alternative to Transformers for language modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page---byline--8d0d3f7d3ea6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8d0d3f7d3ea6--------------------------------)
    ·21 min read·Feb 22, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer architecture has been a major component in the success of Large
    Language Models (LLMs). It has been used for nearly all LLMs that are being used
    today, from open-source models like Mistral to closed-source models like ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve LLMs, new architectures are developed that might even outperform
    the Transformer architecture. One of these methods is *Mamba*, a *State Space
    Model*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75c4b3b7479228a2c579c2ce79a120a9.png)'
  prefs: []
  type: TYPE_IMG
- en: The basic architecture of a State Space Model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mamba was proposed in the paper [Mamba: Linear-Time Sequence Modeling with
    Selective State Spaces](https://arxiv.org/abs/2312.00752). You can find its official
    implementation and model checkpoints in its [repository](https://github.com/state-spaces/mamba).'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, I will introduce the field of State Space Models in the context
    of language modeling and explore concepts one by one to develop an intuition about
    the field. Then, we will cover how Mamba might challenge the Transformers architecture.
  prefs: []
  type: TYPE_NORMAL
- en: As a visual guide, expect many visualizations to develop an intuition about
    Mamba and State Space Models!
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: The Problem with Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To illustrate why Mamba is such an interesting architecture, let’s do a short
    re-cap of transformers first and explore one of its…
  prefs: []
  type: TYPE_NORMAL
