# 知识增强型代理在互动文本游戏中的应用

> 原文：[https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10](https://towardsdatascience.com/knowledge-enhanced-agents-for-interactive-text-games-359e57da5de3?source=collection_archive---------17-----------------------#2024-01-10)

## 用知识增强型AI代理革命化互动文本游戏

[](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[![Prateek Chhikara](../Images/4cabb40cbab34038c0f762b45d58bbba.png)](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------) [Prateek Chhikara](https://medium.com/@prateekchhikara?source=post_page---byline--359e57da5de3--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--359e57da5de3--------------------------------) ·阅读时长12分钟·2024年1月10日

--

# **简介：**

通过自然语言进行交流对机器智能至关重要[9]。计算语言模型（LM）的最新进展使得在有限交互的任务上，如问答和程序化文本理解，取得了显著的性能[10]。认识到互动性是交流的一个关键方面，研究界将目光投向了在互动小说（IF）环境中训练和评估代理，比如基于文本的游戏，这为研究语言模型的推理能力以及人工智能（AI）代理在受限环境中执行多步骤现实任务的潜力提供了独特的测试平台。例如，在图1中，代理需要在客厅采摘水果，并将其放入厨房的蓝色盒子中。在这些游戏中，代理通过基于文本的输入在复杂环境中进行导航，这要求AI代理对自然语言和战略决策做出深刻理解。要在这些游戏中取得成功，代理必须管理其知识、推理并生成基于语言的行动，从而在游戏世界中产生预期且可预测的变化。

![](../Images/719f0bb5fb40ca659de1eea76ddf22ac.png)

图1. 互动小说（IF）游戏的示意图，其中一个代理需要完成采摘水果（例如苹果）并将其放入厨房的蓝色盒子中的任务。

# 背景与动机：

先前的研究表明，基于强化学习和语言模型的代理在 IF 环境中推理或解释科学概念时存在困难[1]，这引发了关于这些模型是否能够在训练过程中观察到的情境之外的未知情境中进行泛化的问题[2]。例如，虽然像‘*获取已知物质的熔点（或沸点）*’这样的任务可能相对简单，但‘*在特定环境中确定未知物质的熔点（或沸点）*’对这些模型来说却可能具有挑战性。为了提高泛化能力，结合世界知识（例如，**关于物体的可用性知识**）可能是有效的，但迄今为止没有相关研究探索这一方向。此外，现有模型在从环境反馈中有效学习方面仍然存在困难。例如，在检查特定物质的导电性时，代理必须理解它已经获得了必要的电线和特定物质，才能继续寻找电源。因此，亟需一个框架，能够分析和评估不同类型的知识及其注入方法对基于文本的游戏代理的有效性。

我们的论文《**增强知识的交互式文本游戏代理**》提出了一种新颖的框架，旨在提升 AI 代理在这些 IF 环境中的表现。

**已发布版本：** [https://dl.acm.org/doi/10.1145/3587259.3627561](https://dl.acm.org/doi/10.1145/3587259.3627561)

> 我们很高兴地宣布，我们的论文在 KCAP 2023 会议上获得了**最佳学生论文**奖，这是我们团队创新研究和奉献精神的体现。🏆🏆🏆

# 核心创新——知识注入框架：

我们的工作提出了一个独特的框架，旨在增强 AI 代理的特定知识。该框架包含两个关键组件：

1.  **正确行为的记忆 (MCA):** 该特性使得 AI 代理能够记住并利用过去的正确行为。通过保持对过去有效行为的记忆，代理可以制定更有效的策略，避免重复犯错。MCA 由环境反馈决定。如果一个行为获得了奖励，那么它就被认为是正确的。因此，正确的行为不能一开始就直接提供给代理，而是随着代理在（训练/测试时间）回合的进行，逐步存储到记忆中。

1.  **可用性知识 (Aff):** 理解游戏世界中与物体的潜在互动至关重要。我们期望可用性能够通过列出与周围物体的可能互动，帮助模型更好地学习。与历史知识不同，环境并不提供这些可用性信息，而是需要从外部资源中获取。为此，我们使用 ConceptNet，并获取其关于给定 IF 游戏回合中物体的 *capableOf* 和 *usedFor* 关系。

我们在两种 AI 代理架构中实现了该框架：

1.  通过奖励的在线策略优化 (RL 方法)

1.  单步离线预测（LM方法）

# **1. 在线策略优化通过奖励（RL方法）**

**纯RL基础模型 — *DRRN* [3](图2)**

基线DRRN模型仅使用观察、库存和任务描述的输入来计算每个动作的Q值。为了增强DRRN基线，我们将外部知识注入到模型中，并创建了DRRN的三种新变体：

1.  **aff：** 通过使用一个独特的GRU编码层，我们将输入中的物体可用性引入到基线模型中。

1.  **mca：** 在该模型中，使用独立的GRU编码层将所有先前正确的动作传递给基线模型。

1.  **aff ⊕ mca：** 该架构的编码由代理的先前正确动作和物体可用性作为独立组件组成。

![](../Images/53858d7b29b4a7115bd197507563c2d7.png)

图2：DRRN架构，增强了先前正确动作和物体可用性的记忆。

**RL增强型KG模型 — *KG-A2C* [4] (图3)**

作为基线，我们使用KG-A2C的修改版，在该版本中，我们利用环境提供的单一黄金动作序列作为目标，尽管可能存在多个可能的黄金序列。我们发现这个目标的表现优于原始的预测有效动作目标。我们设计了以下知识注入策略来整合

KG-A2C的正确动作和物体可用性知识记忆：

1.  **mca：** 在基线的基础上，我们将所有先前正确的

    动作上，通过使用一个独立的GRU编码层并将其与

    输出向量与其他输出表示一起。

1.  **aff：** KG-A2C模型中的KG组件为我们提供了一个方便的方式来添加更多知识。特别是，我们直接将物体可用性知识作为附加三元组添加到KG中，位于

    基线模型。例如，给定KG中的现有关系

    （客厅，hasA，苹果）我们可以添加物体可用性关系：（苹果，

    使用了For，eating）。通过这种方式，KG编码网络可以生成

    游戏状态的更有意义的表示，并可能

    引导模型生成更好的动作。在我们的实验中，我们

    将这种方法与使用

    独立的GRU编码层，类似于DRRN的情况。

1.  **aff ⊕ mca：** 我们将KG中的物体可用性和所有

    先前的修正动作使用独立的GRU编码层。

![](../Images/422c7e31d5898a8c9fab79747319656e.png)

图3：集成了物体可用性和先前正确动作的KG-A2C模型架构。

# **2. 单步离线预测（LM方法）**

**预训练语言模型 — *RoBERTa* [5](图4)**

在这里，我们将任务视为多选题问答。在每一步，当前的游戏状态被视为问题，模型必须从候选动作集合中预测下一个动作。类似于强化学习（RL）智能体，模型在每一步都会接收到环境观察（𝑜𝑏𝑣）、库存（𝑖𝑛𝑣）和任务描述（𝑑𝑒𝑠𝑐）。然后，我们将其与每个动作拼接在一起，让语言模型选择得分最高的动作。由于可能的动作集合非常庞大，在训练过程中我们仅随机选择𝑛=4个干扰动作，以减少计算负担，语言模型通过交叉熵损失进行训练，选择正确的动作。在推理时，模型会为所有有效动作分配分数，我们使用top-p采样来选择动作，以防止模型陷入动作循环。我们为基准RoBERTa模型提出了三种知识注入策略。

1.  **mca:** 在这里，我们通过将过去的正确动作列出为一个字符串并附加到原始输入中，使得语言模型（LM）能够意识到自己的过去正确动作。由于RoBERTa的token限制，我们使用一个大小为𝐴=5的滑动窗口，即在每一步，模型最多只能看到过去的

    𝐴个正确动作。

1.  **aff:** 我们通过首先在一个包含对象效用的常识知识图子集上对语言模型进行适应，向模型注入效用知识。我们通过一个辅助的问答任务来对模型进行适应，遵循之前的知识注入工作[6]。由于效用知识三元组的数量庞大，无法简单地将其拼接到RoBERTa的输入中，因此我们使用预训练而非简单拼接输入。通过辅助问答任务对效用进行预训练可以缓解这一挑战，同时仍能使模型学习到相关知识。接着，我们在增强效用的模型基础上对我们的任务模型进行微调，如基准中所述。

1.  **aff ⊕ mca:** 这种变体仅仅是将mca和aff结合在一起。

![](../Images/0e09e421f92a4a224d8201706e82d999.png)

图4：使用干扰项训练的RoBERTa架构。

**指令调优语言模型 — *Flan T5* [7][8]（图5）**

Swift 模型本身集成了前十个动作的历史背景。值得注意的是，与之前检查过的三个仅考虑最后十个正确动作历史的模型不同，Swift 模型遵循其原始设计，涵盖了前十个动作的完整历史。为了建立一个可与之前三种架构中应用的方法相比较的基准模型，我们从 Swift 模型中省略了动作历史。未做更改的 Swift 变体在这里被表示为**mca**版本。此外，将可供性融入基准模型中，形成了**aff model**。类似地，将可供性融入 mca 版本后，形成了**aff ⊕ mca**模型。这些可供性被引入主输入序列，紧随库存数据之后，并位于已访问房间信息之前。

![](../Images/e50387af9e741dd9de9c7b95f6da8238.png)

图 5：以 Seq2Seq 方式训练的 Swift 架构。

# 实验设置

**环境：** 我们使用了 ScienceWorld [1]，这是一个基于文本的复杂虚拟世界，呈现为英文。它拥有 10 个互联的地点，并包含 218 个独特的物品，包括各种仪器、电气组件、植物、动物以及家具和书籍等日常物品。游戏提供了丰富的互动，具有 25 种高级动作和每步最多 200,000 种可能的组合，尽管其中只有少数是实际有效的。ScienceWorld 包含 10 个任务，共有 30 个子任务。由于 ScienceWorld 的多样性，每个任务作为一个独立的基准，具有不同的推理能力、知识要求，并且需要完成目标状态的动作数不同。此外，每个子任务都有一组必须完成的目标（例如，专注于一个非生物物体并将其放入厨房的红色盒子中）。为了实验目的，我们从每个任务中选择了一个代表性的子任务。任务细节见附录（文章末尾）。

**奖励与评分系统：** ScienceWorld 的奖励系统旨在引导智能体朝着优选的解决方案前进。每个动作执行后，环境都会提供一个数值评分和一个布尔指示器，显示任务是否完成。智能体每个回合最多可以执行 100 步（动作）。最终得分介于 0 到 100 之间，反映了智能体在完成回合目标和子目标方面的表现。当智能体完成任务或达到 100 步限制时，回合结束，并计算累计得分。

# 实验见解：

+   **知识注入有助于**文本游戏中的智能体——在 40 个案例中的 34 个中，我们的知识注入策略优于基准模型。

+   **可用性知识比正确行动的记忆更有益**——可用性模型在15个案例中获得了最佳结果，其次是包括MCA（8个案例）。将这两种知识类型结合在一起在11个案例中取得了最佳结果。

+   就任务的整体影响而言，语言模型变体RoBERTa和Swift从包含可用性知识中获益最大，分别相对于基线提高了48%和8%。图6中展示了一个例子，说明了语言模型在加入可用性知识后受益匪浅。

![](../Images/2754c5fc9218edca5e22c695d8c1c3df.png)

图6：可用性模型在任务4中采取的行动。蓝色=步骤索引，绿色=累积得分，黄色=正确的行动。

+   **在任务中的变动效果**取决于注入知识与任务的相关性——任务中的变动效果通常是由于注入知识与当前任务的相关性，某些任务（例如电力）从知识注入中受益更多。

+   **通过知识图谱注入可用性是最有效的**；将其作为原始输入加入会增加模型的学习复杂度——我们探索了将可用性知识注入到KG-A2C的多种变体（图7）：通过将其作为输入添加到观察、库存和描述中，为可用性创建一个单独的GRU编码层，并将可用性添加到知识图谱本身。我们在三个子任务上评估了每种方法的表现：简单、中等和困难。

![](../Images/17d48932521a83afbb150d9176bf0b83.png)

图7：在KG-A2C中添加可用性的五种方法的效果。

# 结论性思考：

我们的研究代表了朝着更复杂的AI智能体迈出的重要一步。通过赋予它们从过去的行动中学习并深刻理解环境的能力，我们为AI铺平了道路，使其能够在各种生活场景中智能、直观地进行游戏和互动。该框架可以扩展到其他AI应用，如虚拟助手或教育工具，在这些应用中，理解和与环境互动至关重要。

大型语言模型的少样本提示最近在推理任务中显示出了潜力，同时互动交流和输入澄清也带来了明显的好处。探索它们在互动任务中的作用，无论是作为需要较少训练数据的解决方案，还是作为能够为知识蒸馏生成合成数据的组件，都是一个有前景的未来方向。

如果你喜欢我们的工作，请引用它 😁

```py
@inproceedings{chhikara,
author = {Chhikara, Prateek and Zhang, Jiarui and Ilievski, Filip and Francis, Jonathan and Ma, Kaixin},
title = {Knowledge-Enhanced Agents for Interactive Text Games},
year = {2023},
doi = {10.1145/3587259.3627561},
booktitle = {Proceedings of the 12th Knowledge Capture Conference 2023},
pages = {157–165},
numpages = {9},
series = {K-CAP '23}
}
```

# 参考文献

[1] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, 和 Prithviraj Ammanabrolu. 2022\. ScienceWorld: 你的智能体比五年级学生更聪明吗？EMNLP（2022）。

[2] Peter Jansen, Kelly J. Smith, Dan Moreno, 和 Huitzilin Ortiz. 2021\. 在多跳推理中评估组合性解释的挑战：相关性、完整性和专家评级。EMNLP会议论文集。

[3] 何吉, 陈建书, 何晓东, 高剑锋, 李丽红, 邓力, 和 Mari Ostendorf. 2016\. 基于自然语言行动空间的深度强化学习. 载《ACL会议论文集》.

[4] Prithviraj Ammanabrolu 和 Matthew Hausknecht. 2020\. 图约束强化学习在自然语言行动空间中的应用. 载《ICLR》.

[5] 刘寅涵, Myle Ott, Naman Goyal, 丁菲杜, Mandar Joshi, 陈丹琪, Omer Levy, Mike Lewis, Luke Zettlemoyer, 和 Veselin Stoyanov. 2019\. Roberta: 一种稳健优化的BERT预训练方法. (2019).

[6] Filip Ilievski, Alessandro Oltramari, 马凯欣, 张斌, Deborah L McGuinness, 和 Pedro Szekely. 2021\. 常识知识的维度. 《基于知识的系统》229 (2021), 107347.

[7] 洪亨元, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, 王学智, Mostafa Dehghani, Siddhartha Brahma 等. 2022\. 扩展指令微调语言模型.

[8] 林育辰, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, 黄诗雨, Chandra Bhagavatula, 崔烨金, 和 任翔. 2023\. SwiftSage: 一个具有快速与慢速思维的生成性智能体，适用于复杂的交互任务。

[9] 诺姆·乔姆斯基 2014\. 句法理论的若干方面. 第11卷. MIT出版社.

[10] 江一凡, Filip Ilievski 和 马凯欣. 2023\. 跨常识任务传递程序性知识. 载《ECAI》.

# 附录

**任务描述**

1.  **任务 1 — 物质：** 你的任务是将水冻结。首先，聚焦在该物质上。接着，采取行动使其发生状态变化。

    物质。

1.  **任务 2 — 测量：** 你的任务是测量巧克力的熔点，巧克力位于厨房的某个地方。首先，聚焦在温度计上。接着，聚焦在巧克力上。如果巧克力的熔点高于 -10.0 摄氏度，聚焦在蓝色盒子上。如果巧克力的熔点低于 -10.0 摄氏度，聚焦在橙色盒子上。这些盒子位于厨房的各个地方。

1.  **任务 3 — 电学：** 你的任务是通过可再生能源电源打开红色灯泡。首先，聚焦在红色灯泡上。接着，创建一个电路将其通电。

1.  **任务 4 — 分类：** 你的任务是找到一个非生物物体。首先，聚焦在这个物体上。然后，把它移到厨房的红色盒子里。

1.  **任务 5 — 生物学 I：** 你的任务是从种子中种植一个苹果植物。种子可以在厨房找到。首先，聚焦在一个种子上。然后，改变环境以促进植物生长，直到它达到繁殖生命周期阶段。

1.  **任务 6 — 化学：** 你的任务是利用化学方法制造物质 *‘盐水’*。食谱和一些原料可能在厨房附近找到。完成后，聚焦在盐水上。

1.  **任务 7 — 生物学 II：** 你的任务是找到寿命最长的动物，然后是寿命最短的动物。首先，专注于寿命最长的动物。然后，专注于寿命最短的动物。这些动物位于“外部”位置。

1.  **任务 8 — 生物学 III：** 你的任务是专注于海龟的四个生命周期阶段，从最早到最晚。

1.  **任务 9 — 力学：** 你的任务是确定两个倾斜面（未知材料C，未知材料H）中哪一个具有最大的摩擦力。

    摩擦力。完成实验后，专注于摩擦力最大的倾斜面。

1.  **任务 10 — 生物学 IV：** 你的任务是确定蓝色种子颜色在未知E植物中的遗传方式，是显性还是隐性特征。如果该特征是显性的，专注于红色盒子。如果该特征是隐性的，专注于绿色盒子。

**ScienceWorld 游戏示例**

**任务：** 4（找到一个非生物物体）

**变化：** 239（DRRN基准）

**描述：** 你的任务是找到一个非生物物体。首先，专注于该物体。然后，将它移到工作坊中的紫色盒子里。

![](../Images/b5bcbd114d4b75cd171d0e3e417e0034.png)
