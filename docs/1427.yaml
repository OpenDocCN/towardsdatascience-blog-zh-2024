- en: 'Principal Component Analysis Made Easy: A Step-by-Step Tutorial'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08](https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Implement the PCA algorithm from scratch with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    ·9 min read·Jun 8, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2af52467e31c0771ef30de908239e82e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Plenty of well-established Python packages (like scikit-learn) implement Machine
    Learning algorithms such as the Principal Component Analysis (PCA) algorithm.
    So, why bother learning how the algorithms work under the hood?
  prefs: []
  type: TYPE_NORMAL
- en: A deep understanding of the underlying mathematical concepts is crucial for
    making better decisions based on the algorithm’s output and avoiding treating
    the algorithm as a “black box”.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I show the intuition of the inner workings of the PCA algorithm,
    covering key concepts such as *Dimensionality Reduction*, *eigenvectors*, and
    *eigenvalues*, then we’ll implement a Python class to encapsulate these concepts
    and perform PCA analysis on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are a machine learning beginner trying to build a solid understanding
    of the concepts or a practitioner interested in creating custom machine learning
    applications and need to understand how the algorithms work under the hood, that
    article is for you.
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Dimensionality Reduction](#ada8)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[2\. How Does Principal Component Analysis Work?](#607e)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[3\. Implementation in Python](#f118)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[4\. Evaluation and Interpretation](#77b2)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[5\. Conclusions and Next Steps](#3fda)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Dimensionality Reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many real problems in machine learning involve datasets with thousands or even
    millions of features. Training such datasets can be computationally demanding,
    and interpreting the resulting solutions can be even more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: As the number of features increases, the data points become more sparse, and
    distance metrics become less informative, since the distances between points are
    less pronounced making it difficult to distinguish what are close and distant
    points. That is known as the *curse of dimensionality*.
  prefs: []
  type: TYPE_NORMAL
- en: The more sparse data makes models harder to train and more prone to overfitting
    capturing noise rather than the underlying patterns. This leads to poor generalization
    to new, unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction is used in data science and machine learning to reduce
    the number of variables or features in a dataset while retaining as much of the
    original information as possible. This technique is useful for simplifying complex
    datasets, improving computational efficiency, and helping with data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a55aff221424d9a010c30fecaf37ecfe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author using DALL-E.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. How Does Principal Component Analysis Work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most used techniques to mitigate the curse of dimensionality is Principal
    Component Analysis (PCA). The PCA reduces the number of features in a dataset
    while keeping most of the useful information by finding the axes that account
    for the largest variance in the dataset. Those axes are called the *principal
    components*.
  prefs: []
  type: TYPE_NORMAL
- en: Since PCA aims to find a low-dimensional representation of a dataset while keeping
    a great share of the variance instead of performing predictions, It is considered
    an **unsupervised learning** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: But why does keeping the variance mean keeping important information?
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are analyzing a dataset about crimes in a city. The data have numerous
    features including "crime against person - with injuries" and “crime against person
    — without injuries”. Certainly, the places with high rates of the first example
    must also have high rates of the second example.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, the two features of the example are very correlated, so it is
    possible to reduce the dimensions of that dataset by diminishing the redundancies
    in the data (the presence or absence of injuries in the victim).
  prefs: []
  type: TYPE_NORMAL
- en: The PCA algorithm is nothing more than a sophisticated way of doing that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s break down how the PCA algorithm works under the hood in the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1: Centering the data**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is affected by the scale of the data, so the first thing to do is to subtract
    the mean of each feature of the dataset, thus ensuring that all the features have
    a mean equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03ebbfb66492626ae7477637a2b7ddce.png)'
  prefs: []
  type: TYPE_IMG
- en: Data before and after centering (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Calculating the covariance matrix**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we have to calculate the covariance matrix to capture how each pair of
    features in the data varies together. If the dataset has *n* features, the resulting
    covariance matrix will have *n* x *n* shape.
  prefs: []
  type: TYPE_NORMAL
- en: In the image below, features more correlated have colors closer to red. Of course,
    each feature will be highly correlated with itself.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe5fe850cd277fae728a68121c86df55.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmap of the covariance matrix (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Eigenvalue Decomposition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we have to perform the eigenvalue decomposition of the covariance matrix.
    In case you don''t remember, given the covariance matrix Σ (a square matrix),
    eigenvalue decomposition is the process of finding a set of scalars (eigenvalues)
    and vectors (eigenvectors) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/643ea25ef8e21f3abac05bc9a3bf2719.png)'
  prefs: []
  type: TYPE_IMG
- en: Eigenvalue property (image by the author using [codecogs](https://editor.codecogs.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: Σ is the n×n covariance matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*v* is a non-zero vector called the eigenvector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*λ* is a scalar called the eigenvalue associated with the eigenvector *v*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Eigenvectors** indicate the directions of maximum variance in the data (the
    principal components), while **eigenvalues** quantify the variance captured by
    each principal component.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If a matrix **A** can be decomposed into eigenvalues and eigenvectors, it can
    be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5a67b6aea9048c7daf0a0fa1d0f57d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Eigendecomposition of a matrix (image by the author using [codecogs](https://editor.codecogs.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q** is a matrix whose columns are the eigenvectors of **A**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Λ is a diagonal matrix whose diagonal elements are the eigenvalues of **A**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That way, we can use the same steps to find the eigenvalues and eigenvectors
    of the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f6d776527b32054686c907ec2ae7832.png)'
  prefs: []
  type: TYPE_IMG
- en: Plotting eigenvectors (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: In the image above, we can see that the first eigenvector points to the direction
    with the most variance of the data, and the second eigenvector points to the direction
    with the second most variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Selecting the Principal Components'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As said earlier, the eigenvalues quantify the data's variance in the direction
    of its corresponding eigenvector. Thus, we sort the eigenvalues in descending
    order and keep only the top n required *principal components*.
  prefs: []
  type: TYPE_NORMAL
- en: The image below illustrates the proportion of variance captured by each principal
    component in a PCA with two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff80b7c8901b71e9f7f0e8f52fe46c0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Explained variance for 2 principal components (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Project the Data'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we have to project the original data onto the dimensions represented
    by the selected principal components. To do that, we have to multiply the dataset,
    after being centered, by the matrix of eigenvectors found in the decomposition
    of the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b56cd9f76d4c4101644a6e56a0cffcc7.png)'
  prefs: []
  type: TYPE_IMG
- en: Projecting the original dataset to n dimensions (image by the author using [codecogs](https://editor.codecogs.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we deeply understand the key concepts of Principal Component Analysis,
    it's time to create some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to set the environment importing the numpy package for mathematical
    calculations and matplotlib for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will encapsulate all the concepts covered in the previous section
    in a Python class with the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**init method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Constructor method to initialize the algorithm''s parameters: the number of
    components desired, a matrix to store the components vectors, and an array to
    store the explained variance of each selected dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: '**fit method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the fit method, the first four steps presented in the previous section are
    implemented with code. Also, the explained variances of each component are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Transform method**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The transform method performs the last step presented in the previous section:
    project the data onto the selected dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Plot explained variance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last method is a helper function to plot the explained variance of each
    selected principal component as a bar plot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Evaluation and interpretation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now it's time to use the class we just implemented on a simulated dataset created
    using the numpy package. The dataset has 10 features and 100 samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Before performing the PCA, one question remains: **how do we choose the correct
    or optimal number of dimensions**? Generally, we have to look for the number of
    components that add up to at least 95% of the explained variance of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, let''s take a look at how each principal component contributes
    to the total variance of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's plot the cumulative sum of the variance and check at which number
    of dimensions we achieve the optimal value of 95% of the total variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8de65b184309833b30aaaa75eac9fde5.png)'
  prefs: []
  type: TYPE_IMG
- en: Explained variance as a function of the number of components (image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the graph above, the optimal number of dimensions for the dataset
    is 4, totaling 97.064% of the explained variance. In other words, we transformed
    a dataset with 10 features into one with only 3 dimensions while keeping more
    than 97% of the original information.
  prefs: []
  type: TYPE_NORMAL
- en: That means that most of the original 10 features were very correlated and the
    algorithm transformed that high-dimensional data into uncorrelated principal components.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5\. Conclusions and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created a PCA class using only the numpy package that successfully reduced
    the dimensionality of a dataset from 10 features to just 4 while preserving approximately
    97% of the data’s variance.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we explored a method to obtain an optimal number of principal components
    of the PCA analysis that can be customized depending on the problem we are facing
    (we may be interested in retaining only 90% of the variance, for example).
  prefs: []
  type: TYPE_NORMAL
- en: 'That shows the potential of the PCA analysis to deal with the curse of dimensionality
    explained earlier. Additionally, I''d like to leave a few points for further exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform classification or regression tasks using other machine learning algorithms
    on the reduced dataset using the PCA algorithm and compare the performance of
    models trained on the original dataset versus the PCA-transformed dataset to assess
    the impact of dimensionality reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use PCA for data visualization to make high-dimensional data more interpretable
    and to uncover patterns that were not evident in the original feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider exploring other dimensionality reduction techniques, such as *t-Distributed
    Stochastic Neighbor Embedding* (t-SNE) and *Linear Discriminant Analysis* (LDA).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete code available [here](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
    [## ML-and-Ai-from-scratch/PCA at main · Marcussena/ML-and-Ai-from-scratch'
  prefs: []
  type: TYPE_NORMAL
- en: Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/PCA
    at main ·…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Please feel free to use and improve the code, comment, make suggestions, and
    connect with me on [LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/),
    [X](https://twitter.com/MarcusMVLS), and [Github](https://github.com/Marcussena/ML-and-Ai-from-scratch).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Willmott, Paul. (2019). *Machine Learning: An Applied Mathematics Introduction*.
    Panda Ohana Publishing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Géron, A. (2017). *Hands-On Machine Learning*. O’Reilly Media Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Grus, Joel. (2015). *Data Science from Scratch*. O’Reilly Media Inc.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Datagy.io. *How to Perform PCA in Python*. Retrieved June 2, 2024, from
    [https://datagy.io/python-pca/](https://datagy.io/python-pca/).'
  prefs: []
  type: TYPE_NORMAL
