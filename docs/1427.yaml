- en: 'Principal Component Analysis Made Easy: A Step-by-Step Tutorial'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析简明教程：逐步指南
- en: 原文：[https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08](https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08](https://towardsdatascience.com/principal-component-analysis-made-easy-a-step-by-step-tutorial-184f295e97fe?source=collection_archive---------1-----------------------#2024-06-08)
- en: Implement the PCA algorithm from scratch with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头实现PCA算法，使用Python
- en: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Marcus
    Sena](../Images/ff594ec7029e6259f0be6dc031d8a6cd.png)](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    [Marcus Sena](https://marcusmvls-vinicius.medium.com/?source=post_page---byline--184f295e97fe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    ·9 min read·Jun 8, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--184f295e97fe--------------------------------)
    ·阅读时间9分钟·2024年6月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2af52467e31c0771ef30de908239e82e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2af52467e31c0771ef30de908239e82e.png)'
- en: Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Plenty of well-established Python packages (like scikit-learn) implement Machine
    Learning algorithms such as the Principal Component Analysis (PCA) algorithm.
    So, why bother learning how the algorithms work under the hood?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 很多成熟的Python包（如scikit-learn）实现了机器学习算法，如主成分分析（PCA）算法。那么，为什么还要费劲去学习算法的工作原理呢？
- en: A deep understanding of the underlying mathematical concepts is crucial for
    making better decisions based on the algorithm’s output and avoiding treating
    the algorithm as a “black box”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深入理解背后的数学概念对于基于算法输出做出更好的决策并避免将算法视为“黑盒”至关重要。
- en: In this article, I show the intuition of the inner workings of the PCA algorithm,
    covering key concepts such as *Dimensionality Reduction*, *eigenvectors*, and
    *eigenvalues*, then we’ll implement a Python class to encapsulate these concepts
    and perform PCA analysis on a dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示PCA算法内部工作原理的直观理解，涵盖诸如*降维*、*特征向量*和*特征值*等关键概念，之后我们将实现一个Python类来封装这些概念并在数据集上执行PCA分析。
- en: Whether you are a machine learning beginner trying to build a solid understanding
    of the concepts or a practitioner interested in creating custom machine learning
    applications and need to understand how the algorithms work under the hood, that
    article is for you.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是一个试图建立扎实概念理解的机器学习初学者，还是一个有兴趣创建自定义机器学习应用并需要了解算法如何在幕后工作的从业者，这篇文章都适合你。
- en: Table of Contents
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 目录
- en: ''
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1\. Dimensionality Reduction](#ada8)'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[1\. 降维](#ada8)'
- en: '[2\. How Does Principal Component Analysis Work?](#607e)'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[2\. 主成分分析是如何工作的？](#607e)'
- en: '[3\. Implementation in Python](#f118)'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[3\. Python中的实现](#f118)'
- en: '[4\. Evaluation and Interpretation](#77b2)'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[4\. 评估与解释](#77b2)'
- en: '[5\. Conclusions and Next Steps](#3fda)'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[5\. 结论与下一步](#3fda)'
- en: 1\. Dimensionality Reduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 降维
- en: Many real problems in machine learning involve datasets with thousands or even
    millions of features. Training such datasets can be computationally demanding,
    and interpreting the resulting solutions can be even more challenging.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的许多实际问题都涉及具有成千上万甚至数百万个特征的数据集。训练这样的大数据集计算上非常繁重，解释得出的解决方案可能更具挑战性。
- en: As the number of features increases, the data points become more sparse, and
    distance metrics become less informative, since the distances between points are
    less pronounced making it difficult to distinguish what are close and distant
    points. That is known as the *curse of dimensionality*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着特征数量的增加，数据点变得更加稀疏，距离度量的有效性减弱，因为点与点之间的距离不那么明显，这使得很难区分哪些点是接近的，哪些是远离的。这被称为*维度灾难*。
- en: The more sparse data makes models harder to train and more prone to overfitting
    capturing noise rather than the underlying patterns. This leads to poor generalization
    to new, unseen data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的稀疏性使得模型训练更加困难，并且更容易过拟合，从而捕捉噪声而非潜在的模式。这导致模型对新数据的泛化能力较差。
- en: Dimensionality reduction is used in data science and machine learning to reduce
    the number of variables or features in a dataset while retaining as much of the
    original information as possible. This technique is useful for simplifying complex
    datasets, improving computational efficiency, and helping with data visualization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 降维在数据科学和机器学习中被用来减少数据集中的变量或特征数量，同时尽可能保留原始信息。这一技术有助于简化复杂数据集、提高计算效率，并且有助于数据可视化。
- en: '![](../Images/a55aff221424d9a010c30fecaf37ecfe.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a55aff221424d9a010c30fecaf37ecfe.png)'
- en: Image by the author using DALL-E.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E生成。
- en: 2\. How Does Principal Component Analysis Work?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 主成分分析是如何工作的？
- en: One of the most used techniques to mitigate the curse of dimensionality is Principal
    Component Analysis (PCA). The PCA reduces the number of features in a dataset
    while keeping most of the useful information by finding the axes that account
    for the largest variance in the dataset. Those axes are called the *principal
    components*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用来缓解维度灾难的最常用技术之一是主成分分析（PCA）。PCA通过寻找能够解释数据集中最大方差的轴来减少数据集中的特征数量，同时保留大部分有用的信息。这些轴被称为*主成分*。
- en: Since PCA aims to find a low-dimensional representation of a dataset while keeping
    a great share of the variance instead of performing predictions, It is considered
    an **unsupervised learning** algorithm.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于PCA的目标是找到数据集的低维表示，同时保持大部分的方差，而不是进行预测，因此它被视为一种**无监督学习**算法。
- en: But why does keeping the variance mean keeping important information?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么保持方差就意味着保留重要信息呢？
- en: Imagine you are analyzing a dataset about crimes in a city. The data have numerous
    features including "crime against person - with injuries" and “crime against person
    — without injuries”. Certainly, the places with high rates of the first example
    must also have high rates of the second example.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 假设你正在分析一个关于城市犯罪的数据集。数据中有许多特征，包括“侵害人身犯罪 - 有伤”和“侵害人身犯罪 - 无伤”。显然，发生高比例第一类犯罪的地方，也必然发生高比例第二类犯罪。
- en: In other words, the two features of the example are very correlated, so it is
    possible to reduce the dimensions of that dataset by diminishing the redundancies
    in the data (the presence or absence of injuries in the victim).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，示例中的两个特征高度相关，因此可以通过减少数据中的冗余（例如受害人是否受伤）来降低该数据集的维度。
- en: The PCA algorithm is nothing more than a sophisticated way of doing that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PCA算法不过是一个实现这一目标的复杂方法。
- en: 'Now, let''s break down how the PCA algorithm works under the hood in the following
    steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过以下步骤来逐步解析PCA算法的内部工作原理：
- en: '**Step 1: Centering the data**'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤1：居中数据**'
- en: PCA is affected by the scale of the data, so the first thing to do is to subtract
    the mean of each feature of the dataset, thus ensuring that all the features have
    a mean equal to 0.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）受数据尺度的影响，因此首先需要做的是减去数据集每个特征的均值，从而确保所有特征的均值都为0。
- en: '![](../Images/03ebbfb66492626ae7477637a2b7ddce.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03ebbfb66492626ae7477637a2b7ddce.png)'
- en: Data before and after centering (image by the author).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据居中前后的对比（图片由作者提供）。
- en: '**Step 2: Calculating the covariance matrix**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤2：计算协方差矩阵**'
- en: Now, we have to calculate the covariance matrix to capture how each pair of
    features in the data varies together. If the dataset has *n* features, the resulting
    covariance matrix will have *n* x *n* shape.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要计算协方差矩阵，以捕捉数据中每对特征之间的变化关系。如果数据集有*n*个特征，最终得到的协方差矩阵将具有*n* x *n*的形状。
- en: In the image below, features more correlated have colors closer to red. Of course,
    each feature will be highly correlated with itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，相关性较强的特征具有接近红色的颜色。当然，每个特征与自身的相关性最高。
- en: '![](../Images/fe5fe850cd277fae728a68121c86df55.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe5fe850cd277fae728a68121c86df55.png)'
- en: Heatmap of the covariance matrix (image by the author).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵的热图（图片由作者提供）。
- en: 'Step 3: Eigenvalue Decomposition'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：特征值分解
- en: 'Next, we have to perform the eigenvalue decomposition of the covariance matrix.
    In case you don''t remember, given the covariance matrix Σ (a square matrix),
    eigenvalue decomposition is the process of finding a set of scalars (eigenvalues)
    and vectors (eigenvectors) such that:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要执行协方差矩阵的特征值分解。如果你不记得的话，给定协方差矩阵Σ（一个方阵），特征值分解是寻找一组标量（特征值）和向量（特征向量）的过程，使得：
- en: '![](../Images/643ea25ef8e21f3abac05bc9a3bf2719.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/643ea25ef8e21f3abac05bc9a3bf2719.png)'
- en: Eigenvalue property (image by the author using [codecogs](https://editor.codecogs.com/)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值性质（图片由作者使用[codecogs](https://editor.codecogs.com/)绘制）。
- en: 'Where:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: Σ is the n×n covariance matrix.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Σ是n×n的协方差矩阵。
- en: '*v* is a non-zero vector called the eigenvector.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*v*是一个非零向量，称为特征向量。'
- en: '*λ* is a scalar called the eigenvalue associated with the eigenvector *v*.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*λ*是与特征向量*v*相关的标量，称为特征值。'
- en: '**Eigenvectors** indicate the directions of maximum variance in the data (the
    principal components), while **eigenvalues** quantify the variance captured by
    each principal component.'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**特征向量**表示数据中最大方差的方向（主成分），而**特征值**量化了每个主成分所捕获的方差。'
- en: 'If a matrix **A** can be decomposed into eigenvalues and eigenvectors, it can
    be represented as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果矩阵**A**可以分解为特征值和特征向量，则可以表示为：
- en: '![](../Images/d5a67b6aea9048c7daf0a0fa1d0f57d0.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5a67b6aea9048c7daf0a0fa1d0f57d0.png)'
- en: Eigendecomposition of a matrix (image by the author using [codecogs](https://editor.codecogs.com/)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的特征分解（图片由作者使用[codecogs](https://editor.codecogs.com/)绘制）。
- en: 'Where:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '**Q** is a matrix whose columns are the eigenvectors of **A**.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q**是一个矩阵，其列是**A**的特征向量。'
- en: Λ is a diagonal matrix whose diagonal elements are the eigenvalues of **A**.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Λ 是一个对角矩阵，其对角元素是**A**的特征值。
- en: That way, we can use the same steps to find the eigenvalues and eigenvectors
    of the covariance matrix.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以使用相同的步骤来求解协方差矩阵的特征值和特征向量。
- en: '![](../Images/1f6d776527b32054686c907ec2ae7832.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f6d776527b32054686c907ec2ae7832.png)'
- en: Plotting eigenvectors (image by the author).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制特征向量（图片由作者提供）。
- en: In the image above, we can see that the first eigenvector points to the direction
    with the most variance of the data, and the second eigenvector points to the direction
    with the second most variance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以看到第一个特征向量指向数据方差最大的方向，第二个特征向量指向第二大方差的方向。
- en: 'Step 4: Selecting the Principal Components'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：选择主成分
- en: As said earlier, the eigenvalues quantify the data's variance in the direction
    of its corresponding eigenvector. Thus, we sort the eigenvalues in descending
    order and keep only the top n required *principal components*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，特征值量化了数据在其对应特征向量方向上的方差。因此，我们将特征值按降序排序，并仅保留前n个所需的*主成分*。
- en: The image below illustrates the proportion of variance captured by each principal
    component in a PCA with two dimensions.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在二维PCA中，每个主成分捕获的方差比例。
- en: '![](../Images/ff80b7c8901b71e9f7f0e8f52fe46c0a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff80b7c8901b71e9f7f0e8f52fe46c0a.png)'
- en: Explained variance for 2 principal components (image by the author).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2个主成分的解释方差（图片由作者提供）。
- en: 'Step 5: Project the Data'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：投影数据
- en: Finally, we have to project the original data onto the dimensions represented
    by the selected principal components. To do that, we have to multiply the dataset,
    after being centered, by the matrix of eigenvectors found in the decomposition
    of the covariance matrix.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将原始数据投影到所选主成分所表示的维度上。为此，我们必须将中心化后的数据集与协方差矩阵分解中找到的特征向量矩阵相乘。
- en: '![](../Images/b56cd9f76d4c4101644a6e56a0cffcc7.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b56cd9f76d4c4101644a6e56a0cffcc7.png)'
- en: Projecting the original dataset to n dimensions (image by the author using [codecogs](https://editor.codecogs.com/)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据集投影到 n 维空间（图像来自作者使用 [codecogs](https://editor.codecogs.com/)）。
- en: 3\. Implementation in Python
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 在 Python 中的实现
- en: Now that we deeply understand the key concepts of Principal Component Analysis,
    it's time to create some code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经深刻理解了主成分分析的关键概念，是时候编写一些代码了。
- en: 'First, we have to set the environment importing the numpy package for mathematical
    calculations and matplotlib for visualization:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置环境，导入 numpy 包进行数学计算，并导入 matplotlib 进行可视化：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will encapsulate all the concepts covered in the previous section
    in a Python class with the following methods:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把上一节中涉及的所有概念封装成一个 Python 类，并包含以下方法：
- en: '**init method**'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init 方法**'
- en: 'Constructor method to initialize the algorithm''s parameters: the number of
    components desired, a matrix to store the components vectors, and an array to
    store the explained variance of each selected dimension.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数方法，用于初始化算法的参数：所需的组件数量、用于存储组件向量的矩阵，以及用于存储每个选定维度解释方差的数组。
- en: '**fit method**'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fit 方法**'
- en: In the fit method, the first four steps presented in the previous section are
    implemented with code. Also, the explained variances of each component are calculated.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 fit 方法中，上一节介绍的前四个步骤已经通过代码实现。此外，还计算了每个成分的解释方差。
- en: '**Transform method**'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transform 方法**'
- en: 'The transform method performs the last step presented in the previous section:
    project the data onto the selected dimensions.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: transform 方法执行上一节中介绍的最后一步：将数据投影到选定的维度上。
- en: '**Plot explained variance**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绘制解释方差**'
- en: The last method is a helper function to plot the explained variance of each
    selected principal component as a bar plot.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个方法是一个辅助函数，用于绘制每个选定主成分的解释方差的条形图。
- en: 'Here is the full code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码：
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 4\. Evaluation and interpretation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 评估与解释
- en: Now it's time to use the class we just implemented on a simulated dataset created
    using the numpy package. The dataset has 10 features and 100 samples.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候在使用 numpy 包创建的模拟数据集上应用我们刚刚实现的类了。数据集有 10 个特征和 100 个样本。
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Before performing the PCA, one question remains: **how do we choose the correct
    or optimal number of dimensions**? Generally, we have to look for the number of
    components that add up to at least 95% of the explained variance of the dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 PCA 之前，仍然有一个问题：**我们如何选择正确或最佳的维度数量**？通常，我们需要寻找那些加起来至少占数据集解释方差 95% 的组件数量。
- en: 'To do that, let''s take a look at how each principal component contributes
    to the total variance of the dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，让我们看看每个主成分是如何贡献于数据集的总方差的：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, let's plot the cumulative sum of the variance and check at which number
    of dimensions we achieve the optimal value of 95% of the total variance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制方差的累积和，看看在哪个维度数上我们能够达到总方差的 95% 最优值。
- en: '![](../Images/8de65b184309833b30aaaa75eac9fde5.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8de65b184309833b30aaaa75eac9fde5.png)'
- en: Explained variance as a function of the number of components (image by the author).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 解释的方差作为组件数量的函数（图像来自作者）。
- en: As shown in the graph above, the optimal number of dimensions for the dataset
    is 4, totaling 97.064% of the explained variance. In other words, we transformed
    a dataset with 10 features into one with only 3 dimensions while keeping more
    than 97% of the original information.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，数据集的最佳维度数为 4，总解释方差为 97.064%。换句话说，我们将一个具有 10 个特征的数据集转化为一个只有 3 个维度的数据集，同时保留了超过
    97% 的原始信息。
- en: That means that most of the original 10 features were very correlated and the
    algorithm transformed that high-dimensional data into uncorrelated principal components.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这意味着原始的 10 个特征大部分是高度相关的，而算法将这些高维数据转化为不相关的主成分。
- en: 5\. Conclusions and Next Steps
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 结论与下一步
- en: We created a PCA class using only the numpy package that successfully reduced
    the dimensionality of a dataset from 10 features to just 4 while preserving approximately
    97% of the data’s variance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用仅包含 numpy 包的代码创建了一个 PCA 类，它成功地将一个数据集的维度从 10 个特征减少到仅 4 个，同时保留了约 97% 的数据方差。
- en: Also, we explored a method to obtain an optimal number of principal components
    of the PCA analysis that can be customized depending on the problem we are facing
    (we may be interested in retaining only 90% of the variance, for example).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还探索了一种方法，获取 PCA 分析的最佳主成分数量，该数量可以根据我们所面临的问题进行自定义（例如，我们可能只对保留 90% 方差感兴趣）。
- en: 'That shows the potential of the PCA analysis to deal with the curse of dimensionality
    explained earlier. Additionally, I''d like to leave a few points for further exploration:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了PCA分析在解决前面提到的维度灾难中的潜力。此外，我希望留下几点供进一步探索：
- en: Perform classification or regression tasks using other machine learning algorithms
    on the reduced dataset using the PCA algorithm and compare the performance of
    models trained on the original dataset versus the PCA-transformed dataset to assess
    the impact of dimensionality reduction.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA算法对降维后的数据集执行分类或回归任务，并比较在原始数据集与PCA变换后的数据集上训练的模型的性能，以评估降维的影响。
- en: Use PCA for data visualization to make high-dimensional data more interpretable
    and to uncover patterns that were not evident in the original feature space.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA进行数据可视化，将高维数据转化为更易解释的形式，并揭示原始特征空间中未显现的模式。
- en: Consider exploring other dimensionality reduction techniques, such as *t-Distributed
    Stochastic Neighbor Embedding* (t-SNE) and *Linear Discriminant Analysis* (LDA).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑探索其他降维技术，例如 *t-分布随机邻域嵌入*（t-SNE）和 *线性判别分析*（LDA）。
- en: Complete code available [here](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码可在[此处](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA)获取。
- en: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
    [## ML-and-Ai-from-scratch/PCA at main · Marcussena/ML-and-Ai-from-scratch'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
    [## ML-and-Ai-from-scratch/PCA 主分支 · Marcussena/ML-and-Ai-from-scratch'
- en: Python implementation of machine learning and AI algorithms from scratch - ML-and-Ai-from-scratch/PCA
    at main ·…
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从零开始实现机器学习和人工智能算法的Python代码 - ML-and-Ai-from-scratch/PCA 主分支 ·…
- en: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/Marcussena/ML-and-Ai-from-scratch/tree/main/PCA?source=post_page-----184f295e97fe--------------------------------)
- en: Please feel free to use and improve the code, comment, make suggestions, and
    connect with me on [LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/),
    [X](https://twitter.com/MarcusMVLS), and [Github](https://github.com/Marcussena/ML-and-Ai-from-scratch).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请随时使用并改进代码、发表评论、提出建议，并通过 [LinkedIn](https://www.linkedin.com/in/marcus-sena-660198150/)、[X](https://twitter.com/MarcusMVLS)
    和 [Github](https://github.com/Marcussena/ML-and-Ai-from-scratch) 与我联系。
- en: References
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Willmott, Paul. (2019). *Machine Learning: An Applied Mathematics Introduction*.
    Panda Ohana Publishing.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Willmott, Paul. (2019). *机器学习：应用数学导论*。Panda Ohana Publishing.'
- en: '[2] Géron, A. (2017). *Hands-On Machine Learning*. O’Reilly Media Inc.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Géron, A. (2017). *动手学机器学习*。O’Reilly Media Inc.'
- en: '[3] Grus, Joel. (2015). *Data Science from Scratch*. O’Reilly Media Inc.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Grus, Joel. (2015). *从零开始学数据科学*。O’Reilly Media Inc.'
- en: '[4] Datagy.io. *How to Perform PCA in Python*. Retrieved June 2, 2024, from
    [https://datagy.io/python-pca/](https://datagy.io/python-pca/).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Datagy.io. *如何在Python中执行PCA*。2024年6月2日检索自 [https://datagy.io/python-pca/](https://datagy.io/python-pca/).'
