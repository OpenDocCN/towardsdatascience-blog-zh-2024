<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Vision Transformers, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Vision Transformers, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27">https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8?source=collection_archive---------1-----------------------#2024-02-27</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="2b26" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">Vision Transformers Explained Series</h2><div/><div><h2 id="5584" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">A Full Walk-Through of <em class="hi">Vision Transformers in PyTorch</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hj hk hl hm hn ab"><div><div class="ab ho"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------" rel="noopener follow"><div class="l hp hq by hr hs"><div class="l ed"><img alt="Skylar Jean Callis" class="l ep by dd de cx" src="../Images/db4d07b27d7feb86bfbb73b1065aa3a0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*9uFAYZilSG5RVGniO2uBnA.jpeg"/><div class="ht by l dd de em n hu eo"/></div></div></a></div></div><div class="hv ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------" rel="noopener follow"><div class="l hw hx by hr hy"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hz cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ht by l br hz em n hu eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="ia ab q"><div class="ab q ib"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ic id bk"><a class="af ag ah ai aj ak al am an ao ap aq ar ie" data-testid="authorName" href="https://medium.com/@sjcallis?source=post_page---byline--a9d07147e4c8--------------------------------" rel="noopener follow">Skylar Jean Callis</a></p></div></div></div><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ic id dx"><button class="ih ii ah ai aj ak al am an ao ap aq ar ij ik il" disabled="">Follow</button></p></div></div></span></div></div><div class="l im"><span class="bf b bg z dx"><div class="ab cn in io ip"><div class="iq ir ab"><div class="bf b bg z dx ab is"><span class="it l im">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar ie ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a9d07147e4c8--------------------------------" rel="noopener follow"><p class="bf b bg z iu iv iw ix iy iz ja jb bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="if ig" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="jc jd l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 27, 2024</span></div></span></div></span></div></div></div><div class="ab cp je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt"><div class="h k w ea eb q"><div class="kj l"><div class="ab q kk kl"><div class="pw-multi-vote-icon ed it km kn ko"><div class=""><div class="kp kq kr ks kt ku kv am kw kx ky ko"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kz la lb lc ld le lf"><p class="bf b dy z dx"><span class="kq">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kp li lj ab q ee lk ll" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lh"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lg lh">12</span></p></button></div></div></div><div class="ab q ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki"><div class="lm k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ln an ao ap ij lo lp lq" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lr cn"><div class="l ae"><div class="ab cb"><div class="ls lt lu lv lw lx ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ln an ao ap ij ly lz ll ma mb mc md me s mf mg mh mi mj mk ml u mm mn mo"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="d6bb" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk"><em class="nl">Since their introduction in 2017 with </em>Attention is All You Need<em class="nl">¹, transformers have established themselves as the state of the art for natural language processing (NLP). In 2021, </em>An Image is Worth 16x16 Words<em class="nl">² successfully adapted transformers for computer vision tasks. Since then, numerous transformer-based architectures have been proposed for computer vision.</em></p><p id="d1a5" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk"><strong class="mr ga">This article walks through the Vision Transformer (ViT) as laid out in <em class="nl">An Image is Worth 16x16 Words</em>². It includes open-source code for the ViT, as well as conceptual explanations of the components. All of the code uses the PyTorch Python package.</strong></p></div></div><div class="nm"><div class="ab cb"><div class="ls nn lt no lu np cf nq cg nr ci bh"><figure class="nv nw nx ny nz nm oa ob paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt nu"><img src="../Images/cb8f3e52a6e63f4ae89de2cb443aa76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/0*5r-4kq_qAbyFBnTt"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Photo by <a class="af om" href="https://unsplash.com/@sahandbabali?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sahand Babali</a> on <a class="af om" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="89c7" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">This article is part of a collection examining the internal workings of Vision Transformers in depth. Each of these articles is also available as a Jupyter Notebook with executable code. The other articles in the series are:</p><ul class=""><li id="c8f3" class="mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk on oo op bk"><a class="af om" rel="noopener" target="_blank" href="/vision-transformers-explained-a9d07147e4c8"><strong class="mr ga">Vision Transformers, Explained</strong></a><strong class="mr ga"><br/> </strong>→<a class="af om" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/VisionTransformersExplained.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook</a></li><li id="58d1" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">Attention for Vision Transformers, Explained</a><br/> → <a class="af om" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/AttentionExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="e346" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" rel="noopener" target="_blank" href="/position-embeddings-for-vision-transformers-explained-a6f9add341d5">Position Embeddings for Vision Transformers, Explained</a><br/> → <a class="af om" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/PositionEmbeddingExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="9495" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa">Tokens-to-Token Vision Transformers, Explained</a><br/> → <a class="af om" href="https://github.com/lanl/vision_transformers_explained/blob/main/notebooks/TokensToTokenViTExplained.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter Notebook</a></li><li id="6fc0" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub Repository for Vision Transformers, Explained Series</a></li></ul><h2 id="a639" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Table of Contents</h2><ul class=""><li id="7cb3" class="mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk on oo op bk"><a class="af om" href="#1d10" rel="noopener ugc nofollow">What Are Vision Transformers?</a></li><li id="0ca8" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" href="#bce9" rel="noopener ugc nofollow">Model Walk-Through</a><br/> —<a class="af om" href="#691b" rel="noopener ugc nofollow"> Image Tokenization</a><br/>— <a class="af om" href="#a32e" rel="noopener ugc nofollow">Token Processing</a><br/> — <a class="af om" href="#962c" rel="noopener ugc nofollow">Encoding Block</a><br/> — <a class="af om" href="#c80b" rel="noopener ugc nofollow">Neural Network Module</a><br/> —<a class="af om" href="#d3ad" rel="noopener ugc nofollow"> Prediction Processing</a></li><li id="0762" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" href="#3fdb" rel="noopener ugc nofollow">Complete Code</a></li><li id="09a9" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" href="#caea" rel="noopener ugc nofollow">Conclusion</a><br/> — <a class="af om" href="#96ea" rel="noopener ugc nofollow">Further Reading</a><br/> — <a class="af om" href="#dc9b" rel="noopener ugc nofollow">Citations</a></li></ul><h1 id="1d10" class="pu ow fq bf ox pv pw gv pb px py gy pf pz qa qb qc qd qe qf qg qh qi qj qk ql bk">What are Vision Transformers?</h1><p id="f0e8" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">As introduced in <em class="nl">Attention is All You Need</em>¹, transformers are a type of machine learning model utilizing attention as the primary learning mechanism. Transformers quickly became the state of the art for sequence-to-sequence tasks such as language translation.</p><p id="0b86" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk"><em class="nl">An Image is Worth 16x16 Words</em>² successfully modified the transformer put forth in [1] to solve image classification tasks, creating the <strong class="mr ga">Vi</strong>sion <strong class="mr ga">T</strong>ransformer (ViT). The ViT is based on the same attention mechanism as the transformer in [1]. However, while transformers for NLP tasks consist of an encoder attention branch and a decoder attention branch, the ViT only uses an encoder. The output of the encoder is then passed to a neural network “head” that makes a prediction.</p><p id="fc70" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The drawback of ViT as implemented in [2] is that it’s optimal performance requires pretraining on large datasets. The best models pretrained on the proprietary JFT-300M dataset. Models pretrained on the smaller, open source ImageNet-21k perform on par with the state-of-the-art convolutional ResNet models.</p><p id="c58e" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk"><em class="nl">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>³ attempts to remove this pretraining requirement by introducing a novel pre-processing methodology to transform an input image into a series of tokens. More about this method can be found <a class="af om" rel="noopener" target="_blank" href="/tokens-to-token-vision-transformers-explained-2fa4e2002daa">here</a>. For this article, we’ll focus on the ViT as implemented in [2].</p><h1 id="bce9" class="pu ow fq bf ox pv pw gv pb px py gy pf pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Model Walk-Through</h1><p id="53f6" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">This article follows the model structure outlined in <em class="nl">An Image is Worth 16x16 Words</em>². However, code from this paper is not publicly available. Code from the more recent <em class="nl">Tokens-to-Token ViT</em>³ is available on GitHub. The Tokens-to-Token ViT (T2T-ViT) model prepends a Tokens-to-Token (T2T) module to a vanilla ViT backbone. The code in this article is based on the ViT components in the <em class="nl">Tokens-to-Token ViT</em>³<em class="nl"> </em>GitHub code. Modifications made for this article include, but are not limited to, modifying to allow for non-square input images and removing dropout layers.</p><p id="66a7" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">A diagram of the ViT model is shown below.</p><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt qm"><img src="../Images/e78867b93d211770cf4444ad59c6f8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-DxPDAO2HMFMLy-zDGT3Q.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">ViT Model Diagram (image by author)</figcaption></figure><h2 id="691b" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Image Tokenization</h2><p id="b84f" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">The first step of the ViT is to create tokens from the input image. Transformers operate on a <em class="nl">sequence </em>of<em class="nl"> tokens</em>; in NLP, this is commonly a <em class="nl">sentence</em> of <em class="nl">words</em>. For computer vision, it is less clear how to segment the input into tokens.</p><p id="d332" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The<em class="nl"> </em>ViT converts an image to tokens such that each token represents a local area — or <em class="nl">patch</em> — of the image. They describe reshaping an image of height <em class="nl">H</em>, width <em class="nl">W</em>, and channels <em class="nl">C </em>into <em class="nl">N</em> tokens with patch size <em class="nl">P</em>:</p><figure class="nv nw nx ny nz nm"><div class="qn iu l ed"><div class="qo qp l"/></div></figure><p id="a1dc" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Each token is of length <em class="nl">P²∗C</em>.</p><p id="a694" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Let’s look at an example of patch tokenization on this pixel art <em class="nl">Mountain at Dusk</em> by Luis Zuno (<a class="af om" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>)⁴. The original artwork has been cropped and converted to a single channel image. This means that each pixel has a value between zero and one. Single channel images are typically displayed in grayscale; however, we’ll be displaying it in a purple color scheme because its easier to see.</p><p id="62bd" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Note that the patch tokenization is not included in the code associated with [3]. All code in this section is original to the author.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="185e" class="qu ow fq qr b bg qv qw l qx qy">mountains = np.load(os.path.join(figure_path, 'mountains.npy'))<br/><br/>H = mountains.shape[0]<br/>W = mountains.shape[1]<br/>print('Mountain at Dusk is H =', H, 'and W =', W, 'pixels.')<br/>print('\n')<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>plt.clim([0,1])<br/>cbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])<br/>plt.clim([0, 1])<br/>plt.colorbar(cax=cbar_ax);<br/>#plt.savefig(os.path.join(figure_path, 'mountains.png'))</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="3ec1" class="qu ow fq qr b bg qv qw l qx qy">Mountain at Dusk is H = 60 and W = 100 pixels.</span></pre><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt ra"><img src="../Images/bac19092e0adc6376b2a9f26c9bcc604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItBJDmA5Plb3_2pkQypZ5g.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="068a" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">This image has <em class="nl">H=60</em> and <em class="nl">W=100</em>. We’ll set <em class="nl">P=20 </em>since it divides both <em class="nl">H</em> and <em class="nl">W</em> evenly.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="0b89" class="qu ow fq qr b bg qv qw l qx qy">P = 20<br/>N = int((H*W)/(P**2))<br/>print('There will be', N, 'patches, each', P, 'by', str(P)+'.')<br/>print('\n')<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.imshow(mountains, cmap='Purples_r')<br/>plt.hlines(np.arange(P, H, P)-0.5, -0.5, W-0.5, color='w')<br/>plt.vlines(np.arange(P, W, P)-0.5, -0.5, H-0.5, color='w')<br/>plt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))<br/>plt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))<br/>x_text = np.tile(np.arange(9.5, W, P), 3)<br/>y_text = np.repeat(np.arange(9.5, H, P), 5)<br/>for i in range(1, N+1):<br/>    plt.text(x_text[i-1], y_text[i-1], str(i), color='w', fontsize='xx-large', ha='center')<br/>plt.text(x_text[2], y_text[2], str(3), color='k', fontsize='xx-large', ha='center');<br/>#plt.savefig(os.path.join(figure_path, 'mountain_patches.png'), bbox_inches='tight'</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="c070" class="qu ow fq qr b bg qv qw l qx qy">There will be 15 patches, each 20 by 20.</span></pre><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt rb"><img src="../Images/88fd07364a89357fe4edb4af137ee20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x25G43UdDV34yR58B79aoA.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="0ac3" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">By flattening these patches, we see the resulting tokens. Let’s look at patch 12 as an example, since it has four different shades in it.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="818a" class="qu ow fq qr b bg qv qw l qx qy">print('Each patch will make a token of length', str(P**2)+'.')<br/>print('\n')<br/><br/>patch12 = mountains[40:60, 20:40]<br/>token12 = patch12.reshape(1, P**2)<br/><br/>fig = plt.figure(figsize=(10,1))<br/>plt.imshow(token12, aspect=10, cmap='Purples_r')<br/>plt.clim([0,1])<br/>plt.xticks(np.arange(-0.5, 401, 50), labels=np.arange(0, 401, 50))<br/>plt.yticks([]);<br/>#plt.savefig(os.path.join(figure_path, 'mountain_token12.png'), bbox_inches='tight')</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="ea53" class="qu ow fq qr b bg qv qw l qx qy">Each patch will make a token of length 400.</span></pre><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt rc"><img src="../Images/d20a481555baf48c6ee77002eb4e003d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EG8pjwfYTurjcqBPOoAWlw.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Code Output (image by author)</figcaption></figure><p id="b668" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">After extracting tokens from an image, it is common to use a linear projection to change the length of the tokens. This is implemented as a learnable linear layer. The new length of the tokens is referred to as the <em class="nl">latent dimension²</em>, <em class="nl">channel dimension³</em>, or the<em class="nl"> token length. </em>After the projection, the tokens are no longer visually identifiable as a patch from the original image.</p><p id="74f9" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now that we understand the concept, we can look at how patch tokenization is implemented in code.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="a55c" class="qu ow fq qr b bg qv qw l qx qy">class Patch_Tokenization(nn.Module):<br/>    def __init__(self,<br/>                img_size: tuple[int, int, int]=(1, 1, 60, 100),<br/>                patch_size: int=50,<br/>                token_len: int=768):<br/><br/>        """ Patch Tokenization Module<br/>            Args:<br/>                img_size (tuple[int, int, int]): size of input (channels, height, width)<br/>                patch_size (int): the side length of a square patch<br/>                token_len (int): desired length of an output token<br/>        """<br/>        super().__init__()<br/><br/>        ## Defining Parameters<br/>        self.img_size = img_size<br/>        C, H, W = self.img_size<br/>        self.patch_size = patch_size<br/>        self.token_len = token_len<br/>        assert H % self.patch_size == 0, 'Height of image must be evenly divisible by patch size.'<br/>        assert W % self.patch_size == 0, 'Width of image must be evenly divisible by patch size.'<br/>        self.num_tokens = (H / self.patch_size) * (W / self.patch_size)<br/><br/>        ## Defining Layers<br/>        self.split = nn.Unfold(kernel_size=self.patch_size, stride=self.patch_size, padding=0)<br/>        self.project = nn.Linear((self.patch_size**2)*C, token_len)<br/><br/>    def forward(self, x):<br/>        x = self.split(x).transpose(1,0)<br/>        x = self.project(x)<br/>        return x</span></pre><p id="4c07" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Note the two <code class="cx rd re rf qr b">assert</code> statements that ensure the image dimensions are evenly divisible by the patch size. The actual splitting into patches is implemented as a <code class="cx rd re rf qr b">torch.nn.Unfold</code>⁵ layer.</p><p id="3e22" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">We’ll run an example of this code using our cropped, single channel version of <em class="nl">Mountain at Dusk</em>⁴. We should see the values for number of tokens and initial token size as we did above. We’ll use <em class="nl">token_len=768</em> as the projected length, which is the size for the base variant of ViT².</p><p id="7867" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The first line in the code block below is changing the datatype of <em class="nl">Mountain at Dusk</em>⁴ from a NumPy array to a Torch tensor. We also have to <code class="cx rd re rf qr b">unsqueeze</code>⁶ the tensor to create a channel dimension and a batch size dimension. As above, we have one channel. Since there is only one image, <em class="nl">batchsize=1</em>.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="10cf" class="qu ow fq qr b bg qv qw l qx qy">x = torch.from_numpy(mountains).unsqueeze(0).unsqueeze(0).to(torch.float32)<br/>token_len = 768<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of input channels:', x.shape[1], '\n\timage size:', (x.shape[2], x.shape[3]))<br/><br/># Define the Module<br/>patch_tokens = Patch_Tokenization(img_size=(x.shape[1], x.shape[2], x.shape[3]),<br/>                                    patch_size = P,<br/>                                    token_len = token_len)</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="422d" class="qu ow fq qr b bg qv qw l qx qy">Input dimensions are<br/>   batchsize: 1 <br/>   number of input channels: 1 <br/>   image size: (60, 100)</span></pre><p id="eaaa" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now, we’ll split the image into tokens.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="9fa6" class="qu ow fq qr b bg qv qw l qx qy">x = patch_tokens.split(x).transpose(2,1)<br/>print('After patch tokenization, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="b44d" class="qu ow fq qr b bg qv qw l qx qy">After patch tokenization, dimensions are<br/>   batchsize: 1 <br/>   number of tokens: 15 <br/>   token length: 400</span></pre><p id="b1d1" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">As we saw in the example, there are <em class="nl">N=15</em> tokens each of length 400. Lastly, we project the tokens to be the <em class="nl">token_len</em>.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="80a7" class="qu ow fq qr b bg qv qw l qx qy">x = patch_tokens.project(x)<br/>print('After projection, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="62a7" class="qu ow fq qr b bg qv qw l qx qy">After projection, dimensions are<br/>   batchsize: 1 <br/>   number of tokens: 15 <br/>   token length: 768</span></pre><p id="864c" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now that we have tokens, we’re ready to proceed through the ViT.</p><h2 id="a32e" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Token Processing</h2><p id="affa" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">We’ll designate the next two steps of the ViT, before the encoding blocks, as “token processing.” The token processing component of the ViT diagram is shown below.</p><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt qm"><img src="../Images/12ccf966ee3c533f10d3a53652cb4fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dTrfAZ94myPttzj76m74tw.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Token Processing Components of ViT Diagram (image by author)</figcaption></figure><p id="d85b" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The first step is to prepend a blank token, called the <em class="nl">Prediction Token, </em>to the the image tokens. This token will be used at the output of the encoding blocks to make a prediction. It starts off blank — equivalently zero — so that it can gain information from the other image tokens.</p><p id="325b" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">We’ll be starting with 175 tokens. Each token has length 768, which is the size for the base variant of ViT². We’re using a batch size of 13 because it’s prime and won’t be confused for any of the other parameters.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="6c6f" class="qu ow fq qr b bg qv qw l qx qy"># Define an Input<br/>num_tokens = 175<br/>token_len = 768<br/>batch = 13<br/>x = torch.rand(batch, num_tokens, token_len)<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])<br/><br/># Append a Prediction Token<br/>pred_token = torch.zeros(1, 1, token_len).expand(batch, -1, -1)<br/>print('Prediction Token dimensions are\n\tbatchsize:', pred_token.shape[0], '\n\tnumber of tokens:', pred_token.shape[1], '\n\ttoken length:', pred_token.shape[2])<br/><br/>x = torch.cat((pred_token, x), dim=1)<br/>print('Dimensions with Prediction Token are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="78de" class="qu ow fq qr b bg qv qw l qx qy">Input dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 175 <br/>   token length: 768<br/>Prediction Token dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 1 <br/>   token length: 768<br/>Dimensions with Prediction Token are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token length: 768</span></pre><p id="7dde" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now, we add a position embedding for our tokens. The position embedding allows the transformer to understand the order of the image tokens. Note that this is an addition, not a concatenation. The specifics of position embeddings are a tangent best left for <a class="af om" rel="noopener" target="_blank" href="/position-embeddings-for-vision-transformers-explained-a6f9add341d5">another time</a>.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="222d" class="qu ow fq qr b bg qv qw l qx qy">def get_sinusoid_encoding(num_tokens, token_len):<br/>    """ Make Sinusoid Encoding Table<br/><br/>        Args:<br/>            num_tokens (int): number of tokens<br/>            token_len (int): length of a token<br/>            <br/>        Returns:<br/>            (torch.FloatTensor) sinusoidal position encoding table<br/>    """<br/><br/>    def get_position_angle_vec(i):<br/>        return [i / np.power(10000, 2 * (j // 2) / token_len) for j in range(token_len)]<br/><br/>    sinusoid_table = np.array([get_position_angle_vec(i) for i in range(num_tokens)])<br/>    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])<br/>    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) <br/><br/>    return torch.FloatTensor(sinusoid_table).unsqueeze(0)<br/><br/>PE = get_sinusoid_encoding(num_tokens+1, token_len)<br/>print('Position embedding dimensions are\n\tnumber of tokens:', PE.shape[1], '\n\ttoken length:', PE.shape[2])<br/><br/>x = x + PE<br/>print('Dimensions with Position Embedding are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="1ea5" class="qu ow fq qr b bg qv qw l qx qy">Position embedding dimensions are<br/>   number of tokens: 176 <br/>   token length: 768<br/>Dimensions with Position Embedding are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token length: 768</span></pre><p id="01ea" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now, our tokens are ready to proceed to the encoding blocks.</p><h2 id="962c" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Encoding Block</h2><p id="74ab" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">The encoding block is where the model actually learns from the image tokens. The number of encoding blocks is a hyperparameter set by the user. A diagram of the encoding block is below.</p><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div class="ns nt rg"><img src="../Images/1200a46f21d3d72d0176d291146d2dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*BvfoQreojosfjYAOuCXhHg.png"/></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Encoding Block (image by author)</figcaption></figure><p id="242a" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The code for an encoding block is below.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="8c9a" class="qu ow fq qr b bg qv qw l qx qy">class Encoding(nn.Module):<br/><br/>    def __init__(self,<br/>       dim: int,<br/>       num_heads: int=1,<br/>       hidden_chan_mul: float=4.,<br/>       qkv_bias: bool=False,<br/>       qk_scale: NoneFloat=None,<br/>       act_layer=nn.GELU, <br/>       norm_layer=nn.LayerNorm):<br/>        <br/>        """ Encoding Block<br/><br/>            Args:<br/>                dim (int): size of a single token<br/>                num_heads(int): number of attention heads in MSA<br/>                hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component<br/>                qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                                    if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Define Layers<br/>        self.norm1 = norm_layer(dim)<br/>        self.attn = Attention(dim=dim,<br/>                            chan=dim,<br/>                            num_heads=num_heads,<br/>                            qkv_bias=qkv_bias,<br/>                            qk_scale=qk_scale)<br/>        self.norm2 = norm_layer(dim)<br/>        self.neuralnet = NeuralNet(in_chan=dim,<br/>                                hidden_chan=int(dim*hidden_chan_mul),<br/>                                out_chan=dim,<br/>                                act_layer=act_layer)<br/><br/>    def forward(self, x):<br/>        x = x + self.attn(self.norm1(x))<br/>        x = x + self.neuralnet(self.norm2(x))<br/>        return x</span></pre><p id="c7eb" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The<em class="nl"> num_heads, qkv_bias, </em>and<em class="nl"> qk_scale</em> parameters define the <em class="nl">Attention</em> module components. A deep dive into attention for vision transformers is left for <a class="af om" rel="noopener" target="_blank" href="/attention-for-vision-transformers-explained-70f83984c673">another time</a>.</p><p id="efe1" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The <em class="nl">hidden_chan_mul</em> and <em class="nl">act_layer</em> parameters define the <em class="nl">Neural Network</em> module components. The activation layer can be any <code class="cx rd re rf qr b">torch.nn.modules.activation</code>⁷ layer. We’ll look more at the <em class="nl">Neural Network</em> module <a class="af om" href="#c80b" rel="noopener ugc nofollow">later</a>.</p><p id="e712" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The <em class="nl">norm_layer</em> can be chosen from any <code class="cx rd re rf qr b">torch.nn.modules.normalization</code>⁸ layer.</p><p id="7747" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">We’ll now step through each blue block in the diagram and its accompanying code. We’ll use 176 tokens of length 768. We’ll use a batch size of 13 because it’s prime and won’t be confused for any of the other parameters. We’ll use 4 attention heads because it evenly divides token length; however, you won’t see the attention head dimension in the encoding block.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="f5d7" class="qu ow fq qr b bg qv qw l qx qy"># Define an Input<br/>num_tokens = 176<br/>token_len = 768<br/>batch = 13<br/>heads = 4<br/>x = torch.rand(batch, num_tokens, token_len)<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])<br/><br/># Define the Module<br/>E = Encoding(dim=token_len, num_heads=heads, hidden_chan_mul=1.5, qkv_bias=False, qk_scale=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm)<br/>E.eval();</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="1381" class="qu ow fq qr b bg qv qw l qx qy">Input dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token length: 768</span></pre><p id="0327" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now, we’ll pass through a norm layer and an <em class="nl">Attention</em> module. The <em class="nl">Attention</em> module in the encoding block is parameterized so that it don’t change the token length. After the <em class="nl">Attention</em> module, we implement our first split connection.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="56be" class="qu ow fq qr b bg qv qw l qx qy">y = E.norm1(x)<br/>print('After norm, dimensions are\n\tbatchsize:', y.shape[0], '\n\tnumber of tokens:', y.shape[1], '\n\ttoken size:', y.shape[2])<br/>y = E.attn(y)<br/>print('After attention, dimensions are\n\tbatchsize:', y.shape[0], '\n\tnumber of tokens:', y.shape[1], '\n\ttoken size:', y.shape[2])<br/>y = y + x<br/>print('After split connection, dimensions are\n\tbatchsize:', y.shape[0], '\n\tnumber of tokens:', y.shape[1], '\n\ttoken size:', y.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="709a" class="qu ow fq qr b bg qv qw l qx qy">After norm, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768<br/>After attention, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768<br/>After split connection, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768</span></pre><p id="92e9" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Now, we pass through another norm layer, and then the <em class="nl">Neural Network</em> module. We finish with the second split connection.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="36c4" class="qu ow fq qr b bg qv qw l qx qy">z = E.norm2(y)<br/>print('After norm, dimensions are\n\tbatchsize:', z.shape[0], '\n\tnumber of tokens:', z.shape[1], '\n\ttoken size:', z.shape[2])<br/>z = E.neuralnet(z)<br/>print('After neural net, dimensions are\n\tbatchsize:', z.shape[0], '\n\tnumber of tokens:', z.shape[1], '\n\ttoken size:', z.shape[2])<br/>z = z + y<br/>print('After split connection, dimensions are\n\tbatchsize:', z.shape[0], '\n\tnumber of tokens:', z.shape[1], '\n\ttoken size:', z.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="aeab" class="qu ow fq qr b bg qv qw l qx qy">After norm, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768<br/>After neural net, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768<br/>After split connection, dimensions are<br/>   batchsize: 13 <br/>   number of tokens: 176 <br/>   token size: 768</span></pre><p id="9572" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">That’s all for a single encoding block! Since the final dimensions are the same as the initial dimensions, the model can easily pass tokens through multiple encoding blocks, as set by the <em class="nl">depth</em> hyperparameter.</p><h2 id="c80b" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Neural Network Module</h2><p id="2a6d" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">The <em class="nl">Neural Network</em> (NN) module is a sub-component of the encoding block. The NN module is very simple, consisting of a fully-connected layer, an activation layer, and another fully-connected layer. The activation layer can be any <code class="cx rd re rf qr b">torch.nn.modules.activation</code>⁷ layer, which is passed as input to the module. The NN module can be configured to change the shape of an input, or to maintain the same shape. We’re not going to step through this code, as neural networks are common in machine learning, and not the focus of this article. However, the code for the NN module is presented below.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="aa61" class="qu ow fq qr b bg qv qw l qx qy">class NeuralNet(nn.Module):<br/>    def __init__(self,<br/>       in_chan: int,<br/>       hidden_chan: NoneFloat=None,<br/>       out_chan: NoneFloat=None,<br/>       act_layer = nn.GELU):<br/>        """ Neural Network Module<br/><br/>            Args:<br/>                in_chan (int): number of channels (features) at input<br/>                hidden_chan (NoneFloat): number of channels (features) in the hidden layer;<br/>                                        if None, number of channels in hidden layer is the same as the number of input channels<br/>                out_chan (NoneFloat): number of channels (features) at output;<br/>                                        if None, number of output channels is same as the number of input channels<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Define Number of Channels<br/>        hidden_chan = hidden_chan or in_chan<br/>        out_chan = out_chan or in_chan<br/><br/>        ## Define Layers<br/>        self.fc1 = nn.Linear(in_chan, hidden_chan)<br/>        self.act = act_layer()<br/>        self.fc2 = nn.Linear(hidden_chan, out_chan)<br/><br/>    def forward(self, x):<br/>        x = self.fc1(x)<br/>        x = self.act(x)<br/>        x = self.fc2(x)<br/>        return x</span></pre><h2 id="d3ad" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Prediction Processing</h2><p id="0fa9" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">After passing through the encoding blocks, the last thing the model must do is make a prediction. The “prediction processing” component of the ViT diagram is shown below.</p><figure class="nv nw nx ny nz nm ns nt paragraph-image"><div role="button" tabindex="0" class="oc od ed oe bh of"><div class="ns nt rh"><img src="../Images/d09768832b3f7457cf3cf0ac45e80310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5GYj_wq6EhYUxm2IiM5GHA.png"/></div></div><figcaption class="oh oi oj ns nt ok ol bf b bg z dx">Prediction Processing Components of ViT Diagram (image by author)</figcaption></figure><p id="d268" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">We’re going to look at each step of this process. We’ll continue with 176 tokens of length 768. We’ll use a batch size of 1 to illustrate how a single prediction is made. A batch size greater than 1 would be computing this prediction in parallel.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="8240" class="qu ow fq qr b bg qv qw l qx qy"># Define an Input<br/>num_tokens = 176<br/>token_len = 768<br/>batch = 1<br/>x = torch.rand(batch, num_tokens, token_len)<br/>print('Input dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken length:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="9aff" class="qu ow fq qr b bg qv qw l qx qy">Input dimensions are<br/>   batchsize: 1 <br/>   number of tokens: 176 <br/>   token length: 768</span></pre><p id="2b9f" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">First, all the tokens are passed through a norm layer.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="28ea" class="qu ow fq qr b bg qv qw l qx qy">norm = nn.LayerNorm(token_len)<br/>x = norm(x)<br/>print('After norm, dimensions are\n\tbatchsize:', x.shape[0], '\n\tnumber of tokens:', x.shape[1], '\n\ttoken size:', x.shape[2])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="0858" class="qu ow fq qr b bg qv qw l qx qy">After norm, dimensions are<br/>   batchsize: 1 <br/>   number of tokens: 1001 <br/>   token size: 768</span></pre><p id="e985" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Next, we split off the prediction token from the rest of the tokens. Throughout the encoding block(s), the prediction token has become nonzero and gained information about our input image. We’ll use only this prediction token to make a final prediction.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="3ca3" class="qu ow fq qr b bg qv qw l qx qy">pred_token = x[:, 0]<br/>print('Length of prediction token:', pred_token.shape[-1])</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="c9fb" class="qu ow fq qr b bg qv qw l qx qy">Length of prediction token: 768</span></pre><p id="fad9" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Finally, the prediction token is passed through the <em class="nl">head</em> to make a prediction. The <em class="nl">head</em>, usually some variety of neural network, is varied based on the model. In <em class="nl">An Image is Worth 16x16 Words</em>², they use an MLP (<a class="af om" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">multilayer perceptron</a>) with one hidden layer during pretraining and a single linear layer during fine tuning. In <em class="nl">Tokens-to-Token ViT³</em>, they use a single linear layer as a head. This example proceeds with a single linear layer.</p><p id="8814" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">Note that the output shape of the head is set based on the parameters of the learning problem. For classification, it is typically a vector of length <em class="nl">number of classes</em> in a <a class="af om" href="https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics" rel="noopener ugc nofollow" target="_blank">one-hot encoding</a>. For regression, it would be any integer number of predicted parameters. This example will use an output shape of 1 to represent a single estimated regression value.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="13b7" class="qu ow fq qr b bg qv qw l qx qy">head = nn.Linear(token_len, 1)<br/>pred = head(pred_token)<br/>print('Length of prediction:', (pred.shape[0], pred.shape[1]))<br/>print('Prediction:', float(pred))</span></pre><pre class="qz qq qr qs bp qt bb bk"><span id="bf1b" class="qu ow fq qr b bg qv qw l qx qy">Length of prediction: (1, 1)<br/>Prediction: -0.5474240779876709</span></pre><p id="b0bc" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">And that’s all! The model has made a prediction!</p><h1 id="3fdb" class="pu ow fq bf ox pv pw gv pb px py gy pf pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Complete Code</h1><p id="7979" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">To create the complete ViT module, we use the <em class="nl">Patch Tokenization</em> module defined <a class="af om" href="#691b" rel="noopener ugc nofollow">above</a> and the <em class="nl">ViT Backbone</em> module. The <em class="nl">ViT Backbone</em> is defined below, and contains the <a class="af om" href="#a32e" rel="noopener ugc nofollow"><em class="nl">Token Processing</em></a>, <a class="af om" href="#962c" rel="noopener ugc nofollow"><em class="nl">Encoding Blocks</em></a>, and <a class="af om" href="http://d3ad" rel="noopener ugc nofollow" target="_blank"><em class="nl">Prediction Processing</em></a><em class="nl"> </em>components.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="05bb" class="qu ow fq qr b bg qv qw l qx qy">class ViT_Backbone(nn.Module):<br/>    def __init__(self,<br/>                preds: int=1,<br/>                token_len: int=768,<br/>                num_heads: int=1,<br/>                Encoding_hidden_chan_mul: float=4.,<br/>                depth: int=12,<br/>                qkv_bias=False,<br/>                qk_scale=None,<br/>                act_layer=nn.GELU,<br/>                norm_layer=nn.LayerNorm):<br/><br/>        """ VisTransformer Backbone<br/>            Args:<br/>                preds (int): number of predictions to output<br/>                token_len (int): length of a token<br/>                num_heads(int): number of attention heads in MSA<br/>                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module<br/>                depth (int): number of encoding blocks in the model<br/>                qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>                qk_scale (NoneFloat): value to scale the queries and keys by; <br/>                 if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>                act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>        """<br/><br/>        super().__init__()<br/><br/>        ## Defining Parameters<br/>        self.num_heads = num_heads<br/>        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul<br/>        self.depth = depth<br/><br/>        ## Defining Token Processing Components<br/>        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.token_len))<br/>        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(num_tokens=self.num_tokens+1, token_len=self.token_len), requires_grad=False)<br/><br/>        ## Defining Encoding blocks<br/>        self.blocks = nn.ModuleList([Encoding(dim = self.token_len, <br/>                                               num_heads = self.num_heads,<br/>                                               hidden_chan_mul = self.Encoding_hidden_chan_mul,<br/>                                               qkv_bias = qkv_bias,<br/>                                               qk_scale = qk_scale,<br/>                                               act_layer = act_layer,<br/>                                               norm_layer = norm_layer)<br/>             for i in range(self.depth)])<br/><br/>        ## Defining Prediction Processing<br/>        self.norm = norm_layer(self.token_len)<br/>        self.head = nn.Linear(self.token_len, preds)<br/><br/>        ## Make the class token sampled from a truncated normal distrobution <br/>        timm.layers.trunc_normal_(self.cls_token, std=.02)<br/><br/>    def forward(self, x):<br/>        ## Assumes x is already tokenized<br/><br/>        ## Get Batch Size<br/>        B = x.shape[0]<br/>        ## Concatenate Class Token<br/>        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)<br/>        ## Add Positional Embedding<br/>        x = x + self.pos_embed<br/>        ## Run Through Encoding Blocks<br/>        for blk in self.blocks:<br/>            x = blk(x)<br/>        ## Take Norm<br/>        x = self.norm(x)<br/>        ## Make Prediction on Class Token<br/>        x = self.head(x[:, 0])<br/>        return x</span></pre><p id="dcad" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">From the <em class="nl">ViT Backbone</em> module, we can define the full ViT model.</p><pre class="nv nw nx ny nz qq qr qs bp qt bb bk"><span id="dfb6" class="qu ow fq qr b bg qv qw l qx qy">class ViT_Model(nn.Module):<br/> def __init__(self,<br/>    img_size: tuple[int, int, int]=(1, 400, 100),<br/>    patch_size: int=50,<br/>    token_len: int=768,<br/>    preds: int=1,<br/>    num_heads: int=1,<br/>    Encoding_hidden_chan_mul: float=4.,<br/>    depth: int=12,<br/>    qkv_bias=False,<br/>    qk_scale=None,<br/>    act_layer=nn.GELU,<br/>    norm_layer=nn.LayerNorm):<br/><br/>  """ VisTransformer Model<br/><br/>   Args:<br/>    img_size (tuple[int, int, int]): size of input (channels, height, width)<br/>    patch_size (int): the side length of a square patch<br/>    token_len (int): desired length of an output token<br/>    preds (int): number of predictions to output<br/>    num_heads(int): number of attention heads in MSA<br/>    Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module<br/>    depth (int): number of encoding blocks in the model<br/>    qkv_bias (bool): determines if the qkv layer learns an addative bias<br/>    qk_scale (NoneFloat): value to scale the queries and keys by; <br/>         if None, queries and keys are scaled by ``head_dim ** -0.5``<br/>    act_layer(nn.modules.activation): torch neural network layer class to use as activation<br/>    norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization<br/>  """<br/>  super().__init__()<br/><br/>  ## Defining Parameters<br/>  self.img_size = img_size<br/>  C, H, W = self.img_size<br/>  self.patch_size = patch_size<br/>  self.token_len = token_len<br/>  self.num_heads = num_heads<br/>  self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul<br/>  self.depth = depth<br/><br/>  ## Defining Patch Embedding Module<br/>  self.patch_tokens = Patch_Tokenization(img_size,<br/>           patch_size,<br/>           token_len)<br/><br/>  ## Defining ViT Backbone<br/>  self.backbone = ViT_Backbone(preds,<br/>         self.token_len,<br/>         self.num_heads,<br/>         self.Encoding_hidden_chan_mul,<br/>         self.depth,<br/>         qkv_bias,<br/>         qk_scale,<br/>         act_layer,<br/>         norm_layer)<br/>  ## Initialize the Weights<br/>  self.apply(self._init_weights)<br/><br/> def _init_weights(self, m):<br/>  """ Initialize the weights of the linear layers &amp; the layernorms<br/>  """<br/>  ## For Linear Layers<br/>  if isinstance(m, nn.Linear):<br/>   ## Weights are initialized from a truncated normal distrobution<br/>   timm.layers.trunc_normal_(m.weight, std=.02)<br/>   if isinstance(m, nn.Linear) and m.bias is not None:<br/>    ## If bias is present, bias is initialized at zero<br/>    nn.init.constant_(m.bias, 0)<br/>  ## For Layernorm Layers<br/>  elif isinstance(m, nn.LayerNorm):<br/>   ## Weights are initialized at one<br/>   nn.init.constant_(m.weight, 1.0)<br/>   ## Bias is initialized at zero<br/>   nn.init.constant_(m.bias, 0)<br/>   <br/> @torch.jit.ignore ##Tell pytorch to not compile as TorchScript<br/> def no_weight_decay(self):<br/>  """ Used in Optimizer to ignore weight decay in the class token<br/>  """<br/>  return {'cls_token'}<br/><br/> def forward(self, x):<br/>  x = self.patch_tokens(x)<br/>  x = self.backbone(x)<br/>  return x</span></pre><p id="a5b5" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">In the <em class="nl">ViT Model</em>, the <em class="nl">img_size</em>, <em class="nl">patch_size</em>, and <em class="nl">token_len</em> define the <em class="nl">Patch Tokenization</em> module.</p><p id="5276" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The <em class="nl">num_heads</em>,<em class="nl"> Encoding_hidden_channel_mul</em>,<em class="nl"> qkv_bias</em>,<em class="nl"> qk_scale</em>, and <em class="nl">act_layer</em> parameters define the <em class="nl">Encoding Bock</em> modules. The <em class="nl">act_layer</em> can be any <code class="cx rd re rf qr b">torch.nn.modules.activation</code>⁷ layer. The <em class="nl">depth</em> parameter determines how many encoding blocks are in the model.</p><p id="1ab7" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The <em class="nl">norm_layer</em> parameter sets the norm for both within and outside of the <em class="nl">Encoding Block</em> modules. It can be chosen from any <code class="cx rd re rf qr b">torch.nn.modules.normalization</code>⁸ layer.</p><p id="b6c9" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">The <em class="nl">_init_weights</em> method comes from the T2T-ViT³ code. This method could be deleted to initiate all learned weights and biases randomly. As implemented, the weights of linear layers are initialized as a truncated normal distribution; the biases of linear layers are initialized as zero; the weights of normalization layers are initialized as one; the biases of normalization layers are initialized as zero.</p><h1 id="caea" class="pu ow fq bf ox pv pw gv pb px py gy pf pz qa qb qc qd qe qf qg qh qi qj qk ql bk">Conclusion</h1><p id="c154" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">Now, you can go forth and train ViT models with a deep understanding of their mechanics! Below is a list of places to download code for ViT models. Some of them allow for more modifications of the model than others. Happy transforming!</p><ul class=""><li id="0bcf" class="mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk on oo op bk"><a class="af om" href="https://github.com/lanl/vision_transformers_explained" rel="noopener ugc nofollow" target="_blank">GitHub Repository</a> for this Article Series</li><li id="e798" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk"><a class="af om" href="https://github.com/google-research/vision_transformer" rel="noopener ugc nofollow" target="_blank">GitHub Repository</a> for <em class="nl">An Image is Worth 16x16 Words</em>²<br/> → Contains pretrained models and code for fine-tuning; does not contain model definitions</li><li id="dc3e" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk">ViT as implemented in <a class="af om" href="https://github.com/huggingface/pytorch-image-models" rel="noopener ugc nofollow" target="_blank">PyTorch Image Models</a> (<code class="cx rd re rf qr b">timm</code>)⁹<br/><code class="cx rd re rf qr b">timm.create_model('vit_base_patch16_224', pretrained=True)</code></li><li id="fb73" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk">Phil Wang’s <code class="cx rd re rf qr b">vit-pytorch</code> <a class="af om" href="https://github.com/lucidrains/vit-pytorch" rel="noopener ugc nofollow" target="_blank">package</a></li></ul><p id="846b" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">This article was approved for release by Los Alamos National Laboratory as LA-UR-23–33876. The associated code was approved for a BSD-3 open source license under O#4693.</p><h2 id="96ea" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Further Reading</h2><p id="f098" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">To learn more about transformers in NLP contexts, see</p><ul class=""><li id="f6b4" class="mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk on oo op bk">Transformers Explained Visually Part 1 Overview of Functionality: <a class="af om" rel="noopener" target="_blank" href="/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452">https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452</a></li><li id="c9f8" class="mp mq fq mr b gt oq mt mu gw or mw mx my os na nb nc ot ne nf ng ou ni nj nk on oo op bk">Transformers Explained Visually Part 2 How it Works, Step by Step: <a class="af om" rel="noopener" target="_blank" href="/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34">https://towardsdatascience.com/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34</a></li></ul><p id="a4c6" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">For a video lecture broadly about vision transformers, see</p><ul class=""><li id="663e" class="mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk on oo op bk">Vision Transformer and its Applications: <a class="af om" href="https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP" rel="noopener ugc nofollow" target="_blank">https://youtu.be/hPb6A92LROc?si=GaGYiZoyDg0PcdSP</a></li></ul><h2 id="dc9b" class="ov ow fq bf ox oy oz pa pb pc pd pe pf my pg ph pi nc pj pk pl ng pm pn po fw bk">Citations</h2><p id="17e6" class="pw-post-body-paragraph mp mq fq mr b gt pp mt mu gw pq mw mx my pr na nb nc ps ne nf ng pt ni nj nk fj bk">[1] Vaswani et al (2017).<em class="nl"> Attention Is All You Need. </em><a class="af om" href="https://doi.org/10.48550/arXiv.1706.03762" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.1706.03762</a></p><p id="b4ca" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[2] Dosovitskiy et al (2020). <em class="nl">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. </em><a class="af om" href="https://doi.org/10.48550/arXiv.2010.11929" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2010.11929</a></p><p id="4b33" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[3] Yuan et al (2021). <em class="nl">Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</em>. <a class="af om" href="https://doi.org/10.48550/arXiv.2101.11986" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.48550/arXiv.2101.11986</a><br/> → GitHub code: <a class="af om" href="https://github.com/yitu-opensource/T2T-ViT" rel="noopener ugc nofollow" target="_blank">https://github.com/yitu-opensource/T2T-ViT</a></p><p id="dde0" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[4] Luis Zuno (<a class="af om" href="http://twitter.com/ansimuz" rel="noopener ugc nofollow" target="_blank">@ansimuz</a>). <em class="nl">Mountain at Dusk Background. </em>License CC0: <a class="af om" href="https://opengameart.org/content/mountain-at-dusk-background" rel="noopener ugc nofollow" target="_blank">https://opengameart.org/content/mountain-at-dusk-background</a></p><p id="d0a1" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[5] PyTorch. <em class="nl">Unfold. </em><a class="af om" href="https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html#torch.nn.Unfold</a></p><p id="35a3" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[6] PyTorch. <em class="nl">Unsqueeze. </em><a class="af om" href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze</a></p><p id="5ac8" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[7] PyTorch. <em class="nl">Non-linear Activation (weighted sum, nonlinearity). </em><a class="af om" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</a></p><p id="0ca2" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[8] PyTorch. <em class="nl">Normalization Layers</em>. <a class="af om" href="https://pytorch.org/docs/stable/nn.html#normalization-layers" rel="noopener ugc nofollow" target="_blank">https://pytorch.org/docs/stable/nn.html#normalization-layers</a></p><p id="7f87" class="pw-post-body-paragraph mp mq fq mr b gt ms mt mu gw mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk fj bk">[9] Ross Wightman. <em class="nl">PyTorch Image Models. </em><a class="af om" href="https://github.com/huggingface/pytorch-image-models" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/pytorch-image-models</a></p></div></div></div></div>    
</body>
</html>