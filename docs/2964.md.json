["```py\nmlflow server --host 127.0.0.1 --port 8080\n```", "```py\nimport mlflow\n\nmlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n\ndef get_best_model(experiment_name, scoring_metric):\n  \"\"\"\n  Retrieves the model from MLflow logged models in a given experiment \n  with the best scoring metric.\n\n  Args:\n      experiment_name (str): Name of the experiment to search.\n      scoring_metric (str): f1_score is used in this example\n\n  Returns:\n      model_uri: The model path with the best F1 score, \n                                  or None if no model or F1 score is found.\n      artifcat_uri: The path for the artifacts for the best model\n  \"\"\"\n  experiment = mlflow.get_experiment_by_name(experiment_name)\n\n  # Extract the experiment ID\n  if experiment:\n      experiment_id = experiment.experiment_id\n      print(f\"Experiment ID for '{experiment_name}': {experiment_id}\")\n  else:\n      print(f\"Experiment '{experiment_name}' not found.\")\n\n  client = mlflow.tracking.MlflowClient()\n\n  # Find runs in the specified experiment\n  runs = client.search_runs(experiment_ids=experiment_id)\n\n  # Initialize variables for tracking\n  best_run = None\n  best_score = -float(\"inf\")  # Negative infinity for initial comparison\n\n  for run in runs:\n      try:\n          run_score = float(run.data.metrics.get(scoring_metric, 0))  # Get F1 score from params\n          if run_score > best_score:\n              best_run = run\n              best_score = run_score\n              Model_Path = best_run.data.tags.get(\"Model_Type\")\n\n      except (KeyError):  # Skip if score not found or error occurs\n          pass\n\n  # Return the model version from the run with the best F1 score (if found)\n  if best_run:\n\n      model_uri = f\"runs:/{best_run.info.run_id}/{Model_Path}\"\n      artifact_uri = f\"mlflow-artifacts:/{experiment_id}/{best_run.info.run_id}/artifacts\"\n      print(f\"Best Score found for {scoring_metric} for experiment: {experiment_name} is {best_score}\")\n      print(f\"Best Model found for {scoring_metric} for experiment: {experiment_name} is {Model_Path}\")\n      return model_uri, artifact_uri\n\n  else:\n      print(f\"No model found with logged {scoring_metric} for experiment: {experiment_name}\")\n      return None\n\nExperiment_Name = 'Imbalanced_Bank_Dataset'\nbest_model_uri, best_artifact_uri = get_best_model(Experiment_Name, \"f1_score\")\n\nif best_model_uri:\n  loaded_model = mlflow.sklearn.load_model(best_model_uri)\n```", "```py\nimport numpy as np\nimport pandas as pd\nimport os\n\ny_label_column = \"Personal Loan\"\n\ndef y_label_encoding (label):\n\n    try:\n\n        if label == 1:\n            return 1\n        elif label == 0:\n            return 0\n        elif label == 'Yes':\n            return 1\n        elif label == 'No':\n            return 0\n        else:\n            print(f\"Invalid label: {label}. Only 'Yes/1' or 'No/0' are allowed.\")\n    except:\n        print('Exception Raised')\n\ndef df_splitting(df):\n\n    prediction_columns = ['Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\\\n                          'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\\\n                          'CD Account', 'Online', 'CreditCard']\n    y_test = df[y_label_column].apply(y_label_encoding)\n    X_test = df[prediction_columns].drop(columns=y_label_column)\n\n    return X_test, y_test\n\n\"\"\"\n\nload_prediction_data function should refer to the final dataset used for training. The function is not provided here\n\n\"\"\"\n\ndf_pred = load_prediction_data (best_artifact_uri) ##loads dataset into a dataframe\ndf_pred['Probability'] = [x[1] for x in loaded_model.predict_proba(df_splitting(df_pred)[0])]\ndf_pred = df_pred.sort_values(by='Probability', ascending=False)\ndf_potential_cust = df_pred[(df_pred[y_label_column]==0) & (df_pred['Probability']> 0.5)]\nprint(f'Total customers: {df_pred.shape[0]}')\ndf_pred = df_pred[~((df_pred[y_label_column]==0) & (df_pred['Probability']> 0.5))]\nprint(f'Remaining customers: {df_pred.shape[0]}')\ndf_potential_cust\n```", "```py\nimport shap\n\nexplainer = shap.Explainer(loaded_model, df_pred)\nShap_explainer = explainer(df_pred)\nshap.plots.scatter(Shap_explainer[:, \"Income\"], color=Shap_explainer[:, \"Personal Loan\"])\n```", "```py\nshap.plots.scatter(Shap_explainer[:, \"Family\"], color=Shap_explainer[:,'Personal Loan'])\n```", "```py\nshap.plots.scatter(Shap_explainer[:, \"Education\"], color=Shap_explainer[:,'Personal Loan'])\n```", "```py\nX_test = df_splitting(df_pred)[0] ## Keeping only the columns used for prediction\nexplainer = shap.Explainer(loaded_model.predict, X_test) \nShap_explainer = explainer(X_test)\ndf_Shap_values = pd.DataFrame(Shap_explainer.values, columns=X_test.columns)\ndf_Shap_values.to_csv('Credit_Card_Fraud_Shap_Values.csv', index=False)\n```", "```py\ndef Get_Highest_SHAP_Values (row, no_of_values = 1):\n\n    if row.sum() < 0:\n        top_values = row.nsmallest(no_of_values)\n    else:\n        top_values = row.nlargest(no_of_values)\n    return [f\"{col}: {val}\" for col, val in zip(top_values.index, top_values)]\n\ndef read_orig_data_categorized(categorized_filename, shap_filename = ''):\n\n    df = pd.read_csv(categorized_filename)\n    if shap_filename!= '':\n        df_shap = pd.read_csv(shap_filename)\n        df['Most Important Features'] = df_shap.apply(lambda row: Get_Highest_SHAP_Values(row, no_of_values = 1), axis=1)\n\n    return df\n\ndef Column_name_changes (column_description, df):\n\n    df_description = pd.read_excel(column_description, sheet_name='Description',skiprows=6, usecols=[1,2])\n    df_description.replace('#','No of ', inplace=True, regex=True)\n    df_description.replace('\\(\\$000\\)','', inplace=True, regex=True)\n    df_description.loc[df_description['Unnamed: 1']=='Education','Unnamed: 2'] = 'Education Level'\n    mapping_dict = dict(zip(df_description['Unnamed: 1'], df_description['Unnamed: 2']))\n    df = df.rename(columns=mapping_dict)\n\n    return df\n\nOriginal_Categorized_Dataset = r'Bank_Personal_Loan_Modelling_Semantic.csv' ## Dataset with more description of the values  sorted in the same way as df_pred and df_Shap_values\nShap_values_Dataset = r'Credit_Card_Fraud_Shap_Values.csv' ## Shap values dataset \ncolumn_description = r'Bank_Personal_Loan_Modelling.xlsx' ## Original Bank Loan dataset with the Description Sheet\n\ndf_main = read_orig_data_categorized(Original_Categorized_Dataset, Shap_values_Dataset)\ndf_main = df_main.drop(columns=['ID','ZIP Code'])\ndf_main = Column_name_changes(column_description, df_main)\ndf_main.sample(5)\n```", "```py\ny_label_column = 'Did this customer accept the personal loan offered in the last campaign?'\ndf_main_true_cases = df_main[df_main[y_label_column]==\"Yes\"].reset_index(drop=True)\ndf_main_false_cases = df_main[df_main[y_label_column]==\"No\"].reset_index(drop=True)\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\ndef df_to_text(row):\n\n    text = ''\n    for col in row.index:\n        text += f\"\"\"{col}: {row[col]},\"\"\"\n    return text\n\ndef generating_embeddings(df):\n\n    sentences = df.apply(lambda row: df_to_text(row), axis=1).tolist()\n    model = SentenceTransformer(r\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n    output = model.encode(sentences=sentences,\n            show_progress_bar=True,\n            normalize_embeddings=True)\n    df_embeddings = pd.DataFrame(output)\n\n    return df_embeddings\n\ndf_embedding_all = generating_embeddings(df_main)\ndf_embedding_false_cases = generating_embeddings(df_main_false_cases)\ndf_embedding_true_cases = generating_embeddings(df_main_true_cases)\n```", "```py\nimport faiss\n\ndef generating_index(df_embeddings):\n\n    vector_dimension = df_embeddings.shape[1]\n    index = faiss.IndexFlatL2(vector_dimension)\n    faiss.normalize_L2(df_embeddings.values)\n    index.add(df_embeddings.values)\n\n    return index\n\ndef vector_search(index, df_search, df_original, k=1):\n\n    sentences = df_search.apply(lambda row: df_to_text(row), axis=1).tolist()\n    model = SentenceTransformer(r\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n    output = model.encode(sentences=sentences,\n            show_progress_bar=False,\n            normalize_embeddings=True)\n    search_vector = output\n    faiss.normalize_L2(search_vector)\n    distances, ann = index.search(search_vector, k=k)\n    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n    df_results = pd.merge(results, df_original, left_on='ann', right_index= True)\n\n    return df_results\n\ndef cluster_search(index, df_search, df_original, k=1):\n\n    df_temp = pd.DataFrame()\n    for i in range(0,len(df_search)):\n        df_row_search = df_search.iloc[i:i+1].values\n        df_temp = pd.concat([df_temp,vector_search_with_embeddings(df_row_search, df_original, index, k=k)])\n    df_temp = df_temp.sort_values(by='distances')\n    return df_temp\n\ndef vector_search_with_embeddings(search_vector, df_original, index, k=1):\n\n    faiss.normalize_L2(search_vector)\n    distances, ann = index.search(search_vector, k=k)\n    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})\n    df_results = pd.merge(results, df_original, left_on='ann', right_index= True)\n\n    return df_results\n\nindex_all = generating_index(df_embedding_all)\nindex_false_cases = generating_index(df_embedding_false_cases)\nindex_true_cases = generating_index(df_embedding_true_cases)\n\ndf_results = cluster_search(index_false_cases, df_embedding_true_cases, df_main_false_cases, k=1)\ndf_results['Most Important Features'] = [x[0] for x in df_results['Most Important Features'].values]\ndf_results ['Tipping_Feature'] = [x[0] for x in df_results['Most Important Features'].str.split(':')]\ndf_results = df_results.drop_duplicates(subset=['ann'])\ndf_results.head(10)\n```", "```py\ndf_trial_customers = df_pred[df_pred['Personal Loan']==0].iloc[0:10]\ndf_trial_customers\n```", "```py\nfeatures_list = ['Education', 'Family']\nfeatures_list = ('|').join(features_list)\ndf_list_A_Sim_Search = df_results[df_results['Tipping_Feature'].str.contains(features_list, case=False)].head(10)\ndf_list_A_Sim_Search\n```", "```py\ndef main_index_search(results_df, df_given_embeddings, df_original, search_index):\n\n    df_temp = pd.DataFrame()\n    for i in range(0,len(results_df)):\n        index_number = results_df['ann'].iloc[i]\n        df_row_search = df_given_embeddings.iloc[index_number:index_number+1].values\n        df_temp = pd.concat([df_temp,vector_search_with_embeddings(df_row_search, df_original, search_index, k=1)])\n\n    return df_temp\n\ndf_list_A_Sim_Search_pred = pd.concat([(main_index_search(df_list_A_Sim_Search, df_embedding_false_cases, df_pred, index_all).drop(columns=['distances','ann'])),\\\n                    df_list_A_Sim_Search ['Tipping_Feature']], axis=1).reset_index(drop=True)\ndf_list_A_Sim_Search_pred\n```", "```py\ndf_list_B_Probabilities = df_pred.copy().reset_index(drop=True)\ndf_list_B_Probabilities['Tipping_Feature'] = df_Shap_values.apply(lambda row: Get_Highest_SHAP_Values(row, no_of_values = 1), axis=1)\ndf_list_B_Probabilities['Tipping_Feature'] = [x[0] for x in df_list_B_Probabilities['Tipping_Feature'].values]\ndf_list_B_Probabilities ['Tipping_Feature'] = [x[0] for x in df_list_B_Probabilities['Tipping_Feature'].str.split(':')]\ndf_list_B_Probabilities = df_list_B_Probabilities[(df_list_B_Probabilities['Personal Loan']==0) & \\\n    (df_list_B_Probabilities['Tipping_Feature'].str.contains(features_list, case=False))].head(10)\ndf_list_B_Probabilities\n```", "```py\ndef finding_max(df):\n    all_max_values = pd.DataFrame(df.max()).T\n\n    return all_max_values\n\ndef finding_min(df):\n    all_min_values = pd.DataFrame(df.min()).T\n\n    return all_min_values\n\ndef grid_search(row, min_value, max_value, increment, tipping_feature):\n\n    row[tipping_feature] = min_value\n    row['New_Probability'] = [x[1] for x in loaded_model.predict_proba(row_splitting(row).convert_dtypes())][0]\n\n    while (row['New_Probability']) < 0.5:\n\n        if row[tipping_feature] == max_value:\n            row['Tipping_Value'] = 'Max Value Reached'\n            break\n\n        else:\n            row[tipping_feature] = row[tipping_feature] + increment\n            row['Tipping_Value'] = row[tipping_feature]\n            row['New_Probability'] = [x[1] for x in loaded_model.predict_proba(row_splitting(row).convert_dtypes())][0]\n\n    return row\n\ndef row_splitting(row):\n    prediction_columns = ['Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\\\n                          'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\\\n                          'CD Account', 'Online', 'CreditCard']\n    X_test = row.to_frame().transpose()\n    X_test = X_test[prediction_columns].reset_index(drop=True)\n    X_test = X_test.drop(columns=y_label_column)\n\n    return X_test\n\ndef tipping_value(row, all_max_values, all_min_values):\n\n    tipping_feature = row['Tipping_Feature']\n    min_value = row[tipping_feature]\n    max_value = all_max_values[tipping_feature].values[0]\n    if tipping_feature == 'CCAvg':\n        increment = 0.2\n    else:\n        increment = 1\n    row = grid_search(row, min_value, max_value, increment, tipping_feature)\n    row ['Value_Difference'] = row[tipping_feature] - min_value\n    row ['Original_Value'] = min_value\n\n    return row\n\nmin_values = finding_min(df_pred)\nmax_values = finding_max(df_pred)\n\ndf_new_prob = df_list_B_Probabilities.apply(lambda row: tipping_value(row, max_values, min_values), axis=1)\ndf_new_prob\n```", "```py\ndf_new_prob = df_list_A_Sim_Search_pred.apply(lambda row: tipping_value(row, max_values, min_values), axis=1)\ndf_new_prob\n```"]