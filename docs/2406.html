<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Optimizing Inventory Management with Reinforcement Learning: A Hands-on Python Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Optimizing Inventory Management with Reinforcement Learning: A Hands-on Python Guide</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03">https://towardsdatascience.com/optimizing-inventory-management-with-reinforcement-learning-a-hands-on-python-guide-7833df3d25a6?source=collection_archive---------3-----------------------#2024-10-03</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="a5af" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A complete guide on how to apply the Q-Learning method in Python to optimize inventory management and reduce costs</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Peyman Kor" class="l ep by dd de cx" src="../Images/33f92f508120a56ebcc05c2aca7be3c4.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*Om5LrB-tD6uKrshS1jX9jg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@peymankor?source=post_page---byline--7833df3d25a6--------------------------------" rel="noopener follow">Peyman Kor</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7833df3d25a6--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">13 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 3, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/81c435aa8f9c4f70d2abfcefea46f469.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*coNEBjFK73jwJApq"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@petrebels?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Petrebels</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="55b9" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Inventory Management — What Problem Are We Solving?</h1><p id="a0ee" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Imagine you are managing a bike shop. Every day, you need to decide how many bikes to order from your supplier. If you order too many, you incur high holding costs (cost of storing bikes overnight); if you order too few, you might miss out on potential sales. Here, the challenge is to develop a (ordering) strategy that balances these trade-offs optimally. Inventory management is crucial in various industries, where the goal is to determine the optimal quantity of products to order periodically to maximize profitability.</p><p id="c2cb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Why Reinforcement Learning for Inventory Management?</strong></p><p id="2f1f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Previously, we discussed approaching this problem using Dynamic Programming (DP) with the Markov Decision Process (MDP) <a class="af nc" href="https://medium.com/towards-artificial-intelligence/inventory-optimization-with-dynamic-programming-in-less-than-100-lines-of-python-code-ab1cc58ef34c" rel="noopener">Here</a>. However, the DP approach requires a complete model of the environment (in this case, we need to know the probability distribution of demand), which may not always be available or practical.</p><blockquote class="pa"><p id="d4b8" class="pb pc fq bf pd pe pf pg ph pi pj ou dx">Here, the Reinforcement Learning (RL) approach is presented, which overcomes that challenge by following a “data-driven” approach.</p></blockquote><p id="8058" class="pw-post-body-paragraph nz oa fq ob b go pk od oe gr pl og oh oi pm ok ol om pn oo op oq po os ot ou fj bk">The goal is to build a “data-driven” agent that learns the best policy (how much to order) through interacting with the environment (uncertainty). The RL approach removes the need for prior knowledge about the model of the environment. This post explores the RL approach, specifically Q-learning, to find the optimal inventory policy.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b28b" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">How to Frame the Inventory Management Problem?</h1><p id="0977" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Before diving into the Q-learning method, it’s essential to understand the basics of the inventory management problem. At its core, inventory management is a sequential decision-making problem, where decisions made today affect the outcomes and choices available tomorrow. Let’s break down the key elements of this problem: the <em class="qc">state</em>, <em class="qc">uncertainty</em>, and <em class="qc">recurring decisions</em>.</p><p id="fede" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">State</strong>: What’s the Current Situation?</p><p id="6c73" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the context of a bike shop, the state represents the current situation regarding inventory. It’s defined by two key components:</p><p id="e3c1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">α (Alpha): The number of bikes you currently have in the store. (referred to as On-Hand Inventory)</p><p id="e037" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">β (Beta): The number of bikes that you ordered yesterday and are expected to arrive tomorrow morning (<em class="qc">36 hours delivery lead time</em>). These bikes are still in transit. (referred to as On-Order Inventory)</p><p id="00c5" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Together, (α,β) form the state, which gives a snapshot of your inventory status at any given moment.</p><p id="ffdb" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Uncertainty</strong>: What Could Happen?</p><p id="1afd" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Uncertainty in this problem arises from the random demand for bikes each day. You don’t know exactly how many customers will walk in and request a bike, making it challenging to predict the exact demand.</p><p id="b300" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Decisions</strong>: How Many Items Should you Order Every Day?</p><p id="c311" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As the bike shop owner, you face a recurring decision every day: How many bikes should you order from the supplier? . Your decision needs to account for both the current state of your inventory (α,β) and also the uncertainty in customer demand for the following day.</p><p id="74b3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">A typical 24-hour cycle for managing your bike shop’s inventory is as follows:</p><p id="4559" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">6 PM: Observe the current state St:(α,β) of your inventory. (<strong class="ob fr">State</strong>)</p><p id="5ffa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">6 PM: Make the decision on how many new bikes to order. (<strong class="ob fr">Decision</strong>)</p><p id="7fb8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">6 AM: Receive the bikes you ordered 36 hours ago.</p><p id="fcad" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">8 AM: Open the store to customers.</p><p id="1a5d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">8 AM — 6 PM: Experience customer demand throughout the day. (<strong class="ob fr">Uncertainty</strong>)</p><p id="f516" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">6 PM: Close the store and prepare for the next cycle.</p><p id="c806" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">A graphical representation of the inventory management process is shown below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qd"><img src="../Images/0c833c676bd2f78f9574bc6accf7850c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HSUODvSE29AeFhTWObcIw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A typical 24-hour cycle for inventory management — image source: Author</figcaption></figure></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9bd9" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">What is Reinforcement Learning?</h1><p id="bff1" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Reinforcement Learning (RL) is a data-driven method that focuses on learning how to make sequences of decisions (following a policy) to maximize a cumulative reward. It's similar to how humans and animals learn what action to take through trial and error. In the context of inventory management, RL can be used to learn the optimal ordering policy that minimizes the total cost of inventory management.</p><p id="9009" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The key components of the RL approach are:</p><p id="c612" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Agent</strong>: The decision-maker who interacts with the environment.</p><p id="ec87" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Environment</strong>: The external system with which the agent interacts. In this case, the environment is the random customer demand.</p><p id="1a1c" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">State</strong>: The current situation or snapshot of the environment.</p><p id="dc12" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Action</strong>: The decision or choice made by the agent.</p><p id="b6d3" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Reward</strong>: The feedback signal that tells the agent how well it’s doing.</p><p id="4164" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The goal of the agent (decision-maker) is to learn the optimal policy, which is a mapping from states to actions that maximize the cumulative reward over time.</p><blockquote class="pa"><p id="267e" class="pb pc fq bf pd pe pf pg ph pi pj ou dx">In the context of inventory management, the policy tells the agent how many bikes to order each day based on the current inventory status and the uncertainty in customer demand.</p></blockquote></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5252" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">Implementing Reinforcement Learning for Inventory Optimization Problem</h1><p id="e638" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk"><strong class="ob fr">Q-learning</strong> is a model-free reinforcement learning algorithm that learns the optimal action-selection policy for any given state. Unlike the DP approach, which requires a complete model of the environment, Q-learning learns directly from the interaction with the environment (here, uncertainty and the reward it gets) by updating a Q-table.</p><p id="3752" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">The key components of Q-Learning</strong></p><p id="3e08" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In our case, the agent is the decision-maker (the bike shop owner), and the environment is the demand from customers. The state is represented by the current inventory levels (alpha, beta), and the action is how many bikes to order.<strong class="ob fr"> The reward is the cost associated with both holding inventory and missing out on sales</strong>. Q-Table is a table that stores the expected future rewards for each state-action pair.</p><p id="1b88" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Initialization of Q Table</strong></p><p id="cca9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In this work, the Q-table is initialized as a dictionary named Q. States are represented by tuples (alpha, beta), where: alpha is the number of items in stock (on-hand inventory). beta is the number of items on order (on-order inventory).</p><p id="84b6" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Actions are <em class="qc">possible inventory order quantities </em>that can be taken in each state. For each state (alpha, beta), the possible actions depend on how much space is left in the inventory (remaining capacity = Inventory Capacity — (alpha + beta)). The restriction is that the number of items ordered cannot exceed the remaining capacity of the inventory.</p><p id="0340" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The schematic design of the Q value is visualized below:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qe"><img src="../Images/06e67d083e0092ae4d959b46d288342d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKM7GyBD-QsZvs2JTQAEhw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">A schematic design of the Q dictionary is visualized — Image source: Author</figcaption></figure><p id="7576" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The Q dictionary can be initialized as:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="0ca7" class="qj ne fq qg b bg qk ql l qm qn">def initialize_Q(self):<br/>    # Initialize the Q-table as a dictionary<br/>    Q = {}<br/>    for alpha in range(self.user_capacity + 1):<br/>        for beta in range(self.user_capacity + 1 - alpha):<br/>        state = (alpha, beta)<br/>        Q[state] = {}<br/>        max_action = self.user_capacity - (alpha + beta)<br/>        for action in range(max_action + 1):<br/>            Q[state][action] = np.random.uniform(0, 1)  # Small random values<br/>    return Q</span></pre><p id="3b0b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">As the above code shows, Q-values (Q[state][action]) are initialized with small random values to encourage exploration.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7bd0" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">The Q-Learning Algorithm</h1><p id="492a" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The Q-learning method updates a table of state-action pairs based on rewards from the environment (here, interacting with the environment comes). Here’s how the algorithm works in three steps:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qo"><img src="../Images/b271d953abb3487d58b4688cb271b868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11Et_deLlmsX8pjCwImPGA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Q-Learning Equation — Image Source: Author</figcaption></figure><p id="923b" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Where s is the current state, a is the action taken, s’ is the next state, ( α ) is the learning rate. and ( γ ) is the discount factor.</p><p id="d19d" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">We breakdown the equation, and rewrote it in three parts down here:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/9b31dd86a62fd52609a518a0ee87276d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFpaSa0QLAM5Yca63DrggA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Q-Learning Equation — Image Source: Author</figcaption></figure><p id="1a9f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The translation of the above equations to Python code is as follows:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="247b" class="qj ne fq qg b bg qk ql l qm qn">def update_Q(self, state, action, reward, next_state):<br/>        # Update the Q-table based on the state, action, reward, and next state<br/>        best_next_action = max(self.Q[next_state], key=self.Q[next_state].get)<br/><br/>        # reward + gamma * Q[next_state][best_next_action]<br/>        td_target = reward + self.gamma * self.Q[next_state][best_next_action]<br/><br/>        # td_target - Q[state][action]<br/>        td_error = td_target - self.Q[state][action]<br/><br/>        # Q[state][action] = Q[state][action] + alpha * td_error<br/>        self.Q[state][action] += self.alpha * td_error</span></pre><p id="e118" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">At the above function, the equivalent equation of each line has been shown as a comment on top of each line.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2a7d" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">Simulating Transitions and Rewards in Q-Learning for Inventory Optimization</h1><p id="7040" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The current state is represented by a tuple (alpha, beta), where: alpha is the current on-hand inventory (items in stock), beta is the current on-order inventory (items ordered but not yet received), init_inv calculates the total initial inventory by summing alpha and beta.</p><p id="a1da" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Then, we need to simulate customer demand using Poisson distribution with lambda value “self.poisson_lambda”. Here, the demand shows the randomness of customer demand:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="feae" class="qj ne fq qg b bg qk ql l qm qn">alpha, beta = state<br/>init_inv = alpha + beta<br/>demand = np.random.poisson(self.poisson_lambda)</span></pre><p id="5052" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Note</strong>: Poisson distribution is used to model the demand, which is a common choice for modeling random events like customer arrivals. However, we can either train the model with historical demand data or live interaction with environment in real time. In its core, reinforcement learning is about learning from the data, and it does not require prior knowledge of a model.</p><p id="34f8" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now, the “next alpha” which is in-hand inventory can be written as max(0,init_inv-demand). What that means is that if demand is more than the initial inventory, then the new alpha would be zero, if not, init_inv-demand.</p><p id="5e05" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The <strong class="ob fr">cost</strong> comes in two parts. <strong class="ob fr">Holding cost</strong>: is calculated by multiplying the number of bikes in the store by the per-unit holding cost. Then, we have another cost, which is <strong class="ob fr">stockout cost</strong>. It is a cost that we need to pay for the cases of missed demand. These two parts form the “reward” which we try to maximize using reinforcement learning method.( a better way to put is we want to minimize the cost, so we maximize the reward).</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="05c1" class="qj ne fq qg b bg qk ql l qm qn">new_alpha = max(0, init_inv - demand)<br/>holding_cost = -new_alpha * self.holding_cost<br/>stockout_cost = 0<br/><br/>if demand &gt; init_inv:<br/>    <br/>    stockout_cost = -(demand - init_inv) * self.stockout_cost<br/>        <br/>reward = holding_cost + stockout_cost<br/>next_state = (new_alpha, action)</span></pre><h2 id="809f" class="qq ne fq bf nf qr qs qt ni qu qv qw nl oi qx qy qz om ra rb rc oq rd re rf rg bk">Exploration — Exploitation in Q-Learning</h2><p id="4926" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">Choosing action in the Q-learning method involves some degree of exploration to get an overview of the Q value for all the states in the Q table. To do that, at every action chosen, there is an epsilon chance that we take an exploration approach and “randomly” select an action, whereas, with a 1-ϵ chance, we take the best action possible from the Q table.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="2f83" class="qj ne fq qg b bg qk ql l qm qn">def choose_action(self, state):<br/><br/>        # Epsilon-greedy action selection<br/>    if np.random.rand() &lt; self.epsilon:<br/>        <br/>          return np.random.choice(self.user_capacity - (state[0] + state[1]) + 1)<br/>    <br/>    else:<br/>        <br/>        return max(self.Q[state], key=self.Q[state].get)</span></pre><h2 id="0274" class="qq ne fq bf nf qr qs qt ni qu qv qw nl oi qx qy qz om ra rb rc oq rd re rf rg bk">Training RL Agent</h2><p id="f07b" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The training of the RL agent is done by the “train” function, and it is follow as: First, we need to initialize the Q (empty dictionary structure). Then, experiences are collected in each batch (self.batch.append((state, action, reward, next_state))), and the Q table is updated at the end of each batch (self.update_Q(self.batch)). The number of episodes is limited to “max_actions_per_episode” in each batch. The number of episodes is the number of times the agent interacts with the environment to learn the optimal policy.</p><p id="ec50" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Each episode starts with a randomly assigned state, and while the number of actions is lower than max_actions_per_episode, the collecting data for that batch continues.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="3087" class="qj ne fq qg b bg qk ql l qm qn">def train(self):<br/><br/>        self.Q = self.initialize_Q()  # Reinitialize Q-table for each training run<br/><br/>        for episode in range(self.episodes):<br/>            alpha_0 = random.randint(0, self.user_capacity)<br/>            beta_0 = random.randint(0, self.user_capacity - alpha_0)<br/>            state = (alpha_0, beta_0)<br/>            #total_reward = 0<br/>            self.batch = []  # Reset the batch at the start of each episode<br/>            action_taken = 0<br/>            while action_taken &lt; self.max_actions_per_episode:<br/>                action = self.choose_action(state)<br/>                next_state, reward = self.simulate_transition_and_reward(state, action)<br/>                self.batch.append((state, action, reward, next_state))  # Collect experience<br/>                state = next_state<br/>                action_taken += 1<br/>            <br/>            self.update_Q(self.batch)  # Update Q-table using the batch</span></pre></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bbb9" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">Example Case and Results</h1><p id="6d56" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">This is example case is on how to pull together all above codes, and see how the Q-learning agent learns the optimal policy for inventory management. Here, <em class="qc">user_capicty</em> (capacity of storage) is 10, which is the total number of items that inventory can hold (capacity). Then, the <em class="qc">poisson_lambda</em> is the lambda term in the demand distribution, which has a value of 4. Holding costs is 8, which is the cost of holding an item in inventory overnight, and stockout cost, which is the cost of missed demand (assume that the item had a customer that day and the price of the item was, but you did not have the item in your inventory) is 10. <em class="qc">gamma</em> value lower than one is needed in the equation to discount the future reward (0.9), where <em class="qc">alpha</em> (learning rate ) is 0.1. The <em class="qc">epsilon</em> term is the term control exploration-exploitation dilemma. The episodes are 1000, and each batch consists of 1000 (max actions per episode).</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="9d56" class="qj ne fq qg b bg qk ql l qm qn"># Example usage:<br/>user_capacity = 10<br/>poisson_lambda = 4<br/>holding_cost = 8<br/>stockout_cost = 10<br/>gamma = 0.9<br/>alpha = 0.1<br/>epsilon = 0.1<br/>episodes = 1000<br/>max_actions_per_episode = 1000</span></pre><p id="78f9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Having defined these initial parameters of the model, we can define the ql Python class, then use the class to train, and then use the module “get_optimal_policy()” to get the optimal policy.</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="3cfe" class="qj ne fq qg b bg qk ql l qm qn"># Define the Class<br/>ql = QLearningInventory(user_capacity, poisson_lambda, holding_cost, stockout_cost, gamma, <br/>                        alpha, epsilon, episodes, max_actions_per_episode)<br/><br/># Train Agent<br/>ql.train()<br/><br/># Get the Optimal Policy<br/>optimal_policy = ql.get_optimal_policy()</span></pre><p id="bb7f" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Results</strong></p><p id="47db" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now that we have the policy found from the Q-learning method, we can visualize the results and see what they look like. The x-axis is states, which is a tuple of (alpha, beta), and the y-axis is the “Number of Order” found from Q-learning at each state.</p></div></div><div class="mr"><div class="ab cb"><div class="lm rh ln ri lo rj cf rk cg rl ci bh"><figure class="mm mn mo mp mq mr rn ro paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/6b23e9e1d2d2dd54cf9f112b6b4ab5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*bnZ3qlYeDcL1WIq6Dcfbhw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><em class="rp">Number of order (y-axis) for each state (x-axis) found from Q-learning policy — Image Source: Author</em></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5903" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">A couple of learnings can be gained by looking at the plot. First, as we go toward the right, we see that the number of orders decreases. When we go right, the alpha value increases (in-hand inventory), meaning we need to “order” less, as inventory in place can fulfill the demand. Secondly, When alpha is constant, with increasing beta, we lower the order of new sites. It can be understood that this is due to the fact that when “we have more item “on order” we do not need increase the orders.</p><p id="55ca" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Comparing the Q-Learning Policy to the BaseLine Policy</strong></p><p id="4f04" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">Now that we used Q-learning to find the policy (how many items to order in a given state), we can compare it to the baseline policy (a simple policy). The baseline policy is just to “order up to policy,” which simply means you look at the on-hand inventory and the on-order inventory and order up to “meet the target level.” We can write simple code to write this policy in Python format here:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="12fa" class="qj ne fq qg b bg qk ql l qm qn"># Create a simple policy<br/>def order_up_to_policy(state, user_capacity, target_level):<br/>    alpha, beta = state<br/>    max_possible_order = user_capacity - (alpha + beta)<br/>    desired_order = max(0, target_level - (alpha + beta))<br/>    return min(max_possible_order, desired_order)</span></pre><p id="9517" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">In the code, the <strong class="ob fr">target_level</strong> is the desired value we want to order for inventory. If target_level = user_capacity, then we are filling just to fulfill the inventory. First, we can compare the policies of these different methods. For each state, what will be the “number of orders” if we follow the Simple policy and the one from the Q-learning policy? In the figure below, we plotted the comparison of two policies.</p></div></div><div class="mr"><div class="ab cb"><div class="lm rh ln ri lo rj cf rk cg rl ci bh"><figure class="mm mn mo mp mq mr rn ro paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/f90efc18304e79a30955c981f2b915f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*guy3n-XuQpaDCmXqSodlrQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Comparing Ordering policy between Q-Learning and Simple Policy, for each state — Image Source : Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="59aa" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The simple policy is just to order so that it fulfills the inventory, where the Q-learning policy order is often lower than the simple policy order.</p><blockquote class="pa"><p id="7edd" class="pb pc fq bf pd pe pf pg ph pi pj ou dx"><strong class="al">This can be attributed to the fact that “poisson_lambda” here is 4, meaning the demand is much lower than the capacity of the inventory=10, therefore it is not optimal to order “high number of bicycle” as it has a high holding cost.</strong></p></blockquote><p id="216b" class="pw-post-body-paragraph nz oa fq ob b go pk od oe gr pl og oh oi pm ok ol om pn oo op oq po os ot ou fj bk">We can also compare the total cumulative rewards you can get when you apply both policies. To do that, we can use the <em class="qc">test_policy</em> function of “QLearningInventory” which was especially designed to evaluate policies:</p><pre class="mm mn mo mp mq qf qg qh bp qi bb bk"><span id="773f" class="qj ne fq qg b bg qk ql l qm qn">def test_policy(self, policy, episodes):<br/>        """<br/>        Test a given policy on the environment and calculate the total reward.<br/><br/>        Args:<br/>            policy (dict): A dictionary mapping states to actions.<br/>            episodes (int): The number of episodes to simulate.<br/><br/>        Returns:<br/>            float: The total reward accumulated over all episodes.<br/>        """<br/>        total_reward = 0<br/>        alpha_0 = random.randint(0, self.user_capacity)<br/>        beta_0 = random.randint(0, self.user_capacity - alpha_0)<br/>        state = (alpha_0, beta_0)  # Initialize the state<br/>        <br/>        for _ in range(episodes):<br/><br/>            action = policy.get(state, 0)<br/>            next_state, reward = self.simulate_transition_and_reward(state, action)<br/>            total_reward += reward<br/>            state = next_state<br/><br/>        return total_reward</span></pre><p id="9311" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The way the function works is it starts randomly with a new state (state = (alpha_0, beta_0); then for that state, you get action (number of order) for that state from policy, you act and see the reward, and next state, and the process continues as total number of episodes, while you collect the total reward.</p></div></div><div class="mr"><div class="ab cb"><div class="lm rh ln ri lo rj cf rk cg rl ci bh"><figure class="mm mn mo mp mq mr rn ro paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/c3d1c92a43d76dc68bb3566c90a66491.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*MoE2GobFYCtE2PKmLhBR_Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Total costs of manging Inventory, following Q-Learnng policy and Simple policy — Image Source Author</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="e338" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">The plot above compares the total cost of managing inventory when we follow the “Q-Learning” and “Simple Policy”. The aim is to mimimize the cost of running inventory. Since the ‘reward’ in our model represents this cost, we added total cost = -total reward.</p><blockquote class="pa"><p id="52f1" class="pb pc fq bf pd pe pf pg ph pi pj ou dx">Running the inventory with the Q-Learning policy will lead to lower costs compared to the Simple policy.</p></blockquote></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="f788" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">Code in GitHub</h1><p id="06a8" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">The full code for this blog can be found in the GitHub repository <a class="af nc" href="https://github.com/Peymankor/medium_blogs/blob/main/2024/08-Aug/RL-Inventory/main.py" rel="noopener ugc nofollow" target="_blank">here</a>.</p></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5abd" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">Summary and Main Takeaways</h1><p id="9d27" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">In this post, we worked on how reinforcement learning (Q-Learning specifically) can be used to optimize inventory management. We were able to develop a Q-learning algorithm that learns the optimal ordering policy through interaction with the environment (uncertainty). Here, the environment was the “random” demand of the customers (buyers of bikes), and the state was the current inventory status (alpha, beta). The Q-learning algorithm was able to learn the optimal policy that minimizes the total cost of inventory management.</p><p id="8db1" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk"><strong class="ob fr">Main Takeaways</strong></p><ol class=""><li id="54da" class="nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou rq rr rs bk"><strong class="ob fr">Q-Learning</strong>: A model-free reinforcement learning algorithm, Q-learning, can be used to find the optimal inventory policy without requiring a complete model of the environment.</li><li id="f4bb" class="nz oa fq ob b go rt od oe gr ru og oh oi rv ok ol om rw oo op oq rx os ot ou rq rr rs bk"><strong class="ob fr">State Representation</strong>: The state in inventory management is represented by the current on-hand inventory and on-order inventory state = (α, β).</li><li id="3307" class="nz oa fq ob b go rt od oe gr ru og oh oi rv ok ol om rw oo op oq rx os ot ou rq rr rs bk"><strong class="ob fr">Cost Reduction</strong>: We can see that the Q-learning policy leads to lower costs compared to the simple policy of ordering up to capacity.</li><li id="8480" class="nz oa fq ob b go rt od oe gr ru og oh oi rv ok ol om rw oo op oq rx os ot ou rq rr rs bk"><strong class="ob fr">Flexibility</strong>: The Q-learning approach is quite flexible and can be applied to the case of we have past data of demand, or we can interact with the environment to learn the optimal policy.</li><li id="0180" class="nz oa fq ob b go rt od oe gr ru og oh oi rv ok ol om rw oo op oq rx os ot ou rq rr rs bk"><strong class="ob fr">Data-Driven Decisions</strong>: As we showed, the reinforcement learning (RL) approach does not require any prior knowledge on the model of environment , as it is learning from the data.</li></ol></div></div></div><div class="ab cb pp pq pr ps" role="separator"><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv pw"/><span class="pt by bm pu pv"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9800" class="nd ne fq bf nf ng px gq ni nj py gt nl nm pz no np nq qa ns nt nu qb nw nx ny bk">References</h1><p id="f1a9" class="pw-post-body-paragraph nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou fj bk">[1] A. Rao, T. Jelvis, Foundations of Reinforcement Learning with Applications in Finance (2022).</p><p id="a2ce" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[2] S. Sutton, A. Barto, Reinforcement Learning: An Introduction (2018).</p><p id="b112" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[3] W. B. Powell, Sequential Decision Analytics and Modeling: Modeling with Python (2022).</p><p id="6bb9" class="pw-post-body-paragraph nz oa fq ob b go ov od oe gr ow og oh oi ox ok ol om oy oo op oq oz os ot ou fj bk">[4] R. B. Bratvold, Making Good Decisions (2010).</p><h1 id="cef0" class="nd ne fq bf nf ng nh gq ni nj nk gt nl nm nn no np nq nr ns nt nu nv nw nx ny bk">Before you go! 🦸🏻‍♀️</h1><ol class=""><li id="de58" class="nz oa fq ob b go oc od oe gr of og oh oi oj ok ol om on oo op oq or os ot ou rq rr rs bk">If you found value in this article, please follow me on <a class="af nc" href="https://medium.com/@peymankor" rel="noopener">Medium</a> and <a class="af nc" href="https://www.linkedin.com/in/peyman-kor/" rel="noopener ugc nofollow" target="_blank">Linkedin</a> to keep updated with my latest posts/writings.</li><li id="d89f" class="nz oa fq ob b go rt od oe gr ru og oh oi rv ok ol om rw oo op oq rx os ot ou rq rr rs bk">Clap my article 50 times, that will really really help me out and boost this article to others.👏</li></ol></div></div></div></div>    
</body>
</html>