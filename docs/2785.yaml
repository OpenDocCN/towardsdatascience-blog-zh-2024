- en: 'Spoiler Alert: The Magic of RAG Does Not Come from AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/spoiler-alert-the-magic-of-rag-does-not-come-from-ai-8a0ed2ad4800?source=collection_archive---------0-----------------------#2024-11-17](https://towardsdatascience.com/spoiler-alert-the-magic-of-rag-does-not-come-from-ai-8a0ed2ad4800?source=collection_archive---------0-----------------------#2024-11-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why retrieval, not generation, makes RAG systems magical
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@frankw_usa?source=post_page---byline--8a0ed2ad4800--------------------------------)[![Frank
    Wittkampf](../Images/3dbd69f8ef648074fa170fac451645fd.png)](https://medium.com/@frankw_usa?source=post_page---byline--8a0ed2ad4800--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--8a0ed2ad4800--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--8a0ed2ad4800--------------------------------)
    [Frank Wittkampf](https://medium.com/@frankw_usa?source=post_page---byline--8a0ed2ad4800--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--8a0ed2ad4800--------------------------------)
    ·8 min read·Nov 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Quick POCs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most quick proof of concepts (POCs) which allow a user to explore data with
    the help of conversational AI simply blow you away. It feels like pure magic when
    you can all of a sudden talk to your documents, or data, or code base.
  prefs: []
  type: TYPE_NORMAL
- en: 'These POCs work wonders on small datasets with a limited count of docs. However,
    as with almost anything when you bring it to production, you quickly run into
    problems at scale. When you do a deep dive and you inspect the answers the AI
    gives you, you notice:'
  prefs: []
  type: TYPE_NORMAL
- en: Your agent doesn’t reply with complete information. It missed some important
    pieces of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your agent doesn’t reliably give the same answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your agent isn’t able to tell you how and where it got which information, making
    the answer significantly less useful
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It turns out that the **real magic in RAG** does not happen in the generative
    AI step, but in the process of retrieval and composition. Once you dive in, it’s
    pretty obvious why…
  prefs: []
  type: TYPE_NORMAL
- en: '** RAG = Retrieval Augmented Generation —* [*Wikipedia Definition of RAG*](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1546c06f10cd9e6f67fbd43ec17c1601.png)'
  prefs: []
  type: TYPE_IMG
- en: RAG process — Illustration
  prefs: []
  type: TYPE_NORMAL
- en: So, how does a RAG-enabled AI agent answer a question?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A quick recap of how a simple RAG process works:'
  prefs: []
  type: TYPE_NORMAL
- en: It all starts with a **query**. The user asked a question, or some system is
    trying to answer a question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A **search** is done with the query. Mostly you’d embed the query and do a similarity
    search, but you can also do a classic elastic search or a combination of both,
    or a straight lookup of information
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The search result is a set of **documents** (or document snippets, but let’s
    simply call them documents for now)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The documents and the essence of the query are combined into some easily readable
    **context** so that the AI can work with it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **AI interprets** the question and the documents and generates an answer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ideally this answer is **fact checked**, to see if the AI based the answer on
    the documents, and/or if it is appropriate for the audience
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Where’s the magic?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dirty little secret is that the essence of the RAG process is that you have
    to provide the answer to the AI (before it even does anything), so that it is
    able to give you the reply that you’re looking for.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: the work that the AI does (step 5) is **apply judgement, and properly articulate
    the answer**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the work that the engineer does (step 3 and 4) is **find the answer and compose
    it such that AI can digest it**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which is more important? The answer is, of course, it depends, because if judgement
    is the critical element, then the AI model does all the magic. But for an endless
    amount of business use cases, finding and properly composing the pieces that make
    up the answer, is the more important part.
  prefs: []
  type: TYPE_NORMAL
- en: What are the typical engineering problems to solve if you want proper RAG process?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first set of problems to solve when running a RAG process are the data ingestion,
    splitting, chunking, document interpretation issues. I’ve written about a few
    of these in [prior articles](https://medium.com/@frankw_usa), but am ignoring
    them here. For now let’s assume you have properly solved your data ingestion,
    you have a lovely vector store or search index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Duplication** — Even the simplest production systems often have duplicate
    documents. More so when your system is large, you have extensive users or tenants,
    you connect to multiple data sources, or you deal with versioning, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Near duplication** — Documents which largely contain the same data, but with
    minor changes. There are two types of near duplication:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: — Meaningful — E.g. a small correction, or a minor addition, e.g. a date field
    with an update
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '— Meaningless — E.g.: minor punctuation, syntax, or spacing differences, or
    just differences introduced by timing or intake processing'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Volume** — Some queries have a very large relevant response data set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data freshness vs quality** — Which snippets of the response data set have
    the most high quality content for the AI to use vs which snippets are most relevant
    from a time (freshness) perspective?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data variety** — How do we ensure a variety of search results such that the
    AI is properly informed?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query phrasing and ambiguity** — The prompt that triggered the RAG flow,
    might not be phrased in such a way that it yields the optimal result, or might
    even be ambiguous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response Personalization** — The query might require a different response
    based on who asks it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list goes on, but you get the gist.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sidebar: Don’t unlimited context windows solve this?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Short answer: no.'
  prefs: []
  type: TYPE_NORMAL
- en: The cost and performance impact of using extremely large context windows shouldn’t
    be underestimated (you easily 10x or 100x your per query cost), not including
    any follow up interaction that the user/system has.
  prefs: []
  type: TYPE_NORMAL
- en: However, putting that aside. Imagine the following situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We put Anne in room with a piece of paper. The paper says: *patient Joe: complex
    foot fracture.* Now we ask Anne, does the patient have a foot fracture? Her answer
    is “yes, he does”.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now we give Anne a hundred pages of medical history on Joe. Her answer becomes
    “well, depending on what time you are referring to, he had …”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now we give Anne thousands of pages on all the patients in the clinic…
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What you quickly notice, is that how we define the question (or the prompt in
    our case) starts to get very important. **The larger the context window, the more
    nuance the query needs.**
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, **the larger the context window,** **the universe of possible
    answers grows.** This can be a positive thing, but in practice, it’s a method
    that invites lazy engineering behavior, and is likely to reduce the capabilities
    of your application if not handled intelligently.
  prefs: []
  type: TYPE_NORMAL
- en: Suggested approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you scale a RAG system from POC to production, here’s how to address typical
    data challenges with specific solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Duplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Duplication is inevitable in multi-source systems. By using fingerprinting (hashing
    content), document IDs, or semantic hashing, you can identify exact duplicates
    at ingestion and prevent redundant content. However, consolidating metadata across
    duplicates can also be valuable; this lets users know that certain content appears
    in multiple sources, which can add credibility or highlight repetition in the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Near Duplication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Near-duplicate documents (similar but not identical) often contain important
    updates or small additions. Given that a minor change, like a status update, can
    carry critical information, freshness becomes crucial when filtering near duplicates.
    A practical approach is to use cosine similarity for initial detection, then retain
    the freshest version within each group of near-duplicates while flagging any meaningful
    updates.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Volume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a query returns a high volume of relevant documents, effective handling
    is key. One approach is a **layered strategy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theme Extraction**: Preprocess documents to extract specific themes or summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Top-k Filtering**: After synthesis, filter the summarized content based on
    relevance scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevance Scoring**: Use similarity metrics (e.g., BM25 or cosine similarity)
    to prioritize the top documents before retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach reduces the workload by retrieving synthesized information that’s
    more manageable for the AI. Other strategies could involve batching documents
    by theme or pre-grouping summaries to further streamline retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Data Freshness vs. Quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Balancing quality with freshness is essential, especially in fast-evolving
    datasets. Many scoring approaches are possible, but here’s a general tactic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Composite Scoring**: Calculate a quality score using factors like source
    reliability, content depth, and user engagement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recency Weighting**: Adjust the score with a timestamp weight to emphasize
    freshness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter by Threshold**: Only documents meeting a combined quality and recency
    threshold proceed to retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other strategies could involve scoring only high-quality sources or applying
    decay factors to older documents.
  prefs: []
  type: TYPE_NORMAL
- en: Data Variety
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring diverse data sources in retrieval helps create a balanced response.
    Grouping documents by source (e.g., different databases, authors, or content types)
    and selecting top snippets from each source is one effective method. Other approaches
    include scoring by unique perspectives or applying diversity constraints to avoid
    over-reliance on any single document or perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Query Phrasing and Ambiguity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ambiguous queries can lead to suboptimal retrieval results. Using the exact
    user prompt is mostly not be the best way to retrieve the results they require.
    E.g. there might have been an information exchange earlier on in the chat which
    is relevant. Or the user pasted a large amount of text with a question about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'To ensure that you use a refined query, one approach is to ensure that a RAG
    tool provided to the model asks it to rephrase the question into a more detailed
    search query, similar to how one might carefully craft a search query for Google.
    This approach improves alignment between the user’s intent and the RAG retrieval
    process. The phrasing below is suboptimal, but it provides the gist of it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Response Personalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For tailored responses, integrate user-specific context directly into the RAG
    context composition. By adding a user-specific layer to the final context, you
    allow the AI to take into account individual preferences, permissions, or history
    without altering the core retrieval process.
  prefs: []
  type: TYPE_NORMAL
- en: By addressing these data challenges, your RAG system can evolve from a compelling
    POC into a reliable production-grade solution. Ultimately, the effectiveness of
    RAG relies more on careful engineering than on the AI model itself. While AI can
    generate fluent answers, the real magic lies in how well we retrieve and structure
    information. So the next time you’re impressed by an AI system’s conversational
    abilities, remember that it’s likely the result of an expertly designed retrieval
    process working behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article provided you some insight into the RAG process, and why
    the magic that you experience when talking to your data isn’t necessarily coming
    from the AI model, but is largely dependent on the design of your retrieval process.
  prefs: []
  type: TYPE_NORMAL
- en: Please comment with your thoughts.
  prefs: []
  type: TYPE_NORMAL
