<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>What Is a Good Imputation for Missing Values?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>What Is a Good Imputation for Missing Values?</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08">https://towardsdatascience.com/what-is-a-good-imputation-for-missing-values-e9256d45851b?source=collection_archive---------5-----------------------#2024-06-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="64aa" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">My current take on what imputation should be</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jeffrey Näf" class="l ep by dd de cx" src="../Images/0ce6db85501192cdebeeb910eb81a688.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*fTRnnMS3RSMCfs2m5zRcDQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jeffrey_85949?source=post_page---byline--e9256d45851b--------------------------------" rel="noopener follow">Jeffrey Näf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--e9256d45851b--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">18 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="8715" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This article is a first article summarizing and discussing my most recent <a class="af nf" href="https://hal.science/hal-04521894" rel="noopener ugc nofollow" target="_blank">paper</a>. We study general-purpose imputation of tabular datasets. That is, the imputation should be done in a way that works for many different tasks in a second step (sometimes referred to as “broad imputation”).</p><p id="b84b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this article, I will write 3 lessons that I learned working on this problem over the last years. I am very excited about this paper in particular, but also cautious, as the problem of missing values has many aspects and it can be difficult to not miss something. So I invite you to judge for yourself if my lessons make sense to you.</p><p id="ccc3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If you do not want to get into great discussions about missing values, I will summarize my recommendations at the end of the article.</p><p id="b60d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr"><em class="ng">Disclaimer:</em></strong><em class="ng"> The goal of this article is to use imputation to recreate the original data distribution. While I feel this is what most researchers and practitioners actually want, this is a difficult goal that might not be necessary in all applications. For instance, when performing (conditional mean) prediction, there are several recent papers showing that even simple imputation methods are sufficient for large sample sizes.</em></p><p id="9a7c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">All images in this article were created by the author.</p><h2 id="18ea" class="nh ni fq bf nj nk nl nm nn no np nq nr ms ns nt nu mw nv nw nx na ny nz oa ob bk">Preliminaries</h2><p id="fd47" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Before continuing we need to discuss how I think about missing values in this article.</p><p id="0859" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We assume there is an underlying distribution <em class="ng">P*</em> from which observations <em class="ng">X*</em> are drawn. In addition, there is a vector of 0/1s of the same dimension as <em class="ng">X*</em> that is drawn, let’s call this vector <em class="ng">M</em>. The actual observed data vector <em class="ng">X</em> is then <em class="ng">X*</em> masked by <em class="ng">M</em>. Thus, we observe <em class="ng">n</em> independently and identically distributed (i.i.d.) copies of the joint vector <em class="ng">(X,M)</em>. If we write this up in a data matrix, this might look like this:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi oj"><img src="../Images/94912a4e4605beac18aa07a39621e32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fCF9TEO2vf764syUGUXPZw.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">The data generating process: X* and M are drawn, then we observe n i.id. copies of (X,M), where X is X* but masked by M.</figcaption></figure><p id="cc6b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">As usual small values <em class="ng">x, m</em> means “observed”, while large values refer to random quantities. The missingness mechanisms everyone talks about are then assumptions about the relationship or joint distribution of <em class="ng">(X*,M):</em></p><p id="935f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Missing Completely at Random (MCAR): </strong>The probability of a value being missing is a coin flip, independent of any variable in the dataset. Here missing values are but a nuisance. You could ignore them and just focus on the fully observed part of your dataset and there would be no bias. In math for all <em class="ng">m </em>and <em class="ng">x</em>:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div class="oh oi pa"><img src="../Images/1c04f98834fcb6936aad2af377e81393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*QOb9DZ-N7zgtvFdcMOKA7Q.png"/></div></figure><p id="a0e9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Missing at Random (MAR):</strong> The probability of missingness can now depend on the <em class="ng">observed</em> variables in your dataset. A typical example would be two variables, say income and age, whereby age is always observed, but income might be missing for certain values of age. This is the example we study below. This may sound reasonable, but here it can get complicated. In math, for all <em class="ng">m</em> and <em class="ng">x</em>:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div class="oh oi pb"><img src="../Images/c38d327ba903dd79498b41377fbb2ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*-lr0TKZuekzdT7CbHUnvrA.png"/></div></figure><p id="a390" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Missing Not at Random (MNAR):</strong> Everything is possible here, and we cannot say anything about anything in general.</p><p id="0732" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The key is that for imputation, we need to learn the conditional distribution of missing values given observed values in one pattern <em class="ng">m’</em> to impute in another pattern <em class="ng">m</em>.</p><p id="629f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A well-known method of achieving this is the Multiple Imputation by Chained Equations (<strong class="ml fr">MICE</strong>) method: Initially fill the values with a simple imputation, such as mean imputation. Then for each iteration <em class="ng">t</em>, for each variable <em class="ng">j</em> regress the observed <em class="ng">X_j </em>on all other variables (which are imputed). Then plug in the values of these variables into the learned imputer for all <em class="ng">X_j </em>that are not observed. This is explained in detail in <a class="af nf" href="https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3" rel="noopener">this article</a>, with an amazing illustration that will make things immediately clear. In R this is conveniently implemented in the <a class="af nf" href="https://cran.r-project.org/web/packages/mice/index.html" rel="noopener ugc nofollow" target="_blank">mice R package</a>. As I will outline below, I am a huge fan of this method, based on the performance I have seen. In fact, the ability to recreate the underlying distribution of certain instances of MICE, such as mice-cart, is uncanny. In this article, we focus on a very simple example with only one variable missing, and so we can code by hand what MICE would usually do iteratively, to better illustrate what is happening.</p><blockquote class="pc"><p id="4f01" class="pd pe fq bf pf pg ph pi pj pk pl ne dx">A first mini-lesson is that MICE is a host of methods; whatever method you choose to regress <em class="pm">X_j </em>on the other variables gives you a different imputation method. As such, there are countless variants in the mice R package, such as mice-cart, mice-rf, mice-pmm, mice-norm.nob, mice-norm.predict and so on. These methods will perform widely differently as we will see below. Despite this, at least some papers (in top conferences such as NeurIPS) confidently proclaim that they compare their methods to “MICE”, without any detail on what exactly they are using.</p></blockquote><h2 id="5f97" class="nh ni fq bf nj nk pn nm nn no po nq nr ms pp nt nu mw pq nw nx na pr nz oa ob bk">The Example</h2><p id="b138" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">We will look at a very simple but illustrative example: Consider a data set with two jointly normal variables, <em class="ng">X_1, X_2</em>. We assume both variables have variance of 1 and a positive correlation of 0.5. To give some context, we can imagine <em class="ng">X_1 </em>to be (the logarithm of) income and <em class="ng">X_2 </em>to be age. (This is just for illustration, obviously no one is between -3 and 3 years old). Moreover, assume a missing mechanism for the income <em class="ng">X_1</em>, whereby <em class="ng">X_1 </em>tends to be missing whenever age is “high”. That is we set:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi ps"><img src="../Images/b492606e06c00d26ddb636b76c91d888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouF4f8_2k93dVqL94Ce-9w.png"/></div></div></figure><p id="6d0f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So <em class="ng">X_1 </em>(income) is missing with probability 0.8 whenever <em class="ng">X_2 </em>(age) is “large” (i.e., larger zero). As we assume <em class="ng">X_2 </em>is always observed, this is a textbook MAR example with two patterns, one where all variables are fully observed (<em class="ng">m1</em>) and a second (<em class="ng">m2</em>), wherein <em class="ng">X_1 </em>is missing. Despite the simplicity of this example, if we assume that higher age is related to higher income, there is a<em class="ng"> clear shift in the distribution of income and age when moving from one pattern to the other</em>. In pattern <em class="ng">m2</em>, where income is missing, values of both the observed age and the (unobserved) income tend to be higher. Let’s look at this in code:</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="aab0" class="px ni fq pu b bg py pz l qa qb">library(MASS)<br/>library(mice)<br/><br/><br/><br/>set.seed(10)<br/>n&lt;-3000<br/><br/>Xstar &lt;- mvrnorm(n=n, mu=c(0,0), Sigma=matrix( c(1,0.7,0.7,1), nrow=2, byrow=T   ))<br/><br/>colnames(Xstar) &lt;- paste0("X",1:2)<br/><br/><br/><br/>## Introduce missing mechanisms<br/>M&lt;-matrix(0, ncol=ncol(Xstar), nrow=nrow(Xstar))<br/>M[Xstar[,2] &gt; 0, 1]&lt;- sample(c(0,1), size=sum(Xstar[,2] &gt; 0), replace=T, prob = c(1-0.8,0.8) )<br/><br/><br/>## This gives rise to the observed dataset by masking X^* with M:<br/>X&lt;-Xstar<br/>X[M==1] &lt;- NA<br/><br/><br/>## Plot the distribution shift<br/>par(mfrow=c(2,1))<br/>plot(Xstar[!is.na(X[,1]),1:2], xlab="", main="", ylab="", cex=0.8, col="darkblue", xlim=c(-4,4), ylim=c(-3,3))<br/>plot(Xstar[is.na(X[,1]),1:2], xlab="", main="", ylab="", cex=0.8, col="darkblue", xlim=c(-4,4), ylim=c(-3,3))</span></pre><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qc"><img src="../Images/468ab5a6d59712941c96279dab0cf9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1I9okHOAFcHlhKqdGh4ig.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Top: Distribution of (X_1,X_2) in the pattern where X_1 is observed, Bottom: Distribution of (X_1,X_2) in the pattern where X_1 is missing.</figcaption></figure></div></div></div><div class="ab cb qd qe qf qg" role="separator"><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj qk"/><span class="qh by bm qi qj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="231a" class="nh ni fq bf nj nk nl nm nn no np nq nr ms ns nt nu mw nv nw nx na ny nz oa ob bk">Lesson 1: Imputation is a distributional prediction problem</h2><p id="35f5" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">In my view, the goal of (general purpose) imputation should be to replicate the underlying data distribution as well as possible. To illustrate this, consider again the first example with <em class="ng">p=0</em>, such that only <em class="ng">X_1</em> has missing values. We will now try to impute this example, using the famous <a class="af nf" href="https://medium.com/@ofirdi/mice-is-nice-but-why-should-you-care-e66698f245a3" rel="noopener">MICE </a>approach. Since only <em class="ng">X_1</em> is missing, we can implement this by hand. We start with the <em class="ng">mean imputation</em>, which simply calculates the mean of <em class="ng">X_1 </em>in the pattern where it is observed, and plugs this mean in the place of NA. We also use the <em class="ng">regression imputation</em> which is a bit more sophisticated: We regress <em class="ng">X_1 </em>onto <em class="ng">X_2 </em>in the pattern where <em class="ng">X_1 </em>is observed and then for each missing observation of <em class="ng">X_1 </em>we plug in the prediction of the regression. Thus here we impute the conditional mean of <em class="ng">X_1 </em>given <em class="ng">X_2</em>. Finally, for the <em class="ng">Gaussian imputation</em>, we start with the same regression of <em class="ng">X_1 </em>onto <em class="ng">X_2</em>, but then impute each missing value of <em class="ng">X_1 </em>by drawing from a Gaussian distribution. <em class="ng">In other words, instead of imputing the conditional expectation (i.e. just the center of the conditional distribution), we draw from this distribution.</em> This leads to a random imputation, which may be a bit counterintuitive at first, but will actually lead to the best result:</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="7039" class="px ni fq pu b bg py pz l qa qb">## (0) Mean Imputation: This would correspond to "mean" in the mice R package ##<br/><br/><br/># 1. Estimate the mean<br/>meanX&lt;-mean(X[!is.na(X[,1]),1])<br/><br/>## 2. Impute<br/>meanimp&lt;-X<br/>meanimp[is.na(X[,1]),1] &lt;-meanX<br/><br/>## (1) Regression Imputation: This would correspond to "norm.predict" in the mice R package ##<br/><br/># 1. Estimate Regression<br/>lmodelX1X2&lt;-lm(X1~X2, data=as.data.frame(X[!is.na(X[,1]),])   )<br/><br/>## 2. Impute<br/>impnormpredict&lt;-X<br/>impnormpredict[is.na(X[,1]),1] &lt;-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )<br/><br/><br/>## (2) Gaussian Imputation: This would correspond to "norm.nob" in the mice R package ##<br/><br/># 1. Estimate Regression<br/>#lmodelX1X2&lt;-lm(X1~X2, X=as.data.frame(X[!is.na(X[,1]),])   )<br/># (same as before)<br/><br/>## 2. Impute<br/>impnorm&lt;-X<br/>meanx&lt;-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )<br/>var &lt;- var(lmodelX1X2$residuals)<br/>impnorm[is.na(X[,1]),1] &lt;-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var) )<br/><br/><br/><br/>## Plot the different imputations<br/><br/>par(mfrow=c(2,2))<br/><br/><br/>plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation"), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(meanimp[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>plot(impnormpredict[!is.na(X[,1]),c("X2","X1")], main=paste("Regression Imputation"), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(impnormpredict[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation"), col="darkblue", cex.main=1.5)<br/>points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>#plot(Xstar[,c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)<br/>plot(Xstar[!is.na(X[,1]),c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)<br/>points(Xstar[is.na(X[,1]),c("X2","X1")], col="darkgreen", cex=0.8 )<br/><br/></span></pre><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi ql"><img src="../Images/e08bd7255512d697e271005c69023417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_D_vnLDSDPojplK7R-8Ww.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">The distribution of (X_1, X_2) plotted for different imputation methods. Different Imputation methods (red are the imputed points).</figcaption></figure><p id="241d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Studying this plot immediately reveals that the mean and regression imputations might not be ideal, as they completely fail at recreating the original data distribution. In contrast, the Gaussian imputation looks pretty good, in fact, I’d argue it would be hard to differentiate it from the truth. This might just seem like a technical notion, but this has consequences. Imagine you were given any of those imputed data sets and now you would like to find the regression coefficient when regressing <em class="ng">X_2 </em>onto <em class="ng">X_1 </em>(the opposite of what we did for imputation). The truth in this case is given by <em class="ng">beta=cov(X_1,X_2)/var(X_1)=0.7</em>.</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="e93d" class="px ni fq pu b bg py pz l qa qb">## Regressing X_2 onto X_1<br/><br/>## mean imputation estimate<br/>lm(X2~X1, data=data.frame(meanimp))$coefficients["X1"]<br/>## beta= 0.61<br/><br/>## regression imputation estimate<br/>round(lm(X2~X1, data=data.frame(impnormpredict))$coefficients["X1"],2)<br/>## beta= 0.90<br/><br/>## Gaussian imputation estimate<br/>round(lm(X2~X1, data=data.frame(impnorm))$coefficients["X1"],2)<br/>## beta= 0.71<br/><br/>## Truth imputation estimate<br/>round(lm(X2~X1, data=data.frame(Xstar))$coefficients["X1"],2)<br/>## beta= 0.71</span></pre><p id="13c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The Gaussian imputation is pretty close to 0.7 (0.71), and importantly, it is very close to the estimate using the full (unobserved) data! On the other hand, the mean imputation underestimates <em class="ng">beta</em>, while<em class="ng"> the regression imputation overestimates beta</em>. The latter is natural, as the conditional mean imputation artificially inflates the relationship between variables. This effect is particularly important, as this will result in effects that are overestimated in science and (data science) practice!!</p><p id="5f85" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The regression imputation might seem overly simplistic. However, the key is that very commonly used imputation methods in machine learning and other fields work exactly like this. For instance, knn imputation and random forest imputation (i.e., <a class="af nf" href="https://academic.oup.com/bioinformatics/article/28/1/112/219101" rel="noopener ugc nofollow" target="_blank">missForest</a>). Especially the latter has been praised and recommended in several benchmarking papers and appears very widely used. However, missForest fits a Random Forest on the observed data and then simply imputes by the conditional mean. So, using it in this example the result would look very similar to the regression imputation, thus resulting in an artificial strengthening of relations between variable and biased estimates!</p><blockquote class="pc"><p id="a13d" class="pd pe fq bf pf pg ph pi pj pk pl ne dx">A lot of commonly used imputation methods, such as mean imputation, knn imputation, and missForest fail at replicating the distribution. What they estimate and approximate is the (conditional) mean, and so the imputation will look like that of the regression imputation (or even worse for the mean imputation). Instead, we should try to impute by drawing from estimated (conditional) distributions.</p></blockquote><h2 id="d3dc" class="nh ni fq bf nj nk pn nm nn no po nq nr ms pp nt nu mw pq nw nx na pr nz oa ob bk">Lesson 2: Imputation should be evaluated as a distributional prediction problem</h2><p id="0461" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">There is a dual problem connected to the discussion of the first lesson. How should imputation methods be evaluated?</p><p id="68ff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Imagine we developed a new imputation method and now want to benchmark this against methods that exist already such as missForest, MICE, or <a class="af nf" href="https://arxiv.org/abs/1806.02920" rel="noopener ugc nofollow" target="_blank">GAIN</a>. In this setting, we artificially induce the missing values and so we have the actual data set just as above. We now want to compare this true dataset to our imputations. For the sake of the example, let us assume the regression imputation above is our new method, and we would like to compare it to mean and Gaussian imputation.</p><p id="31ad" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even in the most prestigious conferences, this is done by calculating the root mean squared error (RMSE):</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qm"><img src="../Images/e09af5aad14d3f1fb12bc30d5e878641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*oT9oRh8jZIgqGCxKbX9WYw.png"/></div></div></figure><p id="27b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is implemented here:</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="ade6" class="px ni fq pu b bg py pz l qa qb">## Function to calculate the RMSE:<br/># impX is the imputed data set<br/># Xstar is the fully observed data set<br/><br/>RMSEcalc&lt;-function(impX, Xstar){<br/>  <br/>  round(mean(apply(Xstar - impX,1,function(x) norm(as.matrix(x), type="F"  ) )),2)<br/>  <br/>}</span></pre><p id="17a3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This discussion is related to the discussion on how to correctly score predictions. <a class="af nf" href="https://medium.com/towards-data-science/how-to-evaluate-your-predictions-cef80d8f6a69" rel="noopener">In this article</a>, I discussed that (R)MSE is the right score to evaluate (conditional) mean predictions. It turns out the exact same logic applies here; using RMSE like this to evaluate our imputation, will favor methods that impute the conditional mean, such as the regression imputation, knn imputation, and missForest.</p><p id="f387" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead, imputation should be <em class="ng">evaluated</em> as a distributional prediction problem. I suggest using the <em class="ng">energy distance between the distribution of the fully observed data and the imputation “distribution”</em>. Details can be found in the paper, but in R it is easily coded using the nice “energy” R package:</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="5142" class="px ni fq pu b bg py pz l qa qb">library(energy)<br/><br/>## Function to calculate the energy distance:<br/># impX is the imputed data set<br/># Xstar is the fully observed data set<br/><br/>## Calculating the energy distance using the eqdist.e function of the energy package<br/>energycalc &lt;- function(impX, Xstar){<br/>  <br/>  # Note: eqdist.e calculates the energy statistics for a test, which is actually<br/>  # = n^2/(2n)*energydistance(impX,Xstar), but we we are only interested in relative values<br/>  round(eqdist.e( rbind(Xstar,impX), c(nrow(Xstar), nrow(impX))  ),2)<br/>  <br/>}</span></pre><p id="c040" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We now apply the two scores to our imaginary research project and try to figure out whether our regression imputation is better than the other two:</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="f79c" class="px ni fq pu b bg py pz l qa qb">par(mfrow=c(2,2))<br/><br/><br/>## Same plots as before, but now with RMSE and energy distance <br/>## added<br/><br/>plot(meanimp[!is.na(X[,1]),c("X2","X1")], main=paste("Mean Imputation", "\nRMSE", RMSEcalc(meanimp, Xstar), "\nEnergy", energycalc(meanimp, Xstar)), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(meanimp[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>plot(impnormpredict[!is.na(X[,1]),c("X2","X1")], main=paste("Regression Imputation","\nRMSE", RMSEcalc(impnormpredict, Xstar), "\nEnergy", energycalc(impnormpredict, Xstar)), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(impnormpredict[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation","\nRMSE", RMSEcalc(impnorm, Xstar), "\nEnergy", energycalc(impnorm, Xstar)), col="darkblue", cex.main=1.5)<br/>points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/><br/>plot(Xstar[!is.na(X[,1]),c("X2","X1")], main="Truth", col="darkblue", cex.main=1.5)<br/>points(Xstar[is.na(X[,1]),c("X2","X1")], col="darkgreen", cex=0.8 )<br/><br/><br/></span></pre><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qn"><img src="../Images/ea0401b75e1292ebc9b18887aea7cd7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ais6yaMMEz35kyvJKFs6Jg.png"/></div></div></figure><p id="8bc9" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we look at RMSE, then our regression imputation appears great! It beats both mean and Gaussian imputation. However this clashes with the analysis from above, and choosing the regression imputation can and likely will lead to highly biased results. On the other hand, the (scaled) energy distance correctly identifies that the Gaussian imputation is the best method, agreeing with both visual intuition and better parameter estimates.</p><blockquote class="pc"><p id="ca65" class="pd pe fq bf pf pg ph pi pj pk pl ne dx">When evaluating imputation methods (when the true data are available) measures such as RMSE and MAE should be avoided. Instead, the problem should be treated and evaluated as a distributional prediction problem, and distributional metrics such as the energy distance should be used. The overuse of RMSE as an evaluation tool has some serious implications for research in this area.</p></blockquote><p id="7bff" class="pw-post-body-paragraph mj mk fq ml b go qo mn mo gr qp mq mr ms qq mu mv mw qr my mz na qs nc nd ne fj bk">Again this is not surprising, identifying the best mean prediction is what RMSE does. What is surprising, is how consistently it is used in research to evaluate imputation methods. In my view, this throws into question at least some recommendations of recent papers, about what imputation methods to use. Moreover, as new imputation methods get developed they are compared to other methods in terms of RMSE and are thus likely not replicating the distribution correctly. One thus has to question the usefulness of at least some of the myriad of imputation methods developed in recent years.</p><p id="7920" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The question of evaluation gets much harder, <strong class="ml fr">when the underlying observations are not available. </strong>In the paper we develope a score that allows to rank imputation methods, even in this case! (a refinement of the idea presented in <a class="af nf" rel="noopener" target="_blank" href="/i-scores-how-to-choose-the-best-method-to-fill-in-nas-in-your-data-set-43f3f0df971f">this article</a>). The details are reserved for another medium post, but we can try it for this example. The “Iscore.R” function can be found on <a class="af nf" href="https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable" rel="noopener ugc nofollow" target="_blank">Github </a>or at the end of this article.</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="eecc" class="px ni fq pu b bg py pz l qa qb"><br/>library(mice)<br/>source("Iscore.R")<br/><br/><br/>methods&lt;-c("mean",       #mice-mean<br/>           "norm.predict",   #mice-sample<br/>           "norm.nob") # Gaussian Imputation<br/><br/>## We first define functions that allow for imputation of the three methods:<br/><br/>imputationfuncs&lt;-list()<br/><br/>imputationfuncs[["mean"]] &lt;- function(X,m){ <br/># 1. Estimate the mean<br/>  meanX&lt;-mean(X[!is.na(X[,1]),1])<br/>## 2. Impute<br/>  meanimp&lt;-X<br/>  meanimp[is.na(X[,1]),1] &lt;-meanX<br/>  <br/>  res&lt;-list()<br/>  <br/>  for (l in 1:m){<br/>    res[[l]] &lt;- meanimp<br/>  }<br/>  <br/>  return(res)<br/>  <br/>}<br/><br/>imputationfuncs[["norm.predict"]] &lt;- function(X,m){ <br/> # 1. Estimate Regression<br/>  lmodelX1X2&lt;-lm(X1~., data=as.data.frame(X[!is.na(X[,1]),])   )<br/> ## 2. Impute<br/>  impnormpredict&lt;-X<br/>  impnormpredict[is.na(X[,1]),1] &lt;-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )<br/>  <br/>res&lt;-list()<br/><br/>for (l in 1:m){<br/>  res[[l]] &lt;- impnormpredict<br/>}<br/><br/>return(res)<br/>  <br/>  }<br/><br/><br/>imputationfuncs[["norm.nob"]] &lt;- function(X,m){ <br/> # 1. Estimate Regression<br/>  lmodelX1X2&lt;-lm(X1~., data=as.data.frame(X[!is.na(X[,1]),])   )<br/> ## 2. Impute<br/>  impnorm&lt;-X<br/>  meanx&lt;-predict(lmodelX1X2, newdata= as.data.frame(X[is.na(X[,1]),])  )<br/>  var &lt;- var(lmodelX1X2$residuals)<br/>  <br/>  res&lt;-list()<br/>  <br/>  for (l in 1:m){<br/>    impnorm[is.na(X[,1]),1] &lt;-rnorm(n=length(meanx), mean = meanx, sd=sqrt(var) )<br/>    res[[l]] &lt;- impnorm<br/>  }<br/><br/>  <br/>  return(res)<br/>  <br/>}<br/><br/><br/>scoreslist &lt;- Iscores_new(X,imputations=NULL, imputationfuncs=imputationfuncs, N=30)  <br/><br/>scores&lt;-do.call(cbind,lapply(scoreslist, function(x) x$score ))<br/>names(scores)&lt;-methods<br/>scores[order(scores)]<br/><br/>#    mean       norm.predict     norm.nob <br/>#  -0.7455304   -0.5702136   -0.4220387 <br/></span></pre><p id="dd7b" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thus <em class="ng">without every seeing the values of the missing data</em>, our score is able to identify that norm.nob is the best method! This comes in handy, especially when the data has more than two dimensions. I will give more details on how to use the score and how it works in a next article.</p><h2 id="9918" class="nh ni fq bf nj nk nl nm nn no np nq nr ms ns nt nu mw nv nw nx na ny nz oa ob bk">Lesson 3: MAR is weirder than you think</h2><p id="1c89" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">When reading the literature on missing value imputation, it is easy to get a sense that MAR is a solved case, and all the problems arise from whether it can be assumed or not. While this might be true under standard procedures such as maximum likelihood, if one wants to find a good (nonparametric) imputation, this is not the case.</p><p id="0673" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Our paper discusses how complex distribution shifts are possible under MAR when changing from say the fully observed pattern to a pattern one wants to impute. We will focus here on the shift in distribution that can occur in the observed variables. For this, we turn to the example above, where we took <em class="ng">X_1 </em>to be income and <em class="ng">X_2 </em>to be age. As we have seen in the first figure the distribution looks quite different. However, the conditional distribution of <em class="ng">X_1 | X_2</em> remains the same! This allows to identify the right imputation distribution in principle.</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qt"><img src="../Images/ae052c33d825988f15194972c9256b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19oJoQciz20EH9omhh5_BA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">Distribution of X_2 in the pattern where X_1 is observed, Bottom: Distribution of (X_1,X_2) in the pattern where X_1 is missing.</figcaption></figure><p id="37b1" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The problem is that even if we can nonparametrically estimate the conditional distribution in the pattern where <em class="ng">X_1 </em>is missing, we need to extrapolate this to the distribution of <em class="ng">X_2 </em>where <em class="ng">X_1 </em>is missing. To illustrate this I will now introduce two very important nonparametric mice methods. One old (<em class="ng">mice-cart</em>) and one new (<em class="ng">mice-DRF</em>). The former uses one tree to regress <em class="ng">X_j</em> on all the other variables and then imputes by drawing samples from that tree. Thus instead of using the conditional expectation prediction of a tree/forest, as missForest does, it draws from the leaves to approximate sampling from the conditional distribution. In contrast, mice-DRF uses the <a class="af nf" href="https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8" rel="noopener">Distributional Random Forest</a>, a forest method designed to estimate distributions and samples from those predictions. Both work exceedingly well, as I will lay out below!</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="3f16" class="px ni fq pu b bg py pz l qa qb">library(drf)<br/><br/><br/>## mice-DRF ##<br/>par(mfrow=c(2,2))<br/><br/>#Fit DRF<br/>DRF &lt;- drf(X=X[!is.na(X[,1]),2, drop=F], Y=X[!is.na(X[,1]),1, drop=F], num.trees=100)<br/>impDRF&lt;-X<br/># Predict weights for unobserved points<br/>wx&lt;-predict(DRF, newdata= X[is.na(X[,1]),2, drop=F]  )$weights<br/>impDRF[is.na(X[,1]),1] &lt;-apply(wx,1,function(wxi) sample(X[!is.na(X[,1]),1, drop=F], size=1, replace=T, prob=wxi))<br/><br/><br/>plot(impDRF[!is.na(X[,1]),c("X2","X1")], main=paste("DRF Imputation", "\nRMSE", RMSEcalc(impDRF, Xstar), "\nEnergy", energycalc(impDRF, Xstar)), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(impDRF[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/><br/>## mice-cart##<br/>impcart&lt;-X<br/>impcart[is.na(X[,1]),1] &lt;-mice.impute.cart(X[,1], ry=!is.na(X[,1]), X[,2, drop=F], wy = NULL)<br/><br/>plot(impDRF[!is.na(X[,1]),c("X2","X1")], main=paste("cart Imputation", "\nRMSE", RMSEcalc(impcart, Xstar), "\nEnergy", energycalc(impcart, Xstar)), cex=0.8, col="darkblue", cex.main=1.5)<br/>points(impDRF[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )<br/><br/>plot(impnorm[!is.na(X[,1]),c("X2","X1")], main=paste("Gaussian Imputation","\nRMSE", RMSEcalc(impnorm, Xstar), "\nEnergy", energycalc(impnorm, Xstar)), col="darkblue", cex.main=1.5)<br/>points(impnorm[is.na(X[,1]),c("X2","X1")], col="darkred", cex=0.8 )</span></pre><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qu"><img src="../Images/e991ed2be1c9f70f0c552c5361ef0877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ljMLmWmuyFS9R0zoX2Aqew.png"/></div></div></figure><p id="c8cc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Though both mice-cart and mice-DRF do a good job, they are still not quite as good as the Gaussian imputation. This is not surprising per se, as the Gaussian imputation is the ideal imputation in this case (because <em class="ng">(X_1, X_2)</em> are indeed Gaussian). Nonetheless the distribution shift in <em class="ng">X_2</em> likely plays a role in the difficulty of mice-cart and mice-DRF to recover the distribution even for 3000 observations (these methods are usually really really good). Note that this kind of extrapolation is not a problem for the Gaussian imputation.</p><p id="db2e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The paper also discusses a similar, but more extreme example with two variables <em class="ng">(X_1, X_2)</em>. In this example, the distribution shift is much more pronounced, and the forest-based methods struggle accordingly:</p><figure class="ok ol om on oo op oh oi paragraph-image"><div role="button" tabindex="0" class="oq or ed os bh ot"><div class="oh oi qv"><img src="../Images/7a8ab1ef4047e142f30d01eb755c79d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLrjrzUMQB2H5nunyAr0nA.png"/></div></div><figcaption class="ov ow ox oh oi oy oz bf b bg z dx">More extreme example of distribution shift in the paper. While the Gausisan imputation is near perfect, mice-RF and mice-DRF are not able to extrapolate correctly.</figcaption></figure><p id="ffa2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The problem is that these kinds of extreme distribution shifts are possible under MAR and forest-based methods have a hard time extrapolating outside of the data set (so do neural nets btw). Indeed, can you think of a method that can (1) learn a distribution nonparametrically and (2) extrapolate from <em class="ng">X_2</em> coming from the upper distribution to <em class="ng">X_2 </em>drawn from the lower distribution reliably? For now, I cannot.</p><blockquote class="pc"><p id="f589" class="pd pe fq bf pf pg ph pi pj pk pl ne dx">Imputation is hard, even if MAR can be assumed, and the search for reliable imputation methods is not over.</p></blockquote><h2 id="348c" class="nh ni fq bf nj nk pn nm nn no po nq nr ms pp nt nu mw pq nw nx na pr nz oa ob bk">Conclusion: My current recommendations</h2><p id="38e4" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">Missing values are a hairy problem. Indeed, the best way to deal with missing values is to not have them. Accordingly, Lesson 3 shows that the search for imputation methods is not yet concluded, even if one only considers MAR. We still lack a method that can do (1) nonparametric distributional prediction and (2) adapt to distribution shifts that are possible under MAR. That said, I also sometimes feel people make the problem more complicated than it is; some MICE methods perform extremely well and might be enough for many missing value problems already.</p><p id="457f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I first want to mention that that there are very fancy machine learning methods like <a class="af nf" href="https://arxiv.org/abs/1806.02920" rel="noopener ugc nofollow" target="_blank">GAIN</a> and variants, that try to impute data using neural nets. I like these methods because they follow the right idea: Impute the conditional distributions of missing given observed. However, after using them a bit, I am somewhat disappointed by their performance, especially compared to MICE.</p><p id="4be4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Thus, if I had a missing value problem the first thing I’d try is <em class="ng">mice-cart</em> (implemented in the mice R package) or the new <em class="ng">mice-DRF</em> (code on <a class="af nf" href="https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable" rel="noopener ugc nofollow" target="_blank">Github</a>) we developed in the paper. I have tried those two on quite a few examples and their ability to recreate the data is uncanny. However note that these observations of mine are not based on a large, systematic benchmarking and should be taken with a grain of salt. Moreover, this requires at least an intermediate sample size of say above 200 or 300. Imputation is not easy and completely nonparametric methods will suffer if the sample size is too low. In the case of less than 200 observations, I would go with simpler methods such as Gaussian imputation (<em class="ng">mice-norm.nob</em> in the R package). If you would then like to find the best out of these methods I recommend trying our score developed in the paper,<strong class="ml fr"> </strong>as done in Lesson 2 (though the implementation might not be the best).</p><p id="744c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Finally, note that none of these methods are able to effectively deal with <strong class="ml fr">imputation uncertainty</strong>! In a sense, we only discussed single imputation in this article. (Proper) multiple imputation would require that the uncertainty of the imputation method itself is taken into account, which is usually done using Bayesian methods. For frequentist method like we looked at here, this appears to be an open problem.</p><h2 id="6a7a" class="nh ni fq bf nj nk nl nm nn no np nq nr ms ns nt nu mw nv nw nx na ny nz oa ob bk">Appendix 1: m-I-Score</h2><p id="fd8c" class="pw-post-body-paragraph mj mk fq ml b go oc mn mo gr od mq mr ms oe mu mv mw of my mz na og nc nd ne fj bk">The File “Iscore.R”, which can also be found on <a class="af nf" href="https://github.com/JeffNaef/MARimputation/tree/c1f5a1e48e8a60db95c727876086db5b7305f614/Useable" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><pre class="ok ol om on oo pt pu pv bp pw bb bk"><span id="4351" class="px ni fq pu b bg py pz l qa qb">Iscores_new&lt;-function(X, N=50,  imputationfuncs=NULL, imputations=NULL, maxlength=NULL,...){<br/>  <br/>  ## X: Data with NAs<br/>  ## N: Number of samples from imputation distribution H<br/>  ## imputationfuncs: A list of functions, whereby each imputationfuncs[[method]] is a function that takes the arguments<br/>  ## X,m and imputes X m times using method: imputations= imputationfuncs[[method]](X,m).<br/>  ## imputations: Either NULL or a list of imputations for the methods considered, each imputed X saved as <br/>  ##              imputations[[method]], whereby method is a string<br/>  ## maxlength: Maximum number of variables X_j to consider, can speed up the code<br/>  <br/>  <br/>  require(Matrix)<br/>  require(scoringRules)<br/>  <br/>  <br/>  numberofmissingbyj&lt;-sapply(1:ncol(X), function(j)  sum(is.na(X[,j]))  )<br/>  print("Number of missing values per dimension:")<br/>  print(paste0(numberofmissingbyj, collapse=",")  )<br/><br/>  methods&lt;-names(imputationfuncs)<br/><br/>  score_all&lt;-list()<br/>  <br/>  for (method in methods) {<br/>    print(paste0("Evaluating method ", method))<br/>    <br/>    <br/>    # }<br/>    if (is.null(imputations)){<br/>      # If there is no prior imputation<br/>      tmp&lt;-Iscores_new_perimp(X, Ximp=NULL, N=N, imputationfunc=imputationfuncs[[method]], maxlength=maxlength,...)<br/>      score_all[[method]] &lt;- tmp  <br/>      <br/>      <br/>    }else{<br/>      <br/>      tmp&lt;-Iscores_new_perimp(X, Ximp=imputations[[method]][[1]], N=N, imputationfunc=imputationfuncs[[method]], maxlength=maxlength, ...)<br/>      score_all[[method]] &lt;- tmp  <br/>      <br/>    }<br/>    <br/>    <br/>    <br/>  }<br/>  <br/>  return(score_all)<br/>  <br/>}<br/><br/><br/>Iscores_new_perimp &lt;- function(X, Ximp, N=50, imputationfunc, maxlength=NULL,...){<br/>  <br/>  if (is.null(Ximp)){<br/>    # Impute, maxit should not be 1 here!<br/>    Ximp&lt;-imputationfunc(X=X  , m=1)[[1]]<br/>  }<br/>  <br/>  <br/>  colnames(X) &lt;- colnames(Ximp) &lt;- paste0("X", 1:ncol(X))<br/>  <br/>  args&lt;-list(...)<br/>  <br/>  X&lt;-as.matrix(X)<br/>  Ximp&lt;-as.matrix(Ximp)<br/>  <br/>  n&lt;-nrow(X)<br/>  p&lt;-ncol(X)<br/>  <br/>  ##Step 1: Reoder the data according to the number of missing values<br/>  ## (least missing first)<br/>  numberofmissingbyj&lt;-sapply(1:p, function(j)  sum(is.na(X[,j]))  )<br/><br/>  ## Done in the function<br/>  M&lt;-1*is.na(X)<br/>  colnames(M) &lt;- colnames(X)<br/>  <br/>  indexfull&lt;-colnames(X)<br/>  <br/>  <br/>  # Order first according to most missing values<br/>  <br/>  # Get dimensions with missing values (all other are not important)<br/>  dimwithNA&lt;-(colSums(M) &gt; 0)<br/>  dimwithNA &lt;- dimwithNA[order(numberofmissingbyj, decreasing=T)]<br/>  dimwithNA&lt;-dimwithNA[dimwithNA==TRUE]<br/>  <br/>  if (is.null(maxlength)){maxlength&lt;-sum(dimwithNA) }<br/>  <br/>  if (sum(dimwithNA) &lt; maxlength){<br/>    warning("maxlength was set smaller than sum(dimwithNA)")<br/>    maxlength&lt;-sum(dimwithNA)<br/>  }<br/>  <br/>  <br/>  index&lt;-1:ncol(X)<br/>  scorej&lt;-matrix(NA, nrow= min(sum(dimwithNA), maxlength), ncol=1)<br/>  weight&lt;-matrix(NA, nrow= min(sum(dimwithNA), maxlength), ncol=1)<br/>  i&lt;-0<br/>  <br/>  for (j in names(dimwithNA)[1:maxlength]){<br/>    <br/>    i&lt;-i+1<br/><br/>    <br/>    print( paste0("Dimension ", i, " out of ", maxlength )   ) <br/>    <br/>  <br/>    <br/>    # H for all missing values of X_j<br/>    Ximp1&lt;-Ximp[M[,j]==1, ]<br/>    <br/>    # H for all observed values of X_j<br/>    Ximp0&lt;-Ximp[M[,j]==0, ]<br/>    <br/>    X0 &lt;-X[M[,j]==0, ]<br/>    <br/>    n1&lt;-nrow(Ximp1)<br/>    n0&lt;-nrow(Ximp0)<br/>    <br/>    <br/>    if (n1 &lt; 10){<br/>      scorej[i]&lt;-NA<br/>      <br/>      warning('Sample size of missing and nonmissing too small for nonparametric distributional regression, setting to NA')<br/>      <br/>    }else{<br/>      <br/>      <br/>      # Evaluate on observed data<br/>      Xtest &lt;- Ximp0[,!(colnames(Ximp0) %in% j) &amp;  (colnames(Ximp0) %in% indexfull), drop=F]<br/>      Oj&lt;-apply(X0[,!(colnames(Ximp0) %in% j) &amp;  (colnames(Ximp0) %in% indexfull), drop=F],2,function(x) !any(is.na(x)) )<br/>      # Only take those that are fully observed<br/>      Xtest&lt;-Xtest[,Oj, drop=F]<br/>      <br/>      Ytest &lt;-Ximp0[,j, drop=F]<br/>      <br/>      if (is.null(Xtest)){<br/>        scorej[i]&lt;-NA<br/>        #weighted<br/>        weight[i]&lt;-(n1/n)*(n0/n)<br/>        warning("Oj was empty")<br/>        next<br/>      }<br/>      <br/>      ###Test 1:<br/>      # Train DRF on imputed data<br/>      Xtrain&lt;-Ximp1[,!(colnames(Ximp1) %in% j) &amp; (colnames(Ximp1) %in% indexfull), drop=F]<br/>      # Only take those that are fully observed<br/>      Xtrain&lt;-Xtrain[,Oj, drop=F]<br/>      <br/>      Ytrain&lt;-Ximp1[,j, drop=F]<br/><br/>      <br/>      Xartificial&lt;-cbind(c(rep(NA,nrow(Ytest)),c(Ytrain)),rbind(Xtest, Xtrain)   )<br/>      colnames(Xartificial)&lt;-c(colnames(Ytrain), colnames(Xtrain))<br/>      <br/>      Imputationlist&lt;-imputationfunc(X=Xartificial  , m=N)<br/>      <br/>      Ymatrix&lt;-do.call(cbind, lapply(Imputationlist, function(x)  x[1:nrow(Ytest),1]  ))<br/>      <br/>      scorej[i] &lt;- -mean(sapply(1:nrow(Ytest), function(l)  { crps_sample(y = Ytest[l,], dat = Ymatrix[l,]) }))<br/>      <br/>    }<br/>    <br/>    <br/>    <br/>    #weighted<br/>    weight[i]&lt;-(n1/n)*(n0/n)<br/>    <br/>  }<br/>  <br/>  scorelist&lt;-c(scorej)<br/>  names(scorelist) &lt;- names(dimwithNA)[1:maxlength]<br/>  weightlist&lt;-c(weight)<br/>  names(weightlist) &lt;- names(dimwithNA)[1:maxlength]<br/>  <br/>  weightedscore&lt;-scorej*weight/(sum(weight, na.rm=T))<br/>  <br/>  ## Weight the score according to n0/n * n1/n!!<br/>  return( list(score= sum(weightedscore, na.rm=T), scorelist=scorelist, weightlist=weightlist)  )<br/>}</span></pre></div></div></div></div>    
</body>
</html>