- en: Semantic Signal Separation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义信号分离
- en: 原文：[https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11](https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11](https://towardsdatascience.com/semantic-signal-separation-769f43b46779?source=collection_archive---------2-----------------------#2024-02-11)
- en: Understand Semantic Structures with Transformers and Topic Modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变换器和主题建模理解语义结构
- en: '[](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)[![Márton
    Kardos](../Images/8c86c5ea10391a0031cdc18bb77b0736.png)](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)
    [Márton Kardos](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)[![Márton
    Kardos](../Images/8c86c5ea10391a0031cdc18bb77b0736.png)](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)
    [Márton Kardos](https://medium.com/@power.up1163?source=post_page---byline--769f43b46779--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)
    ·10 min read·Feb 11, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--769f43b46779--------------------------------)
    ·阅读时间10分钟·2024年2月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: We live in the age of big data. At this point it’s become a cliche to say that
    data is the oil of the 21st century but it really is so. Data collection practices
    have resulted in huge piles of data in just about everyone’s hands.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在大数据的时代。现在说数据是21世纪的石油已经成了一个陈词滥调，但它确实如此。数据收集的做法使得几乎每个人手中都堆积了大量数据。
- en: Interpreting data, however, is no easy task, and much of the industry and academia
    still rely on solutions, which provide little in the ways of explanations. While
    deep learning is incredibly useful for predictive purposes, it rarely gives practitioners
    an understanding of the mechanics and structures that underlie the data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解释数据并非易事，许多行业和学术界仍然依赖于那些提供极少解释的解决方案。虽然深度学习对于预测非常有用，但它很少能让从业者理解数据背后的机制和结构。
- en: Textual data is especially tricky. While natural language and concepts like
    “topics” are incredibly easy for humans to have an intuitive grasp of, producing
    operational definitions of semantic structures is far from trivial.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据尤其棘手。虽然自然语言和“主题”等概念对人类来说非常直观易懂，但要给语义结构制定操作性定义却远非易事。
- en: In this article I will introduce you to different conceptualizations of discovering
    latent semantic structures in natural language, we will look at operational definitions
    of the theory, and at last I will demonstrate the usefulness of the method with
    a case study.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将介绍不同的关于发现自然语言中潜在语义结构的概念化方法，我们将看看理论的操作性定义，最后我将通过案例研究展示该方法的实用性。
- en: 'Theory: What is a “Topic”?'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论：什么是“主题”？
- en: 'While topic to us humans seems like a completely intuitive and self-explanatory
    term, it is hardly so when we try to come up with a useful and informative definition.
    The Oxford dictionary’s definition is luckily here to help us:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管“主题”对我们人类来说似乎是一个完全直观且不言自明的术语，但当我们试图给它一个有用且信息丰富的定义时，情况却并非如此。幸运的是，牛津词典的定义可以帮助我们：
- en: A subject that is discussed, written about, or studied.
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一种被讨论、写作或研究的主题。
- en: Well, this didn’t get us much closer to something we can formulate in computational
    terms. Notice how the word *subject,* is used to hide all the gory details. This
    need not deter us, however, we can certainly do better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这并没有让我们更接近可以用计算术语表述的内容。请注意，“主题”一词被用来掩盖所有复杂的细节。然而，这不应使我们气馁，我们当然可以做得更好。
- en: '![](../Images/e55cad0c3a012ccf0ae4434f56e7cff2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e55cad0c3a012ccf0ae4434f56e7cff2.png)'
- en: Semantic Space of Academic Disciplines
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 学科的语义空间
- en: In Natural Language Processing, we often use a spatial definition of semantics.
    This might sound fancy, but essentially we imagine that semantic content of text/language
    can be expressed in some continuous space (often high-dimensional), where concepts
    or texts that are related are closer to each other than those that aren’t. If
    we embrace this theory of semantics, we can easily come up with two possible definitions
    for topic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，我们经常使用语义的空间定义。听起来可能有些花哨，但本质上我们假设文本/语言的语义内容可以在某个连续的空间中表达（通常是高维空间），在这个空间中，相关的概念或文本会比不相关的更接近。如果我们接受这种语义理论，我们可以很容易地为主题提出两种可能的定义。
- en: Topics as Semantic Clusters
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为语义聚类的主题
- en: A rather intuitive conceptualization is to imagine topic as groups of passages/concepts
    in semantic space that are closely related to each other, but not as closely related
    to other texts. This incidentally means that one passage *can only belong to one
    topic at a time.*
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观的概念化是将主题视为语义空间中相互紧密相关的段落/概念组，而这些组与其他文本的相关性较低。顺便提一下，这意味着一个段落*只能属于一个主题*。
- en: '![](../Images/c31b1056a21c843dbb872df50db92b77.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c31b1056a21c843dbb872df50db92b77.png)'
- en: Semantic Clusters of Academic Disciplines
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 学科的语义聚类
- en: This clustering conceptualization also lends itself to thinking about topics
    *hierarchically.* You can imagine that the topic “animals” might contain two subclusters,
    one which is “Eukaryates”, while the other is “Prokaryates”, and then you could
    go down this hierarchy, until, at the leaves of the tree you will find actual
    instances of concepts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类概念化也有助于我们从*层级*的角度思考主题。你可以想象，“动物”这个主题可能包含两个子聚类，一个是“真核生物”，另一个是“原核生物”，然后你可以沿着这个层次结构往下走，直到在树的叶子节点，你会发现实际的概念实例。
- en: Of course a limitation of this approach is that longer passages might contain
    multiple topics in them. This could either be addressed by splitting up texts
    to smaller, atomic parts (e.g. words) and modeling over those, but we can also
    ditch the clustering conceptualization alltogether.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种方法的一个局限性是，较长的段落可能包含多个主题。解决这一问题的一种方法是将文本拆分成更小的原子部分（例如单词），然后在这些部分上建模，但我们也可以完全抛弃聚类的概念化。
- en: Topics as Axes of Semantics
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为语义轴的主题
- en: 'We can also think of topics as the underlying dimensions of the semantic space
    in a corpus. Or in other words: Instead of describing what groups of documents
    there are we are explaining variation in documents by finding underlying **semantic
    signals**.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将主题视为语料库中语义空间的潜在维度。换句话说：我们不是描述有哪些文档群组，而是通过寻找潜在的**语义信号**来解释文档中的变化。
- en: '![](../Images/c4da83966529469b816a55f35a1f4079.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4da83966529469b816a55f35a1f4079.png)'
- en: Underlying Axes in the Semantic Space of Academic Disciplines
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 学科语义空间中的潜在轴
- en: We are explaining variation in documents by finding underlying semantic signals.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们通过找到潜在的语义信号来解释文档中的变化。
- en: 'You could for instance imagine that the most important axes that underlie restaurant
    reviews would be:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以举个例子，想象一下，餐厅评论背后最重要的语义轴可能是：
- en: Satisfaction with the food
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对食物的满意度
- en: Satisfaction with the service
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对服务的满意度
- en: I hope you see why this conceptualization is useful for certain purposes. Instead
    of us finding “good reviews” and “bad reviews”, we get an understanding of what
    it is that drives differences between these. A pop culture example of this kind
    of theorizing is of course the political compass. Yet again, instead of us being
    interested in finding “conservatives” and “progressives”, we find the **factors**
    that differentiate these.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你能理解为什么这种概念化对于某些目的有用。我们不再仅仅寻找“好评”和“差评”，而是去理解这些评价之间差异的驱动因素。这种理论化的流行文化例子当然是政治坐标系。同样，我们并不是仅仅寻找“保守派”和“进步派”，而是去找到那些区分它们的**因素**。
- en: Let’s Model!
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始建模吧！
- en: Now that we got the philosophy out of the way, we can get our hands dirty with
    designing computational models based on our conceptual understanding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经解决了哲学问题，就可以动手设计基于我们概念理解的计算模型了。
- en: Semantic Representations
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义表示
- en: Classically the way we represented the semantic content of texts, was the so-called
    **bag-of-words** model. Essentially you make the very strong, and almost trivially
    wrong assumption, that the unordered collection of words in a document is constitutive
    of its semantic content. While these representations are plagued with a number
    of issues ([curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality),
    discrete space, etc.) they have been demonstrated useful by decades of research.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的语义表示方法是所谓的**词袋模型**。本质上，你做出了一个非常强烈、几乎是显而易见的错误假设，即文档中无序的词汇集合构成了它的语义内容。虽然这些表示方式存在许多问题（如[维度灾难](https://en.wikipedia.org/wiki/Curse_of_dimensionality)、离散空间等），但通过数十年的研究，它们已被证明是有用的。
- en: Luckily for us, the state of the art has progressed beyond these representations,
    and we have access to models that can represent text in context. [Sentence Transformers](http://sbert.net)
    are transformer models which can encode passages into a high-dimensional continuous
    space, where semantic similarity is indicated by vectors having high [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).
    In this article I will mainly focus on models that use these representations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，技术已经超越了这些表示方式，我们可以使用能够在上下文中表示文本的模型。[Sentence Transformers](http://sbert.net)是基于变换器（Transformer）的模型，可以将段落编码成一个高维的连续空间，其中语义相似性通过向量之间的高[余弦相似度](https://en.wikipedia.org/wiki/Cosine_similarity)来表示。在这篇文章中，我将主要关注使用这些表示的模型。
- en: Clustering Models
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类模型
- en: Models that are currently the most widespread in the topic modeling community
    for contextually sensitive topic modeling ([Top2Vec](https://github.com/ddangelov/Top2Vec),
    [BERTopic](https://maartengr.github.io/BERTopic/index.html)) are based on the
    clustering conceptualization of topics.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在主题建模社区中最广泛使用的模型（如[Top2Vec](https://github.com/ddangelov/Top2Vec), [BERTopic](https://maartengr.github.io/BERTopic/index.html)）是基于聚类的主题概念化方法。
- en: '![](../Images/0778eab83605e5441f6000960fd4a37e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0778eab83605e5441f6000960fd4a37e.png)'
- en: Clusters in Semantic Space Discovered by BERTopic (figure from BERTopic’s documentation)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic发现的语义空间中的集群（图源自BERTopic文档）
- en: 'They discover topics in a process that consists of the following steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 他们在以下步骤中发现主题：
- en: Reduce dimensionality of semantic representations using [UMAP](https://umap-learn.readthedocs.io/en/latest/)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[UMAP](https://umap-learn.readthedocs.io/en/latest/)降低语义表示的维度
- en: Discover cluster hierarchy using [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html)发现集群层次结构
- en: Estimate importances of terms for each cluster using post-hoc descriptive methods
    (c-TF-IDF, proximity to cluster centroid)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用事后描述性方法（c-TF-IDF，接近聚类中心）估计每个集群的术语重要性
- en: These models have gained a lot of traction, mainly due to their interpretable
    topic descriptions and their ability to recover hierarchies, as well as to learn
    the number of topics from the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已经获得了广泛的关注，主要得益于它们可解释的主题描述、恢复层次结构的能力，以及从数据中学习主题数目的能力。
- en: If we want to model nuances in topical content, and understand factors of semantics,
    clustering models are not enough.
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们想要建模主题内容的细微差别，并理解语义的因素，单靠聚类模型是不够的。
- en: I do not intend to go into great detail about the practical advantages and limitations
    of these approaches, but most of them stem from philosophical considerations outlined
    above.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我不打算深入讨论这些方法的实际优势和局限性，但它们中的大多数源于上述的哲学考虑。
- en: Semantic Signal Separation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义信号分离
- en: If we are to discover the axes of semantics in a corpus, we will need a new
    statistical model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要在语料库中发现语义轴线，我们将需要一个新的统计模型。
- en: We can take inspiration from classical topic models, such as **Latent Semantic
    Allocation.** LSA utilizes matrix decomposition to find latent components in *bag-of-words*
    representations. LSA’s main goal is to find words that are highly correlated,
    and explain their cooccurrence as an underlying semantic component.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从经典的主题模型中汲取灵感，例如**潜在语义分配（LSA）**。LSA利用矩阵分解技术，在*词袋模型*表示中找到潜在的成分。LSA的主要目标是找出高度相关的词汇，并将它们的共现解释为一个潜在的语义成分。
- en: 'Since we are no longer dealing with bag-of-words, explaining away correlation
    might not be an optimal strategy for us. Orthogonality is not statistical independence.
    Or in other words: Just because two components are uncorrelated, it does not mean
    that they are statistically independent.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不再处理词袋模型，单纯解释相关性可能不是最优策略。正交性并不等于统计独立性。换句话说：仅仅因为两个组件不相关，并不意味着它们在统计上是独立的。
- en: Orthogonality is not statistical independence
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正交性并不等于统计独立性
- en: Other disciplines have luckily come up with decomposition models that discover
    maximally independent components. **Independent Component Analysis** has been
    extensively used in Neuroscience to discover and remove noise signals from EEG
    data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其他学科幸运地提出了分解模型，可以发现最大程度上独立的组件。**独立成分分析**已经广泛应用于神经科学中，用于从脑电图数据中发现并去除噪声信号。
- en: '![](../Images/372ef543b05df8a239baf1738c27f957.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/372ef543b05df8a239baf1738c27f957.png)'
- en: Difference between Orthogonality and Independence Demonstrated with PCA and
    ICA (Figure from scikit-learn’s documentation)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA和ICA演示正交性与独立性的区别（图源自scikit-learn的文档）
- en: The main idea behind Semantic Signal Separation is that we can find maximally
    independent underlying semantic signals in a corpus of text by decomposing representations
    with ICA.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 语义信号分离的主要思想是，我们可以通过使用独立成分分析（ICA）分解表示，发现语料库中文本的最大独立语义信号。
- en: We can gain human-readable descriptions of topics by taking terms from the corpus
    that rank highest on a given component.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提取语料库中在给定维度上排名最高的术语，获得人类可读的主题描述。
- en: 'Case Study: Machine Learning Papers'
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：机器学习论文
- en: To demonstrate the usefulness of Semantic Signal Separation for understanding
    semantic variation in corpora, we will fit a model on a dataset of approximately
    118k machine learning abstracts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示语义信号分离在理解语料库中语义变化方面的有效性，我们将在一个大约118k机器学习摘要的数据集上拟合一个模型。
- en: 'To reiterate once again what we’re trying to achieve here: We want to establish
    the dimensions, along which all machine learning papers are distributed. Or in
    other words we would like to build a spatial theory of semantics for this corpus.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 再次重申我们这里要实现的目标：我们希望建立所有机器学习论文分布的维度。换句话说，我们想为这个语料库建立一个语义空间理论。
- en: For this we are going to use a Python library I developed called [Turftopic](https://x-tabdeveloping.github.io/turftopic/),
    which has implementations of most topic models that utilize representations from
    transformers, including Semantic Signal Separation. Additionally we are going
    to install the HuggingFace datasets library so that we can download the corpus
    at hand.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用我开发的一个Python库，叫做[Turftopic](https://x-tabdeveloping.github.io/turftopic/)，它实现了大多数使用transformer表示的主题模型，包括语义信号分离。此外，我们还将安装HuggingFace数据集库，以便下载当前的语料库。
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let us download the data from HuggingFace:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从HuggingFace下载数据：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are then going to run Semantic Signal Separation on this data. We are going
    to use the [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)
    Sentence Transformer, as it is quite fast, but provides reasonably high quality
    embeddings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对这些数据运行语义信号分离。我们将使用[all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2)句子变换器，因为它速度较快，但提供合理高质量的嵌入。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/beadaa38e648757caad11a37033a9f36.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beadaa38e648757caad11a37033a9f36.png)'
- en: Topics Found in the Abstracts by Semantic Signal Separation
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 语义信号分离在摘要中找到的主题
- en: These are highest ranking keywords for the ten axes we found in the corpus.
    You can see that most of these are quite readily interpretable, and already help
    you see what underlies differences in machine learning papers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们在语料库中找到的十个维度的最高排名关键词。你可以看到，大多数这些关键词相当容易理解，并且已经帮助你看出机器学习论文中的差异背后的根本原因。
- en: I will focus on three axes, sort of arbitrarily, because I found them to be
    interesting. I’m a Bayesian evangelist, so Topic 7 seems like an interesting one,
    as it seems that this component describes how probabilistic, model based and causal
    papers are. Topic 6 seems to be about noise detection and removal, and Topic 1
    is mostly concerned with measurement devices.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我将集中讨论三个维度，稍微随意一些，因为我发现它们很有趣。我是一个贝叶斯信徒，所以主题7看起来很有意思，因为它似乎描述了概率性、基于模型和因果性的论文。主题6似乎与噪声检测和去除有关，而主题1主要涉及测量设备。
- en: We are going to produce a plot where we display a subset of the vocabulary where
    we can see how high terms rank on each of these components.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将生成一个图表，展示词汇的一个子集，并显示每个术语在这些组成部分上的排名。
- en: First let’s extract the vocabulary from the model, and select a number of words
    to display on our graphs. I chose to go with words that are in the 99th percentile
    based on frequency (so that they still remain somewhat visible on a scatter plot).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从模型中提取词汇，并选择一些词汇显示在我们的图表上。我选择了基于频率处于99百分位数的词汇（这样它们仍然能在散点图上保持一定的可见性）。
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will make a *DataFrame* with the three selected dimensions and the terms
    so we can easily plot later.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个*DataFrame*，包含三个选定维度和术语，以便后续轻松绘图。
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will use the Plotly graphing library for creating an interactive scatter
    plot for interpretation. The X axis is going to be the inference/Bayesian topic,
    Y axis is going to be the noise topic, and the color of the dots is going to be
    determined by the measurement device topic.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Plotly图形库创建一个交互式散点图进行解释。X轴将是推断/贝叶斯主题，Y轴将是噪声主题，点的颜色将由测量设备主题决定。
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/dfbbf0f51c7a2c319fd967e2dc0c5534.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfbbf0f51c7a2c319fd967e2dc0c5534.png)'
- en: Plot of Most Frequent Terms in the Corpus Distributed by Semantic Axes
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 按语义轴分布的语料库中最常见术语的图
- en: We can already infer a lot about the semantic structure of our corpus based
    on this visualization. For instance we can see that papers that are concerned
    with efficiency, online fitting and algorithms score very low on statistical inference,
    this is somewhat intuitive. On the other hand what Semantic Signal Separation
    has already helped us do in a data-based approach is confirm, that deep learning
    papers are not very concerned with statistical inference and Bayesian modeling.
    We can see this from the words “network” and “networks” (along with “convolutional”)
    ranking very low on our Bayesian axis. This is one of the criticisms the field
    has received. We’ve just given support to this claim with empirical evidence.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以从这个可视化中推断出很多关于语料库语义结构的信息。例如，我们可以看到，关注效率、在线拟合和算法的论文在统计推断上的得分非常低，这在某种程度上是直观的。另一方面，语义信号分离已经帮助我们通过基于数据的方法确认，深度学习论文并不太关注统计推断和贝叶斯建模。我们可以从词汇“网络”和“网络”（以及“卷积”）在我们的贝叶斯轴上的排名非常低看出这一点。这是该领域受到的一项批评。我们刚刚通过实证证据支持了这一说法。
- en: Deep learning papers are not very concerned with statistical inference and Bayesian
    modeling, which is one of the criticisms the field has received. We’ve just given
    support to this claim with empirical evidence.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 深度学习论文对统计推断和贝叶斯建模并不十分关注，这是该领域受到的一项批评。我们刚刚通过实证证据支持了这一说法。
- en: We can also see that clustering and classification is very concerned with noise,
    but that agent-based models and reinforcement learning isn’t.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，聚类和分类对噪声非常关注，但基于代理的模型和强化学习则不太关注。
- en: Additionally an interesting pattern we may observe is the relation of our Noise
    axis to measurement devices. The words “image”, “images”, “detection” and “robust”
    stand out as scoring very high on our measurement axis. These are also in a region
    of the graph where noise detection/removal is relatively high, while talk about
    statistical inference is low. What this suggests to us, is that measurement devices
    capture a lot of noise, and that the literature is trying to counteract these
    issues, but mainly not by incorporating noise into their statistical models, but
    by preprocessing. This makes a lot of sense, as for instance, Neuroscience is
    known for having very extensive preprocessing pipelines, and many of their models
    have a hard time dealing with noise.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可能观察到的一个有趣模式是噪声轴与测量设备之间的关系。术语“图像”、“图像”、“检测”和“鲁棒”在我们的测量轴上得分非常高。这些词汇也位于图表中噪声检测/去除较高的区域，而统计推断的讨论较少。这向我们表明，测量设备捕获了大量噪声，而文献试图解决这些问题，但主要不是通过将噪声纳入统计模型，而是通过预处理。这是非常有道理的，因为例如，神经科学以其非常广泛的预处理流程而著称，而且许多神经科学模型在处理噪声时遇到困难。
- en: '![](../Images/3692b4bfdb6d3e852f642633f9bee09b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3692b4bfdb6d3e852f642633f9bee09b.png)'
- en: Noise in Measurement Devices’ Output is Countered with Preprocessing
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测量设备输出中的噪声通过预处理来应对
- en: We can also observe that the lowest scoring terms on measurement devices is
    “text” and “language”. It seems that NLP and machine learning research is not
    very concerned with neurological bases of language, and psycholinguistics. Observe
    that “latent” and “representation is also relatively low on measurement devices,
    suggesting that machine learning research in neuroscience is not super involved
    with representation learning.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察到，在测量设备上的得分最低的术语是“文本”和“语言”。这似乎表明，自然语言处理和机器学习研究并不太关注语言的神经学基础和心理语言学。注意，“潜在”和“表示”在测量设备上的得分也相对较低，这表明神经科学中的机器学习研究并不特别关注表示学习。
- en: '![](../Images/d20b1791c23bc4bb5681298be1936119.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d20b1791c23bc4bb5681298be1936119.png)'
- en: Text and Language are Rarely Related with Measurement Devices
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和语言与测量设备的关系很少
- en: Of course the possibilities from here are endless, we could spend a lot more
    time interpreting the results of our model, but my intent was to demonstrate that
    we can already find claims and establish a theory of semantics in a corpus by
    using Semantic Signal Separation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，从这里出发的可能性是无限的，我们可以花费更多时间解释模型的结果，但我的目的是展示，通过使用语义信号分离，我们已经能够在语料库中找到观点，并建立一个语义学理论。
- en: Semantic Signal Separation should mainly be used as an exploratory measure for
    establishing theories, rather than taking its results as proof of a hypothesis.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 语义信号分离应该主要作为一种探索性手段，用于建立理论，而不是将其结果视为假设的证明。
- en: One thing I would like to emphasize is that Semantic Signal Separation should
    mainly be used as an exploratory measure for establishing theories, rather than
    taking its results as proof of a hypothesis. What I mean here, is that our results
    are sufficient for gaining an intuitive understanding of differentiating factors
    in our corpus, an then building a theory about what is happening, and why it is
    happening, but it is not sufficient for establishing the theory’s correctness.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我想强调的一点是，语义信号分离应该主要作为一种探索性手段，用于建立理论，而不是将其结果视为假设的证明。我的意思是，我们的结果足以让我们直观地理解语料库中区分因素的差异，然后建立一个关于发生了什么以及为什么发生的理论，但这不足以证明该理论的正确性。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Exploratory data analysis can be confusing, and there are of course no one-size-fits-all
    solutions for understanding your data. Together we’ve looked at how to enhance
    our understanding with a model-based approach from theory, through computational
    formulation, to practice.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析可能会令人困惑，当然也没有一种适用于所有情况的解决方案来理解你的数据。我们一起探讨了如何通过基于模型的方法，从理论到计算公式，再到实践，提升我们对数据的理解。
- en: I hope this article will serve you well when analysing discourse in large textual
    corpora. If you intend to learn more about topic models and exploratory text analysis,
    make sure to have a look at some of my other articles as well, as they discuss
    some aspects of these subjects in greater detail.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望本文在分析大规模文本语料库中的话语时能对你有所帮助。如果你打算深入了解主题模型和探索性文本分析，确保也查看一下我其他的文章，因为它们会更详细地讨论这些主题的某些方面。
- en: '*(( Unless stated otherwise, figures were produced by the author. ))*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*((除非另有说明，所有图表均由作者制作。))*'
