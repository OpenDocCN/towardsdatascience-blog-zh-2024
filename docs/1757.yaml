- en: AI, Write and Style My CV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-write-and-style-my-cv-fb3168a5b10e?source=collection_archive---------7-----------------------#2024-07-18](https://towardsdatascience.com/ai-write-and-style-my-cv-fb3168a5b10e?source=collection_archive---------7-----------------------#2024-07-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e34ee1d63b0f9106d336572f30624c9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Created with Midjourney by Author
  prefs: []
  type: TYPE_NORMAL
- en: Agentic Workflow Writing Structured Documents, A Line-by-Line Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@AJOhrn?source=post_page---byline--fb3168a5b10e--------------------------------)[![Anders
    Ohrn](../Images/64c8f1d9dea93adf16608b1b6d52c5b2.png)](https://medium.com/@AJOhrn?source=post_page---byline--fb3168a5b10e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fb3168a5b10e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fb3168a5b10e--------------------------------)
    [Anders Ohrn](https://medium.com/@AJOhrn?source=post_page---byline--fb3168a5b10e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fb3168a5b10e--------------------------------)
    ·22 min read·Jul 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Not only [the Mona Lisa](https://heni.com/talks/mona-lisa-painting-beyond-portraiture)
    and [the Vitruvian Man](https://nicofranz.art/en/leonardo-da-vinci/vitruvian-man)
    but also [the Curriculum Vitae](https://artsandculture.google.com/story/leonardo-s-cover-letter-biblioteca-ambrosiana/sQVRpZ4EhlnSKg?hl=en)
    (CV), are cultural artifacts by Leonardo Da Vinci’s hand that resonate and reproduce
    in the present time. The CV is not the exclusive way to present oneself to the
    job market. Yet the CV persists despite the many innovations in information and
    graphics technology since Leonardo enumerated on paper his skills and abilities
    to the Duke of Milan.
  prefs: []
  type: TYPE_NORMAL
- en: 'In high-level terms, the creation of a CV:'
  prefs: []
  type: TYPE_NORMAL
- en: summarizes past accomplishments and experiences of a person in a document form,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: in a manner relevant to a specific audience, who in a short time assesses the
    person’s relative and absolute utility to some end,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: where the style and layout of the document form are chosen to be conducive to
    a favourable assessment by said audience.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are semantic operations in service of an objective under vaguely stated
    constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are the premier means to execute semantic operations
    with computers, especially if the operations are ambiguous in the way human communication
    often is. The most common way to date to interact with LLMs is a chat app — [ChatGPT](https://chatgpt.com/),
    [Claude](https://claude.ai/), [Le Chat](https://chat.mistral.ai/chat) etc. We,
    the human users of said chat apps, define somewhat loosely the semantic operations
    by way of our chat messages.
  prefs: []
  type: TYPE_NORMAL
- en: Certain applications, however, are better served by a different interface and
    a different way to create semantic operations. Chat is not the be-all and end-all
    of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: I will use the [APIs for the LLM models of Anthropic](https://www.anthropic.com/api)
    (especially Sonnet and Haiku) to create a basic application for CV assembly. It
    relies on a workflow of agents working in concert (an [*agentic workflow*](https://www.deeplearning.ai/the-batch/welcoming-diverse-approaches-keeps-machine-learning-strong/)),
    each agent performing some semantic operation in the chain of actions it takes
    to go from a blob of personal data and history to a structured CV document worthy
    of its august progenitor…
  prefs: []
  type: TYPE_NORMAL
- en: This is a tutorial on building a small yet complete LLM-powered non-chat application.
    In what follows I describe both the code, my reasons for a particular design,
    and where in the bigger picture each piece of Python code fits.
  prefs: []
  type: TYPE_NORMAL
- en: The CV creation app is a useful illustration of AIs working on the general task
    of structured style-content generation.
  prefs: []
  type: TYPE_NORMAL
- en: Before Code & How — Show What & Wow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine a collection of personal data and lengthy career descriptions, mostly
    text, organized into a few files where information is scattered. In that collection
    is the raw material of a CV. Only it would take effort to separate the relevant
    from the irrelevant, distill and refine it, and give it a good and pleasant form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next imagine running a script `make_cv` and pointing it to a job ad, a CV template,
    a person and a few specification parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then wait a few seconds while the data is shuffled, transformed and rendered,
    after which the script outputs a neatly styled and populated one-pager two-column
    CV.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc89250d3d5365bdbaf4bbcd618e1415.png)'
  prefs: []
  type: TYPE_IMG
- en: A CV, neat style and abstracted content, generated with an agentic workflow
    of Anthropic LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Nice! Minimal layout and style in green hues, good contrast between text and
    background, not just bland default fonts, and the content consists of brief and
    to-the-point descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: But wait… are these documents not supposed to make us stand out?
  prefs: []
  type: TYPE_NORMAL
- en: 'Again with the aid of the Anthropic LLMs, a different template is created (keywords:
    *wild and wacky world of early 1990s web design*), and the same content is given
    a new glorious form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ef1f399ded7688af94137c18729b6c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: A CV, crazy 1990s-webpage-style and content, generated with an agentic workflow
    of Antropic LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: If you ignore the flashy animations and peculiar colour choices, you’ll find
    that the content and layout are almost identical to the previous CV. This isn’t
    by chance. The agentic workflow’s generative tasks deal separately with content,
    form, and style, not resorting to an all-in-one solution. The workflow process
    rather mirrors the modular structure of the standard CV.
  prefs: []
  type: TYPE_NORMAL
- en: That is, the generative process of the agentic workflow is made to operate within
    meaningful constraints. That can enhance the practical utility of generative AI
    applications — design, after all, has been said to [depend largely on constraints](https://markwunsch.com/eames).
    For example, branding, style guides, and information hierarchy are useful, principled
    constraints we should want in the non-chat outputs of the generative AI — be they
    CVs, reports, UX, product packaging etc.
  prefs: []
  type: TYPE_NORMAL
- en: The agentic workflow that accomplishes all that is illustrated below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1625c41c085d5a005ec45bdfbda7487.png)'
  prefs: []
  type: TYPE_IMG
- en: High-level data flow diagram of the application
  prefs: []
  type: TYPE_NORMAL
- en: If you wish to skip past the descriptions of code and software design, *Gregor
    Samsa* is your lodestar. When I return to discussing applications and outputs,
    I will do so for synthetic data for the fictional character Gregor Samsa, so keyword-search
    your way forward.
  prefs: []
  type: TYPE_NORMAL
- en: The complete code is available in [this GitHub repo](https://github.com/anderzzz/everyone_knows_it/),
    free and without any guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: Job Ad Pre-Processing, DAO and Prompt Assembly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is often said that one should tailor a CV's content to the job ad. Since
    job ads are frequently verbose, sometimes containing legal boilerplate and contact
    information, I wish to extract and summarize only the relevant features and use
    that text in subsequent tasks.
  prefs: []
  type: TYPE_NORMAL
- en: To have shared interfaces when retrieving data, I make a basic [data-access
    object (DAO)](https://www.geeksforgeeks.org/data-access-object-pattern/), which
    defines a common interface to the data, which in the tutorial example is stored
    in text and JSON files locally (stored in `registry_job_ads`), but generally can
    be any other job ad database or API.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize or abstract text is a semantic operation LLMs are well-suited for.
    To that end,
  prefs: []
  type: TYPE_NORMAL
- en: an instruction prompt is required to make the LLM process the text appropriately
    for the task;
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and the LLM model from Anthropic has to be selected along with its parameters
    (e.g. temperature);
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: and the instructed LLM is invoked via a third-party API with its specific requirements
    on syntax, error checking etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To keep these three distinct concerns separate, I introduce some abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: The class diagram below illustrates key methods and relationships of the agent
    that extract key qualities of the job ad.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8ca9af115bd8907ec9d09a1569f9fcb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration file `agent_model_extractor_confs` is a JSON file that in
    part looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional configurations are added to this file as further agents are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is what focuses the general LLM onto a specific capability. I use
    Jinja templates to assemble the prompt. This is a [flexible and established method
    to create text files with programmatic content](https://realpython.com/primer-on-jinja-templating/).
    For the fairly straightforward job ad extractor agent, the logic is simple — read
    text from a file and return it — but when I get to the more advanced agents, Jinja
    templating will prove more helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'And the prompt template for `agent_type=''JobAdQualityExtractor` is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Invoking the Agent, Without Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A model name (e.g. `claude-3–5-sonnet-20240620`), a prompt and an Anthropic
    client are the least we need to send a request to the Anthropic APIs to execute
    an LLM. The job ad quality extractor agent has it all. It can therefore instantiate
    and execute the “bare metal” agent type.
  prefs: []
  type: TYPE_NORMAL
- en: Without any memory of prior use or any other functionality, the bare metal agent
    invokes the LLM once. Its scope of concern is how Anthropic formats its inputs
    and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: I create an abstract base class as well, `Agent`. It is not strictly required
    and for a task as basic as CV creation of limited use. However, if we were to
    keep building on this foundation to deal with more complex and diverse tasks,
    [abstract base classes are good practice](https://stackoverflow.com/questions/3570796/why-use-abstract-base-classes-in-python).
  prefs: []
  type: TYPE_NORMAL
- en: The `send_request_to_anthropic_message_creation` is a [simple wrapper around
    the call to the Anthropic](https://github.com/anderzzz/everyone_knows_it/blob/6c96a71b3b1be18918a6caad96b9ec5c2148de2e/src/semantics.py#L24)
    API.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all that is needed to obtain the job ad summary. In short, the steps
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate a job ad quality extractor agent, which entails gathering the associated
    prompt and Anthropic model parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invoke the job ad data access object with a company name and position to get
    the complete job ad text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the extraction on the complete job ad text, which entails a one-time request
    to the APIs of the Anthropic LLMs; a text string is returned with the generated
    summary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In terms of code in the `make_cv` script, these steps read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The top part of the data flow diagram has thus been described.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ca2aea8419f959d09fb2dd39f91c9cc.png)'
  prefs: []
  type: TYPE_IMG
- en: How To Build Agents That Use Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All other types of agents in the agentic workflow use tools. Most LLMs nowadays
    are equipped with this useful capacity. Since I described the bare metal agent
    above, I will describe the tool-using agent next, since it is the foundation for
    much to follow.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs generate string data through a sequence-to-sequence map. In chat applications
    as well as in the job ad quality extractor, the string data is (mostly) text.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the string data can also be an array of function arguments. For example,
    if I have an executable function, `add`, that adds two integer variables, `a`
    and `b`, and returns their sum, then the string data to run `add` could be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So if the LLM outputs this string of function arguments, it can in code lead
    to the function call `add(a=2, b=2)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question is: how should the LLM be instructed such that it knows when and
    how to generate string data of this kind and specific syntax?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alongside the `AgentBareMetal` agent, I define another agent type, which also
    inherits the `Agent` base class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This differs from the bare metal agent in two regards:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self.tools` is a list created during instantiation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tool_return` is created during execution by invoking a function obtained from
    a registry, `registry_tool_name_2_func`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The former object contains the data instructing the Anthropic LLMs on the format
    of the string data it can generate as input arguments to different tools. The
    latter object comes about through the execution of the tool, given the LLM-generated
    string data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tools_cv_data` file contains a JSON string formatted to define a function
    interface (but not the function itself). The string has to conform to a [very
    specific schema for the Anthropic LLM](https://docs.anthropic.com/en/docs/build-with-claude/tool-use#specifying-tools)
    to understand it. A snippet of this JSON string is:'
  prefs: []
  type: TYPE_NORMAL
- en: From the specification above we can tell that if, for example, the initialization
    of `AgentToolInvokeReturn` includes the string `biography` in the `tools` argument,
    then the Anthropic LLM will be instructed that it can generate a function argument
    string to a function called `create_biography`. What kind of data to include in
    each argument is left to the LLM to figure out from the description fields in
    the JSON string. These descriptions are therefore mini-prompts, which guide the
    LLM in its sense-making.
  prefs: []
  type: TYPE_NORMAL
- en: The function that is associated with this specification I implement through
    the following two definitions.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the tool name `create_biography` is associated with the class builder
    function `Biography.build`, which creates and returns an instance of the data
    class `Biography`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the attributes of the data class are perfectly mirrored in the JSON
    string that is added to the `self.tools` variable of the agent. That implies that
    the strings returned from the Anthropic LLM will fit perfectly into the class
    builder function for the data class.
  prefs: []
  type: TYPE_NORMAL
- en: 'To put it all together, take a closer look at the inner loop of the `run` method
    of `AgentToolInvokeRetur` shown again below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The steps are:'
  prefs: []
  type: TYPE_NORMAL
- en: The response from the Anthropic LLM is checked to be a string of function arguments,
    not ordinary text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The name of the tool (e.g. `create_biography`), the string of function arguments
    and a unique tool use id are gathered.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The executable tool is retrieved from the registry (e.g. `Biography.build`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function is executed with the string function arguments (checking for errors)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have the output from the tool, we should decide what to do with it.
    Some applications integrate the tool outputs into the messages and execute another
    request to the LLM API. However, in the current application, I build agents that
    generate data objects, specifically subclasses of `CVData`. Hence, I design the
    agent to invoke the tool, and then simply return its output — hence the class
    name `AgentToolInvokeReturn`.
  prefs: []
  type: TYPE_NORMAL
- en: It is on this foundation I build agents which create the constrained data structures
    I want to be part of the CV.
  prefs: []
  type: TYPE_NORMAL
- en: Structured CV Data Extractor Agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The class diagram for the agent that generates structured biography data is
    shown below. It has much in common with the previous class diagram for the agent
    that extracted the qualities from job ads.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b65dae5790f6ce8214d263ae7a290bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two distinctions to the previous agent `JobAdQualityExtractor`:'
  prefs: []
  type: TYPE_NORMAL
- en: The tool names are retrieved as a function of the class attribute `cv_data`
    (line 47 in the snippet above). So when the agent with tools is instantiated,
    the sequence of tool names is given by a registry that associates a type of CV
    data (e.g. `Biography`) with the key used in the `tools_cv_data` JSON string described
    above, e.g. `biography`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The prompt for the agent is rendered with variables (lines 48–52). Recall the
    use of Jinja templates above. This enables the injection of the relevant qualities
    of the job ad and a target number of words to be used in the “about me” section.
    The specific template for the biography agent is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/b8b0e51c62f7c269fc45dcde8924ada9.png)'
  prefs: []
  type: TYPE_IMG
- en: image of prompt template for biography extractor agent, note the two variables
  prefs: []
  type: TYPE_NORMAL
- en: That means as it is instantiated, the agent is made aware of the job ad it should
    tailor its text output to.
  prefs: []
  type: TYPE_NORMAL
- en: 'So when it receives the raw text data, it performs the instruction and returns
    an instance of the data class `Biography`. With identical reasons and similar
    software design, I generate additional extractor agents and CV data classes and
    tools definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can now go up a level in the abstractions. With extractor agents in place,
    they should be joined to the raw data from which to extract, summarize, rewrite
    and distill the CV data content.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration of Data Retrieval and Extraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The part of the data diagram to explain next is the highlighted part.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd763800d43e6dd6b387a934bb3bb9c0.png)'
  prefs: []
  type: TYPE_IMG
- en: In principle, we can give the extractor agents access to all possible text we
    have for the person we are making the CV for. But that means the agent has to
    process a great deal of data irrelevant to the specific section it is focused
    on, e.g. formal educational details are hardly found in personal stream-of-consciousness
    blogging.
  prefs: []
  type: TYPE_NORMAL
- en: This is where important questions of retrieval and search usually enter the
    design considerations of LLM-powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do we try to find the relevant raw data to apply our agents to, or do we throw
    all we have into the large context window and let the LLM sort out the retrieval
    question? [Many](https://cohere.com/blog/rag-is-here-to-stay) [have had](https://x.com/Francis_YAO_/status/1758934303655030929)
    [their say](https://www.reddit.com/r/MachineLearning/comments/1ax6j73/rag_vs_long_context_models_discussion/)
    [on the](https://www.linkedin.com/posts/emollick_a-problem-with-rag-in-practice-i-asked-activity-7172666908230561792-muvu/)
    [matter](https://www.llamaindex.ai/blog/towards-long-context-rag). It is a worthwhile
    debate because there is a lot of truth in the below statement:'
  prefs: []
  type: TYPE_NORMAL
- en: For my application, I will keep it simple — retrieval and search are saved for
    another day.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, I will work with semi-structured raw data. While we have a general
    understanding of the content of the respective documents, internally they consist
    mostly of unstructured text. This scenario is common in many real-world cases
    where useful information can be extracted from the metadata on a file system or
    data lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first piece in the retrieval puzzle is the data access object (DAO) for
    the template table of contents. At its core, that is a JSON string like this:'
  prefs: []
  type: TYPE_NORMAL
- en: It associates the name of a CV template, e.g. `single_column_0`, with a list
    of required data sections — the `CVData` data classes described in an earlier
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I encode which raw data access object should go with which CV data section.
    In my example, I have a modest collection of raw data sources, each accessible
    through a DAO, e.g. `PersonsEmploymentDAO`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note in this code that the Biography and Skills CV data are created from several
    raw data sources. These associations are easily modified if additional raw data
    sources become available — append the new DAO to the tuple — or made configurable
    at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: It is then a matter of matching the raw data and the CV data extractor agents
    for each required CV section. That is the data flow that the *orchestrator* implements.
    The image below is a zoomed-in data flow diagram for the `CVDataExtractionOrchestrator`
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/764f2ddeeb4df8fc556768bb24f2d28a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In code, the orchestrator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'And putting it all together in the script `make_cv` we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is within the orchestrator therefore that the calls to the Anthropic LLMs
    take place. Each call is done with a programmatically created instruction prompt,
    typically including the job ad summary, some parameters of how wordy the CV sections
    should be, plus the raw data, keyed on the name of the person.
  prefs: []
  type: TYPE_NORMAL
- en: The loop yields a collection of structured CV data class instances once all
    the agents that use tools have concluded their tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interlude: None, <UNKNOWN>, “missing”'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Anthropic LLMs are remarkably good at matching their generated content to
    the output schema required to build the data classes. For example, I do *not*
    sporadically get a phone number in the email field, *nor* are invalid keys dreamt
    up, which would break the build functions of the data classes.
  prefs: []
  type: TYPE_NORMAL
- en: But when I ran tests, I encountered an imperfection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look again at how the Biography CV data is defined:'
  prefs: []
  type: TYPE_NORMAL
- en: If for example, the LLM does not find a GitHub URL in a person’s raw data, then
    it is permissible to return `None` for that field, since that attribute in the
    data class is optional. That is how I want it to be since it makes the rendering
    of the final CV simpler (see below).
  prefs: []
  type: TYPE_NORMAL
- en: But the LLMs regularly return a string value instead, typically `'<UNKNOWN>'`.
    To a human observer, there is no ambiguity about what this means. It is not a
    hallucination in that it is a fabrication that looks real yet is without basis
    in the raw data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it is an issue for a rendering algorithm that uses simple conditional
    logic, such as the following in a Jinja template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: A problem that is semantically obvious to a human, but syntactically messy,
    is perfect for LLMs to deal with. Inconsistent labelling in the pre-LLM days caused
    many headaches and lengthy lists of creative string-matching commands (anyone
    who has done data migrations of databases with many free-text fields can attest
    to that).
  prefs: []
  type: TYPE_NORMAL
- en: So to deal with the imperfection, I create another agent that operates on the
    output of one of the other CV data extractor agents.
  prefs: []
  type: TYPE_NORMAL
- en: This agent uses objects described in previous sections. The difference is that
    it takes a collection of CV data classes as input, and is instructed to empty
    any field “where the value is somehow labelled as unknown, undefined, not found
    or similar” (part of [the full prompt](https://github.com/anderzzz/everyone_knows_it/blob/main/src/prompt_templates/ClearUndefinedCVDataEntries.txt)).
  prefs: []
  type: TYPE_NORMAL
- en: A joint agent is created. It first executes the creation of biography CV data,
    as described earlier. Second, it executes the *clear undefined agent* on the output
    of the former agent to fix issues with any <UNKNOWN> strings.
  prefs: []
  type: TYPE_NORMAL
- en: This agent solves the problem, and therefore I use it in the orchestration.
  prefs: []
  type: TYPE_NORMAL
- en: Could this imperfection be solved with a different instruction prompt? Or would
    a simple string-matching fix be adequate? Maybe.
  prefs: []
  type: TYPE_NORMAL
- en: However, I use the simplest and cheapest LLM of Anthropic (*haiku*), and because
    of the modular design of the agents, it is an easy fix to implement and append
    to the data pipeline. The ability to construct *joint agents* that comprise multiple
    other agents is one of the design patterns advanced agentic workflows use.
  prefs: []
  type: TYPE_NORMAL
- en: Render With CV Data Objects Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final step in the workflow is comparatively simple thanks to that we spent
    the effort to create structured and well-defined data objects. The contents of
    said objects are specifically placed within a Jinja HTML template through syntax
    matching.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if `biography` is an instance of the Biography CV data class and
    `env` a Jinja environment, then the following code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: would for `test_template.html` like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'match the name and email attributes of the `Biography` data class and return
    something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The function `populate_html` takes all the generated CV Data objects and returns
    an HTML file using Jinja functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the script `make_cv` the third and final step is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This completes the agentic workflow. The raw data has been distilled, the content
    put inside structured data objects that mirror the information design of standard
    CVs, and the content rendered in an HTML template that encodes the style choices.
  prefs: []
  type: TYPE_NORMAL
- en: What About the CV Templates — How to Make Them?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The CV templates are Jinja templates of HTML files. Any tool that can create
    and edit HTML files can therefore be used to create a template. As long as the
    variable naming conforms to the names of the CV data classes, it will be compatible
    with the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'So for example, the following part of a Jinja template would retrieve data
    attributes from an instance of [the](https://github.com/anderzzz/everyone_knows_it/blob/6c96a71b3b1be18918a6caad96b9ec5c2148de2e/src/cv_data.py#L123)
    `[Employments](https://github.com/anderzzz/everyone_knows_it/blob/6c96a71b3b1be18918a6caad96b9ec5c2148de2e/src/cv_data.py#L123)`
    [CV data class](https://github.com/anderzzz/everyone_knows_it/blob/6c96a71b3b1be18918a6caad96b9ec5c2148de2e/src/cv_data.py#L123),
    and create a list of employments with descriptions (generated by the LLMs) and
    data on duration (if available):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: I know very little about front-end development — even HTML and CSS are rare
    in the code I’ve written over the years.
  prefs: []
  type: TYPE_NORMAL
- en: I decided therefore to use LLMs to create the CV templates. After all, this
    is a task in which I seek to map an appearance and design sensible and intuitive
    to a human observer to a string of specific HTML/Jinja syntax — a kind of task
    LLMs have proven quite apt at.
  prefs: []
  type: TYPE_NORMAL
- en: I chose not to integrate this with the agentic workflow but appended it in the
    corner of the data flow diagram as a useful appendix to the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb064fae0ba6e2198f511f7f9772e2f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'I used Claude, the chat interface to Anthropic’s Sonnet LLM. I provided Claude
    with two things: an image and a prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: The image is a crude outline of a single-column CV I cook up quickly using a
    word processor and then screen-dump.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41dd269f292f1b11d7d877e4205301bd.png)'
  prefs: []
  type: TYPE_IMG
- en: screen dump of single-column CV layout used to guide Claude
  prefs: []
  type: TYPE_NORMAL
- en: The [prompt I give is fairly lengthy](https://github.com/anderzzz/everyone_knows_it/blob/main/template_creation_prompt/single_column.txt).
    It consists of three parts.
  prefs: []
  type: TYPE_NORMAL
- en: First, a statement of what I wish to accomplish and what information I will
    provide Claude as Claude executes the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of the prompt of this section reads:'
  prefs: []
  type: TYPE_NORMAL
- en: I wish to create a Jinja2 template for a static HTML page. The HTML page is
    going to present a CV for a person. The template is meant to be rendered with
    Python with Python data structures as input.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Second, a verbal description of the layout. In essence, a description of the
    image above, top to bottom, with remarks about relative font sizes, the order
    of the sections etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Third, a description of the data structures that I will use to render the Jinja
    template. In part, this prompt reads as shown in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4515484bcc8bc49d64e0f742d087d9ae.png)'
  prefs: []
  type: TYPE_IMG
- en: The prompt continues listing all the CV data classes.
  prefs: []
  type: TYPE_NORMAL
- en: To a human interpreter, who is knowledgeable in Jinja templating, HTML and Python
    data classes, this information is sufficient to enable matching the semantic description
    of where to place the email in the layout to the syntax `{{ biography.email }}`
    in the HTML Jinja template, and the description of where to place the LinkedIn
    profile URL (if available) in the layout to the syntax `{% if biography.linkedin_url
    %} <a href=”{{ biography.linkedin_url }}”>LinkedIn</a>{% endif }` and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Claude executes the task perfectly — no need for me to manually edit the template.
  prefs: []
  type: TYPE_NORMAL
- en: I ran the agent workflow with the single-column template and synthetic data
    for the persona *Gregor Samsa* (see more about him later).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00c8cb4900f435bcf0f717ad40ac1436.png)'
  prefs: []
  type: TYPE_IMG
- en: A decent CV. But I wanted to create variations and see what Claude and I could
    cook up.
  prefs: []
  type: TYPE_NORMAL
- en: 'So I created another prompt and screen dump. This time for a two-column CV.
    The crude outline I drew up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8a72bd271bd9026d8dd44b5168cf295.png)'
  prefs: []
  type: TYPE_IMG
- en: screen dump of two-column CV layout used to guide Claude
  prefs: []
  type: TYPE_NORMAL
- en: I reused the prompt for the single column, only changing the second part where
    I in words describe the layout.
  prefs: []
  type: TYPE_NORMAL
- en: It worked perfectly again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The styling, though, was a bit too bland for my taste. So as a follow-up prompt
    to Claude, I wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Love it! Can you redo the previous task but with one modification: add some
    spark and colour to it. Arial font, black and white is all a bit boring. I like
    a bit of green and nicer looking fonts. Wow me! Of course, it should be professional-looking
    still.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Had Claude responded with an annoyed comment that I must be a bit more specific,
    I would have empathized (in some sense of that word). Rather, Claude’s generative
    juices flowed and a template was created that when rendered looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc89250d3d5365bdbaf4bbcd618e1415.png)'
  prefs: []
  type: TYPE_IMG
- en: Nice!
  prefs: []
  type: TYPE_NORMAL
- en: 'Notably, the fundamental layout in the crude outline is preserved in this version:
    the placement of sections, the relative width of the two columns, and the lack
    of descriptions in the education entries etc. Only the style changed and was consistent
    with the vague specifications given. Claude’s generative capacities filled in
    the gaps quite well in my judgment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I next explored if Claude could keep the template layout and content specifications
    clear and consistent even when I dialled up the styling to eleven. So I wrote
    next to Claude:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazing. But now I want you to go all out! We are talking early 1990s web page
    aesthetic, blinking stuff, comic sans in the oddest places, weird and crazy colour
    contrasts. Full speed ahead, Claude, have some fun.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The result was glorious.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af654e943018cd04016c4b30ae751e17.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Who is this Gregor Samsa, what a free-thinker and not a trace of anxiety —
    hire the guy!*'
  prefs: []
  type: TYPE_NORMAL
- en: Even with this extreme styling, the specified layout is mostly preserved, and
    the text content as well. With a detailed enough prompt, Claude can seemingly
    create functional and nicely styled templates that can be part of the agentic
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: What About the Text Output?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Eye-catching style and useful layout aside, a CV must contain abbreviated text
    that succinctly and truthfully shows the fit between person and position.
  prefs: []
  type: TYPE_NORMAL
- en: To explore this I created synthetic data for a person [Gregor Samsa](https://de.wikipedia.org/wiki/Die_Verwandlung#Gregor_Samsa)
    — educated in Central Europe, working in lighting sales, with a general interest
    in entomology. I generated raw data on Gregor’s past and present, some from my
    imagination, and some from LLMs. The details are not important. The key point
    is that the text is too muddled and unwieldy to be copy-pasted into a CV. The
    data has to be found (e.g. the email address appears within one of Gregor’s general
    musings), summarized (e.g. the description of Gregor’s PhD work is very detailed),
    distilled and tailored to the relevant position (e.g. which skills are worth bringing
    to the fore), and all reduced to one or two friendly sentences in an about me
    section.
  prefs: []
  type: TYPE_NORMAL
- en: The text outputs were very well made. I had Anthropic’s most advanced and eloquent
    model, Sonnet, write the About Me sections. The tone rang true.
  prefs: []
  type: TYPE_NORMAL
- en: In my tests, I found no outright hallucinations. However, the LLMs had taken
    certain liberties in the Skills section.
  prefs: []
  type: TYPE_NORMAL
- en: Gregor is described in the raw data as working and studying in Prague and Vienna
    mostly with some online classes from English-language educators. In one generated
    CV, language skills in Czech, German and English were listed despite that the
    raw data does not explicitly declare such knowledge. The LLM had made a reasonable
    inference of skills. Still, these were not skills abstracted from the raw data
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: All code and synthetic data are available in [my GitHub repo](https://github.com/anderzzz/everyone_knows_it/).
    I used Python 3.11 to run it, and as long as you have an API key to Anthropic
    (assumed by the script to be stored in the environment variable `ANTHROPIC_API_KEY`),
    you can run and explore the application — and of course, to the best of my understanding,
    there are no errors, but I make no guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial has shown one way to use generative AI, made a case for useful
    constraints in generative applications, and shown how it all can be implemented
    working directly with the Anthropic APIs. Though CV creation is not an advanced
    task, the principles and designs I covered can be a foundation for other non-chat
    applications with greater value and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Happy building!
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, graphs and code created by the Author.*'
  prefs: []
  type: TYPE_NORMAL
