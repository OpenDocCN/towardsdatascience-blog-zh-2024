["```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n# Create dataset\ndata = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', \n                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', \n                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, \n                   67, 85, 73, 88, 77, 79, 80, 66, 84],\n    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, \n                 90, 85, 88, 65, 70, 60, 95, 70, 78],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, \n             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, \n                    14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\n# Process data\ndf = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'])\ndf['Wind'] = df['Wind'].astype(int)\n\n# Split data\nX, y = df.drop(columns='Num_Players'), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Scale numerical features\nnumerical_cols = ['Temperature', 'Humidity']\nct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')\n\n# Transform data\nX_train_scaled = pd.DataFrame(\n    ct.fit_transform(X_train),\n    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],\n    index=X_train.index\n)\n\nX_test_scaled = pd.DataFrame(\n    ct.transform(X_test),\n    columns=X_train_scaled.columns,\n    index=X_test.index\n)\n```", "```py\nimport numpy as np\n\n# Initialize bias as mean of target values and coefficients to 0\nbias = np.mean(y_train)\nbeta = np.zeros(X_train_scaled.shape[1])\nlambda_param = 1\n\n# One cycle through all features\nfor j, feature in enumerate(X_train_scaled.columns):\n    # Get current feature values\n    x_j = X_train_scaled.iloc[:, j].values\n\n    # Calculate prediction excluding the j-th feature\n    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]\n\n    # Calculate partial residuals\n    residual_no_j = y_train.values - y_pred_no_j\n\n    # Calculate the dot product of x_j with itself (sum of squared feature values)\n    sum_squared_x_j = np.dot(x_j, x_j)\n\n    # Calculate temporary beta without regularization (raw update)\n    beta_old = beta[j]\n    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j\n\n    # Apply soft thresholding for Lasso penalty\n    beta[j] = np.sign(beta_temp) * max(abs(beta_temp) - lambda_param / sum_squared_x_j, 0)\n\n# Print results\nprint(\"Coefficients after one cycle:\")\nfor feature, coef in zip(X_train_scaled.columns, beta):\n    print(f\"{feature:11}: {coef:.2f}\")\n```", "```py\n# Update bias (not penalized by lambda)\ny_pred = X_train_scaled.values @ beta  # only using coefficients, no bias\nresiduals = y_train.values - y_pred\nbias = np.mean(residuals)  # this replaces the old bias\n```", "```py\nfrom sklearn.linear_model import Lasso\n\n# Fit Lasso from scikit-learn\nlasso = Lasso(alpha=1) # Default value is 1000 cycle\nlasso.fit(X_train_scaled, y_train)\n\n# Print results\nprint(\"\\nCoefficients after 1000 cycles:\")\nprint(f\"Bias term  : {lasso.intercept_:.2f}\")\nfor feature, coef in zip(X_train_scaled.columns, lasso.coef_):\n   print(f\"{feature:11}: {coef:.2f}\")\n```", "```py\nimport numpy as np\n\n# Initialize bias as mean of target values and coefficients to 0\nbias = np.mean(y_train)\nbeta = np.zeros(X_train_scaled.shape[1])\nlambda_param = 1\nalpha = 0.5  # mixing parameter (0 for Ridge, 1 for Lasso)\n\n# One cycle through all features\nfor j, feature in enumerate(X_train_scaled.columns):\n    # Get current feature values\n    x_j = X_train_scaled.iloc[:, j].values\n\n    # Calculate prediction excluding the j-th feature\n    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]\n\n    # Calculate partial residuals\n    residual_no_j = y_train.values - y_pred_no_j\n\n    # Calculate the dot product of x_j with itself (sum of squared feature values)\n    sum_squared_x_j = np.dot(x_j, x_j)\n\n    # Calculate temporary beta without regularization (raw update)\n    beta_old = beta[j]\n    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j\n\n    # Apply soft thresholding for Elastic Net penalty\n    l1_term = alpha * lambda_param / sum_squared_x_j     # L1 (Lasso) penalty term\n    l2_term = (1-alpha) * lambda_param / sum_squared_x_j # L2 (Ridge) penalty term\n\n    # First apply L1 soft thresholding, then L2 scaling\n    beta[j] = (np.sign(beta_temp) * max(abs(beta_temp) - l1_term, 0)) / (1 + l2_term)\n\n# Print results\nprint(\"Coefficients after one cycle:\")\nfor feature, coef in zip(X_train_scaled.columns, beta):\n    print(f\"{feature:11}: {coef:.2f}\")\n```", "```py\n# Update bias (not penalized by lambda)\ny_pred_with_updated_beta = X_train_scaled.values @ beta  # only using coefficients, no bias\nresiduals_for_bias_update = y_train.values - y_pred_with_updated_beta\nnew_bias = np.mean(y_train.values - y_pred_with_updated_beta)  # this replaces the old bias\n\nprint(f\"Bias term  : {new_bias:.2f}\")\n```", "```py\nfrom sklearn.linear_model import ElasticNet\n\n# Fit Lasso from scikit-learn\nelasticnet = Lasso(alpha=1) # Default value is 1000 cycle\nelasticnet.fit(X_train_scaled, y_train)\n\n# Print results\nprint(\"\\nCoefficients after 1000 cycles:\")\nprint(f\"Bias term  : {elasticnet.intercept_:.2f}\")\nfor feature, coef in zip(X_train_scaled.columns, elasticnet.coef_):\n   print(f\"{feature:11}: {coef:.2f}\")\n```", "```py\n# Define parameters\nl1_ratios = [0, 0.25, 0.5, 0.75, 1]\nlambdas = [0, 0.01, 0.1, 1, 10]\nfeature_names = X_train_scaled.columns\n\n# Create a dataframe for each lambda value\nfor lambda_val in lambdas:\n    # Initialize list to store results\n    results = []\n    rmse_values = []\n\n    # Fit ElasticNet for each l1_ratio\n    for l1_ratio in l1_ratios:\n        # Fit model\n        en = ElasticNet(alpha=lambda_val, l1_ratio=l1_ratio)\n        en.fit(X_train_scaled, y_train)\n\n        # Calculate RMSE\n        y_pred = en.predict(X_test_scaled)\n        rmse = root_mean_squared_error(y_test, y_pred)\n\n        # Store coefficients and RMSE\n        results.append(list(en.coef_.round(2)) + [round(en.intercept_,2),round(rmse,3)])\n\n    # Create dataframe with RMSE column\n    columns = list(feature_names) + ['Bias','RMSE']\n    df = pd.DataFrame(results, index=l1_ratios, columns=columns)\n    df.index.name = f'λ = {lambda_val}'\n\n    print(df)\n```", "```py\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import root_mean_squared_error\nfrom sklearn.linear_model import Lasso  #, ElasticNet\n\n# Create dataset\ndata = {\n    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', \n                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', \n                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],\n    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, \n                   67, 85, 73, 88, 77, 79, 80, 66, 84],\n    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, \n                 90, 85, 88, 65, 70, 60, 95, 70, 78],\n    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, \n             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],\n    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, \n                    14, 34, 29, 49, 36, 57, 21, 23, 41]\n}\n\n# Process data\ndf = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'], prefix='', prefix_sep='', dtype=int)\ndf['Wind'] = df['Wind'].astype(int)\ndf = df[['sunny','overcast','rain','Temperature','Humidity','Wind','Num_Players']]\n\n# Split data\nX, y = df.drop(columns='Num_Players'), df['Num_Players']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)\n\n# Scale numerical features\nnumerical_cols = ['Temperature', 'Humidity']\nct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')\n\n# Transform data\nX_train_scaled = pd.DataFrame(\n    ct.fit_transform(X_train),\n    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],\n    index=X_train.index\n)\nX_test_scaled = pd.DataFrame(\n    ct.transform(X_test),\n    columns=X_train_scaled.columns,\n    index=X_test.index\n)\n\n# Initialize and train the model\nmodel = Lasso(alpha=0.1)  # Option 1: Lasso Regression (alpha is the regularization strength, equivalent to λ, uses coordinate descent)\n#model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Option 2: Elastic Net Regression (alpha is the overall regularization strength, and l1_ratio is the mix between L1 and L2, uses coordinate descent)\n\n# Fit the model\nmodel.fit(X_train_scaled, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test_scaled)\n\n# Calculate and print RMSE\nrmse = root_mean_squared_error(y_test, y_pred)\nprint(f\"RMSE: {rmse:.4f}\")\n\n# Additional information about the model\nprint(\"\\nModel Coefficients:\")\nfor feature, coef in zip(X_train_scaled.columns, model.coef_):\n    print(f\"{feature:13}: {coef:.2f}\")\nprint(f\"Intercept    : {model.intercept_:.2f}\")\n```"]