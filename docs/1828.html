<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Fine-Tune Llama 3.1 Ultra-Efficiently with Unsloth</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Fine-Tune Llama 3.1 Ultra-Efficiently with Unsloth</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29">https://towardsdatascience.com/fine-tune-llama-3-1-ultra-efficiently-with-unsloth-7196c7165bab?source=collection_archive---------0-----------------------#2024-07-29</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fc47" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><em class="hd">A beginner‚Äôs guide to state-of-the-art supervised fine-tuning</em></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="he hf hg hh hi ab"><div><div class="ab hj"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------" rel="noopener follow"><div class="l hk hl by hm hn"><div class="l ed"><img alt="Maxime Labonne" class="l ep by dd de cx" src="../Images/a7efdd305e3cc77d5509bbb1076d57d8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VbPYS4bNf0IrrOF-ZubSGQ.png"/><div class="ho by l dd de em n hp eo"/></div></div></a></div></div><div class="hq ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------" rel="noopener follow"><div class="l hr hs by hm ht"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hu cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="ho by l br hu em n hp eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hv ab q"><div class="ab q hw"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hx hy bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hz" data-testid="authorName" href="https://medium.com/@mlabonne?source=post_page---byline--7196c7165bab--------------------------------" rel="noopener follow">Maxime Labonne</a></p></div></div></div><div class="ia ib l"><div class="ab ic"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hx hy dx"><button class="if ig ah ai aj ak al am an ao ap aq ar ih ii ij" disabled="">Follow</button></p></div></div></span></div></div><div class="l ik"><span class="bf b bg z dx"><div class="ab cn il im in"><div class="io ip ab"><div class="bf b bg z dx ab iq"><span class="ir l ik">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hz ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--7196c7165bab--------------------------------" rel="noopener follow"><p class="bf b bg z is it iu iv iw ix iy iz bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="id ie" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="ja jb l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Jul 29, 2024</span></div></span></div></span></div></div></div><div class="ab cp jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr"><div class="h k w ea eb q"><div class="kh l"><div class="ab q ki kj"><div class="pw-multi-vote-icon ed ir kk kl km"><div class=""><div class="kn ko kp kq kr ks kt am ku kv kw km"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kx ky kz la lb lc ld"><p class="bf b dy z dx"><span class="ko">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kn lg lh ab q ee li lj" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lf"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count le lf">3</span></p></button></div></div></div><div class="ab q js jt ju jv jw jx jy jz ka kb kc kd ke kf kg"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ih lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ih lw lx lj ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo mp"><img src="../Images/442eac2ef94d8b6584ef18418dcbdb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*204gvY4y_NVT-Kd4.jpeg"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image generated with DALL-E 3 by author</figcaption></figure><p id="d5d2" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed-source and open-weight models. Instead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo oc"><img src="../Images/e2bfb879bf9d5078c60062fe6e5f8c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1tpK40n6bj_kRKOR.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="b49b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this article, we will provide a comprehensive overview of supervised fine-tuning. We will compare it to prompt engineering to understand when it makes sense to use it, detail the main techniques with their pros and cons, and introduce major concepts, such as LoRA hyperparameters, storage formats, and chat templates. Finally, we will implement it in practice by fine-tuning Llama 3.1 8B in Google Colab with state-of-the-art optimization using Unsloth.</p><p id="5bae" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">All the code used in this article is available on <a class="af od" href="https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing" rel="noopener ugc nofollow" target="_blank">Google Colab</a> and in the <a class="af od" href="https://github.com/mlabonne/llm-course" rel="noopener ugc nofollow" target="_blank">LLM Course</a>.</p><h1 id="4091" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">üîß Supervised Fine-Tuning</h1><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pa"><img src="../Images/721214d6a9b43ef4eaa195651d4a8119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*g4lKCyFKV_BxlDv_.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="aabe" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Supervised Fine-Tuning (SFT) is a method to <strong class="ni fr">improve and customize</strong> pre-trained LLMs. It involves retraining base models on a smaller dataset of instructions and answers. The main goal is to transform a basic model that predicts text into an assistant that can follow instructions and answer questions. SFT can also enhance the model‚Äôs overall performance, add new knowledge, or adapt it to specific tasks and domains. Fine-tuned models can then go through an optional preference alignment stage (see <a class="af od" href="https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html" rel="noopener ugc nofollow" target="_blank">my article about DPO</a>) to remove unwanted responses, modify their style, and more.</p><p id="469b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The following figure shows an instruction sample. It includes a system prompt to steer the model, a user prompt to provide a task, and the output the model is expected to generate. You can find a list of high-quality open-source instruction datasets in the <a class="af od" href="https://github.com/mlabonne/llm-datasets" rel="noopener ugc nofollow" target="_blank">üíæ LLM Datasets</a> GitHub repo.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo oc"><img src="../Images/4f1f2c16c4190c178e4fa8be8bccb8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aaxURA0K2PFyoe1J.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="7415" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Before considering SFT, I recommend trying prompt engineering techniques like <strong class="ni fr">few-shot prompting</strong> or <strong class="ni fr">retrieval augmented generation</strong> (RAG). In practice, these methods can solve many problems without the need for fine-tuning, using either closed-source or open-weight models (e.g., Llama 3.1 Instruct). If this approach doesn‚Äôt meet your objectives (in terms of quality, cost, latency, etc.), then SFT becomes a viable option when instruction data is available. Note that SFT also offers benefits like additional control and customizability to create personalized LLMs.</p><p id="f4f8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">However, SFT has limitations. It works best when leveraging knowledge already present in the base model. Learning completely new information like an unknown language can be challenging and lead to more frequent hallucinations. For new domains unknown to the base model, it is recommended to continuously pre-train it on a raw dataset first.</p><p id="97c1" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">On the opposite end of the spectrum, instruct models (i.e., already fine-tuned models) can already be very close to your requirements. For example, a model might perform very well but state that it was trained by OpenAI or Meta instead of you. In this case, you might want to slightly steer the instruct model‚Äôs behavior using preference alignment. By providing chosen and rejected samples for a small set of instructions (between 100 and 1000 samples), you can force the LLM to say that you trained it instead of OpenAI.</p><h1 id="a2bd" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">‚öñÔ∏è SFT Techniques</h1><p id="cfcb" class="pw-post-body-paragraph ng nh fq ni b go pb nk nl gr pc nn no np pd nr ns nt pe nv nw nx pf nz oa ob fj bk">The three most popular SFT techniques are full fine-tuning, LoRA, and QLoRA.</p><figure class="mq mr ms mt mu mv mn mo paragraph-image"><div role="button" tabindex="0" class="mw mx ed my bh mz"><div class="mn mo pg"><img src="../Images/a95694ba5600c25c2a3027f40c66b853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nidQUa3pO20RW3KI.png"/></div></div><figcaption class="nb nc nd mn mo ne nf bf b bg z dx">Image by author</figcaption></figure><p id="4437" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">Full fine-tuning</strong> is the most straightforward SFT technique. It involves retraining all parameters of a pre-trained model on an instruction dataset. This method often provides the best results but requires significant computational resources (several high-end GPUs are required to fine-tune a 8B model). Because it modifies the entire model, it is also the most destructive method and can lead to the catastrophic forgetting of previous skills and knowledge.</p><p id="20f4" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">Low-Rank Adaptation (LoRA)</strong> is a popular parameter-efficient fine-tuning technique. Instead of retraining the entire model, it freezes the weights and introduces small adapters (low-rank matrices) at each targeted layer. This allows LoRA to train a number of parameters that is drastically lower than full fine-tuning (less than 1%), reducing both memory usage and training time. This method is non-destructive since the original parameters are frozen, and adapters can then be switched or combined at will.</p><p id="f58f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni fr">QLoRA (Quantization-aware Low-Rank Adaptation)</strong> is an extension of LoRA that offers even greater memory savings. It provides up to 33% additional memory reduction compared to standard LoRA, making it particularly useful when GPU memory is constrained. This increased efficiency comes at the cost of longer training times, with QLoRA typically taking about 39% more time to train than regular LoRA.</p><p id="d61f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">While QLoRA requires more training time, its substantial memory savings can make it the only viable option in scenarios where GPU memory is limited. For this reason, this is the technique we will use in the next section to fine-tune a Llama 3.1 8B model on Google Colab.</p><h1 id="dbfb" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">ü¶ô Fine-Tune Llama 3.1 8B</h1><p id="0386" class="pw-post-body-paragraph ng nh fq ni b go pb nk nl gr pc nn no np pd nr ns nt pe nv nw nx pf nz oa ob fj bk">To efficiently fine-tune a <a class="af od" href="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B" rel="noopener ugc nofollow" target="_blank">Llama 3.1 8B</a> model, we‚Äôll use the <a class="af od" href="https://github.com/unslothai/unsloth" rel="noopener ugc nofollow" target="_blank">Unsloth</a> library by Daniel and Michael Han. Thanks to its custom kernels, Unsloth provides 2x faster training and 60% memory use compared to other options, making it ideal in a constrained environment like Colab. Unfortunately, Unsloth only supports single-GPU settings at the moment. For multi-GPU settings, I recommend popular alternatives like <a class="af od" href="https://huggingface.co/docs/trl/en/index" rel="noopener ugc nofollow" target="_blank">TRL</a> and <a class="af od" href="https://github.com/OpenAccess-AI-Collective/axolotl" rel="noopener ugc nofollow" target="_blank">Axolotl</a> (both also include Unsloth as a backend).</p><p id="7275" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this example, we will QLoRA fine-tune it on the <a class="af od" href="https://huggingface.co/datasets/mlabonne/FineTome-100k" rel="noopener ugc nofollow" target="_blank">mlabonne/FineTome-100k</a> dataset. It‚Äôs a subset of <a class="af od" href="https://huggingface.co/datasets/arcee-ai/The-Tome" rel="noopener ugc nofollow" target="_blank">arcee-ai/The-Tome</a> (without <a class="af od" href="https://huggingface.co/datasets/arcee-ai/qwen2-72b-magpie-en" rel="noopener ugc nofollow" target="_blank">arcee-ai/qwen2‚Äì72b-magpie-en</a>) that I re-filtered using <a class="af od" href="https://huggingface.co/HuggingFaceFW/fineweb-edu-classifier" rel="noopener ugc nofollow" target="_blank">HuggingFaceFW/fineweb-edu-classifier</a>. Note that this classifier wasn‚Äôt designed for instruction data quality evaluation, but we can use it as a rough proxy. The resulting FineTome is an ultra-high quality dataset that includes conversations, reasoning problems, function calling, and more.</p><p id="f2e8" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let‚Äôs start by installing all the required libraries.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="d2f4" class="pl of fq pi b bg pm pn l po pp">!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"<br/>!pip install --no-deps "xformers&lt;0.0.27" "trl&lt;0.9.0" peft accelerate bitsandbytes</span></pre><p id="1f42" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once installed, we can import them as follows.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="6a9a" class="pl of fq pi b bg pm pn l po pp">import torch<br/>from trl import SFTTrainer<br/>from datasets import load_dataset<br/>from transformers import TrainingArguments, TextStreamer<br/>from unsloth.chat_templates import get_chat_template<br/>from unsloth import FastLanguageModel, is_bfloat16_supported</span></pre><p id="b79f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let‚Äôs now load the model. Since we want to use QLoRA, I chose the pre-quantized <a class="af od" href="https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit" rel="noopener ugc nofollow" target="_blank">unsloth/Meta-Llama-3.1‚Äì8B-bnb-4bit</a>. This 4-bit precision version of <a class="af od" href="https://markdown-to-medium.surge.sh/meta-llama/Meta-Llama-3.1-8B" rel="noopener ugc nofollow" target="_blank">meta-llama/Meta-Llama-3.1‚Äì8B</a> is significantly smaller (5.4 GB) and faster to download compared to the original 16-bit precision model (16 GB). We load in NF4 format using the bitsandbytes library.</p><p id="a787" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">When loading the model, we must specify a maximum sequence length, which restricts its context window. Llama 3.1 supports up to 128k context length, but we will set it to 2,048 in this example since it consumes more compute and VRAM. Finally, the <code class="cx pq pr ps pi b">dtype</code> parameter automatically detects if your GPU supports the <a class="af od" href="https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html#background-on-floating-point-representation" rel="noopener ugc nofollow" target="_blank">BF16 format</a> for more stability during training (this feature is restricted to Ampere and more recent GPUs).</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="33ad" class="pl of fq pi b bg pm pn l po pp">max_seq_length = 2048<br/>model, tokenizer = FastLanguageModel.from_pretrained(<br/>    model_name="unsloth/Meta-Llama-3.1-8B-bnb-4bit",<br/>    max_seq_length=max_seq_length,<br/>    load_in_4bit=True,<br/>    dtype=None,<br/>)</span></pre><p id="90d7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that our model is loaded in 4-bit precision, we want to prepare it for parameter-efficient fine-tuning with LoRA adapters. LoRA has three important parameters:</p><ul class=""><li id="06b9" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pu pv bk"><strong class="ni fr">Rank</strong> (r), which determines LoRA matrix size. Rank typically starts at 8 but can go up to 256. Higher ranks can store more information but increase the computational and memory cost of LoRA. We set it to 16 here.</li><li id="facd" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Alpha</strong> (Œ±), a scaling factor for updates. Alpha directly impacts the adapters‚Äô contribution and is often set to 1x or 2x the rank value.</li><li id="60c8" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Target modules</strong>: LoRA can be applied to various model components, including attention mechanisms (Q, K, V matrices), output projections, feed-forward blocks, and linear output layers. While initially focused on attention mechanisms, extending LoRA to other components has shown benefits. However, adapting more modules increases the number of trainable parameters and memory needs.</li></ul><p id="a242" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Here, we set r=16, Œ±=16, and target every linear module to maximize quality. We don‚Äôt use dropout and biases for faster training.</p><p id="ad1b" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In addition, we will use <a class="af od" href="https://arxiv.org/abs/2312.03732" rel="noopener ugc nofollow" target="_blank">Rank-Stabilized LoRA</a> (rsLoRA), which modifies the scaling factor of LoRA adapters to be proportional to 1/‚àör instead of 1/r. This stabilizes learning (especially for higher adapter ranks) and allows for improved fine-tuning performance as rank increases. Gradient checkpointing is handled by Unsloth to offload input and output embeddings to disk and save VRAM.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="e7b0" class="pl of fq pi b bg pm pn l po pp">model = FastLanguageModel.get_peft_model(<br/>    model,<br/>    r=16,<br/>    lora_alpha=16,<br/>    lora_dropout=0,<br/>    target_modules=["q_proj", "k_proj", "v_proj", "up_proj", "down_proj", "o_proj", "gate_proj"], <br/>    use_rslora=True,<br/>    use_gradient_checkpointing="unsloth"<br/>)</span></pre><p id="f2cc" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">With this LoRA configuration, we‚Äôll only train 42 million out of 8 billion parameters (0.5196%). This shows how much more efficient LoRA is compared to full fine-tuning.</p><p id="481a" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let‚Äôs now load and prepare our dataset. Instruction datasets are stored in a <strong class="ni fr">particular format</strong>: it can be Alpaca, ShareGPT, OpenAI, etc. First, we want to parse this format to retrieve our instructions and answers. Our <a class="af od" href="https://huggingface.co/datasets/mlabonne/FineTome-100k" rel="noopener ugc nofollow" target="_blank">mlabonne/FineTome-100k</a> dataset uses the ShareGPT format with a unique ‚Äúconversations‚Äù column containing messages in JSONL. Unlike simpler formats like Alpaca, ShareGPT is ideal for storing multi-turn conversations, which is closer to how users interact with LLMs.</p><p id="3e8e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Once our instruction-answer pairs are parsed, we want to reformat them to follow a <strong class="ni fr">chat template</strong>. Chat templates are a way to structure conversations between users and models. They typically include special tokens to identify the beginning and the end of a message, who‚Äôs speaking, etc. Base models don‚Äôt have chat templates so we can choose any: ChatML, Llama3, Mistral, etc. In the open-source community, the ChatML template (originally from OpenAI) is a popular option. It simply adds two special tokens (<code class="cx pq pr ps pi b">&lt;|im_start|&gt;</code> and <code class="cx pq pr ps pi b">&lt;|im_end|&gt;</code>) to indicate who's speaking.</p><p id="9a4e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">If we apply this template to the previous instruction sample, here‚Äôs what we get:</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="a4b1" class="pl of fq pi b bg pm pn l po pp">&lt;|im_start|&gt;system<br/>You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;user<br/>Remove the spaces from the following sentence: It prevents users to suspect that there are some hidden products installed on theirs device.<br/>&lt;|im_end|&gt;<br/>&lt;|im_start|&gt;assistant<br/>Itpreventsuserstosuspectthattherearesomehiddenproductsinstalledontheirsdevice.&lt;|im_end|&gt;</span></pre><p id="6bde" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the following code block, we parse our ShareGPT dataset with the <code class="cx pq pr ps pi b">mapping</code> parameter and include the ChatML template. We then load and process the entire dataset to apply the chat template to every conversation.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="3a3a" class="pl of fq pi b bg pm pn l po pp">tokenizer = get_chat_template(<br/>    tokenizer,<br/>    mapping={"role": "from", "content": "value", "user": "human", "assistant": "gpt"},<br/>    chat_template="chatml",<br/>)<br/><br/>def apply_template(examples):<br/>    messages = examples["conversations"]<br/>    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]<br/>    return {"text": text}<br/><br/>dataset = load_dataset("mlabonne/FineTome-100k", split="train")<br/>dataset = dataset.map(apply_template, batched=True)</span></pre><p id="37ff" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">We‚Äôre now ready to specify the training parameters for our run. I want to briefly introduce the most important hyperparameters:</p><ul class=""><li id="7547" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pu pv bk"><strong class="ni fr">Learning rate</strong>: It controls how strongly the model updates its parameters. Too low, and training will be slow and may get stuck in local minima. Too high, and training may become unstable or diverge, which degrades performance.</li><li id="7be4" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">LR scheduler</strong>: It adjusts the learning rate (LR) during training, starting with a higher LR for rapid initial progress and then decreasing it in later stages. Linear and cosine schedulers are the two most common options.</li><li id="610e" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Batch size</strong>: Number of samples processed before the weights are updated. Larger batch sizes generally lead to more stable gradient estimates and can improve training speed, but they also require more memory. Gradient accumulation allows for effectively larger batch sizes by accumulating gradients over multiple forward/backward passes before updating the model.</li><li id="8c66" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Num epochs</strong>: The number of complete passes through the training dataset. More epochs allow the model to see the data more times, potentially leading to better performance. However, too many epochs can cause overfitting.</li><li id="d1bf" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Optimizer</strong>: Algorithm used to adjust the parameters of a model to minimize the loss function. In practice, AdamW 8-bit is strongly recommended: it performs as well as the 32-bit version while using less GPU memory. The paged version of AdamW is only interesting in distributed settings.</li><li id="0027" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Weight decay</strong>: A regularization technique that adds a penalty for large weights to the loss function. It helps prevent overfitting by encouraging the model to learn simpler, more generalizable features. However, too much weight decay can impede learning.</li><li id="ed40" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Warmup steps</strong>: A period at the beginning of training where the learning rate is gradually increased from a small value to the initial learning rate. Warmup can help stabilize early training, especially with large learning rates or batch sizes, by allowing the model to adjust to the data distribution before making large updates.</li><li id="ca63" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Packing</strong>: Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.</li></ul><p id="47b7" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I trained the model on the entire dataset (100k samples) using an A100 GPU (40 GB of VRAM) on Google Colab. The training took 4 hours and 45 minutes. Of course, you can use smaller GPUs with less VRAM and a smaller batch size, but they‚Äôre not nearly as fast. For example, it takes roughly 19 hours and 40 minutes on an L4 and a whopping 47 hours on a free T4.</p><p id="402f" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In this case, I recommend only loading a subset of the dataset to speed up training. You can do it by modifying the previous code block, like <code class="cx pq pr ps pi b">dataset = load_dataset("mlabonne/FineTome-100k", split="train[:10000]")</code> to only load 10k samples. Alternatively, you can use cheaper cloud GPU providers like Paperspace, RunPod, or Lambda Labs.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="44a3" class="pl of fq pi b bg pm pn l po pp">trainer=SFTTrainer(<br/>    model=model,<br/>    tokenizer=tokenizer,<br/>    train_dataset=dataset,<br/>    dataset_text_field="text",<br/>    max_seq_length=max_seq_length,<br/>    dataset_num_proc=2,<br/>    packing=True,<br/>    args=TrainingArguments(<br/>        learning_rate=3e-4,<br/>        lr_scheduler_type="linear",<br/>        per_device_train_batch_size=4,<br/>        gradient_accumulation_steps=4,<br/>        num_train_epochs=1,<br/>        fp16=not is_bfloat16_supported(),<br/>        bf16=is_bfloat16_supported(),<br/>        logging_steps=1,<br/>        optim="adamw_8bit",<br/>        weight_decay=0.01,<br/>        warmup_steps=10,<br/>        output_dir="output",<br/>        seed=0,<br/>    ),<br/>)<br/><br/>trainer.train()</span></pre><p id="03dc" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Now that the model is trained, let‚Äôs test it with a simple prompt. This is not a rigorous evaluation but just a quick check to detect potential issues. We use <code class="cx pq pr ps pi b">FastLanguageModel.for_inference()</code> to get 2x faster inference.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="7fb5" class="pl of fq pi b bg pm pn l po pp">model = FastLanguageModel.for_inference(model)<br/><br/>messages = [<br/>    {"from": "human", "value": "Is 9.11 larger than 9.9?"},<br/>]<br/>inputs = tokenizer.apply_chat_template(<br/>    messages,<br/>    tokenize=True,<br/>    add_generation_prompt=True,<br/>    return_tensors="pt",<br/>).to("cuda")<br/><br/>text_streamer = TextStreamer(tokenizer)<br/>_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)</span></pre><p id="2587" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The model‚Äôs response is ‚Äú9.9‚Äù, which is correct!</p><p id="4cdd" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Let‚Äôs now save our trained model. If you remember the part about LoRA and QLoRA, what we trained is not the model itself but a set of adapters. There are three save methods in Unsloth: <code class="cx pq pr ps pi b">lora</code> to only save the adapters, and <code class="cx pq pr ps pi b">merged_16bit</code>/<code class="cx pq pr ps pi b">merged_4bit</code> to merge the adapters with the model in 16-bit/ 4-bit precision.</p><p id="ecca" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In the following, we merge them in 16-bit precision to maximize the quality. We first save it locally in the ‚Äúmodel‚Äù directory and then upload it to the Hugging Face Hub. You can find the trained model on <a class="af od" href="https://huggingface.co/mlabonne/FineLlama-3.1-8B" rel="noopener ugc nofollow" target="_blank">mlabonne/FineLlama-3.1‚Äì8B</a>.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="8ae0" class="pl of fq pi b bg pm pn l po pp">model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")<br/>model.push_to_hub_merged("mlabonne/FineLlama-3.1-8B", tokenizer, save_method="merged_16bit")</span></pre><p id="a55e" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Unsloth also allows you to directly convert your model into GGUF format. This is a quantization format created for llama.cpp and compatible with most inference engines, like <a class="af od" href="https://lmstudio.ai/" rel="noopener ugc nofollow" target="_blank">LM Studio</a>, <a class="af od" href="https://ollama.com/" rel="noopener ugc nofollow" target="_blank">Ollama</a>, and oobabooga‚Äôs <a class="af od" href="https://github.com/oobabooga/text-generation-webui" rel="noopener ugc nofollow" target="_blank">text-generation-webui</a>. Since you can specify different precisions (see <a class="af od" href="https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html" rel="noopener ugc nofollow" target="_blank">my article about GGUF and llama.cpp</a>), we‚Äôll loop over a list to quantize it in <code class="cx pq pr ps pi b">q2_k</code>, <code class="cx pq pr ps pi b">q3_k_m</code>, <code class="cx pq pr ps pi b">q4_k_m</code>, <code class="cx pq pr ps pi b">q5_k_m</code>, <code class="cx pq pr ps pi b">q6_k</code>, <code class="cx pq pr ps pi b">q8_0</code> and upload these quants on Hugging Face. The <a class="af od" href="https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF" rel="noopener ugc nofollow" target="_blank">mlabonne/FineLlama-3.1-8B-GGUF</a> contains all our GGUFs.</p><pre class="mq mr ms mt mu ph pi pj bp pk bb bk"><span id="e128" class="pl of fq pi b bg pm pn l po pp">quant_methods = ["q2_k", "q3_k_m", "q4_k_m", "q5_k_m", "q6_k", "q8_0"]<br/>for quant in quant_methods:<br/>    model.push_to_hub_gguf("mlabonne/FineLlama-3.1-8B-GGUF", tokenizer, quant)</span></pre><p id="ade6" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Congratulations, we fine-tuned a model from scratch and uploaded quants you can now use in your favorite inference engine. Feel free to try the final model available on <a class="af od" href="https://huggingface.co/mlabonne/FineLlama-3.1-8B-GGUF" rel="noopener ugc nofollow" target="_blank">mlabonne/FineLlama-3.1‚Äì8B-GGUF</a>. What to do now? Here are some ideas on how to use your model:</p><ul class=""><li id="a415" class="ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob pt pu pv bk"><strong class="ni fr">Evaluate</strong> it on the <a class="af od" href="https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard" rel="noopener ugc nofollow" target="_blank">Open LLM Leaderboard</a> (you can submit it for free) or using other evals like in <a class="af od" href="https://github.com/mlabonne/llm-autoeval" rel="noopener ugc nofollow" target="_blank">LLM AutoEval</a>.</li><li id="9fd5" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Align</strong> it with Direct Preference Optimization using a preference dataset like <a class="af od" href="https://huggingface.co/datasets/mlabonne/orpo-dpo-mix-40k" rel="noopener ugc nofollow" target="_blank">mlabonne/orpo-dpo-mix-40k</a> to boost performance.</li><li id="79ea" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Quantize</strong> it in other formats like EXL2, AWQ, GPTQ, or HQQ for faster inference or lower precision using <a class="af od" href="https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing" rel="noopener ugc nofollow" target="_blank">AutoQuant</a>.</li><li id="538f" class="ng nh fq ni b go pw nk nl gr px nn no np py nr ns nt pz nv nw nx qa nz oa ob pt pu pv bk"><strong class="ni fr">Deploy</strong> it on a Hugging Face Space with <a class="af od" href="https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC" rel="noopener ugc nofollow" target="_blank">ZeroChat</a> for models that have been sufficiently trained to follow a chat template (~20k samples).</li></ul><h1 id="3008" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Conclusion</h1><p id="a430" class="pw-post-body-paragraph ng nh fq ni b go pb nk nl gr pc nn no np pd nr ns nt pe nv nw nx pf nz oa ob fj bk">This article provided a comprehensive overview of supervised fine-tuning and how to apply it in practice to a Llama 3.1 8B model. By leveraging QLoRA‚Äôs efficient memory usage, we managed to fine-tune an 8B LLM on a super high-quality dataset with limited GPU resources. We also provided more efficient alternatives for bigger runs and suggestions for further steps, including evaluation, preference alignment, quantization, and deployment.</p><p id="84b5" class="pw-post-body-paragraph ng nh fq ni b go nj nk nl gr nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">I hope this guide was useful. If you‚Äôre interested in learning more about LLMs, I recommend checking the <a class="af od" href="https://github.com/mlabonne/llm-course" rel="noopener ugc nofollow" target="_blank">LLM Course</a>. If you enjoyed this article, follow me on X <a class="af od" href="https://x.com/maximelabonne" rel="noopener ugc nofollow" target="_blank">@maximelabonne</a> and on Hugging Face <a class="af od" href="https://huggingface.co/mlabonne" rel="noopener ugc nofollow" target="_blank">@mlabonne</a>. Good luck fine-tuning models!</p></div></div></div></div>    
</body>
</html>