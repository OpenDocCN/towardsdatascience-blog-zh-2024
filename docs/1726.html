<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Topic Modeling Open-Source Research with the OpenAlex API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Topic Modeling Open-Source Research with the OpenAlex API</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15">https://towardsdatascience.com/topic-modeling-open-source-research-with-the-openalex-api-5191c7db9156?source=collection_archive---------7-----------------------#2024-07-15</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c8f8" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Open-source intelligence (OSINT) is something that can add tremendous value to organizations. Insight gained from analyzing social media data, web data, or global research, can be great in supporting all kinds of analyses. This article will give an overview of using topic modeling to help us make sense of thousands of pieces of open-source research.</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Alex Davis" class="l ep by dd de cx" src="../Images/f773cce9438a68856cb8ba486ac8b051.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*T0N-gqDlACyH39JH8hcOdQ@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@adavis08?source=post_page---byline--5191c7db9156--------------------------------" rel="noopener follow">Alex Davis</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5191c7db9156--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 15, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/ee2aac991c4193c160d128a71f553a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ij2ZNwGNOMufJDLgPagOw.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">“resorting to paper… #research# #proposal” by <a class="af nc" href="https://www.flickr.com/photos/catherinecronin/14326101068" rel="noopener ugc nofollow" target="_blank">catherinecronin</a> is licensed under CC BY-SA 2.0. To view a copy of this license, visit <a class="af nc" href="https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse." rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/licenses/by-sa/2.0/?ref=openverse.</a></figcaption></figure></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2fc2" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk"><strong class="al">What is Topic Modeling?</strong></h1><p id="6a58" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">Topic modeling is an unsupervised machine learning technique used to analyze documents and identity ‘topics’ using semantic similarity. This is similar to clustering, but not every document is exclusive to one topic. It is more about grouping the content found in a corpus. Topic modeling has many different applications but is mainly used to better understand large amounts of text data.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/ae6dbdf009e015f0e89bfb45c5c4be55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NLnpzhcaVq4UqZAw_BF5LQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="a75b" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">For example, a retail chain may model customer surveys and reviews to identify negative reviews and drill down into the key issues outlined by their customers. In this case, we will import a large amount of articles and abstracts to understand the key topics in our dataset.</p><p id="7fe9" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk"><em class="pj">Note: Topic modeling can be computationally expensive at scale. In this example, I used the Amazon Sagemaker environment to take advantage of their CPU.</em></p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="5aa6" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">OpenAlex</h1><p id="46bd" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">OpenAlex is a free to use catalogue system of global research. They have indexed over 250 million pieces of news, articles, abstracts, and more.</p><div class="pk pl pm pn po pp"><a href="https://openalex.org/?source=post_page-----5191c7db9156--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab ig"><div class="pr ab co cb ps pt"><h2 class="bf fr hw z io pu iq ir pv it iv fp bk">OpenAlex</h2><div class="pw l"><h3 class="bf b hw z io pu iq ir pv it iv dx">Edit description</h3></div><div class="px l"><p class="bf b dy z io pu iq ir pv it iv dx">openalex.org</p></div></div></div></a></div><p id="4a49" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Luckily for us, they have a free (but limited) and flexible API that will allow us to quickly ingest tens of thousands of articles while also applying filters, such as year, type of media, keywords, etc.</p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="786c" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Creating a Data Pipeline</h1><p id="7dab" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">While we ingest the data from the API, we will apply some criteria. First, we will only ingest documents where the year is between 2016 and 2022. We want fairly recent language as terms and taxonomy of certain subjects can change over long periods of time.</p><p id="c1f5" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">We will also add key terms and conduct multiple searches. While normally we would likely ingest random subject areas, we will use key terms to narrow our search. This way, we will have an idea of how may high-level topics we have, and can compare that to the output of the model. Below, we create a function where we can add key terms and conduct searches through the API.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="40ef" class="qc nm fq pz b bg qd qe l qf qg">import pandas as pd<br/>import requests</span></pre><pre class="qh py pz qa bp qb bb bk"><span id="5788" class="qc nm fq pz b bg qd qe l qf qg">def import_data(pages, start_year, end_year, search_terms):<br/>    <br/>    """<br/>    This function is used to use the OpenAlex API, conduct a search on works, a return a dataframe with associated works.<br/>    <br/>    Inputs: <br/>        - pages: int, number of pages to loop through<br/>        - search_terms: str, keywords to search for (must be formatted according to OpenAlex standards)<br/>        - start_year and end_year: int, years to set as a range for filtering works<br/>    """<br/>    <br/>    #create an empty dataframe<br/>    search_results = pd.DataFrame()<br/>    <br/>    for page in range(1, pages):<br/>        <br/>        #use paramters to conduct request and format to a dataframe<br/>        response = requests.get(f'https://api.openalex.org/works?page={page}&amp;per-page=200&amp;filter=publication_year:{start_year}-{end_year},type:article&amp;search={search_terms}')<br/>        data = pd.DataFrame(response.json()['results'])<br/>        <br/>        #append to empty dataframe<br/>        search_results = pd.concat([search_results, data])<br/>    <br/>    #subset to relevant features<br/>    search_results = search_results[["id", "title", "display_name", "publication_year", "publication_date",<br/>                                        "type", "countries_distinct_count","institutions_distinct_count",<br/>                                        "has_fulltext", "cited_by_count", "keywords", "referenced_works_count", "abstract_inverted_index"]]<br/>    <br/>    return(search_results)</span></pre><p id="c14b" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">We conduct 5 different searches, each being a different technology area. These technology areas are inspired by the DoD “Critical Technology Areas”. See more here:</p><div class="pk pl pm pn po pp"><a href="https://www.cto.mil/usdre-strat-vision-critical-tech-areas/?source=post_page-----5191c7db9156--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pq ab ig"><div class="pr ab co cb ps pt"><h2 class="bf fr hw z io pu iq ir pv it iv fp bk">USD(R&amp;E) Strategic Vision and Critical Technology Areas - DoD Research &amp; Engineering, OUSD(R&amp;E)</h2><div class="pw l"><h3 class="bf b hw z io pu iq ir pv it iv dx">The OUSD(R&amp;E) works closely with the Military Services, Combatant Commands, industry, academia, and other stakeholders…</h3></div><div class="px l"><p class="bf b dy z io pu iq ir pv it iv dx">www.cto.mil</p></div></div><div class="qi l"><div class="qj l qk ql qm qi qn lr pp"/></div></div></a></div><p id="58ae" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Here is an example of a search using the required OpenAlex syntax:</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="d300" class="qc nm fq pz b bg qd qe l qf qg">#search for Trusted AI and Autonomy<br/>ai_search = import_data(35, 2016, 2024, "'artificial intelligence' OR 'deep learn' OR 'neural net' OR 'autonomous' OR drone")</span></pre><p id="c420" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">After compiling our searches and dropping duplicate documents, we must clean the data to prepare it for our topic model. There are 2 main issues with our current output.</p><ol class=""><li id="c46a" class="oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc qo qp qq bk">The abstracts are returned as an inverted index (due to legal reasons). However, we can use these to return the original text.</li><li id="ba15" class="oh oi fq oj b go qr ol om gr qs oo op oq qt os ot ou qu ow ox oy qv pa pb pc qo qp qq bk">Once we obtain the original text, it will be raw and unprocessed, creating noise and hurting our model. We will conduct traditional NLP preprocessing to get it ready for the model.</li></ol><p id="3ec9" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Below is a function to return original text from an inverted index.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="61e5" class="qc nm fq pz b bg qd qe l qf qg">def undo_inverted_index(inverted_index):<br/>    <br/>    """<br/>    The purpose of the function is to 'undo' and inverted index. It inputs an inverted index and<br/>    returns the original string.<br/>    """<br/><br/>    #create empty lists to store uninverted index<br/>    word_index = []<br/>    words_unindexed = []<br/>    <br/>    #loop through index and return key-value pairs<br/>    for k,v in inverted_index.items(): <br/>        for index in v: word_index.append([k,index])<br/><br/>    #sort by the index<br/>    word_index = sorted(word_index, key = lambda x : x[1])<br/>    <br/>    #join only the values and flatten<br/>    for pair in word_index:<br/>        words_unindexed.append(pair[0])<br/>    words_unindexed = ' '.join(words_unindexed)<br/>    <br/>    return(words_unindexed)</span></pre><p id="a631" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Now that we have the raw text, we can conduct our traditional preprocessing steps, such as standardization, removing stop words, lemmatization, etc. Below are functions that can be mapped to a list or series of documents.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="4516" class="qc nm fq pz b bg qd qe l qf qg">def preprocess(text):<br/>    <br/>    """<br/>    This function takes in a string, coverts it to lowercase, cleans<br/>    it (remove special character and numbers), and tokenizes it.<br/>    """<br/>    <br/>    #convert to lowercase<br/>    text = text.lower()<br/>    <br/>    #remove special character and digits<br/>    text = re.sub(r'\d+', '', text)<br/>    text = re.sub(r'[^\w\s]', '', text)<br/>    <br/>    #tokenize<br/>    tokens = nltk.word_tokenize(text)<br/>    <br/>    return(tokens)</span></pre><pre class="qh py pz qa bp qb bb bk"><span id="3c89" class="qc nm fq pz b bg qd qe l qf qg">def remove_stopwords(tokens):<br/>    <br/>    """<br/>    This function takes in a list of tokens (from the 'preprocess' function) and <br/>    removes a list of stopwords. Custom stopwords can be added to the 'custom_stopwords' list.<br/>    """<br/>    <br/>    #set default and custom stopwords<br/>    stop_words = nltk.corpus.stopwords.words('english')<br/>    custom_stopwords = []<br/>    stop_words.extend(custom_stopwords)<br/>    <br/>    #filter out stopwords<br/>    filtered_tokens = [word for word in tokens if word not in stop_words]<br/>    <br/>    return(filtered_tokens)</span></pre><pre class="qh py pz qa bp qb bb bk"><span id="81c9" class="qc nm fq pz b bg qd qe l qf qg">def lemmatize(tokens):<br/>    <br/>    """<br/>    This function conducts lemmatization on a list of tokens (from the 'remove_stopwords' function).<br/>    This shortens each word down to its root form to improve modeling results.<br/>    """<br/>    <br/>    #initalize lemmatizer and lemmatize<br/>    lemmatizer = nltk.WordNetLemmatizer()<br/>    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]<br/>    <br/>    return(lemmatized_tokens)</span></pre><pre class="qh py pz qa bp qb bb bk"><span id="e663" class="qc nm fq pz b bg qd qe l qf qg">def clean_text(text):<br/>    <br/>    """<br/>    This function uses the previously defined functions to take a string and\<br/>    run it through the entire data preprocessing process.<br/>    """<br/>    <br/>    #clean, tokenize, and lemmatize a string<br/>    tokens = preprocess(text)<br/>    filtered_tokens = remove_stopwords(tokens)<br/>    lemmatized_tokens = lemmatize(filtered_tokens)<br/>    clean_text = ' '.join(lemmatized_tokens)<br/>    <br/>    return(clean_text)</span></pre><p id="4fb4" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Now that we have a preprocessed series of documents, we can create our first topic model!</p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="986b" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Creating a Topic Model</h1><p id="1499" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">For our topic model, we will use gensim to create a Latent Dirichlet Allocation (LDA) model. LDA is the most common model for topic modeling, as it is very effective in identifying high-level themes within a corpus. Below are the packages used to create the model.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="b8ee" class="qc nm fq pz b bg qd qe l qf qg">import gensim.corpora as corpora<br/>from gensim.corpora import Dictionary<br/>from gensim.models.coherencemodel import CoherenceModel<br/>from gensim.models.ldamodel import LdaModel</span></pre><p id="524b" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Before we create our model, we must prepare our corpus and ID mappings. This can be done with just a few lines of code.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="6bf2" class="qc nm fq pz b bg qd qe l qf qg">#convert the preprocessed text to a list<br/>documents = list(data["clean_text"])<br/><br/>#seperate by ' ' to tokenize each article<br/>texts = [x.split(' ') for x in documents]<br/><br/>#construct word ID mappings<br/>id2word = Dictionary(texts)<br/><br/>#use word ID mappings to build corpus<br/>corpus = [id2word.doc2bow(text) for text in texts]</span></pre><p id="ac89" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Now we can create a topic model. As you will see below, there are many different parameters that will affect the model’s performance. You can read about the many parameters in gensim’s documentation.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="e05d" class="qc nm fq pz b bg qd qe l qf qg">#build LDA model<br/>lda_model = LdaModel(corpus = corpus, id2word = id2word, num_topics = 10, decay = 0.5,<br/>                     random_state = 0, chunksize = 100, alpha = 'auto', per_word_topics = True)</span></pre><p id="7590" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">The most import parameter will be the number of topics. Here, we set an arbitrary 10. Since we don’t know how many topics there <em class="pj">should </em>be, this parameter should definitely be optimized. But how do we measure the quality of our model?</p><p id="0759" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">This is where coherence scores come in. The coherence score is a measure from 0–1. Coherence scores measure the quality of our topics by making sure they are sound and distinct. We want clear boundaries between well-defined topics. While this is a bit subjective in the end, it gives us a great idea of the quality of our results.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="b0cb" class="qc nm fq pz b bg qd qe l qf qg">#compute coherence score<br/>coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = id2word, coherence = 'c_v')<br/>coherence_score = coherence_model_lda.get_coherence()<br/>print(coherence_score)</span></pre><p id="4c35" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Here, we get a coherence score of about 0.48, which isn’t too bad! But not ready for production.</p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="2025" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Visualize Our Topic Model</h1><p id="5111" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">Topic models can be difficult to visualize. Lucky for us, there is a great module ‘pyLDAvis’ that can automatically produce an interactive visualization that allows us to view our topics in a vector space and drill down into each topic.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="9954" class="qc nm fq pz b bg qd qe l qf qg">import pyLDAvis<br/><br/>#create Topic Distance Visualization <br/>pyLDAvis.enable_notebook()<br/>lda_viz = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)<br/>lda_viz</span></pre><p id="c823" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">As you can see below, this produces a great visualization where we can get a quick idea of how our model performed. By looking into the vector space, we see some topics are distinct and well-defined. However, we also have some overlapping topics.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qw"><img src="../Images/528452b69c598401246c2e00ad1dcb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*5SW1vwUjhpwC1KkkQ1i8_Q.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by Author</figcaption></figure><p id="1f73" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">We can click on a topic to few the most relevant tokens. As we adjust the relevance metric (lambda), we can see topic-specific tokens by sliding it left, and seeing relevant but less topic-specific tokens by sliding it to the right.</p><p id="1f2c" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">When clicking into each topic, I can vaguely see the topics that I originally searched for. For example, topic 5 seems to align with my ‘human-machine interfaces’ search. There is also a cluster of topics that seem to be related to biotechnology, but some are more clear than others.</p></div></div></div><div class="ab cb nd ne nf ng" role="separator"><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj nk"/><span class="nh by bm ni nj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c8dc" class="nl nm fq bf nn no np gq nq nr ns gt nt nu nv nw nx ny nz oa ob oc od oe of og bk">Optimize the Topic Model</h1><p id="974f" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">From the pyLDAvis interface and our coherence score of 0.48, there is definitely room for improvement. For our final step, lets write a function where we can loop through values for different parameters and try to optimize our coherence score. Below, is a function that tests different values of the number of topics and the decay rate. The function computes the coherence score for every combination of parameters and saves them in a data frame.</p><pre class="mm mn mo mp mq py pz qa bp qb bb bk"><span id="8d35" class="qc nm fq pz b bg qd qe l qf qg">def lda_model_evaluation():<br/>    <br/>    """<br/>    This function loops through a number of parameters for an LDA model, creates the model,<br/>    computes the coherenece score, and saves the results in a pandas dataframe. The outputed dataframe<br/>    contains the values of the parameters tested and the resulting coherence score.<br/>    """<br/>    <br/>    #define empty lists to save results<br/>    topic_number, decay_rate_list, score  = [], [], []<br/>    <br/>    #loop through a number of parameters<br/>    for topics in range(5,12):<br/>        for decay_rate in [0.5, 0.6, 0.7]:<br/>                <br/>                #build LDA model<br/>                lda_model = LdaModel(corpus = corpus, id2word = id2word, num_topics = topics, decay = decay_rate,<br/>                               random_state = 0, chunksize = 100, alpha = 'auto', per_word_topics = True)<br/>                <br/>                #compute coherence score<br/>                coherence_model_lda = CoherenceModel(model = lda_model, texts = texts, dictionary = id2word, coherence = 'c_v')<br/>                coherence_score = coherence_model_lda.get_coherence()<br/>                <br/>                #append parameters to lists<br/>                topic_number.append(topics)<br/>                decay_rate_list.append(decay_rate)<br/>                score.append(coherence_score)<br/>                <br/>                print("Model Saved")<br/>    <br/>    #gather result into a dataframe<br/>    results = {"Number of Topics": topic_number,<br/>                "Decay Rate": decay_rate_list,<br/>                "Score": score}<br/>    <br/>    results = pd.DataFrame(results)<br/>    <br/>    return(results) </span></pre><p id="48e1" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">Just by passing a couple of small ranges through two parameters, we identified parameters that increased our coherence score from 0.48 to 0.55, a sizable improvement.</p><h1 id="9ff6" class="nl nm fq bf nn no qx gq nq nr qy gt nt nu qz nw nx ny ra oa ob oc rb oe of og bk">Next Steps</h1><p id="fc9d" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">To continue to build a production-level model, there is plenty of experimentation to be had with the parameters. Because LDA is so computationally expensive, I kept the experiment above limited and only compared about 20 different models. But with more time and power, we can compare hundreds of models.</p><p id="8305" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk">As well, there are improvements to be made with our data pipeline. I noticed several words that may need to be added to our stop word list. Words like ‘use’ and ‘department’ are not adding any semantic value, especially for documents about different technologies. As well, there are technical terms that do not get processed correctly, resulting in a single letter or a group of letters. We could spend some time doing a bag-of-words analysis to identify those stop word opportunities. This would eliminate noise in our dataset.</p><h1 id="0170" class="nl nm fq bf nn no qx gq nq nr qy gt nt nu qz nw nx ny ra oa ob oc rb oe of og bk">Conclusion</h1><p id="e843" class="pw-post-body-paragraph oh oi fq oj b go ok ol om gr on oo op oq or os ot ou ov ow ox oy oz pa pb pc fj bk">In this article, we:</p><ol class=""><li id="6b4b" class="oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc qo qp qq bk">Got an introduction to topic modeling and the OpenAlex data source</li><li id="92f4" class="oh oi fq oj b go qr ol om gr qs oo op oq qt os ot ou qu ow ox oy qv pa pb pc qo qp qq bk">Built a data pipeline to ingest data from an API and prepare it for an NLP model</li><li id="cd86" class="oh oi fq oj b go qr ol om gr qs oo op oq qt os ot ou qu ow ox oy qv pa pb pc qo qp qq bk">Constructed an LDA model and visualized the results using pyLDAvis</li><li id="26a2" class="oh oi fq oj b go qr ol om gr qs oo op oq qt os ot ou qu ow ox oy qv pa pb pc qo qp qq bk">Wrote code to help us find the optimal parameters</li><li id="0085" class="oh oi fq oj b go qr ol om gr qs oo op oq qt os ot ou qu ow ox oy qv pa pb pc qo qp qq bk">Discussed next steps for model improvement</li></ol><p id="2767" class="pw-post-body-paragraph oh oi fq oj b go pe ol om gr pf oo op oq pg os ot ou ph ow ox oy pi pa pb pc fj bk"><em class="pj">This is my first Medium article, so I hope you enjoyed it. Please feel free to leave feedback, ask questions, or request other topics!</em></p></div></div></div></div>    
</body>
</html>