<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Orchestrating a Dynamic Time-Series Pipeline in Azure</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Orchestrating a Dynamic Time-Series Pipeline in Azure</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31">https://towardsdatascience.com/orchestrating-a-dynamic-time-series-pipeline-with-azure-data-factory-and-databricks-810819608231?source=collection_archive---------9-----------------------#2024-05-31</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="235c" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Explore how to build, trigger, and parameterize a time-series data pipeline with Azure Data Factory (ADF) and Databricks, accompanied by a step-by-step tutorial</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="John Leung" class="l ep by dd de cx" src="../Images/ef45063e759e3450fa7f3c32b2f292c3.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*-zL9bLkxy32p8chXW888zQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@johnleungTJ?source=post_page---byline--810819608231--------------------------------" rel="noopener follow">John Leung</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--810819608231--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 31, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="cc1a" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the <a class="af ne" href="https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287" rel="noopener">previous story</a>, we went through the potential of PySpark on Databricks for time-series data. I encourage you to catch up <a class="af ne" href="https://medium.com/towards-data-science/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287" rel="noopener">here</a> to know more. Without configuring a standalone Spark instance, we can ingest static and streaming data, perform transformation, extract useful time-related features, and build visualization using PySpark on Databricks. Its scalability and performance are particularly advantageous when handling complex transformations of enterprise-level data, up to petabytes.</p><p id="abac" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">All the feature engineering tasks were successfully performed within a single Databricks notebook. However, this is only a part of the data engineering story when building a data-centric system. The core part of the data pipeline lies in data orchestration.</p><blockquote class="nf ng nh"><p id="686a" class="mi mj ni mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Data orchestration generally refers to have the centralized control over data flows so that we can automate, manage, and monitor the entire data pipeline.</p></blockquote><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/2a39ebad8db26cfd20e2a647df768903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OvcxSmhvVQ_nMXbL"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Photo by <a class="af ne" href="https://unsplash.com/@juliorionaldo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Julio Rionaldo</a> on <a class="af ne" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="68e1" class="ok ol fq bf om on oo op oq or os ot ou mr ov ow ox mv oy oz pa mz pb pc pd pe bk">Azure Data Factory (ADF) with Azure Databricks</h2><p id="578e" class="pw-post-body-paragraph mi mj fq mk b go pf mm mn gr pg mp mq mr ph mt mu mv pi mx my mz pj nb nc nd fj bk">To satisfy these needs, one of the most popular solutions in the industry is to run <a class="af ne" href="https://azure.microsoft.com/en-gb/products/databricks/#content-card-list-oc803c" rel="noopener ugc nofollow" target="_blank">Azure Databricks</a> notebooks from an <a class="af ne" href="https://azure.microsoft.com/en-us/products/data-factory#features" rel="noopener ugc nofollow" target="_blank">ADF</a> platform.</p><p id="9b96" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">ADF is a cloud-based, serverless, and fully managed data integration service. Though <a class="af ne" href="https://docs.databricks.com/en/workflows/index.html" rel="noopener ugc nofollow" target="_blank">Databricks Workflow</a> gives a good alternative that covers some ADF features, there are still several key benefits to choosing ADF. For example, ADF is a mature tool for integrating with diverse data stores using connectors, including SaaS applications like Salesforce, and Big Data sources like Amazon Redshift, and Google BigQuery. Therefore, it works well for ingestion and integration, especially if the current system has complex dependencies with data systems outside of Databricks. Besides, ADF simplifies and facilitates the quick building of basic pipelines using a drag-and-drop and low-code interface.</p></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a96d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">In this hands-on journey, we will dive deeper into the data engineering project and explore how ADF helps build a dynamic, skeletal data pipeline for time-series data.</strong> I will demonstrate how to mount cloud storage on Azure Databricks, transform data by embedding Notebook on Azure Databricks, and dynamically orchestrate data through custom settings in ADF. Let’s get started!</p><h2 id="bc19" class="ok ol fq bf om on oo op oq or os ot ou mr ov ow ox mv oy oz pa mz pb pc pd pe bk">The initial setup</h2><p id="7f6f" class="pw-post-body-paragraph mi mj fq mk b go pf mm mn gr pg mp mq mr ph mt mu mv pi mx my mz pj nb nc nd fj bk">There are several cloud components and services in the first place.</p><p id="d20f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#1 Create an Azure resource group</strong></p><p id="21a2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This container is used to hold and group the resources for an Azure solution. We will place our necessary cloud service components in this logical group for easier building or deployment.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pk"><img src="../Images/0b3a948d5cc07b62727ff956da16e18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kC5g-rTZ0YoVb1HLktC82Q.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Azure resource group (Image by author)</figcaption></figure><p id="d1ea" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#2 Create an Azure Data Lake Gen 2 storage account</strong></p><p id="3ebb" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You can choose a suitable storage account based on the requirements of performance and replication. In the advanced tab, we enable the Hierarchical Namespace to set up the <a class="af ne" href="https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction" rel="noopener ugc nofollow" target="_blank">Data Lake Storage Gen 2</a>. This allows for storing both structured and unstructured data.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pl"><img src="../Images/1a6a8f5c3ba036f8056555f2f1328079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcmManvddwjZK4wpdkRuEw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Storage account (Image by author)</figcaption></figure><p id="320c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#3 Set up Azure Databricks service</strong></p><p id="0d87" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">If you have used Databricks before, Azure Databricks service is largely the same. Besides, it is natively integrated with other Azure services and provides a unified billing platform. There are two <a class="af ne" href="https://azure.microsoft.com/en-us/pricing/details/databricks/" rel="noopener ugc nofollow" target="_blank">tiers</a>: (1) Standard — sufficient for our proof-of-concept here; and (2) Premium — the features of the Standard tier, with additionally the <a class="af ne" href="https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/" rel="noopener ugc nofollow" target="_blank">Unity Catalog</a> and the advanced networking features that may be necessary for a large enterprise with multiple Databricks workspaces.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pm"><img src="../Images/508e7c988c5787278492f29a7cb7c45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0U84-I5MFZyUEFR8x9guiQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Azure Databricks workspace (Image by author)</figcaption></figure><p id="70d7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#4 Register an application</strong></p><p id="edba" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This service will help mount Azure storage to Databricks, so make sure you note the application ID and tenant ID, and most importantly the app secret value, which cannot be viewed when you revisit it.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pn"><img src="../Images/71347d79943224942a4c8b3849793827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNPwIc3rQgrrNHETI39LCw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">App registration — Setting (Image by author)</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk po"><img src="../Images/899152e47dc2eb94104d5fbe94d58e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LpiOURCPhVgqNheQCrtEHg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">App registration — Info (Image by author)</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pp"><img src="../Images/39029473003e288a43c854dd3933028b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9YNk0Lynvz242-qndWEOLA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">App registration — Client secret (Image by author)</figcaption></figure><p id="ea42" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Afterward, grant the app service access rights to the app service. This is achieved by assigning the “Storage Blob Data Contributor” role to the app we just registered.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pq"><img src="../Images/4307ac970035d3de1b0ff44f080fcd4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wSOMnCPth_lHcmUUZcQ07w.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Storage account — Grant access right (1/3) (Image by author)</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pn"><img src="../Images/9e803eab9aa056940edad8db00b91982.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uCl1BbAp8Xt4YiyXpVOCWw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Storage account — Grant access right (2/3) (Image by author)</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pr"><img src="../Images/8a7941f7932f3f085535d7aa5b247c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUuG9YUa8RamUZBTjUejWg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Storage account — Grant access right (3/3) (Image by author)</figcaption></figure><p id="b40c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#5 Create Azure SQL Database</strong></p><p id="f8d7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To store a transformed data frame, we search for Azure SQL resources and pick a “Single database” as the resource type. There are choices of SQL database servers with different computing hardware, maximum data size, and more. You can instantly get the estimated cost summary while adjusting the server specifications.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ps"><img src="../Images/d1f6640b38d66f1ecf3bc1e31089c0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8xt0k0w7fKgYZF7sqIiEg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Create SQL Database (1/2) (Image by author)</figcaption></figure><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pt"><img src="../Images/bf6eaa445f2dbef9ff9a3aef44dd5ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLbZ4isotj43dBRLxJUuDA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Create SQL Database (2/2) (Image by author)</figcaption></figure><p id="1692" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">After all the initial setups, you are ready to explore how these services are linked together.</p><h2 id="42a6" class="ok ol fq bf om on oo op oq or os ot ou mr ov ow ox mv oy oz pa mz pb pc pd pe bk">Prepare for data orchestrating pipeline</h2><p id="e341" class="pw-post-body-paragraph mi mj fq mk b go pf mm mn gr pg mp mq mr ph mt mu mv pi mx my mz pj nb nc nd fj bk"><strong class="mk fr">#1 Ingest data</strong></p><p id="b577" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">We first upload the electric power consumption data to Azure Data Lake Gen2. This <a class="af ne" href="https://www.kaggle.com/datasets/uciml/electric-power-consumption-data-set/data" rel="noopener ugc nofollow" target="_blank">dataset</a> [with license as <a class="af ne" href="https://opendatacommons.org/licenses/dbcl/1-0/" rel="noopener ugc nofollow" target="_blank">Database: Open Database, Contents: Database Contents</a>], obtained from Kaggle, is sampled at a one-minute rate from December 2006 to November 2010.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pu"><img src="../Images/da8e38c40f958471e0fecc8f8f2c5ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrQJhabM2fvIaduNLMsdDA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Upload input data (Image by author)</figcaption></figure><p id="7af0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Next, we create a Notebook on the Azure Databricks workspace and mount storage by defining the parameters using previously stored ID values.</p><pre class="nm nn no np nq pv pw px bp py bb bk"><span id="9c8a" class="pz ol fq pw b bg qa qb l qc qd"># Define the configuration specifications<br/>configs = {"fs.azure.account.auth.type": "OAuth",<br/>"fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",<br/>"fs.azure.account.oauth2.client.id": "&lt;Client ID&gt;",<br/>"fs.azure.account.oauth2.client.secret": "&lt;Client Secret&gt;",<br/>"fs.azure.account.oauth2.client.endpoint": "https://login.microsoftonline.com/&lt;Tenant ID&gt;/oauth2/token"<br/>}<br/><br/>dbutils.fs.mount(<br/>  source = "abfss://input@adlstsdp.dfs.core.windows.net/", # URI of the object storage<br/>  mount_point = "/mnt/adlstsdp/input",  # local path in the /mnt directory<br/>  extra_configs = configs)</span></pre><p id="d731" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To verify file access, we can run the following command:</p><pre class="nm nn no np nq pv pw px bp py bb bk"><span id="e6a7" class="pz ol fq pw b bg qa qb l qc qd">dbutils.fs.ls(“/mnt/adlstsdp/input”)<br/><br/># Output: [FileInfo(path='dbfs:/mnt/adlstsdp/input/household_power_consumption.csv', name='household_power_consumption.csv', size=132960755, modificationTime=1716798010000)]</span></pre><p id="8cbf" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#2 Embed Notebook on Azure Databricks</strong></p><p id="599e" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Most of the source codes in this section build upon my <a class="af ne" rel="noopener" target="_blank" href="/feature-engineering-for-time-series-using-pyspark-on-databricks-02b97d62a287">previous story</a>. The idea is to perform data cleansing, transformation, and feature engineering (create time-related and moving averaging features). The transformed data is ultimately written to the Azure database table.</p><p id="ea44" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">You can check the complete code below to see the implementation.</p><pre class="nm nn no np nq pv pw px bp py bb bk"><span id="acaa" class="pz ol fq pw b bg qa qb l qc qd"># Define file location, file typem and CSV options<br/>file_location = "/mnt/adlstsdp/input/household_power_consumption.csv"<br/>file_type = "csv"<br/>schema = "Date STRING, Time STRING, Global_active_power DOUBLE, Global_reactive_power DOUBLE, Voltage DOUBLE, Global_intensity DOUBLE, Sub_metering_1 DOUBLE, Sub_metering_2 DOUBLE, Sub_metering_3 DOUBLE"<br/>first_row_is_header = "true"<br/>delimiter = ";"<br/><br/># Read CSV files<br/>org_df = spark.read.format(file_type) \<br/>  .schema(schema) \<br/>  .option("header", first_row_is_header) \<br/>  .option("delimiter", delimiter) \<br/>  .load(file_location)<br/><br/># Data cleansing and transformation<br/>from pyspark.sql.functions import *<br/>cleaned_df = org_df.na.drop()<br/>cleaned_df = cleaned_df.withColumn("Date", to_date(col("Date"),"d/M/y"))<br/>cleaned_df = cleaned_df.withColumn("Date", cleaned_df["Date"].cast("date"))<br/>cleaned_df = cleaned_df.select(concat_ws(" ", to_date(col("Date"),"d/M/y"), col("Time")).alias("DateTime"), "*")<br/>cleaned_df = cleaned_df.withColumn("DateTime", cleaned_df["DateTime"].cast("timestamp"))<br/>df = cleaned_df.groupby("Date").agg(<br/>    round(sum("Global_active_power"), 2).alias("Total_global_active_power"),<br/>    ).sort(["Date"])<br/><br/># Add time-related features<br/>df = df.withColumn("year", year("Date"))<br/>df = df.withColumn("month", month("Date"))<br/>df = df.withColumn("week_num", weekofyear("Date"))<br/><br/># Add lagged value features of total global active power<br/>from pyspark.sql.window import Window<br/>from pyspark.sql.functions import lag<br/><br/>windowSpec = Window.orderBy("Date")<br/>df = df.withColumn("power_lag1", round(lag(col("Total_global_active_power"), 1).over(windowSpec), 2))<br/><br/># Create delta field<br/>df = df.withColumn("power_lag1_delta", round(col("power_lag1") - col("Total_global_active_power"), 2))<br/><br/># Create window average fields<br/>def add_window_avg_fields(df, window_sizes):<br/>    for idx, window_size in enumerate(window_sizes, start=1):<br/>        window_col_name = f"avg_power_lag_{idx}"<br/>        windowSpec = Window.orderBy("Date").rowsBetween(-window_size, 0)<br/>        df = df.withColumn(window_col_name, round(avg(col("Total_global_active_power")).over(windowSpec), 2))<br/>    return df<br/><br/>window_sizes = [14, 30]<br/>df = add_window_avg_fields(df, window_sizes)<br/><br/># Create Exponentially Weighted Moving Average (EWMA) fields<br/>import pyspark.pandas as ps<br/>ps.set_option('compute.ops_on_diff_frames', True)<br/><br/>def add_ewma_fields(df, alphas):<br/>    for idx, alpha in enumerate(alphas, start=1):<br/>        ewma_col_name = f"ewma_power_weight_{idx}"<br/>        windowSpec = Window.orderBy("Date")<br/>        df[ewma_col_name] = df.Total_global_active_power.ewm(alpha=alpha).mean().round(2)<br/>    return df<br/><br/>alphas = [0.2, 0.8]<br/>df_pd = df.pandas_api()<br/>df_pd = add_ewma_fields(df_pd, alphas)<br/>df = df_pd.to_spark()<br/><br/># Write transformed dataframe to the database table "electric_usage_table"<br/>df.write.format("jdbc") \<br/>    .option("url", "jdbc:sqlserver://sql-db-dp.database.windows.net:1433;databaseName=sql-db-dp") \<br/>    .option("dbtable", "dbo.electric_usage_table") \<br/>    .option("user", "&lt;username&gt;") \<br/>    .option("password", "&lt;password&gt;") \<br/>    .mode("overwrite")  \<br/>    .save()</span></pre><p id="2dab" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#3 Build a basic pipeline in ADF</strong></p><p id="7117" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In ADF, we add a “Notebook” activity to the pipeline environment, then configure it to reference the desired Notebook in the Databricks folder. Set up the Databricks linked service, then validate and publish the entire activity pipeline in ADF. You can then run the pipeline in “Debug” mode.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qe"><img src="../Images/77f80f4bd8dac29b014997ad560cb4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fW2eixfAUhQSHmJ6uADb6g.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">The success status of pipeline run (Image by author)</figcaption></figure><p id="6d15" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The activity status shows “Succeeded”, meaning the data should be migrated and inserted into the Azure SQL Database table. We can view the results for verification using the query editor.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qf"><img src="../Images/f9a3cbd5c05477cfd6c63700ff13670f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDK-hqyRSSGXpjhZv-78mA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Query result of the Azure SQL database (Image by author)</figcaption></figure><p id="9091" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#4 Automate the pipeline</strong></p><p id="53d7" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">ADF offers functionalities that are far beyond the above simple implementation. For example, we can automate the pipeline by creating a <a class="af ne" href="https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory" rel="noopener ugc nofollow" target="_blank">storage-based event trigger</a>. Make sure that <code class="cx qg qh qi pw b">Microsoft.EventGrid</code> is registered as one of the resource providers in your account subscription, then set up the trigger: Whenever a new dataset is uploaded to the storage account, the pipeline will automatically execute.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qj"><img src="../Images/cd7dbcf9ffdcf90150c136a408b6bf58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xb0QcReEjsblZj5MIS_0Zw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Set a new trigger in ADF (Image by author)</figcaption></figure><p id="2acd" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">This type of trigger has various use cases in the industries, such as monitoring the inventory level to replenish orders for the supply chain or tracking customer interactions for personalized recommendations in digital marketing.</p><p id="7340" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">#5 Parameterize the Notebook variables</strong></p><p id="ee5c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">To take a further step to build a more dynamic data pipeline, we can make variables more parametric. For example, during feature engineering on time-series data, the window size of data features may not be optimized initially. The window sizes may need to be adjusted to capture seasonal patterns or based on downstream model fine-tuning. For this scenario, we can amend with the below settings.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qk"><img src="../Images/3c3140b421b91d991eb48967cfdc610b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tl3-iR-AnLUdMPi8YCJGdA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Set up parameters for pipeline run (Image by author)</figcaption></figure><p id="d3b9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In the Notebook, add the code below to create a widget that can get the parameters input from the ADF pipeline:</p><pre class="nm nn no np nq pv pw px bp py bb bk"><span id="68b4" class="pz ol fq pw b bg qa qb l qc qd"># Additional code: Access the current value of the widget<br/>inputWindowSizes = dbutils.widgets.get("inputWindowSizes")<br/>window_sizes = inputWindowSizes.split(",")<br/><br/># Original function for adding window average features<br/>df = add_window_avg_fields(df, window_sizes)</span></pre><p id="e22d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">After adjusting the settings and the Notebook codes, we can run the pipeline by providing the window size parameter values, such as 30 and 60.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk ql"><img src="../Images/77ce6fbed57e233a919818d647fdfc8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUlysyx8zM6T8o9ez_fZGA.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Input window size value for pipeline run (Image by author)</figcaption></figure><p id="b97c" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Finally, we can monitor the pipeline status again using ADF or Databricks workspace.</p><h2 id="a5b9" class="ok ol fq bf om on oo op oq or os ot ou mr ov ow ox mv oy oz pa mz pb pc pd pe bk">Wrapping it up</h2><p id="3c4d" class="pw-post-body-paragraph mi mj fq mk b go pf mm mn gr pg mp mq mr ph mt mu mv pi mx my mz pj nb nc nd fj bk">In our hands-on exploration, we mainly used ADF with Azure Databricks to orchestrate a dynamic time-series data pipeline:</p><ul class=""><li id="8b5e" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd qm qn qo bk">Setup the cloud resources for the compute, analytics, and storage</li><li id="aa66" class="mi mj fq mk b go qp mm mn gr qq mp mq mr qr mt mu mv qs mx my mz qt nb nc nd qm qn qo bk">Build the skeleton of the data pipeline from data ingestion to storage</li><li id="1c6d" class="mi mj fq mk b go qp mm mn gr qq mp mq mr qr mt mu mv qs mx my mz qt nb nc nd qm qn qo bk">Bring flexibilities to the pipeline by creating triggers and parameterizing variables</li></ul><p id="3109" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">At the enterprise level, more <a class="af ne" href="https://medium.com/@johnleungTJ/how-to-design-the-ai-architectures-in-azure-for-the-new-era-9531229cfd33" rel="noopener">complex cloud architectures</a> may be implemented to satisfy evolving needs, such as streaming data, model monitoring, and multi-model pipelines. It thus becomes essential to strive for a delicate balance between performance, reliability, and cost-efficiency through team collaboration on governance policies and spending management.</p></div></div></div><div class="ab cb oc od oe of" role="separator"><span class="og by bm oh oi oj"/><span class="og by bm oh oi oj"/><span class="og by bm oh oi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6f2d" class="ok ol fq bf om on oo op oq or os ot ou mr ov ow ox mv oy oz pa mz pb pc pd pe bk">Before you go</h2><p id="74bd" class="pw-post-body-paragraph mi mj fq mk b go pf mm mn gr pg mp mq mr ph mt mu mv pi mx my mz pj nb nc nd fj bk">If you enjoy this reading, I invite you to<strong class="mk fr"> </strong>follow my <a class="af ne" href="https://medium.com/@johnleungTJ" rel="noopener">Medium page</a> and <a class="af ne" href="https://www.linkedin.com/in/john-leung-639800115/" rel="noopener ugc nofollow" target="_blank">LinkedIn page</a>. By doing so, you can stay updated with exciting content related to data science side projects and Machine Learning Operations (MLOps) demonstration methodologies.</p><div class="qu qv qw qx qy qz"><a rel="noopener follow" target="_blank" href="/performing-customer-analytics-with-langchain-and-llms-0af4ea38f7b5?source=post_page-----810819608231--------------------------------"><div class="ra ab ig"><div class="rb ab co cb rc rd"><h2 class="bf fr hw z io re iq ir rf it iv fp bk">Performing Customer Analytics with LangChain and LLMs</h2><div class="rg l"><h3 class="bf b hw z io re iq ir rf it iv dx">Discover the potentials and constraints of LangChain for customer analytics, accompanied by practical implementation…</h3></div><div class="rh l"><p class="bf b dy z io re iq ir rf it iv dx">towardsdatascience.com</p></div></div><div class="ri l"><div class="rj l rk rl rm ri rn lq qz"/></div></div></a></div><div class="qu qv qw qx qy qz"><a rel="noopener follow" target="_blank" href="/managing-the-technical-debts-of-machine-learning-systems-5b85d420ab9d?source=post_page-----810819608231--------------------------------"><div class="ra ab ig"><div class="rb ab co cb rc rd"><h2 class="bf fr hw z io re iq ir rf it iv fp bk">Managing the Technical Debts of Machine Learning Systems</h2><div class="rg l"><h3 class="bf b hw z io re iq ir rf it iv dx">Explore the practices with implementation codes for sustainably mitigating the cost of speedy delivery</h3></div><div class="rh l"><p class="bf b dy z io re iq ir rf it iv dx">towardsdatascience.com</p></div></div><div class="ri l"><div class="ro l rk rl rm ri rn lq qz"/></div></div></a></div></div></div></div></div>    
</body>
</html>