<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Building a Biomedical Entity Linker with LLMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Building a Biomedical Entity Linker with LLMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-biomedical-entity-linker-with-llms-d385cb85c15a?source=collection_archive---------1-----------------------#2024-03-19">https://towardsdatascience.com/building-a-biomedical-entity-linker-with-llms-d385cb85c15a?source=collection_archive---------1-----------------------#2024-03-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="abf0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">How can an LLM be applied effectively for biomedical entity linking?</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@anand.subu10?source=post_page---byline--d385cb85c15a--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Anand Subramanian" class="l ep by dd de cx" src="../Images/096dc5504d6ada2493e0ac26959e60f0.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*IfxBBsal-XaXfAXh_c9g1A.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d385cb85c15a--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@anand.subu10?source=post_page---byline--d385cb85c15a--------------------------------" rel="noopener follow">Anand Subramanian</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d385cb85c15a--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">26 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/0d4e8f951e2ffc6a22bf4f64e0b63818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UqIQ8neIfoDljJVo"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@alinnnaaaa" rel="noopener ugc nofollow" target="_blank">Alina Grubnyak</a> on <a class="af nc" href="https://unsplash.com/photos/low-angle-photography-of-metal-structure-ZiQkhI7417A" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1551" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Biomedical text is a catch-all term that broadly encompasses documents such as research articles, clinical trial reports, and patient records, serving as rich repositories of information about various biological, medical, and scientific concepts. Research papers in the biomedical field present novel breakthroughs in areas like drug discovery, drug side effects, and new disease treatments. Clinical trial reports offer in-depth details on the safety, efficacy, and side effects of new medications or treatments. Meanwhile, patient records contain comprehensive medical histories, diagnoses, treatment plans, and outcomes recorded by physicians and healthcare professionals.</p><p id="3f2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Mining these texts allows practitioners to extract valuable insights, which can be beneficial for various downstream tasks. You could mine text to identify adverse drug reactions, build automated medical coding algorithms or implement information retrieval or question-answering systems for extracting information from vast research corpora. However, one issue affecting biomedical document processing is the often unstructured nature of the text. For example, researchers might use different terms to refer to the same concept. What one researcher calls a <strong class="nf fr">“heart attack</strong>” might be referred to as a <strong class="nf fr">“myocardial infarction”</strong> by another. Similarly, in drug-related documentation, technical and common names may be used interchangeably. For instance, <strong class="nf fr">“Acetaminophen”</strong> is the technical name of a drug, while <strong class="nf fr">“Paracetamol”</strong> is its more common counterpart. The prevalence of abbreviations also adds another layer of complexity; for instance, <strong class="nf fr">“Nitric Oxide”</strong> might be referred to as <strong class="nf fr">“NO”</strong> in another context. Despite these varying terms referring to the same concept, these variations make it difficult for a layman or a text-processing algorithm to determine whether they refer to the same concept. Thus, <strong class="nf fr">Entity Linking</strong> becomes crucial in this situation.</p><h1 id="e9d3" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Table of Contents:</h1><ol class=""><li id="cdce" class="nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny pa pb pc bk"><a class="af nc" href="#558f" rel="noopener ugc nofollow">What is Entity Linking?</a></li><li id="3cf1" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#9285" rel="noopener ugc nofollow">Where do LLMs come in here?</a></li><li id="cac5" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#5023" rel="noopener ugc nofollow">Experimental Setup</a></li><li id="1f0b" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#29ad" rel="noopener ugc nofollow">Processing the Dataset</a></li><li id="a7bc" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#5925" rel="noopener ugc nofollow">Zero-Shot Entity Linking using the LLM</a></li><li id="7451" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#1299" rel="noopener ugc nofollow">LLM with Retrieval Augmented Generation for Entity Linking</a></li><li id="9144" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#8d3c" rel="noopener ugc nofollow">Zero-Shot Entity Extraction with the LLM and an External KB Linker</a></li><li id="78a8" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#274a" rel="noopener ugc nofollow">Fine-tuned Entity Extraction with the LLM and an External KB Linker</a></li><li id="7eea" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#63d3" rel="noopener ugc nofollow">Benchmarking Scispacy</a></li><li id="c723" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#7e94" rel="noopener ugc nofollow">Takeaways</a></li><li id="1563" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#8e52" rel="noopener ugc nofollow">Limitations</a></li><li id="fe4c" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><a class="af nc" href="#fb75" rel="noopener ugc nofollow">References</a></li></ol><h1 id="558f" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">What is Entity Linking?</h1><p id="296f" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">When text is unstructured, accurately identifying and standardizing medical concepts becomes crucial. To achieve this, medical terminology systems such as <strong class="nf fr">Unified Medical Language System (UMLS)</strong> [1], <strong class="nf fr">Systematized Medical Nomenclature for Medicine–Clinical Terminology (SNOMED-CT)</strong> [2], and <strong class="nf fr">Medical Subject Headings (MeSH)</strong> [3] play an essential role. These systems provide a comprehensive and standardized set of medical concepts, each uniquely identified by an alphanumeric code.</p><p id="45b1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Entity linking involves recognizing and extracting entities within the text and mapping them to standardized concepts in a large terminology. In this context, a <strong class="nf fr">Knowledge Base (KB)</strong> refers to a detailed database containing standardized information and concepts related to the terminology, such as medical terms, diseases, and drugs. Typically, a KB is expert-curated and designed, containing detailed information about the concepts, including variations of the terms that could be used to refer to the concept, or how it is related to other concepts.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pi"><img src="../Images/2d49dc0785ee698265fce0c8c4a29456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*foBwD0VsATOVXT6P1tSq7Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">An overview of the Entity Recognition and Linking Pipeline. The entities are first parsed from the text, and then each entity is linked to a Knowledge Base to obtain their corresponding identifiers. The knowledge base considered in this example is MeSH Terminology. The example text is taken from the BioCreative V CDR Corpus [4,5,6,7,8] (Image by Author)</figcaption></figure><p id="3615" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Entity recognition entails extracting words or phrases that are significant in the context of our task. In this context, it usually refers to extraction of biomedical terms such as drugs, diseases etc. Typically, lookup-based methods or machine learning/deep learning-based systems are often used for entity recognition. Linking the entities to a KB usually involves a retriever system that indexes the KB. This system takes each extracted entity from the previous step and retrieves likely identifiers from the KB. The retriever here is also an abstraction, which may be sparse (BM-25), dense (embedding-based), or even a generative system (like a Large Language Model, (LLM)) that has encoded the KB in its parameters.</p><h1 id="9285" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Where do LLMs come in here?</h1><p id="7444" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">I’ve been curious for a while about the best ways to integrate LLMs into biomedical and clinical text-processing pipelines. Given that Entity Linking is an important part of such pipelines, I decided to explore how best LLMs can be utilized for this task. Specifically I investigated the following setups:</p><ol class=""><li id="902c" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">Zero-Shot Entity Linking with an LLM:</strong> Leveraging an LLM to directly identify all entities and concept IDs from input biomedical texts without any fine-tuning.</li><li id="6394" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">LLM with Retrieval Augmented Generation (RAG)</strong>: Utilizing the LLM within a RAG framework by injecting information about relevant concept IDs in the prompt for entity linking.</li><li id="8898" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Zero-Shot Entity Extraction with LLM with an External KB Linker</strong>: Employing the LLM for zero-shot entity extraction from biomedical texts, with an external linker/retriever for mapping the entities to concept IDs.</li><li id="2a62" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Fine-tuned Entity Extraction with an External KB Linker:</strong> Finetuning the LLM first on the entity extraction task, and using it as an entity extractor with an external linker/retriever for mapping the entities to concept IDs.</li><li id="603f" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Comparison with an existing pipeline: </strong>How do these methods fare comparted to Scispacy, a commonly used library for biomedical text processing?</li></ol><h1 id="5023" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Experimental Setup</h1><blockquote class="pj pk pl"><p id="95cf" class="nd ne pm nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">All code and resources related to this article are made available at <a class="af nc" href="https://github.com/anand-subu/blog_resources" rel="noopener ugc nofollow" target="_blank">this Github repository</a>, under the entity_linking folder. Feel free to pull the repository and run the notebooks directly to run these experiments. Please let me know if you have any feedback or observations or if you notice any mistakes!</p></blockquote><p id="4914" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To conduct these experiments, we utilize the <a class="af nc" href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2" rel="noopener ugc nofollow" target="_blank">Mistral-7B Instruct</a> [9] as our LLM. For the medical terminology to link entities against, we utilize the MeSH terminology. To quote the <a class="af nc" href="https://www.nlm.nih.gov/mesh/meshhome.html" rel="noopener ugc nofollow" target="_blank">National Library of Medicine website</a>:</p><blockquote class="pj pk pl"><p id="bde7" class="nd ne pm nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">“T<!-- -->he Medical Subject Headings (MeSH) thesaurus is a controlled and hierarchically-organized vocabulary produced by the National Library of Medicine. It is used for indexing, cataloging, and searching of biomedical and health-related information.”</p></blockquote><p id="a426" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We utilize the BioCreative-V-CDR-Corpus [4,5,6,7,8] for evaluation. This dataset contains annotations of disease and chemical entities, along with their corresponding MeSH IDs. For evaluation purposes, we randomly sample 100 data points from the test set. We used a version of the MeSH KB provided by Scispacy [10,11], which contains information about the MeSH identifiers, such as definitions and entities corresponding to each ID.</p><p id="8d11" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For performance evaluation, we calculate two metrics. The first metric relates to the entity extraction performance. The original dataset contains all mentions of entities in the text, annotated at the substring level. A strict evaluation would check if the algorithm has outputted all occurrences of all entities. However, we simplify this process for easier evaluation; we lower-case and de-duplicate the entities in the ground truth. We then calculated the Precision, Recall and F1 score for each instance and calculate the macro-average for each metric.</p><p id="4589" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Suppose you have a set of actual entities, <code class="cx pn po pp pq b">ground_truth</code>, and a set of entities predicted by a model, <code class="cx pn po pp pq b">pred</code> for each input text. The <strong class="nf fr">true positives</strong> <code class="cx pn po pp pq b">TP</code> can be determined by identifying the common elements between <code class="cx pn po pp pq b">pred</code> and <code class="cx pn po pp pq b">ground_truth</code>, essentially by calculating the intersection of these two sets.</p><p id="6e0b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For each input, we can then calculate:</p><p id="9793" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx pn po pp pq b">precision = len(TP)/ len(pred)</code> ,</p><p id="d537" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx pn po pp pq b">recall = len(TP) / len(ground_truth)</code> and</p><p id="0a4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><code class="cx pn po pp pq b">f1 = 2 * precision * recall / (precision + recall)</code></p><p id="cca5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and finally calculate the macro-average for each metric by summing them all up and dividing by the number of datapoints in our test set.</p><p id="53d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For evaluating the overall entity linking performance, we again calculate the same metrics. In this case, for each input datapoint, we have a set of tuples, where each tuple is a <code class="cx pn po pp pq b">(entity, mesh_id)</code> pair. The metrics are otherwise calculated the same way.</p><h1 id="29ad" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Processing the Dataset</h1><p id="3c46" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Right, let’s kick off things by first defining some helper functions for processing our dataset.</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="3e61" class="pu oa fq pq b bg pv pw l px py">def parse_dataset(file_path):<br/>    """<br/>    Parse the BioCreative Dataset.<br/><br/>    Args:<br/>    - file_path (str): Path to the file containing the documents.<br/><br/>    Returns:<br/>    - list of dict: A list where each element is a dictionary representing a document.<br/>    """<br/>    documents = []<br/>    current_doc = None<br/><br/>    with open(file_path, 'r', encoding='utf-8') as file:<br/>        for line in file:<br/>            line = line.strip()<br/>            if not line:<br/>                continue<br/>            if "|t|" in line:<br/>                if current_doc:<br/>                    documents.append(current_doc)<br/>                id_, title = line.split("|t|", 1)<br/>                current_doc = {'id': id_, 'title': title, 'abstract': '', 'annotations': []}<br/>            elif "|a|" in line:<br/>                _, abstract = line.split("|a|", 1)<br/>                current_doc['abstract'] = abstract<br/>            else:<br/>                parts = line.split("\t")<br/>                if parts[1] == "CID":<br/>                    continue<br/>                annotation = {<br/>                    'text': parts[3],<br/>                    'type': parts[4],<br/>                    'identifier': parts[5]<br/>                }<br/>                current_doc['annotations'].append(annotation)<br/><br/>        if current_doc:<br/>            documents.append(current_doc)<br/><br/>    return documents<br/><br/>def deduplicate_annotations(documents):<br/>    """<br/>    Filter documents to ensure annotation consistency.<br/><br/>    Args:<br/>    - documents (list of dict): The list of documents to be checked.<br/>    """<br/>    for doc in documents:<br/>        doc["annotations"] = remove_duplicates(doc["annotations"])<br/>        <br/>def remove_duplicates(dict_list):<br/>    """<br/>    Remove duplicate dictionaries from a list of dictionaries.<br/><br/>    Args:<br/>    - dict_list (list of dict): A list of dictionaries from which duplicates are to be removed.<br/><br/>    Returns:<br/>    - list of dict: A list of dictionaries after removing duplicates.<br/>    """<br/>    unique_dicts = []  <br/>    seen = set()<br/><br/>    for d in dict_list:<br/>        dict_tuple = tuple(sorted(d.items()))<br/>        if dict_tuple not in seen:<br/>            seen.add(dict_tuple)<br/>            unique_dicts.append(d)<br/><br/>    return unique_dicts</span></pre><p id="e33b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We first parse the dataset from the text files provided in the original dataset. The original dataset includes the title, abstract, and all entities annotated with their entity type (Disease or Chemical), their substring indices indicating their exact location in the text, along with their MeSH IDs. While processing our dataset, we make a few simplifications. We disregard the substring indices and the entity type. Moreover, we de-duplicate annotations that share the same entity name and MeSH ID. At this stage, we only de-duplicate in a case-sensitive manner, meaning if the same entity appears in both lower and upper case across the document, we retain both instances in our processing so far.</p><h1 id="5925" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Zero-Shot Entity Linking using the LLM</strong></h1><p id="dfe7" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">First, we aim to determine whether the LLM already possesses an understanding of MeSH terminology due to its pre-training, and if it can function as a zero-shot entity linker. By zero-shot, we mean the LLM’s capability to directly link entities to their MeSH IDs from biomedical text based on its intrinsic knowledge, without depending on an external KB linker. This hypothesis is not entirely unrealistic, considering the availability of information about MeSH online, which makes it possible that the model might have encountered MeSH-related information during its pre-training phase. However, even if the LLM was trained with such information, it is unlikely that this alone would enable the model to perform zero-shot entity linking effectively, due to the complexity of biomedical terminology and the precision required for accurate entity linking.</p><p id="4eb4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To evaluate this, we provide the input text to the LLM and directly prompt it to predict the entities and corresponding MeSH IDs. Additionally, we create a few-shot prompt by sampling three data points from the training dataset. It is important to clarify the distinction in the use of “zero-shot” and “few-shot” here: “zero-shot” refers to the LLM as a whole performing entity linking without prior specific training on this task, while “few-shot” refers to the prompting strategy employed in this context.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/fd42fe12a7a1f7a02ab36a6869cb6304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*65PB-N2KlMAAylh7aGDp5w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LLM as a Zero-Shot Entity Linker (Image by Author)</figcaption></figure><p id="1f4f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To calculate our metrics, we define functions for evaluating the performance:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="4f0c" class="pu oa fq pq b bg pv pw l px py">def calculate_entity_metrics(gt, pred):<br/>    """<br/>    Calculate precision, recall, and F1-score for entity recognition.<br/><br/>    Args:<br/>    - gt (list of dict): A list of dictionaries representing the ground truth entities. <br/>                         Each dictionary should have a key "text" with the entity text.<br/>    - pred (list of dict): A list of dictionaries representing the predicted entities.<br/>                           Similar to `gt`, each dictionary should have a key "text".<br/><br/>    Returns:<br/>    tuple: A tuple containing precision, recall, and F1-score (in that order).<br/>    """<br/>    ground_truth_set = set([x["text"].lower() for x in gt])<br/>    predicted_set = set([x["text"].lower() for x in pred])<br/><br/>    # True positives are predicted items that are in the ground truth<br/>    true_positives = len(predicted_set.intersection(ground_truth_set))<br/>    <br/>    # Precision calculation<br/>    if len(predicted_set) == 0:<br/>        precision = 0<br/>    else:<br/>        precision = true_positives / len(predicted_set)<br/>    <br/>    # Recall calculation<br/>    if len(ground_truth_set) == 0:<br/>        recall = 0<br/>    else:<br/>        recall = true_positives / len(ground_truth_set)<br/>    <br/>    # F1-score calculation<br/>    if precision + recall == 0:<br/>        f1_score = 0<br/>    else:<br/>        f1_score = 2 * (precision * recall) / (precision + recall)<br/>    <br/>    return precision, recall, f1_score<br/><br/>def calculate_mesh_metrics(gt, pred):<br/>    """<br/>    Calculate precision, recall, and F1-score for matching MeSH (Medical Subject Headings) codes.<br/><br/>    Args:<br/>    - gt (list of dict): Ground truth data<br/>    - pred (list of dict): Predicted data<br/><br/>    Returns:<br/>    tuple: A tuple containing precision, recall, and F1-score (in that order).<br/>    """<br/>    ground_truth = []<br/><br/>    for item in gt:<br/>        mesh_codes = item["identifier"]<br/>        if mesh_codes == "-1":<br/>            mesh_codes = "None"<br/>        mesh_codes_split = mesh_codes.split("|")<br/>        for elem in mesh_codes_split:<br/>            combined_elem = {"entity": item["text"].lower(), "identifier": elem}<br/>            if combined_elem not in ground_truth:<br/>                ground_truth.append(combined_elem)<br/>    <br/>    predicted = []<br/>    for item in pred:<br/>        mesh_codes = item["identifier"]<br/>        mesh_codes_split = mesh_codes.strip().split("|")<br/>        for elem in mesh_codes_split:<br/>            combined_elem = {"entity": item["text"].lower(), "identifier": elem}<br/>            if combined_elem not in predicted:<br/>                predicted.append(combined_elem)<br/>    # True positives are predicted items that are in the ground truth<br/>    true_positives = len([x for x in predicted if x in ground_truth])<br/>    <br/>    # Precision calculation<br/>    if len(predicted) == 0:<br/>        precision = 0<br/>    else:<br/>        precision = true_positives / len(predicted)<br/>    <br/>    # Recall calculation<br/>    if len(ground_truth) == 0:<br/>        recall = 0<br/>    else:<br/>        recall = true_positives / len(ground_truth)<br/>    <br/>    # F1-score calculation<br/>    if precision + recall == 0:<br/>        f1_score = 0<br/>    else:<br/>        f1_score = 2 * (precision * recall) / (precision + recall)<br/>    <br/>    return precision, recall, f1_score</span></pre><p id="068d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s now run the model and get our predictions:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="3a8f" class="pu oa fq pq b bg pv pw l px py">model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2",  torch_dtype=torch.bfloat16).cuda()<br/>tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.2")<br/>model.eval()<br/><br/>mistral_few_shot_answers = []<br/>for item in tqdm(test_set_subsample):<br/>    few_shot_prompt_messages = build_few_shot_prompt(SYSTEM_PROMPT, item, few_shot_example)<br/>    input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors = "pt").cuda()<br/>    outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    <br/>    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    mistral_few_shot_answers.append(parse_answer(gen_text.strip()))</span></pre><p id="11d4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">At the entity extraction level, the LLM performs quite well, considering it has not been explicitly fine-tuned for this task. However, its performance as a zero-shot linker is quite poor, with an overall performance of less than 1%. This outcome is intuitive, though, because the output space for MeSH labels is vast, and it is a hard task to exactly map entities to a specific MeSH ID.</p><figure class="mm mn mo mp mq mr"><div class="qa io l ed"><div class="qb qc l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Zero-Shot Entity Extraction and Entity Linking Scores</figcaption></figure><h1 id="1299" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">LLM with Retrieval Augmented Generation for Entity Linking</strong></h1><p id="c95b" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Retrieval Augmented Generation (RAG) [12] refers to a framework that combines LLMs with an external KB equipped with a querying function, such as a retriever/linker. For each incoming query, the system first retrieves knowledge relevant to the query from the KB using the querying function. It then combines the retrieved knowledge and the query, providing this combined prompt to the LLM to perform the task. This approach is based on the understanding that LLMs may not have all the necessary knowledge or information to answer an incoming query effectively. Thus, knowledge is injected into the model by querying an external knowledge source.</p><p id="c68a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Using a RAG framework can offer several advantages:</p><ol class=""><li id="4cb8" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk">An existing LLM can be utilized for a new domain or task <strong class="nf fr">without the need for domain-specific fine-tuning</strong>, as the relevant information can be queried and provided to the model through a prompt.</li><li id="c5c4" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk">LLMs can sometimes provide <strong class="nf fr">incorrect answers (hallucinate)</strong> when responding to queries. Employing RAG with LLMs can significantly reduce such hallucinations, as the answers provided by the LLM <strong class="nf fr">are more likely to be grounded in facts</strong> due to the knowledge supplied to it.</li></ol><p id="fa8e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Considering that the LLM lacks specific knowledge of MeSH terminologies, we investigate whether a RAG setup could enhance performance. In this approach, for each input paragraph, we utilize a BM-25 retriever to query the KB. For each MeSH ID, we have access to a general description of the ID and the entity names associated with it. After retrieval, we inject this information to the model through the prompt for entity linking.</p><p id="65fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To investigate the effect of the number of retrieved IDs provided as context to the model on the entity linking process, we run this setup by providing top 10, 30 and 50 documents to the model and quantify its performance on entity extraction and MeSH concept identification.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/d148dffe6caaa1a993320d088efbe29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*hjDCq-rA4_hzQhF4onObrg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">LLM with RAG as an Entity Linker (Image by Author)</figcaption></figure><p id="2ff6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s first define our BM-25 Retriever:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="0bb3" class="pu oa fq pq b bg pv pw l px py">from rank_bm25 import BM25Okapi<br/>from typing import List, Tuple, Dict<br/>from nltk.tokenize import word_tokenize<br/>from tqdm import tqdm<br/><br/>class BM25Retriever:<br/>    """<br/>    A class for retrieving documents using the BM25 algorithm.<br/>    <br/>    Attributes:<br/>        index (List[int, str]): A dictionary with document IDs as keys and document texts as values.<br/>        tokenized_docs (List[List[str]]): Tokenized version of the documents in `processed_index`.<br/>        bm25 (BM25Okapi): An instance of the BM25Okapi model from the rank_bm25 package.<br/>    """<br/>    <br/>    def __init__(self, docs_with_ids: Dict[int, str]):<br/>        """<br/>        Initializes the BM25Retriever with a dictionary of documents.<br/>        <br/>        Args:<br/>            docs_with_ids (List[List[str, str]]): A dictionary with document IDs as keys and document texts as values.<br/>        """<br/>        self.index = docs_with_ids<br/>        self.tokenized_docs = self._tokenize_docs([x[1] for x in self.index])<br/>        self.bm25 = BM25Okapi(self.tokenized_docs)<br/>            <br/>    def _tokenize_docs(self, docs: List[str]) -&gt; List[List[str]]:<br/>        """<br/>        Tokenizes the documents using NLTK's word_tokenize.<br/>        <br/>        Args:<br/>            docs (List[str]): A list of documents to be tokenized.<br/>        <br/>        Returns:<br/>            List[List[str]]: A list of tokenized documents.<br/>        """<br/>        return [word_tokenize(doc.lower()) for doc in docs]<br/>    <br/>    def query(self, query: str, top_n: int = 10) -&gt; List[Tuple[int, float]]:<br/>        """<br/>        Queries the BM25 model and retrieves the top N documents with their scores.<br/>        <br/>        Args:<br/>            query (str): The query string.<br/>            top_n (int): The number of top documents to retrieve.<br/>        <br/>        Returns:<br/>            List[Tuple[int, float]]: A list of tuples, each containing a document ID and its BM25 score.<br/>        """<br/>        tokenized_query = word_tokenize(query.lower())<br/>        scores = self.bm25.get_scores(tokenized_query)<br/>        doc_scores_with_ids = [(doc_id, scores[i]) for i, (doc_id, _) in enumerate(self.index)]<br/>        top_doc_ids_and_scores = sorted(doc_scores_with_ids, key=lambda x: x[1], reverse=True)[:top_n]<br/>        return [x[0] for x in top_doc_ids_and_scores]</span></pre><p id="8c0f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now process our KB file and create a BM-25 retriever instance that indexes it. While indexing the KB, we index each ID using a concatenation of their description, aliases and canonical name.</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="e617" class="pu oa fq pq b bg pv pw l px py">def process_index(index):<br/>    """<br/>    Processes the initial document index to combine aliases, canonical names, and definitions into a single text index.<br/><br/>    Args:<br/>    - index (Dict): The MeSH knowledge base<br/>    Returns:<br/>        List[List[int, str]]: A dictionary with document IDs as keys and combined text indices as values.<br/>    """<br/>    processed_index = []<br/>    for key, value in tqdm(index.items()):<br/>        assert(type(value["aliases"]) != list)<br/>        aliases_text = " ".join(value["aliases"].split(","))<br/>        text_index = (aliases_text + " " +  value.get("canonical_name", "")).strip()<br/>        if "definition" in value:<br/>            text_index += " " + value["definition"]<br/>        processed_index.append([value["concept_id"], text_index])<br/>    return processed_index<br/><br/>mesh_data = read_jsonl_file("mesh_2020.jsonl")<br/>process_mesh_kb(mesh_data)<br/>mesh_data_kb = {x["concept_id"]:x for x in mesh_data}<br/>mesh_data_dict = process_index({x["concept_id"]:x for x in mesh_data})<br/>retriever = BM25Retriever(mesh_data_dict)</span></pre><pre class="qe pr pq ps bp pt bb bk"><span id="3b21" class="pu oa fq pq b bg pv pw l px py">mistral_rag_answers = {10:[], 30:[], 50:[]}<br/><br/>for k in [10,30,50]:<br/>    for item in tqdm(test_set_subsample):<br/>        relevant_mesh_ids = retriever.query(item["title"] + " " + item["abstract"], top_n = k)<br/>        relevant_contexts = [mesh_data_kb[x] for x in relevant_mesh_ids]<br/>        rag_prompt = build_rag_prompt(SYSTEM_RAG_PROMPT, item, relevant_contexts)<br/>        input_ids = tokenizer.apply_chat_template(rag_prompt, tokenize=True, return_tensors = "pt").cuda()<br/>        outputs = model.generate(input_ids = input_ids, max_new_tokens=200, do_sample=False)    <br/>        gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>        mistral_rag_answers[k].append(parse_answer(gen_text.strip()))</span></pre><pre class="qe pr pq ps bp pt bb bk"><span id="e0ed" class="pu oa fq pq b bg pv pw l px py">entity_scores_at_k = {}<br/>mesh_scores_at_k = {}<br/><br/>for key, value in mistral_rag_answers.items():<br/>    entity_scores = [calculate_entity_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, value)]<br/>    macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)<br/>    macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)<br/>    macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)<br/>    entity_scores_at_k[key] = {"macro-precision": macro_precision_entity, "macro-recall": macro_recall_entity, "macro-f1": macro_f1_entity}<br/>    <br/>    mesh_scores = [calculate_mesh_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, value)]<br/>    macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)<br/>    macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)<br/>    macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)<br/>    mesh_scores_at_k[key] = {"macro-precision": macro_precision_mesh, "macro-recall": macro_recall_mesh, "macro-f1": macro_f1_mesh}</span></pre><p id="5e45" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In general, the RAG setup improves the overall MeSH Identification process, compared to the original zero-shot setup. But what is the impact of the number of documents provided as information to the model? We plot the scores as a function of the number of retrieved IDs provided to the model as context.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qf ln qg lo qh cf qi cg qj ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr qk ql qm qn qo paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/6ae236d2474983aadfbb5d1e563cb577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*UAmKm1B0qD__KBG7RgOCbg.png"/></div></figure><figure class="lb mr qp ql qm qn qo paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/b3d3431fb1870acf1eb3e99153a7c978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*v5YC68NYgxOn6dq1krfWRg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx qq ed qr qs">Plots of Entity Extraction and Entity Linking performance metrics as a function of the number of retrieved documents in the RAG setting (Image by Author)</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ff3b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We observe interesting trends while investigating the plots. For entity extraction, an increase in the number of retrieved documents correlates with a sharp increase in macro-precision, reaching a score of slightly higher than 50%. This is nearly 10% higher than the zero-shot entity extraction performance of the model. However, the impact on macro-recall is task-dependent; it remains unchanged for entity extraction but improves for entity linking. Overall, increasing the number of documents provided to the model as context improves all metrics significantly in the MeSH Identification setting, but has mixed gains in the entity extraction setting.</p><p id="990b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An important limitation to consider in this experiment is the performance of the upstream retriever. If the retriever fails to retrieve relevant documents, the performance of the LLM will suffer as a consequence because the actual answer is not present in the knowledge provided to the model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/7295ffa080576fa90975f690981e945d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oIS-BSNYvcYhQD0Tl42uDg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">% of ground truth MeSH IDs present in the MeSH IDs fetched by the retriever per input text as a function of total retrieved IDs (Image by Author)</figcaption></figure><p id="77a4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To investigate this, we calculated the average % of ground truth MeSH IDs present in the MeSH IDs fetched by the retriever per input text. Our findings show that the BM-25 retriever manages to retrieve only about 12.6% to 17.7% of the relevant MeSH IDs for each input data point on average. The choice of retriever and the way we retrieve is therefore a significant performance bottleneck for the RAG setup and can potentially be optimized for better performance.</p><h1 id="8d3c" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Zero-Shot Entity Extraction with the LLM and an External KB Linker</strong></h1><p id="95d9" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">So far, we’ve examined how the LLM performs as a zero-shot entity linker and to what extent RAG can enhance its performance. Though RAG improves performance compared to the zero-shot setup, there are limitations to this approach.</p><p id="115f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">When using LLMs in a RAG setup, we have kept the knowledge component (KB + retriever) <strong class="nf fr">upstream </strong>of the model until now. The retrieval of knowledge in the RAG setup is <strong class="nf fr">coarse</strong>, in that we retrieve possible MeSH IDs by querying the retriever using the entire biomedical text. This ensures <strong class="nf fr">diversity </strong>to a certain extent in the retrieved results, as the fetched results are likely to correspond to different entities in the text, but the results are less likely to be <strong class="nf fr">precise</strong>. This may not seem like a problem at first, because you can mitigate this to a certain degree by providing more relevant results as context to the model in the RAG setting. However, this has two drawbacks:</p><ol class=""><li id="d71e" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk">LLMs generally have an upper bound on the context length for processing text. The context length of an LLM roughly refers to the maximum number of tokens the LLM can take into account (the number of tokens in the prompt) before generating new text. This can restrict the amount of knowledge we can provide to the LLM.</li><li id="bc77" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk">Let’s assume we have an LLM capable of processing long context lengths. We can now retrieve and append more context to the model. Great! However, a longer context length may not necessarily correlate with enhanced RAG abilities for the LLM [13]. Even if you pass a lot of relevant knowledge to the LLM by retrieving more results, this does not guarantee that the LLM will accurately extract the correct answer.</li></ol><p id="2b83" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This brings us back to the <strong class="nf fr">traditional pipeline</strong> of entity linking as described initially. In this setting, the knowledge component is kept <strong class="nf fr">downstream</strong> to the model, where after entity extraction, the entities are provided to an external retriever for obtaining the relevant MeSH ID. Provided you have a good entity extractor, you can retrieve more precise MeSH IDs.</p><p id="87b0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Earlier, we observed in the fully zero-shot setting that, while the LLM was poor at predicting the MeSH ID, its entity extraction performance was quite decent. We now extract the entities using the Mistral model and provide them to an external retriever for fetching the MeSH IDs.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pi"><img src="../Images/fd74260b6f485d82ec964f16ec94c936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*OTbkOexkcrCQCuuxniT2_A.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Entity Linking with LLM as Entity Extractor and External Retriever (Image by Author)</figcaption></figure><p id="5c78" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For retrieval here, we again use a BM-25 retriever as our KB linker. However, a small change we make here is to index our IDs based on concatenating their canonical name and aliases. We re-use the entities extracted from the first zero-shot setup for our experiment here. Let’s now evaluate how well this setup performs:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="66db" class="pu oa fq pq b bg pv pw l px py">entity_mesh_data_dict = [[x["concept_id"] , " ".join(x["aliases"].split(",")) + " " + x["canonical_name"]] for x in mesh_data]<br/>entity_retriever = BM25Retriever(entity_mesh_data_dict)<br/><br/>parsed_entities_few_shot = [[y["text"] for y in x] for x in mistral_few_shot_answers]<br/>retrieved_answers = []<br/><br/>for item in tqdm(parsed_entities_few_shot):<br/>    answer_element = []<br/>    for entity in item:<br/>        retrieved_mesh_ids = entity_retriever.query(entity, top_n = 1)<br/>        answer_element.append({"text": entity, "identifier":retrieved_mesh_ids[0]})<br/>    retrieved_answers.append(answer_element)<br/><br/>mesh_scores = [calculate_mesh_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]<br/>macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(metric_scores)<br/>macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(metric_scores)<br/>macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(metric_scores)</span></pre><p id="26b1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The performance in this setting significantly improves over the RAG setting across all the metrics. We achieve more than 12% improvement in Macro-Precision, 20% improvement in Macro-Recall and 16% improvement in Macro-F1 scores compared to the best RAG setting (retrieval at 50 documents). To <strong class="nf fr">stress the point again</strong>, this is more akin to the traditional pipeline of entity extraction where you have entity extraction and linking as separate components.</p><figure class="mm mn mo mp mq mr"><div class="qa io l ed"><div class="qb qc l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Zero-Shot LLM Entity Extraction and External Retriever Scores</figcaption></figure><h1 id="274a" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">Fine-tuned Entity Extraction with the LLM and an External KB Linker</strong></h1><p id="65ac" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">Until now, we got the best performance by using the LLM as an entity extractor within a larger pipeline. However, we did the entity extraction in a zero-shot manner. Could we achieve further performance gains by fine-tuning the LLM specifically for entity extraction?</p><p id="32bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For fine-tuning, we utilize the training set from the BioCreative V dataset, which consists of 500 data points. We employ Q-Lora [14] for fine-tuning our LLM, a process that involves quantizing our LLM to 4-bit and freezing it, while fine-tuning a Low-Rank Adapter. This approach is generally parameter and memory efficient, as the Adapter possesses only a fraction of the weights compared to the original LLM, meaning we are fine-tuning significantly fewer weights than if we were to fine-tune the entire LLM. It also enables us to fine-tune our model on a single GPU.</p><p id="a9f4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s implement the fine-tuning component. For this part, I referred to and modified <a class="af nc" href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Mistral/Supervised_fine_tuning_(SFT)_of_an_LLM_using_Hugging_Face_tooling.ipynb" rel="noopener ugc nofollow" target="_blank">Niels Rogge’s notebook on fine-tuning a Mistral Model with Q-Lora</a>, with the modifications mostly around correctly preparing and processing the dataset.</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="5c9a" class="pu oa fq pq b bg pv pw l px py">from datasets import load_dataset<br/>import json<br/>from tqdm import tqdm<br/>from itertools import chain<br/>from datasets import DatasetDict<br/>from transformers import AutoTokenizer, BitsAndBytesConfig<br/>import torch<br/>from trl import SFTTrainer<br/>from peft import LoraConfig<br/>from transformers import TrainingArguments<br/>from helpers import *<br/><br/><br/>def read_jsonl_file(file_path):<br/>    """<br/>    Parses a JSONL (JSON Lines) file and returns a list of dictionaries.<br/><br/>    Args:<br/>        file_path (str): The path to the JSONL file to be read.<br/><br/>    Returns:<br/>        list of dict: A list where each element is a dictionary representing<br/>            a JSON object from the file.<br/>    """<br/>    jsonl_lines = []<br/>    with open(file_path, 'r', encoding="utf-8") as file:<br/>        for line in file:<br/>            json_object = json.loads(line)<br/>            jsonl_lines.append(json_object)<br/>            <br/>    return jsonl_lines<br/><br/>def convert_to_template(data):<br/>    messages = []<br/>    messages.append({"role": "user", "content": data["question"]})<br/>    messages.append({"role": "assistant", "content": data["answer"]})<br/>    <br/>    return tokenizer.apply_chat_template(messages, tokenize = False)<br/><br/>mesh_dataset = parse_dataset("CDR_TrainingSet.PubTator.txt")</span></pre><p id="b5f9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now load the tokenizer and set the appropriate parameters:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="aad6" class="pu oa fq pq b bg pv pw l px py">model_id = "mistralai/Mistral-7B-Instruct-v0.2"<br/><br/>tokenizer = AutoTokenizer.from_pretrained(model_id)<br/><br/># set pad_token_id equal to the eos_token_id if not set<br/>tokenizer.pad_token_id = tokenizer.eos_token_id<br/>tokenizer.padding_side = "right"<br/><br/># Set reasonable default for models without max length<br/>if tokenizer.model_max_length &gt; 100_000:<br/>  tokenizer.model_max_length = 512</span></pre><p id="beb4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s now prepare and format our dataset properly. We define the prompts for our model and format our datasets in the expected chat template.</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="bf41" class="pu oa fq pq b bg pv pw l px py">prepared_dataset = []<br/>system_prompt = "Answer the question factually and precisely."<br/>entity_prompt = "What are the chemical and disease related entities present in this biomedical text?"<br/>prepared_dataset = []<br/><br/>def prepare_instructions(elem):<br/>    entities = []<br/>    for x in elem["annotations"]:<br/>        if x["text"] not in entities:<br/>            entities.append(x["text"])<br/>            <br/>    return {"question": system_prompt + "\n" + entity_prompt + "\n" + elem["title"] + " " + elem["abstract"] , "answer": "The entities are:" + ",".join(entities)}<br/><br/>questions = [prepare_instructions(x) for x in tqdm(mesh_dataset)]<br/>chat_format_questions = [{"text": convert_to_template(x)} for x in tqdm(questions)]<br/><br/>df = pd.DataFrame(chat_format_questions)<br/>train_dataset = Dataset.from_pandas(df)</span></pre><p id="de2b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let’s now define the appropriate configs for fine-tuning our model. We define the configuration for quantizing the LLM:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="5f53" class="pu oa fq pq b bg pv pw l px py">quantization_config = BitsAndBytesConfig(<br/>            load_in_4bit=True,<br/>            bnb_4bit_quant_type="nf4",<br/>            bnb_4bit_compute_dtype=torch.bfloat16,<br/>)<br/><br/>device_map = {"": torch.cuda.current_device()} if torch.cuda.is_available() else None<br/><br/>model_kwargs = dict(<br/>    torch_dtype=torch.bfloat16,<br/>    use_cache=False, # set to False as we're going to use gradient checkpointing<br/>    device_map=device_map,<br/>    quantization_config=quantization_config,<br/>)</span></pre><p id="e714" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now, we are all-set to finetune our model:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="4f71" class="pu oa fq pq b bg pv pw l px py">output_dir = 'entity_finetune'<br/><br/># based on config<br/>training_args = TrainingArguments(<br/>    bf16=True, # specify bf16=True instead when training on GPUs that support bf16<br/>    do_eval=False,<br/>    # evaluation_strategy="no",<br/>    gradient_accumulation_steps=1,<br/>    gradient_checkpointing=True,<br/>    gradient_checkpointing_kwargs={"use_reentrant": False},<br/>    learning_rate=1.0e-04,<br/>    log_level="info",<br/>    logging_steps=5,<br/>    logging_strategy="steps",<br/>    lr_scheduler_type="cosine",<br/>    max_steps=-1,<br/>    num_train_epochs=5,<br/>    output_dir=output_dir,<br/>    overwrite_output_dir=True,<br/>    per_device_eval_batch_size=1, <br/>    per_device_train_batch_size=8,<br/>    save_strategy="no",<br/>    save_total_limit=None,<br/>    seed=42,<br/>)<br/><br/># based on config<br/>peft_config = LoraConfig(<br/>        r=16,<br/>        lora_alpha=16,<br/>        lora_dropout=0.1,<br/>        bias="none",<br/>        task_type="CAUSAL_LM",<br/>        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],<br/>)<br/><br/>trainer = SFTTrainer(<br/>        model=model_id,<br/>        model_init_kwargs=model_kwargs,<br/>        args=training_args,<br/>        train_dataset=train_dataset,<br/>        eval_dataset=train_dataset,<br/>        dataset_text_field="text",<br/>        tokenizer=tokenizer,<br/>        packing = True,<br/>        peft_config=peft_config,<br/>        max_seq_length=tokenizer.model_max_length,<br/>    )<br/><br/>train_result = trainer.train()<br/>trainer.save_model(output_dir)</span></pre><p id="1678" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now that we’ve completed the fine-tuning process, let’s now utilize the model for inference and obtain the performance metrics:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="14dd" class="pu oa fq pq b bg pv pw l px py">def parse_entities_from_trained_model(content):<br/>    """<br/>    Extracts a list of entities from the output of a trained model.<br/><br/>    Args:<br/>    - content (str): The raw string output from a trained model.<br/><br/>    Returns:<br/>    - list of str: A list of entities extracted from the model's output.<br/>    """<br/>    return content.split("The entities are:")[-1].split(",")<br/><br/>mistral_few_shot_answers = []<br/>for item in tqdm(test_set_subsample):<br/>    few_shot_prompt_messages = build_entity_prompt(item)<br/>    # input_ids = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=True, return_tensors = "pt").cuda()<br/>    prompt = tokenizer.apply_chat_template(few_shot_prompt_messages, tokenize=False)<br/>    tensors = tokenizer(prompt, return_tensors="pt")<br/>    input_ids = tensors.input_ids.cuda()<br/>    attention_mask = tensors.attention_mask.cuda()<br/>    outputs = model.generate(input_ids = input_ids, attention_mask = attention_mask, max_new_tokens=200, do_sample=False)    <br/>    # https://github.com/huggingface/transformers/issues/17117#issuecomment-1124497554<br/>    gen_text = tokenizer.batch_decode(outputs.detach().cpu().numpy()[:, input_ids.shape[1]:], skip_special_tokens=True)[0]<br/>    mistral_few_shot_answers.append(parse_entities_from_trained_model(gen_text.strip()))</span></pre><pre class="qe pr pq ps bp pt bb bk"><span id="8dd5" class="pu oa fq pq b bg pv pw l px py">parsed_entities_few_shot = [[y["text"] for y in x] for x in mistral_few_shot_answers]<br/>retrieved_answers = []<br/><br/>for item in tqdm(parsed_entities_few_shot):<br/>    answer_element = []<br/>    for entity in item:<br/>        retrieved_mesh_ids = entity_ranker.query(entity, top_n = 1)<br/>        answer_element.append({"identifier":retrieved_mesh_ids[0], "text":entity})<br/>    retrieved_answers.append(answer_element)<br/><br/>entity_scores = [calculate_entity_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]<br/>macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)<br/>macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)<br/>macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)<br/><br/>mesh_scores = [calculate_mesh_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, retrieved_answers)]<br/>macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(mesh_scores)<br/>macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(mesh_scores)<br/>macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(mesh_scores)</span></pre><p id="0ca6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This setup is exactly similar to the previous setup in that we continue to use the LLM as an entity extractor, and an external retriever for linking each entity to the MeSH ID. Fine-tuning the model leads to significant improvements across entity extraction and linking.</p><p id="2aa7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Compared to zero-shot entity extraction, fine-tuning improves all metrics by a factor of upto or more than 20%. Similarly, entity linking is also improved by a factor of around 12–14% across all metrics compared to the previous setting. These are not surprising takeaways though, as a task-specific model is expected to perform much better than the zero-shot setup. Still it’s nice to quantify these improvements concretely!</p><figure class="mm mn mo mp mq mr"><div class="qa io l ed"><div class="qb qc l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Fine-tuned LLM as Entity Extractor and External Retriever Scores</figcaption></figure><h1 id="63d3" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Benchmarking Scispacy</h1><p id="347a" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">How does this implementation compare with an existing tool that can perform entity linking? Scispacy is a common work-horse for biomedical and clinical text processing, and provides features for entity extraction and entity linking. Specifically, Scispacy also provides a functionality to link entities to the MeSH KB, which is the file we also use as the KB originally for our LLM experiments. Let’s benchmark the performance of Scispacy on our test set for comparison with our LLM experiments.</p><p id="a149" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use the “<strong class="nf fr">en_ner_bc5cdr_md</strong>” [15] in Scispacy as the entity extraction module, as this model has been specifically trained on the BioCreative V dataset. Let’s evaluate the performance:</p><pre class="mm mn mo mp mq pr pq ps bp pt bb bk"><span id="a46b" class="pu oa fq pq b bg pv pw l px py">from scispacy.linking import EntityLinker<br/>import spacy, scispacy<br/>import pandas as pd<br/>from helpers import *<br/>from tqdm import tqdm<br/><br/>#code for setting up MeSH linker referred from https://github.com/allenai/scispacy/issues/355<br/>config = {<br/>    "resolve_abbreviations": True,  <br/>    "linker_name": "mesh", <br/>    "max_entities_per_mention":1<br/>}<br/><br/>nlp = spacy.load("en_ner_bc5cdr_md")<br/>nlp.add_pipe("scispacy_linker", config=config) <br/><br/>linker = nlp.get_pipe("scispacy_linker")<br/><br/>def extract_mesh_ids(text):<br/>    mesh_entity_pairs = []<br/>    doc = nlp(text)<br/>    for e in doc.ents:<br/>        if e._.kb_ents:<br/>            cui = e._.kb_ents[0][0]<br/>            mesh_entity_pairs.append({"text": e.text, "identifier": cui})<br/>        else:<br/>            mesh_entity_pairs.append({"text": e.text, "identifier": "None"})<br/>    <br/>    return mesh_entity_pairs</span></pre><pre class="qe pr pq ps bp pt bb bk"><span id="ce1e" class="pu oa fq pq b bg pv pw l px py">all_mesh_ids = []<br/>for item in tqdm(test_set_subsample):<br/>    text = item["title"] + " " + item["abstract"]<br/>    mesh_ids = extract_mesh_ids(text)<br/>    all_mesh_ids.append(mesh_ids)<br/><br/>entity_scores = [calculate_entity_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, all_mesh_ids)]<br/>macro_precision_entity = sum([x[0] for x in entity_scores]) / len(entity_scores)<br/>macro_recall_entity = sum([x[1] for x in entity_scores]) / len(entity_scores)<br/>macro_f1_entity = sum([x[2] for x in entity_scores]) / len(entity_scores)<br/><br/>mesh_scores = [calculate_mesh_metrics(gt["annotations"],pred) for gt, pred in zip(test_set_subsample, all_mesh_ids)]<br/>macro_precision_mesh = sum([x[0] for x in mesh_scores]) / len(entity_scores)<br/>macro_recall_mesh = sum([x[1] for x in mesh_scores]) / len(entity_scores)<br/>macro_f1_mesh = sum([x[2] for x in mesh_scores]) / len(entity_scores)</span></pre><figure class="mm mn mo mp mq mr"><div class="qa io l ed"><div class="qb qc l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Scispacy evaluation scores</figcaption></figure><p id="6569" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Scispacy outperforms the fine-tuned LLM on entity extraction by a factor of 10% across all metrics, and by a factor of 14–20% on entity linking! For the task of biomedical entity extraction and linking, Scispacy remains a robust tool.</p><h1 id="7e94" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Takeaways</h1></div></div><div class="mr"><div class="ab cb"><div class="lm qf ln qg lo qh cf qi cg qj ci bh"><figure class="mm mn mo mp mq mr qm qn paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qu"><img src="../Images/c719dd37db7371d0764e5c9e4843dc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*5evYOxnigJgt5zpveHyEiQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Macro-F1 Scores of Entity Extraction and Entity Linking across all setups</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="ddb4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Having come to the end of our experiments, what are the concrete takeaways from them?</p><ol class=""><li id="b110" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">Strengths in Zero-Shot Entity Extraction:</strong> Mistral-Instruct is a decent zero-shot entity extractor for biomedical text. While its parametric knowledge is not sufficient for performing zero-shot MeSH entity linking, we leverage it as an entity extractor in conjunction with an external KB retriever in our experiments to get much better performance.</li><li id="2145" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">RAG’s Improvement over Zero-Shot Prediction:</strong> The LLM in a RAG setup demonstrates an improvement over a purely zero-shot approach for entity linking. However, the retriever component within a RAG setup can be a significant bottleneck, as in our case, the BM-25 retriever only manages to retrieve around 12–17% of relevant IDs per data point. This suggests a need for more effective retrieval methods.</li><li id="9f23" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Pipelined extraction provides the best performance:</strong> Given the capabilities of the LLM as an entity extractor, the best performance is achieved when leveraging these capabilities within a larger pipeline that includes an external retriever to link entities to the MeSH knowledge base (KB). This is identical to the traditional setting, where entity extraction and KB-linking are kept as separate modules.</li><li id="47ee" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Benefits of Fine-Tuning:</strong> Fine-tuning the LLM using QLora for the entity extraction task leads to significant performance improvements on entity extraction and entity linking when used in tandem with an external retriever.</li><li id="e580" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Scispacy performs the best</strong>: Scispacy outperforms all LLM-based methods for entity linking tasks in our experiments. For biomedical text processing, Scispacy remains a robust tool. It also requires less computational power for running compared to an LLM, which needs a good GPU for fast inference. In contrast, Scispacy only requires a good CPU.</li><li id="2dff" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Opportunities for Optimization:</strong> Our current implementations of LLM-based pipelines for entity linking are quite naive with substantial room for improvement. Some areas that could benefit from optimization include the choice of retrieval and the retrieval logic itself. Fine-tuning the LLM with more data could also further boost its entity extraction performance.</li></ol><h1 id="8e52" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk">Limitations</h1><p id="456c" class="pw-post-body-paragraph nd ne fq nf b go ov nh ni gr ow nk nl nm ox no np nq oy ns nt nu oz nw nx ny fj bk">There are some limitations to our experiments so far.</p><ol class=""><li id="d522" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pa pb pc bk"><strong class="nf fr">Multiple MeSH IDs for an entity:</strong> In our dataset, a few entities in each document could be linked to multiple MeSH IDs. Out of a total of 968 entities across 100 documents in our test set, this occurs in 15 cases (1.54%). In the Scispacy evaluation, as well as in all LLM experiments where we used the external KB linker (BM-25 retriever) after entity extraction, we link only one MeSH concept per entity. Although Scispacy offers the possibility of linking more than one MeSH ID per entity, we opt not to use this feature to ensure a fair comparison with our LLM experiments. Extending the functionality to support linking to more than one concept would also be an interesting addition.</li><li id="8db5" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">MeSH IDs not in the Knowledge Base:</strong> In the test dataset, there are MeSH IDs for entities that are not included in KB. Specifically, 64 entities (6.6% of cases) possess a MeSH ID that is absent from our KB. This limitation lies on the retriever side and can be addressed by updating the KB.</li><li id="6c9c" class="nd ne fq nf b go pd nh ni gr pe nk nl nm pf no np nq pg ns nt nu ph nw nx ny pa pb pc bk"><strong class="nf fr">Entities lacking a MeSH ID:</strong> Similarly, another 1.65% of entities (16 out of 968) cannot be mapped to a MeSH ID. In all LLM experiments where we use the external KB linker after entity extraction, we currently lack the ability to determine whether an entity has no MeSH ID.</li></ol><h1 id="fb75" class="nz oa fq bf ob oc od gq oe of og gt oh oi oj ok ol om on oo op oq or os ot ou bk"><strong class="al">References</strong></h1><blockquote class="pj pk pl"><p id="6d78" class="nd ne pm nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">I’ve included all papers and resources referred in this article here. Please let me know if I missed out on anything, and I will add them!</p></blockquote><p id="3803" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[1] Bodenreider O. (2004). The Unified Medical Language System (UMLS): integrating biomedical terminology. <em class="pm">Nucleic acids research</em>, <em class="pm">32</em>(Database issue), D267–D270. <a class="af nc" href="https://doi.org/10.1093/nar/gkh061" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1093/nar/gkh061</a></p><p id="0173" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[2] <a class="af nc" href="https://www.nlm.nih.gov/healthit/snomedct/index.html" rel="noopener ugc nofollow" target="_blank">https://www.nlm.nih.gov/healthit/snomedct/index.html</a></p><p id="837d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[3] <a class="af nc" href="https://www.nlm.nih.gov/mesh/meshhome.html" rel="noopener ugc nofollow" target="_blank">https://www.nlm.nih.gov/mesh/meshhome.html</a></p><p id="f9d9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[4] Wei CH, Peng Y, Leaman R, Davis AP, Mattingly CJ, Li J, Wiegers TC, Lu Z. Overview of the BioCreative V Chemical Disease Relation (CDR) Task, Proceedings of the Fifth BioCreative Challenge Evaluation Workshop, p154–166, 2015</p><p id="39db" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[5] Li J, Sun Y, Johnson RJ, Sciaky D, Wei CH, Leaman R, Davis AP, Mattingly CJ, Wiegers TC, Lu Z. Anotating chemicals, diseases and their interactions in biomedical literature, Proceedings of the Fifth BioCreative Challenge Evaluation Workshop, p173–182, 2015</p><p id="1ccb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[6] Leaman R, Dogan RI, Lu Z. DNorm: disease name normalization with pairwise learning to rank, Bioinformatics 29(22):2909–17, 2013</p><p id="f1aa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[7] Leaman R, Wei CH, Lu Z. tmChem: a high performance approach for chemical named entity recognition and normalization. J Cheminform, 7:S3, 2015</p><p id="3c0d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[8] Li, J., Sun, Y., Johnson, R. J., Sciaky, D., Wei, C. H., Leaman, R., Davis, A. P., Mattingly, C. J., Wiegers, T. C., &amp; Lu, Z. (2016). BioCreative V CDR task corpus: a resource for chemical disease relation extraction. <em class="pm">Database : the journal of biological databases and curation</em>, <em class="pm">2016</em>, baw068. <a class="af nc" href="https://doi.org/10.1093/database/baw068" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1093/database/baw068</a></p><p id="3b38" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[9] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., … &amp; Sayed, W. E. (2023). Mistral 7B. <em class="pm">arXiv preprint arXiv:2310.06825</em>.</p><p id="0010" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[10] Neumann, M., King, D., Beltagy, I., &amp; Ammar, W. (2019, August). ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing. In <em class="pm">Proceedings of the 18th BioNLP Workshop and Shared Task</em> (pp. 319–327).</p><p id="6cb9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[11] <a class="af nc" href="https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/kbs/2020-10-09/mesh_2020.jsonl" rel="noopener ugc nofollow" target="_blank">https://ai2-s2-scispacy.s3-us-west-2.amazonaws.com/data/kbs/2020-10-09/mesh_2020.jsonl</a></p><p id="3b2c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[12] Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., … &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. <em class="pm">Advances in Neural Information Processing Systems</em>, <em class="pm">33</em>, 9459–9474.</p><p id="f6c1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[13] Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., &amp; Liang, P. (2024). Lost in the Middle: How Language Models Use Long Contexts. <em class="pm">Transactions of the Association for Computational Linguistics</em>, <em class="pm">12</em>.</p><p id="8a7f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[14] Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2024). Qlora: Efficient finetuning of quantized llms. <em class="pm">Advances in Neural Information Processing Systems</em>, <em class="pm">36</em>.</p><p id="dfc8" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">[15] <a class="af nc" href="https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_ner_bc5cdr_md-0.5.4.tar.gz" rel="noopener ugc nofollow" target="_blank">https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_ner_bc5cdr_md-0.4.0.tar.gz</a></p></div></div></div></div>    
</body>
</html>