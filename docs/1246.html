<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Evaluate Your Predictions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Evaluate Your Predictions</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-evaluate-your-predictions-cef80d8f6a69?source=collection_archive---------5-----------------------#2024-05-17">https://towardsdatascience.com/how-to-evaluate-your-predictions-cef80d8f6a69?source=collection_archive---------5-----------------------#2024-05-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8ca0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Be mindful of the measure you choose</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@jeffrey_85949?source=post_page---byline--cef80d8f6a69--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jeffrey Näf" class="l ep by dd de cx" src="../Images/0ce6db85501192cdebeeb910eb81a688.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*fTRnnMS3RSMCfs2m5zRcDQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cef80d8f6a69--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@jeffrey_85949?source=post_page---byline--cef80d8f6a69--------------------------------" rel="noopener follow">Jeffrey Näf</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cef80d8f6a69--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/3547acaf064f99ff34fb1719c0dfa9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VVF7D8zvpTOgZY2O"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@isaacmsmith?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Isaac Smith</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="26fb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk nz"><span class="l oa ob oc bo od oe of og oh ed">T</span>esting and benchmarking machine learning models by comparing their predictions on a test set, even after deployment, is of fundamental importance. To do this, one needs to think of a measure or <em class="oi">score </em>that takes a prediction and a test point and assigns a value measuring how successful the prediction is with respect to the test point. However, one should think carefully about which scoring measure is appropriate. In particular, when choosing a method to evaluate a prediction we should adhere to the idea of <em class="oi">proper scoring rules</em>. I only give a loose definition of this idea here, but basically, we want a score that is minimized at the thing we want to measure!</p><blockquote class="oj ok ol"><p id="5cb0" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As<!-- --> a general rule: One can use MSE to evaluate mean predictions, MAE to evaluate median predictions, the quantile score to evaluate more general quantile predictions and the energy or MMD score to evaluate distributional predictions.</p></blockquote><p id="4816" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Consider a variable you want to predict, say a random variable <em class="oi">Y</em>, from a vector of covariates <strong class="nf fr"><em class="oi">X</em></strong>. In the example below, <em class="oi">Y</em> will be income and <strong class="nf fr"><em class="oi">X</em></strong> will be certain characteristics, such as <em class="oi">age </em>and <em class="oi">education</em>. We learned a predictor <em class="oi">f</em> on some training data and now we predict <em class="oi">Y</em> as <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em>. Usually, when we want to predict a variable <em class="oi">Y</em> as well as possible we predict the expectation of <em class="oi">y</em> given <strong class="nf fr">x</strong>, i.e. <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> should approximate <em class="oi">E[Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">]</em>. But more generally, <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> could be an estimator of the median, other quantiles, or even the full conditional distribution <em class="oi">P(Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em>.</p><p id="5594" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now for a new test point <em class="oi">y</em>, we want to score your prediction, that is you want a function <em class="oi">S(y,f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))</em>, that is <em class="oi">minimized </em>(in expectation)<em class="oi"> </em>when <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> is the best thing you can do. For instance, if we want to predict<em class="oi"> E[Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">]</em>, this score is given as the MSE: <em class="oi">S(y, f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))= (y-f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))²</em>.</p><p id="c484" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Here we study the principle of scoring the predictor <em class="oi">f</em> over at test set of <em class="oi">(y_i,</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">_i), i=1,…,ntest</em> in more detail. In all examples we will compare the ideal estimation method to an other that is clearly wrong, or naive, and show that our scores do what they are supposed to. The full code used here can also be found on <a class="af nc" href="https://github.com/JeffNaef/Medium-Articles/blob/main/HowtoScoreprediction.R" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><h2 id="8b93" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">The Example</h2><p id="29f4" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">To illustrate things, I will simulate a simple dataset that should mimic income data. We will use this simple example throughout this article to illustrate the concepts.</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="219d" class="pq on fq pn b bg pr ps l pt pu">library(dplyr)<br/><br/><br/>#Create some variables:<br/># Simulate data for 100 individuals<br/>n &lt;- 5000<br/><br/># Generate age between 20 and 60<br/>age &lt;- round(runif(n, min = 20, max = 60))<br/><br/># Define education levels<br/>education_levels &lt;- c("High School", "Bachelor's", "Master's")<br/><br/># Simulate education level probabilities<br/>education_probs &lt;- c(0.4, 0.4, 0.2)<br/><br/># Sample education level based on probabilities<br/>education &lt;- sample(education_levels, n, replace = TRUE, prob = education_probs)<br/><br/># Simulate experience correlated with age with some random error<br/>experience &lt;- age - 20 + round(rnorm(n, mean = 0, sd = 3)) <br/><br/># Define a non-linear function for wage<br/>wage &lt;- exp((age * 0.1) + (case_when(education == "High School" ~ 1,<br/>                                 education == "Bachelor's" ~ 1.5,<br/>                                 TRUE ~ 2)) + (experience * 0.05) + rnorm(n, mean = 0, sd = 0.5))<br/><br/>hist(wage)</span></pre><p id="6074" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although this simulation may be oversimplified, it reflects certain well-known characteristics of such data: older age, advanced education, and greater experience are all linked to higher wages. The use of the “exp” operator results in a highly skewed wage distribution, which is a consistent observation in such datasets.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/e317a98be03239d90b15e4e4d4fb568b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1aWAHQVZwUbumKf_OjSMKw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Wage distribution over the whole simulated population. Source: Author</figcaption></figure><p id="a9fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Crucially, this skewness is also present when we fix age, education and experience to certain values. Let’s imagine we look at a specific person, Dave, who is 30 years old, has a Bachelor’s in Economics and 10 years of experience and let’s look at his actual income distribution according to our data generating process:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="30f7" class="pq on fq pn b bg pr ps l pt pu">ageDave&lt;-30<br/>educationDave&lt;-"Bachelor's"<br/>experienceDave &lt;- 10<br/><br/><br/>wageDave &lt;- exp((ageDave * 0.1) + (case_when(educationDave == "High School" ~ 1,<br/>                                     educationDave == "Bachelor's" ~ 1.5,<br/>                                     TRUE ~ 2)) + (experienceDave * 0.05) + rnorm(n, mean = 0, sd = 0.5))<br/><br/>hist(wageDave, main="Wage Distribution for Dave", xlab="Wage")</span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pw"><img src="../Images/ae06889fd2bf3960bec23c2fa34b0493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4Hrt419D9695CFoSY9CtA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Wage distrbution for Dave. Source: Author</figcaption></figure><p id="7d5a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thus the distribution of possible wages of Dave, given the information we have about him, is still highly skewed.</p><p id="9af6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We also generate a test set of several people:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="b3c9" class="pq on fq pn b bg pr ps l pt pu"><br/>## Generate test set<br/>ntest&lt;-1000<br/><br/># Generate age between 20 and 60<br/>agetest &lt;- round(runif(ntest, min = 20, max = 60))<br/><br/><br/># Sample education level based on probabilities<br/>educationtest &lt;- sample(education_levels, ntest, replace = TRUE, prob = education_probs)<br/><br/># Simulate experience correlated with age with some random error<br/>experiencetest &lt;- agetest - 20 + round(rnorm(ntest, mean = 0, sd = 3))<br/><br/><br/>## Generate ytest that we try to predict:<br/><br/>wagetest &lt;- exp((agetest * 0.1) + (case_when(educationtest == "High School" ~ 1,<br/>                                             educationtest == "Bachelor's" ~ 1.5,<br/>                                             TRUE ~ 2)) + (experiencetest * 0.05) + rnorm(ntest, mean = 0, sd = 0.5))</span></pre><p id="fdd6" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We now start simple and first look at the scores for mean and median prediction.</p><h2 id="3301" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">The scores for mean and median prediction</h2><p id="003a" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">In data science and machine learning, interest often centers on a single number that signifies the “center” or “middle” of the distribution we aim to predict, namely the (conditional) mean or median. To do this we have the mean squared error (MSE):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/1184bca3cc4aedf8a86d82f97bbddf09.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*OP_7Wi50Pb0CKAxRu_A6zw.png"/></div></figure><p id="d348" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and the mean absolute error (MAE):</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk py"><img src="../Images/ab31df4aa0a26921ad4c084fc1ceb28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*u1NgcuBR_rGwPjesApBZug.png"/></div></figure><p id="5189" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">An important takeaway is that the MSE is the appropriate metric for predicting the conditional mean, while the MAE is the measure to use for the conditional median. Mean and median are not the same thing for skewed distributions like the one we study here.</p><p id="1a57" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Let us illustrate this for the above example with very simple estimators (that we would not have access to in real life), just for illustration:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="36de" class="pq on fq pn b bg pr ps l pt pu">conditionalmeanest &lt;-<br/>  function(age, education, experience, N = 1000) {<br/>    mean(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    ))<br/>  }<br/><br/><br/>conditionalmedianest &lt;-<br/>  function(age, education, experience, N = 1000) {<br/>    median(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    ))<br/>  }</span></pre><p id="6471" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">That is we estimate mean and median, by simply simulating from the model for fixed values of age, education, and experience (this would be a simulation from the correct conditional distribution) and then we simply take the mean/median of that. Let’s test this on Dave:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="6410" class="pq on fq pn b bg pr ps l pt pu"><br/>hist(wageDave, main="Wage Distribution for Dave", xlab="Wage")<br/>abline(v=conditionalmeanest(ageDave, educationDave, experienceDave), col="darkred", cex=1.2)<br/>abline(v=conditionalmedianest(ageDave, educationDave, experienceDave), col="darkblue", cex=1.2)<br/><br/></span></pre><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pz"><img src="../Images/bb2c4d9b49be9138d294dbce07715975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d0FBBXFK6sbyW6eMfWulHg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Blue: estimated conditional median of Dave, Red: estimated conditional mean of Dave. Source: Author</figcaption></figure><p id="c039" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Clearly the mean and median are different, as one would expect from such a distribution. In fact, as is typical for income distributions, the mean is higher (more influenced by high values) than the median.</p><p id="042d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Now let’s use these estimators on the test set:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="f47d" class="pq on fq pn b bg pr ps l pt pu">Xtest&lt;-data.frame(age=agetest, education=educationtest, experience=experiencetest)<br/><br/>meanest&lt;-sapply(1:nrow(Xtest), function(j)  conditionalmeanest(Xtest$age[j], Xtest$education[j], Xtest$experience[j])  )<br/>median&lt;-sapply(1:nrow(Xtest), function(j)  conditionalmedianest(Xtest$age[j], Xtest$education[j], Xtest$experience[j])  )<br/></span></pre><p id="d441" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This gives a diverse range of conditional mean/median values. Now we calculate MSE and MAE:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="32f5" class="pq on fq pn b bg pr ps l pt pu">(MSE1&lt;-mean((meanest-wagetest)^2))<br/>(MSE2&lt;-mean((median-wagetest)^2))<br/><br/>MSE1 &lt; MSE2<br/>### Method 1 (the true mean estimator) is better than method 2!<br/><br/># but the MAE is actually worse of method 1!<br/>(MAE1&lt;-mean(abs(meanest-wagetest)) )<br/>(MAE2&lt;-mean( abs(median-wagetest)))<br/><br/>MAE1 &lt; MAE2<br/>### Method 2 (the true median estimator) is better than method 1!</span></pre><p id="f27a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This shows what is known theoretically: MSE is minimized for the (conditional) expectation <em class="oi">E[Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">]</em>, while MAE is minimized at the conditional median. <em class="oi">In general, it does not make sense to use the MAE when you try to evaluate your mean prediction. </em>In a lot of applied research and data science, people use the MAE or both to evaluate mean predictions (I know because I did it myself). While this may be warranted in certain applications, this can have serious consequences for distributions that are not symmetric, as we saw in this example: When looking at the MAE, method 1 looks worse than method 2, even though the former estimates the mean correctly. In fact, in this highly skewed example, <em class="oi">method 1 should have a lower MAE than method 2</em>.</p><blockquote class="oj ok ol"><p id="7aa3" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To score conditional mean prediction use the mean squared error (MSE) and not the mean absolute error (MAE). The MAE is minimized for the conditional median.</p></blockquote><h2 id="a2fa" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">Scores for quantile and interval prediction</h2><p id="d164" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">Assume we want to score an estimate <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> of the quantile <em class="oi">q_</em><strong class="nf fr"><em class="oi">x</em></strong> such that</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qa"><img src="../Images/cbff4304e2b9dd47bb1b99433db35f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*vlr6vLIH4yICPioIitU25w.png"/></div></figure><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/ddba86bb1cf3ae1b09ce94317ba9c536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xE6hqSj7OOL1MQZngjwtrA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Simple quantile illustration. Source: Author</figcaption></figure><p id="2e8b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this case, we can consider the quantile score:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qc"><img src="../Images/564454526b6c64549f25b8e5243cffb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hx2gl1j94UK1hL7FKfVCZw.png"/></div></div></figure><p id="f789" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">whereby</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qd"><img src="../Images/626dea41b7d98c0d9993f2830876d672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*vTQp7nGHv3sk1ajnbi9Byg.png"/></div></figure><p id="78ad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To unpack this formula, we can consider two cases:</p><p id="0f33" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">(1) <em class="oi">y</em> is smaller than <em class="oi">f(</em><strong class="nf fr"><em class="oi">x):</em></strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qe"><img src="../Images/f40be68d77162ec15f438f90e027de81.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*RZfxK2xu6v_3UzuQ8-Mmpw.png"/></div></figure><p id="0e6e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">i.e. we incur a penalty which gets bigger the further away <em class="oi">y</em> is from <em class="oi">f(</em><strong class="nf fr"><em class="oi">x).</em></strong></p><p id="fdf5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">(2) <em class="oi">y</em> is larger than <em class="oi">f(</em><strong class="nf fr"><em class="oi">x):</em></strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qf"><img src="../Images/e013cda3f26a06ee00f523618c56dc23.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*5gHjeAsBZwLtDKG_4CwnoQ.png"/></div></div></figure><p id="50ed" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">i.e. a penalty which gets bigger the further away <em class="oi">y</em> is from <em class="oi">f(</em><strong class="nf fr"><em class="oi">x).</em></strong></p><p id="dab9" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Notice that the weight is such that for a high <em class="oi">alpha</em>, having the estimated quantile <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> smaller than <em class="oi">y</em> gets penalized more. This is by design and ensures that the right quantile is indeed the minimizer of the expected value of <em class="oi">S(y,f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))</em> over y. This score is in fact the <em class="oi">quantile loss </em>(up to a factor 2), see e.g. this <a class="af nc" rel="noopener" target="_blank" href="/quantile-loss-and-quantile-regression-b0689c13f54d">nice article</a>. It is implemented in the <em class="oi">quantile_score </em>function of the package <em class="oi">scoringutils</em> in R. Finally, note that for <em class="oi">alpha=0.5:</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qg"><img src="../Images/5b834d543f3b92647fdf006820954b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAU-FXaNpvtSH19oh2EQUQ.png"/></div></div></figure><p id="15c2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">simply the MAE! This makes sense, as the 0.5 quantile is the median.</p><p id="b000" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">With the power to predict quantiles, we can also build prediction intervals. Consider (<em class="oi">l_</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">, u_</em><strong class="nf fr"><em class="oi">x)</em></strong>, where <em class="oi">l_</em><strong class="nf fr"><em class="oi">x</em></strong> ≤ <em class="oi">u_</em><strong class="nf fr"><em class="oi">x</em></strong> are quantiles such that</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qh"><img src="../Images/8b1c197758bfd89e29c80695d88eae30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*ZY_F-1xKiDt3XxQwzA_aOw.png"/></div></figure><p id="856f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In fact, this is met if <em class="oi">l_</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi"> </em>is<em class="oi"> </em>the <em class="oi">alpha/2</em> quantile, and <em class="oi">u_</em><strong class="nf fr"><em class="oi">x</em></strong> is the <em class="oi">1-alpha/2</em> quantile. Thus we now estimate and score these two quantiles. Consider <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)=(f_1(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">), f_2(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))</em>, whereby <em class="oi">f_1(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> to be an estimate of <em class="oi">l_</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi"> </em>and <em class="oi">f_2(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> an estimate of <em class="oi">u_</em><strong class="nf fr"><em class="oi">x. </em></strong>We provide two estimators, the “ideal” one that simulates again from the true process to then estimate the required quantiles and a “naive” one, which has the right coverage but is too big:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="7e41" class="pq on fq pn b bg pr ps l pt pu">library(scoringutils)<br/><br/>## Define conditional quantile estimation<br/>conditionalquantileest &lt;-<br/>  function(probs, age, education, experience, N = 1000) {<br/>    quantile(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    )<br/>    , probs =<br/>      probs)<br/>  }<br/><br/>## Define a very naive estimator that will still have the required coverage<br/>lowernaive &lt;- 0<br/>uppernaive &lt;- max(wage)<br/><br/># Define the quantile of interest<br/>alpha &lt;- 0.05<br/><br/>lower &lt;-<br/>  sapply(1:nrow(Xtest), function(j)<br/>    conditionalquantileest(alpha / 2, Xtest$age[j], Xtest$education[j], Xtest$experience[j]))<br/>upper &lt;-<br/>  sapply(1:nrow(Xtest), function(j)<br/>    conditionalquantileest(1 - alpha / 2, Xtest$age[j], Xtest$education[j], Xtest$experience[j]))<br/><br/><br/><br/>## Calculate the scores for both estimators<br/><br/># 1. Score the alpha/2 quantile estimate<br/>qs_lower &lt;- mean(quantile_score(wagetest,<br/>                           predictions = lower,<br/>                           quantiles = alpha / 2))<br/># 2. Score the alpha/2 quantile estimate<br/>qs_upper &lt;- mean(quantile_score(wagetest,<br/>                           predictions = upper,<br/>                           quantiles = 1 - alpha / 2))<br/><br/># 1. Score the alpha/2 quantile estimate<br/>qs_lowernaive &lt;- mean(quantile_score(wagetest,<br/>                                predictions = rep(lowernaive, ntest),<br/>                                quantiles = alpha / 2))<br/># 2. Score the alpha/2 quantile estimate<br/>qs_uppernaive &lt;- mean(quantile_score(wagetest,<br/>                                predictions = rep(uppernaive, ntest),<br/>                                quantiles = 1 - alpha / 2))<br/><br/># Construct the interval score by taking the average<br/>(interval_score &lt;- (qs_lower + qs_upper) / 2)<br/># Score of the ideal estimator: 187.8337<br/><br/># Construct the interval score by taking the average<br/>(interval_scorenaive &lt;- (qs_lowernaive + qs_uppernaive) / 2)<br/># Score of the naive estimator: 1451.464</span></pre><p id="ebda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Again we can clearly see that, on average, the correct estimator has a much lower score than the naive one!</p><p id="e606" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thus with the quantile score, we have a reliable way of scoring individual quantile predictions. However, the way of averaging the score of the upper and lower quantiles for the prediction interval might seem ad hoc. Luckily it turns out that this leads to the so-called <em class="oi">interval score</em>:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/d0514ecf84ab0f43820fec2ac48b0819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYOG3EFagAWDm02ZSefZ7Q.png"/></div></div></figure><p id="0408" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thus through some algebraic magic, we can score a prediction interval by averaging the scores for the <em class="oi">alpha/2</em> and the <em class="oi">1-alpha/2</em> quantiles as we did. Interestingly, the resulting interval score rewards narrow prediction intervals, and induces a penalty, the size of which depends on <em class="oi">alpha</em>, if the observation misses the interval. Instead of using the average of quantile scores, we can also directly calculate this score with the package <em class="oi">scoringutils.</em></p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="7f12" class="pq on fq pn b bg pr ps l pt pu">alpha &lt;- 0.05<br/>mean(interval_score(<br/>  wagetest,<br/>  lower=lower,<br/>  upper=upper,<br/>  interval_range=(1-alpha)*100,<br/>  weigh = T,<br/>  separate_results = FALSE<br/>))<br/>#Score of the ideal estimator: 187.8337</span></pre><p id="7ca7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This is the exact same number we got above when averaging the scores of the two intervals.</p><blockquote class="oj ok ol"><p id="075c" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The quantile score implemented in R in the package scoringutils can be used to score quantile predictions. If one wants to score a prediction interval directly, the interval_score function can be used.</p></blockquote><h2 id="2519" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">Scores for distributional prediction</h2><p id="70f7" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">More and more fields have to deal with <em class="oi">distributional prediction</em>. Luckily there are even scores for this problem. In particular, here I focus on what is called the <em class="oi">energy score:</em></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qj"><img src="../Images/1ffa382ed9a49e05f899c1995262b8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipPWmgZwD83RMCDTgiAF7Q.png"/></div></div></figure><p id="754c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">for <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)</em> being an estimate of the distribution <em class="oi">P(Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">). </em>The second term takes the expectation of the Eucledian distance between two independent samples from <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">). </em>This is akin to a normalizing term, establishing the value if the same distribution was compared. The first term then compares the sample point <em class="oi">y</em> to a draw <em class="oi">X</em> from <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">). </em>In expectation (over <em class="oi">Y </em>drawn from<em class="oi"> P(Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">))</em> this will be minimized if <em class="oi">f(</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)=P(Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">).</em></p><p id="144a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Thus instead of just predicting the mean or the quantiles, we now try to predict the whole distribution of wage at each test point. Essentially we try to predict and evaluate the conditional distribution we plotted for Dave above. This is a bit more complicated; how exactly do we represent a learned distribution? In practice this is resolved by assuming we can obtain a sample from the predicted distribution. Thus we compare a sample of <em class="oi">N</em>, obtained from the predicted distribution, to a single test point. This can be done in R using <em class="oi">es_sample </em>from the <em class="oi">scoringRules </em>package:</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="b990" class="pq on fq pn b bg pr ps l pt pu">library(scoringRules)<br/><br/>## Ideal "estimate": Simply sample from the true conditional distribution <br/>## P(Y | X=x) for each sample point x<br/>distributionestimate &lt;-<br/>  function(age, education, experience, N = 100) {<br/>    exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5))<br/>  }<br/><br/>## Naive Estimate: Only sample from the error distribution, without including the <br/>## information of each person.<br/>distributionestimatenaive &lt;-<br/>  function(age, education, experience, N = 100) {<br/>    exp(rnorm(N, mean = 0, sd = 0.5))<br/>  }<br/><br/><br/><br/><br/>scoretrue &lt;- mean(sapply(1:nrow(Xtest), function(j)  {<br/>  wageest &lt;-<br/>    distributionestimate(Xtest$age[j], Xtest$education[j], Xtest$experience[j])<br/>  return(scoringRules::es_sample(y = wagetest[j], dat = matrix(wageest, nrow=1)))<br/>}))<br/><br/>scorenaive &lt;- mean(sapply(1:nrow(Xtest), function(j)  {<br/>  wageest &lt;-<br/>    distributionestimatenaive(Xtest$age[j], Xtest$education[j], Xtest$experience[j])<br/>  return(scoringRules::es_sample(y = wagetest[j], dat = matrix(wageest, nrow=1)))<br/>}))<br/><br/>## scoretrue: 761.026<br/>## scorenaive: 2624.713</span></pre><p id="57de" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the above code, we again compare the “perfect” estimate (i.e. sampling from the true distribution <em class="oi">P(Y | </em><strong class="nf fr"><em class="oi">X</em></strong><em class="oi">=</em><strong class="nf fr"><em class="oi">x</em></strong><em class="oi">)) </em>to a very naive one, namely one that does not consider any information on wage, edicuation or experience. Again, the score reliably identifies the better of the two methods.</p><blockquote class="oj ok ol"><p id="0e30" class="nd ne oi nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The energy score, implemented in the R package scoringRules can be used to score distributional prediction, if a sample from the predicted distribution is available.</p></blockquote><h2 id="4444" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">Conclusion</h2><p id="07cd" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">We have looked at different ways of scoring predictions. Thinking about the right measure to test predictions is important, as the wrong measure might make us choose and keep the wrong model for our prediction task.</p><p id="f9fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">It should be noted that especially for distributional prediction this scoring is a difficult task and the score might not have much power in practice. That is, even a method that leads to a large improvement might only have a slightly smaller score. However, this is not a problem per se, as long as the score is able to reliably identify the better of the two methods.</p><h2 id="4720" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">References</h2><p id="ab7d" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">[1] Tilmann Gneiting &amp; Adrian E Raftery (2007) Strictly Proper Scoring Rules, Prediction, and Estimation, Journal of the American Statistical Association, 102:477, 359–378, DOI: <a class="af nc" href="https://doi.org/10.1198/016214506000001437" rel="noopener ugc nofollow" target="_blank">10.1198/016214506000001437</a></p><h2 id="1bb1" class="om on fq bf oo op oq or os ot ou ov ow nm ox oy oz nq pa pb pc nu pd pe pf pg bk">Appendix: All the code in one place</h2><p id="42d0" class="pw-post-body-paragraph nd ne fq nf b go ph nh ni gr pi nk nl nm pj no np nq pk ns nt nu pl nw nx ny fj bk">This file can also be found on <a class="af nc" href="https://github.com/JeffNaef/Medium-Articles/blob/main/HowtoScoreprediction.R" rel="noopener ugc nofollow" target="_blank">Github</a>.</p><pre class="mm mn mo mp mq pm pn po bp pp bb bk"><span id="6f75" class="pq on fq pn b bg pr ps l pt pu">library(dplyr)<br/><br/>#Create some variables:<br/># Simulate data for 100 individuals<br/>n &lt;- 5000<br/><br/># Generate age between 20 and 60<br/>age &lt;- round(runif(n, min = 20, max = 60))<br/><br/># Define education levels<br/>education_levels &lt;- c("High School", "Bachelor's", "Master's")<br/><br/># Simulate education level probabilities<br/>education_probs &lt;- c(0.4, 0.4, 0.2)<br/><br/># Sample education level based on probabilities<br/>education &lt;- sample(education_levels, n, replace = TRUE, prob = education_probs)<br/><br/># Simulate experience correlated with age with some random error<br/>experience &lt;- age - 20 + round(rnorm(n, mean = 0, sd = 3)) <br/><br/># Define a non-linear function for wage<br/>wage &lt;- exp((age * 0.1) + (case_when(education == "High School" ~ 1,<br/>                                     education == "Bachelor's" ~ 1.5,<br/>                                     TRUE ~ 2)) + (experience * 0.05) + rnorm(n, mean = 0, sd = 0.5))<br/><br/>hist(wage)<br/><br/><br/><br/>ageDave&lt;-30<br/>educationDave&lt;-"Bachelor's"<br/>experienceDave &lt;- 10<br/><br/>wageDave &lt;- exp((ageDave * 0.1) + (case_when(educationDave == "High School" ~ 1,<br/>                                             educationDave == "Bachelor's" ~ 1.5,<br/>                                             TRUE ~ 2)) + (experienceDave * 0.05) + rnorm(n, mean = 0, sd = 0.5))<br/><br/>hist(wageDave, main="Wage Distribution for Dave", xlab="Wage")<br/><br/><br/><br/>## Generate test set<br/>ntest&lt;-1000<br/><br/># Generate age between 20 and 60<br/>agetest &lt;- round(runif(ntest, min = 20, max = 60))<br/><br/># Sample education level based on probabilities<br/>educationtest &lt;- sample(education_levels, ntest, replace = TRUE, prob = education_probs)<br/><br/># Simulate experience correlated with age with some random error<br/>experiencetest &lt;- agetest - 20 + round(rnorm(ntest, mean = 0, sd = 3))<br/><br/>## Generate ytest that we try to predict:<br/><br/>wagetest &lt;- exp((agetest * 0.1) + (case_when(educationtest == "High School" ~ 1,<br/>                                             educationtest == "Bachelor's" ~ 1.5,<br/>                                             TRUE ~ 2)) + (experiencetest * 0.05) + rnorm(ntest, mean = 0, sd = 0.5))<br/><br/><br/><br/><br/><br/>conditionalmeanest &lt;-<br/>  function(age, education, experience, N = 1000) {<br/>    mean(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    ))<br/>  }<br/><br/>conditionalmedianest &lt;-<br/>  function(age, education, experience, N = 1000) {<br/>    median(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    ))<br/>  }<br/><br/><br/>hist(wageDave, main="Wage Distribution for Dave", xlab="Wage")<br/>abline(v=conditionalmeanest(ageDave, educationDave, experienceDave), col="darkred", cex=1.2)<br/>abline(v=conditionalmedianest(ageDave, educationDave, experienceDave), col="darkblue", cex=1.2)<br/><br/><br/><br/>Xtest&lt;-data.frame(age=agetest, education=educationtest, experience=experiencetest)<br/><br/>meanest&lt;-sapply(1:nrow(Xtest), function(j)  conditionalmeanest(Xtest$age[j], Xtest$education[j], Xtest$experience[j])  )<br/>median&lt;-sapply(1:nrow(Xtest), function(j)  conditionalmedianest(Xtest$age[j], Xtest$education[j], Xtest$experience[j])  )<br/><br/><br/><br/>(MSE1&lt;-mean((meanest-wagetest)^2))<br/>(MSE2&lt;-mean((median-wagetest)^2))<br/><br/>MSE1 &lt; MSE2<br/>### Method 1 (the true mean estimator) is better than method 2!<br/><br/># but the MAE is actually worse of method 1!<br/>(MAE1&lt;-mean(abs(meanest-wagetest)) )<br/>(MAE2&lt;-mean( abs(median-wagetest)))<br/><br/>MAE1 &lt; MAE2<br/>### Method 2 (the true median estimator) is better than method 1!<br/><br/><br/><br/><br/><br/><br/><br/><br/>library(scoringutils)<br/><br/>## Define conditional quantile estimation<br/>conditionalquantileest &lt;-<br/>  function(probs, age, education, experience, N = 1000) {<br/>    quantile(exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5)<br/>    )<br/>    , probs =<br/>      probs)<br/>  }<br/><br/>## Define a very naive estimator that will still have the required coverage<br/>lowernaive &lt;- 0<br/>uppernaive &lt;- max(wage)<br/><br/># Define the quantile of interest<br/>alpha &lt;- 0.05<br/><br/>lower &lt;-<br/>  sapply(1:nrow(Xtest), function(j)<br/>    conditionalquantileest(alpha / 2, Xtest$age[j], Xtest$education[j], Xtest$experience[j]))<br/>upper &lt;-<br/>  sapply(1:nrow(Xtest), function(j)<br/>    conditionalquantileest(1 - alpha / 2, Xtest$age[j], Xtest$education[j], Xtest$experience[j]))<br/><br/>## Calculate the scores for both estimators<br/><br/># 1. Score the alpha/2 quantile estimate<br/>qs_lower &lt;- mean(quantile_score(wagetest,<br/>                                predictions = lower,<br/>                                quantiles = alpha / 2))<br/># 2. Score the alpha/2 quantile estimate<br/>qs_upper &lt;- mean(quantile_score(wagetest,<br/>                                predictions = upper,<br/>                                quantiles = 1 - alpha / 2))<br/><br/># 1. Score the alpha/2 quantile estimate<br/>qs_lowernaive &lt;- mean(quantile_score(wagetest,<br/>                                     predictions = rep(lowernaive, ntest),<br/>                                     quantiles = alpha / 2))<br/># 2. Score the alpha/2 quantile estimate<br/>qs_uppernaive &lt;- mean(quantile_score(wagetest,<br/>                                     predictions = rep(uppernaive, ntest),<br/>                                     quantiles = 1 - alpha / 2))<br/><br/># Construct the interval score by taking the average<br/>(interval_score &lt;- (qs_lower + qs_upper) / 2)<br/># Score of the ideal estimator: 187.8337<br/><br/># Construct the interval score by taking the average<br/>(interval_scorenaive &lt;- (qs_lowernaive + qs_uppernaive) / 2)<br/># Score of the naive estimator: 1451.464<br/><br/><br/>library(scoringRules)<br/><br/>## Ideal "estimate": Simply sample from the true conditional distribution <br/>## P(Y | X=x) for each sample point x<br/>distributionestimate &lt;-<br/>  function(age, education, experience, N = 100) {<br/>    exp((age * 0.1) + (<br/>      case_when(<br/>        education == "High School" ~ 1,<br/>        education == "Bachelor's" ~ 1.5,<br/>        TRUE ~ 2<br/>      )<br/>    ) + (experience * 0.05) + rnorm(N, mean = 0, sd = 0.5))<br/>  }<br/><br/>## Naive Estimate: Only sample from the error distribution, without including the <br/>## information of each person.<br/>distributionestimatenaive &lt;-<br/>  function(age, education, experience, N = 100) {<br/>    exp(rnorm(N, mean = 0, sd = 0.5))<br/>  }<br/><br/>scoretrue &lt;- mean(sapply(1:nrow(Xtest), function(j)  {<br/>  wageest &lt;-<br/>    distributionestimate(Xtest$age[j], Xtest$education[j], Xtest$experience[j])<br/>  return(scoringRules::es_sample(y = wagetest[j], dat = matrix(wageest, nrow=1)))<br/>}))<br/><br/>scorenaive &lt;- mean(sapply(1:nrow(Xtest), function(j)  {<br/>  wageest &lt;-<br/>    distributionestimatenaive(Xtest$age[j], Xtest$education[j], Xtest$experience[j])<br/>  return(scoringRules::es_sample(y = wagetest[j], dat = matrix(wageest, nrow=1)))<br/>}))<br/><br/>## scoretrue: 761.026<br/>## scorenaive: 2624.713<br/><br/></span></pre></div></div></div></div>    
</body>
</html>