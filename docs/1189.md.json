["```py\nimport catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport datetime as dt\n\nfrom dataclasses import dataclass\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\n%load_ext watermark\n%watermark --packages catboost,pandas,sklearn,numpy,google.cloud.bigquery \n```", "```py\ncatboost             : 1.0.4\npandas               : 1.4.2\nnumpy                : 1.22.4\ngoogle.cloud.bigquery: 3.2.0\n```", "```py\nquery = \"\"\"\n    SELECT \n      transactions.user_id,\n      products.brand,\n      products.category,\n      products.department,\n      products.retail_price,\n      users.gender,\n      users.age,\n      users.created_at,\n      users.country,\n      users.city,\n      transactions.created_at\n    FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions\n    LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users\n      ON transactions.user_id = users.id\n    LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products\n      ON transactions.product_id = products.id\n    WHERE status <> 'Cancelled'\n\"\"\"\n\nclient = bigquery.Client()\ndf = client.query(query).to_dataframe() \n```", "```py\n# Compute recurrent customers\nrecurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n# Merge with dataset and filter those with more than 1 purchase\ndf = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\ndf = df.query('n_purchases > 1')\n\n# Fill missing values\ndf.fillna('NA', inplace=True)\n\ntarget_brands = [\n    'Allegra K', \n    'Calvin Klein', \n    'Carhartt', \n    'Hanes', \n    'Volcom', \n    'Nautica', \n    'Quiksilver', \n    'Diesel',\n    'Dockers', \n    'Hurley'\n]\n\naggregation_columns = ['brand', 'department', 'category']\n\n# Group purchases by user chronologically\ndf_agg = (df.sort_values('created_at')\n          .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n          .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n         )\n\n# Create the target\ndf_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\ndf_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\ndf_agg['age'] = df_agg['age'].astype(float)\n\n# Remove last item of sequence features to avoid target leakage :\nfor col in aggregation_columns:\n    df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n```", "```py\n# Remove unecessary features\n\ndf_agg.drop('last_purchase_category', axis=1, inplace=True)\ndf_agg.drop('last_purchase_brand', axis=1, inplace=True)\ndf_agg.drop('user_id', axis=1, inplace=True)\n\n# Split the data into train and eval\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\")\n\ndf_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\nprint(f\"{len(df_train)} samples in train\") \n# 30950 samples in train\n\ndf_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\nprint(f\"{len(df_val)} samples in val\")\nprint(f\"{len(df_test)} samples in test\")\n# 3869 samples in train\n# 3869 samples in test\n```", "```py\nX_train, y_train = df_train.iloc[:, :-1], df_train['target']\nX_val, y_val = df_val.iloc[:, :-1], df_val['target']\nX_test, y_test = df_test.iloc[:, :-1], df_test['target']\n```", "```py\n# Define features\nfeatures = {\n    'numerical': ['retail_price', 'age'],\n    'static': ['gender', 'country', 'city'],\n    'dynamic': ['brand', 'department', 'category']\n}\n\n# Build CatBoost \"pools\", which are datasets\ntrain_pool = cb.Pool(\n    X_train,\n    y_train,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\nvalidation_pool = cb.Pool(\n    X_val,\n    y_val,\n    cat_features=features.get(\"static\"),\n    text_features=features.get(\"dynamic\"),\n)\n\n# Specify text processing options to handle our text features\ntext_processing_options = {\n    \"tokenizers\": [\n        {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n    ],\n    \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n    \"feature_processing\": {\n        \"default\": [\n            {\n                \"dictionaries_names\": [\"Word\"],\n                \"feature_calcers\": [\"BoW\"],\n                \"tokenizers_names\": [\"SemiColon\"],\n            }\n        ],\n    },\n}\n```", "```py\n# Train the model\nmodel = cb.CatBoostClassifier(\n    iterations=200,\n    loss_function=\"Logloss\",\n    random_state=42,\n    verbose=1,\n    auto_class_weights=\"SqrtBalanced\",\n    use_best_model=True,\n    text_processing=text_processing_options,\n    eval_metric='AUC'\n)\n\nmodel.fit(\n    train_pool, \n    eval_set=validation_pool, \n    verbose=10\n)\n```", "```py\nfrom sklearn.metrics import roc_auc_score\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=model.predict(X_train)):.2f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=model.predict(X_val)):.2f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=model.predict(X_test)):.2f}\")\n```", "```py\nROC-AUC for train set      : 0.612\nROC-AUC for validation set : 0.586\nROC-AUC for test set       : 0.622\n```", "```py\n# random predictions\n\nprint(f\"ROC-AUC for train set      : {roc_auc_score(y_true=y_train, y_score=np.random.rand(len(y_train))):.3f}\")\nprint(f\"ROC-AUC for validation set : {roc_auc_score(y_true=y_val, y_score=np.random.rand(len(y_val))):.3f}\")\nprint(f\"ROC-AUC for test set       : {roc_auc_score(y_true=y_test, y_score=np.random.rand(len(y_test))):.3f}\")\n```", "```py\nROC-AUC for train set      : 0.501\nROC-AUC for validation set : 0.499\nROC-AUC for test set       : 0.501\n```", "```py\n# src/preprocess.py\n\nfrom sklearn.model_selection import train_test_split\nfrom google.cloud import bigquery\n\ndef create_dataset_from_bq():\n    query = \"\"\"\n        SELECT \n          transactions.user_id,\n          products.brand,\n          products.category,\n          products.department,\n          products.retail_price,\n          users.gender,\n          users.age,\n          users.created_at,\n          users.country,\n          users.city,\n          transactions.created_at\n        FROM `bigquery-public-data.thelook_ecommerce.order_items` as transactions\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.users` as users\n          ON transactions.user_id = users.id\n        LEFT JOIN `bigquery-public-data.thelook_ecommerce.products` as products\n          ON transactions.product_id = products.id\n        WHERE status <> 'Cancelled'\n    \"\"\"\n    client = bigquery.Client(project='<replace_with_your_project_id>')\n    df = client.query(query).to_dataframe()\n    print(f\"{len(df)} rows loaded.\")\n\n    # Compute recurrent customers\n    recurrent_customers = df.groupby('user_id')['created_at'].count().to_frame(\"n_purchases\")\n\n    # Merge with dataset and filter those with more than 1 purchase\n    df = df.merge(recurrent_customers, left_on='user_id', right_index=True, how='inner')\n    df = df.query('n_purchases > 1')\n\n    # Fill missing value\n    df.fillna('NA', inplace=True)\n\n    target_brands = [\n        'Allegra K', \n        'Calvin Klein', \n        'Carhartt', \n        'Hanes', \n        'Volcom', \n        'Nautica', \n        'Quiksilver', \n        'Diesel',\n        'Dockers', \n        'Hurley'\n    ]\n\n    aggregation_columns = ['brand', 'department', 'category']\n\n    # Group purchases by user chronologically\n    df_agg = (df.sort_values('created_at')\n              .groupby(['user_id', 'gender', 'country', 'city', 'age'], as_index=False)[['brand', 'department', 'category']]\n              .agg({k: \";\".join for k in ['brand', 'department', 'category']})\n             )\n\n    # Create the target\n    df_agg['last_purchase_brand'] = df_agg['brand'].apply(lambda x: x.split(\";\")[-1])\n    df_agg['target'] = df_agg['last_purchase_brand'].isin(target_brands)*1\n\n    df_agg['age'] = df_agg['age'].astype(float)\n\n    # Remove last item of sequence features to avoid target leakage :\n    for col in aggregation_columns:\n        df_agg[col] = df_agg[col].apply(lambda x: \";\".join(x.split(\";\")[:-1]))\n\n    df_agg.drop('last_purchase_category', axis=1, inplace=True)\n    df_agg.drop('last_purchase_brand', axis=1, inplace=True)\n    df_agg.drop('user_id', axis=1, inplace=True)\n    return df_agg\n\ndef make_data_splits(df_agg):\n\n    df_train, df_val = train_test_split(df_agg, stratify=df_agg['target'], test_size=0.2)\n    print(f\"{len(df_train)} samples in train\")\n\n    df_val, df_test = train_test_split(df_val, stratify=df_val['target'], test_size=0.5)\n    print(f\"{len(df_val)} samples in val\")\n    print(f\"{len(df_test)} samples in test\")\n\n    return df_train, df_val, df_test\n```", "```py\n# src/train.py\n\nimport catboost as cb\nimport pandas as pd\nimport sklearn as sk\nimport numpy as np\nimport argparse\n\nfrom sklearn.metrics import roc_auc_score\n\ndef train_and_evaluate(\n        train_path: str,\n        validation_path: str,\n        test_path: str\n    ):\n    df_train = pd.read_csv(train_path)\n    df_val = pd.read_csv(validation_path)\n    df_test = pd.read_csv(test_path)\n\n    df_train.fillna('NA', inplace=True)\n    df_val.fillna('NA', inplace=True)\n    df_test.fillna('NA', inplace=True)\n\n    X_train, y_train = df_train.iloc[:, :-1], df_train['target']\n    X_val, y_val = df_val.iloc[:, :-1], df_val['target']\n    X_test, y_test = df_test.iloc[:, :-1], df_test['target']\n\n    features = {\n        'numerical': ['retail_price', 'age'],\n        'static': ['gender', 'country', 'city'],\n        'dynamic': ['brand', 'department', 'category']\n    }\n\n    train_pool = cb.Pool(\n        X_train,\n        y_train,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    validation_pool = cb.Pool(\n        X_val,\n        y_val,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    test_pool = cb.Pool(\n        X_test,\n        y_test,\n        cat_features=features.get(\"static\"),\n        text_features=features.get(\"dynamic\"),\n    )\n\n    params = CatBoostParams()\n\n    text_processing_options = {\n        \"tokenizers\": [\n            {\"tokenizer_id\": \"SemiColon\", \"delimiter\": \";\", \"lowercasing\": \"false\"}\n        ],\n        \"dictionaries\": [{\"dictionary_id\": \"Word\", \"gram_order\": \"1\"}],\n        \"feature_processing\": {\n            \"default\": [\n                {\n                    \"dictionaries_names\": [\"Word\"],\n                    \"feature_calcers\": [\"BoW\"],\n                    \"tokenizers_names\": [\"SemiColon\"],\n                }\n            ],\n        },\n    }\n\n    # Train the model\n    model = cb.CatBoostClassifier(\n        iterations=200,\n        loss_function=\"Logloss\",\n        random_state=42,\n        verbose=1,\n        auto_class_weights=\"SqrtBalanced\",\n        use_best_model=True,\n        text_processing=text_processing_options,\n        eval_metric='AUC'\n    )\n\n    model.fit(\n        train_pool, \n        eval_set=validation_pool, \n        verbose=10\n    )\n\n    roc_train = roc_auc_score(y_true=y_train, y_score=model.predict(X_train))\n    roc_eval  = roc_auc_score(y_true=y_val, y_score=model.predict(X_val))\n    roc_test  = roc_auc_score(y_true=y_test, y_score=model.predict(X_test))\n    print(f\"ROC-AUC for train set      : {roc_train:.2f}\")\n    print(f\"ROC-AUC for validation set : {roc_eval:.2f}\")\n    print(f\"ROC-AUC for test.      set : {roc_test:.2f}\")\n\n    return {\"model\": model, \"scores\": {\"train\": roc_train, \"eval\": roc_eval, \"test\": roc_test}}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--train-path\", type=str)\n    parser.add_argument(\"--validation-path\", type=str)\n    parser.add_argument(\"--test-path\", type=str)\n    parser.add_argument(\"--output-dir\", type=str)\n    args, _ = parser.parse_known_args()\n    _ = train_and_evaluate(\n        args.train_path,\n        args.validation_path,\n        args.test_path)\n```", "```py\n$ python train.py --train-path xxx --validation-path yyy etc.\n```", "```py\n# Dockerfile\n\nFROM python:3.8-slim\nWORKDIR /\nCOPY requirements.txt /requirements.txt\nCOPY src /src\nRUN pip install --upgrade pip && pip install -r requirements.txt\nENTRYPOINT [ \"bash\" ]\n```", "```py\nPROJECT_ID = ...\nIMAGE_NAME=f'thelook_training_demo'\nIMAGE_TAG='latest'\nIMAGE_URI='eu.gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, IMAGE_TAG)\n\n!gcloud builds submit --tag $IMAGE_URI .\n```", "```py\n# IMPORT REQUIRED LIBRARIES\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import (Artifact,\n                        Dataset,\n                        Input,\n                        Model,\n                        Output,\n                        Metrics,\n                        Markdown,\n                        HTML,\n                        component, \n                        OutputPath, \n                        InputPath)\nfrom kfp.v2 import compiler\nfrom google.cloud.aiplatform import pipeline_jobs\n\n%watermark --packages kfp,google.cloud.aiplatform\n```", "```py\nkfp                    : 2.7.0\ngoogle.cloud.aiplatform: 1.50.0\n```", "```py\n@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"get_data.yaml\"\n)\ndef create_dataset_from_bq(\n    output_dir: Output[Dataset],\n):\n\n    from src.preprocess import create_dataset_from_bq\n\n    df = create_dataset_from_bq()\n\n    df.to_csv(output_dir.path, index=False)\n```", "```py\n@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_test_split.yaml\",\n)\ndef make_data_splits(\n    dataset_full: Input[Dataset],\n    dataset_train: Output[Dataset],\n    dataset_val: Output[Dataset],\n    dataset_test: Output[Dataset]):\n\n    import pandas as pd\n    from src.preprocess import make_data_splits\n\n    df_agg = pd.read_csv(dataset_full.path)\n\n    df_agg.fillna('NA', inplace=True)\n\n    df_train, df_val, df_test = make_data_splits(df_agg)\n    print(f\"{len(df_train)} samples in train\")\n    print(f\"{len(df_val)} samples in train\")\n    print(f\"{len(df_test)} samples in test\")\n\n    df_train.to_csv(dataset_train.path, index=False)\n    df_val.to_csv(dataset_val.path, index=False)\n    df_test.to_csv(dataset_test.path, index=False)\n```", "```py\n@component(\n    base_image=BASE_IMAGE,\n    output_component_file=\"train_model.yaml\",\n)\ndef train_model(\n    dataset_train: Input[Dataset],\n    dataset_val: Input[Dataset],\n    dataset_test: Input[Dataset],\n    model: Output[Model]\n):\n\n    import json\n    from src.train import train_and_evaluate\n\n    outputs = train_and_evaluate(\n        dataset_train.path,\n        dataset_val.path,\n        dataset_test.path\n    )\n    cb_model = outputs['model']\n    scores = outputs['scores']\n\n    model.metadata[\"framework\"] = \"catboost\" \n    # Save the model as an artifact\n    with open(model.path, 'w') as f: \n        json.dump(scores, f)\n```", "```py\n@component(\n    base_image=\"python:3.9\",\n    output_component_file=\"compute_metrics.yaml\",\n)\ndef compute_metrics(\n    model: Input[Model],\n    train_metric: Output[Metrics],\n    val_metric: Output[Metrics],\n    test_metric: Output[Metrics]\n):\n\n    import json\n\n    file_name = model.path\n    with open(file_name, 'r') as file:  \n        model_metrics = json.load(file)\n\n    train_metric.log_metric('train_auc', model_metrics['train'])\n    val_metric.log_metric('val_auc', model_metrics['eval'])\n    test_metric.log_metric('test_auc', model_metrics['test'])\n```", "```py\n# USE TIMESTAMP TO DEFINE UNIQUE PIPELINE NAMES\nTIMESTAMP = dt.datetime.now().strftime(\"%Y%m%d%H%M%S\")\nDISPLAY_NAME = 'pipeline-thelook-demo-{}'.format(TIMESTAMP)\nPIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\n\n# Define the pipeline. Notice how steps reuse outputs from previous steps\n@dsl.pipeline(\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline. Use to determine the pipeline Context.\n    name=\"pipeline-demo\"   \n)\n\ndef pipeline(\n    project: str = PROJECT_ID,\n    region: str = REGION, \n    display_name: str = DISPLAY_NAME\n):\n\n    load_data_op = create_dataset_from_bq()\n    train_test_split_op = make_data_splits(\n        dataset_full=load_data_op.outputs[\"output_dir\"]\n    )\n    train_model_op = train_model(\n        dataset_train=train_test_split_op.outputs[\"dataset_train\"], \n        dataset_val=train_test_split_op.outputs[\"dataset_val\"],\n        dataset_test=train_test_split_op.outputs[\"dataset_test\"],\n        )\n    model_evaluation_op = compute_metrics(\n        model=train_model_op.outputs[\"model\"]\n    )\n\n# Compile the pipeline as JSON\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path='thelook_pipeline.json'\n)\n\n# Start the pipeline\nstart_pipeline = pipeline_jobs.PipelineJob(\n    display_name=\"thelook-demo-pipeline\",\n    template_path=\"thelook_pipeline.json\",\n    enable_caching=False,\n    location=REGION,\n    project=PROJECT_ID\n)\n\n# Run the pipeline\nstart_pipeline.run(service_account=<your_service_account_here>)\n```"]