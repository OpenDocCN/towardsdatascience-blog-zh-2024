["```py\nmountains = np.load(os.path.join(figure_path, 'mountains.npy'))\n\nH = mountains.shape[0]\nW = mountains.shape[1]\nprint('Mountain at Dusk is H =', H, 'and W =', W, 'pixels.')\nprint('\\n')\n\nfig = plt.figure(figsize=(10,6))\nplt.imshow(mountains, cmap='Purples_r')\nplt.xticks(np.arange(-0.5, W+1, 10), labels=np.arange(0, W+1, 10))\nplt.yticks(np.arange(-0.5, H+1, 10), labels=np.arange(0, H+1, 10))\nplt.clim([0,1])\ncbar_ax = fig.add_axes([0.95, .11, 0.05, 0.77])\nplt.clim([0, 1])\nplt.colorbar(cax=cbar_ax);\n#plt.savefig(os.path.join(figure_path, 'mountains.png'), bbox_inches='tight')\n```", "```py\nMountain at Dusk is H = 60 and W = 100 pixels.\n```", "```py\ndef count_tokens(w, h, k, s, p):\n \"\"\" Function to count how many tokens are produced from a given soft split\n\n  Args:\n   w (int): starting width\n   h (int): starting height\n   k (int): kernel size\n   s (int): stride size\n   p (int): padding size\n\n  Returns:\n   new_w (int): number of tokens along the width\n   new_h (int): number of tokens along the height\n   total (int): total number of tokens created\n \"\"\"\n\n new_w = int(math.floor(((w + 2*p - (k-1) -1)/s)+1))\n new_h = int(math.floor(((h + 2*p - (k-1) -1)/s)+1))\n total = new_w * new_h\n\n return new_w, new_h, total\n```", "```py\nk = 20\ns = 10\np = 5\npadded_H = H + 2*p\npadded_W = W + 2*p\nprint('With padding, the image will be H =', padded_H, 'and W =', padded_W, 'pixels.\\n') \n\npatches_w, patches_h, total_patches = count_tokens(w=W, h=H, k=k, s=s, p=p)\nprint('There will be', total_patches, 'patches as a result of the soft split;')\nprint(patches_h, 'along the height and', patches_w, 'along the width.')\n```", "```py\nWith padding, the image will be H = 70 and W = 110 pixels.\n\nThere will be 60 patches as a result of the soft split;\n6 along the height and 10 along the width.\n```", "```py\nmountains_w_padding = np.pad(mountains, pad_width = ((p, p), (p, p)), mode='constant', constant_values=0)\n\nleft_x = np.tile(np.arange(-0.5, padded_W-k+1, s), patches_h)\nright_x = np.tile(np.arange(k-0.5, padded_W+1, s), patches_h)\ntop_y = np.repeat(np.arange(-0.5, padded_H-k+1, s), patches_w)\nbottom_y = np.repeat(np.arange(k-0.5, padded_H+1, s), patches_w)\n\nframe_paths = []\n\nfor i in range(total_patches):\n    fig = plt.figure(figsize=(10,6))\n    plt.imshow(mountains_w_padding, cmap='Purples_r')\n    plt.clim([0,1])\n    plt.xticks(np.arange(-0.5, W+2*p+1, 10), labels=np.arange(0, W+2*p+1, 10))\n    plt.yticks(np.arange(-0.5, H+2*p+1, 10), labels=np.arange(0, H+2*p+1, 10))\n\n    plt.plot([left_x[i], left_x[i], right_x[i], right_x[i], left_x[i]], [top_y[i], bottom_y[i], bottom_y[i], top_y[i], top_y[i]], color='w', lw=3, ls='-')\n\n    for j in range(i):\n        plt.plot([left_x[j], left_x[j], right_x[j], right_x[j], left_x[j]], [top_y[j], bottom_y[j], bottom_y[j], top_y[j], top_y[j]], color='w', lw=2, ls=':', alpha=0.5)\n    save_path = os.path.join(figure_path, 'softsplit_gif', 'frame{:02d}'.format(i))+'.png'\n    frame_paths.append(save_path)\n    #fig.savefig(save_path, bbox_inches='tight')\n    plt.close()\n\nframes = []\nfor path in frame_paths:\n    frames.append(iio.imread(path))\n\n#iio.mimsave(os.path.join(figure_path, 'softsplit.gif'), frames, fps=2, loop=0)\n```", "```py\nprint('Each patch will make a token of length', str(k**2)+'.')\nprint('\\n')\n\npatch = mountains_w_padding[0:20, 0:20]\ntoken = patch.reshape(1, k**2,)\n\nfig = plt.figure(figsize=(10,1))\nplt.imshow(token, cmap='Purples_r', aspect=20)\nplt.clim([0, 1])\nplt.xticks(np.arange(-0.5, k**2+1, 50), labels=np.arange(0, k**2+1, 50))\nplt.yticks([]);\n#plt.savefig(os.path.join(figure_path, 'mountains_w_padding_token01.png'), bbox_inches='tight')\n```", "```py\nEach patch will make a token of length 400.\n```", "```py\nleft_x = np.tile(np.arange(0, padded_W-k+1, s), patches_h)\nright_x = np.tile(np.arange(k, padded_W+1, s), patches_h)\ntop_y = np.repeat(np.arange(0, padded_H-k+1, s), patches_w)\nbottom_y = np.repeat(np.arange(k, padded_H+1, s), patches_w)\n\ntokens = np.zeros((total_patches, k**2))\nfor i in range(total_patches):\n    patch = mountains_w_padding[top_y[i]:bottom_y[i], left_x[i]:right_x[i]]\n    tokens[i, :] = patch.reshape(1, k**2)\n\nfig = plt.figure(figsize=(10,6))\nplt.imshow(tokens, cmap='Purples_r', aspect=5)\nplt.clim([0, 1])\nplt.xticks(np.arange(-0.5, k**2+1, 50), labels=np.arange(0, k**2+1, 50))\nplt.yticks(np.arange(-0.5, total_patches+1, 10), labels=np.arange(0, total_patches+1, 10))\nplt.xlabel('Length of Tokens')\nplt.ylabel('Number of Tokens')\nplt.clim([0,1])\ncbar_ax = fig.add_axes([0.85, .11, 0.05, 0.77])\nplt.clim([0, 1])\nplt.colorbar(cax=cbar_ax);\n#plt.savefig(os.path.join(figure_path, 'mountains_w_padding_tokens_matrix.png'), bbox_inches='tight')\n```", "```py\nclass TokenTransformer(nn.Module):\n\n    def __init__(self,\n       dim: int,\n       chan: int,\n       num_heads: int,\n       hidden_chan_mul: float=1.,\n       qkv_bias: bool=False,\n       qk_scale: NoneFloat=None,\n       act_layer=nn.GELU,\n       norm_layer=nn.LayerNorm):\n\n        \"\"\" Token Transformer Module\n\n            Args:\n                dim (int): size of a single token\n                chan (int): resulting size of a single token \n                num_heads (int): number of attention heads in MSA \n                hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet module\n                qkv_bias (bool): determines if the attention qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation in the NeuralNet module\n                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Layers\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(dim,\n              chan=chan,\n              num_heads=num_heads,\n              qkv_bias=qkv_bias,\n              qk_scale=qk_scale)\n        self.norm2 = norm_layer(chan)\n        self.neuralnet = NeuralNet(in_chan=chan,\n              hidden_chan=int(chan*hidden_chan_mul),\n              out_chan=chan,\n              act_layer=act_layer)\n\n    def forward(self, x):\n        x = self.attn(self.norm1(x))\n        x = x + self.neuralnet(self.norm2(x))\n        return x\n```", "```py\n# Define an Input\ntoken_len = 7*7\nchannels = 64\nnum_tokens = 100\nbatch = 13\nheads = 4\nx = torch.rand(batch, num_tokens, token_len)\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n\n# Define the Module\nTT = TokenTransformer(dim=token_len, \n                      chan=channels, \n                      num_heads=heads, \n                      hidden_chan_mul=1.5, \n                      qkv_bias=False, \n                      qk_scale=None, \n                      act_layer=nn.GELU, \n                      norm_layer=nn.LayerNorm)\nTT.eval();\n```", "```py\nInput dimensions are\n    batchsize: 13 \n    number of tokens: 100 \n    token size: 49\n```", "```py\nx = TT.norm1(x)\nprint('After norm, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\nx = TT.attn(x)\nprint('After attention, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n```", "```py\nAfter norm, dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 49\nAfter attention, dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 64\n```", "```py\ny = TT.norm2(x)\nprint('After norm, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\ny = TT.neuralnet(y)\nprint('After neural net, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\ny = y + x\nprint('After split connection, dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken size:', x.shape[2])\n```", "```py\nAfter norm, dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 64\nAfter neural net, dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 64\nAfter split connection, dimensions are\n   batchsize: 13 \n   number of tokens: 100 \n   token size: 64\n```", "```py\nclass NeuralNet(nn.Module):\n    def __init__(self,\n       in_chan: int,\n       hidden_chan: NoneFloat=None,\n       out_chan: NoneFloat=None,\n       act_layer = nn.GELU):\n        \"\"\" Neural Network Module\n\n            Args:\n                in_chan (int): number of channels (features) at input\n                hidden_chan (NoneFloat): number of channels (features) in the hidden layer;\n                                        if None, number of channels in hidden layer is the same as the number of input channels\n                out_chan (NoneFloat): number of channels (features) at output;\n                                        if None, number of output channels is same as the number of input channels\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Number of Channels\n        hidden_chan = hidden_chan or in_chan\n        out_chan = out_chan or in_chan\n\n        ## Define Layers\n        self.fc1 = nn.Linear(in_chan, hidden_chan)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_chan, out_chan)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n```", "```py\nW, H, _ = count_tokens(w, h, k, s, p)\nx = x.transpose(1,2).reshape(B, C, H, W)\n```", "```py\nclass Tokens2Token(nn.Module):\n  def __init__(self, \n    img_size: tuple[int, int, int]=(1, 1000, 300), \n    token_chan:  int=64,\n    token_len: int=768,):\n\n    \"\"\" Tokens-to-Token Module\n\n    Args:\n    img_size (tuple[int, int, int]): size of input (channels, height, width)\n    token_chan (int): number of token channels inside the TokenTransformers\n    token_len (int): desired length of an output token\n    \"\"\"\n\n    super().__init__()\n\n    ## Seperating Image Size\n    C, H, W = img_size\n    self.token_chan = token_chan\n    ## Dimensions: (channels, height, width)\n\n    ## Define the Soft Split Layers\n    self.soft_split0 = nn.Unfold(kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n    self.soft_split1 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    self.soft_split2 = nn.Unfold(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n    ## Determining Number of Output Tokens\n    W, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)\n    W, H, _ = count_tokens(w=W, h=H, k=3, s=2, p=1)\n    _, _, T = count_tokens(w=W, h=H, k=3, s=2, p=1)\n    self.num_tokens = T\n\n    ## Define the Transformer Layers\n    self.transformer1 = TokenTransformer(dim= C * 7 * 7, \n    chan=token_chan,\n    num_heads=1,\n    hidden_chan_mul=1.0)\n    self.transformer2 = TokenTransformer(dim=token_chan * 3 * 3, \n    chan=token_chan, \n    num_heads=1, \n    hidden_chan_mul=1.0)\n\n    ## Define the Projection Layer\n    self.project = nn.Linear(token_chan * 3 * 3, token_len)\n\n  def forward(self, x):\n\n    B, C, H, W = x.shape\n    ## Dimensions: (batch, channels, height, width)\n\n    ## Initial Soft Split\n    x = self.soft_split0(x).transpose(1, 2)\n\n    ## Token Transformer 1\n    x = self.transformer1(x)\n\n    ## Reconstruct 2D Image\n    W, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)\n    x = x.transpose(1,2).reshape(B, self.token_chan, H, W)\n\n    ## Soft Split 1\n    x = self.soft_split1(x).transpose(1, 2)\n\n    ## Token Transformer 2\n    x = self.transformer2(x)\n\n    ## Reconstruct 2D Image\n    W, H, _ = count_tokens(w=W, h=H, k=3, s=2, p=1)\n    x = x.transpose(1,2).reshape(B, self.token_chan, H, W)\n\n    ## Soft Split 2\n    x = self.soft_split2(x).transpose(1, 2)\n\n    ## Project Tokens to desired length\n    x = self.project(x)\n\n    return x\n```", "```py\n# Define an Input\nH = 400\nW = 100\nchannels = 64\nbatch = 13\nx = torch.rand(batch, 1, H, W)\nprint('Input dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of input channels:', x.shape[1], '\\n\\timage size:', (x.shape[2], x.shape[3]))\n\n# Define the Module\nT2T = Tokens2Token(img_size=(1, H, W), token_chan=64, token_len=768)\nT2T.eval();\n```", "```py\nInput dimensions are\n   batchsize: 13 \n   number of input channels: 1 \n   image size: (400, 100)\n```", "```py\n# Count Tokens\nk = 7\ns = 4\np = 2\n_, _, T = count_tokens(w=W, h=H, k=k, s=s, p=p)\nprint('There should be', T, 'tokens after the soft split.')\nprint('They should be of length', k, '*', k, '* 1 =', k*k*1)\n\n# Perform the Soft Split\nx = T2T.soft_split0(x)\nprint('Dimensions after soft split are\\n\\tbatchsize:', x.shape[0], '\\n\\ttoken length:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2])\nx = x.transpose(1, 2)\n```", "```py\nThere should be 2500 tokens after the soft split.\nThey should be of length 7 * 7 * 1 = 49\nDimensions after soft split are\n   batchsize: 13 \n   token length: 49 \n   number of tokens: 2500\n```", "```py\nx = T2T.transformer1(x)\nprint('Dimensions after transformer are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nDimensions after transformer are\n   batchsize: 13 \n   number of tokens: 2500 \n   token length: 64\n```", "```py\nW, H, _ = count_tokens(w=W, h=H, k=7, s=4, p=2)\nprint('The reconstructed image should have shape', (H, W))\n\nx = x.transpose(1,2).reshape(B, T2T.token_chan, H, W)\nprint('Dimensions of reconstructed image are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of input channels:', x.shape[1], '\\n\\timage size:', (x.shape[2], x.shape[3]))\n```", "```py\nThe reconstructed image should have shape (100, 25)\nDimensions of reconstructed image are\n   batchsize: 13 \n   number of input channels: 64 \n   image size: (100, 25)\n```", "```py\n# Soft Split\nk = 3\ns = 2\np = 1\n_, _, T = count_tokens(w=W, h=H, k=k, s=s, p=p)\nprint('There should be', T, 'tokens after the soft split.')\nprint('They should be of length', k, '*', k, '*', T2T.token_chan, '=', k*k*T2T.token_chan)\nx = T2T.soft_split1(x)\nprint('Dimensions after soft split are\\n\\tbatchsize:', x.shape[0], '\\n\\ttoken length:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2])\nx = x.transpose(1, 2)\n\n# Token Transformer\nx = T2T.transformer2(x)\nprint('Dimensions after transformer are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n\n# Reconstruction\nW, H, _ = count_tokens(w=W, h=H, k=k, s=s, p=p)\nprint('The reconstructed image should have shape', (H, W))\nx = x.transpose(1,2).reshape(batch, T2T.token_chan, H, W)\nprint('Dimensions of reconstructed image are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of input channels:', x.shape[1], '\\n\\timage size:', (x.shape[2], x.shape[3]))\n```", "```py\nThere should be 650 tokens after the soft split.\nThey should be of length 3 * 3 * 64 = 576\nDimensions after soft split are\n   batchsize: 13 \n   token length: 576 \n   number of tokens: 650\nDimensions after transformer are\n   batchsize: 13 \n   number of tokens: 650 \n   token length: 64\nThe reconstructed image should have shape (50, 13)\nDimensions of reconstructed image are\n   batchsize: 13 \n   number of input channels: 64 \n   image size: (50, 13)\n```", "```py\n# Soft Split\n_, _, T = count_tokens(w=W, h=H, k=3, s=2, p=1)\nprint('There should be', T, 'tokens after the soft split.')\nprint('They should be of length 3*3*64=', 3*3*64)\nx = T2T.soft_split2(x)\nprint('Dimensions after soft split are\\n\\tbatchsize:', x.shape[0], '\\n\\ttoken length:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2])\nx = x.transpose(1, 2)\n```", "```py\nThere should be 175 tokens after the soft split.\nThey should be of length 3 * 3 * 64 = 576\nDimensions after soft split are\n   batchsize: 13 \n   token length: 576 \n   number of tokens: 175\n```", "```py\nx = T2T.project(x)\nprint('Output dimensions are\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\ttoken length:', x.shape[2])\n```", "```py\nOutput dimensions are\n   batchsize: 13 \n   number of tokens: 175 \n   token length: 768\n```", "```py\nclass ViT_Backbone(nn.Module):\n    def __init__(self,\n                preds: int=1,\n                token_len: int=768,\n                num_heads: int=1,\n                Encoding_hidden_chan_mul: float=4.,\n                depth: int=12,\n                qkv_bias=False,\n                qk_scale=None,\n                act_layer=nn.GELU,\n                norm_layer=nn.LayerNorm):\n\n        \"\"\" VisTransformer Backbone\n            Args:\n                preds (int): number of predictions to output\n                token_len (int): length of a token\n                num_heads(int): number of attention heads in MSA\n                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module\n                depth (int): number of encoding blocks in the model\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                 if None, queries and keys are scaled by ``head_dim ** -0.5``\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n        \"\"\"\n\n        super().__init__()\n\n        ## Defining Parameters\n        self.num_heads = num_heads\n        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul\n        self.depth = depth\n\n        ## Defining Token Processing Components\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.token_len))\n        self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(num_tokens=self.num_tokens+1, token_len=self.token_len), requires_grad=False)\n\n        ## Defining Encoding blocks\n        self.blocks = nn.ModuleList([Encoding(dim = self.token_len, \n                                               num_heads = self.num_heads,\n                                               hidden_chan_mul = self.Encoding_hidden_chan_mul,\n                                               qkv_bias = qkv_bias,\n                                               qk_scale = qk_scale,\n                                               act_layer = act_layer,\n                                               norm_layer = norm_layer)\n             for i in range(self.depth)])\n\n        ## Defining Prediction Processing\n        self.norm = norm_layer(self.token_len)\n        self.head = nn.Linear(self.token_len, preds)\n\n        ## Make the class token sampled from a truncated normal distrobution \n        timm.layers.trunc_normal_(self.cls_token, std=.02)\n\n    def forward(self, x):\n        ## Assumes x is already tokenized\n\n        ## Get Batch Size\n        B = x.shape[0]\n        ## Concatenate Class Token\n        x = torch.cat((self.cls_token.expand(B, -1, -1), x), dim=1)\n        ## Add Positional Embedding\n        x = x + self.pos_embed\n        ## Run Through Encoding Blocks\n        for blk in self.blocks:\n            x = blk(x)\n        ## Take Norm\n        x = self.norm(x)\n        ## Make Prediction on Class Token\n        x = self.head(x[:, 0])\n        return x\n```", "```py\nclass T2T_ViT(nn.Module):\n    def __init__(self, \n                img_size: tuple[int, int, int]=(1, 1700, 500),\n                softsplit_kernels: tuple[int, int, int]=(31, 3, 3),\n                preds: int=1,\n                token_len: int=768,\n                token_chan:  int=64,\n                num_heads: int=1,\n                T2T_hidden_chan_mul: float=1.,\n                Encoding_hidden_chan_mul: float=4.,\n                depth: int=12,\n                qkv_bias=False,\n                qk_scale=None,\n                act_layer=nn.GELU,\n                norm_layer=nn.LayerNorm):\n\n        \"\"\" Tokens-to-Token VisTransformer Model\n\n            Args:\n                img_size (tuple[int, int, int]): size of input (channels, height, width)\n                softsplit_kernels (tuple[int int, int]): size of the square kernel for each of the soft split layers, sequentially\n                preds (int): number of predictions to output\n                token_len (int): desired length of an output token\n                token_chan (int): number of token channels inside the TokenTransformers\n                num_heads(int): number of attention heads in MSA (only works if =1)\n                T2T_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Tokens-to-Token (T2T) Module\n                Encoding_hidden_chan_mul (float): multiplier to determine the number of hidden channels (features) in the NeuralNet component of the Encoding Module\n                depth (int): number of encoding blocks in the model\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n                act_layer(nn.modules.activation): torch neural network layer class to use as activation\n                norm_layer(nn.modules.normalization): torch neural network layer class to use as normalization\n        \"\"\"\n\n        super().__init__()\n\n        ## Defining Parameters\n        self.img_size = img_size\n        C, H, W = self.img_size\n        self.softsplit_kernels = softsplit_kernels\n        self.token_len = token_len\n        self.token_chan = token_chan\n        self.num_heads = num_heads\n        self.T2T_hidden_chan_mul = T2T_hidden_chan_mul\n        self.Encoding_hidden_chan_mul = Encoding_hidden_chan_mul\n        self.depth = depth\n\n        ## Defining Tokens-to-Token Module\n        self.tokens_to_token = Tokens2Token(img_size = self.img_size, \n                                            softsplit_kernels = self.softsplit_kernels,\n                                            num_heads = self.num_heads,\n              token_chan = self.token_chan,\n              token_len = self.token_len,\n              hidden_chan_mul = self.T2T_hidden_chan_mul,\n              qkv_bias = qkv_bias,\n              qk_scale = qk_scale,\n              act_layer = act_layer,\n              norm_layer = norm_layer)\n        self.num_tokens = self.tokens_to_token.num_tokens\n\n        ## Defining Token Processing Components\n        self.vit_backbone = ViT_Backbone(preds = preds,\n          token_len = self.token_len,\n          num_heads = self.num_heads,\n          Encoding_hidden_chan_mul = self.Encoding_hidden_chan_mul,\n          depth = self.depth,\n          qkv_bias = qkv_bias,\n             qk_scale = qk_scale,\n             act_layer = act_layer,\n             norm_layer = norm_layer)\n\n        ## Initialize the Weights\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        \"\"\" Initialize the weights of the linear layers & the layernorms\n        \"\"\"\n        ## For Linear Layers\n        if isinstance(m, nn.Linear):\n            ## Weights are initialized from a truncated normal distrobution\n            timmm.trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                ## If bias is present, bias is initialized at zero\n                nn.init.constant_(m.bias, 0)\n        ## For Layernorm Layers\n        elif isinstance(m, nn.LayerNorm):\n            ## Weights are initialized at one\n            nn.init.constant_(m.weight, 1.0)\n            ## Bias is initialized at zero\n            nn.init.constant_(m.bias, 0)\n\n    @torch.jit.ignore ##Tell pytorch to not compile as TorchScript\n    def no_weight_decay(self):\n        \"\"\" Used in Optimizer to ignore weight decay in the class token\n        \"\"\"\n        return {'cls_token'}\n\n    def forward(self, x):\n        x = self.tokens_to_token(x)\n        x = self.vit_backbone(x)\n        return x\n```"]