# 使用 Java 和 Python 构建你自己的类似 ChatGPT 的聊天机器人

> 原文：[https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30](https://towardsdatascience.com/build-your-own-chatgpt-like-chatbot-with-java-and-python-5def2c4852c3?source=collection_archive---------1-----------------------#2024-05-30)

## 从零开始创建自定义的 LLM 推理基础设施

[](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Daniel García Solla](../Images/b6e7bc9fdfdfcda7875215b1e0264d9e.png)](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------) [Daniel García Solla](https://cardstdani.medium.com/?source=post_page---byline--5def2c4852c3--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--5def2c4852c3--------------------------------) ·阅读时间 25 分钟·2024年5月30日

--

![](../Images/04f5c0784845e51a157b80a2649323bb.png)

作者图片

## 引言

近年来，**[大型语言模型（LLMs）](https://aws.amazon.com/what-is/large-language-model/)** 已成为一项颠覆性技术，彻底改变了我们与机器互动的方式。这些模型以 OpenAI 的 GPT 系列为代表，举例如 GPT-3.5 或 GPT-4，它们可以接受一段输入文本并生成连贯的、符合上下文的、听起来像人类的回复文本。因此，它们的应用领域广泛，涵盖了客户服务、内容创作、语言翻译或代码生成等多个领域。然而，这些能力的核心是先进的机器学习/统计技术，包括用于提升自然语言理解过程的注意力机制、用于大规模提供基础模型的迁移学习、数据增强，甚至是 [**基于人类反馈的强化学习**](https://huggingface.co/blog/rlhf)，这些技术使得系统能够在推理过程中不断扩展训练并提升性能。

作为人工智能的一个子集，机器学习负责处理数据集，识别模式并开发准确表示数据特性的模型。这种方法生成了有价值的知识，并解锁了各种任务，例如内容生成，这为[**生成式人工智能**](https://en.wikipedia.org/wiki/Generative_artificial_intelligence)领域提供了基础，推动了大语言模型的发展。值得强调的是，这个领域不仅仅专注于自然语言，还包括任何可能被生成的内容。从音频，拥有生成声音、语音或音乐的模型；到视频，最新的模型如OpenAI的[**SORA**](https://openai.com/index/sora/)；再到图像，以及从文本序列生成图像、编辑和风格迁移。后者的数据格式尤其有价值，因为通过使用多模态集成和图像/文本嵌入技术，可以有效地展示通过自然语言进行知识表示的潜力。

然而，创建和维护执行此类操作的模型，尤其是在大规模的情况下，并不是一项容易的工作。主要原因之一是数据，因为它是构建良好运行模型的关键要素。也就是说，使用结构优化的架构和高质量的数据训练模型将产生有价值的结果。相反，如果提供的数据质量差，模型将生成误导性的输出。因此，在创建数据集时，它应该包含适量的数据，以适应特定模型架构的需求。这个要求使得数据处理和质量验证变得更加复杂，此外，如果数据是通过自动化或抓取方式收集的，还必须考虑潜在的法律和隐私问题。

另一个原因在于硬件。现代部署的模型需要同时处理大量来自多个用户的数据，不仅体积庞大，还需要大量的计算资源来执行推理任务并为客户提供高质量的服务。这在经济成本上同样表现为巨大的开销。一方面，搭建配备正确硬件的服务器和数据中心非常昂贵，考虑到为了提供可靠的服务，需要使用**GPU**、[**TPU**](https://cloud.google.com/tpu/docs/intro-to-tpu)、[**DPU**](https://blogs.nvidia.com/blog/whats-a-dpu-data-processing-unit/)以及精心挑选的组件以最大化效率。另一方面，其维护需要技术熟练的人员——合格的工作人员来解决潜在问题并根据需要进行系统升级。

## 目标

在构建这种模型及其大规模部署的过程中，存在许多其他问题。总体而言，构建一个支持基础设施足够强大的系统，以匹敌市场上领先的服务，如[**ChatGPT**](https://openai.com/chatgpt/)，是非常困难的。然而，由于公开领域中有大量开源内容和技术，我们仍然能够实现与参考服务相当可接受和合理的近似。此外，考虑到其中一些技术的高进展度，它们被证明非常易于使用，使我们能够受益于它们的抽象性、模块化、易于集成等优点，这些特性有助于提升开发过程。

因此，本文的目的是展示我们如何设计、实现和部署一个支持ChatGPT类似服务的计算系统。尽管最终的结果可能无法具备预期的服务能力，但通过使用高质量的依赖项和开发工具，并采用良好的架构设计，可以确保系统根据用户需求轻松扩展到所需的计算能力。也就是说，系统将准备在非常少的机器上运行，可能仅需一台，且资源非常有限，提供与这些资源一致的吞吐量，或者在更大的计算机网络上运行，配备适当的硬件，提供扩展服务。

# 架构

最初，系统的主要功能是允许客户端提交文本查询，查询由LLM模型处理并返回给源客户端，所有过程都在合理的时间范围内完成，并提供公平的服务质量。这是我们系统的最高级别描述，具体来说，是该系统提供的应用功能，因为所有实现细节，如组件之间的通信协议、涉及的数据结构等，都故意被省略了。但现在，我们已经有了明确的目标，可以开始进行逐步的分解，逐渐增加解决问题时涉及的细节，这通常被称为[**功能分解**](https://stackoverflow.com/questions/947874/what-is-functional-decomposition)。因此，从一个黑箱系统[*(抽象)*](https://en.wikipedia.org/wiki/Black_box)开始，它接收并返回查询，我们可以开始全面定义客户端如何与系统互动，以及将实现这种互动的技术。

![](../Images/c4a11ecb1f9470fd5e8601f5d500cba8.png)

作者提供的图片

最初，我们必须确定什么构成客户端，特别是用户与系统交互所需的工具或接口。如上所示，我们假设系统目前是一个完全实现并投入运行的功能单元；这使我们能够专注于客户端和客户端-系统的连接。在客户端实例中，接口将通过一个网站提供，该网站设计上具有通用性，但主要面向桌面设备。同时，也可以开发并集成一个移动应用程序，使用相同的系统服务并具有特定的接口，但从抽象角度来看，理想的做法是将所有类型的客户端统一为一个，即 Web 客户端。

随后，需要找到一种方法将客户端与系统连接，以便在它们之间进行信息交换，在这种情况下是查询。此时，值得注意的是，Web 客户端将依赖于特定的技术，如 JavaScript，这带来了所有相应的通信影响。对于其他类型的平台，技术可能会发生变化，例如移动客户端可能会使用 Java，物联网设备可能会使用 C/C++，而兼容性要求可能要求系统相应地进行适配。

![](../Images/1ae51d8c94d610afad19d5df59daf004.png)

图片由作者提供

一种建立通信的方式是使用[**套接字**](https://www.geeksforgeeks.org/socket-in-computer-network/)及类似工具，这样可以在更低的层级上提供对整个协议的完全控制。然而，这种选择需要满足上述所述的所有客户端技术的兼容性要求，因为系统需要能够从所有可用的客户端类型收集查询。此外，拥有完全控制意味着开发过程会更长且可能更加复杂，因为必须考虑许多额外的细节，这会显著增加代码行数，并使其可维护性和可扩展性变得更加复杂。

如上所示，最优的替代方案是构建一个[**应用程序编程接口（API）**](https://www.ibm.com/topics/api)，它作为客户端和负责计算的系统部分之间的中介，即解决查询的部分。使用API的主要优点是，所有内部连接处理，例如打开和关闭套接字、线程池管理以及其他重要细节（*数据序列化*），都由构建API的框架来执行。通过这种方式，我们确保客户端只需将查询发送到执行API的服务器，并等待响应，所有这些都依赖于简化API请求管理的依赖项。另一个来自前一点的好处是，通过修改API的[**端点**](https://www.baeldung.com/cs/api-endpoints)，可以轻松扩展服务。例如，如果我们想为系统添加一个新模型或其他功能，只需添加并实现一个新端点，而无需更改通信协议本身或客户端与系统的交互方式。

## 计算服务

一旦我们设置了一个优雅的机制让客户端与系统进行通信，我们必须解决如何处理传入查询并在合理的时间内将其返回给相应客户端的问题。但首先，需要指出的是，当查询到达系统时，它必须被重定向到一台机器，该机器内存中加载了LLM（大语言模型）及其相应的推理管道，并通过该管道处理查询，获取结果文本（*LLM回答*），然后将其返回。因此，推理过程无法分布在多台机器上进行查询解决。考虑到这一点，我们可以开始设计支撑推理过程的基础设施。

在前面的图像中，计算服务被表示为一个单一的单元。如果我们将其视为通过一个单一通道连接的机器，并使用套接字与API服务器进行通信，我们将能够将所有API查询重定向到该机器，将所有系统负载集中在一个地方。如你所想，这将是一个适合仅供少数人使用的家庭系统的不错选择。然而，在这种情况下，我们需要一种方法来使这种方法具备可扩展性，以便在增加计算资源的情况下，我们可以为更多的用户提供服务。但首先，我们必须将前述的计算资源细分为多个单元。通过这种方式，我们将能够全面了解它们的相互连接，并能够通过改变它们的结构或组成方式来优化我们的项目吞吐量。

一个计算单元，为了方便实现，我们将其称为节点，将由一台物理机器集成，该机器接收需要解决的请求（*并非所有请求*）。此外，我们可以将节点视为一组（*可能较少*）机器的虚拟化，目的是通过在本地引入并行性来增加每个节点的总吞吐量。关于使用的硬件，这将很大程度上取决于服务的方向以及我们希望达到的程度。然而，对于本案例中呈现的版本，我们将假设使用标准的CPU、大量的RAM以避免加载模型或转发查询时出现问题，以及专用的处理器，如GPU，并在某些特定情况下可能包括TPU。

![](../Images/438dae7ba400296bda5f07129e1e5d9b.png)

图片由作者提供

现在，我们可以建立一个网络，将多个节点连接起来，通过其中一个节点（连接到API服务器）可以将查询分发到整个网络中，充分利用系统的所有资源。上面，我们可以看到所有节点在结构上以树状形态连接，其根节点负责收集API查询并按需转发。如何将节点互联的决策在很大程度上取决于系统的具体目的。在本例中，选择了树结构，因其分配原语简单。例如，如果我们希望最大化API与节点之间传输的查询数量，则必须从API到多个树的根节点建立多个连接，或者如果需要，选择其他不同的数据结构。

最后，我们需要定义当查询到达根节点时，查询是如何被转发和处理的。如前所述，有许多可用的且同样有效的替代方案。然而，我们将遵循的算法也将有助于理解为什么选择树状结构来连接系统节点。

![](../Images/56ace988d96c7a69a2176147578c375b.png)

图片由作者提供

由于查询必须在单个节点上解决，因此分配算法的目标是找到一个空闲节点，并将输入查询分配给该节点以进行解决。如上所示，如果我们考虑按自然顺序（*从1开始索引*）编号的有序查询序列，每个编号对应于与被分配去解决该查询的节点相连的边。为了理解这个具体例子中的编号，我们可以假设到达节点的查询需要无限长的时间来解决，因此确保每个节点逐步变得忙碌有助于理解该算法的启发式方法。

简而言之，我们将让根节点不执行任何解析处理，保留其所有能力用于转发与 API 的请求。对于任何其他节点，当它接收到来自层级更高节点的查询时，第一步是检查它是否正在处理之前查询的计算；如果它处于空闲状态，它将解析该查询，否则，它将通过[**轮询调度**](https://en.wikipedia.org/wiki/Round-robin_scheduling)将查询转发给其一个子节点。通过轮询调度，每次查询都会被重定向到不同的子节点，遍历整个子节点列表，仿佛它是一个循环缓冲区。这意味着一个节点的本地负载可以均匀分配到下游，同时高效利用每个节点的资源，并且通过添加更多子节点来实现系统的扩展。

最后，如果系统当前服务于许多用户，并且一个查询到达一个也忙碌的叶节点时，它将没有任何子节点可以将其重定向。因此，所有节点都将有一个查询排队机制，在这种情况下它们会等待，并能够在排队的查询之间应用批处理操作以加速 LLM 推理。此外，当一个查询完成后，为了避免通过向上传递查询直到到达树顶而导致系统过载，它会直接发送到根节点，随后到达 API 和客户端。我们可以将所有节点连接到 API，或实现其他替代方案，但为了保持代码尽可能简单，且系统高效，它们都会发送到根节点。

# Web 客户端

在定义了完整的系统架构和如何执行任务之后，我们可以开始构建用户在与我们的解决方案交互时需要的 Web 客户端。

正如预期的那样，Web 客户端是用基本的 HTML、CSS 和 JavaScript 实现的，所有内容都嵌入在一个单独的 .html 文件中，以便于使用。每次客户端发起与应用启动相关的请求时，API 将提供此文件，也就是说，当客户端进入浏览器并输入 API 托管入口点的地址时，它将返回 .html 文件以便在浏览器中渲染。

随后，当用户希望向系统发送文本查询时，JavaScript会在内部向API提交一个HTTP请求，包含相应的细节，例如数据类型、端点或[**CSRF**](https://en.wikipedia.org/wiki/Cross-site_request_forgery)安全令牌。通过在此过程中使用[**AJAX**](https://www.w3schools.com/js/js_ajax_intro.asp)，可以非常简单地定义一个原语，该原语在API返回请求的值时执行，负责将结果显示在屏幕上。此外，值得一提的是，发送的消息并非直接是书面或返回的文本，而是被封装在一个[**JSON**](https://en.wikipedia.org/wiki/JSON)中，包含其他重要参数，如时间戳，提供了在运行时添加额外字段的可能性，以管理某些系统组件的同步。

# Django API

当网页客户端准备就绪时，我们可以开始实现提供必要服务的API。

有许多可用的技术来构建API，但在本项目中，我们将专门使用[**Django**](https://developer.mozilla.org/en-US/docs/Learn/Server-side/Django/Introduction)通过Python在专用服务器上实现。做出这个决定的原因是该框架提供了较高的可扩展性和与其他Python依赖项的集成便利性，除此之外，还具有安全性和默认管理面板等有用的属性。

![](../Images/622d9f5b9d5667ffb824103b9612709a.png)

作者提供的图片

需要配置的端点之一是网页客户端的入口点，表示为默认的URL斜杠/。因此，当用户通过上述示例中的默认HTTP请求访问服务器时，API将返回显示界面所需的HTML代码，并开始向LLM服务发出请求。

![](../Images/58ad783edf8b7866cec09b9f4a188446.png)

作者提供的图片

同时，一旦访问了接口，它还必须支持客户端的请求。这些请求由于需要特殊管理，将有一个名为“/arranca”的端点，查询数据将以相应的JSON格式发送到该端点，API在处理完查询并通过节点树解决后，返回已解决的查询。在此端点，服务器通过一个预先建立的Socket通道与层级结构中的根节点转发查询，并通过同步机制等待其响应。

关于代码，在**urls.py**文件中，我们将存储URL和端点之间的关联，以便将默认的空URL分配给其对应的函数，该函数读取模板文件夹中的.html文件并将其返回，或者将URL /arranca分配给执行查询解决函数的端点。此外，还将执行一个视图函数来启动主服务器线程。同时，在**settings.py**中，唯一需要修改的就是将DEBUG参数设置为False，并输入允许连接到服务器的主机所需的权限。

最后是**views.py**脚本，在其中实现了所有的API功能。首先，我们有一个主线程负责接收和处理来自根节点的传入连接。最初，这个连接将在整个系统生命周期内保持常驻。然而，它被放置在一个无限循环中，以防连接被中断并需要重新建立。其次，默认的端点通过**index()**函数实现，如果客户端发起GET请求，它将返回.html内容。此外，用户在应用程序中提交的查询会通过***/arranca***端点传递给API，该端点由同名函数实现。在这里，输入的查询被转发到根节点，直到从根节点收到响应并返回给客户端时，才会解除阻塞。

这种阻塞是通过[**锁**](https://docs.python.org/3/library/threading.html#condition-objects)和同步机制实现的，每个查询都有一个唯一标识符，该标识符由***arranca()***函数作为字段插入到JSON消息中，命名为**request_id**。本质上，这是一个自然数，对应于查询到达的顺序。因此，当根节点将已解决的查询发送到API时，可以知道是哪个被阻塞的执行生成了该查询，从而解除阻塞，返回并重新阻塞其余查询。

# Java计算节点

在API运行后，我们将继续在Java中实现节点系统。选择这种语言的主要原因是它提供的技术使我们能够在节点之间进行通信。为了在这一层面上获得尽可能简单的通信语义，我们将放弃使用套接字和手动序列化消息，并用[**RMI**](https://www.javatpoint.com/RMI)替代，尽管在其他平台上这可能会稍显复杂，尽管它们也提供了像[**Pyro4**](https://pyro4.readthedocs.io/en/stable/)这样的Python解决方案。

[**远程方法调用 (RMI)**](https://en.wikipedia.org/wiki/Java_remote_method_invocation)是一种通信范式，使得由托管在不同机器上的远程对象组成的分布式系统的创建成为可能，能够相互获取远程引用并在它们的服务接口中调用远程方法。因此，由于Java中的高度抽象，节点之间的查询传输将通过对发送节点引用的远程对象的调用来实现，复杂的API连接过程将由手动处理，如同之前在Python中做的一样。

起初，我们应定义远程接口，它决定了每个节点可调用的远程方法。一方面，我们有返回调试信息相关方法***(log() 或 getIP())***。另一方面，有一些方法负责获取对其他节点的远程引用，并将其注册到本地层次结构中，作为一个升序或降序节点，使用一个我们假设对每个节点都是唯一的名称。此外，它还有另外两个原语，用于接收来自其他节点的传入查询***(receiveMessage())***和向API发送已解决的查询***(sendMessagePython())***，这些方法仅在根节点中执行。

在接口中，我们可以在节点类中实现其操作，每当我们启动系统并决定向节点树中添加新机器时，就会实例化该类。节点类中包含的主要功能之一是**getRemoteNode()**方法，它通过节点名称获取对另一个节点的远程引用。为此，它访问名称注册表并执行`lookup()`原语，如果该节点已注册，则返回以接口形式呈现的远程引用，否则返回null。

获取远程引用在树的构建中至关重要，特别是对于其他方法，这些方法将父节点连接到后代节点或获取根节点的引用以发送已解决的查询。其一是`connectParent()`，当一个后代节点需要与父节点连接时会调用它。如你所见，它首先使用`getRemoteNode()`来获取父节点，一旦获得引用，就将其分配给每个节点实例的本地变量。然后它会调用**connectChild()**，该方法将从中调用的远程节点附加到后代节点列表中。如果父节点不存在，它会尝试在一个空对象上调用函数，从而抛出异常。接下来需要注意的是，用于接收来自API的查询**receiveMessagePython()**和来自其他节点的查询**receiveMessage()**的方法都通过`synchronized`关键字进行保护，以避免可能干扰系统正确运行的竞争条件。这些方法还负责实现查询分发启发式算法，使用本地变量来确定应该将传入的查询发送到哪个对应的节点。

最后，节点类有一个线程池，用于管理**consultLLM()**方法中的查询解析。通过这种方式，它的调用将在 Java 代码中立即结束，因为线程池将为所需的计算分配一个线程，并将控制权返回给程序，从而可以接受更多查询。这对于检测节点是否正在进行任何计算也有优势，因为只需要检查活动线程的数量是否大于 0。另一方面，节点类中线程的另一个使用，位于线程池之外，是在**connectServer()**方法中，用于将根节点与用于查询交换的 API 连接。

在**Utilities**类中，我们只提供了创建 LDAP 使用上下文的方法，利用该方法可以根据节点的名称注册和查找远程引用。这个方法本可以直接放在节点类中，但如果我们需要更多类似的方法，出于设计模式的考虑，我们将其保留在 Utilities 类中。

节点实例的创建以及每个节点的手动管理是在 Launcher 类中实现的。它使用命令行接口来指示相应的节点，节点在启动时创建，并在指定的 LDAP 服务器上注册特定名称。一些命令包括：

+   **日志：** 打印有用的信息以了解节点的状态。

+   **父节点：** 将节点连接到指定的父节点，依据其名称进行连接。

+   **注册表：** 列出了当前在 LDAP 目录下的所有节点，这些节点位于组织单位**ou=Nodes**下。这对于监控注册表服务器或创建新节点可能很有用。

+   **服务器：** 将节点连接到通过其地址和端口号指定的服务器。主要情况下，服务器将是 Python API，但它也可以提供其他功能。

# LDAP 服务器

由于节点是远程对象，它们必须能够访问注册表，以便从其名称获取其他节点的远程引用。Java 提供的解决方案是使用**rmiregistry**在一台机器上初始化注册表服务。然而，当从其他主机执行如 rebind() 等受保护操作时，会抛出安全异常，阻止新节点在除注册表所在机器以外的其他机器上注册。因此，除了其简便性外，本项目将使用 Apache 服务器作为注册表，采用[**轻量级目录访问协议（LDAP）**](https://learn.microsoft.com/en-us/previous-versions/windows/desktop/ldap/lightweight-directory-access-protocol-ldap-api)。该协议允许在目录系统中管理*名称->远程节点*对，并具有其他附加功能，显著提升了与 Java 注册表提供的服务相比的注册表服务。

使用LDAP的优势始于其操作的复杂性，乍一看这可能显得相反，但实际上，这正是使系统能够在更高的细节级别上适应各种安全性和配置需求的关键。一方面，它所提供的认证和安全功能允许任何主机执行受保护的操作，如注册新节点，只要该主机已通过LDAP服务器验证。例如，当创建一个上下文对象以访问服务器并能够执行操作时，可以选择向其构造函数的HashMap中添加包含认证数据的参数。如果上下文已创建，则意味着数据与服务器期望的匹配，否则可以认为连接是由未经认证的*(“恶意”)*主机发起的，从而确保只有系统节点才能操作服务器信息。另一方面，LDAP允许更高效的节点注册集中化，提供更高级的互操作性，并且可以轻松集成像[**Kerberos**](https://www.ibm.com/docs/en/aix/7.3?topic=network-kerberos)这样的额外服务。

为确保服务器能够作为节点注册中心运行，我们必须对其应用特定配置。首先，由于该项目不会部署在有真实（且可能是恶意）用户的环境中，因此所有认证选项都被省略，以保持简单和干净。接下来，必须定义一个[**区分名称**](https://www.ibm.com/docs/en/i/7.5?topic=eim-distinguished-name)，以便将节点名称与其对应的远程对象关联。在这种情况下，假设我们防止注册多个具有相同名称的节点，我们只需将节点名称存储在某个属性中，如cn=（常用名称），并放置在指定的组织单元ou=Nodes中。因此，区分名称将是：**cn=Node_Name,ou=Nodes**

![](../Images/bb0053eddbdfba4f5fd216e06c3bdb0f.png)

图片来源：作者

每当创建一个新节点时，它会使用其区分名和节点实例，在LDAP服务器中作为目录形式的新条目进行注册。同样，删除节点或从注册表中获取其远程引用也需要使用区分名。对注册表执行这些操作意味着必须保持与LDAP服务器的连接。但是，由于节点是用Java编写的，我们可以使用一些服务来抽象整个连接过程，专注于调用操作。节点将使用的服务是一个目录上下文，通常由[**DirContext**](https://docs.oracle.com/javase/8/docs/api/javax/naming/directory/DirContext.html)接口定义。因此，访问服务器并执行一些管理操作的过程就像创建一个实现了DirContext接口的对象一样简单，在这个例子中是InitialDirContext，并为其分配适当的参数以识别服务器，其中包括一个**ldap://IP:port/**形式的URL、要使用的协议标识符，甚至是认证参数，在本项目中这些认证参数不会被使用。

## 查找、绑定和解绑

为了简单起见，Launcher将拥有自己的上下文对象，而每个节点也将拥有自己的上下文对象。这使得Launcher可以创建条目并执行删除操作，而每个节点则能够执行查找操作，从节点名称中获取远程引用。删除操作是最简单的，因为它只需要对应于要删除的节点的服务器条目的区分名。如果存在，它将被删除，并且对**unbind()**的调用将成功结束，否则它会抛出一个异常。另一方面，查找和注册操作需要遵循[**RFC-2713**](https://www.rfc-editor.org/rfc/rfc2713.html)。在将节点添加到服务器的情况下，将使用**bind()**原语，其参数是该节点将托管的条目的区分名，以及其远程对象。然而，bind函数不会直接传递节点对象或其接口，因为对象不可序列化，而bind()不能直接获取接口*“实例”*。作为解决方法，上述RFC强制将节点实例通过[**MarshalledObject**](https://docs.oracle.com/javase/8/docs/api/java/rmi/MarshalledObject.html)进行封装。因此，bind将接收一个由正在注册到服务器的节点组成的MarshalledObject，而不是原始节点实例。

最后，查找操作通过**lookup()**原语在上下文中执行。如果名称和节点没有被预先注册，或者在过程中发生了意外错误，将抛出异常。相反，如果操作成功，它将返回与查询的独特名称关联的MarshalledObject。但是，lookup()返回的远程引用包含在存储在注册表中的MarshalledObject包装器内。因此，必须使用MarshalledObject的**get()**操作来获取可用的远程引用。此外，借助此功能，可以防止注册与已注册节点同名的节点，因为在执行bind()之前会通过lookup()检查是否存在与该独特名称相关的异常。

# LLM 推理

关于每个节点的推理过程，节点树中有一个LLMProcess类，负责实例化一个用Python实现的进程，查询将在被解决之前传递到该进程，因为在Python中我们可以轻松管理LLM及其推理管道。

当实例化一个新的LLMProcess时，需要在机器上找到一个可用端口，以便Java和Python进程之间进行通信。为了简化，该数据交换将通过套接字（Sockets）完成，因此在通过打开和关闭ServerSocket找到可用端口后，**llm.py**进程将使用端口号作为参数启动。其主要功能包括**destroyProcess()**，在系统停止时终止进程，以及**sendQuery()**，它向llm.py发送查询并等待响应，每个查询使用一个新的连接。

在llm.py内部，有一个循环不断等待接受来自Java进程的传入连接。当建立此类连接时，它将通过[**ThreadPoolExecutor()**](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)线程并通过**handle_connection()**函数进行处理，该函数从通道读取输入数据，按JSON格式解释并将“text”字段转发到推理管道。一旦数据返回，它会被发送回Java进程*(连接的另一端)*，并且函数返回，同时释放相应的线程。

## 模型性能

如脚本中所示，管道实例允许我们选择将在托管节点上执行的 LLM 模型。这使我们可以访问上传到[**Huggingface**](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending)网站上的所有模型，包括代码生成模型、聊天模型、通用响应生成模型等各种选择。

默认情况下，我们使用**gpt2**模型，它大约有[**117M参数**](https://huggingface.co/transformers/v2.2.0/pretrained_models.html)，并且约有500MB的权重，是最轻便且最易于集成的选择。由于它是一个小型模型，它的回答相对基础，注意到一个查询的解答与以下文本的预测非常接近输入文本，例如：

> User: 你好。
> 
> GPT: 你好，首先我想提到的是……

还有其他版本的gpt2，例如**gpt2-large**或**gpt2-xl**，它们都可以从Huggingface获取，其中最强大的是XL，具有15亿个参数和6GB的权重，需要显著更强的硬件来运行它，能够生成像下面这样的连贯回答：

> User: 你好。
> 
> GPT: 大家好——感谢大家这几个月以来的耐心等待！在过去的一年里，我已经整理出了……

除了OpenAI的GPT系列外，你还可以选择许多其他可用的模型，尽管其中大多数需要在脚本中插入[**认证令牌**](https://huggingface.co/docs/hub/en/security-tokens)。例如，最近发布了经过优化的现代模型，优化了占用空间和查询通过整个推理流程所需的时间。Llama3就是其中之一，有[**8B参数**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B)的小型版本，以及[**70B**](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B)的大型版本。

然而，选择一个系统模型不应仅仅基于它的参数数量，因为它的架构决定了它可以建模的知识量。由于这个原因，一些小型模型的表现与大规模模型非常相似，即它们在语言理解层面上生成的回答非常相似，同时优化了生成这些回答所需的计算资源。作为参考，你可以使用[**基准测试**](https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a)，这个基准测试也由Huggingface提供，或者使用[**专业测试**](https://github.com/leobeeson/llm_benchmarks)来衡量任何LLM的上述参数。

上述测试中的结果，以及在特定硬件上响应所需的平均时间，是选择模型的一个相当完整的指标。尽管如此，始终记住，LLM必须适应它运行的芯片内存。因此，如果我们使用GPU推理，如在llm.py脚本中使用CUDA，则图形内存必须大于模型大小。如果不够，你必须将计算分配到[**多个GPU**](https://huggingface.co/docs/diffusers/training/distributed_inference)，无论是在同一台机器上，还是在多台机器上，这取决于你想要达到的复杂度。

## Kotlin 移动客户端

在我们完成之前，可以看看如何将一种新的客户端类型包含到系统中，从而展示我们迄今为止构建的一切所提供的可扩展性。当然，这个项目是一个[**分布式系统**](https://www.geeksforgeeks.org/what-is-a-distributed-system/)的尝试，因此你可以期待它与移动设备兼容，就像常规的ChatGPT应用程序兼容Android和iOS一样。在我们的案例中，我们可以为原生Android开发一个应用程序，尽管一个更好的选择是将系统适配为多平台Jetpack Compose项目。这个选项仍然是未来更新的一种可能性。

最初的想法是将移动客户端连接到API，并使用与Web版本相同的请求，依赖项包括[**HttpURLConnection**](https://developer.android.com/reference/kotlin/java/net/HttpURLConnection)。代码实现并不困难，Android官方页面提供的文档对于此目的也很有帮助。然而，我们也可以使用自定义的Kotlin中间组件来模拟API的功能，使用普通的TCP Android套接字进行通信。套接字相对容易使用，需要一点管理工作，确保一切正常运行，并提供对代码的良好控制。为了弥补缺乏规范API的问题，我们可以在移动客户端和Java节点树之间放置一个Kotlin节点，它将管理根节点和只有移动客户端之间的连接，前提是Web客户端和API是分开的。

关于界面，我们所模仿的应用程序ChatGPT，具有非常简洁和现代的外观。由于HTTP版本已经完成，我们可以尽量在Android Studio编辑器中尽可能接近地复制它。

![](../Images/330bb76614059e14df78a0e575d37464.png)

作者提供的图片

在使用套接字时，我们必须确保用户连接到正确的IP地址和端口，该服务器将解决他的查询。我们可以通过每次打开应用程序时出现的新初始界面来实现这一点。它是一个简单的视图，包含一个按钮，一个用于输入IP地址的文本视图，以及一个小的文本标签，实时向用户提供发生的情况，正如你在上面所看到的那样。

然后，我们需要使界面像一个真实的聊天，新的消息出现在底部，旧的消息则向上移动。为此，我们可以插入一个RecyclerView，它将占据屏幕的约80%。计划是拥有一个预定义的消息视图，可以动态添加到视图中，并且会根据消息是来自用户还是系统来改变。

最后，Android 连接的问题在于，你不能在主线程中执行任何与网络相关的操作，否则会抛出[**NetworkOnMainThreadException**](https://developer.android.com/reference/android/os/NetworkOnMainThreadException)。但同时，如果不在主线程中，你也无法管理组件，因为这会抛出[**CalledFromWrongThreadException**](https://stackoverflow.com/questions/44483224/how-can-i-fix-this-calledfromwrongthreadexception)。我们可以通过将连接视图移动到主线程中来解决这个问题，最重要的是充分利用协程，使你能够从中执行网络相关任务。

![](../Images/7ce7122e49603c9bd56764e9a97a5fc8.png)

作者提供的图片

现在，如果你运行系统并输入一个文本查询，答案应该会在几秒钟内显示出来，就像在 ChatGPT 等大型应用程序中一样。

# 结论

尽管系统已经具备功能，但你可以根据所用技术（无论是软件还是硬件）做出显著改进。然而，它仍然能够为有限数量的用户提供体面的服务，具体范围大致取决于可用资源。最后，需要指出的是，像 ChatGPT 这样的真实系统的性能是非常复杂的，因为支持它所需的模型大小和硬件特别昂贵。本文所展示的系统对于小型甚至中型解决方案具有高度可扩展性，但要实现大规模解决方案则需要更复杂的技术，可能还需要利用该系统的一些架构。

# 致谢

感谢[**deivih84**](https://medium.com/@forexsencillo)在**Kotlin** 移动客户端部分的合作，感谢[**carolinaherasc**](https://medium.com/@carolinaherasc)在 RMI 和分布式系统实现方面的帮助，以及[**hugodiezrubio**](https://medium.com/@hugodiezrubio)在开发部分系统管理组件中的贡献。
