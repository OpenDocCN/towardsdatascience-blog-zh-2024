- en: Build a Document AI Pipeline for Any Type of PDF with Gemini
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gemini 为任何类型的 PDF 构建文档 AI 流水线
- en: 原文：[https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15](https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15](https://towardsdatascience.com/build-a-document-ai-pipeline-for-any-type-of-pdf-with-gemini-9221c8e143db?source=collection_archive---------0-----------------------#2024-12-15)
- en: Tables, Images, figures or equations are not problem anymore! Full Code provided.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表格、图片、图表或公式已经不再是问题！提供完整代码。
- en: '[](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[![Youness
    Mansar](../Images/b68fe2cbbe219ab0231922c7165f2b6a.png)](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    [Youness Mansar](https://medium.com/@CVxTz?source=post_page---byline--9221c8e143db--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    ·10 min read·Dec 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--9221c8e143db--------------------------------)
    ·阅读时间 10 分钟·2024年12月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/04f5f0506207581622a222ae56c38208.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f5f0506207581622a222ae56c38208.png)'
- en: Photo by [Matt Noble](https://unsplash.com/@mcnoble?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Matt Noble](https://unsplash.com/@mcnoble?utm_source=medium&utm_medium=referral)
    via [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Automated document processing is one of the biggest winners of the ChatGPT revolution,
    as LLMs are able to tackle a wide range of subjects and tasks in a zero-shot setting,
    meaning without in-domain labeled training data. This has made building AI-powered
    applications to process, parse, and automatically understand arbitrary documents
    much easier. Though naive approaches using LLMs are still hindered by non-text
    context, such as figures, images, and tables, this is what we will try to address
    in this blog post, with a special focus on PDFs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化文档处理是 ChatGPT 革命中的最大赢家之一，因为大型语言模型（LLM）能够在零样本环境下处理各种主题和任务，这意味着它们不需要领域内的标注训练数据。这使得构建基于
    AI 的应用程序来处理、解析和自动理解任意文档变得更加容易。尽管使用 LLM 的简单方法仍然受到非文本上下文的制约，如图表、图片和表格，但这正是我们将在本博客中尝试解决的问题，特别是关注
    PDF 文件。
- en: At a basic level, PDFs are just a collection of characters, images, and lines
    along with their exact coordinates. They have no inherent “text” structure and
    were not built to be processed as text but only to be viewed as is. This is what
    makes working with them difficult, as text-only approaches fail to capture all
    the layout and visual elements in these types of documents, resulting in a significant
    loss of context and information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础层面来说，PDF 文件仅仅是由字符、图片和线条以及它们的精确坐标组成。它们没有内在的“文本”结构，并且并非为了作为文本进行处理而构建，而只是为了查看而存在。这就是为什么与它们打交道会很困难，因为仅限文本的方法无法捕捉到这些文档中的所有布局和视觉元素，导致大量的上下文和信息丢失。
- en: One way to bypass this “text-only” limitation is to do heavy pre-processing
    of the document by detecting tables, images, and layout before feeding them to
    the LLM. Tables can be parsed to Markdown or JSON, images and figures can be represented
    by their captions, and the text can be fed as is. However, this approach requires
    custom models and will still result in some loss of information, so can we do
    better?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 绕过这一“仅限文本”的限制的一种方法是，通过在将文档输入到 LLM 之前，进行大量的预处理工作，检测表格、图片和布局。表格可以解析为 Markdown
    或 JSON，图片和图表可以通过它们的标题来表示，文本则可以按原样输入。然而，这种方法需要自定义模型，并且仍然会导致一些信息丢失，那么我们能做得更好吗？
- en: Multimodal LLMs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态 LLMs
- en: Most recent large models are now multi-modal, meaning they can process multiple
    modalities like text, code, and images. This opens the way to a simpler solution
    to our problem where one model does everything at once. So, instead of captioning
    images and parsing tables, we can just feed the page as an image and process it
    as is. Our pipeline will be able to load the PDF, extract each page as an image,
    split it into chunks (using the LLM), and index each chunk. If a chunk is retrieved,
    then the full page is included in the LLM context to perform the task. In what
    follows, we will detail how this can be implemented in practice.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型模型大多是多模态的，意味着它们能够处理文本、代码和图像等多种模态。这为我们的问题提供了更简单的解决方案，可以让一个模型一次性处理所有内容。因此，代替为图像加上标题和解析表格，我们可以直接将页面作为图像输入，并按原样处理。我们的管道将能够加载PDF，提取每一页作为图像，使用LLM将其拆分为多个分块，并为每个分块创建索引。如果某个分块被检索到，完整的页面将被包含在LLM的上下文中进行任务处理。接下来，我们将详细说明如何在实践中实现这一点。
- en: The Pipeline
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管道
- en: The pipeline we are implementing is a two-step process. First, we segment each
    page into significant chunks and summarize each of them. Second, we index chunks
    once then search the chunks each time we get a request and include the full context
    with each retrieved chunk in the LLM context.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现的管道是一个两步过程。首先，我们将每一页分割成重要的分块并对每个分块进行摘要。其次，我们在每次获取请求时检索这些分块并将完整上下文与每个检索到的分块一同包含在LLM上下文中。
- en: 'Step 1: Page Segmentation and Summarization'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1：页面分割与摘要
- en: 'We extract the pages as images and pass each of them to the multi-modal LLM
    to segment them. Models like Gemini can understand and process page layout easily:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将页面提取为图像，并将每个图像传递给多模态LLM进行分割。像Gemini这样的模型可以轻松理解并处理页面布局：
- en: '**Tables** are identified as one chunk.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表格**被视为一个分块。'
- en: '**Figures** form another chunk.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图形**构成另一个分块。'
- en: '**Text blocks** are segmented into individual chunks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本块**被分割成独立的分块。'
- en: …
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: …
- en: For each element, the LLM generates a summary than can be embedded and indexed
    into a vector database.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个元素，LLM会生成一个摘要，可以嵌入并索引到向量数据库中。
- en: 'Step 2: Embedding and Contextual Retrieval'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2：嵌入与上下文检索
- en: In this tutorial we will use text embedding only for simplicity but one improvement
    would be to use vision embeddings directly.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将仅使用文本嵌入以简化操作，但一种改进方法是直接使用视觉嵌入。
- en: 'Each entry in the database includes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库中的每一条记录包括：
- en: The summary of the chunk.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分块的摘要。
- en: The page number where it was found.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到该项的页面编号。
- en: A link to the image representation of the full page for added context.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供完整页面图像的链接以增加上下文信息。
- en: This schema allows for **local level searches** (at the chunk level) while keeping
    **track of the context** (by linking back to the full page). For example, if a
    search query retrieves an item, the Agent can include the entire page image to
    provide full layout and extra context to the LLM in order to maximize response
    quality.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构允许**局部级别搜索**（按分块级别），同时保持**上下文跟踪**（通过链接回完整页面）。例如，如果搜索查询检索到某个项，代理可以将整个页面图像包含进来，以便向LLM提供完整的布局和额外的上下文信息，从而最大化响应质量。
- en: By providing the full image, all the visual cues and important layout information
    (like images, titles, bullet points… ) and neighboring items (tables, paragraph,
    …) are available to the LLM at the time of generating a response.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供完整图像，所有的视觉提示和重要的布局信息（如图像、标题、项目符号…）以及邻近的项（表格、段落等）都可以在生成响应时提供给LLM。
- en: Agents
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代理
- en: 'We will implement each step as a separate, re-usable agent:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个步骤实现为一个独立的、可重用的代理：
- en: The first agent is for parsing, chunking, and summarization. This involves the
    segmentation of the document into significant chunks, followed by the generation
    of summaries for each of them. This agent only needs to be run once per PDF to
    preprocess the document.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个代理用于解析、分块和摘要。这涉及到将文档分割成重要的分块，并为每个分块生成摘要。这个代理每个PDF只需要运行一次，用于预处理文档。
- en: The second agent manages indexing, search, and retrieval. This includes inserting
    the embedding of chunks into the vector database for efficient search. Indexing
    is performed once per document, while searches can be repeated as many times as
    needed for different queries.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个代理负责管理索引、搜索和检索。这包括将分块的嵌入插入到向量数据库中，以便高效搜索。索引在每个文档中只执行一次，而搜索可以根据不同的查询重复进行。
- en: For both agents, we use **Gemini**, a multimodal LLM with strong vision understanding
    abilities.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个代理，我们使用**Gemini**，一个具有强大视觉理解能力的多模态LLM。
- en: Parsing and Chunking Agent
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析与分块代理
- en: 'The first agent is in charge of segmenting each page into meaningful chunks
    and summarizing each of them, following these steps:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个代理负责将每一页分割成有意义的片段，并对每个片段进行总结，按照以下步骤进行：
- en: '**Step 1: Extracting PDF Pages as Images**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：将PDF页面提取为图像**'
- en: We use the `pdf2image` library. The images are then encoded in Base64 format
    to simplify adding them to the LLM request.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pdf2image`库。然后将图像编码为Base64格式，以简化将其添加到LLM请求中。
- en: 'Here’s the implementation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现过程：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`extract_images_from_pdf`: Extracts each page of the PDF as a PIL image.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`extract_images_from_pdf`：将PDF的每一页提取为PIL图像。'
- en: '`pil_image_to_base64_jpeg`: Converts the image into a Base64-encoded JPEG format.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`pil_image_to_base64_jpeg`：将图像转换为Base64编码的JPEG格式。'
- en: '**Step 2: Chunking and Summarization**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：分段和总结**'
- en: 'Each image is then sent to the LLM for segmentation and summarization. We use
    structured outputs to ensure we get the predictions in the format we expect:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像将被发送到LLM进行分割和总结。我们使用结构化输出以确保获得我们期望的格式的预测：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `LayoutElements` schema defines the structure of the output, with each layout
    item type (Table, Figure, … ) and its summary.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`LayoutElements`模式定义了输出的结构，其中包括每种布局项类型（表格、图形等）及其总结。'
- en: '**Step 3: Parallel Processing of Pages**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：页面的并行处理**'
- en: 'Pages are processed in parallel for speed. The following method creates a list
    of tasks to handle all the page image at once since the processing is io-bound:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 页面并行处理以提高速度。以下方法创建一个任务列表，以便同时处理所有页面图像，因为处理是IO绑定的：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Each page is sent to the `find_layout_items` function as an independent task.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每一页将作为独立任务发送到`find_layout_items`函数。
- en: '**Full workflow**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整工作流程**'
- en: The agent’s workflow is built using a `StateGraph`, linking the image extraction
    and layout detection steps into a unified pipeline ->
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理的工作流程使用`StateGraph`构建，将图像提取和布局检测步骤链接成一个统一的管道 ->
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To run the agent on a sample PDF we do:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要在一个样本PDF上运行代理，我们需要执行以下操作：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This results in a parsed, segmented, and summarized representation of the PDF,
    which is the input of the second agent we will build next.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个解析、分段和总结后的PDF表示，它是我们接下来要构建的第二个代理的输入。
- en: RAG Agent
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RAG代理
- en: This second agent handles the indexing and retrieval part. It saves the documents
    of the previous agent into a vector database and uses the result for retrieval.
    This can be split into two seprate steps, indexing and retrieval.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第二个代理负责索引和检索部分。它将前一个代理的文档保存到向量数据库中，并使用结果进行检索。这可以分为两个独立的步骤，索引和检索。
- en: '**Step 1: Indexing the Split Document**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：索引拆分文档**'
- en: 'Using the summaries generated, we vectorize them and save them in a ChromaDB
    database:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成的总结，我们将它们向量化并保存到ChromaDB数据库中：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `index_documents` method embeds the chunk summaries into the vector store.
    We keep metadata such as the document path and page number for later use.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_documents`方法将片段总结嵌入到向量存储中。我们保留文档路径和页面编号等元数据，以备后续使用。'
- en: '**Step 2: Handling Questions**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：处理问题**'
- en: When a user asks a question, the agent searches for the most relevant chunks
    in the vector store. It retrieves the summaries and corresponding page images
    for contextual understanding.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出问题时，代理会在向量存储中搜索最相关的片段。它检索总结和相应的页面图像以便理解上下文。
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The retriever queries the vector store to find the chunks most relevant to the
    user’s question. We then build the context for the LLM (Gemini), which combines
    text chunks and images in order to generate a response.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器查询向量存储以查找与用户问题最相关的片段。然后我们为LLM（Gemini）构建上下文，结合文本片段和图像以生成回应。
- en: '**The full agent Workflow**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**完整代理工作流程**'
- en: 'The agent workflow has two stages, an indexing stage and a question answering
    stage:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的工作流程包括两个阶段，一个是索引阶段，另一个是问答阶段：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Example run**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例运行**'
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With this implementation, the pipeline is complete for document processing,
    retrieval, and question answering.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此实现，管道完成了文档处理、检索和问答功能。
- en: 'Example: Using the Document AI Pipeline'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：使用文档AI管道
- en: Let’s walk through a practical example using the document [LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    , a set of 39 slides containing text, equations, and figures (CC BY 4.0).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际示例来操作，使用文档[LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)，它包含39页幻灯片，包含文本、公式和图形（CC
    BY 4.0）。
- en: 'Step 1: Parsing and summarizing the Document (Agent 1)'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：解析和总结文档（代理1）
- en: '**Execution Time**: Parsing the 39-page document took **29 seconds**.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行时间**：解析39页文档花费了**29秒**。'
- en: '**Result**: Agent 1 produces an indexed document consisting of chunk summaries
    and base64-encoded JPEG images of each page.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果**：代理1生成了一个索引文档，其中包含每页的摘要和Base64编码的JPEG图片。'
- en: 'Step 2: Questioning the Document (Agent 2)'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：质疑文档（代理2）
- en: 'We ask the following question:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了以下问题：
- en: '**“**Explain LoRA, give the relevant equations**”**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**“**解释LoRA，给出相关的公式**”**'
- en: 'Result:'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果：
- en: 'Retrieved pages:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 检索的页面：
- en: '![](../Images/a96c96db2579443f470a142aa9c9d1a7.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a96c96db2579443f470a142aa9c9d1a7.png)'
- en: 'Source: [LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    License CC-BY'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[LLM & Adaptation.pdf](https://github.com/SharifiZarchi/Introduction_to_Machine_Learning/blob/main/Slides/Chapter_05_Natural_Language_Processing/04-LLM%26Adaptation/LLM%20%26%20Adaptation.pdf)
    许可证 CC-BY
- en: Response from the LLM
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自LLM的回应
- en: '![](../Images/4af2dc191f5a778641c7a0e0c0b2fcb8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4af2dc191f5a778641c7a0e0c0b2fcb8.png)'
- en: Image by author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: The LLM was able to include equations and figures into its response by taking
    advantage of the visual context in generating a coherent and correct response
    based on the document.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LLM能够通过利用生成连贯且正确回应的视觉上下文，将公式和图形纳入其回应中。
- en: Conclusion
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this quick tutorial, we saw how you can take your document AI processing
    pipeline a step further by leveraging the multi-modality of recent LLMs and using
    the full visual context available in each document, hopefully improving the quality
    of outputs that you are able to get from either your information extraction or
    RAG pipeline.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的教程中，我们看到了如何通过利用近期LLMs的多模态性，进一步推进你的文档AI处理流程，并利用每个文档中的完整视觉上下文，来提升你从信息提取或RAG流程中获得的输出质量。
- en: We built a stronger document segmentation step that is able to detect the important
    items like paragraphs, tables, and figures and summarize them, then used the result
    of this first step to query the collection of items and pages to give relevant
    and precise answers using Gemini. As a next step, you can try it on your use case
    and document, try to use a scalable vector database, and deploy these agents as
    part of your AI app.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个更强大的文档分割步骤，能够检测到重要的项目，如段落、表格和图形，并对其进行总结，随后使用这一第一步的结果来查询项目和页面集合，利用Gemini给出相关且精准的答案。作为下一步，你可以在你的使用案例和文档上尝试，试着使用一个可扩展的向量数据库，并将这些代理部署为你AI应用的一部分。
- en: 'Full code and example are available here : [https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 完整代码和示例请参见：[https://github.com/CVxTz/document_ai_agents](https://github.com/CVxTz/document_ai_agents)
- en: Thank you for reading ! 😃
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！😃
