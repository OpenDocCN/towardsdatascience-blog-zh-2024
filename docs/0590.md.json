["```py\nnp.random.seed(999)\n\n# Create dataset with spurious correlation\ntemperature = np.random.normal(loc=0, scale=1, size=1000)\nice_cream_sales = 2.5 * temperature + np.random.normal(loc=0, scale=1, size=1000)\nshark_attacks = 0.5 * temperature + np.random.normal(loc=0, scale=1, size=1000)\ndf_spurious = pd.DataFrame(data=dict(temperature=temperature, ice_cream_sales=ice_cream_sales, shark_attacks=shark_attacks))\n\n# Pairplot\nsns.pairplot(df_spurious, corner=True)\n```", "```py\n# Create node lookup variables\nnode_lookup = {0: 'Temperature',\n               1: 'Ice cream sales',\n               2: 'Shark attacks'                                                                          \n }\n\ntotal_nodes = len(node_lookup)\n\n# Create adjacency matrix - this is the base for our graph\ngraph_actual = np.zeros((total_nodes, total_nodes))\n\n# Create graph using expert domain knowledge\ngraph_actual[0, 1] = 1.0 # Temperature -> Ice cream sales\ngraph_actual[0, 2] = 1.0 # Temperature -> Shark attacks\n\nplot_graph(input_graph=graph_actual, node_lookup=node_lookup)\n```", "```py\n# Run first conditional independence test\ntest_id_1 = round(gcm.independence_test(ice_cream_sales, shark_attacks, conditioned_on=temperature), 2)\n\n# Run second conditional independence test\ntest_id_2 = round(gcm.independence_test(ice_cream_sales, temperature, conditioned_on=shark_attacks), 2)\n\n# Run third conditional independence test\ntest_id_3 = round(gcm.independence_test(shark_attacks, temperature, conditioned_on=ice_cream_sales), 2)\n```", "```py\n# Create node lookup for channels\nnode_lookup = {0: 'Demand',\n               1: 'Call waiting time',\n               2: 'Call abandoned', \n               3: 'Reported problems',                   \n               4: 'Discount sent',\n               5: 'Churn'                                                                             \n }\n\ntotal_nodes = len(node_lookup)\n\n# Create adjacency matrix - this is the base for our graph\ngraph_actual = np.zeros((total_nodes, total_nodes))\n\n# Create graph using expert domain knowledge\ngraph_actual[0, 1] = 1.0 # Demand -> Call waiting time\ngraph_actual[0, 2] = 1.0 # Demand -> Call abandoned\ngraph_actual[0, 3] = 1.0 # Demand -> Reported problems\ngraph_actual[1, 2] = 1.0 # Call waiting time -> Call abandoned\ngraph_actual[1, 5] = 1.0 # Call waiting time -> Churn\ngraph_actual[2, 3] = 1.0 # Call abandoned -> Reported problems\ngraph_actual[2, 5] = 1.0 # Call abandoned -> Churn\ngraph_actual[3, 4] = 1.0 # Reported problems -> Discount sent\ngraph_actual[3, 5] = 1.0 # Reported problems -> Churn\ngraph_actual[4, 5] = 1.0 # Discount sent -> Churn\n\nplot_graph(input_graph=graph_actual, node_lookup=node_lookup)\n```", "```py\ndef data_generator(max_call_waiting, inbound_calls, call_reduction):\n    '''\n     A data generating function that has the flexibility to reduce the value of node 0 (Call waiting time) - this enables us to calculate ground truth counterfactuals\n\n    Args:\n        max_call_waiting (int): Maximum call waiting time in seconds\n        inbound_calls (int): Total number of inbound calls (observations in data)\n        call_reduction (float): Reduction to apply to call waiting time\n\n    Returns:\n        DataFrame: Generated data\n    '''\n\n    df = pd.DataFrame(columns=node_lookup.values())\n\n    df[node_lookup[0]] = np.random.randint(low=10, high=max_call_waiting, size=(inbound_calls)) # Demand\n    df[node_lookup[1]] = (df[node_lookup[0]] * 0.5) * (call_reduction) + np.random.normal(loc=0, scale=40, size=inbound_calls) # Call waiting time\n    df[node_lookup[2]] = (df[node_lookup[1]] * 0.5) + (df[node_lookup[0]] * 0.2) + np.random.normal(loc=0, scale=30, size=inbound_calls) # Call abandoned\n    df[node_lookup[3]] = (df[node_lookup[2]] * 0.6) + (df[node_lookup[0]] * 0.3) + np.random.normal(loc=0, scale=20, size=inbound_calls) # Reported problems\n    df[node_lookup[4]] = (df[node_lookup[3]] * 0.7) + np.random.normal(loc=0, scale=10, size=inbound_calls) # Discount sent\n    df[node_lookup[5]] = (0.10 * df[node_lookup[1]] ) + (0.30 * df[node_lookup[2]]) + (0.15 * df[node_lookup[3]]) + (-0.20 * df[node_lookup[4]]) # Churn\n\n    return df\n\n# Generate data\nnp.random.seed(999)\ndf = data_generator(max_call_waiting=600, inbound_calls=10000, call_reduction=1.00)\n\n# Pairplot\nsns.pairplot(df, corner=True)\n```", "```py\n# Apply PC method to learn graph\npc = PC(variant='stable')\npc.learn(df)\ngraph_pred = pc.causal_matrix\n\ngraph_pred\n```", "```py\n# GScore\nmetrics = MetricsDAG(\n    B_est=graph_pred, \n    B_true=graph_actual)\nmetrics.metrics['gscore']\n```", "```py\nplot_graph(input_graph=graph_pred, node_lookup=node_lookup)\n```", "```py\n# Apply PC method to learn skeleton\nskeleton_pred, sep_set = find_skeleton(df.to_numpy(), 0.05, 'fisherz')\n\nskeleton_pred\n```", "```py\n# Transform the ground truth graph into an undirected adjacency matrix\nskeleton_actual = graph_actual + graph_actual.T\nskeleton_actual = np.where(skeleton_actual > 0, 1, 0)\n```", "```py\n# GScore\nmetrics = MetricsDAG(\n    B_est=skeleton_pred, \n    B_true=skeleton_actual)\nmetrics.metrics['gscore']\n```", "```py\nplot_graph(input_graph=skeleton_pred, node_lookup=node_lookup)\n```"]