["```py\ntrajectories = []\n\nfor step in range(n_steps):\n  action = actor_network(obs)\n  obs, state, reward, done, info = env.step(action, state)\n  trajectories.append(tuple(obs, state, reward, done, info))\n```", "```py\ndef scan(f, init, xs, length=None):\n  \"\"\"Example provided in the JAX documentation.\"\"\"\n  if xs is None:\n    xs = [None] * length\n\n  carry = init\n  ys = []\n  for x in xs:\n    # apply function f to current state\n    # and element x\n    carry, y = f(carry, x) \n    ys.append(y)\n  return carry, np.stack(ys)\n```", "```py\ndef _update_epoch(update_state, unused):\n  \"\"\"\n  Scans update_minibatch over shuffled and permuted \n  mini batches created from the trajectory batch.\n  \"\"\"\n\n  def _update_minbatch(train_state, batch_info):\n    \"\"\"\n    Wraps loss_fn and computes its gradient over the \n    trajectory batch before updating the network parameters.\n    \"\"\"\n    ...\n\n    def _loss_fn(params, traj_batch, gae, targets):\n      \"\"\"\n      Defines the PPO loss and computes its value.\n      \"\"\"\n      ...\n```"]