["```py\ndef align_labels_with_tokens(labels, word_ids):\n    new_labels = []\n    current_word = None\n    for word_id in word_ids:\n        if word_id != current_word:\n            # Start of a new word!\n            current_word = word_id\n            label = -100 if word_id is None else labels[word_id]\n            new_labels.append(label)\n        elif word_id is None:\n            # Special token\n            new_labels.append(-100)\n        else:\n            # Same word as previous token\n            label = labels[word_id]\n            # If the label is B-XXX we change it to I-XXX\n            if label % 2 == 1:\n                label += 1\n            new_labels.append(label)\n\n    return new_labels\n\ndef tokenize_and_align_labels(examples):\n    tokenizer = AutoTokenizer.from_pretrained(\"../model/xlm-roberta-large\")\n    tokenized_inputs = tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True,\n        padding=\"max_length\", max_length=512)\n    all_labels = examples[\"ner_tags\"]\n    new_labels = []\n    for i, labels in enumerate(all_labels):\n        word_ids = tokenized_inputs.word_ids(i)\n        new_labels.append(align_labels_with_tokens(labels, word_ids))\n\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n```", "```py\ndef postprocess(predictions, labels):\n    predictions = predictions.detach().cpu().clone().numpy()\n    labels = labels.detach().cpu().clone().numpy()\n\n    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    return true_predictions, true_labels\n```", "```py\ndef compute_weights(trainset, num_labels):\n    c = Counter()\n    for t in trainset:\n        c += Counter(t['labels'].tolist())\n    weights = [sum(c.values())/(c[i]+1) for i in range(num_labels)]\n    return weights\n```", "```py\nfrom accelerate import Accelerator, notebook_launcher\nfrom collections import Counter\nfrom datasets import Dataset\nfrom datetime import datetime\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import XLMRobertaConfig, XLMRobertaForTokenClassification\nfrom seqeval.metrics import classification_report, f1_score\n\ndef create_dataloaders(trainset, evalset, batch_size, num_workers):\n    train_dataloader = DataLoader(trainset, shuffle=True, \n                          batch_size=batch_size, num_workers=num_workers)\n    eval_dataloader = DataLoader(evalset, shuffle=False, \n                          batch_size=batch_size, num_workers=num_workers)\n    return train_dataloader, eval_dataloader\n\ndef main(batch_size, num_workers, epochs, model_path, dataset_tr, dataset_ev, training_type, model_params, dt):\n    accelerator = Accelerator(split_batches=True)\n    num_labels = model_params['num_labels']\n\n    # Prepare data #\n    train_ds = Dataset.from_dict(\n                {\"tokens\": [d[2][:512] for d in dataset_tr], \n                 \"ner_tags\": [d[1][:512] for d in dataset_tr]})\n    eval_ds = Dataset.from_dict(\n                {\"tokens\": [d[2][:512] for d in dataset_ev],\n                 \"ner_tags\": [d[1][:512] for d in dataset_ev]})\n    trainset = train_ds.map(tokenize_and_align_labels, batched=True,\n                 remove_columns=[\"tokens\", \"ner_tags\"])\n    evalset = eval_ds.map(tokenize_and_align_labels, batched=True,\n                 remove_columns=[\"tokens\", \"ner_tags\"])\n    trainset.set_format(\"torch\")\n    evalset.set_format(\"torch\")\n    train_dataloader, eval_dataloader = create_dataloaders(trainset, evalset,\n                                          batch_size, num_workers)\n\n    # Type of training #\n    if training_type=='from_scratch':\n        config = XLMRobertaConfig.from_pretrained(model_path, **model_params)\n        model = XLMRobertaForTokenClassification(config)\n    elif training_type=='transfer_learning':\n        model = AutoModelForTokenClassification.from_pretrained(model_path, \n                    ignore_mismatched_sizes=True, **model_params)\n        for param in model.parameters():\n            param.requires_grad=False\n        for param in model.classifier.parameters():\n            param.requires_grad=True\n    elif training_type=='fine_tuning':\n        model = AutoModelForTokenClassification.from_pretrained(model_path,\n                     **model_params)\n        for param in model.parameters():\n            param.requires_grad=True\n        for param in model.classifier.parameters():\n            param.requires_grad=True\n\n    # Intantiate the optimizer #\n    optimizer = torch.optim.AdamW(params=model.parameters(), lr=2e-5)\n\n    # Instantiate the learning rate scheduler #\n    lr_scheduler = ReduceLROnPlateau(optimizer, patience=5)\n\n    # Define loss function #\n    weights = compute_weights(trainset, num_labels)\n    loss_fct = CrossEntropyLoss(weight=torch.tensor(weights))\n\n    # Prepare objects for distributed training #\n    loss_fct, train_dataloader, model, optimizer, eval_dataloader, lr_scheduler = accelerator.prepare(\n        loss_fct, train_dataloader, model, optimizer, eval_dataloader, lr_scheduler)\n\n    # Training loop #\n    max_f1 = 0 # for early stopping\n    for t in range(epochs):\n        # training\n        accelerator.print(f\"\\n\\nEpoch {t+1}\\n-------------------------------\")\n        model.train()\n        tr_loss = 0\n        preds = list()\n        labs = list()\n        for batch in train_dataloader:\n            outputs = model(input_ids=batch['input_ids'],\n                            attention_mask=batch['attention_mask'])\n            labels = batch[\"labels\"]\n            loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            tr_loss += loss\n            predictions = outputs.logits.argmax(dim=-1)\n            predictions_gathered = accelerator.gather(predictions)\n            labels_gathered = accelerator.gather(labels)\n            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n            preds.extend(true_predictions)\n            labs.extend(true_labels)\n\n        lr_scheduler.step(tr_loss)\n\n        accelerator.print(f\"Train loss: {tr_loss/len(train_dataloader):>8f} \\n\")\n        accelerator.print(classification_report(labs, preds))\n\n        # evaluation\n        model.eval()\n        ev_loss = 0\n        preds = list()\n        labs = list()\n        for batch in eval_dataloader:\n            with torch.no_grad():\n                outputs = model(input_ids=batch['input_ids'],\n                                attention_mask=batch['attention_mask'])\n                labels = batch[\"labels\"]\n                loss = loss_fct(outputs.logits.view(-1, num_labels), labels.view(-1))\n\n            ev_loss += loss\n            predictions = outputs.logits.argmax(dim=-1)\n            predictions_gathered = accelerator.gather(predictions)\n            labels_gathered = accelerator.gather(labels)\n            true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n            preds.extend(true_predictions)\n            labs.extend(true_labels)\n\n        accelerator.print(f\"Eval loss: {ev_loss/len(eval_dataloader):>8f} \\n\")\n        accelerator.print(classification_report(labs, preds))\n\n        accelerator.print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']}\")\n\n        # checkpoint best model\n        if f1_score(labs, preds) > max_f1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(f\"../model/xlml_ner/{dt}/\",\n                               is_main_process=accelerator.is_main_process,\n                               save_function=accelerator.save)\n            accelerator.print(f\"Model saved during {t+1}. epoch.\")\n            max_f1 = f1_score(labs, preds)\n            best_epoch = t\n\n        # early stopping\n        if (t - best_epoch) > 10:\n            accelerator.print(f\"Early stopping after {t+1}. epoch.\")\n            break\n\n    accelerator.print(\"Done!\") \n```", "```py\nlabel_list = [\n    \"O\",\n    \"B-evcu\", \"I-evcu\", # variable symbol of creditor\n    \"B-rc\", \"I-rc\", # birth ID\n    \"B-prijmeni\", \"I-prijmeni\", # surname\n    \"B-jmeno\", \"I-jmeno\", # given name\n    \"B-datum\", \"I-datum\", # birth date\n]\nid2label = {a: b for a,b in enumerate(label_list)}\nlabel2id = {b: a for a,b in enumerate(label_list)}\n\nnum_workers = 6 # number of GPUs\nbatch_size = num_workers*2\nepochs = 100\nmodel_path = \"../model/xlm-roberta-large\"\ntraining_type = \"fine_tuning\" # from_scratch / transfer_learning / fine_tuning\nmodel_params = {\"id2label\": id2label, \"label2id\": label2id, \"num_labels\": 11}\ndt = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nos.mkdir(f\"../model/xlml_ner/{dt}\")\n\nnotebook_launcher(main, args=(batch_size, num_workers, epochs, model_path,\n                   dataset_tr, dataset_ev, training_type, model_params, dt),\n                   num_processes=num_workers, mixed_precision=\"fp16\", use_port=\"29502\")\n```"]