<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>A Patch is More than 16*16 Pixels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>A Patch is More than 16*16 Pixels</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17">https://towardsdatascience.com/a-patch-is-more-than-16-16-pixels-699359211513?source=collection_archive---------1-----------------------#2024-06-17</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="c93a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">On Pixel Transformer and Ultra-long Sequence Distributed Transformer</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mengliu Zhao" class="l ep by dd de cx" src="../Images/0b950a0785fa065db3319ed5be4a91de.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*siAyGzGqa7K3xsa639R_2w.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://mengliuz.medium.com/?source=post_page---byline--699359211513--------------------------------" rel="noopener follow">Mengliu Zhao</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--699359211513--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">7 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jun 17, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="1259" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Have you ever wondered why the Vision Transformer (ViT) uses 16*16 size patches as input tokens?</p><p id="91b2" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">It all dates back to the earlier days of the Transformers. The original Transformer model was proposed in 2017 and only works with natural language data. When the BERT model was released in 2018, it could only handle a max token sequence of length 512. Later in 2020, when GPT-3 was released, it could handle a sequence of lengths 2048 and 4096 at 3.5. All these models showed amazing performance in handling sequence-to-sequence and text-generation tasks.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ng"><img src="../Images/93f6207bcc1de07dfba8b303c8aa6e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsjPKzAg8ZVXsqi6aumXGQ.jpeg"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Image source: <a class="af nx" href="https://pxhere.com/en/photo/141620" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/141620</a></figcaption></figure><p id="389f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, these sequences were too short for images when tokens were taken at the pixel level. For example, in Cifar-100, the image size is 32* 32 = 1024 pixels. In ImageNet, the image size is 224* 224 = 50176 pixels. The sequence length would be an immediate barrier if the transformer were directly applied to the pixel level.</p><p id="2db3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The ViT paper was released in 2020. It proposed using patches rather than pixels as the input tokens. For an image of size 224* 224, using a patch size of 16* 16, the sequence length would be largely reduced to 196, which perfectly solved the issue.</p><p id="4e07" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">However, the issue was only partially solved. <strong class="mk fr">For tasks requiring features of finer details, different approaches have to be utilized to get the pixel-level accuracy back</strong>. Segformer proposed to fuse features from hierarchical transformer encoders of different resolutions. SwinIR had to combine CNN with multiple levels of skip connection around the transformer module for fine-grained feature extraction. The Swin Transformer, a model for universal computer vision tasks, started with a patch size 4*4 in each local window and then gradually built toward the global 16*16 patch size to obtain both globality and granularity. Intrinsically, these efforts pointed to one fact — simply using the 16*16 size patch is insufficient.</p><p id="e321" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The natural question is, can "pixel" be used as a direct token for transformers? The question further splits into two: 1. Is it possible to feed an ultralong sequence (e.g., 50k) to a transformer? 2. does feeding pixels as tokens provide more information than patches?</p><p id="32a0" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">In this article, I will summarize two recent papers: 1. <strong class="mk fr">Pixel Transformer</strong>, a technical report released by Meta AI last week, comparing pixel-wise tokens and patch-wise tokens to transformer models from the perspective of reducing the inductive bias of locality on three different tasks: image classification, pre-training, and generation. 2. <strong class="mk fr">Ultra-long sequence distributed transformer</strong>: by distributing the query vector, the authors showed the possibility to scale an input sequence of length 50k on 3k GPUs.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5cd9" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Pixel Transformer (PiT) — from an inductive bias perspective</strong></p><p id="6ad3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Meta AI released The technical report last week on arXiv: "<strong class="mk fr">An image is worth more than 16*16 patches”</strong>. Instead of proposing a novel method, the technical report answered a long-lasting question: Does it make sense to use pixels instead of patches as input tokens? If so, why?</p><p id="c5b6" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper took the perspective of the <strong class="mk fr">Inductive Bias of Locality</strong>. According to K. Murphy’s well-known <a class="af nx" href="https://mitpress.mit.edu/9780262018029/machine-learning/" rel="noopener ugc nofollow" target="_blank">machine learning book</a>, <strong class="mk fr">inductive bias</strong> is the “<em class="og">assumptions about the nature of the data distribution</em>.” In the early “non-deep learning” era, the inductive bias was more “feature-related,” coming from the manual feature engineered for specific tasks. This inductive bias was not a bad thing, especially for specific tasks in which very good prior knowledge from human experts is gained, making the engineered features very useful. However, from the generalization perspective, the engineered features are very hard to generalize to universal tasks, like general image classification and segmentation.</p><p id="043d" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">But beyond feature bias, the architecture itself contains inductive bias as well. The ViT is a great example showing less inductive bias than CNN models in terms of <strong class="mk fr">architecture hierarchy, propagation uniformness, representation scale, and attention locality</strong>. <a class="af nx" href="https://mengliuz.medium.com/paper-reading-do-vision-transformers-see-like-convolutional-neural-networks-94d4fdd85ff3" rel="noopener">See my previous medium post for a detailed discussion</a>. But still, ViT remains a special type of inductive bias — <strong class="mk fr">locality</strong>. When the ViT processes a sequence of patch tokens, the pixels within the same patch are naturally treated by the model differently than those from different patches. And that’s where the locality comes from.</p><p id="49ed" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">So, is it possible to remove the inductive bias of locality further? The answer is yes. The PiT proposed using the “pixel set” as input with different position embedding (PE) strategies: sin-cos, learnt, and none. It showed superior performance over ViT on supervised, self-supervised, and generation tasks. The proposed pipeline is shown in the figure below.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oh"><img src="../Images/7db566bffc4ae8c33bdb8c827f407ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPUBzJU8T1lZylTy9Glagg.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Pixel Transformer pipeline. Image source: <a class="af nx" href="https://arxiv.org/abs/2406.09415" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2406.09415</a></figcaption></figure><p id="611f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The idea seems simple and straightforward, and the authors claim they are “not introducing a new method” here. But still, the PiT shows great potential. On CIFAR-100 and ImageNet (reduced input size to 28*28) supervised classification tasks, the classification accuracy increased by more than 2% over ViT. See the table below.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oi"><img src="../Images/097eeadf6fa6b09160fd8a8c18143bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jGyQPEXpjHIixue4PtNT2Q.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Supervised learning classification. Image source: <a class="af nx" href="https://arxiv.org/pdf/2406.09415" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2406.09415</a></figcaption></figure><p id="d100" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Similar improvement was also observed in self-supervised learning tasks and image generation tasks. What’s more, the authors also showed the trend of a performance increase when reducing the patch size from 8*8 to 1*1 (single pixel) as below:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oj"><img src="../Images/b221c83fb05588849fe8f07208d9dec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yIcZUKQ1ZvckAYuR8-enHA.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Performance increase from ViT (8*8 patch) to PiT (1*1 patch). Image from: <a class="af nx" href="https://arxiv.org/pdf/2406.09415" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2406.09415</a></figcaption></figure><p id="5903" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">In terms of positional encoding.</strong></p><p id="9ff3" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">As pointed out in <a class="af nx" href="https://www.sciencedirect.com/science/article/abs/pii/S1047320322001845" rel="noopener ugc nofollow" target="_blank">this research paper</a>, positional encoding is a prerequisite in transformer-based models for input token sequence ordering and improving accuracy. However, the PiT shows that even after dropping the PE, the model performance drops is minimal:</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ok"><img src="../Images/f32198dc6792e2edbafdeaafb7df8a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPIIZZyqB1_7IGz527ODTw.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">PiT performance using three different PE: 1. fixed sin-cos PE; 2. learnable PE; 3. no PE. Image source: <a class="af nx" href="https://arxiv.org/pdf/2406.09415" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2406.09415</a></figcaption></figure><p id="9b67" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">Why drop the positional encoding? It is not only because dropping the positional encoding means a good reduction of the locality bias. If we think of self-attention computation in a distributed manner, it will largely reduce the cross-device communication effort, which we’ll discuss in detail in the next section.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2d38" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Ultra-long Sequences Transformers — a distributed query vector solution</strong></p><p id="c4f1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">The inductive bias of locality only told part of the story</strong>. If we look closely at the results in the PiT paper, we see that the experiments were limited to 28*28 resized images due to the computational limit. But the real world rarely uses images of such a small size. So the natural question is, even though PiT might be useful and outperform ViT, could it work on natural images of standard resolution, e.g., 244*244?</p><p id="8fe5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The paper “Ultra-long sequence distributed transformer,” released in 2023, answers the question. The paper proposed a solution to scale the transformer computation of a 50k-long sequence onto 3k GPUS.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf ol"><img src="../Images/70b5468468d4ea47bf4ca4644141f8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQ73ySJ1K8_S_vP7l_jC9Q.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Original transformer, baseline parallelization, and proposed ultra-long sequence transformer. Image source: <a class="af nx" href="https://arxiv.org/pdf/2311.02382" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2311.02382</a></figcaption></figure><p id="270f" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">The idea is simple: The transformer's bottleneck is self-attention computation. To ensure global attention computation, the proposed method <strong class="mk fr">only distributes the query vectors</strong> across different devices while maintaining the same copies of key and value vectors on all devices.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div class="ne nf om"><img src="../Images/472f0c435f3feb421a0b4ea20a4b965c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*KjCGXK72Lrr4c7du-huQFA.png"/></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">The equation for distributing self-attention calculation. The core idea is to distribute Q and keep the same copies of K and V. Image source: <a class="af nx" href="https://arxiv.org/pdf/2311.02382" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2311.02382</a></figcaption></figure><p id="c2b5" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">Positional Encoding-aware double gradient averaging</strong>. For architectures with learnable positional encoding parameters, gradient backpropagation is related to the positional encoding distribution. So, the authors proposed the double gradient averaging technique: when a gradient average is performed on two different segments from the same sequence, no positional encoding is involved, but when the corresponding segments from two sequences need to average gradients, the positional encoding parameters will be synced.</p><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf on"><img src="../Images/4dae73c9a02ec7f02e8dbde88032e6ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHNVj7Ae3LhnUPat2I9c7A.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Positional encoding-aware double gradient averaging. Image source: <a class="af nx" href="https://arxiv.org/pdf/2311.02382" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2311.02382</a></figcaption></figure><figure class="nh ni nj nk nl nm ne nf paragraph-image"><div role="button" tabindex="0" class="nn no ed np bh nq"><div class="ne nf oo"><img src="../Images/6c374168141f6f3ebf2daa014fd39012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RW_f9A0Gb5QmGEgChnl9gQ.png"/></div></div><figcaption class="ns nt nu ne nf nv nw bf b bg z dx">Maximal sequence length regarding the number of GPUs used. Image source: <a class="af nx" href="https://arxiv.org/pdf/2311.02382" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2311.02382</a></figcaption></figure></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7189" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk">When we combine these two papers, things become interesting. Reducing the inductive bias not only helps with model performance but also plays a crucial role in distributed computation.</p><ol class=""><li id="85ec" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd op oq or bk"><strong class="mk fr">Inductive bias of locality and device communication</strong>. The PiT paper shows the potential of building a positional encoding-free model by removing the locality bias. Furthermore, if we look at things from the distributed computing perspective, reducing the need for PE could further reduce the communication burden across devices.</li><li id="2a09" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd op oq or bk"><strong class="mk fr">Inductive bias of locality in distributed sequence computation</strong>. Distributing self-attention on multi-GPUs means the query vector is segment-based. There is a natural locality bias if the segment depends on contiguous tokens. The PiT computes on a “token set, " meaning there is no need for continuous segments, and it will make the query vector bias-free.</li></ol></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="cbd1" class="pw-post-body-paragraph mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd fj bk"><strong class="mk fr">References:</strong></p><ul class=""><li id="0d93" class="mi mj fq mk b go ml mm mn gr mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ox oq or bk">Nguyen et al., An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels. arXiv preprint 2024.</li><li id="0e9d" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Wang et al., Ultra long sequence distributed transformer. arXiv preprint 2023.</li><li id="1153" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Keles et al., On the computational complexity of self-attention. ALT 2023.</li><li id="40d6" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Jiang et al. The encoding method of position embeddings in vision transformer. Journal of Visual Communication and Image Representation. 2022.</li><li id="2287" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Xie et al., SegFormer: Simple and efficient design for semantic segmentation with transformers. NeurIPS 2021. Github: <a class="af nx" href="https://github.com/NVlabs/SegFormer" rel="noopener ugc nofollow" target="_blank">https://github.com/NVlabs/SegFormer</a></li><li id="6b31" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Liang et al., <a class="af nx" href="https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html" rel="noopener ugc nofollow" target="_blank">Swinir: Image restoration using swin transformer</a>. ICCV 2021. Github: <a class="af nx" href="https://github.com/JingyunLiang/SwinIR" rel="noopener ugc nofollow" target="_blank">https://github.com/JingyunLiang/SwinIR</a></li><li id="7ddd" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Liu et al., Swin Transformer: Hierarchical vision transformer using shifted windows. ICCV 2021. Github: <a class="af nx" href="https://github.com/microsoft/Swin-Transformer" rel="noopener ugc nofollow" target="_blank">https://github.com/microsoft/Swin-Transformer</a></li><li id="35ba" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Dosovitsckiy et al., An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint, 2020.</li><li id="909b" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Brown et al., Language models are few-shot learners. NeurIPS 2020. Github: <a class="af nx" href="https://github.com/openai/gpt-3" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/gpt-3</a></li><li id="e794" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Devlin et al., BERT: Pre-training of deep bidirectional transformers for language understanding. <em class="og">arXiv preprint 2018</em>. HuggingFace Official: <a class="af nx" href="https://huggingface.co/docs/transformers/en/model_doc/bert" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/docs/transformers/en/model_doc/bert</a></li><li id="d554" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Vaswani et al., Attention is all You need. NeurIPS 2017.</li><li id="ef10" class="mi mj fq mk b go os mm mn gr ot mp mq mr ou mt mu mv ov mx my mz ow nb nc nd ox oq or bk">Murphy, Machine learning: a probabilistic perspective. MIT press 2012.</li></ul></div></div></div></div>    
</body>
</html>