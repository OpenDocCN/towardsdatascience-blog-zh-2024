- en: The Math Behind Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-recurrent-neural-networks-2de4e0098ab8?source=collection_archive---------1-----------------------#2024-04-27](https://towardsdatascience.com/the-math-behind-recurrent-neural-networks-2de4e0098ab8?source=collection_archive---------1-----------------------#2024-04-27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dive into RNNs, the backbone of time series, understand their mathematics, implement
    them from scratch, and explore their applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--2de4e0098ab8--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--2de4e0098ab8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2de4e0098ab8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2de4e0098ab8--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--2de4e0098ab8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2de4e0098ab8--------------------------------)
    ·22 min read·Apr 27, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86a8ac61de3ca0810a2b5a964dd666a1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: RNNs stand out from other types of neural networks because they handle sequences
    of inputs. This capability allows them to take on tasks that depend on the order
    of the data, such as forecasting stock market trends, monitoring patient health
    over time, or predicting the next word in a sentence. This makes them extremely
    valuable for many cutting-edge AI applications. In this article, we will dive
    into their architecture and math and create them from scratch in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '**Index**'
  prefs: []
  type: TYPE_NORMAL
- en: '[**1: Introduction**](#357e)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**2: RNN’s Architecture**](#d214)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [2.1: The Structure of RNNs](#a6c0)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [2.2: Key Operations in RNNs](#2ec6)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**3: Challenges in Training RNNs**](#43fa)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.1: Vanishing Gradients](#3d31)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.2: Exploding Gradients](#5f2b)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.3: Gradient Clipping](#4563)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [3.4: Adjusted Initialization Strategies](#fffa)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**4: Building RNN from Scratch**](#8517)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [4.1: Defining the RNN Class](#2ed9)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [4.2: Early Stopping Mechanism](#e81e)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [4.3: RNN Trainer Class:](#01f1)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [4.4: Data Loading and](#17c5)…'
  prefs: []
  type: TYPE_NORMAL
