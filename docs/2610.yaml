- en: Untangling AI Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/untangling-ai-systems-d0dcfa3a04eb?source=collection_archive---------4-----------------------#2024-10-26](https://towardsdatascience.com/untangling-ai-systems-d0dcfa3a04eb?source=collection_archive---------4-----------------------#2024-10-26)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How physics can help us understand neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://javier-marin.medium.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)[![Javier
    Marin](../Images/31800b2fbfd1f7c841c9f6a2579d5681.png)](https://javier-marin.medium.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)
    [Javier Marin](https://javier-marin.medium.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d0dcfa3a04eb--------------------------------)
    ·13 min read·Oct 26, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d9ff64d6133948ec3f46afd682c6f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by author using DALL-E
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we could open up an AI system and find a well-organized factory of
    components that work together? The article explores a new approach that combines
    two powerful concepts: sparse neural circuits and physics-inspired mathematics.
    By combining these different areas, we could find new approaches for analyzing
    and building AI systems. While neural networks appear to be elusive black boxes,
    researchers have uncovered something fascinating: they contain interpretable “circuits”
    that function similarly to machine components. Let me explain in simple terms.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural Circuits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What if, instead of trying to understand an entire neural network at once, we
    could examine it piece by piece, just as biologists study individual cells and
    neural pathways? This approach, inspired by neurology and cellular biology, was
    pioneered by [Chris Olah](https://www.anthropic.com/research#interpretability)
    in 2018, offering a more thorough way to understand neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb43cebb8565de35ea71fd1810db7b2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure from [https://distill.pub/2020/circuits/zoom-in/](https://distill.pub/2020/circuits/zoom-in/)
  prefs: []
  type: TYPE_NORMAL
- en: 'Think about how we recognize a dog in a picture. Our brain processes different
    features: the curve of the ears, the texture of the fur, the roundness of the
    eyes. Neural networks…'
  prefs: []
  type: TYPE_NORMAL
