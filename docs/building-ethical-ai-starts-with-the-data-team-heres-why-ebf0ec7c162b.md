# 构建伦理人工智能从数据团队开始——这是为什么

> 原文：[https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20](https://towardsdatascience.com/building-ethical-ai-starts-with-the-data-team-heres-why-ebf0ec7c162b?source=collection_archive---------5-----------------------#2024-03-20)

## 生成性人工智能是一个伦理困境。数据负责人在其中应承担什么责任？本文将探讨伦理人工智能的必要性，以及为什么数据伦理就是人工智能伦理。

[](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Barr Moses](../Images/4c74558ee692a85196d5a55ac1920718.png)](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------) [Barr Moses](https://barrmoses.medium.com/?source=post_page---byline--ebf0ec7c162b--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ebf0ec7c162b--------------------------------) ·9分钟阅读·2024年3月20日

--

![](../Images/c864bd33c0bb4f391c469b5a237f6fa7.png)

图片由 aniqpixel 提供，来源于 [Shutterstock](https://www.shutterstock.com/g/aniqpixel)。

在科技竞赛中，迅速行动一直是未来成功的标志。

不幸的是，行动过快也意味着我们可能会忽视潜伏的危险。

这是一个古老的故事。你一会儿还在测序史前蚊子的基因，下一秒你就要开设恐龙主题公园，设计世界上第一个失败的超级高铁（但肯定不会是最后一个）。

当谈到生成性人工智能（GenAI）时，生活仿佛在模仿艺术。

无论我们多么希望认为人工智能是一种已知的技术，残酷的现实是，[甚至这个技术的创造者们也不能完全确定它是如何工作的](https://www.scientificamerican.com/article/how-ai-knows-things-no-one-told-it/)。

在[联合健康](https://www.forbes.com/sites/douglaslaney/2023/11/16/ai-ethics-essentials-lawsuit-over-ai-denial-of-healthcare/?sh=30a0094e3ac6)、[谷歌](https://www.businessinsider.com/google-gemini-firestorm-big-tech-ai-arms-race-2024-3)甚至[加拿大法院](https://vancouversun.com/news/local-news/fake-case-law-in-b-c-divorce-court-points-up-pitfalls-with-ai-tools-for-lawyers)等高调的人工智能失误事件之后，是时候考虑我们在哪些地方出了问题。

现在，明确一点，我相信生成式人工智能（以及更广泛的人工智能）最终将对每个行业至关重要——从加速工程工作流程到回答常见问题。然而，要实现人工智能的潜在价值，我们首先必须开始批判性地思考*如何*开发人工智能应用——以及数据团队在其中的角色。

在本文中，我们将探讨人工智能的三个伦理问题，数据团队的参与方式，以及作为数据领导者的你今天可以做些什么，以提供更加伦理和可靠的人工智能，为未来铺路。

# 人工智能伦理的三层次

当我与我的同事Shane Murray——前纽约时报数据与洞察高级副总裁——聊天时，他分享了他第一次遇到真正的伦理困境的经历。在为纽约时报开发一个关于财务激励的机器学习模型时，讨论提出了一个问题：一个能够决定折扣的机器学习模型的伦理影响。

从表面上看，折扣码的机器学习模型似乎是一个相对无害的请求，考虑到所有因素。但是，尽管自动化一些折扣码看起来无害，但从商业问题中剔除人类的同理心，给团队带来了各种伦理上的考虑。

自动化简单但传统上由人类完成的活动，似乎是一个纯粹的务实决策——一个简单的二元选择：是提高效率，还是不提高效率。但一旦你从任何方程中剔除人类的判断，不论是否涉及人工智能，你也失去了直接管理该过程对人类产生的影响的能力。

这是一个真实的问题。

在人工智能开发中，有三个主要的伦理考虑：

**1. 模型偏差**

这正是我们在纽约时报讨论的核心问题。模型本身是否会带来一些未预见的后果，可能会使某个人相对于其他人占优势或处于不利地位？

这里的挑战是，要设计出一种生成式人工智能，使得——在其他考虑因素相同的情况下——它能够在每次互动中持续提供公平和公正的输出。

**2. 人工智能的使用**

无疑，人工智能伦理考量中最具存在性——也最有趣的——是理解[技术将如何被使用](https://www.wired.com/story/ai-generated-voices-robocalls-illegal-fcc/)，以及这种使用场景可能对公司或社会带来的影响。

这个人工智能是为了伦理目的而设计的吗？它的使用是否会直接或间接地伤害任何个人或群体？最终，这个模型是否会在长期内带来净收益？

正如伊恩·马尔科姆博士在《侏罗纪公园》第一幕中深刻定义的那样，仅仅因为你能建造某样东西，并不意味着你应该建造它。

**3. 数据责任**

最后，数据团队最重要的关切（也是我将在本文中大部分时间讨论的内容）是：数据本身如何影响人工智能的构建和负责任使用？

这个问题涉及理解我们使用的数据，在哪些情况下它可以安全使用，以及与之相关的风险。

比如，我们是否知道数据来自哪里，以及它是如何获取的？为某个特定模型提供数据是否存在隐私问题？我们是否在利用任何个人数据，这些数据可能让个体面临不必要的伤害风险？

在不知道它被训练用的是什么数据的情况下，构建在一个封闭源 LLM 上是否安全？

正如[《纽约时报》对 OpenAI 提起的诉讼](https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html)中所强调的——我们是否有权使用这些数据？

这也是我们数据的*质量*发挥作用的地方。我们能否信任供给特定模型的数据的可靠性？如果质量问题未被解决，允许它们进入 AI 生产环境，可能会产生什么后果？

既然我们已经从 30,000 英尺的高度审视了这些伦理问题，让我们考虑一下数据团队在其中的责任。

# 为什么数据团队要对 AI 伦理负责

在所有与数据团队相关的伦理 AI 考虑中，最突出的问题无疑是**数据责任**。

就像 GDPR 强迫业务和数据团队合作，重新思考数据是如何被收集和使用的一样，GenAI 将迫使公司重新思考哪些工作流程可以——而哪些不能——被自动化。

尽管作为数据团队，我们确实有责任参与构建任何 AI 模型，但我们无法直接影响其设计结果。然而，通过避免将错误的数据放入该模型，我们可以在很大程度上缓解这些设计缺陷所带来的风险。

如果模型本身超出了我们的控制范围，那么关于*能否*和*是否应该*的问题就完全是另一个层次了。再次强调，我们有责任在发现问题时指出，但最终，火箭无论我们是否上船，都会发射。

我们能做的最重要的事情是确保火箭安全发射。（或者偷走飞机的机身。）

所以——就像数据工程师生活中的所有领域一样——我们想花费时间和精力的地方，正是我们能为最多人带来最大直接影响的地方。而这个机会就在数据本身。

# 为什么数据责任对数据团队至关重要

这似乎太显而易见了，但我还是要说一遍：

数据团队需要对数据如何被用于 AI 模型中负责，因为说实话，他们是唯一能够做到这一点的团队。当然，也有合规团队、安全团队，甚至是法律团队会在忽视伦理时承担责任。但无论责任如何分担，最终，这些团队永远无法像数据团队一样深入理解数据。

想象一下，你的软件工程团队使用OpenAI或Anthropic的第三方LLM创建了一个应用程序，但没有意识到你们正在追踪和存储位置数据——除了他们实际上需要的应用数据外，他们还利用了整个数据库来支持模型。若逻辑上存在缺陷，恶意行为者可能会轻松构造一个提示语，利用存储在数据集中的数据追踪任何个人。（这正是[开源与闭源LLM之间的张力](https://www.montecarlodata.com/blog-the-moat-for-enterprise-ai-is-rag-fine-tuning/)）

比如，假设软件团队知道那个位置数据，但他们没有意识到这个位置数据实际上可能是近似的。他们可能使用这些位置数据创建AI地图技术，而无意间导致一名16岁的少年晚上走进一条黑暗的巷子，而不是走到街角的必胜客。当然，这种错误并非故意的，但它突显了数据使用中固有的意外风险。

这些例子和其他类似的案例凸显了数据团队在伦理AI方面作为“看门人”的角色。

# 那么，数据团队如何保持伦理性呢？

在大多数情况下，数据团队习惯于处理近似数据和代理数据，以使他们的模型正常工作。但当涉及到为AI模型提供数据时，实际上你需要更高水平的验证。

为了有效地为消费者站稳脚跟，数据团队需要有意识地审视自己的数据实践，以及这些实践与整个组织的关系。

在我们考虑如何减轻AI的风险时，以下是数据团队必须采取的三步措施，以推动AI走向更加伦理的未来。

# 1. 获取席位

数据团队不是鸵鸟——他们不能埋头沙里，希望问题会消失。就像数据团队曾为获得领导层席位而奋斗一样，数据团队还需要为在AI领域中争取到一个席位而努力。

就像任何数据质量的应急演练一样，事后再跳进混战并不足够。当我们面对生成型AI所固有的存在性风险时，比以往任何时候都更需要主动应对我们个人的责任。

如果他们不让你坐在桌子旁，那么你有责任从外部进行教育。竭尽全力提供出色的发现、治理和数据质量解决方案，以便为那些掌舵的团队提供信息，使他们能够做出关于数据的负责任决策。教他们什么时候使用什么工具，并说明无法通过你们团队内部协议验证的第三方数据的使用风险。

这不仅仅是一个商业问题。正如United Healthcare和不列颠哥伦比亚省所证明的那样，在许多情况下，这些事关的是人们的生命——和生计——。因此，让我们确保从这个角度来操作。

# 2. 利用像 RAG 这样的方式策划更负责任的 — 以及更可靠的 — 数据

我们常常将检索增强生成（RAG）视为从 AI 中创造价值的一种资源。但它同样也是一项资源，能保障如何构建和使用该 AI。

例如，假设一个模型正在访问私人客户数据，并将其用于面向消费者的聊天应用。一个正确的用户提示可能会让各种关键的个人身份信息（PII）泄露出来，供不法分子利用。因此，验证和控制这些数据来源的能力对于保护 AI 产品的完整性至关重要。

有经验的数据团队通过利用像 RAG 这样的方式，大大降低了这些风险，精心策划符合规范、更安全、更适合模型的数据。

采取 RAG 方法开发 AI 还帮助最小化与摄取*过多*数据相关的风险 — 就像我们在位置数据的例子中提到的那样。

那么，实践中这看起来是什么样的呢？假设你是一家像 Netflix 这样的媒体公司，需要利用一定程度的客户数据和自有内容数据来创建个性化推荐模型。一旦你定义了该用例的特定 — 且有限 — 数据点，你就能更有效地定义：

1.  谁负责维护和验证这些数据，

1.  在什么情况下这些数据可以安全使用，

1.  而且，谁最适合随着时间的推移来构建和维护这个 AI 产品。

像数据血缘（data lineage）这样的工具也能派上用场，它能帮助团队快速验证数据的来源，以及这些数据在团队的 AI 产品中是如何被使用 — 或误用 — 的。

# 3. 优先考虑数据可靠性

当我们谈论数据产品时，我们常常说“垃圾进，垃圾出”，但在生成式 AI（GenAI）的情况下，这个格言有些不完全准确。实际上，当垃圾数据输入到 AI 模型中时，输出的就不仅仅是垃圾 — 它还会带来真正的人类后果。

这就是为什么，除了需要一个 RAG 架构来控制输入模型的数据外，你还需要强大的[数据可观察性](https://www.montecarlodata.com/blog-what-is-data-observability/)，并连接到像[Pinecone](https://www.montecarlodata.com/blog-pinecone-vector-database-observability-announcement)这样的向量数据库，确保数据实际上是干净、安全和可靠的。

我从开始使用 AI 的客户那里听到的最常见的抱怨之一是，如果你没有积极监控向向量数据管道中索引的输入数据，就几乎不可能验证数据的可信度。

事与愿违，数据和 AI 工程师往往只有在模型输出错误的提示响应时，才知道数据出了问题 — 而那时，已经为时已晚。

# 现在正是最佳时机

对更高数据可靠性和可信度的需求正是促使我们团队在2019年创建数据可观测性类别的动力。

今天，随着人工智能承诺颠覆我们日常依赖的许多过程和系统，数据质量的挑战——更重要的是，数据质量的伦理影响——变得更加严峻。
