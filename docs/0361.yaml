- en: '3 Key Encoding Techniques for Machine Learning: A Beginner-Friendly Guide with
    Pros, Cons, and Python Code Examples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07](https://towardsdatascience.com/3-key-encoding-techniques-for-machine-learning-a-beginner-friendly-guide-aff8a01a7b6a?source=collection_archive---------1-----------------------#2024-02-07)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How should we choose between label, one-hot, and target encoding?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Ryu
    Sonoda](../Images/52445252872ed381dd86d3ada5665e1b.png)](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    [Ryu Sonoda](https://medium.com/@ryuryu09030903?source=post_page---byline--aff8a01a7b6a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--aff8a01a7b6a--------------------------------)
    ·15 min read·Feb 7, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '**Why Do We Need Encoding?**'
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of machine learning, most algorithms demand inputs in numeric form,
    especially in many popular Python frameworks. For instance, in scikit-learn, linear
    regression, and neural networks require numerical variables. This means we need
    to transform categorical variables into numeric ones for these models to understand
    them. However, this step isn’t always necessary for models like tree-based ones.
  prefs: []
  type: TYPE_NORMAL
- en: Today, I’m thrilled to introduce three fundamental encoding techniques that
    are essential for every budding data scientist! Plus, I’ve included a practical
    tip to help you see these techniques in action at the end! Unless stated, all
    the codes and pictures are created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Label Encoding / Ordinal Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Both label encoding and ordinal encoding involve assigning integers to different
    classes. The distinction lies in whether the categorical variable inherently has
    an order. For example, responses like ‘strongly agree,’ ‘agree,’ ‘neutral,’ ‘disagree,’
    and ‘strongly disagree’ are ordinal as they follow a specific sequence. When a
    variable doesn’t have such an order, we use label encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s delve into **label encoding**.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve prepared a synthetic dataset with math test scores and students’ favorite
    subjects. This dataset is designed to reflect higher scores for students who prefer
    STEM subjects. The following code shows how it is synthesized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/425df2f19fb2c7f82ab651ebec06fa83.png)'
  prefs: []
  type: TYPE_IMG
- en: You’ll be amazed at how simple it is to encode your data — it takes just a single
    line of code! You can pass a dictionary that maps between the subject name and
    number to the default method of the pandas dataframe like the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/669fe3aa0c74e4e450d749624c259f84.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoded manually
  prefs: []
  type: TYPE_NORMAL
- en: But what if you’re dealing with a vast array of classes, or perhaps you’re looking
    for a more straightforward approach? That’s where the **scikit-learn** library’s
    `**LabelEncoder**` function comes in handy. It automatically encodes your classes
    based on their alphabetical order. For the best experience, I recommend using
    version 1.4.0, which supports all the encoders we’re discussing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c834b34b76ab0d0baaf65601dac39920.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoded using scikit-learn library
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there’s a catch. Consider this: our dataset doesn’t imply an ordinal
    relationship between favorite subjects. For instance, ‘History’ is encoded as
    0, but that doesn’t mean it’s ‘inferior’ to ‘Math,’ which is encoded as 3\. Similarly,
    the numerical gap between ‘English’ and ‘Science’ is smaller than that between
    ‘English’ and ‘History,’ but this doesn’t necessarily reflect their relative similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: This encoding approach also affects interpretability in some algorithms. For
    example, in linear regression, each coefficient indicates the expected change
    in the outcome variable for a one-unit change in a predictor. But how do we interpret
    a ‘unit change’ in a subject that’s been numerically encoded? Let’s put this into
    perspective with a linear regression on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/283806185bd22927dfab305bad5004c1.png)'
  prefs: []
  type: TYPE_IMG
- en: How can we interpret the coefficient 8.26 here? The naive way would be when
    the label changes by 1 unit, the test score changes by 8\. However, it is not
    really true from Science (encoded as 1) to History (encoded as 2) since I synthesized
    in a way that the mean score would be 80 and 70 respectively. So, we should not
    interpret the coefficient when there is no meaning in the way we label each class!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, moving on to **ordinal encoding**, let’s apply it to another synthetic
    dataset, this time focusing on height and school categories. I’ve tailored this
    dataset to reflect average heights for different school levels: 110 cm for kindergarten,
    140 cm for elementary school, and so on. Let’s see how this plays out.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d812fbef79a68c55496d48fc51d143cb.png)'
  prefs: []
  type: TYPE_IMG
- en: A part of the synthesized shcool height data
  prefs: []
  type: TYPE_NORMAL
- en: The `**OrdinalEncoder**` from scikit-learn’s preprocessing toolkit is a real
    gem for handling ordinal variables. It’s intuitive, automatically determining
    the ordinal structure and encoding it accordingly. If you look at encoder.categories_,
    you can check how the variable was encoded.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6bfbcd0470e04147051ae7fb70ebdc82.png)'
  prefs: []
  type: TYPE_IMG
- en: After encoded
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to ordinal categorical variables, interpreting linear regression
    models becomes more straightforward. The encoding reflects the degree of education
    in a numerical order — the higher the education level, the higher its corresponding
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/14d3d127f4382cfa23145eabdf63eb02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model reveals something quite intuitive: a one-unit change in school type
    corresponds to a 17.5 cm increase in height. This makes perfect sense given our
    dataset!'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let’s wrap up with a quick summary of **label/ordinal** encoding:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Simplicity: It’s user-friendly and easy to implement.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Efficiency: This method is light on computational resources and memory, creating
    just one new numerical feature.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Ideal for Ordinal Categories: It shines when dealing with categorical variables
    that have a natural order.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Implied Order: One potential downside is that it can introduce a sense of
    order where none exists, potentially leading to misinterpretation (like assuming
    a category labeled ‘3’ is superior to one labeled ‘2’).'
  prefs: []
  type: TYPE_NORMAL
- en: '- Not Always Suitable: Certain algorithms, such as linear or logistic regression,
    might incorrectly interpret the encoded numerical values as having ordinal significance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-hot encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, let’s dive into another encoding technique that addresses the interpretability
    issue: **One-hot encoding**.'
  prefs: []
  type: TYPE_NORMAL
- en: The core issue with label encoding is that it imposes an ordinal structure on
    variables that don’t inherently have one, by replacing categories with numerical
    values. **One-hot encoding tackles this by creating a separate column for each
    class. Each of these columns contains binary values, indicating whether the row
    belongs to that class.** It’s like pivoting the data to a wider format, for those
    who are familiar with that concept. To make this clearer, let’s see an example
    using the math_score and subject data. The `**OneHotEncoder**` from sklearn.preprocessing
    is perfect for this task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ff50f995059ddc727f541bdec6365237.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoded by one-hot encoding
  prefs: []
  type: TYPE_NORMAL
- en: Now, instead of having a single ‘Subject’ column, our dataset features individual
    columns for each subject. This effectively eliminates any unintended ordinal structure!
    However, the process here is a bit more involved, so let me explain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like with label/ordinal encoding, you first need to define your encoder. But
    the output of one-hot encoding differs: while label/ordinal encoding returns a
    numpy array, one-hot encoding typically produces a `scipy.sparse._csr.csr_matrix`.
    To integrate this with a pandas dataframe, you’ll need to convert it into an array.
    Then, create a new dataframe with this array and assign column names, which you
    can get from the encoder’s `get_feature_names_out()` method. Alternatively, you
    can get numpy array directly by setting `sparse_output=False` when defining the
    encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: However, in practical applications, you don’t need to go through all these steps.
    I’ll show you a more streamlined approach using `**make_column_transformer**`
    towards the end of our discussion!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s proceed with running a linear regression on our one-hot encoded data.
    This should make the interpretation much easier, right?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a4ff2048605cc43f2b11c754ec38397a.png)'
  prefs: []
  type: TYPE_IMG
- en: Intercept and coefficients for each column
  prefs: []
  type: TYPE_NORMAL
- en: But wait, why are the coefficients so tiny, and the intercept so large? What’s
    going wrong here? This conundrum is a specific issue in linear regression known
    as perfect multicollinearity. Perfect multicollinearity occurs when when one variable
    in a linear regression model can be perfectly predicted from the others, which
    in the case of one-hot encoding happens because one class can be inferred if all
    other classes are zero. To sidestep this problem, we can drop one of the classes
    by setting `OneHotEncoder(drop=”first”)`. Let’s check out the impact of this adjustment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0900f84041b8739332f51a45eb66a95f.png)'
  prefs: []
  type: TYPE_IMG
- en: Intercept and coeffcients for each column with dropping one column
  prefs: []
  type: TYPE_NORMAL
- en: Here, the column for English has been dropped, and now the coefficients seem
    much more reasonable! Plus, they’re easier to interpret. When all the one-hot
    encoded columns are zero (indicating English as the favorite subject), we predict
    the test score to be around 71 (aligned with our defined average score for English).
    For History, it would be 71 minus 11 equals 60, for Math, 71 plus 19, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there’s a significant caveat with one-hot encoding: it can lead to
    high-dimensional datasets, especially when the variable has a large number of
    classes. Let’s consider a dataset that includes 1000 rows, each representing a
    unique product with various features, including a category that spans 100 different
    types.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1ac7b8734b85d8db3f69932a87037dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: The synthesized dataset for products
  prefs: []
  type: TYPE_NORMAL
- en: Note that the dataset’s dimensions are 1000 rows by 5 columns. Now, let’s observe
    the changes after applying a one-hot encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/925f370c09ba99ca0a025fc32e89fdbf.png)'
  prefs: []
  type: TYPE_IMG
- en: The dimension increased significantly!
  prefs: []
  type: TYPE_NORMAL
- en: After applying one-hot encoding, our dataset’s dimension balloons to 1000x201
    — a whopping 40 times larger than before. This increase is a concern, as it demands
    more memory. Moreover, you’ll notice that most of the values in the newly created
    columns are zeros, resulting in what we call a sparse dataset. Certain models,
    especially tree-based ones, struggle with sparse data. Furthermore, other challenges
    arise when dealing with high-dimensional data often referred to as the ‘curse
    of dimensionality.’ Also, since one-hot encoding treats each class as an individual
    column, we lose any ordinal information. Therefore, if the classes in your variable
    inherently have a hierarchical order, one-hot encoding might not be your best
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: How do we tackle these disadvantages? One approach is to use a different encoding
    method. Alternatively, you can limit the number of classes in the variable. Often,
    even with a large number of classes, the majority of values for a variable are
    concentrated in just a few classes. In such cases, treating these minority classes
    as ‘others’ can be effective. This can be achieved by setting parameters like
    `**min_frequency**` or `**max_categories**` in OneHotEncoder. Another strategy
    for dealing with sparse data involves techniques like feature hashing, which essentially
    simplifies the representation by mapping multiple categories to a lower-dimensional
    space using a hash function, or dimension reduction techniques like PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick summary of **One-hot encoding**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Prevents Misleading Interpretations: It avoids the risk of models misinterpreting
    the data as having some sort of order, an issue prevalent in label/target encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Suitable for Non-Ordinal Features: Ideal for categorical data without an
    ordinal relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Dimensionality Increase: Leads to a significant increase in the dataset’s
    dimensionality, which can be problematic, especially for variables with many categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Sparse Matrix: Results in many columns filled with zeros, creating sparse
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Not Efficient with High Cardinality Features: Less effective for variables
    with a large number of categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore target encoding, a technique particularly effective with high-cardinality
    data and in models like tree-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The essence of target encoding is to leverage the information from the value
    of the dependent variable. Its implementation varies depending on the task. In
    regression, we encode the target variable by the mean of the dependent variable
    for each class. For binary classification, it’s done by encoding the target variable
    with the probability of being in one class (calculated as the number of rows in
    that class where the outcome is 1, divided by the total number of rows in the
    class). In multiclass classification, the categorical variable is encoded based
    on the probability of belonging to each class, resulting in as many new columns
    as there are classes in the dependent variable. To clarify, let’s use the same
    product dataset we employed for one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with target encoding for a regression task. Imagine we want to predict
    the price of goods and aim to encode the product type. Similar to other encodings,
    we use **TargetEncoder** from sklearn.preprocessing!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2bb9fff8787574918ac3354c70c58f01.png)'
  prefs: []
  type: TYPE_IMG
- en: Target encoding for regression
  prefs: []
  type: TYPE_NORMAL
- en: After the encoding, you’ll notice that, despite the variable having many classes,
    the dataset’s dimension remains unchanged (1000 x 5). You can also observe how
    each class is encoded. Although I mentioned that the encoding for each class is
    based on the mean of the target variable for that class, you’ll find that the
    actual mean differs slightly from the encoding using the default settings. This
    discrepancy arises because, by default, the function automatically selects a smoothing
    parameter. This parameter blends the local category mean with the overall global
    mean, which is particularly useful to prevent overfitting in categories with limited
    samples. If we set `smooth=0`, the encoded values align precisely with the actual
    means.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s consider binary classification. Imagine our goal is to classify whether
    the quality of a product is satisfactory. In this scenario, the encoded value
    represents the probability of a category being ‘satisfactory.’
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f79e8ec250bc34257fd730c1849880c.png)'
  prefs: []
  type: TYPE_IMG
- en: Target encoding for binary classification
  prefs: []
  type: TYPE_NORMAL
- en: You can indeed see that the encoded_category represent the probability being
    “Satisfied” (float value between 0 and 1). To see how each class is encoded, you
    can check the `classes_` attribute of the encoder. For binary classification,
    the first value in the list is typically dropped, meaning that the column here
    indicates the probability of being satisfied. Conveniently, the encoder automatically
    detects the type of task, so there’s no need to specify that it’s a binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, let’s see multi-class classification example. Suppose we’re predicting
    which manufacturer produced a product.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2bdc98734f9d85e95b1efc30c5c275a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Target encoding for multi-class classification
  prefs: []
  type: TYPE_NORMAL
- en: After encoding, you’ll see that we now have columns for each manufacturer. These
    columns indicate the probability of a product belonging to a certain category
    being produced by that manufacturer. Although our dataset has expanded slightly,
    the number of classes for the dependent variable is usually much smaller, so it’s
    unlikely to cause issues.
  prefs: []
  type: TYPE_NORMAL
- en: Target encoding is particularly advantageous for tree-based models. These models
    make splits based on feature values that most effectively separate the target
    variable. By directly incorporating the mean of the target variable, target encoding
    provides a clear and efficient means for the model to make these splits, often
    more so than other encoding methods.
  prefs: []
  type: TYPE_NORMAL
- en: However, caution is needed with target encoding. If there are only a few observations
    for a class, and these don’t represent the true mean for that class, there’s a
    risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to another crucial point: it’s vital to perform target encoding
    after splitting your data into training and testing sets. Doing it beforehand
    can lead to data leakage, as the encoding would be influenced by the outcomes
    in the test dataset. This could result in the model performing exceptionally well
    on the training dataset, giving you a false impression of its efficacy. Therefore,
    to accurately assess your model’s performance, ensure target encoding is done
    post train-test split.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a quick summary of **target encoding**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pros:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Keeps Cardinality in Check: It’s highly effective for high cardinality features
    as it doesn’t increase the feature space.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Can Capture Information Within Labels: By incorporating target data, it often
    enhances predictive performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Useful for Tree-Based Models: Particularly advantageous for complex models
    such as random forests or gradient boosting machines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Risk of Overfitting: There’s a heightened risk of overfitting, especially
    when categories have a limited number of observations.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Target Leakage: It may inadvertently introduce future information into the
    model, i.e., details from the target variable that wouldn’t be accessible during
    actual predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Less Interpretable: Since the transformations are based on the target, they
    can be more challenging to interpret compared to methods like one-hot or label
    encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Final tip**'
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up, I’d like to offer some practical tips. Throughout this discussion,
    we’ve looked at different encoding techniques, but in reality, you might want
    to apply various encodings to different variables within a dataset. This is where
    `**make_column_transformer**` from sklearn.compose comes in handy. For example,
    suppose you’re predicting product prices and decide to use target encoding for
    the ‘Category’ due to its high cardinality, while applying one-hot encoding for
    ‘Manufacturer’ and ‘Quality’. To do this, you would define arrays containing the
    names of the variables for each encoding type and apply the function as shown
    below. This approach allows you to handle the transformed data seamlessly, leading
    you to an efficiently encoded dataset ready for your analyses!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3cb2e44c1f3d49351b860bced3a3f104.png)'
  prefs: []
  type: TYPE_IMG
- en: Combination of target and one-hot encoding using make_column_faster
  prefs: []
  type: TYPE_NORMAL
- en: Thank you so much for taking the time to read through this! When I first embarked
    on my machine learning journey, choosing the right encoding techniques and understanding
    their implementation was quite a maze for me. I genuinely hope this article has
    shed some light for you and made your path a bit clearer!
  prefs: []
  type: TYPE_NORMAL
- en: '**Source:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825–2830,
    2011.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation of Scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ordinal encoder: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Target encoder: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html#sklearn.preprocessing.TargetEncoder)'
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoder [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)
  prefs: []
  type: TYPE_NORMAL
