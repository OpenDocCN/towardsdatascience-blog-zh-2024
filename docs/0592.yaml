- en: Comparison of Methods to Inform K-Means Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/comparison-of-methods-to-inform-k-means-clustering-a830cdc8db50?source=collection_archive---------3-----------------------#2024-03-04](https://towardsdatascience.com/comparison-of-methods-to-inform-k-means-clustering-a830cdc8db50?source=collection_archive---------3-----------------------#2024-03-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Brief Tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cjtayl2?source=post_page---byline--a830cdc8db50--------------------------------)[![Chris
    Taylor](../Images/a5a0b096777cc262cc5adc3350fadab4.png)](https://medium.com/@cjtayl2?source=post_page---byline--a830cdc8db50--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--a830cdc8db50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--a830cdc8db50--------------------------------)
    [Chris Taylor](https://medium.com/@cjtayl2?source=post_page---byline--a830cdc8db50--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--a830cdc8db50--------------------------------)
    ·12 min read·Mar 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43b07d07fdaae134bb91af08c2cb97e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nabeel Hussain](https://unsplash.com/@nabeelhussainphotos?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: K-Means is a popular unsupervised algorithm for clustering tasks. Despite its
    popularity, it can be difficult to use in some contexts due to the requirement
    that the number of clusters (or k) be chosen before the algorithm has been implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Two quantitative methods to address this issue are the elbow plot and the silhouette
    score. Some authors regard the elbow plot as “coarse” and recommend data scientists
    use the silhouette score [1]. Although general advice is useful in many situations,
    it is best to evaluate problems on a case-by-case basis to determine what is best
    for the data.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this article is to provide a tutorial on how to implement k-means
    clustering using an elbow plot and silhouette score and how to evaluate their
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Google Colab notebook containing the code reviewed in this article can be
    accessed through the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://colab.research.google.com/drive/1saGoBHa4nb8QjdSpJhhYfgpPp3YCbteU?usp=sharing](https://colab.research.google.com/drive/1saGoBHa4nb8QjdSpJhhYfgpPp3YCbteU?usp=sharing)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Description of Data**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Seeds dataset was originally published in a study by Charytanowiscz et al.
    [2] and can be accessed through the following link [https://archive.ics.uci.edu/dataset/236/seeds](https://archive.ics.uci.edu/dataset/236/seeds)
  prefs: []
  type: TYPE_NORMAL
- en: The dataset is comprised of 210 entries and eight variables. One column contains
    information about a seed’s variety (i.e., 1, 2, or 3) and seven columns contain
    information about the geometric properties of the seeds. The properties include
    (a) area, (b) perimeter, (c) compactness, (d) kernel length, (e) kernel width,
    (f) asymmetry coefficient, and (g) kernel groove length.
  prefs: []
  type: TYPE_NORMAL
- en: Before building the models, we’ll need to conduct an exploratory data analysis
    to ensure we understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory Data Analysis**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll start by loading the data, renaming the columns, and setting the column
    containing seed variety to a categorical variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then we’ll display the structure of the dataframe and its descriptive statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c098de499eb25a79f86df3d96b6a08cd.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/214454cf04629a22934a5fdd37efa9c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately, there are no missing data (which is rare when dealing with real-world
    data), so we can continue exploring the data.
  prefs: []
  type: TYPE_NORMAL
- en: An imbalanced dataset can affect quality of clusters, so let’s check how many
    instances we have from each variety of seed.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Based on the output of the code, we can see that we are working with a balanced
    dataset. Specifically, the dataset is comprised of 70 seeds from each group.
  prefs: []
  type: TYPE_NORMAL
- en: A useful visualization used during EDAs is the histogram since it can be used
    to determine the distribution of the data and detect the presence of skew. Since
    there are three varieties of seeds in the dataset, it might be beneficial to plot
    the distribution of each numeric variable grouped by the variety.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3cfd7bc123a323e0d07ba844a3a6eee1.png)'
  prefs: []
  type: TYPE_IMG
- en: One example of the histograms generated by the code
  prefs: []
  type: TYPE_NORMAL
- en: From this plot, we can see there is some skewness in the data. To provide a
    more precise measure of skewness, we can used the `skew()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although there is some skewness in the data, none of the individual values appear
    to be extremely high (i.e., absolute values greater than 1), therefore, a transformation
    is not necessary at this time.
  prefs: []
  type: TYPE_NORMAL
- en: Correlated features can affect the k-means algorithm, so we’ll generate a heat
    map of correlations to determine if the features in the dataset are associated.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eea20683301b97698f7531623509f242.png)'
  prefs: []
  type: TYPE_IMG
- en: There are strong (0.60 ≤ ∣*r*∣ <0.80) and very strong (0.80 ≤ ∣*r*∣ ≤ 1.00)
    correlations between some of the variables; however, the principal component analysis
    (PCA) we will conduct will address this issue.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Preparation**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we won’t use them in the k-means algorithm, the Seeds dataset contains
    labels (i.e., ‘variety’ column). This information will be useful when we evaluate
    the performance of the implementations, so we’ll set it aside for now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Before entering the data into the k-means algorithm, we’ll need to scale the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: After scaling the data, we’ll conduct PCA to reduce the dimensions of the data
    and address the correlated variables we identified earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The output of the code indicates that one dimension accounts for 72% of the
    variance, two dimensions accounts for 89% of the variance, and three dimensions
    accounts for 99% of the variance. To confirm the correct number of dimensions
    were retained, use the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now the data are ready to be inputted into the k-means algorithm. We’re going
    to examine two implementations of the algorithm — one informed by an elbow plot
    and another informed by the Silhouette Score.
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Informed by Elbow Plot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To generate an elbow plot, use the code snippet below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The number of clusters is displayed on the x-axis and the inertia is displayed
    on the y-axis. Inertia refers to the sum of squared distances of samples to their
    nearest cluster center. Basically, it is a measure of how close the data points
    are to the mean of their cluster (i.e., the centroid). When inertia is low, the
    clusters are more dense and defined clearly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6ebd812bbbd25f0fd53361595919966e.png)'
  prefs: []
  type: TYPE_IMG
- en: When interpreting an elbow plot, look for the section of the line that looks
    similar to an elbow. In this case, the elbow is at three. When k = 1, the inertia
    will be large, then it will gradually decrease as k increases.
  prefs: []
  type: TYPE_NORMAL
- en: The “elbow” is the point where the decrease begins to plateau and the addition
    of new clusters does not result in a significant decrease in inertia.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this elbow plot, the value of k should be three. Using an elbow plot
    has been described as more of an art than a science, which is why it has been
    referred to as “coarse”.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the k-means algorithm when k = 3, we’ll run the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The code below can be used to visualize the output of k-means clustering informed
    by the elbow plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/56420e8bc22d373e3e8d3037a9f09a7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the data were reduced to three dimensions, they are plotted on a 3D plot.
    To gain additional information about the clusters, we can use `countplot` from
    the `Seaborn` package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cda60bea498cb8ae89672050712b6cbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Earlier, we determined that each group was comprised of 70 seeds. The data displayed
    in this plot indicate k-means implemented with the elbow plot *may* have performed
    moderately well since each count of each group is around 70; however, there are
    better ways to evaluate performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide a more precise measure of how well the algorithm performed, we will
    use three metrics: (a) Davies-Bouldin Index, (b) Calinski-Harabasz Index, and
    (c) Adjusted Rand Index. We’ll talk about how to interpret them in the Results
    and Analysis section, but the following code snippet can be used to calculate
    their values.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**K-Means Informed by Silhouette Score**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A silhouette score is the mean silhouette coefficient over all the instances.
    The values can range from -1 to 1, with
  prefs: []
  type: TYPE_NORMAL
- en: 1 indicating an instance is well inside its cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 indicating an instance is close to its cluster’s boundary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: -1 indicates the instance could be assigned to the incorrect cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When interpreting the silhouette score, we should choose the number of clusters
    with the highest score.
  prefs: []
  type: TYPE_NORMAL
- en: To generate a plot of silhouette scores for multiple values of k, we can use
    the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f3a56a5f7129d2a3e95225518808a8b4.png)'
  prefs: []
  type: TYPE_IMG
- en: The data indicate that k should equal two.
  prefs: []
  type: TYPE_NORMAL
- en: Using this information, we can implement the K-Means algorithm again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: To generate a plot of the algorithm when k = 2, we can use the code presented
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/794d6fc3863c5bf01a24632194677896.png)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the K-Means implementation informed by the elbow plot, additional
    information can be gleaned using `countplot`from `Seaborn`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23e7a00c1dbe2bbe696b452033da1cff.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on our understanding of the dataset (i.e., it includes three varieties
    of seeds with 70 samples from each category), an initial reading of the plot *may*
    suggest that the implementation informed by the silhouette score did not perform
    as well on the clustering task; however, we cannot use this plot in isolation
    to make a determination.
  prefs: []
  type: TYPE_NORMAL
- en: To provide a more robust and detailed comparison of the implementations, we
    will calculate the three metrics that were used on the implementation informed
    by the elbow plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '**Results and Analysis**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To compare the results from both implementations, we can create a dataframe
    and display it as a table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/36efb4d22c637363266fc4112f9357c7.png)'
  prefs: []
  type: TYPE_IMG
- en: The metrics used to compare the implementations of k-means clustering include
    internal metrics (e.g., Davies-Bouldin, Calinski-Harabasz) which do not include
    ground truth labels and external metrics (e.g., Adjusted Rand Index) which do
    include external metrics. A brief description of the three metrics is provided
    below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Davies-Bouldin Index (DBI): The DBI captures the trade-off between cluster
    compactness and the distance between clusters. Lower values of DBI indicate there
    are tighter clusters with more separation between clusters [3].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Calinski-Harabasz Index (CHI): The CHI measures cluster density and distance
    between clusters. Higher values indicate that clusters are dense and well-separated
    [4].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adjusted Rand Index (ARI): The ARI measures agreement between cluster labels
    and ground truth. The values of the ARI range from -1 to 1\. A score of 1 indicates
    perfect agreement between labels and ground truth; a scores of 0 indicates random
    assignments; and a score of -1 indicates worse than random assignment [5].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing the two implementations, we observed k-mean informed by the silhouette
    score performed best on the two internal metrics, indicating more compact and
    separated clusters. However, k-means informed by the elbow plot performed best
    on the external metric (i.e., ARI) which indicating better alignment with the
    ground truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ultimately, the best performing implementation will be determined by the task.
    If the task requires clusters that are cohesive and well-separated, then internal
    metrics (e.g., DBI, CHI) might be more relevant. If the task requires the clusters
    to align with the ground truth labels, then external metrics, like the ARI, may
    be more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this project was to provide a comparison between k-means clustering
    informed by an elbow plot and the silhouette score, and since there wasn’t a defined
    task beyond a pure comparison, we cannot provide a definitive answer as to which
    implementation is better.
  prefs: []
  type: TYPE_NORMAL
- en: Although the absence of a definitive conclusion may be frustrating, it highlights
    the importance of considering multiple metrics when comparing machine learning
    models and remaining focused on the project’s objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this article. If you have any feedback
    or questions, please leave a comment.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] A. Géron, Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow:
    Concepts, Tools, and Techniques to Build Intelligent Systems (2021), O’Reilly.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] M. Charytanowicz, J. Niewczas, P. Kulczycki, P. Kowalski, S. Łukasik, &
    S. Zak, Complete Gradient Clustering Algorithm for Features Analysis of X-Ray
    Images (2010), Advances in Intelligent and Soft Computing [https://doi.org/10.1007/978-3-642-13105-9_2](https://doi.org/10.1007/978-3-642-13105-9_2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] D. L. Davies, D.W. Bouldin, A Cluster Separation Measure (1979), IEEE Transactions
    on Pattern Analysis and Machine Intelligence https://[doi](https://en.wikipedia.org/wiki/Doi_(identifier)):[10.1109/TPAMI.1979.4766909](https://doi.org/10.1109%2FTPAMI.1979.4766909)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] T. Caliński, J. Harabasz, A Dendrite Method for Cluster Analysis (1974)
    Communications in Statistics https://[doi](https://en.wikipedia.org/wiki/Doi_(identifier)):[10.1080/03610927408827101](https://doi.org/10.1080%2F03610927408827101)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] N. X. Vinh, J. Epps, J. Bailey, Information Theoretic Measures for Clusterings
    Comparison: Variants, Properties, Normalization and Correction for Chance (2010),
    Journal of Machine Learning Research [https://www.jmlr.org/papers/volume11/vinh10a/vinh10a.pdf](https://www.jmlr.org/papers/volume11/vinh10a/vinh10a.pdf)'
  prefs: []
  type: TYPE_NORMAL
