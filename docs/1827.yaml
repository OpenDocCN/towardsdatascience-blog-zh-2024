- en: Can AI Agents Do Your Day-to-Day Tasks on Apps?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/appworld-a-controllable-world-of-apps-and-people-for-benchmarking-interactive-coding-agents-37517dd9d498?source=collection_archive---------2-----------------------#2024-07-28](https://towardsdatascience.com/appworld-a-controllable-world-of-apps-and-people-for-benchmarking-interactive-coding-agents-37517dd9d498?source=collection_archive---------2-----------------------#2024-07-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Benchmarking coding agents in a world of apps and people
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@harshjtrivedi94?source=post_page---byline--37517dd9d498--------------------------------)[![Harsh
    Trivedi](../Images/d1b40b336bd757cceab41d5f1ec64aa2.png)](https://medium.com/@harshjtrivedi94?source=post_page---byline--37517dd9d498--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--37517dd9d498--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--37517dd9d498--------------------------------)
    [Harsh Trivedi](https://medium.com/@harshjtrivedi94?source=post_page---byline--37517dd9d498--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--37517dd9d498--------------------------------)
    ¬∑7 min read¬∑Jul 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Imagine a world where AI agents can act as your personal assistant, completing
    tasks for you like setting up a return on Amazon or canceling meetings based on
    your emails. This would require agents to operate your applications interactively
    in complex workflows, and there really hasn‚Äôt been a great way to benchmark such
    agents. Until now.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**ü§ñ 1\. Coding agents for personal apps**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI assistants (e.g., those on our mobile phones) are improving as the underlying
    AI models improve. A few years back, they had difficulty answering simple factual
    questions correctly. Today, they have *started* to get to the point where they
    can operate apps on our behalf to do basic tasks. E.g., much of the recent [GoogleIO](https://youtu.be/uFroTufv6es?si=u6MjyEBxN4hRQ11T&t=1088)
    and [Apple WWDC](https://www.youtube.com/live/RXeOiIDNNek?si=60ztV3xEV1GTSrAE&t=4199)
    events were about this vision of AI assistants being autonomous agents working
    on our behalf.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, they will be able to autonomously complete more complex tasks
    on our apps. For example, you could say, ‚ÄúHey, some of my coworkers have canceled
    meetings via email; please delete my corresponding phone reminders.‚Äù The agent
    would autonomously check your email inbox, figure out which coworkers canceled,
    go to the calendar app, determine which meetings are with those coworkers, and
    cancel them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9acda03fd66c49d04858fbcb62f1710.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of day-to-day tasks involving apps like Amazon, Venmo, Gmail, etc.
  prefs: []
  type: TYPE_NORMAL
- en: One of the ways AI models can tackle such tasks is by **interactively** writing
    **code** and calling **APIs**. *APIs* allow agents to undertake elementary actions
    on apps, *code* allows them to orchestrate them in complex logic and control flow,
    and *interaction* allows them to explore user accounts and adapt based on the
    code execution results.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an example in the figure below, where the agent is tasked to launch
    a playlist with enough songs covering the user‚Äôs workout duration for today. For
    this, the agent first needs to write code calling SimpleNote **APIs** (1st code
    block) to find and ‚Äúread‚Äù (print) the note containing the workout schedule. Only
    after this **interaction** to observe how the note is structured ‚Äî seeing that
    duration is listed day-wise ‚Äî can the agent write the necessary code (2nd code
    block), which involves finding today‚Äôs day-of-the-week and extracting the associated
    duration. To select a playlist, it must write rich **code** with for-loops and
    other control flow to iterate over playlists, compute playlist durations, and
    play one covering the workout duration (3rd code block).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bee66dc18e20c12d499be11238c7f226.png)'
  prefs: []
  type: TYPE_IMG
- en: An agent solving a task on behalf of a user by interactively writing rich code
    containing API calls to various apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know how an agent can complete such tasks, the question is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we develop and benchmark such coding agents for everyday digital
    tasks across various apps?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For this, we need (i) a rich, stable, and reproducible execution environment
    where agents can interact with many day-to-day apps via code and APIs, (ii) complex
    tasks requiring API calls and rich and interactive coding, and (iii) a reliable
    evaluation framework.
  prefs: []
  type: TYPE_NORMAL
- en: Existing benchmarks like [Gorilla](https://arxiv.org/pdf/2305.15334), [ToolBench](https://arxiv.org/abs/2307.16789),
    [API-Bank](https://arxiv.org/abs/2304.08244), [ToolTalk](https://arxiv.org/abs/2311.10775),
    [RestBench](https://arxiv.org/abs/2306.06624) do not meet any of these three requirements.
    Besides lacking the aforementioned type of environment, their tasks only involve
    a linear sequence of 1‚Äì4 API calls, without the need for rich and interactive
    coding, and they evaluate via comparing the agent‚Äôs solution to a reference solution
    (using an LLM or a human), which does not work well for complex tasks that admit
    many varying solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**üåé 2\. Introducing AppWorld**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address this gap, we introduce **AppWorld,** which constitutes (1) a controllable
    and simulated world environment (**engine**) where coding agents can operate various
    apps via APIs on behalf of people, (2) a **benchmark** of complex tasks defined
    on top of this environment, and (3) a robust **evaluation** framework for assessing
    agent performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2ec329bf4e6e60d19f8b8c05cefe6d1.png)'
  prefs: []
  type: TYPE_IMG
- en: An overview of AppWorld consisting of a simulated world environment of apps
    and people, a benchmark of complex tasks built on top of it, and a robust evaluation
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚öôÔ∏è 2.1 Engine: simulated digital world**'
  prefs: []
  type: TYPE_NORMAL
- en: AppWorld Engine is a high-fidelity API-based simulator (60K lines of code) simulating
    an ecosystem of 9 day-to-day apps from various domains (Gmail for email, Amazon
    for shopping, Spotify for music, etc.). This engine is backed by a fully controllable
    local backend with 457 APIs and 100+ DB tables, closely mimicking the rich features
    of the real apps. These APIs have detailed documentation ([explore interactively](https://appworld.dev/api-explorer))
    that agents can read to understand their use.
  prefs: []
  type: TYPE_NORMAL
- en: We then simulate a digital world of people and their digital activities across
    these apps on top of this engine. In particular, we populate the app databases
    (DBs) with 106 fictitious people living in this simulated world. They are related
    to each other via various relationships, like roommates, friends, managers, etc,
    to allow for interpersonal tasks, like splitting bills with roommates. Then, their
    everyday lives are simulated to perform various personal and interpersonal activities
    on their app accounts, such as ordering t-shirts on Amazon for home delivery,
    asking a roommate for car keys over the phone, and so on. The final DBs have 300K+
    rows spanning 726 columns).
  prefs: []
  type: TYPE_NORMAL
- en: '**üìä 2.2 Benchmark of complex tasks**'
  prefs: []
  type: TYPE_NORMAL
- en: AppWorld Benchmark builds 750 day-to-day tasks on top of this engine (examples
    shown above), requiring many APIs (often 15+), spanning multiple apps (1‚Äì4), and
    requiring rich & interactive coding (often 80+ lines with many programming constructs).
    See the statistics in the figure below and [explore tasks](https://appworld.dev/task-explorer)
    interactively on our playground.
  prefs: []
  type: TYPE_NORMAL
- en: Each task instruction comes with a supervisor (person in AppWorld) on whose
    behalf the agent is to do the task. The agent has access to all of their app accounts.
    Each task‚Äôs initial database state is carefully designed (programmatically) to
    ensure the task is well-defined and has realistic distractions and hurdles. The
    tasks also come with task variations, which holistically check if an agent can
    solve the task reliably under different initial conditions and instruction variations.
  prefs: []
  type: TYPE_NORMAL
- en: All task implementations are designed and developed by us (not crowdsourced).
    Their implementations span over 40K lines of code (yes, a lot is going into task
    development; see the paper).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df888266009fd4e8d5b751fba06a4cba.png)'
  prefs: []
  type: TYPE_IMG
- en: The percentage of tasks in AppWorld Benchmark across difficulty levels and properties
    of our-written solutions, like the number of apps, unique APIs and code lines,
    and the number of evaluation tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**‚úîÔ∏è 2.3\. Robust evaluation framework**'
  prefs: []
  type: TYPE_NORMAL
- en: The complex tasks in AppWorld can be completed in many ways (e.g., an order
    receipt may be downloaded from its Amazon API or its confirmation email). Further,
    an agent solving the task can cause collateral damage in many different ways (e.g.,
    initiating a return not asked for). So, a ***process*-based** approach that compares
    agent-generated code to reference code or API calls is inadequate for evaluating
    task completion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, AppWorld uses a ***state-based*** approach. In particular, for each
    task, we define a programmatic suite of unit tests that take snapshots of database
    states as inputs: (1) state before the agent starts and (2) after it ends. We
    then check that all expected and no unexpected database changes are made. This
    allows us to *robustly* check if an agent completed the task correctly without
    causing collateral damage.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to ensure the tasks are solvable, we write validation solution codes
    and programmatically verify that running them passes all evaluation tests.
  prefs: []
  type: TYPE_NORMAL
- en: '**üß™ 3\. How do agents perform?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have benchmarked many LLMs with several few-shot prompting methods, like
    ReAct, plan and execute, generating full code with reflection, and function calling.
    Even the best LLM, GPT-4o, performs quite poorly. E.g., it completes only ~30%
    of the tasks in the challenge test set correctly. GPT-4 Turbo and open LLMs lag
    much further behind.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the scores are much lower for our stricter robustness metric, which
    checks whether agents can reliably complete all task variations under different
    starting conditions and instructions perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdd4249667f1040387d25ca09a75912a.png)'
  prefs: []
  type: TYPE_IMG
- en: Plots showing scores of state-of-the-art LLMs using various prompting methods.
    AppWorld is challenging for the current models. E.g., GPT-4o solves only ~30%
    of Test-Challenge tasks correctly, and the score drops to 13.0 on our robustness
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the scores substantially drop with increasing difficulty, as per
    our hand-given labels and other indicators of difficulty, like the number of APIs
    and lines of code based on our written validation solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7e113ced561f5493dcba3248aad4307a.png)'
  prefs: []
  type: TYPE_IMG
- en: Plots showing scores of the best model, GPT4-o, across various task difficulty
    indicators. Model scores significantly drop with increasing difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: '**üîÆ 4\. What‚Äôs next for AppWorld?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AppWorld is a modular and extensible foundation that opens up many exciting
    possibilities in automating digital tasks. E.g., future works can:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the AppWorld engine to support browser/mobile UI-based control for the
    existing tasks to provide a unified benchmark for code, API, and UI-based autonomous
    agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extend the AppWorld benchmark to have tasks requiring multi-agent (and human)
    coordination and collaboration (e.g., set up a calendar meeting with a friend
    by coordinating with their agent over email).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overlay our digital world engine onto a physical world engine, like [Simulacra](https://arxiv.org/abs/2304.03442),
    with role-playing agents to study social dynamics and behavior in a controlled
    environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the engine as a no-consequence sandbox to study potential privacy and safety
    risks that may arise when digital assistants are given the ‚Äúagency‚Äù to act on
    our behalf in the real world.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And, of course, extend AppWorld to a yet larger ecosystem of apps and tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are excited for ourselves and others to pursue these directions (and more!)
    on top of AppWorld. Reach out if you need help or want to collaborate!
  prefs: []
  type: TYPE_NORMAL
- en: üöÄ **5\. Ready to give it a try?**
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AppWorld is easy to use and fast. You can pip install its open-source Python
    package and start building and testing your agent. If you have an agent, the following
    code is all you need to run and evaluate it on AppWorld.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6fb57e36d63cb2f0a396c2202024fe6d.png)'
  prefs: []
  type: TYPE_IMG
- en: A minimal usage example of the AppWorld environment.
  prefs: []
  type: TYPE_NORMAL
- en: For paper, code, leaderboard, data explorer (tasks, APIs, agent trajectories),
    interactive playground (interact directly with AppWorld tasks), video explainer,
    and more, visit [**https://appworld.dev**](https://appworld.dev).
  prefs: []
  type: TYPE_NORMAL
- en: '**NEW**: AppWorld won the **Best Resource Paper** award at ACL‚Äô24\. **üèÜ** üéâ'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Source**: All images are created by the author.'
  prefs: []
  type: TYPE_NORMAL
