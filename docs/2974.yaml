- en: 'Synthetic Data in Practice: A Shopify Case Study'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/synthetic-data-in-practice-a-shopify-case-study-79b0af024880?source=collection_archive---------6-----------------------#2024-12-10](https://towardsdatascience.com/synthetic-data-in-practice-a-shopify-case-study-79b0af024880?source=collection_archive---------6-----------------------#2024-12-10)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Testing new Snowflake functionality with a 30k records dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@piotr.gruszecki_22364?source=post_page---byline--79b0af024880--------------------------------)[![Piotr
    Gruszecki](../Images/0d8b89957a88b258e87887cf7e1de093.png)](https://medium.com/@piotr.gruszecki_22364?source=post_page---byline--79b0af024880--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--79b0af024880--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--79b0af024880--------------------------------)
    [Piotr Gruszecki](https://medium.com/@piotr.gruszecki_22364?source=post_page---byline--79b0af024880--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--79b0af024880--------------------------------)
    ·13 min read·Dec 10, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86f740f9908c2f0378251c2b79b24a67.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created with DALL·E, based on author’s prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with data, I keep running into the same problem more and more often.
    On one hand, we have growing requirements for data privacy and confidentiality;
    on the other — the need to make quick, data-driven decisions. Add to this the
    modern business reality: freelancers, consultants, short-term projects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a decision maker, I face a dilemma: I need analysis right now, the internal
    team is overloaded, and I can’t just hand over confidential data to every external
    analyst.'
  prefs: []
  type: TYPE_NORMAL
- en: And this is where synthetic data comes in.
  prefs: []
  type: TYPE_NORMAL
- en: 'But wait — I don’t want to write another theoretical article about what synthetic
    data is. There are enough of those online already. Instead, I’ll show you a specific
    comparison: 30 thousand real Shopify transactions versus their synthetic counterpart.'
  prefs: []
  type: TYPE_NORMAL
- en: What exactly did I check?
  prefs: []
  type: TYPE_NORMAL
- en: How faithfully does synthetic data reflect real trends?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where are the biggest discrepancies?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When can we trust synthetic data, and when should we be cautious?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This won’t be another “how to generate synthetic data” guide (though I’ll show
    the code too). I’m focusing on what really matters — whether this data is actually
    useful and what its limitations are.
  prefs: []
  type: TYPE_NORMAL
- en: I’m a practitioner — less theory, more specifics. Let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: Data Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When testing synthetic data, you need a solid reference point. In our case,
    we’re working with real transaction data from a growing e-commerce business:'
  prefs: []
  type: TYPE_NORMAL
- en: 30,000 transactions spanning 6 years
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clear growth trend year over year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mix of high and low-volume sales months
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diverse geographical spread, with one dominant market
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/31ef72599e5edf82fdb5519351e97c5c.png)'
  prefs: []
  type: TYPE_IMG
- en: All charts created by author, using his own R code
  prefs: []
  type: TYPE_NORMAL
- en: For practical testing, I focused on transaction-level data such as order values,
    dates, and basic geographic information. Most assessments require only essential
    business information, without personal or product specifics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The procedure was simple: export raw Shopify data, analyze it to maintain only
    the most important information, produce synthetic data in Snowflake, then compare
    the two datasets side by side. One can think of it as generating a “digital twin”
    of your business data, with comparable trends but entirely anonymized.'
  prefs: []
  type: TYPE_NORMAL
- en: '*[Technical note: If you’re interested in the detailed data preparation process,
    including R code and Snowflake setup, check the appendix at the end of this article.]*'
  prefs: []
  type: TYPE_NORMAL
- en: Monthly Revenue Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first test for any synthetic dataset is how well it captures core business
    metrics. Let’s start with monthly revenue — arguably the most important metric
    for any business (for sure in top 3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the raw trends (Figure 1), both datasets follow a similar pattern:
    steady growth over the years with seasonal fluctuations. The synthetic data captures
    the general trend well, including the business’s growth trajectory. However, when
    we dig deeper into the differences, some interesting patterns emerge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify these differences, I calculated a monthly delta:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b90f2b2c5623a7444841831b96c2d95f.png)'
  prefs: []
  type: TYPE_IMG
- en: We see from the plot, that monthly revenue delta varies — sometimes original
    is bigger, and sometimes synthetic. But the bars seem to be symmetrical and also
    the differences are getting smaller with time. I added number of records (transactions)
    per month, maybe it has some impact? Let’s dig a bit deeper.
  prefs: []
  type: TYPE_NORMAL
- en: The deltas are indeed quite well balanced, and if we look at the cumulative
    revenue lines, they are very well aligned, without large variations. I am skipping
    this chart.
  prefs: []
  type: TYPE_NORMAL
- en: Sample Size Impact
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The deltas are getting smaller, and we intuitively feel it is because of larger
    number of records. Let us check it — next plot shows absolute values of revenue
    deltas as a function of records per month. While the number of records does grow
    with time, the X axis is not exactly time — it’s the records.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cfe8da970d0b893877a5aa979e66f96a.png)'
  prefs: []
  type: TYPE_IMG
- en: The deltas (absolute values) do decrease, as the number of records per month
    is higher — as we expected. But there is one more thing, quite intriguing, and
    not that obvious, at least at first glance. Above around 500 records per month,
    the deltas do not fall further, they stay at (in average) more or less same level.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this specific number is derived from our dataset and might vary for different
    business types or data structures, the pattern itself is important: there exists
    a threshold where synthetic data stability improves significantly. Below this
    threshold, we see high variance; above it, the differences stabilize but don’t
    disappear entirely — synthetic data maintains some variation by design, which
    actually helps with privacy protection.'
  prefs: []
  type: TYPE_NORMAL
- en: There is a noise, which makes monthly values randomized, also with larger samples.
    All, while preserves consistency on higher aggregates (yearly, or cumulative).
    And while reproducing overall trend very well.
  prefs: []
  type: TYPE_NORMAL
- en: It would be quite interesting to see similar chart for other metrics and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We already know revenue delta depends on number of records, but is it just that
    more records in a given month, the higher the revenue of synthetic data? Let us
    find out …
  prefs: []
  type: TYPE_NORMAL
- en: So we want to check how revenue delta depends on number of records delta. And
    we mean by delta Synthetic-Shopify, whether it is monthly revenue or monthly number
    of records.
  prefs: []
  type: TYPE_NORMAL
- en: The chart below shows exactly this relationship. There is some (light) correlation
    - if number of records per month differ substantially between Synthetic and Shopify,
    or vice-versa (high delta values), the revenue delta follows. But it is far from
    simple linear relationship - there is extra noise there as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/765bdbe50a11b668f852fffec32616f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Dimensional Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When generating synthetic data, we often need to preserve not just overall metrics,
    but also their distribution across different dimensions like geography. I kept
    country and state columns in our test dataset to see how synthetic data handles
    dimensional analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results reveal two important aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: The reliability of synthetic data strongly depends on the sample size within
    each dimension
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dependencies between dimensions are not preserved
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Looking at revenue by country:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ae2bbb8dc5959e1b5593f23ea6ee3d2.png)'
  prefs: []
  type: TYPE_IMG
- en: For the dominant market with thousands of transactions, the synthetic data provides
    a reliable representation — revenue totals are comparable between real and synthetic
    datasets. However, for countries with fewer transactions, the differences become
    significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'A critical observation about dimensional relationships: in the original dataset,
    state information appears only for US transactions, with empty values for other
    countries. However, in the synthetic data, this relationship is lost — we see
    randomly generated values in both country and state columns, including states
    assigned to other countries, not US. This highlights an important limitation:
    synthetic data generation does not maintain logical relationships between dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, a practical way to overcome this country-state dependency
    issue. Before generating synthetic data, we could preprocess our input by concatenating
    country and state into a single dimension (e.g., ‘US-California’, ‘US-New York’,
    while keeping just ‘Germany’ or ‘France’ for non-US transactions). This simple
    preprocessing step would preserve the business logic of states being US-specific
    and prevent the generation of invalid country-state combinations in the synthetic
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This has important practical implications:'
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data works well for high-volume segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be cautious when analyzing smaller segments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always check sample sizes before drawing conclusions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware that logical relationships between dimensions may be lost, consider
    pre-aggregation of some columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider additional data validation if dimensional integrity is crucial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction value distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most interesting findings in this analysis comes from examining transaction
    value distributions. Looking at these distributions year by year reveals both
    the strengths and limitations of synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81ea005b766e189066459f614c5fe583.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The original Shopify data shows what you’d typically expect in e-commerce:
    highly asymmetric distribution with a long tail towards higher values, and distinct
    peaks corresponding to popular single-product transactions, showing clear bestseller
    patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The synthetic data tells an interesting story: it maintains very well the overall
    shape of the distribution, but the distinct peaks from bestseller products are
    smoothed out. The distribution becomes more “theoretical”, losing some real-world
    specifics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This smoothing effect isn’t necessarily a bad thing. In fact, it might be preferable
    in some cases:'
  prefs: []
  type: TYPE_NORMAL
- en: For general business modeling and forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want to avoid overfitting to specific product patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re looking for underlying trends rather than specific product effects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, if you’re specifically interested in bestseller analysis or single-product
    transaction patterns, you’ll need to factor in this limitation of synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing, the goal is product analysis, we’d prepare original dataset differently.
  prefs: []
  type: TYPE_NORMAL
- en: To quantify how well the synthetic data matches the real distribution, we’ll
    look at statistical validation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical Validation (K-S Test)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s validate our observations with the Kolmogorov-Smirnov test — a standard
    statistical method for comparing two distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4eb956f09eb0fd83d377d84b726b859.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The findings are positive, but what do these figures mean in practice? The
    Kolmogorov-Smirnov test compares two distributions and returns two essential metrics:
    D = 0.012201 (smaller is better, with 0 indicating identical distributions), and
    p-value = 0.0283 (below the normal 0.05 level, indicating statistically significant
    differences).'
  prefs: []
  type: TYPE_NORMAL
- en: 'While the p-value indicates some variations between distributions, the very
    low D statistic (nearly to 0) verifies the plot’s findings: a near-perfect match
    in the middle, with just slight differences at the extremities. The synthetic
    data captures crucial patterns while keeping enough variance to ensure anonymity,
    making it suitable for commercial analytics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical terms, this means:'
  prefs: []
  type: TYPE_NORMAL
- en: The synthetic data provides an excellent match in the most important mid-range
    of transaction values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The match is particularly strong where we have the most data points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences appear mainly in edge cases, which is expected and even desirable
    from a privacy perspective
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The statistical validation confirms our visual observations from the distribution
    plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This kind of statistical validation is crucial before deciding to use synthetic
    data for any specific analysis. In our case, the results suggest that the synthetic
    dataset is reliable for most business analytics purposes, especially when focusing
    on typical transaction patterns rather than extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s summarize our journey from real Shopify transactions to their synthetic
    counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: Overall business trends and patterns are maintained, including transactions
    value distributions. Spikes are ironed out, resulting in more theoretical distributions,
    while maintaining key characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Sample size matters, by design. Going too granular we will get noise, preserving
    confidentiality (in addition to removing all PII of course).
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies between columns are not preserved (country-state), but there is
    an easy walk around, so I think it is not a real issue.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand how the generated dataset will be used — what
    kind of analysis we expect, so that we can take it into account while reshaping
    the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The synthetic dataset will work perfectly for applications testing, but we should
    manually check edge cases, as these might be missed during generation.
  prefs: []
  type: TYPE_NORMAL
- en: In our Shopify case, the synthetic data proved reliable enough for most business
    analytics scenarios, especially when working with larger samples and focusing
    on general patterns rather than specific product-level analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Future Work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This analysis focused on transactions, as one of key metrics and an easy case
    to start with.
  prefs: []
  type: TYPE_NORMAL
- en: We can proceed with products analysis and also explore multi-table scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth to develop internal guidelines how to use synthetic data, including
    check and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: Data Preparation and Methodology'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can scroll through this section, as it is quite technical on how to prepare
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Raw Data Export
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of relying on pre-aggregated Shopify reports, I went straight for the
    raw transaction data. At Alta Media, this is our standard approach — we prefer
    working with raw data to maintain full control over the analysis process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The export process from Shopify is straightforward but not immediate:'
  prefs: []
  type: TYPE_NORMAL
- en: Request raw transaction data export from the admin panel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait for email with download links
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download multiple ZIP files containing CSV data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Reshaping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used R for exploratory data analysis, processing, and visualization. The code
    snippets are in R, copied from my working scripts, but of course one can use other
    languages to achieve the same final data frame.
  prefs: []
  type: TYPE_NORMAL
- en: The initial dataset had dozens of columns, so the first step was to select only
    the relevant ones for our synthetic data experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '*Code formatting is adjusted, so that we don’t have horizontal scroll.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We need one data frame, so we need to combine three files. Since we use data.table
    package, the syntax is very simple. And we pipe combined dataset to trim columns,
    keeping only selected ones.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s also change column names to single string, replacing spaces with underscore
    “_” — we don’t need to deal with extra quotations in SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: I also change transaction id from character “#1234”, to numeric “1234”. I create
    a new column, so we can easily compare if transformation went as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Of course you can also overwrite.
  prefs: []
  type: TYPE_NORMAL
- en: Extra experimentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this was an experiment with Snowflake’s synthetic data generation, I made
    some additional preparations. The Shopify export contains actual customer emails,
    which would be masked in Snowflake while generating synthetic data, but I hashed
    them anyway.
  prefs: []
  type: TYPE_NORMAL
- en: So I hashed these emails using MD5 and created an additional column with numerical
    hashes. This was purely experimental — I wanted to see how Snowflake handles different
    types of unique identifiers.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Snowflake masks text-based unique identifiers as it considers them
    personally identifiable information. For a real application, we’d want to remove
    any data that could potentially identify customers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I was also curious how logical column will be handled, so I changed type of
    a binary column, which has “yes/no” values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Filter transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset contains records per each item, while for this particular analysis
    we need only transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/505e4fd5854a0a53d702c5419b4b126a.png)'
  prefs: []
  type: TYPE_IMG
- en: Final subset of columns and filtering records with total amount paid.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Export dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have a dataset, we nee to export it as a csv file. I export full dataset,
    and I also produce a 5% sample, which I use for initial test run in Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: And also saving in Rds format, so I don’t need to repeat all the preparatory
    steps (which are scripted, so they are executed in seconds anyway).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Appendix: Data generation in Snowflake'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have our dataset, prepared according to our needs, generation of it’s
    synthetic “sibling” is straightforward. One needs to upload the data, run generation,
    and export results. For details follow Snowflake guidelines. Anyway, I will add
    here short summary, for complteness of this article.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to make some preparations — role, database and warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Create schema and stage, if not defined yet.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Upload csv files(s) to stage, and then import them to table(s).
  prefs: []
  type: TYPE_NORMAL
- en: Then, run generation of synthetic data. I like having a small “pilot”, somethiong
    like 5% records to make initial check if it goes through. It is a time saver (and
    costs too), in case of more complicated cases, where we might need some SQL adjustment.
    In this case it is rather pro-forma.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: It is good to inspect what we have as a result — checking tables directly in
    Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: And then run a full dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The execution time is non-linear, for the full dataset it is way, way faster
    than what data volume would suggest.
  prefs: []
  type: TYPE_NORMAL
- en: Now we export files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some preparations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And export (small and full dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: So now we have both original Shopify dataset and Synthetic. Time to analyze,
    compare, and make some plots.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix: Data comparison & charting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this analysis, I used R for both data processing and visualization. The
    choice of tools, however, is secondary — the key is having a systematic approach
    to data preparation and validation. Whether you use R, Python, or other tools,
    the important steps remain the same:'
  prefs: []
  type: TYPE_NORMAL
- en: Clean and standardize the input data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Validate the transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create reproducible analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document key decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The detailed code and visualization techniques could indeed be a topic for another
    article.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in specific aspects of the implementation, feel free to
    reach out.
  prefs: []
  type: TYPE_NORMAL
