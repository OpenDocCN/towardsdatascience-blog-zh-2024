["```py\n# tools from the text chunking package mentioned in this article\nfrom text_chunking.SemanticClusterVisualizer import SemanticClusterVisualizer\n# put your open ai api key in a .env file in the top level of the package\nfrom text_chunking.utils.secrets import load_secrets\n# the example text we're talking about \nfrom text_chunking.datasets.test_text_dataset import TestText\n\n# basic splitter\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport seaborn as sns\n```", "```py\nsplitter = RecursiveCharacterTextSplitter(\n        chunk_size=250,\n        chunk_overlap=0,\n        separators=[\"\\n\\n\", \"\\n\", \".\"],\n        is_separator_regex=False\n)\n```", "```py\noriginal_split_texts = semantic_chunker.split_documents(\n    splitter, \n    TestText.testing_text, \n    min_chunk_len=100, \n    verbose=True\n)\n\n### Output\n# 2024-09-14 16:17:55,014 - Splitting text with original splitter\n# 2024-09-14 16:17:55,014 - Creating 53 chunks\n# Mean len: 178.88679245283018\n# Max len: 245\n# Min len: 103\n```", "```py\noriginal_split_text_embeddings = semantic_chunker.embed_original_document_splits(original_split_texts) \n```", "```py\ndef get_breakpoints(\n    embeddings: List[np.ndarray],\n    start: int = 0,\n    end: int = None,\n    threshold: float = 0.95,\n) -> np.ndarray:\n    \"\"\"\n    Identifies breakpoints in embeddings based on cosine distance threshold.\n\n    Args:\n        embeddings (List[np.ndarray]): A list of embeddings.\n        start (int, optional): The starting index for processing. Defaults to 0.\n        end (int, optional): The ending index for processing. Defaults to None.\n        threshold (float, optional): The percentile threshold for determining significant distance changes. Defaults to 0.95.\n\n    Returns:\n        np.ndarray: An array of indices where breakpoints occur.\n    \"\"\"\n    if end is not None:\n        embeddings_windowed = embeddings[start:end]\n    else:\n        embeddings_windowed = embeddings[start:]\n\n    len_embeddings = len(embeddings_windowed)\n    cdists = np.empty(len_embeddings - 1)\n\n    # get the cosine distances between each chunk and the next one\n    for i in range(1, len_embeddings):\n        cdists[i - 1] = cosine(embeddings_windowed[i], embeddings_windowed[i - 1])\n\n    # get the breakpoints\n    difference_threshold = np.percentile(cdists, 100 * threshold, axis=0)\n    difference_exceeding = np.argwhere(cdists >= difference_threshold).ravel()\n\n    return difference_exceeding\n\ndef build_chunks_stack(\n    self, length_threshold: int = 20000, cosine_distance_percentile_threshold: float = 0.95\n) -> np.ndarray:\n    \"\"\"\n    Builds a stack of text chunks based on length and cosine distance thresholds.\n\n    Args:\n        length_threshold (int, optional): Minimum length for a text chunk to be considered valid. Defaults to 20000.\n        cosine_distance_percentile_threshold (float, optional): Cosine distance percentile threshold for determining breakpoints. Defaults to 0.95.\n\n    Returns:\n        np.ndarray: An array of indices representing the breakpoints of the chunks.\n    \"\"\"\n\n    # self.split texts are the original split texts \n    # self.split text embeddings are their embeddings \n    S = [(0, len(self.split_texts))]\n    all_breakpoints = set()\n    while S:\n        # get the start and end of this chunk\n        id_start, id_end = S.pop()\n\n        # get the breakpoints for this chunk\n        updated_breakpoints = self.get_breakpoints(\n            self.split_text_embeddings,\n            start=id_start,\n            end=id_end,\n            threshold=cosine_distance_percentile_threshold,\n        )\n        updated_breakpoints += id_start\n\n        # add the updated breakpoints to the set\n        updated_breakpoints = np.concatenate(\n            (np.array([id_start - 1]), updated_breakpoints, np.array([id_end]))\n        )\n\n        # for each updated breakpoint, add its bounds to the set and\n        # to the stack if it is long enough\n        for index in updated_breakpoints:\n            text_group = self.split_texts[id_start : index + 1]\n            if (len(text_group) > 2) and (\n                self.get_text_length(text_group) >= length_threshold\n            ):\n                S.append((id_start, index))\n            id_start = index + 1\n        all_breakpoints.update(updated_breakpoints)\n\n    # get all the breakpoints except the start and end (which will correspond to the start\n    # and end of the text splits)\n    return np.array(sorted(all_breakpoints))[1:-1]\n```", "```py\nfrom umap import UMAP\n\ndimension_reducer = UMAP(\n            n_neighbors=5, \n            n_components=2, \n            min_dist=0.0, \n            metric=\"cosine\", \n            random_state=0\n)\nreduced_embeddings = dimension_reducer.fit_transform(semantic_embeddings)\n\nsplits_df = pd.DataFrame(\n            {\n                \"reduced_embeddings_x\": reduced_embeddings[:, 0],\n                \"reduced_embeddings_y\": reduced_embeddings[:, 1],\n                \"idx\": np.arange(len(reduced_embeddings[:, 0])),\n            }\n)\n\nsplits_df[\"chunk_end\"] = np.cumsum([len(x) for x in semantic_text_groups])\n\nax = splits_df.plot.scatter(\n            x=\"reduced_embeddings_x\", \n            y=\"reduced_embeddings_y\", \n            c=\"idx\", \n            cmap=\"viridis\"\n)\n\nax.plot(\n            reduced_embeddings[:, 0],\n            reduced_embeddings[:, 1],\n            \"r-\",\n            linewidth=0.5,\n            alpha=0.5,\n)\n```", "```py\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import pdist\nfrom umap import UMAP\nimport seaborn as sns\n\n# set up the UMAP\ndimension_reducer_clustering = UMAP(\n            n_neighbors=umap_neighbors,\n            n_components=n_components_reduced,\n            min_dist=0.0,\n            metric=\"cosine\",\n            random_state=0\n)\nreduced_embeddings_clustering = dimension_reducer_clustering.fit_transform(\n    semantic_group_embeddings\n)\n\n# create the hierarchy\nrow_linkage = hierarchy.linkage(\n    pdist(reduced_embeddings_clustering),\n    method=\"average\",\n    optimal_ordering=True,\n)\n\n# plot the heatmap and dendogram\ng = sns.clustermap(\n    pd.DataFrame(reduced_embeddings_clustering),\n    row_linkage=row_linkage,\n    row_cluster=True,\n    col_cluster=False,\n    annot=True,\n    linewidth=0.5,\n    annot_kws={\"size\": 8, \"color\": \"white\"},\n    cbar_pos=None,\n    dendrogram_ratio=0.5\n)\n\ng.ax_heatmap.set_yticklabels(\n  g.ax_heatmap.get_yticklabels(), rotation=0, size=8\n)\n```", "```py\ncluster_labels = hierarchy.cut_tree(linkage, n_clusters=n_clusters).ravel()\ndimension_reducer = UMAP(\n  n_neighbors=umap_neighbors, \n  n_components=2, \n  min_dist=0.0, \n  metric=\"cosine\", \n  random_state=0\n)\nreduced_embeddings = dimension_reducer.fit_transform(semantic_embeddings)\n\nsplits_df = pd.DataFrame(\n            {\n                \"reduced_embeddings_x\": reduced_embeddings[:, 0],\n                \"reduced_embeddings_y\": reduced_embeddings[:, 1],\n                \"cluster_label\": cluster_labels,\n            }\n        )\n\nsplits_df[\"chunk_end\"] = np.cumsum(\n            [len(x) for x in semantic_text_groups]\n        ).reshape(-1, 1)\n\nax = splits_df.plot.scatter(\n            x=\"reduced_embeddings_x\",\n            y=\"reduced_embeddings_y\",\n            c=\"cluster_label\",\n            cmap=\"rainbow\",\n        )\n\nax.plot(\n  reduced_embeddings[:, 0],\n  reduced_embeddings[:, 1],\n  \"r-\",  \n  linewidth=0.5,\n  alpha=0.5,\n)\n```", "```py\nimport langchain\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers.string import StrOutputParser\nfrom langchain.callbacks import get_openai_callback\nfrom dataclasses import dataclass\n\n@dataclass\nclass ChunkSummaryPrompt:\n    system_prompt: str = \"\"\"\n        You are an expert at summarization and information extraction from text. You will be given a chunk of text from a document and your\n        task is to summarize what's happening in this chunk using fewer than 10 words. \n\n        Read through the entire chunk first and think carefully about the main points. Then produce your summary.\n\n        Chunk to summarize: {current_chunk}\n    \"\"\"\n\n    prompt: langchain.prompts.PromptTemplate = PromptTemplate(\n        input_variables=[\"current_chunk\"],\n        template=system_prompt,\n    )\n\nclass ChunkSummarizer(object):\n    def __init__(self, llm):\n        self.prompt = ChunkSummaryPrompt()\n        self.llm = llm\n        self.chain = self._set_up_chain()\n\n    def _set_up_chain(self):\n        return self.prompt.prompt | self.llm | StrOutputParser()\n\n    def run_and_count_tokens(self, input_dict):\n        with get_openai_callback() as cb:\n            result = self.chain.invoke(input_dict)\n\n        return result, cb\n\nllm_model = \"gpt-4o-mini\"\nllm = ChatOpenAI(model=llm_model, temperature=0, api_key=api_key)\nsummarizer = ChunkSummarizer(llm)\n```", "```py\nfrom text_chunking.SemanticClusterVisualizer import SemanticClusterVisualizer\nfrom text_chunking.utils.secrets import load_secrets\nfrom text_chunking.datasets.test_text_dataset import TestText, TestTextNovel\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nsecrets = load_secrets()\nsemantic_chunker = SemanticClusterVisualizer(api_key=secrets[\"OPENAI_API_KEY\"])\n\nsplitter = RecursiveCharacterTextSplitter(\n        chunk_size=250,\n        chunk_overlap=0,\n        separators=[\"\\n\\n\", \"\\n\", \".\"],\n        is_separator_regex=False\n)\n\noriginal_split_texts = semantic_chunker.split_documents(\n    splitter, \n    TestTextNovel.testing_text, \n    min_chunk_len=100, \n    verbose=True\n)\noriginal_split_text_embeddings = semantic_chunker.embed_original_document_splits(original_split_texts)\n\nbreakpoints, semantic_groups = semantic_chunker.generate_breakpoints(\n    original_split_texts,\n    original_split_text_embeddings,\n    length_threshold=10000 #may need some iteration to find a good value for this parameter\n)\n```"]