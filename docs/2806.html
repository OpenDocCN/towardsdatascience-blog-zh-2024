<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Generate 3D Images with Nvidia’s LLaMa-Mesh</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Generate 3D Images with Nvidia’s LLaMa-Mesh</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19">https://towardsdatascience.com/generate-3d-images-with-nvidias-llama-mesh-69a6929a4580?source=collection_archive---------10-----------------------#2024-11-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="4c76" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">DEEP LEARNING PAPERS</h2><div/><div><h2 id="414c" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">5-Minute Deep Dive into the Paper</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Dr. Varshita Sher" class="l ep by dd de cx" src="../Images/a3f2e9bf1dc1d8cbe018e54f9341f608.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*ImYwhIFTYTspDAzmdk7i6w.jpeg"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://varshitasher.medium.com/?source=post_page---byline--69a6929a4580--------------------------------" rel="noopener follow">Dr. Varshita Sher</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--69a6929a4580--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">5 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Nov 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lh li ab q ee lj lk" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lg"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count lf lg">4</span></p></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="ll k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lm an ao ap ii ln lo lp" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lq cn"><div class="l ae"><div class="ab cb"><div class="lr ls lt lu lv lw ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lm an ao ap ii lx ly lk lz ma mb mc md s me mf mg mh mi mj mk u ml mm mn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp mq"><img src="../Images/54a94d7a83e9d4615209cfcd3b48eefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elCK7LByBKKwPmVkbJkqpQ.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Taken from the paper</figcaption></figure><h2 id="6aec" class="nh ni fq bf nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fw bk">Introduction</h2><p id="6f53" class="pw-post-body-paragraph oe of fq og b gt oh oi oj gw ok ol om ns on oo op nw oq or os oa ot ou ov ow fj bk">Last week, NVIDIA published a fascinating paper (<a class="af ox" href="https://arxiv.org/abs/2411.09595" rel="noopener ugc nofollow" target="_blank">LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</a>) that allows the generation of 3D mesh objects using natural language.</p><p id="5c49" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">In simple words, if you can say, <em class="pd">"Tell me a joke</em>," now you can say, <em class="pd">"Give me the 3D mesh for a car,"</em> and it can give the output in the OBJ format (more on this shortly) containing the output.</p><blockquote class="pe pf pg"><p id="1e43" class="oe of pd og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">If you’d like to try out few examples, you can do so here — <a class="af ox" href="https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh</a></p></blockquote><p id="edaa" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">The most amazing part for me was that it did so without extending the vocabulary or introducing new tokens as is typical for most fine-tuning tasks.</p><p id="0fab" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk"><strong class="og ga"><em class="pd">But first, what is a 3D mesh?</em></strong></p><p id="fe5f" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">A 3D mesh is a digital representation of a 3D object that consists of vertices, edges, and faces.</p><p id="bad1" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">For example, consider a cube. It has 8 vertices (the corners), 12 edges (the lines connecting the corners), and 6 faces (the square sides). This is a basic 3D mesh representation of a cube. The cube’s vertices (<code class="cx ph pi pj pk b">v</code>) define its corners, and the faces (<code class="cx ph pi pj pk b">f</code>) describe how those corners connect to form the surfaces.</p><p id="e4d6" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">Here is an example of OBJ file that represents the geometry of the 3D object</p><pre class="mr ms mt mu mv pl pk pm bp pn bb bk"><span id="f8fb" class="po ni fq pk b bg pp pq l pr ps"># Vertices<br/>v: (0, 0, 0)<br/>v: (1, 0, 0)<br/>v: (1, 1, 0)<br/>v: (0, 1, 0)<br/>v: (0, 0, 1)<br/>v: (1, 0, 1)<br/>v: (1, 1, 1)<br/>v: (0, 1, 1)<br/><br/># Faces<br/>f 1 2 3 4<br/>f 5 6 7 8<br/>f 1 5 8 4<br/>f 2 6 7 3<br/>f 4 3 7 8<br/>f 1 2 6 5</span></pre><p id="5140" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">These numbers are then interpreted by software that will render the final image i.e. 3D cube. (or you can use HuggingFace spaces like <a class="af ox" href="https://huggingface.co/spaces/Zhengyi/LLaMA-Mesh" rel="noopener ugc nofollow" target="_blank">this</a> to render the object)</p><p id="dce0" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">As objects increase in complexity (compared to the simple cube above), they will have thousands or even millions of vertices, edges, and faces to create detailed shapes and textures. Additionally, they will have more dimensions to capture things like texture, direction it is facing, etc.</p><p id="a865" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">Realistically speaking, this is what the obj file for an everyday object (a bench) would look like:</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div class="mo mp pt"><img src="../Images/17e377768d383fdca3851d49b2b687f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*0ArUwqsSInNK0bev6PLx1w.png"/></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Examples of obj file for different objects (Taken from paper)</figcaption></figure><p id="7b1c" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">As you may have noticed from the image above, LLMs like GPT4o and LLama3.1 are capable, to some extent, of producing the obj file out-of-the-box. However, if you look at the rendered mesh image of the bench in both cases, you can see why fine-tuning is necessary from a quality standpoint.</p><h2 id="2c16" class="nh ni fq bf nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fw bk">How is an LLM able to work with 3D mesh?</h2><p id="486f" class="pw-post-body-paragraph oe of fq og b gt oh oi oj gw ok ol om ns on oo op nw oq or os oa ot ou ov ow fj bk">It is common knowledge that LLMs understand text by converting tokens (like <code class="cx ph pi pj pk b">cat</code>) into token ids (like <code class="cx ph pi pj pk b">456</code>). Similarly, in order to work with the standard OBJ format, we must somehow convert the vertices coordinates which are typically decimals into integers.</p><p id="d535" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">They use vertex quantization to achieve this in the paper and split a single coordinate into multiple tokens (similar to how a long word like <code class="cx ph pi pj pk b">operational</code> would be split into two tokens — <code class="cx ph pi pj pk b">oper</code> and <code class="cx ph pi pj pk b">ational</code> as per<a class="af ox" href="https://platform.openai.com/tokenizer" rel="noopener ugc nofollow" target="_blank"> GPT4o tokenizer).</a> As expected, reducing the number of tokens to represent the decimal has a normal precision-cost tradeoff.</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div class="mo mp pu"><img src="../Images/5fb1a7ee1d43422858e1fc7445211069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*BLDdQothUKUxO7bKcnPwOw.png"/></div></figure><p id="590b" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">To achieve vertex quantization, they scale all three axes in the mesh to the range (0, 64) and quantize the coordinates to the nearest integer, i.e. each of the 3 axes can take a value between 0 and 64 (in this case 39, 19 and 35). Finally, by reading and generating such a format, the LLM is able to work with 3D objects.</p><h2 id="e31e" class="nh ni fq bf nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fw bk">What was the training procedure for LlaMa-Mesh?</h2><p id="126e" class="pw-post-body-paragraph oe of fq og b gt oh oi oj gw ok ol om ns on oo op nw oq or os oa ot ou ov ow fj bk">LLama-Mesh was created by fine-tuning LLama3.1–8B instruct model using the SFT (Supervised Fine Tuning) method to improve its mesh understanding and generation capabilities.</p><p id="ef8a" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">Since it is an SFT, we need to provide it with input-output examples of Text-3D instructions. Here’s an example:</p><pre class="mr ms mt mu mv pl pk pm bp pn bb bk"><span id="1ee6" class="po ni fq pk b bg pp pq l pr ps">Input<br/>User: Create a 3D obj file using the following description: a 3D model of a car.<br/><br/>Output<br/>Assistant: &lt;start of mesh&gt; v 0 3 4 v 0 4 6 v 0 3 … f 1 3 2 f 4 3 5 … . &lt;end of mesh&gt;</span></pre><p id="1f56" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">In addition to generating the 3D mesh, LLama-Mesh is also capable of interpreting the 3d mesh. To this end, its training data also contained several examples for mesh understanding and mesh generation as part of a conversation-style format. Here are a few examples from the dataset</p><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div role="button" tabindex="0" class="mx my ed mz bh na"><div class="mo mp pv"><img src="../Images/dfc043eef6d5b257e2f48c7a48e81bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1BFOtZa6WkKuWOCjg4cfQ.png"/></div></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Training dataset curated for LLama-Mesh</figcaption></figure><h2 id="89fb" class="nh ni fq bf nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fw bk">Most interesting bits from the paper</h2><ul class=""><li id="91c3" class="oe of fq og b gt oh oi oj gw ok ol om ns on oo op nw oq or os oa ot ou ov ow pw px py bk">LlaMa-Mesh can communicate with both text and 3D objects <strong class="og ga">without</strong> needing special tokenizers or extending the LLM’s vocabulary (thanks to the use of OBJ format and the vertex quantization discussed above which can effectively tokenize 3D mesh data into discrete tokens that LLMs can process seamlessly).</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div class="mo mp pz"><img src="../Images/9c9d63bf68147f6a8d5ca8cbf283a39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*bChuatbjJtT8mB3nR40zVg.png"/></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Image taken from paper</figcaption></figure><ul class=""><li id="657f" class="oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow pw px py bk">LlaMa-Mesh can generate diverse shapes from the same input text.</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div class="mo mp qa"><img src="../Images/9765acd06efa01af4b7afca377607486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*BL5R_e1XT1flNMAX-aj_BA.png"/></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Taken from paper</figcaption></figure><ul class=""><li id="6609" class="oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow pw px py bk">Even though the fine-tuning process slightly degraded the model’s underlying language understanding and reasoning capabilities (they call it out as a limitation imposed by the choice of instruction dataset, and size of the smaller 8B model), it is offset by the fact that the fine-tuned model can generate high-quality OBJ files for 3D mesh generation.</li></ul><figure class="mr ms mt mu mv mw mo mp paragraph-image"><div class="mo mp qb"><img src="../Images/a9f3f98035525faf5b3b6f84f0f917ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*tpKg9-bYQeB8ys81viGhEA.png"/></div><figcaption class="nc nd ne mo mp nf ng bf b bg z dx">Comparison of the base model and fine tuned version on metrics which assess the model’s general knowledge, common sense reasoning, and mathematical problem-solving abilities (Image taken from paper)</figcaption></figure><h2 id="abf7" class="nh ni fq bf nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fw bk">Why should you care about this paper?</h2><p id="70c1" class="pw-post-body-paragraph oe of fq og b gt oh oi oj gw ok ol om ns on oo op nw oq or os oa ot ou ov ow fj bk">I am already amazed by the capabilities of large language models to generate human-like text, code, and reason with visual content. Adding 3D mesh to this list is just brilliant.</p><p id="c7a1" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">LLMs like LLaMa-Mesh have the potential to revolutionize various industries including gaming, education, and healthcare.</p><p id="4dbf" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">It can be useful for generating realistic assets like characters, environments, and objects directly from text descriptions for video games.</p><p id="8fe0" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">Similarly, it can speed up the product development and ideation process as any company will require a design so they know what to create.</p><p id="474e" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">It can also be useful for architectural designs for buildings, machinery, bridges, and other infrastructure projects. Finally, in the edtech space, it can be used for embedding interactive 3D simulations within the training material.</p></div></div></div><div class="ab cb qc qd qe qf" role="separator"><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi qj"/><span class="qg by bm qh qi"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d19a" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk">The paper is a straightforward and quick read, and I highly encourage you to do it.</p><p id="22df" class="pw-post-body-paragraph oe of fq og b gt oy oi oj gw oz ol om ns pa oo op nw pb or os oa pc ou ov ow fj bk"><strong class="og ga">Paper page</strong> — <a class="af ox" href="https://arxiv.org/pdf/2411.09595" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2411.09595</a><br/><strong class="og ga">Code</strong> — <a class="af ox" href="https://github.com/nv-tlabs/LLaMA-Mesh" rel="noopener ugc nofollow" target="_blank">https://github.com/nv-tlabs/LLaMA-Mesh</a><br/><strong class="og ga">Nvidia’s Blog</strong> — <a class="af ox" href="https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/" rel="noopener ugc nofollow" target="_blank">https://research.nvidia.com/labs/toronto-ai/LLaMA-Mesh/</a></p></div></div></div></div>    
</body>
</html>