["```py\nfrom dataclasses import dataclass, field\nfrom typing import List\n\n@dataclass\nclass NeuralNetNode:\n    \"\"\"A node in our neural network tree\"\"\"\n    children: List['NeuralNetNode'] = field(default_factory=list)\n\n    def op(self, x: List[float]) -> float:\n        \"\"\"The operation that this node performs\"\"\"\n        raise NotImplementedError\n\n    def forward(self) -> float:\n        \"\"\"Evaluate this node on the given input\"\"\"\n        return self.op([child.forward() for child in self.children])\n\n    # This is just for convenience\n    def __call__(self) -> List[float]:\n        return self.forward()\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.children})'\n```", "```py\ndef grad(self) -> List[float]:\n    \"\"\"The gradient of this node with respect to its inputs\"\"\"\n    raise NotImplementedError\n\ndef backward(self, derivative_from_parent: float):\n    \"\"\"Propagate the derivative from the parent to the children\"\"\"\n    self.on_backward(derivative_from_parent)\n    deriv_wrt_children = self.grad()\n    for child, derivative_wrt_child in zip(self.children, deriv_wrt_children):\n        child.backward(derivative_from_parent * derivative_wrt_child)\n\ndef on_backward(self, derivative_from_parent: float):\n    \"\"\"Hook for subclasses to override. Things like updating parameters\"\"\"\n    pass\n```", "```py\nimport random\n\n@dataclass\nclass Input(NeuralNetNode):\n    \"\"\"A leaf node that represents an input to the network\"\"\"\n    value: float=0.0\n\n    def op(self, x):\n        return self.value\n\n    def grad(self) -> List[float]:\n        return [1.0]\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.value})'\n\n@dataclass\nclass Parameter(NeuralNetNode):\n    \"\"\"A leaf node that represents a parameter to the network\"\"\"\n    value: float=field(default_factory=lambda: random.uniform(-1, 1))\n    learning_rate: float=0.01\n\n    def op(self, x):\n        return self.value\n\n    def grad(self):\n        return [1.0]\n\n    def on_backward(self, derivative_from_parent: float):\n        self.value -= derivative_from_parent * self.learning_rate\n\n    def __repr__(self):\n        return f'{self.__class__.__name__}({self.value})'\n```", "```py\nimport math\n\n@dataclass\nclass Operation(NeuralNetNode):\n    \"\"\"A node that performs an operation on its inputs\"\"\"\n    pass\n\n@dataclass\nclass Add(Operation):\n    \"\"\"A node that adds its inputs\"\"\"\n    def op(self, x):\n        return sum(x)\n\n    def grad(self):\n        return [1.0] * len(self.children)\n\n@dataclass\nclass Multiply(Operation):\n    \"\"\"A node that multiplies its inputs\"\"\"\n    def op(self, x):\n        return math.prod(x)\n\n    def grad(self):\n        grads = []\n        for i in range(len(self.children)):\n            cur_grad = 1\n            for j in range(len(self.children)):\n                if i == j:\n                    continue\n                cur_grad *= self.children[j].forward()\n            grads.append(cur_grad)\n        return grads\n\n@dataclass\nclass ReLU(Operation):\n    \"\"\"\n    A node that applies the ReLU function to its input.\n    Note that this should only have one child.\n    \"\"\"\n    def op(self, x):\n        return max(0, x[0])\n\n    def grad(self):\n        return [1.0 if self.children[0].forward() > 0 else 0.0]\n\n@dataclass\nclass Sigmoid(Operation):\n    \"\"\"\n    A node that applies the sigmoid function to its input.\n    Note that this should only have one child.\n    \"\"\"\n    def op(self, x):\n        return 1 / (1 + math.exp(-x[0]))\n\n    def grad(self):\n        return [self.forward() * (1 - self.forward())]\n```", "```py\nlinear_classifier = Add([\n    Multiply([\n        Parameter(),\n        Input()\n    ]),\n    Parameter()\n])\n```", "```py\ndef find_input_nodes(self) -> List[Input]:\n    \"\"\"Find all of the input nodes in the subtree rooted at this node\"\"\"\n    input_nodes = []\n    for child in self.children:\n        if isinstance(child, Input):\n            input_nodes.append(child)\n        elif isinstance(child, Operation):\n            input_nodes.extend(child.find_input_nodes())\n    return input_nodes\n```", "```py\ndef predict(self, inputs: List[float]) -> float:\n    \"\"\"Evaluate the network on the given inputs\"\"\"\n    input_nodes = self.find_input_nodes()\n    assert len(input_nodes) == len(inputs)\n    for input_node, value in zip(input_nodes, inputs):\n        input_node.value = value\n    return self.forward()\n```", "```py\nfrom typing import Callable, Tuple\n\ndef train_model(\n    model: Operation, \n    loss_fn: Callable[[float, float], float], \n    loss_grad_fn: Callable[[float, float], float],\n    data: List[Tuple[List[float], float]], \n    epochs: int=1000,\n    print_every: int=100\n):\n    \"\"\"Train the given model on the given data\"\"\"\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for x, y in data:\n            prediction = model.predict(x)\n            total_loss += loss_fn(y, prediction)\n            model.backward(loss_grad_fn(y, prediction))\n        if epoch % print_every == 0:\n            print(f'Epoch {epoch}: loss={total_loss/len(data)}')\n```", "```py\ndef mse_loss(y_true: float, y_pred: float) -> float:\n    return (y_true - y_pred) ** 2\n\ndef mse_loss_grad(y_true: float, y_pred: float) -> float:\n    return -2 * (y_true - y_pred)\n\ndef fahrenheit_to_celsius(x: float) -> float:\n    return (x - 32) * 5 / 9\n\ndef generate_f_to_c_data() -> List[List[float]]:\n    data = []\n    for _ in range(1000):\n        f = random.uniform(-1, 1)\n        data.append([[f], fahrenheit_to_celsius(f)])\n    return data\n\nlinear_classifier = Add([\n    Multiply([\n        Parameter(),\n        Input()\n    ]),\n    Parameter()\n])\n\ntrain_model(linear_classifier, mse_loss, mse_loss_grad, generate_f_to_c_data())\n```", "```py\nprint(linear_classifier)\nprint(linear_classifier.predict([32]))\n\n>> Add(children=[Multiply(children=[Parameter(0.5555555555555556), Input(0.8930639016107234)]), Parameter(-17.777777777777782)])\n>> -1.7763568394002505e-14\n```", "```py\ndef bce_loss(y_true: float, y_pred: float, eps: float=0.00000001) -> float:\n    y_pred = min(max(y_pred, eps), 1 - eps)\n    return -y_true * math.log(y_pred) - (1 - y_true) * math.log(1 - y_pred)\n\ndef bce_loss_grad(y_true: float, y_pred: float, eps: float=0.00000001) -> float:\n    y_pred = min(max(y_pred, eps), 1 - eps)\n    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n\ndef generate_binary_data():\n    data = []\n    for _ in range(1000):\n        x = random.uniform(-1, 1)\n        y = random.uniform(-1, 1)\n        data.append([(x, y), 1 if y > x else 0])\n    return data\n\nmodel_binary = Sigmoid(\n    [\n        Add(\n            [\n                Multiply(\n                    [\n                        Parameter(),\n                        ReLU(\n                            [\n                                Add(\n                                    [\n                                        Multiply(\n                                            [\n                                                Parameter(),\n                                                Input()\n                                            ]\n                                        ),\n                                        Multiply(\n                                            [\n                                                Parameter(),\n                                                Input()\n                                            ]\n                                        ),\n                                        Parameter()\n                                    ]\n                                )\n                            ]\n                        )\n                    ]\n                ),\n                Parameter()\n            ]\n        )\n    ]\n)\n\ntrain_model(model_binary, bce_loss, bce_loss_grad, generate_binary_data())\n```", "```py\nprint(model_binary.predict([1, 0]))\nprint(model_binary.predict([0, 1]))\nprint(model_binary.predict([0, 1000]))\nprint(model_binary.predict([-5, 3]))\nprint(model_binary.predict([0, 0]))\n\n>> 3.7310797619230176e-66\n>> 0.9997781079343139\n>> 0.9997781079343139\n>> 0.9997781079343139\n>> 0.23791579184662365\n```"]