["```py\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, api_key=\"\")\n```", "```py\nollama run llama3.1\n```", "```py\npython --version\n```", "```py\npip install streamlit PyPDF2 langchain langchain-community spacy faiss-cpu\n```", "```py\npython -m spacy download en_core_web_sm\n```", "```py\nimport subprocess\nimport streamlit as st\nfrom PyPDF2 import PdfReader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.tools.retriever import create_retriever_tool\nfrom langchain_core.prompts import ChatPromptTemplate\nimport os\nimport re\nimport psutil\n```", "```py\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n```", "```py\ndef pdf_read(pdf_doc):\n    \"\"\"Read the text from PDF document.\"\"\"\n    text = \"\"\n    for pdf in pdf_doc:\n        pdf_reader = PdfReader(pdf)\n        for page in pdf_reader.pages:\n            page_text = page.extract_text()\n            if page_text:\n                text += page_text\n    return text\n```", "```py\ndef create_text_chunks(text, chunk_size=1000, chunk_overlap=200):\n    \"\"\"Create text chunks from a large text block.\"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap\n    )\n    text_chunks = text_splitter.split_text(text)\n    return text_chunks\n```", "```py\nembeddings = SpacyEmbeddings(model_name=\"en_core_web_sm\")\n```", "```py\ndef vector_store(text_chunks):\n    \"\"\"Create a vector store for the text chunks.\"\"\"\n    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n    vector_store.save_local(\"faiss_db\")\n```", "```py\ndef query_llama_via_cli(input_text):\n    \"\"\"Query the Llama model via the CLI.\"\"\"\n    try:\n        # Start the interactive process\n        process = subprocess.Popen(\n            [\"ollama\", \"run\", \"llama3.1\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,  # Ensure that communication takes place as text (UTF-8)\n            encoding='utf-8',  # Set UTF-8 encoding explicitly\n            errors='ignore',  # Ignore incorrect characters\n            bufsize=1\n        )\n\n        # Send the input to the process\n        stdout, stderr = process.communicate(input=f\"{input_text}\\n\", timeout=30)\n\n        # Check error output\n        if process.returncode != 0:\n            return f\"Error in the model request: {stderr.strip()}\"\n\n        # Filter response and remove control characters\n        response = re.sub(r'\\x1b\\[.*?m', '', stdout)  # Remove ANSI codes\n\n        # Extract the relevant answer\n        return extract_relevant_answer(response)\n\n    except subprocess.TimeoutExpired:\n        process.kill()\n        return \"Timeout for the model request\"\n    except Exception as e:\n        return f\"An unexpected error has occurred: {str(e)}\"\n```", "```py\ndef extract_relevant_answer(full_response):\n    \"\"\"Extract the relevant response from the full model response.\"\"\"\n    response_lines = full_response.splitlines()\n\n    # Search for the relevant answer; if there is a marker, it can be used here\n    if response_lines:\n        # Assume that the answer comes as a complete return to be filtered\n        return \"\\n\".join(response_lines).strip()\n\n    return \"No answer received\"\n```", "```py\ndef get_conversational_chain(context, ques):\n    \"\"\"Create the input for the model based on the prompt and context.\"\"\"\n    # Define the prompt behavior\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"\"\"You are an intelligent and helpful assistant. Your goal is to provide the most accurate and detailed answers \n                possible to any question you receive. Use all available context to enhance your answers, and explain complex \n                concepts in a simple manner. If additional information might help, suggest further areas for exploration. If the \n                answer is not available in the provided context, state this clearly and offer related insights when possible.\"\"\",\n            ),\n            (\"human\", \"{input}\"),\n            (\"placeholder\", \"{agent_scratchpad}\"),\n        ]\n    )\n\n    # Combine the context and the question\n    input_text = f\"Prompt: {prompt.format(input=ques)}\\nContext: {context}\\nQuestion: {ques}\"\n\n    # Request to the model\n    response = query_llama_via_cli(input_text)\n    st.write(\"PDF: \", response)  # The answer is displayed here\n```", "```py\ndef user_input(user_question, pdf_text):\n    \"\"\"Processes the user input and calls up the model.\"\"\"\n    # Use the entire text of the PDF as context\n    context = pdf_text\n\n    # Configure and request\n    get_conversational_chain(context, user_question)\n```", "```py\ndef main():\n    \"\"\"Main function of the Streamlit application.\"\"\"\n    st.set_page_config(page_title=\"CHAT WITH YOUR PDF\")\n    st.header(\"PDF CHAT APP\")\n\n    pdf_text = \"\"\n    pdf_doc = st.file_uploader(\"Upload your PDF Files and confirm your question\", accept_multiple_files=True)\n\n    if pdf_doc:\n        pdf_text = pdf_read(pdf_doc)  # Read the entire PDF text\n\n    user_question = st.text_input(\"Ask a Question from the PDF Files\")\n\n    if user_question and pdf_text:\n        user_input(user_question, pdf_text)\n\n    # Monitor RAM consumption\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss / (1024 ** 2)  # Conversion to megabytes\n    st.sidebar.write(f\"Memory usage: {memory_usage:.2f} MB\")\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nstreamlit run pca1.py\n```", "```py\n prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                \"\"\"You are an intelligent and helpful assistant. Your goal is to provide the most accurate and detailed answers \n                possible to any question you receive. Use all available context to enhance your answers, and explain complex \n                concepts in a simple manner. If additional information might help, suggest further areas for exploration. If the \n                answer is not available in the provided context, state this clearly and offer related insights when possible.\"\"\",\n            ),\n```", "```py\n # Monitor RAM consumption\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss / (1024 ** 2)  # Conversion to megabytes\n    st.sidebar.write(f\"Memory usage: {memory_usage:.2f} MB\")\n```"]