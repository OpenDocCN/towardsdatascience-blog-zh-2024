["```py\n# Clone the rerun GitHub repository to your local machine.\ngit clone https://github.com/rerun-io/rerun\n\n# Navigate to the rerun repository directory.\ncd rerun\n\n# Install the required Python packages specified in the requirements file\npip install -r examples/python/gesture_detection/requirements.txt\n\n# Run the main Python script for the example\npython examples/python/gesture_detection/main.py\n\n# Run the main Python script for a specific image\npython examples/python/gesture_detection/main.py --image path/to/your/image.jpg\n\n# Run the main Python script for a specific video\npython examples/python/gesture_detection/main.py --video path/to/your/video.mp4\n\n# Run the main Python script with camera stream\npython examples/python/gesture_detection/main.py --camera\n```", "```py\n# Install the required Python packages specified in the requirements file\npip install -r examples/python/gesture_detection/requirements.txt\n```", "```py\nfrom mediapipe.tasks.python import vision\nfrom mediapipe.tasks import python\n\nclass GestureDetectorLogger:\n\n    def __init__(self, video_mode: bool = False):\n        self._video_mode = video_mode\n\n        base_options = python.BaseOptions(\n            model_asset_path='gesture_recognizer.task'\n        )\n        options = vision.GestureRecognizerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO if self._video_mode else mp.tasks.vision.RunningMode.IMAGE\n        )\n        self.recognizer = vision.GestureRecognizer.create_from_options(options)\n\n    def detect(self, image: npt.NDArray[np.uint8]) -> None:\n          image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n          # Get results from Gesture Detection model\n          recognition_result = self.recognizer.recognize(image)\n\n          for i, gesture in enumerate(recognition_result.gestures):\n              # Get the top gesture from the recognition result\n              print(\"Top Gesture Result: \", gesture[0].category_name)\n\n          if recognition_result.hand_landmarks:\n              # Obtain hand landmarks from MediaPipe\n              hand_landmarks = recognition_result.hand_landmarks\n              print(\"Hand Landmarks: \" + str(hand_landmarks))\n\n              # Obtain hand connections from MediaPipe\n              mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n              print(\"Hand Connections: \" + str(mp_hands_connections))\n```", "```py\ndef run_from_sample_image(path)-> None:\n    image = cv2.imread(str(path))\n    show_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    logger = GestureDetectorLogger(video_mode=False)\n    logger.detect_and_log(show_image)\n\n# Run the gesture recognition on a sample image\nrun_from_sample_image(SAMPLE_IMAGE_PATH)\n```", "```py\ndef run_from_video_capture(vid: int | str, max_frame_count: int | None) -> None:\n    \"\"\"\n    Run the detector on a video stream.\n\n    Parameters\n    ----------\n    vid:\n        The video stream to run the detector on. Use 0/1 for the default camera or a path to a video file.\n    max_frame_count:\n        The maximum number of frames to process. If None, process all frames.\n    \"\"\"\n    cap = cv2.VideoCapture(vid)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    detector = GestureDetectorLogger(video_mode=True)\n\n    try:\n        it: Iterable[int] = itertools.count() if max_frame_count is None else range(max_frame_count)\n\n        for frame_idx in tqdm.tqdm(it, desc=\"Processing frames\"):\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            if np.all(frame == 0):\n                continue\n\n            frame_time_nano = int(cap.get(cv2.CAP_PROP_POS_MSEC) * 1e6)\n            if frame_time_nano == 0:\n                frame_time_nano = int(frame_idx * 1000 / fps * 1e6)\n\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            rr.set_time_sequence(\"frame_nr\", frame_idx)\n            rr.set_time_nanos(\"frame_time\", frame_time_nano)\n            detector.detect_and_log(frame, frame_time_nano)\n            rr.log(\n                \"Media/Video\",\n                rr.Image(frame)\n            )\n\n    except KeyboardInterrupt:\n        pass\n\n    cap.release()\n    cv2.destroyAllWindows()\n```", "```py\nclass GestureDetectorLogger:\n\n    def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -> None:\n        # Recognize gestures in the image\n        height, width, _ = image.shape\n        image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n\n        recognition_result = (\n            self.recognizer.recognize_for_video(image, int(frame_time_nano / 1e6))\n            if self._video_mode\n            else self.recognizer.recognize(image)\n        )\n\n        # Clear the values\n        for log_key in [\"Media/Points\", \"Media/Connections\"]:\n            rr.log(log_key, rr.Clear(recursive=True))\n\n        for i, gesture in enumerate(recognition_result.gestures):\n            # Get the top gesture from the recognition result\n            gesture_category = gesture[0].category_name if recognition_result.gestures else \"None\"\n            print(\"Gesture Category: \", gesture_category) # Log the detected gesture\n\n        if recognition_result.hand_landmarks:\n            hand_landmarks = recognition_result.hand_landmarks\n\n            # Convert normalized coordinates to image coordinates\n            points = self.convert_landmarks_to_image_coordinates(hand_landmarks, width, height)\n\n            # Log points to the image and Hand Entity\n            rr.log(\n               \"Media/Points\",\n                rr.Points2D(points, radii=10, colors=[255, 0, 0])\n            )\n\n            # Obtain hand connections from MediaPipe\n            mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n            points1 = [points[connection[0]] for connection in mp_hands_connections]\n            points2 = [points[connection[1]] for connection in mp_hands_connections]\n\n            # Log connections to the image and Hand Entity \n            rr.log(\n               \"Media/Connections\",\n                rr.LineStrips2D(\n                   np.stack((points1, points2), axis=1),\n                   colors=[255, 165, 0]\n                )\n             )\n\n    def convert_landmarks_to_image_coordinates(hand_landmarks, width, height):\n        return [(int(lm.x * width), int(lm.y * height)) for hand_landmark in hand_landmarks for lm in hand_landmark]\n```", "```py\nclass GestureDetectorLogger:\nâ€“\n\n  def __init__(self, video_mode: bool = False):\n      # ... existing code ...\n      rr.log(\n            \"/\",\n            rr.AnnotationContext(\n                rr.ClassDescription(\n                    info=rr.AnnotationInfo(id=0, label=\"Hand3D\"),\n                    keypoint_connections=mp.solutions.hands.HAND_CONNECTIONS\n                )\n            ),\n            timeless=True,\n        )\n       rr.log(\"Hand3D\", rr.ViewCoordinates.RIGHT_HAND_X_DOWN, timeless=True)\n\n   def detect_and_log(self, image: npt.NDArray[np.uint8], frame_time_nano: int | None) -> None:\n      # ... existing code ...\n\n      if recognition_result.hand_landmarks:\n         hand_landmarks = recognition_result.hand_landmarks\n\n         landmark_positions_3d = self.convert_landmarks_to_3d(hand_landmarks)\n         if landmark_positions_3d is not None:\n              rr.log(\n                 \"Hand3D/Points\",\n                 rr.Points3D(landmark_positions_3d, radii=20, class_ids=0, keypoint_ids=[i for i in range(len(landmark_positions_3d))]),\n              )\n\n      # ... existing code ...\n```", "```py\n# For image\nrun_from_sample_image(IMAGE_PATH)\n\n# For saved video\nrun_from_video_capture(VIDEO_PATH)\n\n# For Real-Time\nrun_from_video_capture(0) # mac may need 1\n```"]