# é€»è¾‘å›å½’è§£æï¼šå¸¦ä»£ç ç¤ºä¾‹çš„å¯è§†åŒ–æŒ‡å—ï¼Œé€‚åˆåˆå­¦è€…

> åŸæ–‡ï¼š[`towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10`](https://towardsdatascience.com/logistic-regression-explained-a-visual-guide-with-code-examples-for-beginners-81baf5871505?source=collection_archive---------0-----------------------#2024-09-10)

## åˆ†ç±»ç®—æ³•

## æ‰¾åˆ°é€‚åˆæ•°æ®çš„æœ€ä½³æƒé‡

[](https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------)![Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------)![Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------) [Samy Baladram](https://medium.com/@samybaladram?source=post_page---byline--81baf5871505--------------------------------)

Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--81baf5871505--------------------------------) Â·10 åˆ†é’Ÿé˜…è¯»Â·2024 å¹´ 9 æœˆ 10 æ—¥

--

![](img/08286a411c63fdbe68e056f07d6540a2.png)

`â›³ï¸ æ›´å¤š[åˆ†ç±»ç®—æ³•](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c)è§£æï¼šÂ· è™šæ‹Ÿåˆ†ç±»å™¨ Â· K è¿‘é‚»åˆ†ç±»å™¨ Â· ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ Â· é«˜æ–¯æœ´ç´ è´å¶æ–¯ Â· å†³ç­–æ ‘åˆ†ç±»å™¨ â–¶ é€»è¾‘å›å½’ Â· æ”¯æŒå‘é‡åˆ†ç±»å™¨ Â· å¤šå±‚æ„ŸçŸ¥å™¨`

å°½ç®¡ä¸€äº›åŸºäºæ¦‚ç‡çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚æœ´ç´ è´å¶æ–¯ï¼‰å¯¹ç‰¹å¾ç‹¬ç«‹æ€§åšå‡ºå¤§èƒ†å‡è®¾ï¼Œä½†é€»è¾‘å›å½’é‡‡ç”¨äº†æ›´ä¸ºè°¨æ…çš„æ–¹æ³•ã€‚å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ç»˜åˆ¶ä¸€æ¡ï¼ˆæˆ–ä¸€å¹³é¢ï¼‰å°†ä¸¤ç§ç»“æœåˆ†å¼€çš„çº¿ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä»¥æ›´å¤§çš„çµæ´»æ€§é¢„æµ‹æ¦‚ç‡ã€‚

![](img/add544fe281d9cac3d807605d793740a.png)

æ‰€æœ‰è§†è§‰æ•ˆæœï¼šä½œè€…ä½¿ç”¨ Canva Pro åˆ›å»ºã€‚é’ˆå¯¹ç§»åŠ¨è®¾å¤‡è¿›è¡Œäº†ä¼˜åŒ–ï¼›åœ¨æ¡Œé¢ç«¯å¯èƒ½ä¼šæ˜¾å¾—è¿‡å¤§ã€‚

# å®šä¹‰

é€»è¾‘å›å½’æ˜¯ä¸€ç§ç”¨äºé¢„æµ‹äºŒå…ƒç»“æœçš„ç»Ÿè®¡æ–¹æ³•ã€‚å°½ç®¡åå­—ä¸­æœ‰â€œå›å½’â€ï¼Œä½†å®ƒå®é™…ä¸Šç”¨äºåˆ†ç±»è€Œéå›å½’ã€‚å®ƒä¼°è®¡å®ä¾‹å±äºæŸä¸ªç‰¹å®šç±»åˆ«çš„æ¦‚ç‡ã€‚å¦‚æœä¼°è®¡çš„æ¦‚ç‡å¤§äº 50%ï¼Œæ¨¡å‹é¢„æµ‹è¯¥å®ä¾‹å±äºè¯¥ç±»åˆ«ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

# ğŸ“Š ä½¿ç”¨çš„æ•°æ®é›†

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªäººå·¥é«˜å°”å¤«æ•°æ®é›†ï¼ˆçµæ„Ÿæ¥è‡ª[1]ï¼‰ä½œä¸ºç¤ºä¾‹ã€‚è¯¥æ•°æ®é›†æ ¹æ®å¤©æ°”æ¡ä»¶é¢„æµ‹ä¸€ä¸ªäººæ˜¯å¦ä¼šæ‰“é«˜å°”å¤«ã€‚

ä¸ KNN ç±»ä¼¼ï¼Œé€»è¾‘å›å½’ä¹Ÿè¦æ±‚å…ˆå¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ã€‚å°†ç±»åˆ«åˆ—è½¬æ¢ä¸º 0 å’Œ 1ï¼ŒåŒæ—¶ç¼©æ”¾æ•°å€¼ç‰¹å¾ï¼Œä»¥é¿å…æŸä¸€ç‰¹å¾ä¸»å¯¼è·ç¦»åº¦é‡ã€‚

![](img/be47d8fc7fb892e46cc3ac4f26cdf8f5.png)

åˆ—ï¼šâ€˜Outlookâ€™ï¼ˆå¤©æ°”çŠ¶å†µï¼‰ã€â€˜Temperatureâ€™ï¼ˆæ¸©åº¦ï¼‰ã€â€˜Humidityâ€™ï¼ˆæ¹¿åº¦ï¼‰ã€â€˜Windâ€™ï¼ˆé£é€Ÿï¼‰å’Œâ€˜Playâ€™ï¼ˆç›®æ ‡ç‰¹å¾ï¼‰ã€‚ç±»åˆ«åˆ—ï¼ˆOutlook å’Œ Windyï¼‰ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰è¿›è¡Œç¼–ç ï¼Œè€Œæ•°å€¼åˆ—åˆ™ä½¿ç”¨æ ‡å‡†ç¼©æ”¾ï¼ˆz-æ ‡å‡†åŒ–ï¼‰è¿›è¡Œç¼©æ”¾ã€‚

```py
# Import required libraries
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Create dataset from dictionary
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)

# Prepare data: encode categorical variables
df = pd.get_dummies(df, columns=['Outlook'], prefix='', prefix_sep='', dtype=int)
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Rearrange columns
column_order = ['sunny', 'overcast', 'rainy', 'Temperature', 'Humidity', 'Wind', 'Play']
df = df[column_order]

# Split data into features and target
X, y = df.drop(columns='Play'), df['Play']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Scale numerical features
scaler = StandardScaler()
X_train[['Temperature', 'Humidity']] = scaler.fit_transform(X_train[['Temperature', 'Humidity']])
X_test[['Temperature', 'Humidity']] = scaler.transform(X_test[['Temperature', 'Humidity']])

# Print results
print("Training set:")
print(pd.concat([X_train, y_train], axis=1), '\n')
print("Test set:")
print(pd.concat([X_test, y_test], axis=1))
```

# ä¸»è¦æœºåˆ¶

é€»è¾‘å›å½’é€šè¿‡å¯¹è¾“å…¥ç‰¹å¾çš„çº¿æ€§ç»„åˆåº”ç”¨é€»è¾‘å‡½æ•°æ¥å·¥ä½œã€‚å…¶æ“ä½œè¿‡ç¨‹å¦‚ä¸‹ï¼š

1.  è®¡ç®—è¾“å…¥ç‰¹å¾çš„åŠ æƒå’Œï¼ˆç±»ä¼¼äºçº¿æ€§å›å½’ï¼‰ã€‚

1.  å¯¹è¿™ä¸ªå’Œåº”ç”¨é€»è¾‘å‡½æ•°ï¼ˆä¹Ÿç§°ä¸º Sigmoid å‡½æ•°ï¼‰ï¼Œå®ƒå°†ä»»ä½•å®æ•°æ˜ å°„åˆ° 0 å’Œ 1 ä¹‹é—´çš„å€¼ã€‚

1.  å°†æ­¤å€¼è§£é‡Šä¸ºå±äºæ­£ç±»çš„æ¦‚ç‡ã€‚

1.  ä½¿ç”¨é˜ˆå€¼ï¼ˆé€šå¸¸æ˜¯ 0.5ï¼‰åšå‡ºæœ€ç»ˆçš„åˆ†ç±»å†³ç­–ã€‚

![](img/a7075b4e966c4487e4ddf0397a705947.png)

å¯¹äºæˆ‘ä»¬çš„é«˜å°”å¤«æ•°æ®é›†ï¼Œé€»è¾‘å›å½’å¯èƒ½ä¼šå°†å¤©æ°”å› ç´ åˆå¹¶ä¸ºä¸€ä¸ªå•ä¸€çš„åˆ†æ•°ï¼Œç„¶åå°†æ­¤åˆ†æ•°è½¬æ¢ä¸ºæ‰“é«˜å°”å¤«çš„æ¦‚ç‡ã€‚

# è®­ç»ƒæ­¥éª¤

é€»è¾‘å›å½’çš„è®­ç»ƒè¿‡ç¨‹æ¶‰åŠä¸ºè¾“å…¥ç‰¹å¾æ‰¾åˆ°æœ€ä½³çš„æƒé‡ã€‚ä»¥ä¸‹æ˜¯ä¸€èˆ¬çš„æ­¥éª¤æ¦‚è¿°ï¼š

1.  åˆå§‹åŒ–æƒé‡ï¼ˆé€šå¸¸ä¸ºå°çš„éšæœºå€¼ï¼‰ã€‚

![](img/3b3d36e76e221058bfb5bdef6dbcb540.png)

```py
# Initialize weights (including bias) to 0.1
initial_weights = np.full(X_train_np.shape[1], 0.1)

# Create and display DataFrame for initial weights
print(f"Initial Weights: {initial_weights}")
```

2\. å¯¹äºæ¯ä¸ªè®­ç»ƒç¤ºä¾‹ï¼š

a. ä½¿ç”¨å½“å‰çš„æƒé‡è®¡ç®—é¢„æµ‹æ¦‚ç‡ã€‚

![](img/7ad914e2c81f2ef22d3423c61b065128.png)

```py
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def calculate_probabilities(X, weights):
    z = np.dot(X, weights)
    return sigmoid(z)

def calculate_log_loss(probabilities, y):
    return -y * np.log(probabilities) - (1 - y) * np.log(1 - probabilities)

def create_output_dataframe(X, y, weights):
    probabilities = calculate_probabilities(X, weights)
    log_losses = calculate_log_loss(probabilities, y)

    df = pd.DataFrame({
        'Probability': probabilities,
        'Label': y,
        'Log Loss': log_losses
    })

    return df

def calculate_average_log_loss(X, y, weights):
    probabilities = calculate_probabilities(X, weights)
    log_losses = calculate_log_loss(probabilities, y)
    return np.mean(log_losses)

# Convert X_train and y_train to numpy arrays for easier computation
X_train_np = X_train.to_numpy()
y_train_np = y_train.to_numpy()

# Add a column of 1s to X_train_np for the bias term
X_train_np = np.column_stack((np.ones(X_train_np.shape[0]), X_train_np))

# Create and display DataFrame for initial weights
initial_df = create_output_dataframe(X_train_np, y_train_np, initial_weights)
print(initial_df.to_string(index=False, float_format=lambda x: f"{x:.6f}"))
print(f"\nAverage Log Loss: {calculate_average_log_loss(X_train_np, y_train_np, initial_weights):.6f}")
```

b. é€šè¿‡è®¡ç®—å…¶å¯¹æ•°æŸå¤±ï¼Œå°†è¯¥æ¦‚ç‡ä¸å®é™…ç±»åˆ«æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚

![](img/d03bec8cdef352fdd52c9e1cf0a8985b.png)

3\. æ›´æ–°æƒé‡ä»¥æœ€å°åŒ–æŸå¤±ï¼ˆé€šå¸¸ä½¿ç”¨ä¸€äº›ä¼˜åŒ–ç®—æ³•ï¼Œå¦‚æ¢¯åº¦ä¸‹é™ã€‚è¿™åŒ…æ‹¬åå¤è¿›è¡Œæ­¥éª¤ 2ï¼Œç›´åˆ°å¯¹æ•°æŸå¤±æ— æ³•è¿›ä¸€æ­¥å‡å°ï¼‰ã€‚

![](img/7970c48934b6717e83f2f94d7f770846.png)

```py
def gradient_descent_step(X, y, weights, learning_rate):
    m = len(y)
    probabilities = calculate_probabilities(X, weights)
    gradient = np.dot(X.T, (probabilities - y)) / m
    new_weights = weights - learning_rate * gradient  # Create new array for updated weights
    return new_weights

# Perform one step of gradient descent (one of the simplest optimization algorithm)
learning_rate = 0.1
updated_weights = gradient_descent_step(X_train_np, y_train_np, initial_weights, learning_rate)

# Print initial and updated weights
print("\nInitial weights:")
for feature, weight in zip(['Bias'] + list(X_train.columns), initial_weights):
    print(f"{feature:11}: {weight:.2f}")

print("\nUpdated weights after one iteration:")
for feature, weight in zip(['Bias'] + list(X_train.columns), updated_weights):
    print(f"{feature:11}: {weight:.2f}")
```

```py
# With sklearn, you can get the final weights (coefficients)
# and final bias (intercepts) easily.
# The result is almost the same as doing it manually above.

from sklearn.linear_model import LogisticRegression

lr_clf = LogisticRegression(penalty=None, solver='saga')
lr_clf.fit(X_train, y_train)

coefficients = lr_clf.coef_
intercept = lr_clf.intercept_

y_train_prob = lr_clf.predict_proba(X_train)[:, 1]
loss = -np.mean(y_train * np.log(y_train_prob) + (1 - y_train) * np.log(1 - y_train_prob))

print(f"Weights & Bias Final: {coefficients[0].round(2)}, {round(intercept[0],2)}")
print("Loss Final:", loss.round(3))
```

# åˆ†ç±»æ­¥éª¤

ä¸€æ—¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼š

1\. å¯¹äºæ–°å®ä¾‹ï¼Œä½¿ç”¨æœ€ç»ˆæƒé‡ï¼ˆä¹Ÿç§°ä¸ºç³»æ•°ï¼‰è®¡ç®—æ¦‚ç‡ï¼Œå°±åƒè®­ç»ƒæ­¥éª¤ä¸­ä¸€æ ·ã€‚

2\. é€šè¿‡æŸ¥çœ‹æ¦‚ç‡æ¥è§£é‡Šè¾“å‡ºï¼šå¦‚æœ *p* â‰¥ 0.5ï¼Œé¢„æµ‹ä¸ºç±»åˆ« 1ï¼›å¦åˆ™ï¼Œé¢„æµ‹ä¸ºç±»åˆ« 0

![](img/99a363cd8b98d3754e27643122b7c9c7.png)

```py
# Calculate prediction probability
predicted_probs = lr_clf.predict_proba(X_test)[:, 1]

z_values = np.log(predicted_probs / (1 - predicted_probs))

result_df = pd.DataFrame({
    'ID': X_test.index,
    'Z-Values': z_values.round(3),
    'Probabilities': predicted_probs.round(3)
}).set_index('ID')

print(result_df)

# Make predictions
y_pred = lr_clf.predict(X_test)
print(y_pred)
```

## è¯„ä¼°æ­¥éª¤

![](img/c7574523a49fa326eb623dcf5cf0d23c.png)

```py
result_df = pd.DataFrame({
    'ID': X_test.index,
    'Label': y_test,
    'Probabilities': predicted_probs.round(2),
    'Prediction': y_pred,
}).set_index('ID')

print(result_df)
```

# å…³é”®å‚æ•°

é€»è¾‘å›å½’æœ‰å‡ ä¸ªé‡è¦çš„å‚æ•°æ¥æ§åˆ¶å…¶è¡Œä¸ºï¼š

**1.æƒ©ç½šé¡¹**ï¼šä½¿ç”¨çš„æ­£åˆ™åŒ–ç±»å‹ï¼ˆâ€˜l1â€™ï¼Œâ€˜l2â€™ï¼Œâ€˜elasticnetâ€™ æˆ– â€˜noneâ€™ï¼‰ã€‚é€»è¾‘å›å½’ä¸­çš„æ­£åˆ™åŒ–é€šè¿‡åœ¨æ¨¡å‹çš„æŸå¤±å‡½æ•°ä¸­åŠ å…¥æƒ©ç½šé¡¹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¹¶é¼“åŠ±ç®€åŒ–æ¨¡å‹ã€‚

![](img/d6388db7b9a3516fa793253062783429.png)

```py
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

regs = [None, 'l1', 'l2']
coeff_dict = {}

for reg in regs:
    lr_clf = LogisticRegression(penalty=reg, solver='saga')
    lr_clf.fit(X_train, y_train)
    coefficients = lr_clf.coef_
    intercept = lr_clf.intercept_
    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]
    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))
    predictions = lr_clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    coeff_dict[reg] = {
        'Coefficients': coefficients,
        'Intercept': intercept,
        'Loss': loss,
        'Accuracy': accuracy
    }

for reg, vals in coeff_dict.items():
    print(f"{reg}: Coeff: {vals['Coefficients'][0].round(2)}, Intercept: {vals['Intercept'].round(2)}, Loss: {vals['Loss'].round(3)}, Accuracy: {vals['Accuracy'].round(3)}")
```

**2.æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆCï¼‰**ï¼šæ§åˆ¶æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸ä¿æŒæ¨¡å‹ç®€æ´ä¹‹é—´çš„æƒè¡¡ã€‚è¾ƒå°çš„ C æ„å‘³ç€æ›´å¼ºçš„æ­£åˆ™åŒ–ã€‚

![](img/b456b69f56e095c9ced88959e77bab79.png)

```py
# List of regularization strengths to try for L1
strengths = [0.001, 0.01, 0.1, 1, 10, 100]

coeff_dict = {}

for strength in strengths:
    lr_clf = LogisticRegression(penalty='l1', C=strength, solver='saga')
    lr_clf.fit(X_train, y_train)

    coefficients = lr_clf.coef_
    intercept = lr_clf.intercept_

    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]
    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))
    predictions = lr_clf.predict(X_test)

    accuracy = accuracy_score(y_test, predictions)

    coeff_dict[f'L1_{strength}'] = {
        'Coefficients': coefficients[0].round(2),
        'Intercept': round(intercept[0],2),
        'Loss': round(loss,3),
        'Accuracy': round(accuracy*100,2)
    }

print(pd.DataFrame(coeff_dict).T)
```

```py
# List of regularization strengths to try for L2
strengths = [0.001, 0.01, 0.1, 1, 10, 100]

coeff_dict = {}

for strength in strengths:
    lr_clf = LogisticRegression(penalty='l2', C=strength, solver='saga')
    lr_clf.fit(X_train, y_train)

    coefficients = lr_clf.coef_
    intercept = lr_clf.intercept_

    predicted_probs = lr_clf.predict_proba(X_train)[:, 1]
    loss = -np.mean(y_train * np.log(predicted_probs) + (1 - y_train) * np.log(1 - predicted_probs))
    predictions = lr_clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    coeff_dict[f'L2_{strength}'] = {
        'Coefficients': coefficients[0].round(2),
        'Intercept': round(intercept[0],2),
        'Loss': round(loss,3),
        'Accuracy': round(accuracy*100,2)
    }

print(pd.DataFrame(coeff_dict).T)
```

**3.æ±‚è§£å™¨**ï¼šç”¨äºä¼˜åŒ–çš„ç®—æ³•ï¼ˆâ€˜liblinearâ€™ï¼Œâ€˜newton-cgâ€™ï¼Œâ€˜lbfgsâ€™ï¼Œâ€˜sagâ€™ï¼Œâ€˜sagaâ€™ï¼‰ã€‚æŸäº›æ­£åˆ™åŒ–å¯èƒ½éœ€è¦ç‰¹å®šçš„ç®—æ³•ã€‚

**4.æœ€å¤§è¿­ä»£æ¬¡æ•°**ï¼šæ±‚è§£å™¨æ”¶æ•›çš„æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚

å¯¹äºæˆ‘ä»¬çš„é«˜å°”å¤«æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯èƒ½ä»¥â€˜l2â€™æƒ©ç½šé¡¹ã€â€˜liblinearâ€™æ±‚è§£å™¨å’Œ C=1.0 ä½œä¸ºåŸºå‡†è¿›è¡Œå°è¯•ã€‚

# ä¼˜ç‚¹ä¸ç¼ºç‚¹

åƒæœºå™¨å­¦ä¹ ä¸­çš„ä»»ä½•ç®—æ³•ä¸€æ ·ï¼Œé€»è¾‘å›å½’ä¹Ÿæœ‰å…¶ä¼˜ç‚¹å’Œå±€é™æ€§ã€‚

## ä¼˜ç‚¹ï¼š

1.  **ç®€å•æ€§**ï¼šæ˜“äºå®ç°å’Œç†è§£ã€‚

1.  **å¯è§£é‡Šæ€§**ï¼šæƒé‡ç›´æ¥æ˜¾ç¤ºæ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ã€‚

1.  **æ•ˆç‡**ï¼šä¸éœ€è¦è¿‡å¤šçš„è®¡ç®—èƒ½åŠ›ã€‚

1.  **æ¦‚ç‡è¾“å‡º**ï¼šæä¾›æ¦‚ç‡è€Œä¸ä»…ä»…æ˜¯åˆ†ç±»ã€‚

## ç¼ºç‚¹ï¼š

1.  **çº¿æ€§å‡è®¾**ï¼šå‡è®¾ç‰¹å¾ä¸ç»“æœçš„å¯¹æ•°å‡ ç‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»ã€‚

1.  **ç‰¹å¾ç‹¬ç«‹æ€§**ï¼šå‡è®¾ç‰¹å¾ä¹‹é—´æ²¡æœ‰é«˜åº¦ç›¸å…³æ€§ã€‚

1.  **æœ‰é™çš„å¤æ‚æ€§**ï¼šåœ¨å†³ç­–è¾¹ç•Œé«˜åº¦éçº¿æ€§çš„æƒ…å†µä¸‹ï¼Œå¯èƒ½å‡ºç°æ¬ æ‹Ÿåˆã€‚

1.  **éœ€è¦æ›´å¤šæ•°æ®**ï¼šéœ€è¦ç›¸å¯¹è¾ƒå¤§çš„æ ·æœ¬é‡ä»¥è·å¾—ç¨³å®šçš„ç»“æœã€‚

åœ¨æˆ‘ä»¬çš„é«˜å°”å¤«ç¤ºä¾‹ä¸­ï¼Œé€»è¾‘å›å½’å¯èƒ½æä¾›ä¸€ä¸ªæ¸…æ™°ã€å¯è§£é‡Šçš„æ¨¡å‹ï¼Œè¯´æ˜æ¯ä¸ªå¤©æ°”å› ç´ å¦‚ä½•å½±å“æ‰“é«˜å°”å¤«çš„å†³ç­–ã€‚ç„¶è€Œï¼Œå¦‚æœå†³ç­–æ¶‰åŠå¤©æ°”æ¡ä»¶ä¹‹é—´çš„å¤æ‚äº¤äº’ï¼Œæ— æ³•é€šè¿‡çº¿æ€§æ¨¡å‹æ•æ‰ï¼Œé‚£ä¹ˆå®ƒå¯èƒ½ä¼šé‡åˆ°å›°éš¾ã€‚

# æœ€åå¤‡æ³¨

é€»è¾‘å›å½’ä½œä¸ºä¸€ç§å¼ºå¤§è€Œç®€æ´çš„åˆ†ç±»å·¥å…·è„±é¢–è€Œå‡ºã€‚å®ƒçš„ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿå¤„ç†å¤æ‚æ•°æ®çš„åŒæ—¶ä¿æŒæ˜“äºè§£é‡Šã€‚ä¸ä¸€äº›å…¶ä»–åŸºç¡€æ¨¡å‹ä¸åŒï¼Œå®ƒæä¾›å¹³æ»‘çš„æ¦‚ç‡ä¼°è®¡ï¼Œå¹¶ä¸”èƒ½å¾ˆå¥½åœ°å¤„ç†å¤šä¸ªç‰¹å¾ã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œä»é¢„æµ‹å®¢æˆ·è¡Œä¸ºåˆ°åŒ»å­¦è¯Šæ–­ï¼Œé€»è¾‘å›å½’å¾€å¾€è¡¨ç°å‡ºæƒŠäººçš„æ•ˆæœã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªè¿‡æ¸¡å·¥å…·â€”â€”å®ƒæ˜¯ä¸€ä¸ªå¯é çš„æ¨¡å‹ï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹èƒ½ä¸æ›´å¤æ‚çš„æ¨¡å‹åŒ¹æ•Œã€‚

# ğŸŒŸ é€»è¾‘å›å½’ä»£ç æ€»ç»“

```py
# Import required libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load the dataset
dataset_dict = {
    'Outlook': ['sunny', 'sunny', 'overcast', 'rainy', 'rainy', 'rainy', 'overcast', 'sunny', 'sunny', 'rainy', 'sunny', 'overcast', 'overcast', 'rainy', 'sunny', 'overcast', 'rainy', 'sunny', 'sunny', 'rainy', 'overcast', 'rainy', 'sunny', 'overcast', 'sunny', 'overcast', 'rainy', 'overcast'],
    'Temperature': [85.0, 80.0, 83.0, 70.0, 68.0, 65.0, 64.0, 72.0, 69.0, 75.0, 75.0, 72.0, 81.0, 71.0, 81.0, 74.0, 76.0, 78.0, 82.0, 67.0, 85.0, 73.0, 88.0, 77.0, 79.0, 80.0, 66.0, 84.0],
    'Humidity': [85.0, 90.0, 78.0, 96.0, 80.0, 70.0, 65.0, 95.0, 70.0, 80.0, 70.0, 90.0, 75.0, 80.0, 88.0, 92.0, 85.0, 75.0, 92.0, 90.0, 85.0, 88.0, 65.0, 70.0, 60.0, 95.0, 70.0, 78.0],
    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],
    'Play': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']
}
df = pd.DataFrame(dataset_dict)

# Prepare data: encode categorical variables
df = pd.get_dummies(df, columns=['Outlook'],  prefix='', prefix_sep='', dtype=int)
df['Wind'] = df['Wind'].astype(int)
df['Play'] = (df['Play'] == 'Yes').astype(int)

# Split data into training and testing sets
X, y = df.drop(columns='Play'), df['Play']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)

# Scale numerical features
scaler = StandardScaler()
float_cols = X_train.select_dtypes(include=['float64']).columns
X_train[float_cols] = scaler.fit_transform(X_train[float_cols])
X_test[float_cols] = scaler.transform(X_test[float_cols])

# Train the model
lr_clf = LogisticRegression(penalty='l2', C=1, solver='saga')
lr_clf.fit(X_train, y_train)

# Make predictions
y_pred = lr_clf.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
```

## è¿›ä¸€æ­¥é˜…è¯»

å…³äº[é€»è¾‘å›å½’](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)åŠå…¶åœ¨ scikit-learn ä¸­çš„å®ç°ï¼Œè¯»è€…å¯ä»¥å‚è€ƒå®˜æ–¹æ–‡æ¡£[2]ï¼Œè¯¥æ–‡æ¡£æä¾›äº†å…³äºå…¶ä½¿ç”¨å’Œå‚æ•°çš„å…¨é¢ä¿¡æ¯ã€‚

## æŠ€æœ¯ç¯å¢ƒ

æœ¬æ–‡ä½¿ç”¨ Python 3.7 å’Œ scikit-learn 1.5 ç‰ˆæœ¬ã€‚è™½ç„¶æ‰€è®¨è®ºçš„æ¦‚å¿µä¸€èˆ¬é€‚ç”¨ï¼Œä½†å…·ä½“çš„ä»£ç å®ç°å¯èƒ½ä¼šå› ç‰ˆæœ¬ä¸åŒè€Œç•¥æœ‰å·®å¼‚ã€‚

## å…³äºæ’å›¾

é™¤éå¦æœ‰è¯´æ˜ï¼Œæ‰€æœ‰å›¾ç‰‡å‡ç”±ä½œè€…åˆ›ä½œï¼Œé‡‡ç”¨äº†æ¥è‡ª Canva Pro çš„æˆæƒè®¾è®¡å…ƒç´ ã€‚

![](img/3954bb576147e89326d88edd83f57b51.png)

è·å–ç®€æ´çš„è§†è§‰æ€»ç»“ï¼Œè¯·æŸ¥çœ‹[Instagram ç›¸å…³å¸–å­](https://www.instagram.com/p/C_viu8bSXJ8/)ã€‚

## å‚è€ƒæ–‡çŒ®

[1] T. M. Mitchell, [æœºå™¨å­¦ä¹ ](https://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)ï¼ˆ1997ï¼‰ï¼ŒMcGraw-Hill ç§‘å­¦/å·¥ç¨‹/æ•°å­¦ï¼Œç¬¬ 59 é¡µ

ğ™ğ™šğ™š ğ™¢ğ™¤ğ™§ğ™š ğ˜¾ğ™¡ğ™–ğ™¨ğ™¨ğ™ğ™›ğ™ğ™˜ğ™–ğ™©ğ™ğ™¤ğ™£ ğ˜¼ğ™¡ğ™œğ™¤ğ™§ğ™ğ™©ğ™ğ™¢ğ™¨ ğ™ğ™šğ™§ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------)

## åˆ†ç±»ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----81baf5871505--------------------------------)8 ä¸ªæ•…äº‹ï¼[](../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png)![](img/6ea70d9d2d9456e0c221388dbb253be8.png)![](img/7221f0777228e7bcf08c1adb44a8eb76.png)

ğ™”ğ™¤ğ™ª ğ™¢ğ™ğ™œğ™ğ™© ğ™–ğ™¡ğ™¨ğ™¤ ğ™¡ğ™ğ™ ğ™š:

![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------)

## å›å½’ç®—æ³•

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----81baf5871505--------------------------------)5 ä¸ªæ•…äº‹ï¼ä¸€ä¸ªæˆ´ç€ç²‰è‰²å¸½å­çš„åŒé©¬å°¾å¡é€šå¨ƒå¨ƒã€‚è¿™ä¸ªâ€œå‡äººâ€å¨ƒå¨ƒï¼Œé€šè¿‡å…¶åŸºæœ¬çš„è®¾è®¡å’Œå¿ƒå½¢è£…é¥°çš„è¡¬è¡«ï¼Œç›´è§‚åœ°å‘ˆç°äº†æœºå™¨å­¦ä¹ ä¸­â€œå‡å›å½’å™¨â€çš„æ¦‚å¿µã€‚å°±åƒè¿™ä¸ªç©å…·èˆ¬çš„å½¢è±¡æ˜¯ä¸€ä¸ªç®€åŒ–ã€é™æ€çš„äººçš„ä»£è¡¨ä¸€æ ·ï¼Œå‡å›å½’å™¨æ˜¯ä½œä¸ºåŸºå‡†çš„åŸºæœ¬æ¨¡å‹ï¼Œç”¨äºæ›´å¤æ‚çš„åˆ†æã€‚![](img/44e6d84e61c895757ff31e27943ee597.png)![](img/7f3e5f3e2aca2feec035ca92e1bc440a.png)![Samy Baladram](img/835013c69e08fec04ad9ca465c2adf6c.png)

[Samy Baladram](https://medium.com/@samybaladram?source=post_page-----81baf5871505--------------------------------)

## é›†æˆå­¦ä¹ 

[æŸ¥çœ‹åˆ—è¡¨](https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----81baf5871505--------------------------------)4 ä¸ªæ•…äº‹ï¼[](../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png)![](img/22a5d43568e70222eb89fd36789a9333.png)![](img/8ea1a2f29053080a5feffc709f5b8669.png)
