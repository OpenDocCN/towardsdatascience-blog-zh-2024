<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>The German Tank Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>The German Tank Problem</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-german-tank-problem-3455237aaf43?source=collection_archive---------2-----------------------#2024-03-06">https://towardsdatascience.com/the-german-tank-problem-3455237aaf43?source=collection_archive---------2-----------------------#2024-03-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1eb1" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Estimating your chances of winning the lottery with sampling</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@doriandrost?source=post_page---byline--3455237aaf43--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Dorian Drost" class="l ep by dd de cx" src="../Images/1795395ad0586eafd83d3e2f7b975ca8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*5v2p4tW_yrIG7b5PxYyFvA.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--3455237aaf43--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@doriandrost?source=post_page---byline--3455237aaf43--------------------------------" rel="noopener follow">Dorian Drost</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--3455237aaf43--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">11 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="b868" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Statistical estimates can be fascinating, can’t they? By just sampling a few instances from a population, you can infer properties of that population such as the mean value or the variance. Likewise, under the right circumstances, it is possible to estimate the<strong class="ml fr"> total size</strong> of the population, as I want to show you in this article.</p><p id="b8db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will use the example of drawing lottery tickets to estimate how many tickets there are in total, and hence calculate the likelihood of winning. More formally, this means to estimate the population size given a discrete uniform distribution. We will see different estimates and discuss their differences and weaknesses. In addition, I will point you to some other use cases this approach can be used in.</p><h2 id="3410" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Playing the lottery</h2><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob oc"><img src="../Images/3a2591c798b46eb0e58bb2b309b57953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zbgF4-2ecSKD_VK1"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">I’m too anxious to ride one of those, but if it pleases you…Photo by <a class="af ot" href="https://unsplash.com/@shapeyourbliss?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Oneisha Lee</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d74f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Let’s imagine I go to a state fair and buy some tickets in the lottery. As a data scientist, I want to know the probability of winning the main prize, of course. Let’s assume there is just a single ticket that wins the main prize. So, to estimate the likelihood of winning, I need to know the total number of lottery tickets N, as my chance of winning is 1/N then (or k/N, if I buy k tickets). But how can I estimate that N by just buying a few tickets (which are, as I saw, all losers)?</p><p id="66cf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I will make use of the fact, that the lottery tickets have numbers on them, and I assume, that these are consecutive running numbers (which means, that I assume a discrete uniform distribution). Say I have bought some tickets and their numbers in order are [242,412,823,1429,1702]. What do I know about the total number of tickets now? Well, obviously there are at least 1702 tickets (as that is the highest number I have seen so far). That gives me a first lower bound of the number of tickets, but how accurate is it for the actual number of tickets? Just because the highest number I have drawn is 1702, that doesn’t mean that there are any numbers higher than that. It is very unlikely, that I caught the lottery ticket with the highest number in my sample.</p><p id="8165" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, we can make more out of the data. Let us think as follows: If we knew the middle number of all the tickets, we could easily derive the total number from that: If the middle number is <em class="ou">m</em>, then there are <em class="ou">m-1</em> tickets below that middle number, and there are <em class="ou">m-1</em> tickets above that. That is, the total number of tickets would be <em class="ou">(m-1) + (m-1) + 1</em>, (with the <em class="ou">+1</em> being the ticket of number m itself), which is equal to <em class="ou">2m-1</em>. We don’t know that middle number m, but we can estimate it by the mean or the median of our sample. My sample above has the (rounded) average of 922, which yields 2*922-1 = 1843. That is, from that calculation the estimated number of tickets is 1843.</p><p id="3ee8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That was quite interesting so far, as just from a few lottery ticket numbers, I was able to give an estimate of the total number of tickets. However, you may wonder if that is the best estimate we can get. Let me spoil you right away: It is not.</p><p id="b39e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The method we used has some drawbacks. Let me demonstrate that to you with another example: Say we have the numbers [12,30,88], which leads us to 2*43–1 = 85. That means, the formula suggests there are 85 tickets in total. However, we have ticket number 88 in our sample, so this can not be true at all! There is a general problem with this method: The estimated N can be lower than the highest number in the sample. In that case, the estimate has no meaning at all, as we already know, that the highest number in the sample is a natural lower bound of the overall N.</p><h2 id="58aa" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">A better approach: Using even spacing</h2><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob ov"><img src="../Images/60ee63bed681c8bf2a8a74cfe01c2319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KxWJ6rkH9vf-hGMX"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">These birds are quite evenly spaced on the power line, which is an important concept for our next approach. Photo by <a class="af ot" href="https://unsplash.com/@ridham?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ridham Nagralawala</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="cc06" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Okay, so what can we do? Let us think in a different direction. The lottery tickets I bought have been sampled randomly from the distribution that goes from 1 to unknown N. My ticket with the highest number is number 1702, and I wonder, how far away is this from being the highest ticket at all. In other words, what is the gap between 1702 and N? If I knew that gap, I could easily calculate N from that. What do I know about that gap, though? Well, I have reason to assume that this gap is expected to be as big as all the other gaps between two consecutive tickets in my sample. The gap between the first and the second ticket should, on average, be as big as the gap between the second and the third ticket, and so on. There is no reason why any of those gaps should be bigger or smaller than the others, except for random deviation, of course. I sampled my lottery tickets independently, so they should be evenly spaced on the range of all possible ticket numbers. On average, the numbers in the range of 0 to N would look like birds on a power line, all having the same gap between them.</p><p id="9205" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">That means I expect N-1702 to equal the average of all the other gaps. The other gaps are 242–0=242, 412–242=170, 823–412=411, 1429–823=606, 1702–1429=273, which gives the average 340. Hence I estimate N to be 1702+340=2042. In short, this can be denoted by the following formula:</p><figure class="od oe of og oh oi oa ob paragraph-image"><div class="oa ob ow"><img src="../Images/02570dd48cf176f3be391c20b40f0e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/0*W220qVvRBYXuB-Hb"/></div></figure><p id="2d96" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here <em class="ou">x</em> is the biggest number observed (1702, in our case), and <em class="ou">k</em> is the number of samples (5, in our case). This is just a short form of calculating the average as we just did.</p><h2 id="fdd2" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Let’s do a simulation</h2><p id="ac6c" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">We just saw two estimates of the total number of lottery tickets. First, we calculated <em class="ou">2*m-1</em>, which gave us 1843, and then we used the more sophisticated approach of <em class="ou">x + (x-k)/k</em> and obtained 2042. I wonder which estimation is more correct now? Are my chances of winning the lottery 1/1843 or 1/2042?</p><p id="86c7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To show some properties of the estimates we just used, I did a simulation. I drew samples of different sizes k from a distribution, where the highest number is 2000, and that I did a few hundred times each. Hence we would expect that our estimates also return 2000, at least on average. This is the outcome of the simulation:</p><figure class="od oe of og oh oi oa ob paragraph-image"><div class="oa ob pc"><img src="../Images/fd0f5750fe18415a8727f13c5106914b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*kOhiCwixzQAK9i2mAo0aZw.png"/></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Probability densities of the different estimates for varying k. Note that the ground truth N is 2000. Image by author.</figcaption></figure><p id="dbc4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">What do we see here? On the x-axis, we see the k, i.e. the number of samples we take. For each k, we see the distribution of the estimates based on a few hundred simulations for the two formulas we just got to know. The dark point indicates the mean value of the simulations each, which is always 2000, independent of the k. That is a very interesting point: Both estimates converge to the correct value if they are repeated an infinite number of times.</p><p id="30ed" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, besides the common average, the distributions differ quite a lot. We see, that the formula <em class="ou">2*m-1</em> has higher variance, i.e. its estimates are far away from the real value more often than for the other formula. The variance has a tendency to decrease with higher k though. This decrease does not always hold perfectly, as this is just as simulation and is still subject to random influences. However, it is quite understandable and expected: The more samples I take, the more precise is my estimation. That is a very common property of statistical estimates.</p><p id="bfec" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We also see that the deviations are symmetrical, i.e. underestimating the real value is as likely as overestimating it. For the second approach, this symmetry does not hold: While most of the density is above the real mean, there are more and larger outliers below. How does that come? Let’s retrace how we computed that estimate. We took the biggest number in our sample and added the average gap size to that. Naturally, the biggest number in our sample can only be as big as the biggest number in total (the N that we want to estimate). In that case, we add the average gap size to N, but we can’t get any higher than that with our estimate. In the other direction, the biggest number can be very low. If we are unlucky, we could draw the sample [1,2,3,4,5], in which case the biggest number in our sample (5) is very far away from the actual N. That is why larger deviations are possible in underestimating the real value than in overestimating it.</p><h2 id="bde2" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Which is better?</h2><p id="478a" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">From what we just saw, which estimate is better now? Well, both give the correct value on average. However, the formula <em class="ou">x + (x-k)/k</em> has lower variance, and that is a big advantage. It means, that you are closer to the real value more often. Let me demonstrate that to you. In the following, you see the probability density plots of the two estimates for a sample size of <em class="ou">k=5</em>.</p><figure class="od oe of og oh oi oa ob paragraph-image"><div class="oa ob pc"><img src="../Images/54497bfc9bf4dd95c6320bf84619f8c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*QElQ48b2WPmjNUZPbNeyTQ.png"/></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Probability densities for the two estimates for k=5. The colored shape under the curves is covering the space from N=1750 to N=2250. Image by author.</figcaption></figure><p id="a157" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I highlighted the point N=2000 (the real value for N) with a dotted line. First of all, we still see the symmetry that we have seen before already. In the left plot, the density is distributed symmetrically around N=2000, but in the right plot, it is shifted to the right and has a longer tail to the left. Now let’s take a look at the grey area under the curves each. In both cases, it goes from N=1750 to N=2250. However, in the left plot, this area accounts for 42% of the total area under the curve, while for the right plot, it accounts for 73%. In other words, in the left plot, you have a chance of 42% that your estimate is <strong class="ml fr">not</strong> deviating more than 250 points in either direction. In the right plot, that chance is 73%. That means, you are much more likely to be that close to the real value. However, you are more likely to slightly overestimate than underestimate.</p><p id="c6a4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I can tell you, that x<em class="ou">+ (x-k)/k</em> is the so-called <strong class="ml fr">uniformly minimum variance unbiased estimator</strong>, i.e. it is the estimator with the smallest variance. You won’t find any estimate with lower variance, so this is the best you can use, in general.</p><h2 id="e0d7" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Use cases</h2><figure class="od oe of og oh oi oa ob paragraph-image"><div role="button" tabindex="0" class="oj ok ed ol bh om"><div class="oa ob pd"><img src="../Images/2c9e8ec7cdff2ce5c710fb5f9bd6b7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*67PvTHBZgzyLoJtX"/></div></div><figcaption class="oo op oq oa ob or os bf b bg z dx">Make love, not war 💙. Photo by <a class="af ot" href="https://unsplash.com/@marcoxu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Marco Xu</a> on <a class="af ot" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="69ab" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We just saw how to estimate the total number of elements in a pool, if these elements are indicated by consecutive numbers. Formally, this is a discrete uniform distribution. This problem is commonly known as the <strong class="ml fr">German tank problem</strong>. In the Second World War, the Allies used this approach to estimate how many tanks the German forces had, just by using the serial numbers of the tanks they had destroyed or captured so far.</p><p id="2614" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">We can now think of more examples where we can use this approach. Some are:</p><ul class=""><li id="30a8" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk">You can estimate how many instances of a product have been produced if they are labeled with a running serial number.</li><li id="1779" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk">You can estimate the number of users or customers if you are able to sample some of their IDs.</li><li id="f9ec" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk">You can estimate how many students are (or have been) at your university if you sample students’ matriculation numbers (given that the university has not yet used the first numbers again after reaching the maximum number already).</li></ul><p id="f532" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, be aware that some requirements need to be fulfilled to use that approach. The most important one is, that you indeed draw your samples randomly and independently of each other. If you ask your friends, who have all enrolled in the same year, for their matriculation numbers, they won’t be evenly spaced on the whole range of matriculation numbers but will be quite clustered. Likewise, if you buy articles with running numbers from a store, you need to make sure, that this store got these articles in a random fashion. If it was delivered with the products of numbers 1000 to 1050, you don’t draw randomly from the whole pool.</p><h2 id="030c" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">Conclusion</h2><p id="a4df" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">We just saw different ways of estimating the total number of instances in a pool under discrete uniform distribution. Although both estimates give the same expected value in the long run, they differ in terms of their variance, with one being superior to the other. This is interesting because neither of the approaches is wrong or right. Both are backed by reasonable theoretical considerations and estimate the real population size correctly (in frequentist statistical terms).</p><p id="d7c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">I now know that my chance of winning the state fair lottery is estimated to be 1/2042 = 0.041% (or 0.24% with the 5 tickets I bought). Maybe I should rather invest my money in cotton candy; that would be a save win.</p><h2 id="0d78" class="nf ng fq bf nh ni nj nk nl nm nn no np ms nq nr ns mw nt nu nv na nw nx ny nz bk">References &amp; Literature</h2><p id="0bb3" class="pw-post-body-paragraph mj mk fq ml b go ox mn mo gr oy mq mr ms oz mu mv mw pa my mz na pb nc nd ne fj bk">Mathematical background on the estimates discussed in this article can be found here:</p><ul class=""><li id="7438" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk">Johnson, R. W. (1994). Estimating the size of a population. <em class="ou">Teaching Statistics</em>, <em class="ou">16</em>(2), 50–52.</li></ul><p id="6568" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Also feel free to check out the Wikipedia articles on the German tank problem and related topics, which are quite explanatory:</p><ul class=""><li id="d58a" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pe pf pg bk"><a class="af ot" href="https://en.wikipedia.org/wiki/German_tank_problem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/German_tank_problem</a></li><li id="08bb" class="mj mk fq ml b go ph mn mo gr pi mq mr ms pj mu mv mw pk my mz na pl nc nd ne pe pf pg bk"><a class="af ot" href="https://en.wikipedia.org/wiki/Discrete_uniform_distribution" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Discrete_uniform_distribution</a></li></ul><p id="be87" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This is the script to do the simulation and create the plots shown in the article:</p><pre class="od oe of og oh pm pn po bp pp bb bk"><span id="b744" class="pq ng fq pn b bg pr ps l pt pu">import numpy as np<br/>import random<br/>from scipy.stats import gaussian_kde<br/>import matplotlib.pyplot as plt<br/><br/>if __name__ == "__main__":<br/>    N = 2000<br/>    n_simulations = 500<br/><br/>    estimate_1 = lambda sample: 2 * round(np.mean(sample)) - 1<br/>    estimate_2 = lambda sample: round(max(sample) + ((max(sample) - k) / k))<br/><br/>    estimate_1_per_k, estimate_2_per_k = [],[]<br/>    k_range = range(2,10)<br/>    for k in k_range:<br/>        diffs_1, diffs_2 = [],[]<br/>        # sample without duplicates:<br/>        samples = [random.sample(range(N), k) for _ in range(n_simulations)]<br/>        estimate_1_per_k.append([estimate_1(sample) for sample in samples])<br/>        estimate_2_per_k.append([estimate_2(sample) for sample in samples])<br/><br/>    fig,axs = plt.subplots(1,2, sharey=True, sharex=True)<br/>    axs[0].violinplot(estimate_1_per_k, positions=k_range, showextrema=True)<br/>    axs[0].scatter(k_range, [np.mean(d) for d in estimate_1_per_k], color="purple")<br/>    axs[1].violinplot(estimate_2_per_k, positions=k_range, showextrema=True)<br/>    axs[1].scatter(k_range, [np.mean(d) for d in estimate_2_per_k], color="purple")<br/><br/>    axs[0].set_xlabel("k")<br/>    axs[1].set_xlabel("k")<br/>    axs[0].set_ylabel("Estimated N")<br/>    axs[0].set_title(r"$2\times m-1$")<br/>    axs[1].set_title(r"$x+\frac{x-k}{k}$")<br/>    plt.show()<br/><br/>    plt.gcf().clf()<br/>    k = 5<br/>    xs = np.linspace(500,3500, 500)<br/><br/>    fig, axs = plt.subplots(1,2, sharey=True)<br/>    density_1 = gaussian_kde(estimate_1_per_k[k])<br/>    axs[0].plot(xs, density_1(xs))<br/>    density_2 = gaussian_kde(estimate_2_per_k[k])<br/>    axs[1].plot(xs, density_2(xs))<br/>    axs[0].vlines(2000, ymin=0, ymax=0.003, color="grey", linestyles="dotted")<br/>    axs[1].vlines(2000, ymin=0, ymax=0.003, color="grey", linestyles="dotted")<br/>    axs[0].set_ylim(0,0.0025)<br/><br/>    a,b = 1750, 2250<br/>    ix = np.linspace(a,b)<br/>    verts = [(a, 0), *zip(ix, density_1(ix)), (b, 0)]<br/>    poly = plt.Polygon(verts, facecolor='0.9', edgecolor='0.5')<br/>    axs[0].add_patch(poly)<br/>    print("Integral for estimate 1: ", density_1.integrate_box(a,b))<br/><br/>    verts = [(a, 0), *zip(ix, density_2(ix)), (b, 0)]<br/>    poly = plt.Polygon(verts, facecolor='0.9', edgecolor='0.5')<br/>    axs[1].add_patch(poly)<br/>    print("Integral for estimate 2: ", density_2.integrate_box(a,b))<br/><br/>    axs[0].set_ylabel("Probability Density")<br/>    axs[0].set_xlabel("N")<br/>    axs[1].set_xlabel("N")<br/>    axs[0].set_title(r"$2\times m-1$")<br/>    axs[1].set_title(r"$x+\frac{x-k}{k}$")<br/><br/>    plt.show()<br/><br/></span></pre><p id="765e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ou">Like this article? </em><a class="af ot" href="/@doriandrost" rel="noopener ugc nofollow" target="_blank"><em class="ou">Follow me</em></a><em class="ou"> to be notified of my future posts.</em></p></div></div></div></div>    
</body>
</html>