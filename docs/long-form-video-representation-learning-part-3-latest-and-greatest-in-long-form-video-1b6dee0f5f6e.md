# 长时视频表示学习（第三部分：长时自我中心视频表示学习）

> 原文：[https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14](https://towardsdatascience.com/long-form-video-representation-learning-part-3-latest-and-greatest-in-long-form-video-1b6dee0f5f6e?source=collection_archive---------11-----------------------#2024-05-14)

## 我们探索了具备长时推理能力的新型视频表示学习方法。这是第三部分，提供了我们关于“长时”自我中心视频表示学习的最新研究成果的预览。请参见[第一部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-1-video-as-graphs-c55b609d9100)，讲解视频作为图的内容，以及[第二部分](https://medium.com/@subarna.tripathi/long-form-video-representation-learning-part-2-video-as-sparse-transformers-29fbd0ed9e71)，介绍稀疏视频-文本转换器。

[](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Subarna Tripathi](../Images/0a949764464eeef40a6d3ae0d183873f.png)](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------) [Subarna Tripathi](https://medium.com/@subarna.tripathi?source=post_page---byline--1b6dee0f5f6e--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--1b6dee0f5f6e--------------------------------) ·8分钟阅读·2024年5月14日

--

本系列的前两篇博客描述了从图神经网络到稀疏转换器等不同架构模式如何应对“长时”视频表示学习的挑战。我们展示了显式基于图的方法如何聚合5-10倍更大的时间上下文，但它们是两阶段的方法。接下来，我们探索了如何基于转换器设计出既高效又能端到端学习的模型，并能聚合2倍以上更大的时间上下文。

在这篇博客中，我将带你走进我们最新最伟大的探索，特别是关于自我中心视频理解的部分。正如你所能想象的，自我中心视频或第一人称视频（通常通过头戴式相机拍摄）很可能来自于始终开启的相机，这意味着这些视频通常非常非常长，并且包含大量无关的视觉信息，尤其是在相机佩戴者移动头部时。而且，这种情况在头戴式相机中经常发生。对这类第一人称视频的适当分析可以帮助我们详细了解人类如何与环境互动，如何操作物体，以及最终，他们的目标和意图是什么。自我中心视觉系统的典型应用需要能够表示和处理视频的算法，这些视频的时间跨度通常在几分钟或几个小时之间。例如，行动预测、视频总结和情节记忆检索等应用。

# 自我中心行动场景图：

![](../Images/23d0558b28f8b7f83369cb2f3083eeb5.png)

图1：（图片由作者提供）*自我中心行动场景图是时序动态图（G(t)），捕捉行动动词（蓝色节点）、直接或活跃物体（绿色节点）以及其他与相机佩戴者执行的活动相关的物体（黄色节点）。节点之间的边表示动词与物体之间的关系，或物体对之间的关系。该图随着时间发展，提供了自我中心视频的长时间段表示（虚线）。交互物体通过边界框进行定位*

在与卡塔尼亚大学的联合研究中，我们提出了自我中心行动场景图（EASGs），这是一种用于长时间段理解自我中心视频的新表示方法。EASGs通过提供一个基于图的时序描述，扩展了标准的手动注释自我中心视频表示，例如动词-名词行动标签，从而描述相机佩戴者执行的动作。该描述还包括交互的物体、它们之间的关系，以及动作如何随着时间展开。通过一种新的注释程序，我们扩展了Ego4D数据集，增加了手动标注的自我中心行动场景图，为长时间段的自我中心视频理解提供了丰富的注释集。

EASG提供以动态图形式标注的视频片段。我们将EASG形式化为一个时间变化的有向图G(t) = (V(t), E(t))，其中V(t)是时间t时刻的节点集合，E(t)是这些节点之间的边的集合（见图2）。图G(t)的每一时刻表示一个跨越三帧的自我中心动作，这三帧定义如下[[Ego4D](https://openaccess.thecvf.com/content/CVPR2022/papers/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.pdf)]：前置条件（PRE）、不可回头点（PNR）和后置条件（POST）帧。因此，图G(t)有效地与三帧关联：F(t) = {PREₜ, PNRₜ, POSTₜ}，如图1所示。

## 以自我为中心的场景图生成：

图2展示了一个详细的标注图示例。

![](../Images/f86b56508dd81639cb80b238588b53f1.png)

作者提供的图片

我们通过利用来自Ego4D的现有标注，结合初始化和精炼过程，得到初始的EASG。例如，我们从添加摄像机佩戴者节点、动词节点以及从摄像机佩戴者节点到动词节点的默认动作边开始。标注流程如图3所示。

![](../Images/741dc3085e12cc53e6f1cffdc88c7b9d.png)

作者提供的图片

接下来，我们通过三名标注者的输入对图进行精炼。验证阶段会汇总三名标注者收到的数据，并确保最终标注的质量，如下所示。

![](../Images/85a017a1199a4556f35822bb4c5c2eaa.png)

图4（作者提供图片）：在验证阶段，针对标注者提出的问题示例（正确答案用红色标出），以解决标注中的歧义。

在标注阶段提供的标签。

如可以注意到的，EASG数据集在其标签上具有独特性。下表展示了该新数据集与其他具有视觉关系的视频数据集在标签和大小方面的对比。

![](../Images/81de1def11cca0f0c1ba817ee558221d.png)

作者提供的图片：与现有的视频场景图数据集进行比较。我们的Ego4D-EASG数据集是唯一明确设计用于长格式自我中心视频理解的数据集，具有自我中心视频、动态图、平均序列长度为3.1分钟，以及每个序列平均28.3个图。*以对象-关系-对象三元组为单位测量。**不及物+及物动词谓词

上述视频直观地展示了一个标注用的EASG示例，它会随着视频内容的变化而动态变化。

在创建了这个独特的数据集之后，我们将描述在该数据集上评估的不同任务。第一组任务是生成动作场景图，源自图像场景图生成文献。换句话说，我们的目标是以监督方式学习EASG表示，并测量其在场景图文献中常用的标准Recall指标上的表现。我们设计了基准，并比较了不同基准在该数据集上的EASG生成表现。

![](../Images/ea0522254893e4e567c963e15017e4a9.png)

（作者提供的图片）三项EASG生成任务（即Edge Cls，SG Cls和EASG Cls）在Recall@K上的基准结果

## **使用EASG进行长篇理解任务：**

我们展示了EASG表示在动作预测和活动摘要下游任务中的潜力。这两项任务都需要对以自我为中心的视频进行长篇推理，处理跨越不同时间步骤的长视频序列。根据最近的结果，展示了大型语言模型（LLMs）作为符号推理机器的灵活性，我们通过OpenAI API进行这些实验。实验旨在检查EASG表示的表达能力及其对下游应用的有用性。我们表明，EASG提供了一种有效的方式来建模长篇活动，相比于广泛采用的金标准动词-名词动作编码，EASG在自我中心视频社区中的表现更具优势。

**使用EASG进行动作预测：**

对于动作预测任务，我们使用GPT3的text-davinci-003模型。我们提示该模型从长度为T ∈ {5，20}的序列中预测未来的动作。我们比较了两种表示——EASG和动词-名词对序列。下表展示了此实验的结果。

![](../Images/4c45c4973e60d12c8a2fa4e038e9f73d.png)

作者提供的图片：动作预测任务的性能比较

即使是较短的EASG序列（T = 5）也往往超过较长的V-N序列（T = 20），这突显了EASG在与标准动词-名词表示相比时更强的表示能力。EASG表示在长序列（T = 20）中取得了最佳结果。

**使用EASG进行长篇活动摘要：**

我们选择了147个Ego4D-EASG片段的子集，这些片段包含人类注释的摘要，描述了片段中执行的活动，并用1到2个句子总结。我们构建了三种类型的输入序列：图序列S-EASG = [G(1)，G(2)，…，G(Tmax)]，动词-名词对序列svn = [s-vn(1)，s-vn(2)，…，s-vn(Tmax)]，以及与EASG序列匹配的原始Ego4D叙述序列。最后一种输入用于参考，因为我们预计来自叙述的摘要将带来最佳的表现，考虑到语言模型对这种表示的自然偏好。

以下表格中报告的结果表明，相较于动词-名词序列输入，CIDEr得分显著提高，表明处理EASG输入的模型能够捕捉详细的物体动作关系，从而生成更加具体、富有信息的句子，与参考描述高度契合。

![](../Images/0a1c6d4d80fc0fb45487d2a43b89d225.png)

作者提供的图片：使用EASG和动词-名词表示进行活动总结的结果

我们相信，这些贡献标志着在长篇自我中心视频理解方面迈出了重要的一步。

**亮点：**

+   我们提出了自我中心动作场景图（Egocentric Action Scene Graphs），这是一种用于长篇自我中心视频理解的新型表示方法；

+   我们通过一种新颖的注释程序，手动标注了EASG标签，并将其扩展到Ego4D数据集中；

+   我们提出了EASG生成基准，并提供了初步的基准结果；

+   我们展示了实验，突出了EASG表示在长篇自我中心视频理解中的有效性。我们将发布数据集和代码，以便复制数据注释过程；

    实验；

+   我们将在下个月的**CVPR 2024**上展示这项工作。

+   论文：[https://arxiv.org/abs/2312.03391](https://arxiv.org/abs/2312.03391) 和代码：[https://github.com/fpv-iplab/EASG](https://github.com/fpv-iplab/EASG)

# 任务的时间定位：

近年来，自我中心视频-语言预训练（VLP）在学术界和工业界得到了广泛应用。一些研究工作，如[EgoVLP](https://github.com/showlab/EgoVLP/blob/main/README.md)、[EgoVLPv2](https://shramanpramanick.github.io/EgoVLPv2/)，从大规模视频-文本数据集中学习可转移的时空表示。最近，[LaViLa](https://facebookresearch.github.io/LaViLa/)证明了VLP可以受益于大型语言模型（LLMs）生成的密集叙述。然而，所有这些方法在处理视频序列时都会遇到内存和计算瓶颈，每个序列包含少量帧（例如8帧或16帧的模型），导致有限的时间上下文聚合能力。相反，我们的模型，称为**LAVITI**，具备长篇推理能力（**1,000**帧对比16帧），并且不受限于少量输入帧。

在这项正在进行的工作中，我们设计了一种新颖的方法，通过对比学习学习语言、视频和***时间表示***，用于长篇视频的处理。与现有方法不同，这种新方法旨在通过提取未剪辑视频中的有意义时刻，将语言、视频和时间特征对齐，并将其表述为一个直接的集合预测问题。LAVITI在自我中心动作识别方面显著超过了现有的最先进方法，同时可以在内存和计算受限的系统上进行训练。我们的模型可以在Ego4D数据集上，仅用8个NVIDIA RTX-3090 GPU，训练一天。

![](../Images/cafb9f150d4a0d6617a9f5261cc8c744.png)

图片来自作者：在 CharadesEgo 上的表现。我们的方法在零-shot 和微调设置中均取得了显著的提升。ZS 和 FT 分别代表零-shot 和微调。

由于我们的模型能够进行具有显式时间对齐的长篇视频理解，Ego4D 自然语言查询（NLQ）任务与预训练目标非常契合。给定视频，我们可以直接预测与语言查询对齐的时间间隔；因此，LAVITI 可以

在零-shot 设置下执行 NLQ 任务（无需修改架构或重新训练 NLQ 注释）。

在不久的将来，我们计划评估其在学习改进的表征方面的潜力，尤其是在包括 NLQ 和 Moment Query (MQ) 在内的情节记忆任务中。总而言之，我们正在利用现有的基础模型（本质上是“短期”模型）来创建旨在进行 20 倍到 50 倍更大上下文聚合的“长篇”推理模块。

**亮点：**

我们设计了令人兴奋的新方法来进行自我中心的视频理解。我们的贡献是多方面的。

+   预训练目标通过提取**未修剪**视频中的有意义时刻，共同对齐语言、视频和*时间特征*；

+   将视频、语言和时间对齐问题表述为一个直接的集合预测问题；

+   以内存计算高效的方式，支持对视频中可能涉及的***成千上万帧***进行长篇推理；

+   通过其在 CharadesEgo 动作识别上的优越表现，展示了 LAVITI 的有效性；

+   支持零-shot 自然语言查询（NLQ）任务，而无需训练额外的子网络或 NLQ 注释。

关注这个新范式的“长篇”视频表示学习带来的更多激动人心的结果！
