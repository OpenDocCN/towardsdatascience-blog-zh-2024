<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Achieve Better Classification Results with ClassificationThresholdTuner</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Achieve Better Classification Results with ClassificationThresholdTuner</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/achieve-better-classification-results-with-classificationthresholdtuner-39c5d454637e?source=collection_archive---------0-----------------------#2024-09-07">https://towardsdatascience.com/achieve-better-classification-results-with-classificationthresholdtuner-39c5d454637e?source=collection_archive---------0-----------------------#2024-09-07</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="16a2" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A python tool to tune and visualize the threshold choices for binary and multi-class classification problems</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@wkennedy934?source=post_page---byline--39c5d454637e--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="W Brett Kennedy" class="l ep by dd de cx" src="../Images/b3ce55ffd028167326c117d47c64c467.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*v8pf2r3SPMLuHoSmF4IwlA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--39c5d454637e--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@wkennedy934?source=post_page---byline--39c5d454637e--------------------------------" rel="noopener follow">W Brett Kennedy</a></p></div></div></div><div class="hz ia l"><div class="ab ib"><div class="ab"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="none" viewbox="0 0 16 16"><path fill="#437AFF" d="M15.163 8c0 .65-.459 1.144-.863 1.575-.232.244-.471.5-.563.719s-.086.543-.092.875c-.006.606-.018 1.3-.49 1.781-.47.481-1.15.494-1.744.5-.324.006-.655.013-.857.094s-.465.337-.704.575c-.422.412-.906.881-1.542.881-.637 0-1.12-.469-1.543-.881-.239-.238-.49-.482-.704-.575-.214-.094-.532-.088-.857-.094-.593-.006-1.273-.019-1.744-.5s-.484-1.175-.49-1.781c-.006-.332-.012-.669-.092-.875-.08-.207-.33-.475-.563-.719-.404-.431-.863-.925-.863-1.575s.46-1.144.863-1.575c.233-.244.472-.5.563-.719.092-.219.086-.544.092-.875.006-.606.019-1.3.49-1.781s1.15-.494 1.744-.5c.325-.006.655-.012.857-.094.202-.081.465-.337.704-.575C7.188 1.47 7.671 1 8.308 1s1.12.469 1.542.881c.239.238.49.481.704.575s.533.088.857.094c.594.006 1.273.019 1.745.5.47.481.483 1.175.49 1.781.005.331.011.669.091.875s.33.475.563.719c.404.431.863.925.863 1.575"/><path fill="#fff" d="M7.328 10.5c.195 0 .381.08.519.22.137.141.215.331.216.53 0 .066.026.13.072.177a.24.24 0 0 0 .346 0 .25.25 0 0 0 .071-.177c.001-.199.079-.389.216-.53a.73.73 0 0 1 .519-.22h1.959c.13 0 .254-.053.346-.146a.5.5 0 0 0 .143-.354V6a.5.5 0 0 0-.143-.354.49.49 0 0 0-.346-.146h-1.47c-.324 0-.635.132-.865.366-.23.235-.359.552-.359.884v2.5c0 .066-.025.13-.071.177a.24.24 0 0 1-.346 0 .25.25 0 0 1-.072-.177v-2.5c0-.332-.13-.65-.359-.884A1.21 1.21 0 0 0 6.84 5.5h-1.47a.49.49 0 0 0-.346.146A.5.5 0 0 0 4.88 6v4c0 .133.051.26.143.354a.49.49 0 0 0 .347.146z"/></svg></div></div></div><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ie if ah ai aj ak al am an ao ap aq ar ig ih ii" disabled="">Follow</button></p></div></div></span></div></div><div class="l ij"><span class="bf b bg z dx"><div class="ab cn ik il im"><div class="in io ab"><div class="bf b bg z dx ab ip"><span class="iq l ij">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--39c5d454637e--------------------------------" rel="noopener follow"><p class="bf b bg z ir is it iu iv iw ix iy bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ic id" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">30 min read</span><div class="iz ja l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 7, 2024</span></div></span></div></span></div></div></div><div class="ab cp jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq"><div class="h k w ea eb q"><div class="kg l"><div class="ab q kh ki"><div class="pw-multi-vote-icon ed iq kj kk kl"><div class=""><div class="km kn ko kp kq kr ks am kt ku kv kl"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kw kx ky kz la lb lc"><p class="bf b dy z dx"><span class="kn">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao km lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count ld le">3</span></p></button></div></div></div><div class="ab q jr js jt ju jv jw jx jy jz ka kb kc kd ke kf"><div class="lj k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lk an ao ap ig ll lm ln" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lo cn"><div class="l ae"><div class="ab cb"><div class="lp lq lr ls lt lu ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lk an ao ap ig lv lw li lx ly lz ma mb s mc md me mf mg mh mi u mj mk ml"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="ffbf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Adjusting the thresholds used in classification problems (that is, adjusting the cut-offs in the probabilities used to decide between predicting one class or another) is a step that’s sometimes forgotten, but is quite easy to do and can significantly improve the quality of a model. It’s a step that should be performed with most classification problems (with some exceptions depending on what we wish to optimize for, described below).</p><p id="059e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this article, we look closer at what’s actually happening when we do this — with multi-class classification particularly, this can be a bit nuanced. And we look at an open source tool, written by myself, called <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner" rel="noopener ugc nofollow" target="_blank">ClassificationThesholdTuner</a>, that automates and describes the process to users.</p><p id="b97e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given how common the task of tuning the thresholds is with classification problems, and how similar the process usually is from one project to another, I’ve been able to use this tool on many projects. It eliminates a lot of (nearly duplicate) code I was adding for most classification problems and provides much more information about tuning the threshold that I would have otherwise.</p><p id="636f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Although ClassificationThesholdTuner is a useful tool, you may find the ideas behind the tool described in this article more relevant — they’re easy enough to replicate where useful for your classification projects.</p><p id="dfb3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In a nutshell, <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner" rel="noopener ugc nofollow" target="_blank">ClassificationThesholdTuner</a> is a tool to optimally set the thresholds used for classification problems and to present clearly the effects of different thresholds. Compared to most other available options (and the code we would most likely develop ourselves for optimizing the threshold), it has two major advantages:</p><ol class=""><li id="53fe" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh nj nk nl bk">It provides visualizations, which help data scientists understand the implications of using the optimal threshold that’s discovered, as well as alternative thresholds that may be selected. This can also be very valuable when presenting the modeling decisions to other stakeholders, for example where it’s necessary to find a good balance between false positives and false negatives. Frequently business understanding, as well as data modeling knowledge, is necessary for this, and having a clear and full understanding of the choices for threshold can facilitate discussing and deciding on the best balance.</li><li id="7015" class="mm mn fq mo b go nm mq mr gr nn mt mu mv no mx my mz np nb nc nd nq nf ng nh nj nk nl bk">It supports multi-class classification, which is a common type of problem in machine learning, but is more complicated with respect to tuning the thresholds than binary classification (for example, it requires identifying multiple thresholds). Optimizing the thresholds used for multi-class classification is, unfortunately, not well-supported by other tools of this type.</li></ol><p id="cef9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Although supporting multi-class classification is one of the important properties of <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner" rel="noopener ugc nofollow" target="_blank">ClassificationThesholdTuner</a>, binary classification is easier to understand, so we’ll begin by describing this.</p><h1 id="a4b9" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">What are the thresholds used in classification?</h1><p id="f46b" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">Almost all modern classifiers (including those in scikit-learn, CatBoost, LGBM, XGBoost, and most others) support producing both predictions and probabilities.</p><p id="f66a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, if we create a binary classifier to predict which clients will churn in the next year, then for each client we can generally produce either a binary prediction (a Yes or a No for each client), or can produce a probability for each client (e.g. one client may be estimated to have a probability of 0.862 of leaving in that time frame).</p><p id="7576" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given a classifier that can produce probabilities, even where we ask for binary predictions, behind the scenes it will generally actually produce a probability for each record. It will then convert the probabilities to class predictions.</p><p id="8e8e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">By default, binary classifiers will predict the positive class where the predicted probability of the positive class is greater than or equal to 0.5, and the negative class where the probability is under 0.5. In this example (predicting churn), it would, by default, predict Yes if the predicted probability of churn is ≥ 0.5 and No otherwise.</p><p id="8f05" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, this may not be the ideal behavior, and often a threshold other than 0.5 can work preferably, possibly a threshold somewhat lower or somewhat higher, and sometimes a threshold substantially different from 0.5. This can depend on the data, the classifier built, and the relative importance of false positives vs false negatives.</p><p id="54b0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In order to create a strong model (including balancing well the false positives and false negatives), we will often wish to optimize for some metric, such as F1 Score, F2 Score (or others in the family of f-beta metrics), Matthews Correlation Coefficient (MCC), Kappa Score, or another. If so, a major part of optimizing for these metrics is setting the threshold appropriately, which will most often set it to a value other than 0.5. We’ll describe soon how this works.</p><p id="9102" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is a key point. It’s not generally immediately clear where to best set the threshold, but we can usually determine the best metric to optimize for. An example is using the F2 or F3 scores where we wish to place more emphasis on the recall of the positive class.</p><h1 id="1dfb" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Support in Scikit-learn for threshold tuning</h1><p id="ca93" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">Scikit-learn provides good background on the idea of threshold tuning in its <a class="af ni" href="https://scikit-learn.org/stable/modules/classification_threshold.html" rel="noopener ugc nofollow" target="_blank">Tuning the decision threshold for class prediction</a> page. Scikit-learn also provides two tools: <a class="af ni" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.FixedThresholdClassifier.html" rel="noopener ugc nofollow" target="_blank">FixedThresholdClassifier </a>and <a class="af ni" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html#sklearn.model_selection.TunedThresholdClassifierCV" rel="noopener ugc nofollow" target="_blank">TunedThresholdClassifierCV</a> (introduced in version 1.5 of scikit-learn) to assist with tuning the threshold. They work quite similarly to ClassificationThesholdTuner.</p><p id="05c4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Scikit-learn’s tools can be considered convenience methods, as they’re not strictly necessary; as indicated, tuning is fairly straightforward in any case (at least for the binary classification case, which is what these tools support). But, having them is convenient — it is still quite a bit easier to call these than to code the process yourself.</p><p id="579e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ClassificationThresholdTuner was created as an alternative to these, but where scikit-learn’s tools work well, they are very good choices as well. Specifically, where you have a binary classification problem, and don’t require any explanations or descriptions of the threshold discovered, scikit-learn’s tools can work perfectly, and may even be slightly more convenient, as they allow us to skip the small step of installing ClassificationThresholdTuner.</p><p id="9d0d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ClassificationThresholdTuner may be more valuable where explanations of the thresholds found (including some context related to alternative values for the threshold) are necessary, or where you have a multi-class classification problem.</p><p id="e97e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As indicated, it also may at times be the case that the <em class="os">ideas </em>described in this article are what is most valuable, not the specific tools, and you may be best to develop your own code — perhaps along similar lines, but possibly optimized in terms of execution time to more efficiently handle the data you have, possibly more able support other metrics to optimize for, or possibly providing other plots and descriptions of the threshold-tuning process, to provide the information relevant for your projects.</p><h1 id="3417" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Thresholds in Binary Classification</h1><p id="8bc4" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">With most scikit-learn classifiers, as well as CatBoost, XGBoost, and LGBM, the probabilities for each record are returned by calling predict_proba(). The function outputs a probability for each class for each record. In a binary classification problem, they will output two probabilities for each record, for example:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="5b14" class="pc ns fq oz b bg pd pe l pf pg">[[0.6, 0.4], <br/> [0.3, 0.7], <br/> [0.1, 0.9],<br/>  …<br/>]</span></pre><p id="5c67" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For each pair of probabilities, we can take the first as the probability of the negative class and the second as the probability of the positive class.</p><p id="d122" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">However, with binary classification, one probability is simply 1.0 minus the other, so only the probabilities of one of the classes are strictly necessary. In fact, when working with class probabilities in binary classification problems, we often use only the probabilities of the positive class, so could work with an array such as: [0.4, 0.7, 0.9, …].</p><p id="66ba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Thresholds are easy to understand in the binary case, as they can be viewed simply as the minimum predicted probability needed for the positive class to actually predict the positive class (in the churn example, to predict customer churn). If we have a threshold of, say, 0.6, it’s then easy to convert the array of probabilities above to predictions, in this case, to: [No, Yes, Yes, ….].</p><p id="e282" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">By using different thresholds, we allow the model to be more, or less, eager to predict the positive class. If a relatively low threshold, say, 0.3 is used, then the model will predict the positive class even when there’s only a moderate chance this is correct. Compared to using 0.5 as the threshold, more predictions of the positive class will be made, increasing both true positives and false positives, and also reducing both true negatives and false negatives.</p><p id="9e35" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the case of churn, this can be useful if we want to focus on catching most cases of churn, even though doing so, we also predict that many clients will churn when they will not. That is, a low threshold is good where false negatives (missing churn) is more of a problem than false positives (erroneously predicting churn).</p><p id="d4bf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Setting the threshold higher, say to 0.8, will have the opposite effect: fewer clients will be predicted to churn, but of those that are predicted to churn, a large portion will quite likely actually churn. We will increase the false negatives (miss some who will actually churn), but decrease the false positives. This can be appropriate where we can only follow up with a small number of potentially-churning clients, and want to label only those that are most likely to churn.</p><p id="690e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There’s almost always a strong business component to the decision of where to set the threshold. Tools such as ClassificationThresholdTuner can make these decisions more clear, as there’s otherwise not usually an obvious point for the threshold. Picking a threshold, for example, simply based on intuition (possibly determining 0.7 feels about right) will not likely work optimally, and generally no better than simply using the default of 0.5.</p><p id="7fed" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Setting the threshold can be a bit unintuitive: adjusting it a bit up or down can often help or hurt the model more than would be expected. Often, for example, increasing the threshold can greatly decrease false positives, with only a small effect on false negatives; in other cases the opposite may be true. Using a Receiver Operator Curve (ROC) is a good way to help visualize these trade-offs. We’ll see some examples below.</p><p id="542f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Ultimately, we’ll set the threshold so as to optimize for some metric (such as F1 score). ClassificationThresholdTuner is simply a tool to automate and describe that process.</p><h1 id="07e0" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">AUROC and F1 Scores</h1><p id="e79b" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">In general, we can view the metrics used for classification as being of three main types:</p><ul class=""><li id="32bd" class="mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ph nk nl bk">Those that examine how well-ranked the prediction probabilities are, for example: Area Under Receiver Operator Curve (AUROC), Area Under Precision Recall Curve (AUPRC)</li><li id="caf2" class="mm mn fq mo b go nm mq mr gr nn mt mu mv no mx my mz np nb nc nd nq nf ng nh ph nk nl bk">Those that examine how well-calibrated the prediction probabilities are, for example: Brier Score, Log Loss</li><li id="fc15" class="mm mn fq mo b go nm mq mr gr nn mt mu mv no mx my mz np nb nc nd nq nf ng nh ph nk nl bk">Those that look at how correct the predicted labels are, for example: F1 Score, F2 Score, MCC, Kappa Score, Balanced Accuracy</li></ul><p id="ea75" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first two categories of metric listed here work based on predicted probabilities, and the last works with predicted labels.</p><p id="10b1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">While there are numerous metrics within each of these categories, for simplicity, we will consider for the moment just two of the more common, the Area Under Receiver Operator Curve (AUROC) and the F1 score.</p><p id="8896" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">These two metrics have an interesting relationship (as does AUROC with other metrics based on predicted labels), which ClassificationThresholdTuner takes advantage of to tune and to explain the optimal thresholds.</p><p id="66d3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The idea behind ClassificationThresholdTuner is to, once the model is well-tuned to have a strong AUROC, take advantage of this to optimize for other metrics — metrics that are based on predicted labels, such as the F1 score.</p><h1 id="c351" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Metrics Based on Predicted Labels</h1><p id="3274" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">Very often metrics that look at how correct the predicted labels are are the most relevant for classification. This is in cases where the model will be used to assign predicted labels to records and what’s relevant is the number of true positives, true negatives, false positives, and false negatives. That is, if it’s the predicted labels that are used downstream, then once the labels are assigned, it’s no longer relevant what the underlying predicted probabilities were, just these final label predictions.</p><p id="c50f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, if the model assigns labels of Yes and No to clients indicating if they’re expected to churn in the next year and the clients with a prediction of Yes receive some treatment and those with a prediction of No do not, what’s most relevant is how correct these labels are, not in the end, how well-ranked or well-calibrated the prediction probabilities (that these class predications are based on) were. Though, how well-ranked the predicted probabilities are is relevant, as we’ll see, to assign predicted labels accurately.</p><p id="b421" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This isn’t true for every project: often metrics such as AUROC or AUPRC that look at how well the predicted probabilities are ranked are the most relevant; and often metrics such as Brier Score and Log Loss that look at how accurate the predicted probabilities are most relevant.</p><p id="0b8f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Tuning the thresholds will not affect these metrics and, where these metrics are the most relevant, there is no reason to tune the thresholds. But, for this article, we’ll consider cases where the F1 score, or another metric based on the predicted labels, is what we wish to optimize.</p><p id="86fd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">ClassificationThresholdTuner starts with the predicted probabilities (the quality of which can be assessed with the AUROC) and then works to optimize the specified metric (where the specified metric is based on predicted labels).</p><p id="addc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Metrics based on the correctness of the predicted labels are all, in different ways, calculated from the confusion matrix. The confusion matrix, in turn, is based on the threshold selected, and can look quite quite different depending if a low or high threshold is used.</p><h1 id="546f" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Adjusting the Threshold</h1><p id="b6d6" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">The AUROC metric is, as the name implies, based on the ROC, a curve showing how the true positive rate relates to the false positive rate. An ROC curve doesn’t assume any specific threshold is used. But, each point on the curve corresponds to a specific threshold.</p><p id="1de0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the plot below, the blue curve is the ROC. The area under this curve (the AUROC) measures how strong the model is generally, averaged over all potential thresholds. It measures how well ranked the probabilities are: if the probabilities are well-ranked, records that are assigned higher predicted probabilities of being in the positive class are, in fact, more likely to be in the positive class.</p><p id="bf88" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">For example, an AUROC of 0.95 means a random positive sample has a 95% chance of being ranked higher than random negative sample.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pk"><img src="../Images/4f6b09a80e830f53671f89b2bb7cf940.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*H6PT51hOKE-4DXGy.png"/></div></figure><p id="c6a7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">First, having a model with a strong AUROC is important — this is the job of the model tuning process (which may actually optimize for other metrics). This is done before we begin tuning the threshold, and coming out of this, it’s important to have well-ranked probabilities, which implies a high AUROC score.</p><p id="8dc8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Then, where the project requires class predictions for all records, it’s necessary to select a threshold (though the default of 0.5 can be used, but likely with sub-optimal results), which is equivalent to picking a point on the ROC curve.</p><p id="6b4f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The figure above shows two points on the ROC. For each, a vertical and a horizonal line are drawn to the x &amp; y-axes to indicate the associated True Positive Rates and False Positive Rates.</p><p id="0a29" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Given an ROC curve, as we go left and down, we are using a higher threshold (for example from the green to the red line). Less records will be predicted positive, so there will be both less true positives and less false positives.</p><p id="6809" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As we move right and up (for example, from the red to the green line), we are using a lower threshold. More records will be predicted positive, so there will be both more true positives and more false positives.</p><p id="15a0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">That is, in the plot here, the red and green lines represent two possible thresholds. Moving from the green line to the red, we see a small drop in the true positive rate, but a larger drop in the false positive rate, making this quite likely a better choice of threshold than that where the green line is situated. But not necessarily — we also need to consider the relative cost of false positives and false negatives.</p><p id="f160" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">What’s important, though, is that moving from one threshold to another can often adjust the False Positive Rate much more or much less than the True Positive Rate.</p><p id="2da6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The following presents a set of thresholds with a given ROC curve. We can see where moving from one threshold to another can affect the true positive and false positive rates to significantly different extents.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pk"><img src="../Images/fdb58fb5016981a701c926b0da9ed734.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*Hd-VlmNAen5E2nTD.png"/></div></figure><p id="0047" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is the main idea behind adjusting the threshold: it’s often possible to achieve a large gain in one sense, while taking only a small loss in the other.</p><p id="b1d9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s possible to look at the ROC curve and see the effect of moving the thresholds up and down. Given that, it’s possible, to an extent, to eye-ball the process and pick a point that appears to best balance true positives and false positives (which also effectively balances false positives and false negatives). In some sense, this is what ClassificationThesholdTuner does, but it does so in a principled way, in order to optimize for a certain, specified metric (such as the F1 score).</p><p id="32ea" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Moving the threshold to different points on the ROC generates different confusion matrixes, which can then be converted to metrics (F1 Score, F2 score, MCC etc.). We can then take the point that optimizes this score.</p><p id="f764" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So long as a model is trained to have a strong AUROC, we can usually find a good threshold to achieve a high F1 score (or other such metric).</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pn"><img src="../Images/eabef9883aa792d33898de49b0ff719e.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*AyB_4YKvC92pb4rzctcl3w.png"/></div></figure><p id="0bd5" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this ROC plot, the model is very accurate, with an AUROC of 0.98. It will, then, be possible to select a threshold that results in a high F1 score, though it is still necessary to select a good threshold, and the optimal may easily not be 0.5.</p><p id="5de3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Being well-ranked, the model is not necessarily also well-calibrated, but this isn’t necessary: so long as records that are in the positive class tend to get higher predicted probabilities than those in the negative class, we can find a good threshold where we separate those predicted to be positive from those predicted to be negative.</p><p id="6569" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Looking at this another way, we can view the distribution of probabilities in a binary classification problem with two histograms, as shown here (actually using KDE plots). The blue curve shows the distribution of probabilities for the negative class and the orange for the positive class. The model is not likely well-calibrated: the probabilities for the positive class are consistently well below 1.0. But, they are well-ranked: the probabilities for the positive class tend to be higher than those for the negative class, which means the model would have a high AUROC and the model can assign labels well if using an appropriate threshold, in this case, likely about 0.25 or 0.3. Given that there is overlap in the distributions, though, it’s not possible to have a perfect system to label the records, and the F1 score can never be quite 1.0.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj pn"><img src="../Images/51b21f477acb1721e97fdd4c126e8e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*u2KeEmosHJJarKpbFLQ7Zw.png"/></div></div></figure><p id="e2ae" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It is possible to have, even with a high AUROC score, a low F1 score: where there is a poor choice of threshold. This can occur, for example, where the ROC hugs the axis as in the ROC shown above — a very low or very high threshold may work poorly. Hugging the y-axis can also occur where the data is imbalanced.</p><p id="bf7b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the case of the histograms shown here, though the model is well-calibrated and would have a high AUROC score, a poor choice of threshold (such as 0.5 or 0.6, which would result in everything being predicted as the negative class) would result in a very low F1 score.</p><p id="9f2c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s also possible (though less likely) to have a low AUROC and high F1 Score. This is possible with a particularly good choice of threshold (where most thresholds would perform poorly).</p><p id="9ba1" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As well, it’s not common, but possible to have ROC curves that are asymmetrical, which can greatly affect where it is best to place the threshold.</p><h1 id="1476" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Example using ClassificationThresholdTuner for Binary Classification</h1><p id="3337" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">This is taken from a <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner/blob/main/notebooks/binary_classification_threshold_demo.ipynb" rel="noopener ugc nofollow" target="_blank">notebook </a>available on the github site (where it’s possible to see the full code). We’ll go over the main points here. For this example, we first generate a test dataset.</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="6699" class="pc ns fq oz b bg pd pe l pf pg">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from threshold_tuner import ClassificationThresholdTuner<br/><br/>NUM_ROWS = 100_000<br/><br/>def generate_data():<br/>    num_rows_per_class = NUM_ROWS // 2<br/>    np.random.seed(0)<br/>    d = pd.DataFrame(<br/>      {"Y": ['A']*num_rows_per_class + ['B']*num_rows_per_class,<br/>       "Pred_Proba": <br/>          np.random.normal(0.7, 0.3, num_rows_per_class).tolist() + \<br/>          np.random.normal(1.4, 0.3, num_rows_per_class).tolist()<br/>      })<br/>    return d, ['A', 'B']<br/><br/>d, target_classes = generate_data()</span></pre><p id="b4fd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, for simplicity, we don’t generate the original data or the classifier that produced the predicted probabilities, just a test dataset containing the true labels and the predicted probabilities, as this is what ClassificationThresholdTuner works with and is all that is necessary to select the best threshold.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj ps"><img src="../Images/3277acbd53468e2b584a86c1f20a5360.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*tEsNmsbgXZsm0mWGYiZTqQ.png"/></div></figure><p id="85b0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There’s actually also code in the notebook to scale the probabilities, to ensure they are between 0.0 and 1.0, but for here, we’ll just assume the probabilities are well-scaled.</p><p id="9b5c" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can then set the Pred column using a threshold of 0.5:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="5c25" class="pc ns fq oz b bg pd pe l pf pg">d['Pred'] = np.where(d["Pred_Proba"] &gt; 0.50, "B", "A")</span></pre><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pt"><img src="../Images/ea3ac56911fab0f8dad15da7e4f2e786.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*v3jGnVOLOxkmIHY136gRlA.png"/></div></figure><p id="6dc7" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This simulates what’s normally done with classifiers, simply using 0.5 as the threshold. This is the baseline we will try to beat.</p><p id="1840" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then create a ClassificationThresholdTuner object and use this, to start, just to see how strong the current predictions are, calling one of it’s APIs, print_stats_lables().</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="96c2" class="pc ns fq oz b bg pd pe l pf pg">tuner = ClassificationThresholdTuner()<br/><br/>tuner.print_stats_labels(<br/>    y_true=d["Y"], <br/>    target_classes=target_classes,<br/>    y_pred=d["Pred"])</span></pre><p id="47ef" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This indicates the precision, recall, and F1 scores for both classes (was well as the macro scores for these) and presents the confusion matrix.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pu"><img src="../Images/b80a327ff35d0606256f110bb7070bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*KAPElyqytIRt5_QnM6SAog.png"/></div></figure><p id="c393" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This API assumes the labels have been predicted already; where only the probabilities are available, this method cannot be used, though we can always, as in this example, select a threshold and set the labels based on this.</p><p id="d11e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can also call the print_stats_proba() method, which also presents some metrics, in this case related to the predicted probabilities. It shows: the Brier Score, AUROC, and several plots. The plots require a threshold, though 0.5 is used if not specified, as in this example:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="8215" class="pc ns fq oz b bg pd pe l pf pg">tuner.print_stats_proba(<br/>    y_true=d["Y"], <br/>    target_classes=target_classes, <br/>    y_pred_proba=d["Pred_Proba"])</span></pre><p id="9e74" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This displays the effects of a threshold of 0.5. It shows the ROC curve, which itself does not require a threshold, but draws the threshold on the curve. It then presents how the data is split into two predicted classes based on the threshold, first as a histogram, and second as a swarm plot. Here there are two classes, with class A in green and class B (the positive class in this example) in blue.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj pv"><img src="../Images/bd0be7a84dad55c1c7c7b73c4e11435c.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*N0J2HehFwFMQXjwBq9R3nQ.png"/></div></figure><p id="21b4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the swarm plot, any misclassified records are shown in red. These are those where the true class is A but the predicted probability of B is above the threshold (so the model would predict B), and those where the true class is B but the predicted probability of B is below the threshold (so the model would predict A).</p><p id="8561" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can then examine the effects of different thresholds using plot_by_threshold():</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="d5bb" class="pc ns fq oz b bg pd pe l pf pg">tuner.plot_by_threshold(<br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d["Pred_Proba"])</span></pre><p id="2165" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we use the default set of potential thresholds: 0.1, 0.2, 0.3, … up to 0.9. For each threshold, it will predict any records with predicted probabilities over the threshold as the positive class and anything lower as the negative class. Misclassified records are shown in red.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj pw"><img src="../Images/1869104b0c54c931020dc23e4b476f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBP2nmADCGal652u144Uxg.png"/></div></div></figure><p id="0019" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To save space in this article, this image shows just three potential thresholds: 0.2, 0.3, and 0.4. For each we see: the position on the ROC curve this threshold represents, the split in the data it leads to, and the resulting confusion matrix (along with the F1 macro score associated with that confusion matrix).</p><p id="b330" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can see that setting the threshold to 0.2 results in almost everything being predicted as B (the positive class) — almost all records of class A are misclassified and so drawn in red. As the threshold is increased, more records are predicted to be A and less as B (though at 0.4 most records that are truly B are correctly predicted as B; it is not until a threshold of about 0.8 where almost all records that are truly class B are erroneously predicted as A: very few have predicted probability over 0.8).</p><p id="c775" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Examining this for nine possible values from 0.1 to 0.9 gives a good overview of the possible thresholds, but it may be more useful to call this function to display a narrower, and more realistic, range of possible values, for example:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="7377" class="pc ns fq oz b bg pd pe l pf pg">tuner.plot_by_threshold(<br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d["Pred_Proba"], <br/>    start=0.50, end=0.55, num_steps=6)</span></pre><p id="0849" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This will show each threshold from 0.50 to 0.55. Showing the first two of these:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj px"><img src="../Images/398801066ab07d8afc4c9e286fc044b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTvGGMC8sAgvKzbGQWZqMA.png"/></div></div></figure><p id="0be4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The API helps present the implications of different thresholds.</p><p id="1972" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can also view this calling describe_slices(), which describes the data between pairs of potential thresholds (i.e., within slices of the data) in order to see more clearly what the specific changes will be of moving the threshold from one potential location to the next (we see how many of each true class will be re-classified).</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="505c" class="pc ns fq oz b bg pd pe l pf pg">tuner.describe_slices(    <br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d["Pred_Proba"], <br/>    start=0.3, end=0.7, num_slices=5)</span></pre><p id="25cf" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This shows each slice visually and in table format:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj py"><img src="../Images/97914fa63002cc3fe5fb03ca6ec8d0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrKIQ0N4-y7e6zlkBN2uyw.png"/></div></div></figure><p id="5cba" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Here, the slices are fairly thin, so we see plots both showing them in context of the full range of probabilities (the left plot) and zoomed in (the right plot).</p><p id="384a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can see, for example, that moving the threshold from 0.38 to 0.46 we would re-classify the points in the 3rd slice, which has 17,529 true instances of class A and 1,464 true instances of class B. This is evident both in the rightmost swarm plot and in the table (in the swarm plot, there are far more green than blue points within slice 3).</p><p id="4aad" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This API can also be called for a narrower, and more realistic, range of potential thresholds:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="34f5" class="pc ns fq oz b bg pd pe l pf pg">tuner.describe_slices(    <br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d["Pred_Proba"], <br/>    start=0.4, end=0.6, num_slices=10)</span></pre><p id="d26e" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This produces:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj pz"><img src="../Images/da5ab6866d1b86849bc9353e632302ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l2--kO_tmk3V7uSzZndT5Q.png"/></div></div></figure><p id="84a6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Having called these (or another useful API, print_stats_table(), skipped here for brevity, but described on the github page and in the example notebooks), we can have some idea of the effects of moving the threshold.</p><p id="05c9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can then move to the main task, searching for the optimal threshold, using the tune_threshold() API. With some projects, this may actually be the only API called. Or it may be called first, with the above APIs being called later to provide context for the optimal threshold discovered.</p><p id="9f39" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, we optimize the F1 macro score, though any metric supported by scikit-learn and based on class labels is possible. Some metrics require additional parameters, which can be passed here as well. In this example, scikit-learn’s f1_score() requires the ‘average’ parameter, passed here as a parameter to tune_threshold().</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="2dfc" class="pc ns fq oz b bg pd pe l pf pg">from sklearn.metrics import f1_score<br/><br/>best_threshold = tuner.tune_threshold(<br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d["Pred_Proba"],<br/>    metric=f1_score,<br/>    average='macro',<br/>    higher_is_better=True,<br/>    max_iterations=5<br/>)<br/>best_threshold</span></pre><p id="b5a0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This, optionally, displays a set of plots demonstrating how the method over five iterations (in this example max_iterations is specified as 5) narrows in on the threshold value that optimizes the specified metric.</p><p id="4e5a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first iteration considers the full range of potential thresholds between 0.0 and 1.0. It then narrows in on the range 0.5 to 0.6, which is examined closer in the next iteration and so on. In the end a threshold of 0.51991 is selected.</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj qa"><img src="../Images/388aed8adf00e382e0b6b6e832540151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*nPJX4GirNN1I92-6oEMDMQ.png"/></div></figure><p id="265d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">After this, we can call print_stats_labels() again, which shows:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj qb"><img src="../Images/f97102f666acca6171907243366fb74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*EcOQqg9OR2eJ_Qm3c8PBow.png"/></div></figure><p id="d495" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can see, in this example, an increase in Macro F1 score from 0.875 to 0.881. In this case, the gain is small, but comes for almost free. In other cases, the gain may be smaller or larger, sometimes much larger. It’s also never counter-productive; at worst the optimal threshold found will be the default, 0.5000, in any case.</p><h1 id="015e" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Thresholds in Multi-class Classification</h1><p id="d5c0" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">As indicated, multi-class classification is a bit more complicated. In the binary classification case, a single threshold is selected, but with multi-class classification, ClassificationThesholdTuner identifies an optimal threshold per class.</p><p id="ac7a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Also different from the binary case, we need to specify one of the classes to be the default class. Going through an example should make it more clear why this is the case.</p><p id="f664" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In many cases, having a default class can be fairly natural. For example, if the target column represents various possible medical conditions, the default class may be “No Issue” and the other classes may each relate to specific conditions. For each of these conditions, we’d have a minimum predicted probability we’d require to actually predict that condition.</p><p id="5048" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Or, if the data represents network logs and the target column relates to various intrusion types, then the default may be “Normal Behavior”, with the other classes each relating to specific network attacks.</p><p id="2388" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the example of network attacks, we may have a dataset with four distinct target values, with the target column containing the classes: “Normal Behavior”, “Buffer Overflow”, “Port Scan”, and “Phishing”. For any record for which we run prediction, we will get a probability of each class, and these will sum to 1.0. We may get, for example: [0.3, 0.4, 0.1, 0.2] (the probabilities for each of the four classes, in the order above).</p><p id="7706" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Normally, we would predict “Buffer Overflow” as this has the highest probability, 0.4. However, we can set a threshold in order to modify this behavior, which will then affect the rate of false negatives and false positives for this class.</p><p id="e8a4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We may specify, for example that: the default class is ‘Normal Behavior”; the threshold for “Buffer Overflow” is 0.5; for “Port Scan” is 0.55; and for “Phishing” is 0.45. By convention, the threshold for the default class is set to 0.0, as it does not actually use a threshold. So, the set of thresholds here would be: 0.0, 0.5, 0.55, 0.45.</p><p id="4dd4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Then to make a prediction for any given record, we consider only the classes where the probability is over the relevant threshold. In this example (with predictions [0.3, 0.4, 0.1, 0.2]), none of the probabilities are over their thresholds, so the default class, “Normal Behavior” is predicted.</p><p id="94e9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the predicted probabilities were instead: [0.1, 0.6, 0.2, 0.1], then we would predict “Buffer Overflow”: the probability (0.6) is the highest prediction and is over its threshold (0.5).</p><p id="e5af" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the predicted probabilities were: [0.1, 0.2, 0.7, 0.0], then we would predict “Port Scan”: the probability (0.7) is over its threshold (0.55) and this is the highest prediction.</p><p id="a642" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This means: if one or more classes have predicted probabilities over their threshold, we take the one of these with the highest predicted probability. If none are over their threshold, we take the default class. And, if the default class has the highest predicted probability, it will be predicted.</p><p id="83c4" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">So, a default class is needed to cover the case where none of the predictions are over the the threshold for that class.</p><p id="b32a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">If the predictions are: [0.1, 0.3, 0.4, 0.2] and the thresholds are: 0.0, 0.55, 0.5, 0.45, another way to look at this is: the third class would normally be predicted: it has the highest predicted probability (0.4). But, if the threshold for that class is 0.5, then a prediction of 0.4 is not high enough, so we go to the next highest prediction, which is the second class, with a predicted probability of 0.3. That is below its threshold, so we go again to the next highest predicted probability, which is the forth class with a predicted probability of 0.2. It is also below the threshold for that target class. Here, we have all classes with predictions that are fairly high, but not sufficiently high, so the default class is used.</p><p id="8895" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This also highlights why it’s convenient to use 0.0 as the threshold for the default class — when examining the prediction for the default class, we do not need to consider if its prediction is under or over the threshold for that class; we can always make a prediction of the default class.</p><p id="d48f" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It’s actually, in principle, also possible to have more complex policies — not just using a single default class, but instead having multiple classes that can be selected under different conditions. But these are beyond the scope of this article, are often unnecessary, and are not supported by ClassificationThresholdTuner, at least at present. For the remainder of this article, we’ll assume there’s a single default class specified.</p><h1 id="0055" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Example using ClassificationThresholdTuner for Multi-class Classification</h1><p id="a11e" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">Again, we’ll start by creating the test data (using one of the test data sets provided in the <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner/blob/main/notebooks/multiclass_classification_threshold_demo.ipynb" rel="noopener ugc nofollow" target="_blank">example notebook</a> for multi-class classification on the github page), in this case, having three, instead of just two, target classes:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="7e42" class="pc ns fq oz b bg pd pe l pf pg">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from threshold_tuner import ClassificationThresholdTuner<br/><br/>NUM_ROWS = 10_000<br/><br/>def generate_data():<br/>    num_rows_for_default = int(NUM_ROWS * 0.9) <br/>    num_rows_per_class = (NUM_ROWS - num_rows_for_default) // 2<br/>    np.random.seed(0)<br/>    d = pd.DataFrame({<br/>      "Y": ['No Attack']*num_rows_for_default + ['Attack A']*num_rows_per_class + ['Attack B']*num_rows_per_class,<br/>      "Pred_Proba No Attack": <br/>          np.random.normal(0.7, 0.3, num_rows_for_default).tolist() + \<br/>          np.random.normal(0.5, 0.3, num_rows_per_class * 2).tolist(),<br/>      "Pred_Proba Attack A": <br/>          np.random.normal(0.1, 0.3, num_rows_for_default).tolist() + \<br/>          np.random.normal(0.9, 0.3, num_rows_per_class).tolist() + \<br/>          np.random.normal(0.1, 0.3, num_rows_per_class).tolist(),<br/>      "Pred_Proba Attack B": <br/>          np.random.normal(0.1, 0.3, num_rows_for_default).tolist() + \<br/>          np.random.normal(0.1, 0.3, num_rows_per_class).tolist() + \<br/>          np.random.normal(0.9, 0.3, num_rows_per_class).tolist()                    <br/>    })<br/>    d['Y'] = d['Y'].astype(str)<br/>    return d, ['No Attack', 'Attack A', 'Attack B']<br/><br/>d, target_classes = generate_data()</span></pre><p id="37f3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">There’s some code in the notebook to scale the scores and ensure they sum to 1.0, but for here, we can just assume this is done and that we have a set of well-formed probabilities for each class for each record.</p><p id="b996" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As is common with real-world data, one of the classes (the ‘No Attack’ class) is much more frequent than the others; the dataset in imbalanced.</p><p id="6905" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We then set the target predictions, for now just taking the class with the highest predicted probability:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="7867" class="pc ns fq oz b bg pd pe l pf pg">def set_class_prediction(d):    <br/>    max_cols = d[proba_cols].idxmax(axis=1)<br/>    max_cols = [x[len("Pred_Proba_"):] for x in max_cols]<br/>    return max_cols   <br/><br/>d['Pred'] = set_class_prediction(d)</span></pre><p id="6fee" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This produces:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj qc"><img src="../Images/08bf1dae2829cd7aa9d1b78704229122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*zNF-9BdRduwWgxANwDiSHQ.png"/></div></figure><p id="832d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Taking the class with the highest probability is the default behaviour, and in this example, the baseline we wish to beat.</p><p id="8ab6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can, as with the binary case, call print_stats_labels(), which works similarly, handling any number of classes:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="3b94" class="pc ns fq oz b bg pd pe l pf pg">tuner.print_stats_labels(<br/>    y_true=d["Y"], <br/>    target_classes=target_classes,<br/>    y_pred=d["Pred"])</span></pre><p id="a54d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This outputs:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj qd"><img src="../Images/89efe0526a77487ebcefdbc27f89ccc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*yTJWOCtoi6Z7Fm8Yk9nZIQ.png"/></div></figure><p id="f232" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Using these labels, we get an F1 macro score of only 0.447.</p><p id="9b7a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Calling print_stats_proba(), we also get the output related to the prediction probabilities:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj qe"><img src="../Images/82488ef664db4d967a261c772e6c5cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4G1JC0uF92z7pmi07hnPmA.png"/></div></div></figure><p id="8645" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This is a bit more involved than the binary case, since we have three probabilities to consider: the probabilities of each class. So, we first show how the data lines up relative to the probabilities of each class. In this case, there are three target classes, so three plots in the first row.</p><p id="0ba0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As would be hoped, when plotting the data based on the predicted probability of ‘No Attack’ (the left-most plot), the records for ‘No Attack’ are given a higher probabilities of this class than for other classes. Similar for ‘Attack A’ (the middle plot) and ‘Attack B’ (the right-most plot).</p><p id="2c9a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can also see that the classes are not perfectly separated, so there is no set of thresholds that can result in a perfect confusion matrix. We will need to chose a set of thresholds that best balances correct and incorrect predictions for each class.</p><p id="13a3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In the figure above, the bottom plot shows each point based on the probability of its true class. So for the the records where the true class is ‘No Attack’ (the green points), we plot these by their predicted probability of ‘No Attack’, for the records where the true class is ‘Attack A’, (in dark blue) we plot these by their predicted probability of ‘Attack A’, and similar for Attack B (in dark yellow). We see that the model has similar probabilities for Attack A and Attack B, and higher probabilities for these than for No Attack.</p><p id="7385" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The above plots did not consider any specific thresholds that may be used. We can also, optionally, generate more output, passing a set of thresholds (one per class, using 0.0 for the default class):</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="8b18" class="pc ns fq oz b bg pd pe l pf pg">tuner.print_stats_proba(<br/>    y_true=d["Y"], <br/>    target_classes=target_classes, <br/>    y_pred_proba=d[proba_cols].values,<br/>    default_class='No Attack',<br/>    thresholds=[0.0, 0.4, 0.4]<br/>)</span></pre><p id="7f8b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This may be most useful to plot the set of thresholds discovered as optimal by the tool, but can also be used to view other potential sets of thresholds.</p><p id="caa9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This produces a report for each class. To save space, we just show one here, for class Attack A (the full report is shown in the example notebook; viewing the reports for the other two classes as well is helpful to understand the full implications of using, in this example, [0.0, 0.4, 0.4] as the thresholds):</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj qf"><img src="../Images/b4fb08fda5476d5559a4847801913976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccPvHriTSpw0nh1043xC_g.png"/></div></div></figure><p id="455d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As we have a set of thresholds specified here, we can see the implications of using these thresholds, including how many of each class will be correctly and incorrectly classified.</p><p id="cd5a" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We see first where the threshold appears on the ROC curve. In this case, we are viewing the report for Class A so see a threshold of 0.4 (0.4 was specified for class A in the API call above).</p><p id="1ec6" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The AUROC score is also shown. This metric applies only to binary prediction, but in a multi-class problem we can calculate the AUROC score for each class by treating the problem as a series of one-vs-all problems. Here we can treat the problem as ‘Attack A’ vs not ‘Attack A’ (and similarly for the other reports).</p><p id="7f44" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The next plots show the distribution of each class with respect to the predicted probabilities of Attack A. As there are different counts of the different classes, these are shown two ways: one showing the actual distributions, and one showing them scaled to be more comparable. The former is more relevant, but the latter can allow all classes to be seen clearly where some classes are much more rare than others.</p><p id="2878" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can see that records where the true class is ‘Attack A’ (in dark blue) do have higher predicted probabilities of ‘Attack A’, but there is some decision to be made as to where the threshold is specifically placed. We see here the effect using 0.4 for this class. It appears that 0.4 is likely close to ideal, if not exactly.</p><p id="29ca" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We also see this in the form a swarm plot (the right-most plot), with the misclassified points in red. We can see that using a higher threshold (say 0.45 or 0.5), we would have more records where the true class is Attack A misclassified, but less records where the true class is ‘No Attack’ misclassified. And, using a lower threshold (say 0.3 or 0.35) would have the opposite effect.</p><p id="a0b3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can also call plot_by_threshold() to look at different potential thresholds:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="3eed" class="pc ns fq oz b bg pd pe l pf pg">tuner.plot_by_threshold(<br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d[proba_cols].values,<br/>    default_class='No Attack'<br/>)</span></pre><p id="6441" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This API is simply for explanation and not tuning, so for simplicity uses (for each potential threshold), the same threshold for each class (other than the default class). Showing this for the potential thresholds 0.2, 0.3, and 0.4:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div role="button" tabindex="0" class="po pp ed pq bh pr"><div class="pi pj qg"><img src="../Images/8d9a904beadcd0d787ce82839fa9206d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*egATGRZTIvqU17Tpc6WdjQ.png"/></div></div></figure><p id="b000" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The first row of figures shows the implication of using 0.2 for the threshold for all classes other than the default (that is not predicting Attack A unless the estimated probability of Attack A is at least 0.2; and not predicting Attack B unless the predicted probability of Attack B is at least 0.2 — though always otherwise taking the class with the highest predicted probability). Similarly in the next two rows for thresholds of 0.3 and 0.4.</p><p id="de86" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">We can see here the trade-offs to using lower or higher thresholds for each class, and the confusion matrixes that will result (along with the F1 score associated with these confusion matrixes).</p><p id="8655" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">In this example, moving from 0.2 to 0.3 to 0.4, we can see how the model will less often predict Attack A or Attack B (raising the thresholds, we will less and less often predict anything other than the default) and more often No Attack, which results in less misclassifications where the true class is No Attack, but more where the true class is Attack A or Attack B.</p><p id="50f9" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">When the threshold is quite low, such as 0.2, then of those records where the true class is the default, only those with the highest predicted probability of the class being No Attack (about the top half) were predicted correctly.</p><p id="b1f8" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Once the threshold is set above about 0.6, nearly everything is predicted as the default class, so all cases where the ground truth is the default class are correct and all others are incorrect.</p><p id="b22b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">As expected, setting the thresholds higher means predicting the default class more often and missing less of these, though missing more of the other classes. Attack A and B are generally predicted correctly when using low thresholds, but mostly incorrectly when using higher thresholds.</p><p id="49a0" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To tune the thresholds, we again use tune_threshold(), with code such as:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="8e63" class="pc ns fq oz b bg pd pe l pf pg">from sklearn.metrics import f1_score<br/><br/>best_thresholds = tuner.tune_threshold(<br/>    y_true=d['Y'], <br/>    target_classes=target_classes,<br/>    y_pred_proba=d[proba_cols].values,<br/>    metric=f1_score,<br/>    average='macro',<br/>    higher_is_better=True,<br/>    default_class='No Attack',<br/>    max_iterations=5<br/>)<br/>best_thresholds</span></pre><p id="f8bc" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This outputs: [0.0, 0.41257, 0.47142]. That is, it found a threshold of about 0.413 for Attack A, and 0.471 for Attack B works best to optimize for the specified metric, macro F1 score in this case.</p><p id="0497" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Calling print_stats_proba() again, we get:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="4a27" class="pc ns fq oz b bg pd pe l pf pg">tuner.print_stats_proba(<br/>    y_true=d["Y"], <br/>    target_classes=target_classes, <br/>    y_pred_proba=d[proba_cols].values,<br/>    default_class='No Attack',<br/>    thresholds=best_thresholds<br/>)</span></pre><p id="dc4d" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Which outputs:</p><figure class="ot ou ov ow ox pl pi pj paragraph-image"><div class="pi pj qh"><img src="../Images/055dbacf61aea08921cc545007ada920.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*DwmZSZbg4oH5vuJioXXjWA.png"/></div></figure><p id="55af" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">The macro F1 score, using the thresholds discovered here, has improved from about 0.44 to 0.68 (results will vary slightly from run to run).</p><h1 id="93e7" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Get_predictions()</h1><p id="ce57" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">One additional API is provided which can be very convenient, get_predictions(), to get label predictions given a set of predictions and thresholds. This can be called such as:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="9f92" class="pc ns fq oz b bg pd pe l pf pg">tuned_pred = tuner.get_predictions(<br/>    target_classes=target_classes,<br/>    d["Pred_Proba"], <br/>    None, <br/>    best_threshold)</span></pre><h1 id="5a95" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Tests with Real Datasets</h1><p id="e682" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">Testing has been performed with many real datasets as well. Often the thresholds discovered work no better than the defaults, but more often they work noticeably better. One <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner/blob/main/notebooks/Real_Datasets.ipynb" rel="noopener ugc nofollow" target="_blank">notebook </a>is included on the github page covering a small number (four) real datasets. This was provided more to provide real examples of using the tool and the plots it generates (as opposed to the synthetic data used to explain the tool), but also gives some examples where the tool does, in fact, improve the F1 macro scores.</p><p id="b177" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">To summarize these quickly, in terms of the thresholds discovered and the gain in F1 macro scores:</p><p id="7a48" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Breast cancer: discovered an optimal threshold of 0.5465, which improved the macro F1 score from 0.928 to 0.953.</p><p id="3790" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Steel plates fault: discovered an optimal threshold of 0.451, which improved the macro F1 score from 0.788 to 0.956.</p><p id="8583" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">Phenome discovered an optimal threshold of 0.444, which improved the macro F1 score from 0.75 to 0.78.</p><p id="a571" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With the digits dataset, no improvement over the default was found, though may be with different classifiers or otherwise different conditions.</p><h1 id="dd0d" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Installation</h1><p id="e5e5" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">This project uses a <a class="af ni" href="https://github.com/Brett-Kennedy/ClassificationThresholdTuner/blob/main/threshold_tuner.py" rel="noopener ugc nofollow" target="_blank">single .py file</a>.</p><p id="f1f3" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">This must be copied into your project and imported. For example:</p><pre class="ot ou ov ow ox oy oz pa bp pb bb bk"><span id="1d7b" class="pc ns fq oz b bg pd pe l pf pg">from threshold_tuner import ClassificationThesholdTuner<br/><br/>tuner = ClassificationThesholdTuner()</span></pre><h1 id="03e3" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Implications of Setting the Thresholds in multi-class problems</h1><p id="1fbc" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">There are some subtle points about setting thresholds in multi-class settings, which may or may not be relevant for any given project. This may get more into the weeds than is necessary for your work, and this articles is already quite long, but a section is provided on the main github page to cover cases where this is relevant. In particular, thresholds set above 0.5 can behave slightly differently than those below 0.5.</p><h1 id="2af0" class="nr ns fq bf nt nu nv gq nw nx ny gt nz oa ob oc od oe of og oh oi oj ok ol om bk">Conclusions</h1><p id="1ac5" class="pw-post-body-paragraph mm mn fq mo b go on mq mr gr oo mt mu mv op mx my mz oq nb nc nd or nf ng nh fj bk">While tuning the thresholds used for classification projects won’t always improve the quality of the model, it quite often will, and often significantly. This is easy enough to do, but using ClassificationThesholdTuner makes this a bit easier, and with multi-class classification, it can be particularly useful.</p><p id="253b" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">It also provides visualizations that explain the choices for threshold, which can be helpful, either in understanding and accepting the threshold(s) it discovers, or in selecting other thresholds to better match the goals of the project.</p><p id="05be" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">With multi-class classification, it can still take a bit of effort to understand well the effects of moving the thresholds, but this is much easier with tools such as this than without, and in many cases, simply tuning the thresholds and testing the results will be sufficient in any case.</p><p id="b7bd" class="pw-post-body-paragraph mm mn fq mo b go mp mq mr gr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh fj bk">All images are by the author</p></div></div></div></div>    
</body>
</html>