- en: 'Advanced Retrieval-Augmented Generation: From Theory to LlamaIndex Implementation'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先进的检索增强生成：从理论到 LlamaIndex 实现
- en: 原文：[https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19](https://towardsdatascience.com/advanced-retrieval-augmented-generation-from-theory-to-llamaindex-implementation-4de1464a9930?source=collection_archive---------0-----------------------#2024-02-19)
- en: How to address limitations of naive RAG pipelines by implementing targeted advanced
    RAG techniques in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过在 Python 中实现针对性的先进 RAG 技术，解决简单 RAG 流水线的局限性
- en: '[](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page---byline--4de1464a9930--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)
    ·10 min read·Feb 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--4de1464a9930--------------------------------)
    ·阅读时长 10 分钟·2024年2月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/de34a1aedcd7cbeadae031e9f75a3fc4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de34a1aedcd7cbeadae031e9f75a3fc4.png)'
- en: Difference between Naive and Advanced RAG (Image by the author, inspired by
    [1])
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 简单 RAG 与先进 RAG 的区别（图源：作者，灵感来源于 [1]）
- en: 'A recent survey on [Retrieval-Augmented Generation (RAG)](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)
    [1] summarized three recently evolved paradigms:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 一项关于[检索增强生成（RAG）](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)
    [1]的最新调查总结了三种近期演变的范式：
- en: Naive RAG,
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单 RAG，
- en: advanced RAG, and
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先进的 RAG，以及
- en: modular RAG.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模块化 RAG。
- en: The advanced RAG paradigm comprises of a set of techniques targeted at addressing
    known limitations of naive RAG. This article first discusses these techniques,
    which can be categorized into *pre-retrieval, retrieval, and post-retrieval optimizations*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 先进的 RAG 范式包括一系列技术，旨在解决简单 RAG 的已知局限性。本文首先讨论了这些技术，它们可以分为 *检索前、检索和检索后优化*。
- en: 'In the second half, you will learn how to implement a naive RAG pipeline using
    [Llamaindex](https://www.llamaindex.ai/) in Python, which will then be enhanced
    to an advanced RAG pipeline with a selection of the following advanced RAG techniques:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下半部分，您将学习如何使用 [Llamaindex](https://www.llamaindex.ai/) 在 Python 中实现一个简单的 RAG
    流水线，并通过选择以下先进的 RAG 技术将其增强为一个先进的 RAG 流水线：
- en: '[Pre-retrieval optimization: Sentence window retrieval](#c968)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索前优化：句子窗口检索](#c968)'
- en: '[Retrieval optimization: Hybrid search](#3275)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索优化：混合搜索](#3275)'
- en: '[Post-retrieval optimization: Re-ranking](#c1e2)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索后优化：重新排序](#c1e2)'
- en: 'This article focuses on the **advanced RAG paradigm** and its implementation.
    If you are unfamiliar with the fundamentals of RAG, you can catch up on it here:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点介绍**先进的 RAG 范式**及其实现。如果你不熟悉 RAG 的基本概念，可以在这里了解：
- en: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----4de1464a9930--------------------------------)
    [## Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----4de1464a9930--------------------------------)
    [## 检索增强生成（RAG）：从理论到 LangChain 实现'
- en: From the theory of the original academic paper to its Python implementation
    with OpenAI, Weaviate, and LangChain
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2?source=post_page-----4de1464a9930--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: What is Advanced RAG
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the recent advancements in the RAG domain, advanced RAG has evolved as
    a new paradigm with targeted enhancements to address some of the limitations of
    the naive RAG paradigm. As summarized in a recent survey [1], advanced RAG techniques
    can be categorized into pre-retrieval, retrieval, and post-retrieval optimizations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4448aad435386e145364d6e85076d755.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Difference between Naive and Advanced RAG (Image by the author, inspired by
    [1])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Pre-retrieval optimization
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Pre-retrieval optimizations focus on data indexing optimizations as well as
    query optimizations. Data indexing optimization techniques aim to store the data
    in a way that helps you improve retrieval efficiency, such as [1]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Sliding window** uses an overlap between chunks and is one of the simplest
    techniques.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhancing data granularity** applies data cleaning techniques, such as removing
    irrelevant information, confirming factual accuracy, updating outdated information,
    etc.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adding metadata**, such as dates, purposes, or chapters, for filtering purposes.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing index structures** involves different strategies to index data,
    such as adjusting the chunk sizes or using multi-indexing strategies. One technique
    we will implement in this article is sentence window retrieval, which embeds single
    sentences for retrieval and replaces them with a larger text window at inference
    time.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/42d3ba853f8b1794f5f0eadd32d9b55e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Sentence window retrieval
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, pre-retrieval techniques aren’t limited to data indexing and can
    cover **techniques at inference time**, such as query routing, query rewriting,
    and query expansion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval optimization
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The retrieval stage aims to identify the most relevant context. Usually, the
    retrieval is based on vector search, which calculates the semantic similarity
    between the query and the indexed data. Thus, the majority of retrieval optimization
    techniques revolve around the embedding models [1]:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning embedding models** customizes embedding models to domain-specific
    contexts, especially for domains with evolving or rare terms. For example, `BAAI/bge-small-en`
    is a high-performance embedding model that can be fine-tuned (see [Fine-tuning
    guide](https://betterprogramming.pub/fine-tuning-your-embedding-model-to-maximize-relevance-retrieval-in-rag-pipeline-2ea3fa231149))'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dynamic Embedding** adapts to the context in which words are used, unlike
    static embedding, which uses a single vector for each word. For example, OpenAI’s
    `embeddings-ada-02` is a sophisticated dynamic embedding model that captures contextual
    understanding. [1]'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also other retrieval techniques besides vector search, such as hybrid
    search, which often refers to the concept of combining vector search with keyword-based
    search. This retrieval technique is beneficial if your retrieval requires exact
    keyword matches.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----4de1464a9930--------------------------------)
    [## Improving Retrieval Performance in RAG Pipelines with Hybrid Search'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: How to find more relevant search results by combining traditional keyword-based
    search with modern vector search
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/improving-retrieval-performance-in-rag-pipelines-with-hybrid-search-c75203c2f2f5?source=post_page-----4de1464a9930--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Post-retrieval optimization
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Additional processing of the retrieved context can help address issues such
    as exceeding the context window limit or introducing noise, thus hindering the
    focus on crucial information. Post-retrieval optimization techniques summarized
    in the RAG survey [1] are:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt compression** reduces the overall prompt length by removing irrelevant
    and highlighting important context.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Re-ranking** uses machine learning models to recalculate the relevance scores
    of the retrieved contexts.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b53b6b4dead55ba22f8b77f052e09a30.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Re-ranking
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'For additional ideas on how to improve the performance of your RAG pipeline
    to make it production-ready, continue reading here:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=post_page-----4de1464a9930--------------------------------)
    [## A Guide on 12 Tuning Strategies for Production-Ready RAG Applications'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: How to improve the performance of your Retrieval-Augmented Generation (RAG)
    pipeline with these “hyperparameters” and…
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-guide-on-12-tuning-strategies-for-production-ready-rag-applications-7ca646833439?source=post_page-----4de1464a9930--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section discusses the required packages and API keys to follow along in
    this article.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Required Packages
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article will guide you through implementing a naive and an advanced RAG
    pipeline using [LlamaIndex](https://www.llamaindex.ai/) in Python.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In this article, we will be using [LlamaIndex](https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8)
    `[v0.10](https://blog.llamaindex.ai/llamaindex-v0-10-838e735948f8)`. If you are
    upgrading from an older LlamaIndex version, you need to run the following commands
    to install and run LlamaIndex properly:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: LlamaIndex offers an option to store vector embeddings locally in JSON files
    for persistent storage, which is great for quickly prototyping an idea. However,
    we will use a vector database for persistent storage since advanced RAG techniques
    aim for production-ready applications.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Since we will need metadata storage and hybrid search capabilities in addition
    to storing the vector embeddings, we will use the open source vector database
    [Weaviate](http://weaviate.io) (`v3.26.2`), which supports these features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: API Keys
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using Weaviate embedded, which you can use for free without registering
    for an API key. However, this tutorial uses an embedding model and LLM from [OpenAI](https://openai.com/),
    for which you will need an OpenAI API key. To obtain one, you need an OpenAI account
    and then “Create new secret key” under [API keys](https://platform.openai.com/account/api-keys).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create a local `.env` file in your root directory and define your API
    keys in it:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Afterwards, you can load your API keys with the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Implementing Naive RAG with LlamaIndex
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section discusses how to implement a naive RAG pipeline using LlamaIndex.
    You can find the entire naive RAG pipeline in this [Jupyter Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/naive_rag.ipynb).
    For the implementation using LangChain, you can continue in [this article (naive
    RAG pipeline using LangChain](https://medium.com/towards-data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Define the embedding model and LLM'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you can define an embedding model and LLM in a global settings object.
    Doing this means you don’t have to specify the models explicitly in the code again.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedding model: used to generate vector embeddings for the document chunks
    and the query.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LLM: used to generate an answer based on the user query and the relevant context.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Step 2: Load data'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, you will create a local directory named `data` in your root directory
    and download some example data from the [LlamaIndex GitHub repository](https://github.com/run-llama/llama_index)
    (MIT license).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Afterward, you can load the data for further processing:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Step 3: Chunk documents into nodes'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the entire document is too large to fit into the context window of the LLM,
    you will need to partition it into smaller text chunks, which are called `Nodes`
    in LlamaIndex. You can parse the loaded documents into nodes using the `SimpleNodeParser`
    with a defined chunk size of 1024.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Step 4: Build index'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, you will build the index that stores all the external knowledge in [Weaviate](https://weaviate.io/),
    an open source vector database.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: First, you will need to connect to a Weaviate instance. In this case, we’re
    using [Weaviate Embedded](https://weaviate.io/developers/weaviate/installation/embedded),
    which allows you to experiment in Notebooks for free without an API key. For a
    production-ready solution, deploying Weaviate yourself, e.g., [via Docker](https://weaviate.io/developers/weaviate/installation/docker-compose)
    or utilizing a [managed service](https://weaviate.io/developers/weaviate/installation/weaviate-cloud-services),
    is recommended.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Next, you will build a `VectorStoreIndex` from the Weaviate client to store
    your data in and interact with.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Step 5: Setup query engine'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lastly, you will set up the index as the query engine.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Step 6: Run a naive RAG query on your data'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, you can run a naive RAG query on your data, as shown below:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Implementing Advanced RAG with LlamaIndex
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover some simple adjustments you can make to turn
    the above naive RAG pipeline into an advanced one. This walkthrough will cover
    the following selection of advanced RAG techniques:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[Pre-retrieval optimization: Sentence window retrieval](#c968)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Retrieval optimization: Hybrid search](#3275)'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Post-retrieval optimization: Re-ranking](#c1e2)'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we will only cover the modifications here, you can find the [full end-to-end
    advanced RAG pipeline in this Jupyter Notebook](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/advanced_rag.ipynb).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Indexing optimization example: Sentence window retrieval'
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the [sentence window retrieval technique](https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo.html),
    you need to make two adjustments: First, you must adjust how you store and post-process
    your data. Instead of the `SimpleNodeParser`, we will use the `SentenceWindowNodeParser`.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `SentenceWindowNodeParser` does two things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: It separates the document into single sentences, which will be embedded.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each sentence, it creates a context window. If you specify a `window_size
    = 3`, the resulting window will be three sentences long, starting at the previous
    sentence of the embedded sentence and spanning the sentence after. The window
    will be stored as metadata.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: During retrieval, the sentence that most closely matches the query is returned.
    After retrieval, you need to replace the sentence with the entire window from
    the metadata by defining a `MetadataReplacementPostProcessor` and using it in
    the list of `node_postprocessors`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Retrieval optimization example: Hybrid search'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a hybrid search in LlamaIndex is as easy as two parameter changes
    to the `query_engine` if the underlying vector database supports hybrid search
    queries. The `alpha` parameter specifies the weighting between vector search and
    keyword-based search, where `alpha=0` means keyword-based search and `alpha=1`
    means pure vector search.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Post-retrieval optimization example: Re-ranking'
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding a reranker to your advanced RAG pipeline only takes three simple steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: First, define a reranker model. Here, we are using the `[BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)`[from
    Hugging Face](https://huggingface.co/BAAI/bge-reranker-base).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the query engine, add the reranker model to the list of `node_postprocessors`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the `similarity_top_k` in the query engine to retrieve more context
    passages, which can be reduced to `top_n` after reranking.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'There are many more different techniques within the advanced RAG paradigm.
    If you are interested in further implementations, I recommend the following two
    resources:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在高级RAG范式中还有许多不同的技术。如果您对进一步的实现感兴趣，推荐以下两个资源：
- en: '[](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----4de1464a9930--------------------------------)
    [## Building and Evaluating Advanced RAG Applications'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----4de1464a9930--------------------------------)
    [## 构建和评估高级RAG应用'
- en: Learn methods like sentence-window retrieval and auto-merging retrieval, improving
    your RAG pipeline's performance…
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习句子窗口检索和自动合并检索等方法，提高您的RAG管道性能…
- en: 'www.deeplearning.ai](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----4de1464a9930--------------------------------)
    [](/advanced-rag-01-small-to-big-retrieval-172181b396d4?source=post_page-----4de1464a9930--------------------------------)
    [## Advanced RAG 01: Small-to-Big Retrieval'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: www.deeplearning.ai](https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag/?source=post_page-----4de1464a9930--------------------------------)
    [](/advanced-rag-01-small-to-big-retrieval-172181b396d4?source=post_page-----4de1464a9930--------------------------------)
    [## 高级RAG 01：从小到大的检索
- en: Child-Parent RecursiveRetriever and Sentence Window Retrieval with LlamaIndex
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Child-Parent递归检索器与LlamaIndex中的句子窗口检索
- en: towardsdatascience.com](/advanced-rag-01-small-to-big-retrieval-172181b396d4?source=post_page-----4de1464a9930--------------------------------)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/advanced-rag-01-small-to-big-retrieval-172181b396d4?source=post_page-----4de1464a9930--------------------------------)
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This article covered the concept of advanced RAG, which covers a set of techniques
    to address the limitations of the naive RAG paradigm. After an overview of advanced
    RAG techniques, which can be categorized into pre-retrieval, retrieval, and post-retrieval
    techniques, this article implemented a naive and advanced RAG pipeline using LlamaIndex
    for orchestration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了高级RAG的概念，这是一套技术，旨在解决朴素RAG范式的局限性。在概述了可以分为检索前、检索和检索后技术的高级RAG技术后，本文使用LlamaIndex实施了一个朴素和高级RAG管道来进行编排。
- en: The RAG pipeline components were language models from [OpenAI](https://openai.com/),
    a reranker model from [BAAI](https://www.baai.ac.cn/english.html) hosted on [Hugging
    Face](https://huggingface.co/), and a [Weaviate](https://weaviate.io/) vector
    database.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: RAG管道的组件包括来自[OpenAI](https://openai.com/)的语言模型，来自[BAAI](https://www.baai.ac.cn/english.html)的重排序模型，该模型托管在[Hugging
    Face](https://huggingface.co/)上，以及[Weaviate](https://weaviate.io/)向量数据库。
- en: 'We implemented the following selection of techniques using LlamaIndex in Python:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Python中的LlamaIndex实现了以下技术：
- en: '[Pre-retrieval optimization: Sentence window retrieval](#c968)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索前优化：句子窗口检索](#c968)'
- en: '[Retrieval optimization: Hybrid search](#3275)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索优化：混合搜索](#3275)'
- en: '[Post-retrieval optimization: Re-ranking](#c1e2)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[检索后优化：重排序](#c1e2)'
- en: 'You can find the Jupyter Notebooks containing the full end-to-end pipelines
    here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这里找到包含完整端到端管道的Jupyter Notebook：
- en: '[Naive RAG in LlamaIndex](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/naive_rag.ipynb)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LlamaIndex中的朴素RAG](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/naive_rag.ipynb)'
- en: '[Advanced RAG in LlamaIndex](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/advanced_rag.ipynb)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LlamaIndex中的高级RAG](https://github.com/weaviate/recipes/blob/main/integrations/llamaindex/retrieval-augmented-generation/advanced_rag.ipynb)'
- en: Enjoyed This Story?
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这篇文章吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----4de1464a9930--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----4de1464a9930--------------------------------)
    [## 每当Leonie Monigatti发布新内容时，接收电子邮件通知。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don't already…
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当Leonie Monigatti发布新内容时，您都可以收到电子邮件通知。通过注册，如果您还没有Medium账户，将会创建一个账户…
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----4de1464a9930--------------------------------)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----4de1464a9930--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)，[*Twitter*](https://twitter.com/helloiamleonie)*，和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: Disclaimer
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 免责声明
- en: I am a Developer Advocate at Weaviate at the time of this writing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，我是Weaviate的开发者倡导者。
- en: References
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Literature
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., … & Wang, H. (2023).
    Retrieval-augmented generation for large language models: A survey. [*arXiv preprint
    arXiv:2312.10997*](https://arxiv.org/pdf/2312.10997.pdf).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 高, Y., 熊, Y., 高, X., 贾, K., 潘, J., 毕, Y., … & 王, H. (2023). 面向大语言模型的检索增强生成：一项调查。[*arXiv预印本
    arXiv:2312.10997*](https://arxiv.org/pdf/2312.10997.pdf)。'
- en: Images
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图片
- en: If not otherwise stated, all images are created by the author.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者创建。
