<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>NLP: Text Summarization and Keyword Extraction on Property Rental Listings — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>NLP: Text Summarization and Keyword Extraction on Property Rental Listings — Part 1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-text-summarization-and-keyword-extraction-on-property-rental-listings-part-1-f1b760cc7bbb?source=collection_archive---------1-----------------------#2024-07-08">https://towardsdatascience.com/nlp-text-summarization-and-keyword-extraction-on-property-rental-listings-part-1-f1b760cc7bbb?source=collection_archive---------1-----------------------#2024-07-08</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="5610" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A practical implementation of NLP techniques such as text summarization, NER, topic modeling, and text classification on rental listing data</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@kristiyanto_?source=post_page---byline--f1b760cc7bbb--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Kristiyanto" class="l ep by dd de cx" src="../Images/b9e0fea509c17ef9507f5a9998f7ba5b.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u_kBvOryhJYuXUeWIo7TWQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--f1b760cc7bbb--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@kristiyanto_?source=post_page---byline--f1b760cc7bbb--------------------------------" rel="noopener follow">Daniel Kristiyanto</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--f1b760cc7bbb--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 8, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="2898" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Introduction</h2><p id="d68b" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Natural Language Processing (NLP) can significantly enhance the analysis and usability of rental listing descriptions. In this exercise, we’ll explore the practical application of NLP techniques such as text summarization, Named Entity Recognition (NER), and topic modeling to extract insights and enrich the listing description on Airbnb listing data in Tokyo. Using publicly available data and tools like spaCy and SciKit-Learn, you can follow along, reproduce the results, or apply these techniques to your own text data with minimal adjustments. The codebase is available on <a class="af oa" href="https://github.com/kristiyanto/nlp_on_airbnb_dataset" rel="noopener ugc nofollow" target="_blank">GitHub</a> for you to fork and experiment with.</p></div></div><div class="ob bh"><figure class="oc od oe of og ob bh paragraph-image"><img src="../Images/264198623a04de117747b2f69f02975b.png" data-original-src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*r7dKt6cjgS873u2wDKa2Bg.png"/><figcaption class="oi oj ok ol om on oo bf b bg z dx">This article demonstrates the use of various NLP techniques to extract information from property listing description (left) data written by property owner, into more informative description (right). All images in this article are produced by the author. Codes and Jupyter notebook is available on <a class="af oa" href="https://github.com/kristiyanto/nlp_on_airbnb_dataset" rel="noopener ugc nofollow" target="_blank">GitHub</a>, and the data is available under creative common attribution from <a class="af oa" href="https://insideairbnb.com/get-the-data/" rel="noopener ugc nofollow" target="_blank"><em class="op">insideairbnb.com</em></a>.</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4346" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Part 1 (this article) covers the basics: </strong>the goal, the data and its preparation, and the methods used to extract keywords and text summaries using various techniques such as named entity recognition (NER), TF-IDF / sentence scoring, and Google’s T5 (Text-to-Text Transformer). We’ll also touch on leveraging these insights to improve user experience — serving suggestions included.</p><p id="eb48" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Part 2 (coming soon)</strong> <strong class="nj fr">covers topic modeling and text prediction</strong>: Part 2 will demonstrate how to perform topic modeling on unlabeled data. This upcoming article will discuss techniques like clustering to uncover hidden themes and building a predictive model to classify property rentals based on their categories and themes.</p><h1 id="b5f5" class="ov mk fq bf ml ow ox gq mp oy oz gt mt pa pb pc pd pe pf pg ph pi pj pk pl pm bk">Goal</h1><p id="e6cb" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The task is straightforward:</p><p id="00b0" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Given an example input:</strong> The rental description</p><p id="a115" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Generate output:</strong></p><ul class=""><li id="0d45" class="nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz pn po pp bk"><strong class="nj fr">Keywords:</strong> “<em class="pq">commercial street</em>”, “<em class="pq">stores</em>”, or “<em class="pq">near station</em>”<br/>Keywords help visualize data, uncover themes, identify similarities, and improve search functionality on the front end. Suggestions to serve these keywords are included at the bottom of this article.</li><li id="5825" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Summary:</strong> A sentence or two, roughly about 80 characters.<br/>Summaries provide concise information, enhancing the user experience by quickly conveying the most essential aspects of a listing.</li><li id="0bd0" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Theme/Topic:</strong> “<em class="pq">Excellent Access</em>”, “<em class="pq">Family Friendly</em>.”<br/>Categorizing listings that share the same theme can serve as a recommender system, aiding users in finding properties that match their preferences. Unlike individual keywords, these themes can cover a group of multiple keywords (kitchen, desk, queen bed, long-term =&gt; “Digital-Nomad Friendly”). We will deep-dive this in Part 2 (upcoming article).</li></ul><p id="0a96" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Chapters:</strong></p><ol class=""><li id="a733" class="nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz pw po pp bk"><strong class="nj fr">Data and Preparation</strong><br/>Getting the data, cleaning, custom lemma</li><li id="f8c5" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pw po pp bk"><strong class="nj fr">Text Summarization</strong><br/>TFIDF/sentence scoring, Deep-Learning, LLM (T5), evaluation</li><li id="f810" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pw po pp bk"><strong class="nj fr">Keyword Extraction using NER</strong><br/>Regex, Matcher, Deep-Learning</li><li id="fe66" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pw po pp bk"><strong class="nj fr">Serving Suggestion</strong></li></ol></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="0328" class="ov mk fq bf ml ow qf gq mp oy qg gt mt pa qh pc pd pe qi pg ph pi qj pk pl pm bk">1. Data and Preparation</h1><p id="0106" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Our dataset consists of rental listing descriptions sourced from <a class="af oa" href="https://insideairbnb.com/get-the-data/" rel="noopener ugc nofollow" target="_blank"><em class="pq">insideairbnb.com</em></a><em class="pq">, </em>licensed under a <a class="af oa" href="http://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">Creative Commons Attribution 4.0 International License</a>. We focus on the text written by property owners. The data contains nearly 15,000 rental descriptions, mainly in English. Records written in Japanese (surprisingly, only a handful of them!) were removed as part of the data cleaning task, which also involved removing duplicates and HTML artifacts left by the scraper. Due to a lot of data deduplication, which could be the byproduct of the web scraper, or possibly even more complex issues (such as owners posting multiple identical listings), data cleaning removed about half of the original size.</p><h2 id="6ad3" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1a. spaCy Pipeline</h2><p id="e223" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Once the data is clean, we can start building the spaCy pipeline. We can begin with a blank slate or use a pre-trained model like en_core_web_sm to process documents in English. This model includes a robust pipeline with:</p><ul class=""><li id="aaec" class="nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz pn po pp bk"><strong class="nj fr">Tokenization:</strong> Splitting text into words, punctuation marks, etc.</li><li id="e322" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Part-of-Speech Tagging:</strong> Tagging words as nouns, verbs, etc.</li><li id="0506" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Dependency Parsing:</strong> Identifying relationships between words.</li><li id="e043" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Sentencizer</strong>: Breaking down the documents into sentences.</li><li id="fadd" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Lemmatization:</strong> Reducing words to their base forms (e.g., seeing, see, saw, seen).</li><li id="031c" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Attribute Ruler: </strong>Adding, removing, or changing token attributes.</li><li id="e42c" class="nh ni fq nj b go pr nl nm gr ps no np mu pt nr ns my pu nu nv nc pv nx ny nz pn po pp bk"><strong class="nj fr">Named Entity Recognition:</strong> Identifying categories of named entities (persons, locations, etc.).</li></ul><h2 id="f9ff" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">1b. Custom Lemmatization</h2><p id="fd78" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Even with a battle-tested pipeline like en_core_web_sm, adjustments are often needed to cover specific use cases. For example, abbreviations commonly used in the rental industry (e.g., br for bedroom, apt for apartment, st for street) could be introduced into the pipeline through custom lemmatization. To evaluate this, we can count the number of <code class="cx qk ql qm qn b">token.lemma_</code> between pipeline with and without the custom lemma. If needed, other more robust pre-made pipeline, such as en_core_web_md (medium), or en_core_web_lg (large), are also available.</p><figure class="oc od oe of og ob"><div class="qo io l ed"><div class="qp qq l"/></div></figure><p id="b4da" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">In production-level projects, a more thorough list is needed and more rigorous data cleaning might be required. For example, emojis and emoji-like symbols are frequently included in culturally influenced writing, like by Japanese users. These symbols can introduce noise and require specific handling, such as removal or transformation. Other data pre-processing, such as a more robust proper sentence boundary detector may also be necessary to address sentences with missing spaces, such as <em class="pq">“This is a sentence.This is too. And also this.and this. But, no, this Next.js is a valid term and not two sentences!”</em></p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="369f" class="ov mk fq bf ml ow qf gq mp oy qg gt mt pa qh pc pd pe qi pg ph pi qj pk pl pm bk">2. Text Summarization</h1><p id="dbdc" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Navigating rental options in Tokyo can be overwhelming. Each listing promises to be the ideal home. Still, the data suggests that the property descriptions often fall short — they can be overly long, frustratingly brief, or muddled with irrelevant details; this is why text summarization can come in handy.</p></div></div><div class="ob"><div class="ab cb"><div class="lm qr ln qs lo qt cf qu cg qv ci bh"><figure class="oc od oe of og ob qx qy paragraph-image"><div role="button" tabindex="0" class="qz ra ed rb bh rc"><div class="ol om qw"><img src="../Images/a24a755fb9b1d8b693867f92b9a33236.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ecW4gtDkUfEdt8OdW3R6UQ.png"/></div></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">Sentence scoring to select the most informative sentences as the summary (right) from the description (left).</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="0185" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2a. Level: Easy — TF-IDF</h2><p id="f361" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">One typical approach to text summarization involves leveraging a technique called TF-IDF (Term Frequency-Inverse Document Frequency). TF-IDF considers both how frequently a word appears in a specific document (the rental listing) and how uncommon it is across the entire dataset of listings or corpus. This technique is also helpful for various text analysis tasks, such as indexing, similarity detection, and clustering (which we will explore in Part 2).</p><figure class="oc od oe of og ob"><div class="qo io l ed"><div class="qp qq l"/></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">Calculating the sentence ranking based on the relevance of detected keywords.</figcaption></figure><p id="f3c5" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Another variation of the technique is sentence scoring based on word co-occurrence. Like TF-IDF, this method calculates scores by comparing word occurrences within the document. This approach is fast and easy and requires no additional tools or even awareness of other documents. You can even do this on the fly at the front end using typescript, although it is not recommended.</p><p id="2757" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">However, <em class="pq">extractive summarization</em> techniques like these have a pitfall: they only find the best sentence in the document, which means that typos or other issues in the chosen sentence will appear in the summary. These typos also affect the scoring, making this model less forgiving of mistakes and important information not included in the selected sentence (or sentences) might be missed.</p><h2 id="698f" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2b. Level: Intermediate — Deep Learning</h2><p id="9504" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Beyond frequency-based methods, we can leverage the power of deep learning for text summarization. Sequence-to-sequence (Seq2Seq) models are a neural network architecture designed to translate sequences from one form to another. In text summarization tasks, these models act like complex translators.</p><p id="891f" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">A Seq2Seq model typically consists of two parts: an encoder and a decoder. The encoder processes the entire input text, capturing its meaning and structure. This information is then compressed into a hidden representation. Then, the decoder takes this hidden representation from the encoder to generate a new sequence — the text summary. During training, the decoder learns to translate the encoded representation that captures the key points of the original text. Unlike extractive methods, these models perform <em class="pq">abstractive summarization:</em> generating summaries in their own words rather than extracting sentences directly from the text.</p><h2 id="5296" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2c. Level: Advanced — Pre-Trained Language Models</h2><figure class="oc od oe of og ob"><div class="qo io l ed"><div class="qp qq l"/></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">For abstractive summarization, consider using T5 (Text-To-Text Transfer Transformer) models. While the <code class="cx qk ql qm qn b">t5-small </code>offers a good starting point, you might achieve superior results with a larger model like <code class="cx qk ql qm qn b">t5-base</code> or <code class="cx qk ql qm qn b">t5-large</code>. Be aware that larger models may require more computational resources and take longer to run.</figcaption></figure></div></div><div class="ob"><div class="ab cb"><div class="lm qr ln qs lo qt cf qu cg qv ci bh"><figure class="oc od oe of og ob qx qy paragraph-image"><div role="button" tabindex="0" class="qz ra ed rb bh rc"><div class="ol om rd"><img src="../Images/5fa0b7994b8c847d53c02bf4cd7aba2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*WK0llDeWWhRPYLcV2tzi3Q.png"/></div></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">LLMs can summarize documents in a creative way (going beyond just copying sentences), but getting the best results might involve additional steps before, during, and after the summarization process, including proper prompt engineering.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="94c3" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Pre-trained language models like T5 (Text-To-Text Transfer Transformer) or BERT (Bidirectional Encoder Representations from Transformers) can significantly enhance summarization for those with the resources and setup capabilities. However, while these models can be effective for large text, they might be overkill for this specific use case. Not only does it require more setup to function optimally, but it also includes the need for prompt engineering (pre-processing), retraining or fine-tuning, and post-processing (such as grammar, text capitalization, or even fact-checking and sanity check) to guide the model toward the desired output.</p><h2 id="7aa7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">2.d Evaluating Text Summarization</h2></div></div><div class="ob"><div class="ab cb"><div class="lm qr ln qs lo qt cf qu cg qv ci bh"><figure class="oc od oe of og ob qx qy paragraph-image"><div role="button" tabindex="0" class="qz ra ed rb bh rc"><div class="ol om re"><img src="../Images/696b776edf6bd88cb54cc7a5d59a6675.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*d2qnphip3tFGzAk7ZH1hGQ.png"/></div></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">Extractive (left) versus Abstractive (right) text summarization. Given that the quality of summaries is subjective, the winner is not always definitive. Comparison gets even more complex by factoring the efforts, cost and computing power needed.</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="27ab" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">As seen from the picture above, when comparing “simple” model using TFIDF versus complex model using LLM, the winner isn’t always clear. Evaluating the quality of a text summarization system is a complicated challenge. Unlike tasks with a single, definitive answer, there’s no single perfect summary for a given text. Humans can prioritize different aspects of the original content, which further makes it hard to design automatic metrics that perfectly align with human judgment.</p><p id="8349" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Evaluation metrics like ROUGE (Recall-Oriented Understudy for Gisting Evaluation) attempt to do just this. By comparing the overlaps between n-grams (sequences of words) between generated summaries and human-written summaries, ROUGE systematically scores the quality of the summaries. This method relies on a collection of human-written summaries as a baseline for evaluation, which often not available.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="c039" class="ov mk fq bf ml ow qf gq mp oy qg gt mt pa qh pc pd pe qi pg ph pi qj pk pl pm bk">3. Keyword Extraction Using Named Entity Recognition (NER)</h1><p id="f91a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">While summaries are helpful, keywords have different purposes. Keywords capture the most essential aspects that potential renters might be looking for. To extract keywords, we can use NLP techniques such as Named Entity Recognition (NER). This process goes beyond just identifying frequent words. We can extract critical information by considering factors like word co-occurrence and relevance to the domain of rental listings. This information can be a single word, such as ‘luxurious’ (adjective), ‘Ginza’ (location), or a phrase like ‘quiet environment’ (noun phrases) or ‘near to Shinjuku’ (proximity).</p><figure class="oc od oe of og ob ol om paragraph-image"><div role="button" tabindex="0" class="qz ra ed rb bh rc"><div class="ol om rf"><img src="../Images/e5d3925865319e0c06d69ec78b94d253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtZeaZuySNVCW12p-p-Ong.png"/></div></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">Evaluating NER: SpaCy’s built-in NER performs well, but certain entity types might require additional training data for optimal accuracy. (NER stands for Named Entity Recognition, GPE: Geo Political Entity)</figcaption></figure><h2 id="23b7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3a. Level: Easy — Regex</h2><p id="7f9f" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">The ‘find’ function in string operations, along with regular expressions, can do the job of finding keywords. However, this approach requires an exhaustive list of words and patterns, which is sometimes not practical. If an exhaustive list of keywords to look for is available (like stock exchange abbreviations for finance-related projects), regex might be the simplest way to do it.</p><h2 id="c7b4" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3b. Level: Intermediate — The Matcher</h2><p id="d3f3" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">While regular expressions can be used for simple keyword extraction, the need for extensive lists of rules makes it hard to cover all bases. Fortunately, most NLP tools have this NER capability that is out of the box. For example, Natural Language Toolkit (NLTK) has Named Entity Chunkers, and spaCy has Matcher.</p><p id="7b3a" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Matcher allows you to define patterns based on linguistic features like part-of-speech tags or specific keywords. These patterns can be matched against the rental descriptions to identify relevant keywords and phrases. This approach captures single words (like, Tokyo) and meaningful phrases (like, beautiful house) that better represent the selling points of a property.</p><pre class="oc od oe of og rg qn rh bp ri bb bk"><span id="cacb" class="rj mk fq qn b bg rk rl l rm rn">noun_phrases_patterns = [<br/>      [{'POS': 'NUM'}, {'POS': 'NOUN'}], #example: 2 bedrooms<br/>      [{'POS': 'ADJ', 'OP': '*'}, {'POS': 'NOUN'}], #example: beautiful house<br/>      [{'POS': 'NOUN', 'OP': '+'}], #example: house<br/>  ]<br/><br/># Geo-political entity<br/>gpe_patterns = [<br/>      [{'ENT_TYPE': 'GPE'}], #example: Tokyo<br/>  ]<br/><br/># Proximity<br/>proximity_patterns = [<br/># example: near airport<br/>[{'POS': 'ADJ'}, {'POS': 'ADP'}, {'POS': 'NOUN', 'ENT_TYPE': 'FAC', 'OP': '?'}], <br/># example: near to Narita<br/>[{'POS': 'ADJ'}, {'POS': 'ADP'}, {'POS': 'PROPN', 'ENT_TYPE': 'FAC', 'OP': '?'}] <br/>]</span></pre><h2 id="f0d1" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">3c. Level: Advanced — Deep Learning-Based Matcher</h2><p id="d5ff" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Even with Matcher, some terms may not be captured by rule-based matching due to the context of the words in the sentence. For example, the Matcher might miss a term like ‘a stone’s throw away from Ueno Park’ since it won’t pass any predefined patterns, or mistake “Shinjuku Kabukicho” as a person (it’s a neighborhood, or LOC).</p><p id="7c09" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">In such cases, deep-learning-based approaches can be more effective. By training on a large corpus of rental listing with associated keywords these model learn the semantic relationships between words. This makes this method more adaptable to evolving language use and can uncover hidden insights.</p><p id="c210" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Using spaCy, performing deep-learning-based NER is straightforward. However, the major building block for this method is usually the availability of the labeled training data, as also the case for this exercise. The label is a pair of the target terms and the entity name (example: ‘a stone throw away’ is a noun phrase — or as shown in picture: Shinjuku Kabukicho is a LOC, not a person), formatted in a certain way. Unlike rule-based where we describe the terms into noun, location, and others from the built-in functionality, data exploration or domain expert are needed to discover the target terms that we want to identify.</p><p id="9a60" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">Part 2 of the article will discuss this technique of discovering themes or labels from the data for topic modeling using clustering, bootstrapping, and other methods.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ddbe" class="ov mk fq bf ml ow qf gq mp oy qg gt mt pa qh pc pd pe qi pg ph pi qj pk pl pm bk">4. Serving Suggestions</h1><p id="0b54" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk"><strong class="nj fr">Extracted keywords </strong>are valuable for both backend and frontend applications. We can use them for various downstream analyses, such as theme and topic exploration (discussed in Part 2). On the front end, these keywords can empower users to find listings with similar characteristics — think of them like hashtags on Instagram or Twitter (but automatic!). You can also highlight and display these keywords or make them clickable. For example, named entity recognition (NER) can identify locations like “Iidabashi” or “Asakusa.” When a user hovers over these keywords, a pop-up can display relevant information about those places.</p><p id="cc69" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Summaries</strong> provide a concise overview of the listing, making them ideal for quickly grasping the key details, or for mobile displays.</p><figure class="oc od oe of og ob ol om paragraph-image"><div role="button" tabindex="0" class="qz ra ed rb bh rc"><div class="ol om ro"><img src="../Images/7edfd94e96e80f98979310e6250eedf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyhKFXuJ-sEp2RK2q7Lwqg.gif"/></div></div><figcaption class="oi oj ok ol om on oo bf b bg z dx">Keywords and text summaries can enrich user experience. In this example, we use the extracted text summary to provide quick overview of the listing description. The select keywords (example, LOC) are also used to provide more context of the listing description. This process can be done either at the back end (for faster load), or at the front end (for more convenience).</figcaption></figure></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="df1d" class="ov mk fq bf ml ow qf gq mp oy qg gt mt pa qh pc pd pe qi pg ph pi qj pk pl pm bk">Moving Forward</h1><p id="4137" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In this article, we demonstrated the practical implementation of various NLP techniques, such as text summarization and named entity recognition (NER) on a rental listing dataset. These techniques can significantly improve user experience by providing concise, informative, and easily searchable rental listings.</p><p id="8e27" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">In the upcoming article (Part 2), we will use methods like clustering to discover hidden themes and labels. This will allow us to build a robust model that can act as a recommender engine. We will also explore advanced NLP techniques like topic modeling and text classification further to enhance the analysis and usability of rental listing descriptions.</p></div></div></div><div class="ab cb px py pz qa" role="separator"><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd qe"/><span class="qb by bm qc qd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8c4d" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk">以上です★これからもうよろしくおねがいします☆また今度｡</p><p id="c401" class="pw-post-body-paragraph nh ni fq nj b go oq nl nm gr or no np mu os nr ns my ot nu nv nc ou nx ny nz fj bk"><strong class="nj fr">Note: <br/>1) Github Repository: </strong><a class="af oa" href="https://github.com/kristiyanto/nlp_on_airbnb_dataset" rel="noopener ugc nofollow" target="_blank"><strong class="nj fr">https://github.com/kristiyanto/nlp_on_airbnb_dataset</strong></a><strong class="nj fr"><br/>2) Data (</strong><a class="af oa" href="http://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"><strong class="nj fr">Creative Commons Attribution 4.0 International License</strong></a><strong class="nj fr">): </strong><a class="af oa" href="https://insideairbnb.com/get-the-data/" rel="noopener ugc nofollow" target="_blank"><strong class="nj fr">https://insideairbnb.com/get-the-data/</strong></a><strong class="nj fr"><br/>3) All images in this article are produced by the author.</strong></p></div></div></div></div>    
</body>
</html>