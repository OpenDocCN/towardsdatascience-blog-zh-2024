- en: The Math Behind the Adam Optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30](https://towardsdatascience.com/the-math-behind-adam-optimizer-c41407efe59b?source=collection_archive---------0-----------------------#2024-01-30)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is Adam the most popular optimizer in Deep Learning? Let’s understand it
    by diving into its math, and recreating the algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[![Cristian
    Leo](../Images/99074292e7dfda50cf50a790b8deda79.png)](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    [Cristian Leo](https://medium.com/@cristianleo120?source=post_page---byline--c41407efe59b--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c41407efe59b--------------------------------)
    ·16 min read·Jan 30, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/70d06b2ec8e8cb2a346295529f913efd.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALLE-2
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve clicked on this article, you’ve likely heard about Adam, a name that
    has gained notable recognition in many winning Kaggle competitions. It’s common
    to experiment with a few optimizers like SGD, Adagrad, Adam, or AdamW, but truly
    understanding their mechanics is a different story. By the end of this post, you’ll
    be among the select few who not only know about Adam optimization but also understand
    how to leverage its power effectively.
  prefs: []
  type: TYPE_NORMAL
- en: By the way, in my previous post I described the math behind Stochastic Gradient
    Descent, if you missed it, I recommend reading it.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
    [## Stochastic Gradient Descent: Math and Python Code'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Dive on Stochastic Gradient Descent. Algorithm, assumptions, benefits,
    formula, and practical implementation.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@cristianleo120/stochastic-gradient-descent-math-and-python-code-35b5e66d6f79?source=post_page-----c41407efe59b--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Index**'
  prefs: []
  type: TYPE_NORMAL
- en: '**·** [**1: Understanding the Basics**](#e382)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [1.1: What is the Adam Optimizer?](#63c0)'
  prefs: []
  type: TYPE_NORMAL
- en: '∘ [1.2: The Mechanics of Adam](#de8c)'
  prefs: []
  type: TYPE_NORMAL
- en: '**·** [**2\. Adam’s Algorithm Explained**](#bc0c)'
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [2.1 The Mathematics Behind Adam](#5ff9)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [2.2 The Role](#3663)…
  prefs: []
  type: TYPE_NORMAL
