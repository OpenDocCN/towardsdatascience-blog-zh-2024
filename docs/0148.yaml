- en: Time Series Prediction with Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16](https://towardsdatascience.com/time-series-prediction-with-transformers-2b64478a4cbd?source=collection_archive---------4-----------------------#2024-01-16)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Complete Guide to Transformers in Pytorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page---byline--2b64478a4cbd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2b64478a4cbd--------------------------------)
    ·17 min read·Jan 16, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: At the latest since the advent of [ChatGPT](https://chat.openai.com/), [Large
    Language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) have
    created a huge hype, and are known even to those outside the AI community. Even
    though one needs to understand that [LLMs inherently are “just” sequence prediction
    models](https://www.linkedin.com/posts/yann-lecun_i-have-claimed-that-auto-regressive-llms-activity-7045908925660950528-hJGk)
    without any form of intelligence or reasoning — the achieved results are certainly
    extremely impressive, with some even talking about another step in the “AI Revolution”.
  prefs: []
  type: TYPE_NORMAL
- en: Essential to the success of LLMs are their core building blocks, **transformers**.
    In this post, we will give a complete guide of using them in Pytorch, with particular
    focus on time series prediction. Thanks for stopping by, and I hope you enjoy
    the ride!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b34cf6d50a71d4ef98f9bdb04c6a7be8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Tim Meyer](https://unsplash.com/@timmeyer?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/yellow-chevrolet-coupe-close-up-photography-GIm7wxiAZys?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: One could argue that all problems solved via transformers essentially are time
    series problems. While that is true, here we will put special focus to continuous
    series and data — such as predicting the spreading of diseases or forecasting
    the weather. The difference to the prominent application of Natural Language Processing
    ([NLP](https://en.wikipedia.org/wiki/Natural_language_processing)) simply (if
    this word is allowed in this context — developing a model like ChatGPT and making
    it work naturally does require a multitude of further optimization steps and tricks)
    is the continuous input space, while NLP works with discrete tokens. However,
    apart from…
  prefs: []
  type: TYPE_NORMAL
