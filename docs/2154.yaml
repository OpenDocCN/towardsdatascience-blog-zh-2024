- en: How to Train a Vision Transformer (ViT) from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04](https://towardsdatascience.com/how-to-train-a-vision-transformer-vit-from-scratch-f26641f26af2?source=collection_archive---------3-----------------------#2024-09-04)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide to implementing the Vision Transformer (ViT)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page---byline--f26641f26af2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--f26641f26af2--------------------------------)
    ·12 min read·Sep 4, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Hi everyone! For those who do not know me yet, my name is Francois, I am a Research
    Scientist at Meta. I have a passion for explaining advanced AI concepts and making
    them more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, let’s dive into one of the most significant contribution in the field
    of Computer Vision: the **Vision Transformer (ViT).**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This post focuses on the state-of-the-art implementation of the Vision Transformer
    since its release. To fully understand how a ViT works, I strongly recommend reading
    my other post on the theoretical foundations: [The Ultimate Guide to Vision Transformers](https://medium.com/towards-data-science/the-ultimate-guide-to-vision-transformers-0a6df32cb248)'
  prefs: []
  type: TYPE_NORMAL
- en: How to train a VIT From scratch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ffb3dc35549048018b8f386287afdb0f.png)'
  prefs: []
  type: TYPE_IMG
- en: ViT Architecture, image from [original article](https://arxiv.org/pdf/2010.11929)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Attention Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/f8025d10646523676e2805e3db31de95.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention Layer, image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the most well-known building block of the Transformer Encoder:
    the Attention Layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
