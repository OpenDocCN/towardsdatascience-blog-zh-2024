- en: Tips for Getting the Generation Part Right in Retrieval Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/tips-for-getting-the-generation-part-right-in-retrieval-augmented-generation-7deaa26f28dc?source=collection_archive---------2-----------------------#2024-04-06](https://towardsdatascience.com/tips-for-getting-the-generation-part-right-in-retrieval-augmented-generation-7deaa26f28dc?source=collection_archive---------2-----------------------#2024-04-06)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/0b1cd357c6cc8229b1a1bdc4fc421ba1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Dall-E 3
  prefs: []
  type: TYPE_NORMAL
- en: Results from experiments to evaluate and compare GPT-4, Claude 2.1, and Claude
    3.0 Opus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://aparnadhinak.medium.com/?source=post_page---byline--7deaa26f28dc--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page---byline--7deaa26f28dc--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7deaa26f28dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7deaa26f28dc--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page---byline--7deaa26f28dc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7deaa26f28dc--------------------------------)
    ·6 min read·Apr 6, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*My thanks to* [*Evan Jolley*](https://www.linkedin.com/in/evanjolley/) *for
    his contributions to this piece*'
  prefs: []
  type: TYPE_NORMAL
- en: New evaluations of RAG systems are published seemingly every day, and many of
    them focus on the retrieval stage of the framework. However, the generation aspect
    — how a model synthesizes and articulates this retrieved information — may hold
    equal if not greater significance in practice. Many use cases in production are
    not simply returning a fact from the context, but also require synthesizing the
    fact into a more complicated response.
  prefs: []
  type: TYPE_NORMAL
- en: We ran several experiments to evaluate and compare GPT-4, Claude 2.1 and [Claude
    3](https://www.anthropic.com/news/claude-3-family) Opus’ generation capabilities.
    This article details our research methodology, results, and model nuances encountered
    along the way as well as why this matters to people building with generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Everything needed to reproduce the results can be found in this [GitHub repository](https://github.com/Arize-ai/LLMTest_NeedleInAHaystack).
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although initial findings indicate that Claude outperforms GPT-4, subsequent
    tests reveal that with strategic prompt engineering GPT-4 demonstrated superior
    performance across a broader range of evaluations. Inherent model behaviors and
    prompt engineering matter A LOT in RAG systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simply adding “Please explain yourself then answer the question” to a prompt
    template significantly improves (more than 2X) GPT-4’s performance. It’s clear
    that when an LLM talks answers out, it seems to help in unfolding ideas. It’s
    possible that by explaining, a model is re-enforcing the right answer in embedding/attention
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phases of RAG and Why Generation is Important
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b85c5041b6dc28b4a086b9796ec04480.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Diagram created by author'
  prefs: []
  type: TYPE_NORMAL
- en: While retrieval is responsible for identifying and retrieving the most pertinent
    information, it is the generation phase that takes this raw data and transforms
    it into a coherent, meaningful, and contextually appropriate response. The generative
    step is tasked with synthesizing the retrieved information, filling in gaps, and
    presenting it in a manner that is easily understandable and relevant to the user’s
    query.
  prefs: []
  type: TYPE_NORMAL
- en: In many real-world applications, the value of RAG systems lies not just in their
    ability to locate a specific fact or piece of information but also in their capacity
    to integrate and contextualize that information within a broader framework. The
    generation phase is what enables RAG systems to move beyond simple fact retrieval
    and deliver truly intelligent and adaptive responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test #1: Date Mapping'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The initial test we ran involved generating a date string from two randomly
    retrieved numbers: one representing the month and the other the day. The models
    were tasked with:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Retrieving Random Number #1*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Isolating the last digit and incrementing by 1*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Generating a month for our date string from the result*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Retrieving Random Number #2*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Generating the day for our date string from Random Number 2*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, random numbers 4827143 and 17 would represent April 17th.
  prefs: []
  type: TYPE_NORMAL
- en: These numbers were placed at varying depths within contexts of varying length.
    The models initially had quite a difficult time with this task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1169b511246aaad65af5519ac3416916.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Initial test results (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: While neither model performed great, Claude 2.1 significantly outperformed GPT-4
    in our initial test, almost quadrupling its success rate. It was here that Claude’s
    verbose nature — providing detailed, explanatory responses — seemed to give it
    a distinct advantage, resulting in more accurate outcomes compared to GPT-4’s
    initially concise replies.
  prefs: []
  type: TYPE_NORMAL
- en: Prompted by these unexpected results, we introduced a new variable to the experiment.
    We instructed GPT-4 to “explain yourself then answer the question,” a prompt that
    encouraged a more verbose response akin to Claude’s natural output. The impact
    of this minor adjustment was profound.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5dc3161bdfb87275c02bb1900769007e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Initial test with targeted prompt results (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4’s performance improved dramatically, achieving flawless results in subsequent
    tests. Claude’s results also improved to a lesser extent.
  prefs: []
  type: TYPE_NORMAL
- en: This experiment not only highlights the differences in how language models approach
    generation tasks but also showcases the potential impact of prompt engineering
    on their performance. The verbosity that appeared to be Claude’s advantage turned
    out to be a replicable strategy for GPT-4, suggesting that the way a model processes
    and presents its reasoning can significantly influence its accuracy in generation
    tasks. Overall, including the seemingly minute “explain yourself” line to our
    prompt played a role in improving the models’ performance across all of our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Further Testing and Results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/e644fb91beeb1871e295b10ecc286e1f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Four further tests used to evaluate generation (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted four more tests to assess prevailing models’ ability to synthesize
    and transform retrieved information into various formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '***String Concatenation***: Combining pieces of text to form coherent strings,
    testing the models’ basic text manipulation skills.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Money Formatting***: Formatting numbers as currency, rounding them, and
    calculating percentage changes to evaluate the models’ precision and ability to
    handle numerical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Date Mapping***: Converting a numerical representation into a month name
    and date, requiring a blend of retrieval and contextual understanding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Modulo Arithmetic***: Performing complex number operations to test the models’
    mathematical generation capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsurprisingly, each model exhibited strong performance in string concatenation,
    reaffirming previous understanding that text manipulation is a fundamental strength
    of language models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f3758466dd21ade4fd7d72b365f556e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Money formatting test results (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: As for the money formatting test, Claude 3 and GPT-4 performed almost flawlessly.
    Claude 2.1’s performance was generally poorer overall. Accuracy did not vary considerably
    across token length, but was generally lower when the needle was closer to the
    beginning of the context window.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91f0318ea917c7bc7eeb3b603296b9c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Normal haystack test results (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Despite stellar results in the generation tests, Claude 3’s accuracy declined
    in a retrieval-only experiment. Theoretically, simply retrieving numbers should
    be an easier task than manipulating them as well — making this decrease in performance
    surprising and an area where we’re planning further testing to examine. If anything,
    this counterintuitive dip only further confirms the notion that both retrieval
    and generation should be tested when developing with RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By testing various generation tasks, we observed that while both models excel
    in menial tasks like string manipulation, their strengths and weaknesses [become
    apparent](https://arize.com/blog-course/research-techniques-for-better-retrieved-generation-rag/)
    in more complex scenarios. LLMs are still not great at math! Another key result
    was that the introduction of the “explain yourself” prompt notably enhanced GPT-4’s
    performance, underscoring the importance of how models are prompted and how they
    articulate their reasoning in achieving accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: These findings have broader implications for the evaluation of LLMs. When comparing
    models like the verbose Claude and the initially less verbose GPT-4, it becomes
    evident that the [RAG evaluation](https://arize.com/blog-course/rag-evaluation/)
    criteria must extend beyond mere correctness. The verbosity of a model’s responses
    introduces a variable that can significantly influence their perceived performance.
    This nuance may suggest that future model evaluations should consider the average
    length of responses as a noted factor, providing a better understanding of a model’s
    capabilities and ensuring a fairer comparison.
  prefs: []
  type: TYPE_NORMAL
