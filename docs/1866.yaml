- en: Evaluating Long Context Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluating-long-context-large-language-models-fbd665b3544c?source=collection_archive---------9-----------------------#2024-07-31](https://towardsdatascience.com/evaluating-long-context-large-language-models-fbd665b3544c?source=collection_archive---------9-----------------------#2024-07-31)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is a race towards language models with longer context windows. But how
    good are they, and how can we know?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page---byline--fbd665b3544c--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page---byline--fbd665b3544c--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fbd665b3544c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fbd665b3544c--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page---byline--fbd665b3544c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fbd665b3544c--------------------------------)
    ·9 min read·Jul 31, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc86bcc54c0578652766a0d4fdb2c64e.png)'
  prefs: []
  type: TYPE_IMG
- en: The context window of language models have been growing at an exponential rate
    in the last few years. Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was originally published on* [*Art Fish Intelligence*](https://www.artfish.ai/p/long-context-llms)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The context window of large language models — the amount of text they can process
    at once — has been increasing at an exponential rate.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, language models like [BERT](https://arxiv.org/abs/1810.04805), [T5](https://arxiv.org/abs/1910.10683),
    and [GPT-1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
    could take up to 512 tokens as input. Now, in summer of 2024, this number has
    jumped to 2 million tokens (in publicly available LLMs). But what does this mean
    for us, and how do we evaluate these increasingly capable models?
  prefs: []
  type: TYPE_NORMAL
- en: What does a large context window mean?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recently released [Gemini 1.5 Pro model can take in up to 2 million tokens](https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/).
    But what does 2 million tokens even mean?
  prefs: []
  type: TYPE_NORMAL
- en: If we estimate 4 words to roughly equal about 3 tokens, it means that 2 million
    tokens can (*almost*) fit the entire Harry Potter and Lord of the Ring series.
  prefs: []
  type: TYPE_NORMAL
- en: '*(The total word count of all seven books in the Harry Potter series is* [*1,084,625*](https://brokebybooks.com/the-word-count-of-175-favorite-novels/)*.
    The total word count of all seven books in the Lord of the Ring series is*[](https://brokebybooks.com/the-word-count-of-175-favorite-novels/#:~:text=Narnia%20series%20is-,345%2C535,-.%20That%E2%80%99s%20approximately%20the)[*481,103*](https://www.reddit.com/r/todayilearned/comments/fnheyt/til_that_tolkeins_lord_of_the_rings_trilogy_has_a/)*.
    (1,084,625 +*…'
  prefs: []
  type: TYPE_NORMAL
