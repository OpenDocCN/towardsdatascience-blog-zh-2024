- en: 'Navigating Your Data Platform’s Growing Pains: A Path from Data Mess to Data
    Mesh'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/navigating-your-data-platforms-growing-pains-a-path-from-data-mess-to-data-mesh-c16df72f5463?source=collection_archive---------3-----------------------#2024-04-03](https://towardsdatascience.com/navigating-your-data-platforms-growing-pains-a-path-from-data-mess-to-data-mesh-c16df72f5463?source=collection_archive---------3-----------------------#2024-04-03)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unlike their software counterparts, data teams lack established methodologies
    for overcoming scalability challenges. This article offers a set of guiding principles
    to effectively scale your data platform while maximizing its business impact.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mahdiqb.medium.com/?source=post_page---byline--c16df72f5463--------------------------------)[![Mahdi
    Karabiben](../Images/f1aac76435b8db295c306c76796a3201.png)](https://mahdiqb.medium.com/?source=post_page---byline--c16df72f5463--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c16df72f5463--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c16df72f5463--------------------------------)
    [Mahdi Karabiben](https://mahdiqb.medium.com/?source=post_page---byline--c16df72f5463--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c16df72f5463--------------------------------)
    ·10 min read·Apr 3, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f53715373f78a1e603307565cec75f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When working on software components, developers can leverage a wide range of
    frameworks, design patterns, and principles to scale their products and seamlessly
    adjust their architecture to support new use cases and handle increasing usage
    and complexity. This allows software engineering teams to ensure optimized performance
    and reliability as their platform (and its value) grows in scale.
  prefs: []
  type: TYPE_NORMAL
- en: Data teams, however, are not so fortunate. While the initial months of a data
    platform’s lifecycle are often marked by the excitement of tackling complex technical
    challenges and the joy of delivering a first wave of data products, what often
    follows is a daunting spiral of mounting complexity, rising costs, and diminishing
    returns.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other problems that we need to navigate as data teams, our scalability
    struggles are inherently different from the ones faced by software teams. In the
    data world, these struggles come in the form of unavoidable technical complexity
    (like mixing a multitude of patterns to move and transform data across an ever-growing
    list of systems) and the data platform’s unique positioning within the company
    (since eventually every business unit gets connected to it either directly or
    indirectly).
  prefs: []
  type: TYPE_NORMAL
- en: So, [in this post-MDS world](https://benn.substack.com/p/the-problem-was-the-product),
    where data teams are thoroughly scrutinized over their spending and continuously
    asked to showcase their value, it is more important than ever to define standards
    and tenets for successfully scaling a data platform. This article will focus on
    five principles critical to achieving this while also offering strategies for
    applying them.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Don’t lose sight of what matters (i.e. business value)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most cases, data platforms have the potential to be one of the company’s
    most valuable assets. However, in their journey towards proving their value, it
    is unfortunately very common for data teams to focus on the *what* instead of
    the *why*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you go through the data content available on the web, you’ll mostly come
    across complex data architectures or praise/criticism of certain technologies.
    On the other hand, content that focuses on enabling downstream use cases or measuring
    the impact of data initiatives is relatively rare. I consider this a symptom of
    a key problem that data teams need to address: **we quickly get too focused on
    what we’re building and lose sight of why we’re building it**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This problem can eventually lead data teams to the path of building for the
    sake of building. Instead, the more scalable approach is to be on the hunt for
    value-generating data initiatives and to continuously reflect on (and measure
    when possible) the value being generated. To do so, I recommend the following
    four strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure out which data matters (Not all data is created equal)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since today’s technologies make it extremely easy to generate, store, and transform
    massive amounts of data, data teams can quickly get overwhelmed by the rapidly
    increasing number of datasets they have to support. However, the amount of attention
    you should give to a dataset must be based on its importance, which is determined
    by connecting it to its downstream use cases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Connect every initiative/project to the value it’s going to generate**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before a data project is greenlit, it should be evaluated for the business value
    it will generate (directly or indirectly). This important exercise ensures alignment
    with your stakeholders and allows you to map your data projects to downstream
    initiatives. Writing [design docs](/writing-design-docs-for-data-pipelines-d49550f95580)
    for data pipelines and other types of data components is a great approach to ensure
    that what you’re building aligns with clearly defined business goals and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuously look for new high-impact use cases**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data teams commonly rely on receiving requests and potential projects from other
    departments and business units. Although it may be tempting to rely on such processes,
    you should always be on the lookout for potentially value-generating areas in
    which data can be leveraged. Whether it’s an internal data product or a data app
    that can be embedded within the company’s product, key use cases might slip away
    simply because the data team was building things in its own corner.
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequently ask for feedback from your users**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s easy for data teams to focus on the wrong metrics. For example, elevated
    usage might give us a sense of accomplishment. However, in a data-driven environment,
    people need data across the board — so elevated usage doesn’t necessarily mean
    that everything is going great or that the usage is going into value-generating
    use cases. As the platform scales, cracks will eventually show (ranging from data
    quality problems to low development speed), and being disconnected from your users
    means that you’ll have to play catch-up to gain their trust again. Instead, try
    to frequently ask for feedback, whether via embedded feedback mechanisms in the
    tools you offer or a simple feedback form that you send to users every few months.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Automated standards are your best friend
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the biggest challenges of scaling a data platform is getting pulled
    in many directions. Whether it’s due to the influx of projects that require new
    data components or the endless stream of ad-hoc requests, there will come a time
    in the data team’s journey when they become a bottleneck. That time usually leads
    to an unfortunate decision: adopting a self-serve model without setting the right
    foundations for it. This means that other teams get to build their own datasets
    or pipelines to generate insights — which starts well until someone eventually
    realizes that no dataset can be trusted anymore.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e81c19f270750bc99b9fba19e99fdd99.png)'
  prefs: []
  type: TYPE_IMG
- en: The pitfalls of adopting a self-serve approach without governance (image by
    author)
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation here isn’t to avoid self-serve. Instead, you should define
    standards for how things should be done and automate their enforcement as much
    as possible. These standards can include basic things like table/column/model
    naming conventions and mandatory documentation or more nuanced approaches like
    mandatory tests and only using governed datasets in production pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Applying such standards — via Continuous Integration (CI) automation for example
    — ensures a minimum of consistency and prevents scenarios in which the data platform
    eventually becomes a data mess or swamp.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that the standards you choose to apply should
    have a concrete business justification and shouldn’t impact the development speed
    without tangible benefits. The right balance between standardization and iteration/delivery
    speed depends on the company’s context and the use cases in which the data is
    being used, but in all cases, a minimum of standardization is required.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Frequently ask yourself whether you have the right tools for the current
    scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an engineer myself, I know how easy it is to get too attached to the neat
    package you built to do X or the open-source project you leveraged (and maybe
    contributed to) to do Y. But as your data platform grows in scale, routinely revisiting
    your architecture to identify areas where the tooling needs an upgrade (or a downgrade)
    is an important exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Migrations have a cost, of course, and they’re always tricky — but performing
    them at the right time and [having a playbook for replacing components of your
    platform](https://lethain.com/migrations/) is a critical skill for every data
    team. To assess which tools may not be suitable for your current scale, I think
    it’s essential to have an up-to-date, high-level answer to the following questions
    for every component within your stack:'
  prefs: []
  type: TYPE_NORMAL
- en: How much does it cost? (In terms of the engineering time to maintain it, its
    infra costs, its pricing, etc.) Is this cost acceptable to us? How will this cost
    evolve throughout the next year?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are we spending resources (engineering time, money, etc.) on it that would deliver
    more business value if spent elsewhere?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does it have all the main features we currently need and the ones we’ll need
    in the next year? If not, are there other options that deliver the features we’re
    looking for? If such other options exist, what would the migration cost be?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By doing this exercise, you’ll definitely come across areas where you can decommission
    things altogether (like pipelines or tools that are not providing any meaningful
    value), others where you need to replace one system with another one (maybe you
    built a real-time ingestion process but all the current business-critical consumption
    scenarios only require batch), and finally areas where you need an upgrade (like
    moving from an in-house data movement process to an established tool that comes
    with more features out-of-the-box).
  prefs: []
  type: TYPE_NORMAL
- en: As a more practical example, even though dbt tests are a great starting point
    to ensure data quality for your key assets and allow consumers to trust the data
    they use, they quickly reach the limit of their usefulness as you add more systems
    to your data platform and start supporting production (or even operational) use
    cases. Once the effort you spend on data incidents and debugging data issues starts
    impacting your roadmap, it may be time to upgrade from dbt tests to a data observability
    component.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Don’t be too ambitious (prioritize your problems)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This principle goes hand-in-hand with the above one. Just like it’s extremely
    important to perform migrations when needed, it’s even more important to avoid
    making unnecessary ones.
  prefs: []
  type: TYPE_NORMAL
- en: The current pace at which the data space is progressing means that new technologies,
    paradigms, and architectures are a constant. Tech behemoths are constantly rethinking
    their approaches to data, and no matter how much you try, most of your platform
    will always be a few steps behind the data pioneers. This isn’t, however, a bad
    thing.
  prefs: []
  type: TYPE_NORMAL
- en: As you reflect on the current state of the art and try to identify gaps in your
    platform, the guiding light should be the business value you expect to generate
    via the upgrade. Sure, capability X sounds nice on paper, but which specific business
    use cases does it solve, and which other initiatives can you spend the migration
    effort on?
  prefs: []
  type: TYPE_NORMAL
- en: I believe that it’s crucial to stay up-to-date with the state of the art in
    the field and read about the journeys of other data teams, but the decision to
    adopt a new tool or paradigm always needs to be based on the expected business
    value. Additionally, it’s critical to always have your business priorities in
    mind and update them accordingly as the environment around the data team changes.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Shift some of the responsibility left (and right)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you cover the above areas, and especially once the data platform starts
    supporting high-impact projects, the data team can start shifting into a *platform
    team* role. This means that you can confidently start adding more nuance to ownership
    and relieve the data team from the burden of managing business metrics and source
    data issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'The timing of this transition is critical since the decentralization of responsibilities
    can only succeed if you have already built the right foundations:'
  prefs: []
  type: TYPE_NORMAL
- en: Making the data platform a value-generating system that is critical to the success
    of the business (and so other teams are incentivized to invest their time in data-related
    work)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardizing the different processes, defining clear interfaces, and minimizing
    the amount of guess-work done by the teams that contribute to the platform (as
    producers, consumers, or contributors)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having the right toolset that allows you to onboard new teams and navigate a
    more complex structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, instead of trying to transition all of your data products to
    a decentralized model in one go, you’ll increase your chances of success if you
    perform the transition in stages. First, start by identifying the most critical
    datasets and data pipelines, for which you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Establish contracts with the data producers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move the ownership of metrics definition to the corresponding business teams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once this first initiative succeeds, you can start widening the range of pipelines
    that adhere to the new model based on their importance to the business. It’s completely
    fine not to apply this model to all datasets — if a data pipeline doesn’t have
    a meaningful business use case yet, it doesn’t need to be highly governed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc9fd8a84493e6387e80a7318a8cba24.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample data platform functioning at scale (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: The above points are, ultimately, the culmination of the first four principles
    discussed in this article. Many decentralization / data mesh initiatives fail
    either because the right foundations are lacking or the data team attempts to
    make a very optimistic (and unrealistic) transition from a completely centralized
    to a decentralized approach instead of performing the shift in stages.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explored five principles for successfully scaling your data
    platform and turning it into a value-generating component as it grows in size,
    complexity, and importance. Their application doesn’t require you to use specific
    tools or technologies. Instead, they’re guidelines that can be universally applied
    when working on data projects (except for certain edge cases / niche industries).
  prefs: []
  type: TYPE_NORMAL
- en: The end state that we discussed, which is a hybrid setup between a centralized
    approach (in which the data team owns the platform end-to-end) and a decentralized
    / data mesh one (in which different teams build and own their data products end-to-end),
    elevates your chances of successfully scaling the platform as you transition from
    one approach to another — but its suitability for your team would also depend
    on your industry and the context in which you’re operating.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if there’s just one principle/rule to remember, then make it this
    one: **It’s extremely important to frequently reevaluate your approach and the
    path you’re on. What got you from 0 to 1 is very different from what would get
    you from 1 to 10 (and so on).** This is more of a universal principle that I believe
    is especially relevant for data platforms.'
  prefs: []
  type: TYPE_NORMAL
- en: '*For more data engineering content you can subscribe to my newsletter, Data
    Espresso, in which I discuss various topics related to data engineering and technology
    in general:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://dataespresso.substack.com/?source=post_page-----c16df72f5463--------------------------------)
    [## Data Espresso | Mahdi Karabiben | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering updates and commentary to accompany your afternoon espresso.
    Click to read Data Espresso, by Mahdi…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: dataespresso.substack.com](https://dataespresso.substack.com/?source=post_page-----c16df72f5463--------------------------------)
  prefs: []
  type: TYPE_NORMAL
