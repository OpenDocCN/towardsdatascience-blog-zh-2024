["```py\ndataset = Dataset(example_data_downloader(\"correctness\"))\n\n# Samples are annotated with \"correct\", \"incorrect\" or \"refuse-to-answer\"\n# We remove the samples where the LLL refused to answer (i.e., said \"I don't know\")\ndataset.filter(lambda x: x[\"annotation\"] != \"refuse-to-answer\")\ndataset.sample(300)  # Only for this example: randomly sample 300 examples\n```", "```py\ntone = LLMBasedCustomMetric(\n    name=\"Tone\",\n    definition=\"The Tone/Content Issues metric evaluates the appropriateness and accuracy of the tone and content in responses to specific questions. It focuses on ensuring that the tone is professional and suitable for the context, and that the content accurately addresses the question without unnecessary deviations or inaccuracies. This metric is crucial for maintaining a professional image and ensuring clear, direct communication.\",\n    scoring_rubric=\"\"\"Use the following rubric to assign a score to the answer based on its tone:\n- Score 1: The response is inappropriate or inaccurate, with a tone that is either too informal, overly strong, or not suited to the professional context. The content may be irrelevant, incorrect, or fail to directly address the question posed.\n- Score 2: The response is mostly appropriate and accurate but may contain minor tone or content issues. The tone is generally professional but may slip into informality or unnecessary strength in places. The content addresses the question but may include minor inaccuracies or unnecessary details.\n- Score 3: The response is appropriate and accurate, with a tone that is professional and suited to the context. The content directly and correctly addresses the question without unnecessary deviations or inaccuracies.\"\"\",\n    scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),\n    model_parameters={\"temperature\": 0},\n)\n```", "```py\nconciseness = LLMBasedCustomMetric(\n    name=\"Conciseness\",\n    definition=\"Conciseness in communication refers to the expression of ideas in a clear and straightforward manner, using the fewest possible words without sacrificing clarity or completeness of information. It involves eliminating redundancy, verbosity, and unnecessary details, focusing instead on delivering the essential message efficiently. \",\n    scoring_rubric=\"\"\"Use the following rubric to assign a score to the answer based on its conciseness:\n- Score 1: The answer is overly verbose, containing a significant amount of unnecessary information, repetition, or redundant expressions that do not contribute to the understanding of the topic.\n- Score 2: The answer includes some unnecessary details or slightly repetitive information, but the excess does not severely hinder understanding.\n- Score 3: The answer is clear, direct, and to the point, with no unnecessary words, details, or repetition.\"\"\",\n    scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),\n    model_parameters={\"temperature\": 0},\n)\n```", "```py\npipeline = SingleModulePipeline(\n    dataset=dataset,\n    eval=[\n        DeterministicAnswerCorrectness().use(\n            answer=dataset.answer,\n            ground_truth_answers=dataset.ground_truths,\n        ),\n        LLMBasedAnswerCorrectness().use(\n            question=dataset.question,\n            answer=dataset.answer,\n            ground_truth_answers=dataset.ground_truths,\n        ),\n        LLMBasedAnswerRelevance().use(\n            question=dataset.question, answer=dataset.answer\n        ),\n        LLMBasedStyleConsistency().use(\n            answer=dataset.answer, ground_truth_answers=dataset.ground_truths\n        ),\n        FleschKincaidReadability().use(answer=dataset.answer),\n        tone.use(\n            question=dataset.question,\n            answer=dataset.answer,\n            ground_truth_answers=dataset.ground_truths,\n        ),\n        conciseness.use(\n            question=dataset.question,\n            answer=dataset.answer,\n            ground_truth_answers=dataset.ground_truths,\n        ),\n    ],\n)\n```", "```py\neval_manager = EvaluationManager(pipeline)\n# The dataset already contains the model output so we just set the evaluation results\neval_manager.evaluation.results = dataset.data\neval_manager.run_metrics()  # Note: there is no progress bar, it might take a few minutes\n```", "```py\ndatasplit = DataSplit(\n    X=eval_manager.metrics.to_pandas(),\n    y=map(lambda x: 1 if x == \"correct\" else 0, dataset[\"annotation\"]),\n    split_ratios=SplitRatios(train=0.6, test=0.2, calibration=0.2),\n)\n\n# We use the train and calibration sets to train the classifier\npredictor = EnsembleMetric(training=datasplit.train, calibration=datasplit.calibration)\n```"]