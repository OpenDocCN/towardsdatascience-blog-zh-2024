- en: How I Dockerized Apache Flink, Kafka, and PostgreSQL for Real-Time Data Streaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何将 Apache Flink、Kafka 和 PostgreSQL Docker 化，实现实时数据流处理
- en: 原文：[https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19](https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19](https://towardsdatascience.com/how-i-dockerized-apache-flink-kafka-and-postgresql-for-real-time-data-streaming-c4ce38598336?source=collection_archive---------1-----------------------#2024-06-19)
- en: Integrating pyFlink, Kafka, and PostgreSQL using Docker
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Docker 集成 pyFlink、Kafka 和 PostgreSQL
- en: '[](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Augusto
    de Nevrezé](../Images/bd7d6509149ddb447dd7e5af9f09e4b1.png)](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    [Augusto de Nevrezé](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Augusto
    de Nevrezé](../Images/bd7d6509149ddb447dd7e5af9f09e4b1.png)](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    [Augusto de Nevrezé](https://adenevreze.medium.com/?source=post_page---byline--c4ce38598336--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    ·10 min read·Jun 19, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--c4ce38598336--------------------------------)
    ·阅读时间：10分钟·2024年6月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/11d218990f84e7153933c512352bd704.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11d218990f84e7153933c512352bd704.png)'
- en: Get your pyFlink applications ready using docker — author generated image using
    [https://www.dall-efree.com/](https://www.dall-efree.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Docker 准备你的 pyFlink 应用程序 — 作者使用 [https://www.dall-efree.com/](https://www.dall-efree.com/)
    生成的图片
- en: Why Read This?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要阅读这篇文章？
- en: '**Real-World Insights**: Get practical tips from my personal journey of overcoming
    integration hurdles.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实世界的洞察**：从我个人克服集成难题的经历中获取实用的建议。'
- en: '**Complete Setup**: Learn how to integrate Flink, Kafka, and PostgreSQL seamlessly
    using Docker-Compose.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整设置**：学习如何使用 Docker-Compose 无缝集成 Flink、Kafka 和 PostgreSQL。'
- en: '**Step-by-Step Guide**: Perfect for both beginners and experienced developers
    looking to streamline their data streaming stack.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步指南**：非常适合初学者和有经验的开发者，帮助你简化数据流处理架构。'
- en: Setting Up the Scene
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置场景
- en: I embarked on a mission to integrate Apache Flink with Kafka and PostgreSQL
    using Docker. What makes this endeavor particularly exciting is the use of pyFlink
    — the Python flavor of Flink — which is both powerful and relatively rare. This
    setup aims to handle real-time data processing and storage efficiently. In the
    following sections, I’ll demonstrate how I achieved this, discussing the challenges
    encountered and how I overcame them. I’ll conclude with a step-by-step guide so
    you can build and experiment with this streaming pipeline yourself.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '我开始了一个任务，旨在使用 Docker 集成 Apache Flink、Kafka 和 PostgreSQL。这项工作尤其令人兴奋，因为我使用了 pyFlink
    —— Flink 的 Python 版本 —— 它既强大又相对罕见。这个设置的目标是高效地处理和存储实时数据。在接下来的部分，我将展示我是如何实现这一目标的，讨论我遇到的挑战以及如何克服它们。最后，我将提供一个逐步指南，帮助你自己搭建并实验这个数据流管道。  '
- en: The infrastructure we’ll build is illustrated below. Externally, there’s a publisher
    module that simulates IoT sensor messages, similar to what was discussed in a
    [previous post](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd).
    Inside the Docker container, we will create two Kafka topics. The first topic,
    *sensors*, will store incoming messages from IoT devices in real-time. A Flink
    application will then consume messages from this topic, filter those with temperatures
    above 30°C, and publish them to a second topic, *alerts*. Additionally, the Flink
    application will insert the consumed messages into a PostgreSQL table created
    specifically for this purpose. This setup allows us to persist sensor data in
    a structured, tabular format, providing opportunities for further transformation
    and analysis. Visualization tools like Tableau or Power BI can be connected to
    this data for real-time plotting and dashboards.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的基础设施如下所示。从外部来看，有一个发布者模块，用于模拟物联网传感器消息，类似于[之前的文章](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd)中讨论的内容。在
    Docker 容器内部，我们将创建两个 Kafka 主题。第一个主题 *sensors* 用于实时存储来自物联网设备的消息。Flink 应用程序将从该主题消费消息，过滤出温度高于30°C的消息，并将其发布到第二个主题
    *alerts*。此外，Flink 应用程序还将把消费的消息插入到专门为此目的创建的 PostgreSQL 表中。这个设置使我们能够以结构化的表格格式持久化传感器数据，提供进一步转化和分析的机会。像
    Tableau 或 Power BI 这样的可视化工具可以连接到这些数据，用于实时绘图和仪表盘展示。
- en: Moreover, the alerts topic can be consumed by other clients to initiate actions
    based on the messages it holds, such as activating air conditioning systems or
    triggering fire safety protocols.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，alerts 主题可以被其他客户端消费，以根据其包含的消息启动相应的动作，例如启用空调系统或触发消防安全协议。
- en: '![](../Images/a1ad055bc5105bf1c170c1d526b9702c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1ad055bc5105bf1c170c1d526b9702c.png)'
- en: Services included in the docker container — image by author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: docker 容器中包含的服务 —— 图片由作者提供
- en: In order to follow up the tutorial, you can clone the following [repo](https://github.com/augustodn/pyflink-docker).
    A docker-compose.yml is placed in the root of the project so you can initialize
    the multi-container application. Furthermore, you can find detailed instructions
    in the README file.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本教程，你可以克隆以下[仓库](https://github.com/augustodn/pyflink-docker)。项目根目录中包含一个
    docker-compose.yml 文件，便于你初始化多容器应用程序。此外，你可以在 README 文件中找到详细的说明。
- en: Issues With Kafka Ports in docker-compose.yml
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: docker-compose.yml 中 Kafka 端口的问题
- en: Initially, I encountered problems with Kafka’s port configuration when using
    the confluentinc Kafka Docker image, a popular choice for such setups. This issue
    became apparent through the logs, emphasizing the importance of not running docker-compose
    up in detached mode (-d) during initial setup and troubleshooting phases.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，在使用 confluentinc Kafka Docker 镜像时，我遇到了 Kafka 端口配置的问题，这是这种设置的常见选择。通过日志可以明显看到这个问题，突显出在初始设置和故障排除阶段，不应该以分离模式（-d）运行
    docker-compose up 的重要性。
- en: The reason for the failure was that the internal and external hosts were using
    the same port, which led to connectivity problems. I fixed this by changing the
    internal port to 19092\. I’ve found [this](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)
    blog post pretty clarifying.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 失败的原因是内部和外部主机使用了相同的端口，导致了连接问题。我通过将内部端口更改为19092来解决了这个问题。我发现[这篇](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)博客文章对问题的解释相当清晰。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Configuring Flink in Session Mode
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置 Flink 会话模式
- en: To run Flink in [session mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)
    (allowing multiple jobs in a single cluster), I’m using the following directives
    in the docker-compose.yml.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要在[会话模式](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)下运行
    Flink（允许在一个集群中运行多个作业），我在 docker-compose.yml 文件中使用了以下指令。
- en: Custom Docker Image for PyFlink
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyFlink 的自定义 Docker 镜像
- en: Given the limitations of the default Apache Flink Docker image, which doesn’t
    include Python support, I created a [custom Docker image](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)
    for pyFlink. This custom image ensures that Flink can run Python jobs and includes
    the necessary dependencies for integration with Kafka and PostgreSQL. The Dockerfile
    used for this is located in the pyflink subdirectory.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于默认的 Apache Flink Docker 镜像不包括 Python 支持，我为 pyFlink 创建了一个[自定义 Docker 镜像](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)。这个自定义镜像确保
    Flink 可以运行 Python 作业，并包含与 Kafka 和 PostgreSQL 集成所需的依赖项。用于创建此镜像的 Dockerfile 位于 pyflink
    子目录中。
- en: '**Base Image**: We start with the official Flink image.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基础镜像**：我们从官方 Flink 镜像开始。'
- en: '**Python Installation**: Python and pip are installed, upgrading pip to the
    latest version.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Python 安装**：安装了 Python 和 pip，并将 pip 升级到最新版本。'
- en: '**Dependency Management**: Dependencies are installed via requirements.txt.
    Alternatively, lines are commented to demonstrate how to manually install dependencies
    from local files, useful for deployment in environments without internet access.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**依赖管理**：通过 requirements.txt 安装依赖项。或者，可以将某些行注释掉，演示如何从本地文件手动安装依赖项，这对于在没有互联网访问的环境中部署很有用。'
- en: '**Connector Libraries**: Connectors for Kafka and PostgreSQL are downloaded
    directly into the Flink lib directory. This enables Flink to interact with Kafka
    and PostgreSQL during job execution.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**连接器库**：Kafka 和 PostgreSQL 的连接器会直接下载到 Flink 的 lib 目录中。这使得 Flink 在作业执行过程中能够与
    Kafka 和 PostgreSQL 进行交互。'
- en: '**Script Copying**: Scripts from the repository are copied into the /opt/flink
    directory to be executed by the Flink task manager.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**脚本复制**：从代码库中的脚本会复制到 /opt/flink 目录，由 Flink 任务管理器执行。'
- en: With this custom Docker image, we ensure pyFlink can run properly within the
    Docker container, equipped with the necessary libraries to interact with Kafka
    and PostgreSQL seamlessly. This approach provides flexibility and is suitable
    for both development and production environments.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个自定义 Docker 镜像，我们确保 pyFlink 可以在 Docker 容器中正常运行，并配备了与 Kafka 和 PostgreSQL 无缝交互所需的库。这种方法提供了灵活性，适用于开发和生产环境。
- en: '**Note:** Ensure that any network or security considerations for downloading
    connectors and other dependencies are addressed according to your deployment environment’s
    policies.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 请确保根据您的部署环境的政策，处理下载连接器和其他依赖项时的网络或安全性问题。'
- en: Integrating PostgreSQL
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 PostgreSQL
- en: To connect Apache Flink to the PostgreSQL database, a proper JDBC connector
    is required. The custom Docker image for pyFlink downloads the JDBC connector
    for PostgreSQL, which is compatible with PostgreSQL 16.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 Apache Flink 连接到 PostgreSQL 数据库，必须使用适当的 JDBC 连接器。pyFlink 的自定义 Docker 镜像会下载与
    PostgreSQL 16 兼容的 PostgreSQL JDBC 连接器。
- en: To simplify this process, a download_libs.sh script is included in the repository,
    mirroring the actions performed in the Flink Docker container. This script automates
    the download of the necessary libraries, ensuring consistency between the Docker
    and local environments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这一过程，代码库中包含一个 download_libs.sh 脚本，模拟了在 Flink Docker 容器中执行的操作。该脚本自动下载所需的库，确保
    Docker 和本地环境之间的一致性。
- en: '**Note:** Connectors usually have two versions. In this particular case, since
    I’m using Flink 1.18, the latest stable version available, I’ve downloaded 3.1.2–1.18\.
    My guess is that the first version tracks JDBC implementation for several databases.
    They’re available in the [maven directory](https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc/3.1.2-1.18).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 连接器通常有两个版本。在这种情况下，由于我使用的是最新的稳定版本 Flink 1.18，我下载了 3.1.2–1.18 版本。我的猜测是，第一个版本跟踪了多个数据库的
    JDBC 实现。它们可以在 [maven 目录](https://mvnrepository.com/artifact/org.apache.flink/flink-connector-jdbc/3.1.2-1.18)中找到。'
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Defining JDBC Sink**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义 JDBC Sink**'
- en: In our Flink task, there’s a crucial function named configure_postgre_sink located
    in the usr_jobs/postgres_sink.py file. This function is responsible for configuring
    a generic PostgreSQL sink. To use it effectively, you need to provide the SQL
    Data Manipulation Language (DML) statement and the corresponding value types.
    The types used in the streaming data are defined as TYPE_INFO … it took me a while
    to come up with the correct declaration 😅.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Flink 任务中，有一个至关重要的函数，名为 `configure_postgre_sink`，位于 `usr_jobs/postgres_sink.py`
    文件中。这个函数负责配置一个通用的 PostgreSQL 接收器。为了有效使用它，你需要提供 SQL 数据操作语言（DML）语句和相应的值类型。用于流数据的类型定义为
    `TYPE_INFO`... 我花了些时间才找出正确的声明方式 😅。
- en: Notice also that the JdbcSink has an optional parameter to define the ExecutionOptions.
    For this particular case, I’ll use an update interval of 1 second and limit the
    amount of rows to 200\. You can find more information in the [official documentation](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/#jdbc-execution-options).
    Yes, you guessed it, since I’m defining an interval, this can be considered a
    micro-batch ETL. However, due to Flink parallelism you can handle multiple streams
    at once in a simple script which is at the same time, easy to follow.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，JdbcSink 有一个可选参数，用于定义 `ExecutionOptions`。在这个特定的例子中，我将使用 1 秒的更新间隔，并将行数限制为
    200。你可以在[官方文档](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/#jdbc-execution-options)中找到更多信息。是的，你猜对了，由于我定义了一个间隔，这可以视为一个微批次
    ETL。然而，由于 Flink 的并行性，你可以在一个简单的脚本中同时处理多个数据流，而且它非常易于理解。
- en: '**Note:** Don’t forget to create the raw_sensors_data table in Postgres, where
    raw data coming from the IoT sensors will be received. This is covered in the
    step-by-step guide in the sections below.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 别忘了在 Postgres 中创建 `raw_sensors_data` 表，这是接收来自 IoT 传感器的原始数据的地方。这个步骤在下面的逐步指南中已经涵盖。'
- en: Sinking Data to Kafka
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据传输到 Kafka
- en: I’ve covered how to consume data from a Kafka topic in [a previous discussion](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd).
    However, I haven’t configured a sink yet and that’s what we’ll do. The configuration
    has some intricacies and it’s defined in a function, similarly to the Postgres
    sink. Additionally, you have to define the type for the data stream before sinking
    it to Kafka. Notice that the alarms_data stream is properly casted as a string
    with output_type=Types.STRING() before sinking it to Kafka, since I’ve declared
    the serializer as SimpleStringSchema().
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[之前的讨论](https://medium.com/dev-genius/detecting-iot-alerts-with-apache-flink-7a2be19ad9dd)中介绍了如何从
    Kafka 主题消费数据。然而，我还没有配置接收器，接下来我们就会配置。这个配置有一些复杂之处，它是定义在一个函数中的，类似于 Postgres 接收器。此外，在将数据流传输到
    Kafka 之前，你必须定义数据流的类型。请注意，`alarms_data` 流在传输到 Kafka 之前已经正确地转换为字符串，`output_type=Types.STRING()`，因为我已经将序列化器声明为
    `SimpleStringSchema()`。
- en: I’ll show you how to fetch data from the alerts topic in the following steps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤我将展示如何从 `alerts` 主题获取数据。
- en: Local or Containerized configuration
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地或容器化配置
- en: One of the greatest things about this docker configuration is that you can run
    Flink from local or inside the container as a managed task. The local Flink setup
    is depicted in the following figure, where you can see our Flink application detached
    from the docker container. This may help to troubleshoot Flink, which doesn’t
    have a good suite of native observability tools. Actually, we would like to give
    a try to [datorios](https://datorios.com/) tools for Flink, they are very promising
    for monitoring purposes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Docker 配置的一个最大优点是，你可以在本地或容器内部以托管任务的方式运行 Flink。下图展示了本地 Flink 设置，你可以看到我们的 Flink
    应用程序与 Docker 容器是分离的。这有助于排查 Flink 的问题，因为 Flink 本身并没有很好的本地可观察工具。实际上，我们想尝试一下[datorios](https://datorios.com/)提供的
    Flink 工具，它们在监控方面非常有前景。
- en: '![](../Images/6d8993906980fe9862badc9ecdcad799.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d8993906980fe9862badc9ecdcad799.png)'
- en: Runing Flink applications in local with other services running inside the container
    — image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行 Flink 应用程序，并且容器内部还运行着其他服务 — 图像来自作者
- en: 'If you want to try the Flink application locally, you have to correctly define
    the hosts and ports used by the script which actually are two constants in the
    usr_jobs/postgres_sink.py file:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在本地尝试 Flink 应用程序，你需要正确地定义脚本所使用的主机和端口，这实际上是 `usr_jobs/postgres_sink.py` 文件中的两个常量：
- en: 'For container run, use:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时，使用：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For local run, use:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本地运行时，使用：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By default the repo sets up the Flink application to run inside the container.
    You can monitor the jobs running using the web UI, accessing from [http://localhost:8081](http://localhost:8081).
    You won’t be able to see it if you choose to run the job locally.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，仓库设置Flink应用程序在容器内运行。你可以通过Web UI监控正在运行的作业，访问[http://localhost:8081](http://localhost:8081)。如果你选择在本地运行作业，则无法查看。
- en: '![](../Images/add4733c451e9d50906584d638693caf.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/add4733c451e9d50906584d638693caf.png)'
- en: Screenshot of the Flink web UI with the running job — image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 显示Flink Web UI中运行作业的截图——作者提供的图片
- en: '**Note**: If you run the job locally, you need to install the Flink dependencies
    located in the requirements.txt. Also a pyproject.toml file is provided if you
    like to set up the environment with poetry.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：如果在本地运行作业，则需要安装位于requirements.txt中的Flink依赖项。如果你想使用poetry设置环境，还提供了一个pyproject.toml文件。'
- en: Step-by-Step Guide to Run the Streaming Pipeline
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分步指南：运行流处理管道
- en: 'Step 1: Launch the multi-container application'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：启动多容器应用程序
- en: Launch the containers by running docker-compose. I preferred to do it without
    detached mode to see the logs while the containers are spinning up and then running.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行docker-compose启动容器。我更倾向于不使用分离模式，以便在容器启动并运行时查看日志。
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Check for the logs to see if the services are running properly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 检查日志以查看服务是否正常运行。
- en: 'Step 2: Create the Kafka topics'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：创建Kafka主题
- en: Next, we’re going to create the topics to receive data from the IoT sensors
    and store the alerts filtered by the Flink application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建主题以接收来自IoT传感器的数据，并存储Flink应用程序过滤后的警报。
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: To check if the topics were created correctly you can execute the following
    command
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查主题是否正确创建，可以执行以下命令
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Step 3: Create Postgres table'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：创建Postgres表
- en: Login to the postgres console
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 登录到Postgres控制台
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Enter the password flinkpassword to log into the postgres console, remember
    this is a local configuration so default access has been configured in the docker-compose.yml.
    Then create the table
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输入密码flinkpassword登录到Postgres控制台，请记住这是本地配置，因此默认访问权限已在docker-compose.yml中配置。然后创建表格
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can check if the table is properly created by doing the following
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式检查表是否正确创建
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will show you a result similar to the following one:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示类似以下的结果：
- en: '![](../Images/d60419e34e6e3d6595fb7dabb57d16ff.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d60419e34e6e3d6595fb7dabb57d16ff.png)'
- en: 'Step 4: Launching the Kafka producer'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：启动Kafka生产者
- en: 'Create a local environment with conda or poetry and install python kafka package:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用conda或poetry创建一个本地环境并安装python kafka包：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then execute the kafka producer, which mimics IoT sensor messages and publishes
    messages to the sensors topic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后执行Kafka生产者，它模拟IoT传感器消息并将消息发布到传感器主题。
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Leave it running for the rest of the tutorial.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让它在接下来的教程中一直运行。
- en: 'Step 5: Initializing the Flink task'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：初始化Flink任务
- en: 'We’re going to launch the Flink application from within the container, so you
    can monitor it from the web UI through localhost:8081\. Run the following command
    from the repository root:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从容器内启动Flink应用程序，因此你可以通过Web UI通过localhost:8081监控它。从仓库根目录运行以下命令：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You’ll see some logging information, additionally alerts will also be displayed
    in the flink-jobmanager container logs. Also, you can check if the job is running
    from the Flink web UI [http://localhost:8081/#/job/running](http://localhost:8081/#/job/running).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到一些日志信息，此外，警报也会显示在flink-jobmanager容器的日志中。同时，你可以从Flink Web UI [http://localhost:8081/#/job/running](http://localhost:8081/#/job/running)检查作业是否正在运行。
- en: '![](../Images/61e46067d510dfa78e078de834b3ad0a.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61e46067d510dfa78e078de834b3ad0a.png)'
- en: Details of running job — image by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 运行作业的详细信息——作者提供的图片
- en: Apparently the monitoring tells that there are no messages going through the
    Flink job, which is not true, since alerts can be seen in the docker log.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，监控显示Flink作业中没有消息流动，但这不是真的，因为可以在docker日志中看到警报。
- en: '![](../Images/a76525be2f6b2ec01ef7e1d6d89b2518.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a76525be2f6b2ec01ef7e1d6d89b2518.png)'
- en: We’ll check the messages using the Postgres table and read the alerts topic,
    which were created for this purpose.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过Postgres表检查消息，并读取为此目的创建的警报主题。
- en: 'Step 6: Read Alerts in Kafka Topic'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步：读取Kafka主题中的警报
- en: 'To read data in the alerts topic, you can execute the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要读取警报主题中的数据，可以执行以下命令：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That will bring all the messages that the topic has received so far.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将展示到目前为止该主题接收到的所有消息。
- en: 'Step 7: Read raw data from Postgres table'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：从Postgres表读取原始数据
- en: 'Additionally you can query the raw messages from the IoT sensor and even parse
    the JSON data in PostgreSQL:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以查询 IoT 传感器的原始消息，甚至在 PostgreSQL 中解析 JSON 数据：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 8: Stopping Services'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 步：停止服务
- en: 'You can easily stop everything by doing ctrl-c on the docker terminal. If you
    prefer, to make proper shutdown, proceed with the following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在 Docker 终端按 Ctrl-c 来轻松停止所有服务。如果你更倾向于正确地关闭服务，可以按照以下步骤进行操作：
- en: Cancel the Flink job by clicking in the top right corner of job details in the
    web UI.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Web UI 的作业详情页右上角点击以取消 Flink 作业。
- en: Stop the kafka_producer.py script which was running locally.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止本地运行的 kafka_producer.py 脚本。
- en: Ctrl-c on the docker terminal to stop the services
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Docker 终端按 Ctrl-c 停止服务
- en: The information exchanged in the session, while the services were running, is
    permanently stored. So in the case you want to query the Postgres table or the
    Kafka topics, the data is going to be there.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 会话中交换的信息，在服务运行期间会被永久存储。因此，如果你想查询 Postgres 表或 Kafka 主题，数据将会在其中。
- en: Insights on Using Multiple Sinks in a PyFlink Job
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个 Sink 在 PyFlink 作业中的见解
- en: 'In the Flink job used for demonstration, I’m managing 2 data streams simultaneously,
    in the same task. The one that writes raw data coming from the sensors topic (IoT
    devices) and the filtered alerts which are set to another topic. This has some
    advantages and drawbacks, as a simple summary, here are the pros and cons:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于演示的 Flink 作业中，我同时管理 2 个数据流，它们在同一个任务中运行。一个是写入来自传感器主题（IoT 设备）的原始数据，另一个是过滤后的警报数据，写入另一个主题。这种方式有一些优点和缺点，简单总结如下：
- en: '**Pros of Single Job with Multiple Sinks:**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**单一作业使用多个 Sink 的优点：**'
- en: '- Simplicity in resource management.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '- 资源管理的简便性。'
- en: '- Consistency in data flow.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '- 数据流的一致性。'
- en: '**Cons of Single Job:**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**单一作业的缺点：**'
- en: '- Can become complex as logic grows.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '- 随着逻辑的增长，可能变得复杂。'
- en: '- Scalability might be an issue.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '- 可扩展性可能是个问题。'
- en: '**Pros of Multiple Jobs:**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**多个作业的优点：**'
- en: '- Better fault isolation.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更好的故障隔离。'
- en: '- Focused optimization.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '- 聚焦优化。'
- en: '**Cons of Multiple Jobs:**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**多个作业的缺点：**'
- en: '- Resource overhead.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '- 资源开销。'
- en: '- Coordination complexity.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '- 协调复杂性。'
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This setup offers a robust solution for real-time data streaming and processing,
    integrating Flink, Kafka, and PostgreSQL effectively. The main purpose of using
    Postgres in the loop is to check the raw messages coming from the IoT devices
    without relying on queries to the topic itself. It also helped to demonstrate
    how to sink data using a JDBC connector, which might be pretty standard. The message
    transformations were done using the DataStream API. I would like to dive further
    into the SQL API which introduces a friendlier interface. Finally, regarding how
    to manage data streams, choose between single or multiple jobs based on your specific
    requirements ensuring scalability and maintainability.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 该设置为实时数据流和处理提供了一个强大的解决方案，有效地集成了 Flink、Kafka 和 PostgreSQL。使用 Postgres 的主要目的是检查来自
    IoT 设备的原始消息，而不是依赖于直接查询主题本身。它还展示了如何使用 JDBC 连接器进行数据输出，这可能是一个相对标准的做法。消息转换通过 DataStream
    API 完成。我希望进一步深入学习 SQL API，它提供了一个更加友好的接口。最后，关于如何管理数据流，建议根据具体需求选择单一作业或多个作业，确保可扩展性和可维护性。
- en: Next Steps
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下一步
- en: 1\. Use SQL API to make transformations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 使用 SQL API 进行数据转换。
- en: 2\. Optimize resource usage based on job complexity.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 根据作业复杂性优化资源使用。
- en: 3\. Explore advanced Flink features for complex data processing tasks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 探索 Flink 的高级功能以应对复杂的数据处理任务。
- en: Happy streaming! 🚀
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 快乐流式处理！🚀
- en: '**Stay tuned for more tutorials on integrating and scaling data engineering
    solutions with Docker!**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**敬请期待更多关于如何通过 Docker 集成和扩展数据工程解决方案的教程！**'
- en: '*Feel free to reach out for any questions or suggestions in the comments below!*'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*如有任何问题或建议，请随时在下方评论区与我们联系！*'
- en: Ready to Optimize Your Streaming Data Applications?
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好优化你的流数据应用了吗？
- en: Unlock the full potential of your data with our [expert consulting services](https://www.squadralabs.com/),
    tailored for streaming data applications. Whether you’re looking to enhance real-time
    analytics, streamline data pipelines, or optimize performance, we’re here to help.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 解锁数据的全部潜力，使用我们的[专家咨询服务](https://www.squadralabs.com)，为流数据应用量身定制。无论你是想增强实时分析、优化数据管道，还是提升性能，我们都能为你提供帮助。
- en: References
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/](https://www.confluent.io/blog/kafka-client-cannot-connect-to-broker-on-aws-on-docker-etc/)'
- en: '[https://mvnrepository.com/](https://mvnrepository.com/)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://mvnrepository.com/](https://mvnrepository.com/)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/](https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/jdbc/)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/overview/#session-mode)'
- en: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker](https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/resource-providers/standalone/docker/#using-flink-python-on-docker)'
- en: '[https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580](https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580](https://medium.com/@sant1/flink-docker-kafka-faee9c0f1580)'
