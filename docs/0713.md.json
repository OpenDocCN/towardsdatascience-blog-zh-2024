["```py\ntensorflow[and-cuda]==2.15.0.post1 # if you want to use GPU or\ntensorflow==2.15.0.post1 # if you want to only use CPU\ntransformers==4.36.2 # for using the bert tokenizer\neinops==0.7.0 # useful to make matrix manipulation faster\ndatasets==2.16.1 # to load datasets\n# all other modules (like numpy) will be auto installed\n```", "```py\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, Model\n\nfrom dataclasses import dataclass\nfrom einops import rearrange, repeat\nfrom typing import Union\n\nfrom transformers import AutoTokenizer\n\nimport datasets\nimport math\nimport numpy as np\n```", "```py\n@dataclass\nclass ModelArgs:\n    model_input_dims: int = 64\n    model_states: int = 64\n    projection_expand_factor: int = 2\n    conv_kernel_size: int = 4\n    delta_t_min: float = 0.001\n    delta_t_max: float = 0.1\n    delta_t_scale: float = 0.1\n    delta_t_init_floor: float = 1e-4\n    conv_use_bias: bool = True\n    dense_use_bias: bool = False\n    layer_id: int = -1\n    seq_length: int = 128\n    num_layers: int = 5\n    dropout_rate: float = 0.2\n    use_lm_head: float = False\n    num_classes: int = None\n    vocab_size: int = None\n    final_activation = None\n    loss:Union[str, keras.losses.Loss] = None\n    optimizer: Union[str, keras.optimizers.Optimizer] = keras.optimizers.AdamW()\n    metrics = ['accuracy']\n\n    def __post_init__(self):\n        self.model_internal_dim: int = int(self.projection_expand_factor * self.model_input_dims)\n\n        self.delta_t_rank = math.ceil(self.model_input_dims/16)\n        if self.layer_id == -1:\n            self.layer_id = np.round(np.random.randint(0, 1000), 4)\n\n        if self.vocab_size == None:\n            raise ValueError(\"vocab size cannot be none\")\n\n        if self.use_lm_head:\n            self.num_classes=self.vocab_size\n        else:\n            if self.num_classes == None:\n                raise ValueError(f'num classes cannot be {self.num_classes}')\n\n            if self.num_classes == 1:\n                self.final_activation = 'sigmoid'\n            else:\n                self.final_activation = 'softmax'\n\n        if self.loss == None:\n            raise ValueError(f\"loss cannot be {self.loss}\")\n```", "```py\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nvocab_size = tokenizer.vocab_size\n```", "```py\ndef selective_scan(u, delta, A, B, C, D):\n    # first step of A_bar = exp(ΔA), i.e., ΔA\n    dA = tf.einsum('bld,dn->bldn', delta, A) \n    dB_u = tf.einsum('bld,bld,bln->bldn', delta, u, B)\n\n    dA_cumsum = tf.pad(\n        dA[:, 1:], [[0, 0], [1, 1], [0, 0], [0, 0]])[:, 1:, :, :]\n\n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip along axis 1\n\n    # Cumulative sum along all the input tokens, parallel prefix sum, \n    # calculates dA for all the input tokens parallely\n    dA_cumsum = tf.math.cumsum(dA_cumsum, axis=1)  \n\n    # second step of A_bar = exp(ΔA), i.e., exp(ΔA)\n    dA_cumsum = tf.exp(dA_cumsum)  \n    dA_cumsum = tf.reverse(dA_cumsum, axis=[1])  # Flip back along axis 1\n\n    x = dB_u * dA_cumsum\n    # 1e-12 to avoid division by 0\n    x = tf.math.cumsum(x, axis=1)/(dA_cumsum + 1e-12) \n\n    y = tf.einsum('bldn,bln->bld', x, C)\n\n    return y + u * D \n```", "```py\nclass MambaBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        args = modelargs\n        self.layer_id = modelargs.layer_id\n\n        self.in_projection = layers.Dense(\n            args.model_internal_dim * 2, \n            input_shape=(args.model_input_dims,), use_bias=False)\n\n        self.conv1d = layers.Conv1D(\n            filters=args.model_internal_dim,\n            use_bias=args.conv_use_bias,\n            kernel_size=args.conv_kernel_size,\n            groups=args.model_internal_dim,\n            data_format='channels_first',\n            padding='causal'\n        )\n\n        # this layer takes in current token 'x' \n        # and outputs the input-specific Δ, B, C (according to S6)\n        self.x_projection = layers.Dense(args.delta_t_rank + args.model_states * 2, use_bias=False)\n\n        # this layer projects Δ from delta_t_rank to the mamba internal \n        # dimension\n        self.delta_t_projection = layers.Dense(args.model_internal_dim, \n                                               input_shape=(args.delta_t_rank,), use_bias=True)\n\n        self.A = repeat(\n                tf.range(1, args.model_states+1, dtype=tf.float32), \n                'n -> d n', d=args.model_internal_dim)\n\n        self.A_log = tf.Variable(\n                tf.math.log(self.A), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_A_log_{args.layer_id}\")\n\n        self.D = tf.Variable(\n                np.ones(args.model_internal_dim), \n                trainable=True, dtype=tf.float32, \n                name=f\"SSM_D_{args.layer_id}\")\n\n        self.out_projection = layers.Dense(\n                args.model_input_dims, \n                input_shape=(args.model_internal_dim,), \n                use_bias=args.dense_use_bias)\n\n    def call(self, x):\n        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba pape.\n        Official Implementation:\n            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n\n        (batch_size, seq_len, dimension) = x.shape\n\n        x_and_res = self.in_projection(x) # shape = (batch, seq_len, 2 * model_internal_dimension)\n        (x, res) = tf.split(x_and_res, \n                            [self.args.model_internal_dim, \n                             self.args.model_internal_dim], axis=-1)\n\n        x = rearrange(x, 'b l d_in -> b d_in l')\n        x = self.conv1d(x)[:, :, :seq_len]\n        x = rearrange(x, 'b d_in l -> b l d_in')\n\n        x = tf.nn.swish(x)\n        y = self.ssm(x)\n        y = y * tf.nn.swish(res)\n        return self.out_projection(y)\n\n    def ssm(self, x):\n        \"\"\"Runs the SSM. See:\n            - Algorithm 2 in Section 3.2 in the Mamba paper\n            - run_SSM(A, B, C, u) in The Annotated S4\n            Official Implementation:\n            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n        \"\"\"\n        (d_in, n) = self.A_log.shape\n\n        # Compute ∆ A B C D, the state space parameters.\n        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n        #                                  and is why Mamba is called **selective** state spaces)\n\n        A = -tf.exp(tf.cast(self.A_log, tf.float32)) # shape -> (d_in, n)\n        D = tf.cast(self.D, tf.float32)\n\n        x_dbl = self.x_projection(x) # shape -> (batch, seq_len, delta_t_rank + 2*n)\n\n        (delta, B, C) = tf.split(\n                x_dbl, \n                num_or_size_splits=[self.args.delta_t_rank, n, n], \n                axis=-1) # delta.shape -> (batch, seq_len) & B, C shape -> (batch, seq_len, n)\n\n        delta = tf.nn.softplus(self.delta_t_projection(delta)) # shape -> (batch, seq_len, model_input_dim)\n\n        return selective_scan(x, delta, A, B, C, D)\n```", "```py\nclass ResidualBlock(layers.Layer):\n    def __init__(self, modelargs: ModelArgs, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.args = modelargs\n        self.mixer = MambaBlock(modelargs)\n        self.norm = layers.LayerNormalization(epsilon=1e-5)\n\n    def call(self, x):\n        \"\"\"\n        Official Implementation:\n            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n\n            Note: the official repo chains residual blocks that look like\n                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n            where the first Add is a no-op. This is purely for performance reasons as this\n            allows them to fuse the Add->Norm.\n\n            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n\n        \"\"\"\n        return self.mixer(self.norm(x)) + x\n```", "```py\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\ndataset = load_dataset(\"ajaykarthick/imdb-movie-reviews\")\n```", "```py\ndef init_model(args: ModelArgs):\n    input_layer = layers.Input(shape=(args.seq_length,), name='input_ids')\n    x = layers.Embedding(\n                args.vocab_size, \n                args.model_input_dims, \n                input_length=args.seq_length)(input_layer)\n\n    for i in range(args.num_layers):\n        x = ResidualBlock(args, name=f\"Residual_{i}\")(x)\n        x = layers.Dropout(args.dropout_rate)(x) # for regularization\n\n    x = layers.LayerNormalization(epsilon=1e-5)(x) # normalization layer\n\n    # use flatten only if we are not using the model as an LM\n    if not args.use_lm_head: \n        x = layers.Flatten()(x)\n    x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n    output_layer = layers.Dense(\n                args.num_classes, \n                activation=args.final_activation)(x)\n\n    model = Model(\n                inputs=input_layer, \n                outputs=output_layer, name='Mamba_ka_Mamba')\n    model.compile(\n        loss=args.loss,\n        optimizer=args.optimizer,\n        metrics=args.metrics\n    )\n\n    return model\n```", "```py\nargs = ModelArgs(\n    model_input_dims=128,\n    model_states=32,\n    num_layers=12,\n    dropout_rate=0.2,\n    vocab_size=vocab_size,\n    num_classes=1,\n    loss='binary_crossentropy',\n)\nmodel = init_model(args)\nmodel.summary()\n```", "```py\nModel: \"Mamba_ka_Mamba\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_ids (InputLayer)      [(None, 128)]             0         \n\n embedding_2 (Embedding)     (None, 128, 128)          3906816   \n\n Residual_0 (ResidualBlock)  (None, 128, 128)          129024    \n\n dropout_24 (Dropout)        (None, 128, 128)          0         \n\n Residual_1 (ResidualBlock)  (None, 128, 128)          129024    \n\n dropout_25 (Dropout)        (None, 128, 128)          0\n\n ... (I have shrinked this to make it more readable)\n\n dropout_35 (Dropout)        (None, 128, 128)          0         \n\n layer_normalization_38 (La  (None, 128, 128)          256       \n yerNormalization)                                               \n\n flatten_2 (Flatten)         (None, 16384)             0         \n\n dense_148 (Dense)           (None, 1024)              16778240  \n\n dense_149 (Dense)           (None, 1)                 1025      \n\n=================================================================\nTotal params: 22234625 (84.82 MB)\nTrainable params: 22234625 (84.82 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n```", "```py\ntrain_labels, test_labels = [], []\ntrain_ids = np.zeros((len(dataset['train']), args.seq_length))\ntest_ids = np.zeros((len(dataset['test']), args.seq_length))\n\nfor i, item in enumerate(tqdm(dataset['train'])):\n    text = item['review']\n    train_ids[i, :] = tokenizer.encode_plus(\n            text, \n            max_length=args.seq_length, \n            padding='max_length', \n            return_tensors='np')['input_ids'][0][:args.seq_length]\n\n    train_labels.append(item['label'])\n\nfor i, item in enumerate(tqdm(dataset['test'])):\n    text = item['review']\n    test_ids[i, :] = tokenizer.encode_plus(\n            text, \n            max_length=args.seq_length, \n            padding='max_length', \n            return_tensors='np')['input_ids'][0][:args.seq_length]\n\n    test_labels.append(item['label'])\n\ndel dataset # delete the original dataset to save some memory\n\nBATCH_SIZE = 32\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_ids, train_labels)).batch(BATCH_SIZE).shuffle(1000)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_ids, test_labels)).batch(BATCH_SIZE).shuffle(1000)\n```", "```py\nhistory = model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n```", "```py\ndef infer(text: str, model: Model, tokenizer):\n    tokens = tokenizer.encode(\n            \"Hello what is up\", \n            max_length=args.seq_length, \n            padding='max_length', return_tensors='np')\n    output = model(tokens)[0, 0]\n    return output\n```"]