["```py\nfrom dataclasses import dataclass\n\n@dataclass\nclass ModelConfig:\n  vocab_size: int = 50257\n  n_head: int = 12\n  n_embd: int = 768\n  block_size: int = 1024\n  n_layer: int = 12\n  dropout_rate: float = 0.1\n```", "```py\nfrom flax import linen as nn\nimport jax.numpy as jnp\n\nclass CausalSelfAttention(nn.Module):\n\n  config: ModelConfig\n\n  @nn.compact\n  def __call__(self, x, deterministic=True):\n\n    assert len(x.shape) == 3\n\n    b, l, d = x.shape\n\n    q     = nn.Dense(self.config.n_embd)(x)\n    k     = nn.Dense(self.config.n_embd)(x)\n    v     = nn.Dense(self.config.n_embd)(x)\n    # q*k / sqrt(dim) -> softmax -> @v\n    q     = jnp.reshape(q, (b, l, d//self.config.n_head , self.config.n_head))\n    k     = jnp.reshape(k, (b, l, d//self.config.n_head , self.config.n_head))\n    v     = jnp.reshape(v, (b, l, d//self.config.n_head , self.config.n_head))\n    norm  = jnp.sqrt(list(jnp.shape(k))[-1])\n    attn  = jnp.matmul(q,jnp.transpose(k, (0,1,3,2))) / norm\n    mask  = jnp.tril(attn)\n    attn  = jnp.where(mask[:,:,:l,:l], attn, float(\"-inf\"))\n    probs = jax.nn.softmax(attn, axis=-1)\n    y     = jnp.matmul(probs, v)\n    y     = jnp.reshape(y, (b,l,d))\n    y     = nn.Dense(self.config.n_embd)(y)\n    return y\n```", "```py\nclass MLP(nn.Module):\n\n  config: ModelConfig\n\n  @nn.compact\n  def __call__(self, x, deterministic=True):\n    x = nn.Dense(self.config.n_embd*4)(x)\n    x = nn.gelu(x, approximate=True)\n    x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=deterministic)\n    x = nn.Dense(self.config.n_embd)(x)\n    x = nn.Dropout(rate=self.config.dropout_rate)(x, deterministic=deterministic)\n    return x\n\nclass Block(nn.Module):\n\n  config: ModelConfig\n\n  @nn.compact\n  def __call__(self, x):\n    x = nn.LayerNorm()(x)\n    x = x + CausalSelfAttention(self.config)(x)\n    x = nn.LayerNorm()(x)\n    x = x + MLP(self.config)(x)\n    return x\n```", "```py\nclass GPT(nn.Module):\n\n  config: ModelConfig\n\n  @nn.compact\n  def __call__(self, x, deterministic=False):\n\n    B, T = x.shape\n    assert T <= self.config.block_size\n\n    pos     = jnp.arange(0, T)[None]\n    pos_emb = nn.Embed(self.config.block_size, self.config.n_embd)(pos)\n    wte     = nn.Embed(self.config.vocab_size, self.config.n_embd)\n    tok_emb = wte(x)\n    x       = tok_emb + pos_emb\n\n    for _ in range(self.config.n_layer):\n      x = Block(self.config)(x)\n    x = nn.LayerNorm()(x)\n    logits = nn.Dense(config.n_embd, config.vocab_size)(x)\n    # logits = wte.attend(x) # parameter sharing\n    return logits\n\n  def init(self, rng):\n    tokens = jnp.zeros((1, self.config.block_size), dtype=jnp.uint16)\n    params = jax.jit(super().init, static_argnums=(2,))(rng, tokens, True)\n    return params \n```", "```py\nclass DataLoader:\n  def __init__(self, B, T):\n    self.current_position = 0\n    self.B = B\n    self.T = T\n\n    with open(\"input.txt\",\"r\") as f:\n      text = f.read()\n    enc = tiktoken.get_encoding(\"gpt2\")\n    self.tokens = jnp.array(enc.encode(text))\n    print(f\"loaded {len(self.tokens)} tokens in the datasets\" )\n    print(f\" 1 epoch = {len(self.tokens)//(B*T)} batches\")\n\n  def next_batch(self):\n    B,T = self.B, self.T\n    buf = self.tokens[self.current_position:self.current_position+B*T+1]\n    x,y = jnp.reshape(buf[:-1],(B,T)), jnp.reshape(buf[1:],(B,T))\n    self.current_position += B*T\n    if self.current_position + B*T+1 > len(self.tokens):\n      self.current_position = 0\n    return x,y\n```", "```py\ndef init_train_state(key, config) -> TrainState:\n  model = GPT(config)\n  params = model.init(key)\n  optimizer = optax.adamw(3e-4, b1=0.9, b2=0.98, eps=1e-9, weight_decay=1e-1)\n  train_state = TrainState.create(\n        apply_fn=model.apply,\n        params=params,\n        tx=optimizer)\n  return train_state\n\n@jax.jit\ndef train_step(state: TrainState, x: jnp.ndarray, y: jnp.ndarray) -> Tuple[jnp.ndarray, TrainState]:\n\n  def loss_fn(params: FrozenDict) -> jnp.ndarray:\n\n      logits = state.apply_fn(params, x, False)\n      loss = optax.softmax_cross_entropy_with_integer_labels(logits, y).mean()\n      return loss\n\n  loss, grads = jax.value_and_grad(loss_fn, has_aux=False)(state.params)\n  new_state = state.apply_gradients(grads=grads)\n  return loss, new_state\n```"]