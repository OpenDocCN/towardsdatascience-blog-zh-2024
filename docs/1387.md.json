["```py\npython make_splits --output-dir gs://your-bucket/\n```", "```py\nimport pandas as pd\nimport argparse\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef make_splits(output_dir):\n    df=pd.concat([\n        pd.read_csv(\"data/farisdurrani/twitter_filtered.csv\"),\n        pd.read_csv(\"data/farisdurrani/facebook_filtered.csv\")\n    ])\n    df = df.dropna(subset=['sentiment'], axis=0)\n    df['Target'] = df['sentiment'].apply(lambda x: 1 if x==0 else np.sign(x)+1).astype(int)\n\n    df_train, df_ = train_test_split(df, stratify=df['Target'], test_size=0.2)\n    df_eval, df_test = train_test_split(df_, stratify=df_['Target'], test_size=0.5)\n\n    print(f\"Files will be saved in {output_dir}\")\n    df_train.to_csv(output_dir + \"/train.csv\", index=False)\n    df_eval.to_csv(output_dir + \"/eval.csv\", index=False)\n    df_test.to_csv(output_dir + \"/test.csv\", index=False)\n\n    print(f\"Train : ({df_train.shape}) samples\")\n    print(f\"Val : ({df_eval.shape}) samples\")\n    print(f\"Test : ({df_test.shape}) samples\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--output-dir')\n    args, _ = parser.parse_known_args()\n    make_splits(args.output_dir)\n```", "```py\nimport pandas as pd\nimport argparse\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport logging\nimport os\nos.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"\n\ndef train_and_evaluate(**params):\n    pass\n    # will be updated as we go\n```", "```py\ntransformers==4.40.1\ntorch==2.2.2\npandas==2.0.3\nscikit-learn==1.3.2\ngcsfs\n```", "```py\n # Load pretrained tokenizers and bert model\ntokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\nmodel = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n```", "```py\nclass SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n        # Uncomment the below if you only want to retrain certain layers.\n        # self.bert_module.requires_grad_(False)\n        # for param in self.bert_module.encoder.parameters():\n        #     param.requires_grad = True\n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        # print(ids.size(), mask.size(), token_type_ids.size())\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n```", "```py\nclass BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n```", "```py\ndef train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n```", "```py\nimport pandas as pd\nimport time\nimport torch.nn as nn\nimport torch\nimport logging\nimport numpy as np\nimport argparse\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertModel\n\nlogging.basicConfig(format='%(asctime)s [%(levelname)s]: %(message)s', level=logging.DEBUG)\nlogging.getLogger().setLevel(logging.INFO)\n\n# --- CONSTANTS ---\nBERT_MODEL_NAME = 'small_bert/bert_en_uncased_L-2_H-128_A-2'\n\nif torch.cuda.is_available():\n    logging.info(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n    DEVICE = torch.device('cuda')\nelse:\n    logging.info(\"No GPU available. Training will run on CPU.\")\n    DEVICE = torch.device('cpu')\n\n# --- Data preparation and tokenization ---\nclass BertDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length=100):\n        super(BertDataset, self).__init__()\n        self.df=df\n        self.tokenizer=tokenizer\n        self.target=self.df['Target']\n        self.max_length=max_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n\n        X = self.df['bodyText'].values[idx]\n        y = self.target.values[idx]\n\n        inputs = self.tokenizer.encode_plus(\n            X,\n            pad_to_max_length=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        x = {\n            'ids': torch.tensor(ids, dtype=torch.long).to(DEVICE),\n            'mask': torch.tensor(mask, dtype=torch.long).to(DEVICE),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(DEVICE)\n            }\n        y = torch.tensor(y, dtype=torch.long).to(DEVICE)\n\n        return x, y\n\n# --- Model definition ---\nclass SentimentBERT(nn.Module):\n    def __init__(self, bert_model):\n        super().__init__()\n        self.bert_module = bert_model\n        self.dropout = nn.Dropout(0.1)\n        self.final = nn.Linear(in_features=128, out_features=3, bias=True) \n\n    def forward(self, inputs):\n        ids, mask, token_type_ids = inputs['ids'], inputs['mask'], inputs['token_type_ids']\n        x = self.bert_module(ids, mask, token_type_ids)\n        x = self.dropout(x['pooler_output'])\n        out = self.final(x)\n        return out\n\n# --- Training loop ---\ndef train(epoch, model, dataloader, loss_fn, optimizer, max_steps=None):\n    model.train()\n    total_acc, total_count = 0, 0\n    log_interval = 50\n    start_time = time.time()\n\n    for idx, (inputs, label) in enumerate(dataloader):\n        optimizer.zero_grad()\n        predicted_label = model(inputs)\n\n        loss = loss_fn(predicted_label, label)\n        loss.backward()\n        optimizer.step()\n\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n\n        if idx % log_interval == 0:\n            elapsed = time.time() - start_time\n            print(\n                \"Epoch {:3d} | {:5d}/{:5d} batches \"\n                \"| accuracy {:8.3f} | loss {:8.3f} ({:.3f}s)\".format(\n                    epoch, idx, len(dataloader), total_acc / total_count, loss.item(), elapsed\n                )\n            )\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\n        if max_steps is not None:\n            if idx == max_steps:\n                return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Validation loop ---\ndef evaluate(model, dataloader, loss_fn):\n    model.eval()\n    total_acc, total_count = 0, 0\n\n    with torch.no_grad():\n        for idx, (inputs, label) in enumerate(dataloader):\n            predicted_label = model(inputs)\n            loss = loss_fn(predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n\n    return {'loss': loss.item(), 'acc': total_acc / total_count}\n\n# --- Main function ---\ndef train_and_evaluate(**params):\n\n    logging.info(\"running with the following params :\")\n    logging.info(params)\n\n    # Load pretrained tokenizers and bert model\n    # update the paths to whichever you are using\n    tokenizer = BertTokenizer.from_pretrained('models/bert_uncased_L-2_H-128_A-2/vocab.txt')\n    model = BertModel.from_pretrained('models/bert_uncased_L-2_H-128_A-2')\n\n    # Training parameters\n    epochs = int(params.get('epochs'))\n    batch_size = int(params.get('batch_size'))\n    learning_rate = float(params.get('learning_rate'))\n\n    #  Load the data\n    df_train = pd.read_csv(params.get('training_file'))\n    df_eval = pd.read_csv(params.get('validation_file'))\n    df_test = pd.read_csv(params.get('testing_file'))\n\n    # Create dataloaders\n    train_ds = BertDataset(df_train, tokenizer, max_length=100)\n    train_loader = DataLoader(dataset=train_ds,batch_size=batch_size, shuffle=True)\n    eval_ds = BertDataset(df_eval, tokenizer, max_length=100)\n    eval_loader = DataLoader(dataset=eval_ds,batch_size=batch_size)\n    test_ds = BertDataset(df_test, tokenizer, max_length=100)\n    test_loader = DataLoader(dataset=test_ds,batch_size=batch_size)\n\n    # Create the model\n    classifier = SentimentBERT(bert_model=model).to(DEVICE)\n    total_parameters = sum([np.prod(p.size()) for p in classifier.parameters()])\n    model_parameters = filter(lambda p: p.requires_grad, classifier.parameters())\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    logging.info(f\"Total params : {total_parameters} - Trainable : {params} ({params/total_parameters*100}% of total)\")\n\n    # Optimizer and loss functions\n    optimizer = torch.optim.Adam([p for p in classifier.parameters() if p.requires_grad], learning_rate)\n    loss_fn = nn.CrossEntropyLoss()\n\n    # If dry run we only\n    logging.info(f'Training model with {BERT_MODEL_NAME}')\n    if args.dry_run:\n        logging.info(\"Dry run mode\")\n        epochs = 1\n        steps_per_epoch = 1\n    else:\n        steps_per_epoch = None\n\n    # Action !\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train_metrics = train(epoch, classifier, train_loader, loss_fn=loss_fn, optimizer=optimizer, max_steps=steps_per_epoch)\n        eval_metrics = evaluate(classifier, eval_loader, loss_fn=loss_fn)\n\n        print(\"-\" * 59)\n        print(\n            \"End of epoch {:3d} - time: {:5.2f}s - loss: {:.4f} - accuracy: {:.4f} - valid_loss: {:.4f} - valid accuracy {:.4f} \".format(\n                epoch, time.time() - epoch_start_time, train_metrics['loss'], train_metrics['acc'], eval_metrics['loss'], eval_metrics['acc']\n            )\n        )\n        print(\"-\" * 59)\n\n    if args.dry_run:\n        # If dry run, we do not run the evaluation\n        return None\n\n    test_metrics = evaluate(classifier, test_loader, loss_fn=loss_fn)\n\n    metrics = {\n        'train': train_metrics,\n        'val': eval_metrics,\n        'test': test_metrics,\n    }\n    logging.info(metrics)\n\n    # save model and architecture to single file\n    if params.get('job_dir') is None:\n        logging.warning(\"No job dir provided, model will not be saved\")\n    else:\n        logging.info(\"Saving model to {} \".format(params.get('job_dir')))\n        torch.save(classifier.state_dict(), params.get('job_dir'))\n    logging.info(\"Bye bye\")\n\nif __name__ == '__main__':\n    # Create arguments here\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--training-file', required=True, type=str)\n    parser.add_argument('--validation-file', required=True, type=str)\n    parser.add_argument('--testing-file', type=str)\n    parser.add_argument('--job-dir', type=str)\n    parser.add_argument('--epochs', type=float, default=2)\n    parser.add_argument('--batch-size', type=float, default=1024)\n    parser.add_argument('--learning-rate', type=float, default=0.01)\n    parser.add_argument('--dry-run', action=\"store_true\")\n\n    # Parse them\n    args, _ = parser.parse_known_args()\n\n    # Execute training\n    train_and_evaluate(**vars(args))\n```", "```py\nFROM pytorch/pytorch:2.2.2-cuda11.8-cudnn8-runtime\n\nWORKDIR /src\nCOPY . .\nRUN pip install --upgrade pip && pip install -r requirements.txt\n\nENTRYPOINT [\"python\", \"main.py\"]\n```", "```py\n# build.sh\n\nexport PROJECT_ID=<your-project-id>\nexport IMAGE_REPO_NAME=pt_bert_sentiment\nexport IMAGE_TAG=dev\nexport IMAGE_URI=eu.gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n\ngcloud builds submit --tag $IMAGE_URI .\n```", "```py\n# job.sh\n\nexport PROJECT_ID=<your-project-id>\nexport BUCKET=<your-bucket-id>\nexport REGION=\"europe-west4\"\nexport SERVICE_ACCOUNT=<your-service-account>\nexport JOB_NAME=\"pytorch_bert_training\"\nexport MACHINE_TYPE=\"n1-standard-4\"  # We can specify GPUs here\nexport ACCELERATOR_TYPE=\"NVIDIA_TESLA_T4\"\nexport IMAGE_URI=\"eu.gcr.io/$PROJECT_ID/pt_bert_sentiment:dev\"\n\ngcloud ai custom-jobs create \\\n--region=$REGION \\\n--display-name=$JOB_NAME \\\n--worker-pool-spec=machine-type=$MACHINE_TYPE,accelerator-type=$ACCELERATOR_TYPE,accelerator-count=1,replica-count=1,container-image-uri=$IMAGE_URI \\\n--service-account=$SERVICE_ACCOUNT \\\n--args=\\\n--training-file=gs://$BUCKET/data/train.csv,\\\n--validation-file=gs://$BUCKET/data/eval.csv,\\\n--testing-file=gs://$BUCKET/data/test.csv,\\\n--job-dir=gs://$BUCKET/model/model.pt,\\\n--epochs=10,\\\n--batch-size=128,\\\n--learning-rate=0.0001\n```"]