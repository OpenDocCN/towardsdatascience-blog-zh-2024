["```py\n# File location and type\nfile_location = \"/FileStore/tables/household_power_consumption.csv\"\nfile_type = \"csv\"\n\n# CSV options\nschema = \"Date STRING, Time STRING, Global_active_power DOUBLE, Global_reactive_power DOUBLE, Voltage DOUBLE, Global_intensity DOUBLE, Sub_metering_1 DOUBLE, Sub_metering_2 DOUBLE, Sub_metering_3 DOUBLE\"\nfirst_row_as_header = \"true\"\ndelimiter = \";\"\n\n# Read CSV files\norg_df = spark.read.format(file_type) \\\n.schema(schema) \\\n.option(\"header\", first_row_as_header) \\\n.option(\"delimiter\", delimiter) \\\n.load(file_location)\n\ndisplay(org_df)\n```", "```py\nsourceStream=spark.readStream.format(\"csv\") \\\n.option(\"header\",True) \\\n.schema(schema) \\\n.option(\"mode\",\"dropMalformed\") \\\n.option(\"maxFilesPerTrigger\",1) \\\n.option(\"ignoreLeadingWhiteSpace\",True) \\\n.load(\"dbfs:/FileStore/tables/stream\") \\\n```", "```py\nimport time\n\n# Stream the content of the DataFrame\nquery = sourceStream.writeStream \\\n.queryName(\"count\") \\\n.format(\"memory\") \\\n.outputMode(\"append\") \\\n.start()\n\n# Display the count of rows\nfor _ in range(10):\n  spark.sql(\"SELECT COUNT(*) AS no_of_rows FROM count\").show()\n  time.sleep(10)\n```", "```py\nfrom pyspark.sql.functions import col, concat_ws, to_date\n\n# Drop rows with missing values\ndf = org_df.na.drop()\n\n# Convert columns \"Date\" and \"Time\" into new column \"DateTime\"\ndf = df.withColumn(\"Date\", to_date(col(\"Date\"),\"d/M/y\"))\ndf = df.withColumn(\"Date\", df[\"Date\"].cast(\"date\"))\ndf = df.select(concat_ws(\" \", to_date(col(\"Date\"),\"d/M/y\"), col(\"Time\")).alias(\"DateTime\"), \"*\")\ndf = df.withColumn(\"DateTime\", df[\"DateTime\"].cast(\"timestamp\"))\n\n# Add time-related features\ndf = df.withColumn(\"year\", year(\"DateTime\"))\ndf = df.withColumn(\"month\", month(\"DateTime\"))\ndf = df.withColumn(\"week_num\", weekofyear(\"DateTime\"))\ndf = df.withColumn(\"hour\", hour(\"DateTime\"))\n```", "```py\ndf.select(“DateTime”, “Global_active_power”, “Global_intensity”).sort(“Global_active_power”, ascending=False).show(5)\n```", "```py\ndf.filter(\n    (col(\"year\") == 2009) &\n    (col(\"Global_intensity\") > 40)\n).count()\n\n# Output: 10\n```", "```py\ndf.groupby(\"month\").agg(\n     round(mean(\"Global_active_power\"), 2).alias(\"Avg_global_active_power\"),\n     round(mean(\"Sub_metering_1\"), 2).alias(\"Avg_sub_metering_1\"),\n     round(mean(\"Sub_metering_2\"), 2).alias(\"Avg_sub_metering_2\"),\n     round(mean(\"Sub_metering_3\"), 2).alias(\"Avg_sub_metering_3\"),\n).sort([\"month\"]).show(5)\n```", "```py\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag, round\n\n# Create a Window specification based on the 'Date' column\nwindowSpec = Window.orderBy(\"Date\")\n\n# Calculate the lagged value of 'Total_global_active_power'\ndf2 = df2.withColumn(\"power_lag1\", round(lag(col(\"Total_global_active_power\"), 1).over(windowSpec), 2))\n\ndisplay(df2)\n```", "```py\n# Calculate the difference between columns\ndf2 = df2.withColumn(\"power_lag1_delta\", round(col(\"power_lag1\") - col(\"Total_global_active_power\"), 2))\n\ndisplay(df2)\n```", "```py\n# Add window average fields to the DataFrame for the specified window sizes\ndef add_window_avg_features(df, window_sizes):\n    for window_size in window_sizes:\n        window_col_name = f\"avg_power_l{window_size}\"\n        windowSpec = Window.orderBy(\"Date\").rowsBetween(-window_size, 0)\n        df = df.withColumn(window_col_name, round(avg(col(\"Total_global_active_power\")).over(windowSpec), 2))\n    return df\n\nwindow_sizes = [14, 30]\ndf2 = add_window_avg_features(df2, window_sizes)\n\ndf2.select(\"Date\", \"Total_global_active_power\", \"avg_power_l14\", \"avg_power_l30\").sort(\"Date\", ascending=False).show(5)\n```", "```py\nimport pyspark.pandas as ps\n\n#  Add EWMA features to the DataFrame for the specified alpha values\ndef add_ewma_features(df, alphas):\n    for alpha in alphas:\n        ewma_col_name = f\"ewma_power_w{str(alpha).replace('.', '')}\"\n        windowSpec = Window.orderBy(\"Date\")\n        df[ewma_col_name] = df.Total_global_active_power.ewm(alpha=alpha).mean().round(2)\n    return df\n\nalphas = [0.2, 0.8]\n# Convert into a pandas-on-Spark DataFrame, to use EWM function\ndf2_pd = df2.pandas_api()\ndf2_pd = add_ewma_features(df2_pd, alphas)\n# Convert back to a Spark DataFrame\ndf2 = df2_pd.to_spark()\n\ndf2.select(\"Date\", \"Total_global_active_power\", \"ewma_power_w02\", \"ewma_power_w08\").sort(\"Date\", ascending=False).show(5)\n```"]