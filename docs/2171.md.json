["```py\nfrom weaviate.util import generate_uuid5\nimport weaviate\nimport json\nimport pandas as pd\n\n#Read in the pubmed data\ndf = pd.read_csv(\"PubMed Multi Label Text Classification Dataset Processed.csv\")\n```", "```py\nclient = weaviate.Client(\n    url = \"XXX\",  # Replace with your Weaviate endpoint\n    auth_client_secret=weaviate.auth.AuthApiKey(api_key=\"XXX\"),  # Replace with your Weaviate instance API key\n    additional_headers = {\n        \"X-OpenAI-Api-Key\": \"XXX\"  # Replace with your inference API key\n    }\n)\n```", "```py\nclass_obj = {\n    # Class definition\n    \"class\": \"articles\",\n\n    # Property definitions\n    \"properties\": [\n        {\n            \"name\": \"title\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstractText\",\n            \"dataType\": [\"text\"],\n        },\n    ],\n\n    # Specify a vectorizer\n    \"vectorizer\": \"text2vec-openai\",\n\n    # Module settings\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n            \"vectorizeClassName\": True,\n            \"model\": \"ada\",\n            \"modelVersion\": \"002\",\n            \"type\": \"text\"\n        },\n        \"qna-openai\": {\n          \"model\": \"gpt-3.5-turbo-instruct\"\n        },\n        \"generative-openai\": {\n          \"model\": \"gpt-3.5-turbo\"\n        }\n    },\n}\n```", "```py\nclient.schema.create_class(class_obj)\n```", "```py\nimport logging\nimport numpy as np\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n\n# Replace infinity values with NaN and then fill NaN values\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.fillna('', inplace=True)\n\n# Convert columns to string type\ndf['Title'] = df['Title'].astype(str)\ndf['abstractText'] = df['abstractText'].astype(str)\n\n# Log the data types\nlogging.info(f\"Title column type: {df['Title'].dtype}\")\nlogging.info(f\"abstractText column type: {df['abstractText'].dtype}\")\n\nwith client.batch(\n    batch_size=10,  # Specify batch size\n    num_workers=2,   # Parallelize the process\n) as batch:\n    for index, row in df.iterrows():\n        try:\n            question_object = {\n                \"title\": row.Title,\n                \"abstractText\": row.abstractText,\n            }\n            batch.add_data_object(\n                question_object,\n                class_name=\"articles\",\n                uuid=generate_uuid5(question_object)\n            )\n        except Exception as e:\n            logging.error(f\"Error processing row {index}: {e}\")\n```", "```py\nclient.query.aggregate(\"articles\").with_meta_count().do()\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles\", [\"title\",\"abstractText\"])\n    .with_additional([\"id\"])\n    .with_near_text({\"concepts\": [\"Mouth Neoplasms\"]})\n    .with_limit(10)\n    .do()\n)\n\nprint(json.dumps(response, indent=4))\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles\", [\"title\", \"abstractText\"])\n    .with_near_object({\n        \"id\": \"a7690f03-66b9-5d17-b765-8c6eb21f99c8\" #id for a given article\n    })\n    .with_limit(10)\n    .with_additional([\"distance\"])\n    .do()\n)\n\nprint(json.dumps(response, indent=2))\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles\", [\"title\", \"abstractText\"])\n    .with_near_text({\"concepts\": [\"Gingival metastasis as first sign of multiorgan dissemination of epithelioid malignant mesothelioma\"]})\n    .with_generate(single_prompt=\"Please explain this article {title} like you would to someone without a medical degree.\")\n    .with_limit(1)\n    .do()\n)\n\nprint(json.dumps(response, indent=4))\n```", "```py\n response = (\n    client.query\n    .get(collection_name, [\"title\", \"abstractText\"])\n    .with_near_text({\"concepts\": [\"Mouth Neoplasms\"]})\n    .with_limit(3)\n    .with_generate(grouped_task=\"Summarize the key information here in bullet points. Make it understandable to someone without a medical degree.\")\n    .do()\n)\n\nprint(response[\"data\"][\"Get\"][\"Articles\"][0][\"_additional\"][\"generate\"][\"groupedResult\"])\n```", "```py\n- Metastatic malignant mesothelioma to the oral cavity is rare, with more cases in jaw bones than soft tissue\n- Average survival rate for this type of cancer is 9-12 months\n- Study of 13 patients who underwent neoadjuvant chemotherapy and surgery showed a median survival of 11 months\n- One patient had a gingival mass as the first sign of multiorgan recurrence of mesothelioma\n- Biopsy of new growing lesions, even in uncommon sites, is important for patients with a history of mesothelioma\n- Myoepithelioma of minor salivary gland origin can show features indicative of malignant potential\n- Metastatic neuroblastoma in the mandible is very rare and can present with osteolytic jaw defects and looseness of deciduous molars in children\n```", "```py\nfrom rdflib import Graph, RDF, RDFS, Namespace, URIRef, Literal\nfrom rdflib.namespace import SKOS, XSD\nimport pandas as pd\nimport urllib.parse\nimport random\nfrom datetime import datetime, timedelta\n\n# Create a new RDF graph\ng = Graph()\n\n# Define namespaces\nschema = Namespace('http://schema.org/')\nex = Namespace('http://example.org/')\nprefixes = {\n    'schema': schema,\n    'ex': ex,\n    'skos': SKOS,\n    'xsd': XSD\n}\nfor p, ns in prefixes.items():\n    g.bind(p, ns)\n\n# Define classes and properties\nArticle = URIRef(ex.Article)\nMeSHTerm = URIRef(ex.MeSHTerm)\ng.add((Article, RDF.type, RDFS.Class))\ng.add((MeSHTerm, RDF.type, RDFS.Class))\n\ntitle = URIRef(schema.name)\nabstract = URIRef(schema.description)\ndate_published = URIRef(schema.datePublished)\naccess = URIRef(ex.access)\n\ng.add((title, RDF.type, RDF.Property))\ng.add((abstract, RDF.type, RDF.Property))\ng.add((date_published, RDF.type, RDF.Property))\ng.add((access, RDF.type, RDF.Property))\n\n# Function to clean and parse MeSH terms\ndef parse_mesh_terms(mesh_list):\n    if pd.isna(mesh_list):\n        return []\n    return [term.strip().replace(' ', '_') for term in mesh_list.strip(\"[]'\").split(',')]\n\n# Function to create a valid URI\ndef create_valid_uri(base_uri, text):\n    if pd.isna(text):\n        return None\n    sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))\n    return URIRef(f\"{base_uri}/{sanitized_text}\")\n\n# Function to generate a random date within the last 5 years\ndef generate_random_date():\n    start_date = datetime.now() - timedelta(days=5*365)\n    random_days = random.randint(0, 5*365)\n    return start_date + timedelta(days=random_days)\n\n# Function to generate a random access value between 1 and 10\ndef generate_random_access():\n    return random.randint(1, 10)\n\n# Load your DataFrame here\n# df = pd.read_csv('your_data.csv')\n\n# Loop through each row in the DataFrame and create RDF triples\nfor index, row in df.iterrows():\n    article_uri = create_valid_uri(\"http://example.org/article\", row['Title'])\n    if article_uri is None:\n        continue\n\n    # Add Article instance\n    g.add((article_uri, RDF.type, Article))\n    g.add((article_uri, title, Literal(row['Title'], datatype=XSD.string)))\n    g.add((article_uri, abstract, Literal(row['abstractText'], datatype=XSD.string)))\n\n    # Add random datePublished and access\n    random_date = generate_random_date()\n    random_access = generate_random_access()\n    g.add((article_uri, date_published, Literal(random_date.date(), datatype=XSD.date)))\n    g.add((article_uri, access, Literal(random_access, datatype=XSD.integer)))\n\n    # Add MeSH Terms\n    mesh_terms = parse_mesh_terms(row['meshMajor'])\n    for term in mesh_terms:\n        term_uri = create_valid_uri(\"http://example.org/mesh\", term)\n        if term_uri is None:\n            continue\n\n        # Add MeSH Term instance\n        g.add((term_uri, RDF.type, MeSHTerm))\n        g.add((term_uri, RDFS.label, Literal(term.replace('_', ' '), datatype=XSD.string)))\n\n        # Link Article to MeSH Term\n        g.add((article_uri, schema.about, term_uri))\n\n# Serialize the graph to a file (optional)\ng.serialize(destination='ontology.ttl', format='turtle')\n```", "```py\nfrom SPARQLWrapper import SPARQLWrapper, JSON\n\ndef get_concept_triples_for_term(term):\n    sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")\n    query = f\"\"\"\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n    PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#>\n    PREFIX mesh: <http://id.nlm.nih.gov/mesh/>\n\n    SELECT ?subject ?p ?pLabel ?o ?oLabel\n    FROM <http://id.nlm.nih.gov/mesh>\n    WHERE {{\n        ?subject rdfs:label \"{term}\"@en .\n        ?subject ?p ?o .\n        FILTER(CONTAINS(STR(?p), \"concept\"))\n        OPTIONAL {{ ?p rdfs:label ?pLabel . }}\n        OPTIONAL {{ ?o rdfs:label ?oLabel . }}\n    }}\n    \"\"\"\n\n    sparql.setQuery(query)\n    sparql.setReturnFormat(JSON)\n    results = sparql.query().convert()\n\n    triples = set()  # Using a set to avoid duplicate entries\n    for result in results[\"results\"][\"bindings\"]:\n        obj_label = result.get(\"oLabel\", {}).get(\"value\", \"No label\")\n        triples.add(obj_label)\n\n    # Add the term itself to the list\n    triples.add(term)\n\n    return list(triples)  # Convert back to a list for easier handling\n\ndef get_narrower_concepts_for_term(term):\n    sparql = SPARQLWrapper(\"https://id.nlm.nih.gov/mesh/sparql\")\n    query = f\"\"\"\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n    PREFIX meshv: <http://id.nlm.nih.gov/mesh/vocab#>\n    PREFIX mesh: <http://id.nlm.nih.gov/mesh/>\n\n    SELECT ?narrowerConcept ?narrowerConceptLabel\n    WHERE {{\n        ?broaderConcept rdfs:label \"{term}\"@en .\n        ?narrowerConcept meshv:broaderDescriptor ?broaderConcept .\n        ?narrowerConcept rdfs:label ?narrowerConceptLabel .\n    }}\n    \"\"\"\n\n    sparql.setQuery(query)\n    sparql.setReturnFormat(JSON)\n    results = sparql.query().convert()\n\n    concepts = set()  # Using a set to avoid duplicate entries\n    for result in results[\"results\"][\"bindings\"]:\n        subject_label = result.get(\"narrowerConceptLabel\", {}).get(\"value\", \"No label\")\n        concepts.add(subject_label)\n\n    return list(concepts)  # Convert back to a list for easier handling\n\ndef get_all_narrower_concepts(term, depth=2, current_depth=1):\n    # Create a dictionary to store the terms and their narrower concepts\n    all_concepts = {}\n\n    # Initial fetch for the primary term\n    narrower_concepts = get_narrower_concepts_for_term(term)\n    all_concepts[term] = narrower_concepts\n\n    # If the current depth is less than the desired depth, fetch narrower concepts recursively\n    if current_depth < depth:\n        for concept in narrower_concepts:\n            # Recursive call to fetch narrower concepts for the current concept\n            child_concepts = get_all_narrower_concepts(concept, depth, current_depth + 1)\n            all_concepts.update(child_concepts)\n\n    return all_concepts\n\n# Fetch alternative names and narrower concepts\nterm = \"Mouth Neoplasms\"\nalternative_names = get_concept_triples_for_term(term)\nall_concepts = get_all_narrower_concepts(term, depth=2)  # Adjust depth as needed\n\n# Output alternative names\nprint(\"Alternative names:\", alternative_names)\nprint()\n\n# Output narrower concepts\nfor broader, narrower in all_concepts.items():\n    print(f\"Broader concept: {broader}\")\n    print(f\"Narrower concepts: {narrower}\")\n    print(\"---\")\n```", "```py\ndef flatten_concepts(concepts_dict):\n    flat_list = []\n\n    def recurse_terms(term_dict):\n        for term, narrower_terms in term_dict.items():\n            flat_list.append(term)\n            if narrower_terms:\n                recurse_terms(dict.fromkeys(narrower_terms, []))  # Use an empty dict to recurse\n\n    recurse_terms(concepts_dict)\n    return flat_list\n\n# Flatten the concepts dictionary\nflat_list = flatten_concepts(all_concepts)\n```", "```py\n#Convert the MeSH terms to URI\ndef convert_to_mesh_uri(term):\n    formatted_term = term.replace(\" \", \"_\").replace(\",\", \"_\").replace(\"-\", \"_\")\n    return URIRef(f\"http://example.org/mesh/_{formatted_term}_\")\n\n# Convert terms to URIs\nmesh_terms = [convert_to_mesh_uri(term) for term in flat_list]\n```", "```py\nfrom rdflib import URIRef\n\nquery = \"\"\"\nPREFIX schema: <http://schema.org/>\nPREFIX ex: <http://example.org/>\n\nSELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm\nWHERE {\n  ?article a ex:Article ;\n           schema:name ?title ;\n           schema:description ?abstract ;\n           schema:datePublished ?datePublished ;\n           ex:access ?access ;\n           schema:about ?meshTerm .\n\n  ?meshTerm a ex:MeSHTerm .\n}\n\"\"\"\n\n# Dictionary to store articles and their associated MeSH terms\narticle_data = {}\n\n# Run the query for each MeSH term\nfor mesh_term in mesh_terms:\n    results = g.query(query, initBindings={'meshTerm': mesh_term})\n\n    # Process results\n    for row in results:\n        article_uri = row['article']\n\n        if article_uri not in article_data:\n            article_data[article_uri] = {\n                'title': row['title'],\n                'abstract': row['abstract'],\n                'datePublished': row['datePublished'],\n                'access': row['access'],\n                'meshTerms': set()\n            }\n\n        # Add the MeSH term to the set for this article\n        article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))\n\n# Rank articles by the number of matching MeSH terms\nranked_articles = sorted(\n    article_data.items(),\n    key=lambda item: len(item[1]['meshTerms']),\n    reverse=True\n)\n\n# Get the top 3 articles\ntop_3_articles = ranked_articles[:3]\n\n# Output results\nfor article_uri, data in top_3_articles:\n    print(f\"Title: {data['title']}\")\n    print(\"MeSH Terms:\")\n    for mesh_term in data['meshTerms']:\n        print(f\"  - {mesh_term}\")\n    print()\n```", "```py\nfrom rdflib import Graph, URIRef\nfrom rdflib.namespace import RDF, RDFS, Namespace, SKOS\nimport urllib.parse\n\n# Define namespaces\nschema = Namespace('http://schema.org/')\nex = Namespace('http://example.org/')\nrdfs = Namespace('http://www.w3.org/2000/01/rdf-schema#')\n\n# Function to calculate Jaccard similarity and return overlapping terms\ndef jaccard_similarity(set1, set2):\n    intersection = set1.intersection(set2)\n    union = set1.union(set2)\n    similarity = len(intersection) / len(union) if len(union) != 0 else 0\n    return similarity, intersection\n\n# Load the RDF graph\ng = Graph()\ng.parse('ontology.ttl', format='turtle')\n\ndef get_article_uri(title):\n    # Convert the title to a URI-safe string\n    safe_title = urllib.parse.quote(title.replace(\" \", \"_\"))\n    return URIRef(f\"http://example.org/article/{safe_title}\")\n\ndef get_mesh_terms(article_uri):\n    query = \"\"\"\n    PREFIX schema: <http://schema.org/>\n    PREFIX ex: <http://example.org/>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\n    SELECT ?meshTerm\n    WHERE {\n      ?article schema:about ?meshTerm .\n      ?meshTerm a ex:MeSHTerm .\n      FILTER (?article = <\"\"\" + str(article_uri) + \"\"\">)\n    }\n    \"\"\"\n    results = g.query(query)\n    mesh_terms = {str(row['meshTerm']) for row in results}\n    return mesh_terms\n\ndef find_similar_articles(title):\n    article_uri = get_article_uri(title)\n    mesh_terms_given_article = get_mesh_terms(article_uri)\n\n    # Query all articles and their MeSH terms\n    query = \"\"\"\n    PREFIX schema: <http://schema.org/>\n    PREFIX ex: <http://example.org/>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\n    SELECT ?article ?meshTerm\n    WHERE {\n      ?article a ex:Article ;\n               schema:about ?meshTerm .\n      ?meshTerm a ex:MeSHTerm .\n    }\n    \"\"\"\n    results = g.query(query)\n\n    mesh_terms_other_articles = {}\n    for row in results:\n        article = str(row['article'])\n        mesh_term = str(row['meshTerm'])\n        if article not in mesh_terms_other_articles:\n            mesh_terms_other_articles[article] = set()\n        mesh_terms_other_articles[article].add(mesh_term)\n\n    # Calculate Jaccard similarity\n    similarities = {}\n    overlapping_terms = {}\n    for article, mesh_terms in mesh_terms_other_articles.items():\n        if article != str(article_uri):\n            similarity, overlap = jaccard_similarity(mesh_terms_given_article, mesh_terms)\n            similarities[article] = similarity\n            overlapping_terms[article] = overlap\n\n    # Sort by similarity and get top 5\n    top_similar_articles = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:15]\n\n    # Print results\n    print(f\"Top 15 articles similar to '{title}':\")\n    for article, similarity in top_similar_articles:\n        print(f\"Article URI: {article}\")\n        print(f\"Jaccard Similarity: {similarity:.4f}\")\n        print(f\"Overlapping MeSH Terms: {overlapping_terms[article]}\")\n        print()\n\n# Example usage\narticle_title = \"Gingival metastasis as first sign of multiorgan dissemination of epithelioid malignant mesothelioma.\"\nfind_similar_articles(article_title)\n```", "```py\n# Function to combine titles and abstracts\ndef combine_abstracts(top_3_articles):\n    combined_text = \"\".join(\n        [f\"Title: {data['title']} Abstract: {data['abstract']}\" for article_uri, data in top_3_articles]\n    )\n    return combined_text\n\n# Combine abstracts from the top 3 articles\ncombined_text = combine_abstracts(top_3_articles)\nprint(combined_text)\n```", "```py\nimport openai\n\n# Set up your OpenAI API key\napi_key = \"YOUR API KEY\"\nopenai.api_key = api_key\n```", "```py\ndef generate_summary(combined_text):\n    response = openai.Completion.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        prompt=f\"Summarize the key information here in bullet points. Make it understandable to someone without a medical degree:\\n\\n{combined_text}\",\n        max_tokens=1000,\n        temperature=0.3\n    )\n\n    # Get the raw text output\n    raw_summary = response.choices[0].text.strip()\n\n    # Split the text into lines and clean up whitespace\n    lines = raw_summary.split('\\n')\n    lines = [line.strip() for line in lines if line.strip()]\n\n    # Join the lines back together with actual line breaks\n    formatted_summary = '\\n'.join(lines)\n\n    return formatted_summary\n\n# Generate and print the summary\nsummary = generate_summary(combined_text)\nprint(summary)\n```", "```py\n- A 14-year-old boy had a gingival tumor in his anterior maxilla that was removed and studied by light and electron microscopy\n- The tumor was made up of myoepithelial cells and appeared to be malignant\n- Electron microscopy showed that the tumor originated from a salivary gland\n- This is the only confirmed case of a myoepithelioma with features of malignancy\n- A feasibility study was conducted to improve early detection of oral cancer and premalignant lesions in a high incidence region\n- Tobacco vendors were involved in distributing flyers to invite smokers for free examinations by general practitioners\n- 93 patients were included in the study and 27% were referred to a specialist\n- 63.6% of those referred actually saw a specialist and 15.3% were confirmed to have a premalignant lesion\n- A study found a correlation between increased expression of the protein HuR and the enzyme COX-2 in oral squamous cell carcinoma (OSCC)\n- Cytoplasmic HuR expression was associated with COX-2 expression and lymph node and distant metastasis in OSCCs\n- Inhibition of HuR expression led to a decrease in COX-2 expression in oral cancer cells.\n```", "```py\n# Function to create a valid URI\ndef create_valid_uri(base_uri, text):\n    if pd.isna(text):\n        return None\n    # Encode text to be used in URI\n    sanitized_text = urllib.parse.quote(text.strip().replace(' ', '_').replace('\"', '').replace('<', '').replace('>', '').replace(\"'\", \"_\"))\n    return URIRef(f\"{base_uri}/{sanitized_text}\")\n\n# Add a new column to the DataFrame for the article URIs\ndf['Article_URI'] = df['Title'].apply(lambda title: create_valid_uri(\"http://example.org/article\", title)) \n```", "```py\nclass_obj = {\n    # Class definition\n    \"class\": \"articles_with_abstracts_and_URIs\",\n\n    # Property definitions\n    \"properties\": [\n        {\n            \"name\": \"title\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"abstractText\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"meshMajor\",\n            \"dataType\": [\"text\"],\n        },\n        {\n            \"name\": \"Article_URI\",\n            \"dataType\": [\"text\"],\n        },\n    ],\n\n    # Specify a vectorizer\n    \"vectorizer\": \"text2vec-openai\",\n\n    # Module settings\n    \"moduleConfig\": {\n        \"text2vec-openai\": {\n            \"vectorizeClassName\": True,\n            \"model\": \"ada\",\n            \"modelVersion\": \"002\",\n            \"type\": \"text\"\n        },\n        \"qna-openai\": {\n          \"model\": \"gpt-3.5-turbo-instruct\"\n        },\n        \"generative-openai\": {\n          \"model\": \"gpt-3.5-turbo\"\n        }\n    },\n}\n```", "```py\nclient.schema.create_class(class_obj)\n```", "```py\nimport logging\nimport numpy as np\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n\n# Replace infinity values with NaN and then fill NaN values\ndf.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf.fillna('', inplace=True)\n\n# Convert columns to string type\ndf['Title'] = df['Title'].astype(str)\ndf['abstractText'] = df['abstractText'].astype(str)\ndf['meshMajor'] = df['meshMajor'].astype(str)\ndf['Article_URI'] = df['Article_URI'].astype(str)\n\n# Log the data types\nlogging.info(f\"Title column type: {df['Title'].dtype}\")\nlogging.info(f\"abstractText column type: {df['abstractText'].dtype}\")\nlogging.info(f\"meshMajor column type: {df['meshMajor'].dtype}\")\nlogging.info(f\"Article_URI column type: {df['Article_URI'].dtype}\")\n\nwith client.batch(\n    batch_size=10,  # Specify batch size\n    num_workers=2,   # Parallelize the process\n) as batch:\n    for index, row in df.iterrows():\n        try:\n            question_object = {\n                \"title\": row.Title,\n                \"abstractText\": row.abstractText,\n                \"meshMajor\": row.meshMajor,\n                \"article_URI\": row.Article_URI,\n            }\n            batch.add_data_object(\n                question_object,\n                class_name=\"articles_with_abstracts_and_URIs\",\n                uuid=generate_uuid5(question_object)\n            )\n        except Exception as e:\n            logging.error(f\"Error processing row {index}: {e}\")\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles_with_abstracts_and_URIs\", [\"title\",\"abstractText\",\"meshMajor\",\"article_URI\"])\n    .with_additional([\"id\"])\n    .with_near_text({\"concepts\": [\"mouth neoplasms\"]})\n    .with_limit(10)\n    .do()\n)\n\nprint(json.dumps(response, indent=4))\n```", "```py\nfrom rdflib import Graph, Namespace, URIRef, Literal\nfrom rdflib.namespace import RDF, RDFS, XSD\n\n# Define namespaces\nschema = Namespace('http://schema.org/')\nex = Namespace('http://example.org/')\nrdfs = Namespace('http://www.w3.org/2000/01/rdf-schema#')\nxsd = Namespace('http://www.w3.org/2001/XMLSchema#')\n\ndef get_articles_after_date(graph, article_uris, date_cutoff):\n    # Create a dictionary to store results for each URI\n    results_dict = {}\n\n    # Define the SPARQL query using a list of article URIs and a date filter\n    uris_str = \" \".join(f\"<{uri}>\" for uri in article_uris)\n    query = f\"\"\"\n    PREFIX schema: <http://schema.org/>\n    PREFIX ex: <http://example.org/>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n\n    SELECT ?article ?title ?datePublished\n    WHERE {{\n      VALUES ?article {{ {uris_str} }}\n\n      ?article a ex:Article ;\n               schema:name ?title ;\n               schema:datePublished ?datePublished .\n\n      FILTER (?datePublished > \"{date_cutoff}\"^^xsd:date)\n    }}\n    \"\"\"\n\n    # Execute the query\n    results = graph.query(query)\n\n    # Extract the details for each article\n    for row in results:\n        article_uri = str(row['article'])\n        results_dict[article_uri] = {\n            'title': str(row['title']),\n            'date_published': str(row['datePublished'])\n        }\n\n    return results_dict\n\ndate_cutoff = \"2023-01-01\"\narticles_after_date = get_articles_after_date(g, article_uris, date_cutoff)\n\n# Output the results\nfor uri, details in articles_after_date.items():\n    print(f\"Article URI: {uri}\")\n    print(f\"Title: {details['title']}\")\n    print(f\"Date Published: {details['date_published']}\")\n    print()\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles_with_abstracts_and_URIs\", [\"title\",\"abstractText\",\"meshMajor\",\"article_URI\"])\n    .with_near_object({\n        \"id\": \"37b695c4-5b80-5f44-a710-e84abb46bc22\"\n    })\n    .with_limit(50)\n    .with_additional([\"distance\"])\n    .do()\n)\n\nprint(json.dumps(response, indent=2))\n```", "```py\n# Assuming response is the data structure with your articles\narticle_uris = [URIRef(article[\"article_URI\"]) for article in response[\"data\"][\"Get\"][\"Articles_with_abstracts_and_URIs\"]]\n```", "```py\nfrom rdflib import URIRef\n\n# Constructing the SPARQL query with a FILTER for the article URIs\nquery = \"\"\"\nPREFIX schema: <http://schema.org/>\nPREFIX ex: <http://example.org/>\n\nSELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm\nWHERE {\n  ?article a ex:Article ;\n           schema:name ?title ;\n           schema:description ?abstract ;\n           schema:datePublished ?datePublished ;\n           ex:access ?access ;\n           schema:about ?meshTerm .\n\n  ?meshTerm a ex:MeSHTerm .\n\n  # Filter to include only articles from the list of URIs\n  FILTER (?article IN (%s))\n}\n\"\"\"\n\n# Convert the list of URIRefs into a string suitable for SPARQL\narticle_uris_string = \", \".join([f\"<{str(uri)}>\" for uri in article_uris])\n\n# Insert the article URIs into the query\nquery = query % article_uris_string\n\n# Dictionary to store articles and their associated MeSH terms\narticle_data = {}\n\n# Run the query for each MeSH term\nfor mesh_term in mesh_terms:\n    results = g.query(query, initBindings={'meshTerm': mesh_term})\n\n    # Process results\n    for row in results:\n        article_uri = row['article']\n\n        if article_uri not in article_data:\n            article_data[article_uri] = {\n                'title': row['title'],\n                'abstract': row['abstract'],\n                'datePublished': row['datePublished'],\n                'access': row['access'],\n                'meshTerms': set()\n            }\n\n        # Add the MeSH term to the set for this article\n        article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))\n\n# Rank articles by the number of matching MeSH terms\nranked_articles = sorted(\n    article_data.items(),\n    key=lambda item: len(item[1]['meshTerms']),\n    reverse=True\n)\n\n# Output results\nfor article_uri, data in ranked_articles:\n    print(f\"Title: {data['title']}\")\n    print(f\"Abstract: {data['abstract']}\")\n    print(\"MeSH Terms:\")\n    for mesh_term in data['meshTerms']:\n        print(f\"  - {mesh_term}\")\n    print()\n```", "```py\nfrom rdflib import Graph, Namespace, URIRef, Literal\nfrom rdflib.namespace import RDF, RDFS, XSD, SKOS\n\n# Assuming your RDF graph (g) is already loaded\n\n# Define namespaces\nschema = Namespace('http://schema.org/')\nex = Namespace('http://example.org/')\nrdfs = Namespace('http://www.w3.org/2000/01/rdf-schema#')\n\ndef filter_articles_by_access(graph, article_uris, access_values):\n    # Construct the SPARQL query with a dynamic VALUES clause\n    uris_str = \" \".join(f\"<{uri}>\" for uri in article_uris)\n    query = f\"\"\"\n    PREFIX schema: <http://schema.org/>\n    PREFIX ex: <http://example.org/>\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\n    SELECT ?article ?title ?abstract ?datePublished ?access ?meshTermLabel\n    WHERE {{\n      VALUES ?article {{ {uris_str} }}\n\n      ?article a ex:Article ;\n               schema:name ?title ;\n               schema:description ?abstract ;\n               schema:datePublished ?datePublished ;\n               ex:access ?access ;\n               schema:about ?meshTerm .\n      ?meshTerm rdfs:label ?meshTermLabel .\n\n      FILTER (?access IN ({\", \".join(map(str, access_values))}))\n    }}\n    \"\"\"\n\n    # Execute the query\n    results = graph.query(query)\n\n    # Extract the details for each article\n    results_dict = {}\n    for row in results:\n        article_uri = str(row['article'])\n        if article_uri not in results_dict:\n            results_dict[article_uri] = {\n                'title': str(row['title']),\n                'abstract': str(row['abstract']),\n                'date_published': str(row['datePublished']),\n                'access': str(row['access']),\n                'mesh_terms': []\n            }\n        results_dict[article_uri]['mesh_terms'].append(str(row['meshTermLabel']))\n\n    return results_dict\n\naccess_values = [3,5,7]\nfiltered_articles = filter_articles_by_access(g, ranked_article_uris, access_values)\n\n# Output the results\nfor uri, details in filtered_articles.items():\n    print(f\"Article URI: {uri}\")\n    print(f\"Title: {details['title']}\")\n    print(f\"Abstract: {details['abstract']}\")\n    print(f\"Date Published: {details['date_published']}\")\n    print(f\"Access: {details['access']}\")\n    print()\n```", "```py\nresponse = (\n    client.query\n    .get(\"Articles_with_abstracts_and_URIs\", [\"title\", \"abstractText\",'article_URI','meshMajor'])\n    .with_near_text({\"concepts\": [\"therapies for mouth neoplasms\"]})\n    .with_limit(3)\n    .with_generate(grouped_task=\"Summarize the key information here in bullet points. Make it understandable to someone without a medical degree.\")\n    .do()\n)\n\nprint(response[\"data\"][\"Get\"][\"Articles_with_abstracts_and_URIs\"][0][\"_additional\"][\"generate\"][\"groupedResult\"])\n```", "```py\n- Metastatic malignant mesothelioma to the oral cavity is rare, with an average survival rate of 9-12 months.\n- Neoadjuvant chemotherapy and radical pleurectomy decortication followed by radiotherapy were used in 13 patients from August 2012 to September 2013.\n- In January 2014, 11 patients were still alive with a median survival of 11 months, while 8 patients had a recurrence and 2 patients died at 8 and 9 months after surgery.\n- A 68-year-old man had a gingival mass that turned out to be a metastatic deposit of malignant mesothelioma, leading to multiorgan recurrence.\n- Biopsy is important for new growing lesions, even in uncommon sites, when there is a history of mesothelioma.\n\n- Neoadjuvant radiochemotherapy for locally advanced rectal carcinoma can be effective, but some patients may not respond well.\n- Genetic alterations may be associated with sensitivity or resistance to neoadjuvant therapy in rectal cancer.\n- Losses of chromosomes 1p, 8p, 17p, and 18q, and gains of 1q and 13q were found in rectal cancer tumors.\n- Alterations in specific chromosomal regions were associated with the response to neoadjuvant therapy.\n- The cytogenetic profile of tumor cells may influence the response to radiochemotherapy in rectal cancer.\n\n- Intensity-modulated radiation therapy for nasopharyngeal carcinoma achieved good long-term outcomes in terms of local control and overall survival.\n- Acute toxicities included mucositis, dermatitis, and xerostomia, with most patients experiencing Grade 0-2 toxicities.\n- Late toxicity mainly included xerostomia, which improved over time.\n- Distant metastasis remained the main cause of treatment failure, highlighting the need for more effective systemic therapy.\n```", "```py\n# Extract article URIs\narticle_uris = [article[\"article_URI\"] for article in response[\"data\"][\"Get\"][\"Articles_with_abstracts_and_URIs\"]]\n\n# Function to filter the response for only the given URIs\ndef filter_articles_by_uri(response, article_uris):\n    filtered_articles = []\n\n    articles = response['data']['Get']['Articles_with_abstracts_and_URIs']\n    for article in articles:\n        if article['article_URI'] in article_uris:\n            filtered_articles.append(article)\n\n    return filtered_articles\n\n# Filter the response\nfiltered_articles = filter_articles_by_uri(response, article_uris)\n\n# Output the filtered articles\nprint(\"Filtered articles:\")\nfor article in filtered_articles:\n    print(f\"Title: {article['title']}\")\n    print(f\"URI: {article['article_URI']}\")\n    print(f\"Abstract: {article['abstractText']}\")\n    print(f\"MeshMajor: {article['meshMajor']}\")\n    print(\"---\")\n```", "```py\nresponse = (\n    client.query\n    .get(\"articles_with_abstracts_and_URIs\", [\"title\", \"abstractText\", \"meshMajor\", \"article_URI\"])\n    .with_additional([\"id\"])\n    .with_near_text({\"concepts\": [\"therapies for mouth neoplasms\"]})\n    .with_limit(20)\n    .do()\n)\n\n# Extract article URIs\narticle_uris = [article[\"article_URI\"] for article in response[\"data\"][\"Get\"][\"Articles_with_abstracts_and_URIs\"]]\n\n# Print the extracted article URIs\nprint(\"Extracted article URIs:\")\nfor uri in article_uris:\n    print(uri)\n```", "```py\nfrom rdflib import URIRef\n\n# Constructing the SPARQL query with a FILTER for the article URIs\nquery = \"\"\"\nPREFIX schema: <http://schema.org/>\nPREFIX ex: <http://example.org/>\n\nSELECT ?article ?title ?abstract ?datePublished ?access ?meshTerm\nWHERE {\n  ?article a ex:Article ;\n           schema:name ?title ;\n           schema:description ?abstract ;\n           schema:datePublished ?datePublished ;\n           ex:access ?access ;\n           schema:about ?meshTerm .\n\n  ?meshTerm a ex:MeSHTerm .\n\n  # Filter to include only articles from the list of URIs\n  FILTER (?article IN (%s))\n}\n\"\"\"\n\n# Convert the list of URIRefs into a string suitable for SPARQL\narticle_uris_string = \", \".join([f\"<{str(uri)}>\" for uri in article_uris])\n\n# Insert the article URIs into the query\nquery = query % article_uris_string\n\n# Dictionary to store articles and their associated MeSH terms\narticle_data = {}\n\n# Run the query for each MeSH term\nfor mesh_term in mesh_terms:\n    results = g.query(query, initBindings={'meshTerm': mesh_term})\n\n    # Process results\n    for row in results:\n        article_uri = row['article']\n\n        if article_uri not in article_data:\n            article_data[article_uri] = {\n                'title': row['title'],\n                'abstract': row['abstract'],\n                'datePublished': row['datePublished'],\n                'access': row['access'],\n                'meshTerms': set()\n            }\n\n        # Add the MeSH term to the set for this article\n        article_data[article_uri]['meshTerms'].add(str(row['meshTerm']))\n\n# Rank articles by the number of matching MeSH terms\nranked_articles = sorted(\n    article_data.items(),\n    key=lambda item: len(item[1]['meshTerms']),\n    reverse=True\n)\n\n# Output results\nfor article_uri, data in ranked_articles:\n    print(f\"Title: {data['title']}\")\n    print(f\"Abstract: {data['abstract']}\")\n    print(\"MeSH Terms:\")\n    for mesh_term in data['meshTerms']:\n        print(f\"  - {mesh_term}\")\n    print()\n```", "```py\n# Filter the response\nfiltered_articles = filter_articles_by_uri(response, matching_articles)\n\n# Function to combine titles and abstracts into one chunk of text\ndef combine_abstracts(filtered_articles):\n    combined_text = \"\\n\\n\".join(\n        [f\"Title: {article['title']}\\nAbstract: {article['abstractText']}\" for article in filtered_articles]\n    )\n    return combined_text\n\n# Combine abstracts from the filtered articles\ncombined_text = combine_abstracts(filtered_articles)\n\n# Generate and print the summary\nsummary = generate_summary(combined_text)\nprint(summary)\n```", "```py\n- Oral cavity cancer is common and often not detected until it is advanced\n- A feasibility study was conducted to improve early detection of oral cancer and premalignant lesions in a high-risk region\n- Tobacco vendors were involved in distributing flyers to smokers for free examinations by general practitioners\n- 93 patients were included in the study, with 27% being referred to a specialist\n- 63.6% of referred patients actually saw a specialist, with 15.3% being diagnosed with a premalignant lesion\n- Photodynamic therapy (PDT) was studied as an experimental cancer therapy in rats with chemically-induced premalignant lesions and squamous cell carcinoma of the palatal mucosa\n- PDT was performed using Photofrin and two different activation wavelengths, with better results seen in the 514.5 nm group\n- Gingival metastasis from malignant mesothelioma is extremely rare, with a low survival rate\n- A case study showed a patient with a gingival mass as the first sign of multiorgan recurrence of malignant mesothelioma, highlighting the importance of biopsy for all new lesions, even in uncommon anatomical sites.\n```"]