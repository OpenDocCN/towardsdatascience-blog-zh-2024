<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Self-Instruct Framework, Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Self-Instruct Framework, Explained</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05">https://towardsdatascience.com/self-instruct-framework-explained-16bce90f4683?source=collection_archive---------10-----------------------#2024-03-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="343a" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Or how to “eliminate” human annotators</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Tsiu-zhen-tsin Dmitrii" class="l ep by dd de cx" src="../Images/e210c94ae2a6415cba7189c59f7eafa5.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*GYzj9Arxzns6SDPw0mnmdg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@dmitry.tsyuzhentsin?source=post_page---byline--16bce90f4683--------------------------------" rel="noopener follow">Tsiu-zhen-tsin Dmitrii</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--16bce90f4683--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">10 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi bh"><figure class="mj mk ml mm mn mi bh paragraph-image"><img src="../Images/a2032623dfbb4f53b21fa2fade0a02d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:3584/format:webp/1*VuinR8NJdclKOrt2ZeW55g.jpeg"/><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Image generated by DALL·E</figcaption></figure></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8ceb" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Motivation</h1><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt ns"><img src="../Images/846718fbfb023aecc4441a424b81e209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdXtwkokrfp3UmIN_NEg9g.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">High-level overview of InstructGPT with human annotated outputs and ranking for supervised learning and reward model training | Source: <a class="af nx" href="https://arxiv.org/pdf/2203.02155" rel="noopener ugc nofollow" target="_blank">Training language models to follow instructions with human feedback</a>.</figcaption></figure><p id="1b44" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk ou"><span class="l ov ow ox bo oy oz pa pb pc ed">As</span> Large Language Models (LLMs) revolutionize our life, the growth of instruction-tuned LLMs faces significant challenges: the critical need for vast, varied, and high-quality datasets. Traditional methods, such as employing human annotators to generate datasets — a strategy used in InstructGPT (image above)— face high costs, limited diversity, creativity, and allignment challenges. To address these limitations, the Self-Instruct framework² was introduced. Its core idea is simple and powerful: let language models (LM) generate training data, leading to more cost-effective, diverse and creative datasets.</p><p id="70ec" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Therefore, in this article, I would like to lead you through the framework step-by-step, demonstrating all the details so that after reading it, you will be able to reproduce the results yourself :)</p></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="de0f" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">❗ This article provides all steps from code perspective, so please feel free to visit the original <a class="af nx" href="https://github.com/yizhongw/self-instruct#" rel="noopener ugc nofollow" target="_blank">GitHub repository</a> .❗</p></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="e7d5" class="mw mx fq bf my mz pl gq nb nc pm gt ne nf pn nh ni nj po nl nm nn pp np nq nr bk">Self-Instruct Framework</h1></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt pv"><img src="../Images/c52708b5f5a0ce82c6b7da94255ad7e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*KFYhyLVH-yOCIQAedXnL-A.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">A high-level overview of the Self-Instruct framework</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="050b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The recipe is relatively straightforward:</p><ul class=""><li id="fb28" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot py pz qa bk"><strong class="oa fr">Step 0 </strong>— Define Instruction Data: <br/> — Add a seed of high-quality and diverse human-written tasks in different domains as tuples (instruction, instances) to the task pool;</li><li id="66d6" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk"><strong class="oa fr">Step 1 — </strong>Instruction Generation: <br/> — Sample 8 (6 human-written and 2 model-generated) instructions from the task pool;<br/> — Insert bootstrapped instructions into the prompt in a few-shot way and ask an LM to come up with more instructions;<br/> — Filter generated instructions out based on ROUGE-metric (a method to evaluate the similarity between text outputs and reference texts) and some heuristics (I will cover this later);<br/>— Repeat Step 1 until reaching some amount of instructions;</li><li id="f06e" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk"><strong class="oa fr">Step 2 — </strong>Classification Task Identification: <br/> — For every generated instruction in the task pool, we need to identify its type (classification or non-classification) via a few-shot manner;</li><li id="57e8" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk"><strong class="oa fr">Step 3 — </strong>Instance Generation: <br/> — Given the instructions and task types, generate instances (inputs and outputs) and filter them out based on heuristics;</li><li id="2d3a" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk"><strong class="oa fr">Step 4</strong> — Finetuning the LM to Follow Instructions:<br/>— Utilize generated tasks to finetune a pre-trained model.</li></ul><p id="54b6" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Voila, that’s how the Self-Instruct works, but the devil is in the details, so let’s dive into every step!</p><h2 id="4090" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Step 0 — Define Instruction Data</h2><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qx"><img src="../Images/c1ad6c077176a803126a54ade25f05bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BamfbiH6V40bbq4L3pi7aw.jpeg"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Step 0</figcaption></figure><p id="0181" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Let’s begin by understanding what is inside the initial “Seed of tasks”: it consists of 175 seed tasks (25 classification and 150 non-classifications) with <strong class="oa fr">one</strong> <strong class="oa fr">instruction</strong> and <strong class="oa fr">one</strong> <strong class="oa fr">instance</strong> per task in different domains. Each task has an id, name, instruction, instances (<strong class="oa fr">input and output</strong>), and is_classification binary flag, identifying whether the task has a limited output label space.</p><p id="b144" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">There are some examples of classification and non-classification tasks with empty and non-empty input fields:</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/445da55a377333a447a8e31681447d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rnVT_z7vkVtxDqOolZtOrg.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Example of classification task with non-empty input</figcaption></figure><figure class="le mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/45fde17cc82f3f58851167a597992e3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*1sijy_3hJphNQWf9-nlP4g.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Example of non-classification task with empty input</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="9532" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Therefore, we can see in the first example how the input field clarifies and provides context to the more general instruction, while in the second example, we don’t need an input field as long as the instruction is already self-contained. Also, the first example is the classification task — we can answer it by assigning some labels from limited space, while we can’t do the same with the second example.</p><p id="5b80" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">This step is <strong class="oa fr">crucial</strong> as long as we encourage task diversity via data formats in the dataset and demonstrate correct ways of solving various tasks.</p><p id="b7f1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As long as we define the instruction format, we add them to the task pool to store our final dataset.</p><h2 id="7288" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Step 1 — Instruction Generation</h2><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qx"><img src="../Images/3ddb5fb00f8753b2ce69ee59bfec769e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ox6JRfr3fNUf5727zLwUA.jpeg"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Step 1</figcaption></figure><p id="e7e4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk"><strong class="oa fr">Sampling and prompting</strong></p><p id="67cb" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">By adding a human-written seed set of tasks to the task pool, we can start with instructions generation. To do so, we need to sample 8 instructions from the task pool (6 human-written and 2 machine-generated) and encode them into the following prompt:</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/3fe0f35250659fbce7e3c14511796cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*Ri4Q1LH25deMnhszJhwRIg.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Prompt to generate new instructions</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="63e4" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">However, in the beginning, we do not have any machine-generated instructions. Therefore, we just replaced them with empty strings in the prompt.</p><p id="00c7" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">After generation, we extract instructions from the LM’s response (via regular expressions), filter them out, and add filtered instructions to the task pool:</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/56ae357136a1310a25a128bc1cf65cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*u6AXD14AmhfIkaholNNOhQ.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Pseudo-code of instruction generation step</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="7e6a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">We repeat the instruction generation step until we reach some number of machine-generated instructions (specified at the beginning of the step).</p><p id="52e1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk"><strong class="oa fr">Filtering</strong></p><p id="1182" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">To obtain a diverse dataset, we need to define somehow which instructions will be added or not to the task pool, and the easiest way is a heuristically chosen set of rules, for instance:</p><ul class=""><li id="13c7" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot py pz qa bk">Filter out instructions that are too short or too long;</li><li id="cd89" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Filter based on keywords unsuitable for language models (image, graph, file, plot, …);</li><li id="0516" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Filter those starting with punctuation;</li><li id="3bad" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Filter those starting with non-English characters;</li><li id="bfba" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Filter those when their ROUGE-L similarity with any existing instruction is higher than 0.7;</li></ul><h2 id="142a" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Step 2— Classification Task Identification</h2><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qx"><img src="../Images/c6ffd4aca9d8e7ec419a64b2f5c7b11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ea_hnjM6GqNVFWYpPHwFXQ.jpeg"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Step 2</figcaption></figure><p id="7c4a" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">The authors of Self-Instruct noticed that depending on an instruction, the language models can be biased towards one label, especially for classification tasks. Therefore, to eliminate such such, we need to classify every instruction via few-shot prompting:</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/af86a11dae91aa778edbb0a0b6f0bb1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*a0smsmRI6NjFxI-0oO8oDA.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Prompt used to classify whether a task instruction is a classification or non-classification task (12 classification and 19 non-classification instructions are used in this template)</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="1781" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Step 3 — Instance Generation</h2><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qx"><img src="../Images/c1f43525948bb2d7bb6ef780a01d3f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K5GB9hIH01M-1rVihIxRVA.jpeg"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Step 3</figcaption></figure><p id="da05" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">After identifying the instruction type, we can finally generate input and output, considering that we have two types of instructions (classification or non-classification). How? <strong class="oa fr">Few-shot prompting!</strong></p><p id="303e" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">For non-classification instructions, we ask the model to generate input and only then output (<strong class="oa fr">Input-First Approach</strong>), but for classification tasks, we ask the model to generate output (class label) first and then condition input generation based on output (<strong class="oa fr">Output-First Approach</strong>). Compared to Step 0, we don’t restrict the number of generated instances per every instruction.</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/936041e4deb8138940508512b2d2fc22.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*7lXqsrMp8T31cF5-2zrjcQ.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Prompt used for the Input-First Approach of instance generation</figcaption></figure><figure class="le mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/81ce9945254ce624197510383efa8904.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*zybTEHlwQTU4GdxMJObvmA.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Prompt used for the Output-First Approach of instance generation</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5fa1" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">After generation, we extract instances and format them (regular expressions); after formatting, we filter them out using some rules, for example:</p><ul class=""><li id="a69d" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot py pz qa bk">If input and output are the same,</li><li id="cff3" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">If instances are already in the task pool,</li><li id="4b78" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">If the output is empty,</li><li id="e358" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">These are usually incomplete generations if the input or output ends with a colon;</li></ul><p id="384b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">And some other heuristics. In the end, we have the following example of a generated task with 1 instruction and 1 instance:</p></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt qy"><img src="../Images/e66b61fa15e4b849411a66a67414099d.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ZrrfrOPxY9fZAqKgL9eHQg.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Instance generation example</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c206" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">That’s the main idea behind Self-Intsruct!</p><h2 id="e9dc" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Step 4— Finetuning the LM to Follow Instructions</h2><p id="d954" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">After completing all previous steps, we can take a pre-trained LM and instruction-tune it on the generated dataset to achieve better metrics.</p><h1 id="aa0b" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Overcoming challenges</h1><p id="bdcd" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">At the beginning of the article, I covered some challenges that “instruction-tuned” LLMs face; let’s see how Self-Instruct enables overcoming them.</p><h2 id="ba38" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Quantity</h2><p id="5a52" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">With the help of only 175 initial human-written tasks, 52K instructions and 82K instances were generated:</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div class="ms mt re"><img src="../Images/7c26e7706615a94f7c70e392cccd8c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*yUzTqqigih4ZapEnVzP86w.png"/></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Source: <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></figcaption></figure><h2 id="c2fc" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Diversity</h2><p id="cbfa" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">To investigate how diverse the generated dataset is, authors of Self-Instruct used Berkley Neural Parser to parse instructions and then extract the closest verb to the root and its first direct noun object. 26K out of 52K instructions have a verb-noun format, but the other 26K instructions have more complex structure (e.g., “Classify whether this tweet contains political content or not.”) or are framed as questions (e.g., “Which of these statements are true?”).</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div class="ms mt rf"><img src="../Images/3321770df0415c9c20e8478a05891469.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*ljMd-97OUp6YuVxB7Th7Ig.png"/></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">The top 20 most common root verbs (inner circle) and their top 4 direct noun objects (outer circle) in the generated instructions | Source: <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></figcaption></figure><h2 id="06c9" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Quality</h2><p id="d499" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">To prove that Self-Instruct can generate high-quality tasks, it was randomly selected 200 generated instructions and sampled 1 instance per instruction, and then the author of the framework assessed them, obtaining the following results:</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div class="ms mt rg"><img src="../Images/75c61fdf98529ce8b391c552b4fe9f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*Qd39NnghqMSuOCTT_lveAg.png"/></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Source: <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></figcaption></figure><p id="b449" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As we can see, 92% of all tasks describe a valid task, and 54% — have all valid fields (given that we generated 52K tasks, at least 26K will represent high-quality data, which is fantastic!)</p><h2 id="387a" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk"><strong class="al">Costs</strong></h2><p id="a10f" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">The Self-Instruct framework also introduces significant cost advantages as well. The initial phases of task generation (Steps 1-3 ) amount to a mere $600, while the last step of fine-tuning using the GPT-3 model incurs a cost of $338. It’s vital to keep in mind when we look at results!</p><h1 id="3ef2" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Results</h1><p id="0c4e" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">How Self-Instruct can enhance the ROUGE-L metric on the SuperNI (<strong class="oa fr">Super-Natural Instructions</strong>) dataset? For that, we can compare the results of 1) off-the-shelf pre-trained LMs without any instruction fine-tuning (Vanilla LMs), 2) Instruction-tuned models (Instruction-tuned w/o SuperNI), and 3) Instruction-tuned models trained on SuperNI (Instruction-tuned w/ SuperNI):</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div class="ms mt rh"><img src="../Images/998b6798d954b2297b79ba9b0c2aa56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*E93f1Vl80DW8OdQUeHgjtA.png"/></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Evaluation results on <strong class="bf my"><em class="ri">unseen</em></strong><em class="ri"> </em>tasks from SuperNI | Source: <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></figcaption></figure><p id="973b" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">As we can see, using Self-Instruct demonstrates a 33% absolute improvement over the original model on the dataset (1); simultaneously, it shows that using the framework can also slightly improve metrics after fine-tuning the SuperNI dataset (3).</p><p id="878c" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Moreover, if we create a new (=unseen) dataset of 252 instructions and 1 instance per instruction and evaluate a selection of instruction-tuned variants, we can see the following results:</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt rj"><img src="../Images/8a00ab05695c5026ef4cc894e16e4a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R8T4W825o7uxfEba7Q0zpA.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Performance of GPT3 model and its instruction-tuned variants, evaluated by human experts on our 252 user-oriented instructions | Source: <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></figcaption></figure><p id="5060" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">GPT3 + Self-Instruct shows impressive results compared to other instruction-tuned variants, but there is still a place for improvement compared to InstructGPT (previously available LLMs by OpenAI) variants.</p><h1 id="5124" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Enhancements</h1><p id="0a8c" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">The idea behind Self-Instruct is straightforward, but at the same time, it is compelling, so let’s look at how we can use it in different cases.</p><h2 id="16a6" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Stanford Alpaca³</h2><p id="070f" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">In 2023, Alpaca LLM from Stanford gained colossal interest due to affordability, accessibility, and the fact that it was developed for less than $600, and at the same time, it combined LLaMA and Self-Instruct ideas.</p><figure class="mj mk ml mm mn mi ms mt paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt rk"><img src="../Images/b4d952b864fe39083fd674bcca9c7f51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IQSBf1YeAdjwAsop3RT-UA.jpeg"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">High-level overview of Alpaca | Source: <a class="af nx" href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="noopener ugc nofollow" target="_blank">Alpaca: A Strong, Replicable Instruction-Following Model</a></figcaption></figure><p id="8396" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Alpaca’s version of Self-Instruct were slightly modified:</p><ul class=""><li id="e4be" class="ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot py pz qa bk">Step 1 (instruction generation): more aggressive batch decoding was applied, i.e., generating 20 instructions at once</li><li id="654b" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Step 2 (classification task): this step was wholly excluded</li><li id="3425" class="ny nz fq oa b go qb oc od gr qc of og oh qd oj ok ol qe on oo op qf or os ot py pz qa bk">Step 3 (instance generation): only one instance is generated per instruction</li></ul><p id="d0a2" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In the end, researchers from Stanford could achieve significant improvements in comparison to the initial set-up in Self-Instruct and based on performed a blind pairwise comparison between text-davinci-003 (InstructGPT-003) and Alpaca 7B: Alpaca wins 90 versus 89 comparisons against text-davinci-003.</p><h2 id="db10" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Self-Rewarding Language Models⁴</h2></div></div><div class="mi"><div class="ab cb"><div class="ll pq lm pr ln ps cf pt cg pu ci bh"><figure class="mj mk ml mm mn mi pw px paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="ms mt rl"><img src="../Images/0219851ef32d6d7ffd7ab7fa267ece14.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*ClUqt8CI35t_A5GP13yiQA.png"/></div></div><figcaption class="mp mq mr ms mt mu mv bf b bg z dx">Source: <a class="af nx" href="https://arxiv.org/pdf/2401.10020" rel="noopener ugc nofollow" target="_blank">Self-Rewarding Language Models</a></figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="eb03" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">In 2024, Self-Instruct is a practical framework used in more complex set-ups like in Self-Rewarding Language Models by Meta. As in Self-Instruct, initially, we have a seed set of human-written tasks; we then generate new instructions {xᵢ} and prompt model Mₜ to generate outputs {yᵢ¹, …, yᵢᵏ} and later generate rewards {rᵢ¹, …, rᵢᵏ } — that’s how we could ““eliminate”” human-annotators in InstructGPT by self-instruction process. The last block of Self-Rewarding models is instruction following training — on this step, we compose preference pairs and via DPO train Mₜ₊₁ — next iteration model. Therefore, we can repeat this procedure repeatedly to enrich the dataset and improve the initial pre-trained model.</p><h1 id="0026" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Exploring Limitations</h1><p id="1804" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">Although Self-Instruct offers an innovative approach to autonomous dataset generation, its reliance on large pre-trained models introduces potential limitations.</p><h2 id="e3b0" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Data quality</h2><p id="e5cf" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">Despite the impressive capability to generate synthetic data, the quality — marked by a 54% validity in the Overcoming Challenges section — remains a concern. It underscores a critical issue: the biases inherent in pre-trained models could replicate, or even amplify, within the generated datasets.</p><h2 id="517b" class="qg mx fq bf my qh qi qj nb qk ql qm ne oh qn qo qp ol qq qr qs op qt qu qv qw bk">Tail phenomena</h2><p id="692e" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">Instructions vary in frequency: some instructions are frequently requested, while others are rare. Nonetheless, it’s crucial to effectively manage these infrequent requests, as they highlight the brittleness of LLMs in processing uncommon and creative tasks.</p><h1 id="0e6d" class="mw mx fq bf my mz na gq nb nc nd gt ne nf ng nh ni nj nk nl nm nn no np nq nr bk">Conclusion</h1><p id="194b" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">In conclusion, the Self-Instruct framework represents an advancement in developing instruction-tuned LMs, offering an innovative solution to the challenges of dataset generation. Enabling LLMs to autonomously produce diverse and high-quality data significantly reduces dependency on human annotators, therefore driving down costs.</p></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="0126" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">Unless otherwise noted, all images are by the author, inspired by <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-Instruct</a> :)</p></div></div></div><div class="ab cb pd pe pf pg" role="separator"><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj pk"/><span class="ph by bm pi pj"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="4927" class="mw mx fq bf my mz pl gq nb nc pm gt ne nf pn nh ni nj po nl nm nn pp np nq nr bk"><strong class="al">References:</strong></h1><p id="fb6a" class="pw-post-body-paragraph ny nz fq oa b go qz oc od gr ra of og oh rb oj ok ol rc on oo op rd or os ot fj bk">[1] Ouyang, Long, et al. “<a class="af nx" href="https://arxiv.org/pdf/2203.02155" rel="noopener ugc nofollow" target="_blank">Training language models to follow instructions with human feedback</a>.” <em class="rm">Advances in Neural Information Processing Systems</em> 35 (2022): 27730–27744</p><p id="2cb8" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[2] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.A., Khashabi, D. and Hajishirzi, H., 2022. <a class="af nx" href="https://arxiv.org/pdf/2212.10560" rel="noopener ugc nofollow" target="_blank">Self-instruct: Aligning language model with self generated instructions</a>. <em class="rm">arXiv preprint arXiv:2212.10560</em>.</p><p id="8816" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[3] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P. and Hashimoto, T.B., 2023. <a class="af nx" href="https://crfm.stanford.edu/2023/03/13/alpaca.html" rel="noopener ugc nofollow" target="_blank">Stanford alpaca: An instruction-following llama model</a>.</p><p id="8a76" class="pw-post-body-paragraph ny nz fq oa b go ob oc od gr oe of og oh oi oj ok ol om on oo op oq or os ot fj bk">[4] Yuan, W., Pang, R.Y., Cho, K., Sukhbaatar, S., Xu, J. and Weston, J., 2024. <a class="af nx" href="https://arxiv.org/pdf/2401.10020" rel="noopener ugc nofollow" target="_blank">Self-rewarding language models</a>. <em class="rm">arXiv preprint arXiv:2401.10020</em>.</p></div></div></div></div>    
</body>
</html>