<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Line-By-Line, Let’s Reproduce GPT-2: Section 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Line-By-Line, Let’s Reproduce GPT-2: Section 1</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23">https://towardsdatascience.com/line-by-line-lets-reproduce-gpt-2-section-1-b26684f98492?source=collection_archive---------8-----------------------#2024-07-23</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="2849" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post will go line-by-line through the code in Section 1 of Andrej Karpathy’s “Let’s reproduce GPT-2 (124M)”</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Gunton" class="l ep by dd de cx" src="../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F8sHS2ai6w95qbGIZ9qM_g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mgunton7?source=post_page---byline--b26684f98492--------------------------------" rel="noopener follow">Matthew Gunton</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--b26684f98492--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jul 23, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp mq"><img src="../Images/6eaaa728ea0ae6b47ebb70bbe5b558fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rSIqu7ZQVFjHFL66tQgCjA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author — SDXL</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="a929" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Andrej Karpathy is one of the foremost Artificial Intelligence (AI) researchers out there. He is a founding member of OpenAI, previously led AI at Tesla, and continues to be at the forefront of the AI community. He recently released an incredible <a class="af oe" href="https://www.youtube.com/watch?v=l8pRSuU81PU" rel="noopener ugc nofollow" target="_blank">4 hour video walking through how to build a high-quality LLM model from scratch</a>.</p><p id="ca15" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In that video, we go through all of the major parts of training an LLM, from coding the architecture to speeding up its training time to adjusting the hyperparameters for better results. There’s an incredible amount of knowledge there, so I wanted to expand upon it by going line-by-line through the code Karpathy creates and explaining how it is working. This blog post will be part of a series I do covering each section of Karpathy’s video.</p><p id="3b73" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In section one, we focus on implementing the architecture of GPT-2. While GPT-2 was open-sourced by OpenAI in 2018, it was written in Tensor Flow, which is a harder framework to debug than PyTorch. Consequently, we are going to recreate GPT-2 using more commonly used tools. Using only the code we are going to create today, you can create a LLM of your own!</p><p id="9e67" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Let’s dive in!</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="b9f5" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">High Level Vocabulary</h1><p id="26ca" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Before we begin, let’s get on the same page about some terminology. While there may be some naming collisions with other sources, I’ll try to be consistent within these blog posts.</p><p id="77db" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><em class="po">Block Size</em> — tells us how many positions in the input length our Transformer can process. Once you go over this limit, performance degrades as you have to wrap around (y<a class="af oe" rel="noopener" target="_blank" href="/understanding-long-rope-in-llms-29337dc7e4a9">ou can learn more about how we expand this without training a new model from scratch in my Long RoPE Blog</a>)</p><p id="1b48" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><em class="po">Vocabulary Size</em> — tells us how many unique tokens the model will be able to understand and use. In general, researchers have found that larger vocabulary sizes allow models to be more precise with their language and to capture more nuances in their responses.</p><p id="b8d5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><em class="po">Layer — </em>part of the hidden layers of our neural network. Specifically here we refer to how many times we repeat the calculations shown in the grey box below:</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pp"><img src="../Images/eaf2574408a9f1c397a50330b3e663ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqFowMqfFS-BoKzsq5qMzA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">A layer in our model from <a class="af oe" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a></figcaption></figure><p id="b6e1" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><em class="po">Embedding</em> — a vector representation of data we pass to the model.</p><p id="688b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk"><em class="po">Multi-Head Attention</em> — rather than running attention once, we run it n-times and then concatenate all of the results together to get the final result.</p><p id="58f7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Let’s go into the code!</p><h1 id="2013" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">GPT Class &amp; Its Parameters</h1><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="d462" class="pz oo fq pw b bg qa qb l qc qd">@dataclass<br/>class GPTConfig:<br/>    block_size : int = 1024<br/>    vocab_size : int = 50257<br/>    n_layer : int = 12<br/>    n_head : int = 12<br/>    n_embd : int = 768</span></pre><p id="6880" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To begin, we are setting 5 hyper-parameters in the GPTConfig class. <code class="cx qe qf qg pw b">block_size</code> appears to be somewhat arbitrary along with <code class="cx qe qf qg pw b">n_layer</code>and <code class="cx qe qf qg pw b">n_head</code>. Put differently, these values were chosen empirically based on what the researchers saw had the best performance. Moreover, we choose 786 for <code class="cx qe qf qg pw b">n_embd</code> as this is the value chosen for the GPT-2 paper, which we’ve decided to emulate.</p><p id="b699" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">However, <code class="cx qe qf qg pw b">vocab_size</code> is set based off the <code class="cx qe qf qg pw b">tiktoken</code> gpt-2 tokenizer that we will use. The GPT-2 tokenizer was created by using the <em class="po">Byte-Pair Encoding</em> algorithm (<a class="af oe" href="https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/" rel="noopener ugc nofollow" target="_blank">read more here</a>). This starts off with an initial set of vocab (in our case 256) and then goes through the training data creating new vocab based on the frequency it sees the new vocabulary appearing in the training set. It keeps doing this until it has hit a limit (in our case 50,000). Finally, we have vocab set aside for internal use (in our case the end token character). Adding these up we get 50,257.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="4ef6" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/>    def __init__(self, config):<br/>        super().__init__()<br/>        self.config = config<br/><br/>    # ...</span></pre><p id="9f64" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">With our configs set, we create a GPT class which is an instance of the torch <code class="cx qe qf qg pw b">nn.Module</code> class. This is the base class for all PyTorch neural networks, and so by using this we get access to all of the optimizations that PyTorch has for these types of models. Each <code class="cx qe qf qg pw b">nn.Module</code> will have a <code class="cx qe qf qg pw b">forward</code> function that defines what happens during a forward pass of the model (more on these in a moment).</p><p id="d0c5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We begin by running the super constructor in the base class and then create a <code class="cx qe qf qg pw b">transformer</code> object as a <code class="cx qe qf qg pw b">ModuleDict</code>. This was created because it allows us to index into <code class="cx qe qf qg pw b">transformer</code> like an object, which will come in handy both when we want to load in weights from HuggingFace and when we want to debug and quickly go through our model.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="7043" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/>    def __init__(self, config):<br/>        # ...<br/><br/>        self.transformer = nn.ModuleDict(dict(<br/>            wte = nn.Embedding(config.vocab_size, config.n_embd),<br/>            wpe = nn.Embedding(config.block_size, config.n_embd),<br/>            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),<br/>            ln_f = nn.LayerNorm(config.n_embd)<br/>        ))<br/></span></pre><p id="3cf0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Our <code class="cx qe qf qg pw b">transformer</code> here has 4 major pieces we are going to load in: the weights of the token embeddings (<code class="cx qe qf qg pw b">wte</code>), the weights of the positional encodings (<code class="cx qe qf qg pw b">wpe</code>), the hidden layers (<code class="cx qe qf qg pw b">h</code>), and the layer normalization (<code class="cx qe qf qg pw b">ln_f</code>). This setup is following <em class="po">mostly</em> the decoder part of the Transformer architecture from <a class="af oe" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a> (output embeddings ~ <code class="cx qe qf qg pw b">wte</code>, positional encoding ~ <code class="cx qe qf qg pw b">wte</code>, hidden layers ~<code class="cx qe qf qg pw b">h</code> ). One key difference is that we have an additional normalization layer <code class="cx qe qf qg pw b">ln_f</code> done after all of the hidden layers have finished in our architecture.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div class="mo mp qh"><img src="../Images/a919d91bb480f5f3fede3f15e3f10fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*lMeDaPhr2mZ4qP_R_qwa0Q.png"/></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Decoder Half of the Architecture shown in <a class="af oe" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a></figcaption></figure><p id="4e4b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">The <code class="cx qe qf qg pw b">wte</code> and the <code class="cx qe qf qg pw b">wpe</code> are both embeddings so naturally we use the <code class="cx qe qf qg pw b">nn.Embedding</code> class to represent them. Our hidden layers are where we will have most of the logic for the Transformer, so I will go into this more later. For now, just note that we are creating a loop of the object <code class="cx qe qf qg pw b">Block</code> so that we have <code class="cx qe qf qg pw b">n.layer</code>‘s of them. Finally, we use the built-in <code class="cx qe qf qg pw b">nn.LayerNorm</code> for <code class="cx qe qf qg pw b">ln_f</code> , which will normalize our output based on the equation below (where x and y are input and output, E[x] is the mean value, and γ and β are learnable weights).</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qi"><img src="../Images/5a98c158cb71d90f31987dcb5e82662e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PpjzF7VrTcIOJDKv2AOwQ.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Equation for <a class="af oe" href="https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html" rel="noopener ugc nofollow" target="_blank">Layer Normalization in PyTorch</a></figcaption></figure><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="e3bd" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/>    def __init__(self, config):<br/>        # ...<br/>        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)<br/><br/>        # weight sharing scheme<br/>        self.transformer.wte.weight = self.lm_head.weight<br/><br/>        # initalize weights<br/>        self.apply(self._init_weights)</span></pre><p id="9347" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Next, we setup the final linear layer of our network which will generate the logits of the model. Here we are projecting from the embedding dimension of our model (768) to the vocabulary size of our model (50,257). The idea here is that we have taken the hidden state and expanded it to map onto our vocabulary so that our decoder head can use the values on each vocab to figure out what the next token should be.</p><p id="666f" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Finally in our constructor, we have an interesting optimization where we tell the model to make the tokenizer weights the same as the linear layer weights. This is done because we want the linear layer and the tokenizer to have the same understanding of the tokens (if two tokens are similar when being input into the model, the same two tokens should be similar when being output by the model). Finally, we initialize the weights for the model so we can start training.</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="e8cf" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/># ...<br/>    def forward(self, idx, targets=None):<br/>        B, T = idx.size() <br/>        assert T &lt;= self.config.block_size, f"maximum sequence length breached"<br/>        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)<br/>        pos_emb = self.transformer.wpe(pos)<br/>        tok_emb = self.transformer.wte(idx)<br/><br/>        x = tok_emb + pos_emb # hidden broadcast<br/><br/>        for block in self.transformer.h:<br/>            x = block(x)<br/>        x = self.transformer.ln_f(x)<br/>        logits = self.lm_head(x)<br/>        loss = None<br/>        if targets is not None:<br/>            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))<br/>        return logits, loss</span></pre><p id="d14d" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Our forward function is where we lay out exactly how our model will behave during a forward pass. We start off by verifying that our sequence length is not greater than our configured max value (<code class="cx qe qf qg pw b">block_size</code>). Once that’s true, we create a tensor with values of 0 to T-1 (for example if T = 4, we’d have tensor([0, 1, 2, 3]) and run them through our positional embedding weights. Once that’s complete, we run the input tensor through the token embedding weights.</p><p id="50ac" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We combine both the token and the positional embeddings into <code class="cx qe qf qg pw b">x</code>, requiring a broadcast to combine them. As the <code class="cx qe qf qg pw b">tok_emb</code> are bigger than the <code class="cx qe qf qg pw b">pos_emb</code> (in our example 50257 vs 1024), x will have the dimensions of <code class="cx qe qf qg pw b">tok_emb</code> . <code class="cx qe qf qg pw b">x</code> is now our hidden state, which we will pass through the hidden layers via the for loop. We are careful to update <code class="cx qe qf qg pw b">x</code> after each time through a Block.</p><p id="4035" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Next, we normalize <code class="cx qe qf qg pw b">x</code> via our LayerNormalization <code class="cx qe qf qg pw b">ln_f</code> and then do our linear projection to get the logits necessary to predict the next token. If we are training the model (which we signal via the <code class="cx qe qf qg pw b">targets</code> parameter), we will then compute cross entropy between the logits we have just produced and the ground truth values held in our <code class="cx qe qf qg pw b">targets</code> variable. We accomplish this via our <code class="cx qe qf qg pw b">cross_entropy</code> loss function. To do this right, we need to convert our <code class="cx qe qf qg pw b">logits</code> and <code class="cx qe qf qg pw b">target</code> to the right shape via <code class="cx qe qf qg pw b">.view()</code>. We ask pytorch to infer the correct size when we pass through -1.</p><p id="0dcd" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">There’s one more function in this class, the initialization function, but we’ll get to the initialization logic a little later. For now, let’s dive into the Block logic that will help us implement our multi-head attention and MLPs.</p><h1 id="dd3c" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Block Class</h1><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="32f3" class="pz oo fq pw b bg qa qb l qc qd">class Block(nn.Module):<br/>    def __init__(self, config):<br/>        super().__init__()<br/>        self.ln_1 = nn.LayerNorm(config.n_embd)<br/>        self.attn = CausalSelfAttention(config)<br/>        self.ln_2 = nn.LayerNorm(config.n_embd)<br/>        self.mlp = MLP(config)<br/># ...</span></pre><p id="f0fe" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Block is instantiated as a <code class="cx qe qf qg pw b">nn.Module</code> , so we also call the super constructor at the beginning for its optimizations. Next, we setup the same calculations as set out in the <a class="af oe" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a> paper — 2 layer normalizations, an attention calculation, and a feed forward layer via MLPs.</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp pp"><img src="../Images/eaf2574408a9f1c397a50330b3e663ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqFowMqfFS-BoKzsq5qMzA.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">A Hidden Layer from <a class="af oe" href="https://arxiv.org/pdf/1706.03762" rel="noopener ugc nofollow" target="_blank">“Attention is All You Need”</a></figcaption></figure><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="001b" class="pz oo fq pw b bg qa qb l qc qd">class Block(nn.Module):<br/># ...<br/>    def forward(self, x):<br/>        x = x + self.attn(self.ln_1(x))<br/>        x = x + self.mlp(self.ln_2(x))<br/>        return x</span></pre><p id="204a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We then define our <code class="cx qe qf qg pw b">forward</code> function which PyTorch will call for every forward pass of the model. Note that this is where we do something different than Attention is All You Need. We setup the layer normalizations to happen before attention and the feedforward respectively. This is part of the insights from GPT-2 paper, and you can see how making little changes like this can make a big difference. Note the addition to the original tensor remains in the corresponding same position. These 2 additions will be important when we setup our weight initialization function.</p><p id="254e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This class is a nice abstraction, as it lets us swap out implementations of attention or choose another type of feed forward function other than MLP without having to majorly refactor the code.</p><h1 id="da6e" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">CausalSelfAttention Class</h1><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="3e71" class="pz oo fq pw b bg qa qb l qc qd">class CausalSelfAttention(nn.Module):<br/>    def __init__(self, config):<br/>        super().__init__()<br/>        assert config.n_embd % config.n_head == 0<br/>        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)<br/>        self.c_proj = nn.Linear(config.n_embd, config.n_embd)<br/>        self.c_proj.NANOGPT_SCALE_INIT = 1<br/>        self.n_head = config.n_head<br/>        self.n_embd = config.n_embd<br/>        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size))<br/>                                    .view(1,1, config.block_size, config.block_size))<br/># ...</span></pre><p id="2c75" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Attention is an important part of our model, so naturally there are a number of configurations here. We have the assert statement as a debugging tool to make sure that the configuration dimensions we pass through are compatible. Then we create some helper functions that will assist us when we do our self-attention. First, we have our <code class="cx qe qf qg pw b">c_attn</code> and <code class="cx qe qf qg pw b">c_proj</code> which are linear projections that convert our hidden state into new dimensions needed for the attention calculation. The <code class="cx qe qf qg pw b">c_proj.NANOGPT_SCALE_INIT</code> is a flag we set here and in the MLP that will help us with the weight initialization later (in truth this could be named anything).</p><p id="947a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Finally, we tell torch to create a buffer that will not be updated during training called <code class="cx qe qf qg pw b">bias</code>. Bias will be a lower triangular matrix of dimensions <code class="cx qe qf qg pw b">block_size</code> x <code class="cx qe qf qg pw b">block_size</code> that we will then turn into a 4D tensor with dimensions 1 x 1 x <code class="cx qe qf qg pw b">block_size</code> x <code class="cx qe qf qg pw b">block_size</code> . The 1 x 1 is done so that we can compute these in a batch in a single channel. This buffer will be used to apply a mask on our multi-headed attention.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="ceb6" class="pz oo fq pw b bg qa qb l qc qd">class CausalSelfAttention(nn.Module):<br/># ...<br/>    def forward(self, x):<br/>        B, T, C = x.size() # batch size, sequence length, channels<br/>        qkv = self.c_attn(x)<br/>        q, k, v = qkv.split(self.n_embd, dim=2)<br/>        # transpose is done for efficiency optimization<br/>        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)<br/>        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)<br/>        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)<br/><br/>        att = (q @ k.transpose(-2,-1)) * (1.0 / math.sqrt(k.size(-1)))<br/>        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float("-inf"))<br/>        att = F.softmax(att, dim=-1)<br/>        y = att @ v<br/>        y = y.transpose(1,2).contiguous().view(B, T, C)<br/>        <br/>        y = self.c_proj(y)<br/>        return y</span></pre><p id="6870" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now comes the implementation of attention, with a focus on making this performant in torch. Going line by line, we begin by finding the batch size, sequence length, and channels in our input tensor x. We then will call our <code class="cx qe qf qg pw b">c_attn</code> from before to project our hidden state into the dimensions we’ll need. We then split that result into 3 tensors of (B, T, C) shape (specifically one for query, one for key, and one for value).</p><p id="8e2c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We then adjust the dimensions of q, k, and v so that we can do multi-head attention on these performantly. By changing the dimensions from (B, T, C) to (B, T, self.n_head, C // self.n_head), we are dividing up the data so that each head gets its own unique data to operate on. We transpose our view so that we can make <code class="cx qe qf qg pw b">T</code> the third dimension and <code class="cx qe qf qg pw b">self.n_head</code> the second dimension, allowing us to more easily concatenate the heads.</p></div></div><div class="mi"><div class="ab cb"><div class="ll mj lm mk ln ml cf mm cg mn ci bh"><figure class="mr ms mt mu mv mi mw mx paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qj"><img src="../Images/bb9433415775d70b3ab32f88ec34a6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*lReWUi9cBRCJrdUqJtdYIw.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Attention equation from “<a class="af oe" href="https://youtu.be/l8pRSuU81PU?feature=shared" rel="noopener ugc nofollow" target="_blank">Attention is All You Need</a>”</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="2a1e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now that we have our values, we can start to calculate. We perform a matrix multiplication between query and key (making sure to transpose key so that it is in the proper direction), then divide by the square root of the size of k. After this calculation, we then apply the bias from our register so that the attention data from tokens in the future cannot impact tokens in the present (hence why we apply the mask only for tokens greater than T for the time and channel dimension). Once that is complete, we apply the softmax to only pass through certain information through.</p><p id="020c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Once the mask is on, we multiply the values by v, and then transpose our values back to (B, T, self.n_head, C // self.n_head) setup. We call <code class="cx qe qf qg pw b">.contiguous()</code> to ensure that in memory all of the data is laid out next to each other, and finally convert our tensor back to the (B, T, C) dimensions it came in with (thus, concatenating our attention heads in this step).</p><p id="fde0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Finally, we use our linear projection <code class="cx qe qf qg pw b">c_proj</code> to convert back to the original dimensions of the hidden state.</p><h1 id="0e93" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">MLP Class</h1><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="b93f" class="pz oo fq pw b bg qa qb l qc qd">class MLP(nn.Module):<br/>    def __init__(self, config):<br/>        super().__init__()<br/>        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)<br/>        self.gelu = nn.GELU(approximate="tanh")<br/>        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)<br/>        self.c_proj.NANOGPT_SCALE_INIT = 1<br/># ...</span></pre><p id="7fb9" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Like all the classes before, MLP inherits from <code class="cx qe qf qg pw b">nn.Module</code>. We begin by setting some helper functions — specifically the <code class="cx qe qf qg pw b">c_fc</code> and <code class="cx qe qf qg pw b">c_proj</code> linear projection layers, expanding from our embedding to 4 times the size and then back again respectively. Next, we have GELU. Karpathy makes a point to say that the approximate parameter here is only set so that we can closely match the GPT-2 paper. While at the time, the approximation of GELU was necessary, now a days we no longer need to approximate — we can calculate precisely.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="747f" class="pz oo fq pw b bg qa qb l qc qd">class MLP(nn.Module):<br/># ...<br/>    def forward(self, x):<br/>        x = self.c_fc(x)<br/>        x = self.gelu(x)<br/>        x = self.c_proj(x)<br/>        return x</span></pre><p id="2634" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Our forward pass then is relatively straight forward. We call each function on our input tensor and return the final result.</p><h1 id="bba5" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Hugging Face Connection Code</h1><p id="4f3a" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Because GPT-2 is open-source, it is available on Hugging Face. While our goal here is to train our own model, it is nice to be able to compare what our results will be with the ones OpenAI found in their training. To allow us to do so, we have the below function that pulls in the weights and populates them into our <code class="cx qe qf qg pw b">GPT</code> class.</p><p id="693e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">This code also allows us to reuse this code to pull in foundation models from Hugging Face and fine-tune them (<em class="po">with some modifications as right now it’s optimized only for gpt-2</em>).</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="cdb2" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/># ...<br/>    @classmethod<br/>    def from_pretrained(cls, model_type):<br/>        """Loads pretrained GPT-2 model weights from huggingface"""<br/>        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}<br/>        from transformers import GPT2LMHeadModel<br/>        print("loading weights from pretrained gpt: %s" % model_type)<br/><br/>        # n_layer, n_head and n_embd are determined from model_type<br/>        config_args = {<br/>            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params<br/>            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params<br/>            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params<br/>            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params<br/>        }[model_type]<br/>        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints<br/>        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints<br/>        # create a from-scratch initialized minGPT model<br/>        config = GPTConfig(**config_args)<br/>        model = GPT(config)<br/>        sd = model.state_dict()<br/>        sd_keys = sd.keys()<br/>        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param<br/># ...</span></pre><p id="a979" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Starting from the top, we bring in HuggingFace’s <code class="cx qe qf qg pw b">transformers</code> library and setup the hyperparameters that vary between different variants of the GPT-2 model. As the <code class="cx qe qf qg pw b">vocab_size</code> and <code class="cx qe qf qg pw b">block_size</code> don’t change, you can see we hard-code them in. We then pass these variables into the <code class="cx qe qf qg pw b">GPTConfig</code> class from before, and then instantiate the model object (<code class="cx qe qf qg pw b">GPT</code>). Finally, we remove all keys from the model that end with <code class="cx qe qf qg pw b">.attn.bias</code> , as these are not weights, but rather the register we setup to help with our attention function before.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="3638" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/># ...<br/>    @classmethod<br/>    def from_pretrained(cls, model_type):<br/># ...<br/>        model_hf = GPT2LMHeadModel.from_pretrained(model_type)<br/>        sd_hf = model_hf.state_dict()<br/><br/>        # copy while ensuring all of the parameters are aligned and match in names and shapes<br/>        sd_keys_hf = sd_hf.keys()<br/>        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer<br/>        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)<br/>        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']<br/>        # basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear<br/>        # this means that we have to transpose these weights when we import them<br/>        assert len(sd_keys_hf) == len(sd_keys), f"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}"</span></pre><p id="b527" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Next, we load in the model from the HuggingFace class <code class="cx qe qf qg pw b">GPT2LMHeadModel</code>. We take the keys out from this model and likewise ignore the <code class="cx qe qf qg pw b">attn.masked_bias</code> and <code class="cx qe qf qg pw b">attn.bias</code> keys. We then have an assert to make sure that we have the same number of keys in the hugging face model as we do in our model.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="d7df" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/># ...<br/>    @classmethod<br/>    def from_pretrained(cls, model_type):<br/># ...<br/>        for k in sd_keys_hf:<br/>            if any(k.endswith(w) for w in transposed):<br/>                # special treatment for the Conv1D weights we need to transpose<br/>                assert sd_hf[k].shape[::-1] == sd[k].shape<br/>                with torch.no_grad():<br/>                    sd[k].copy_(sd_hf[k].t())<br/>            else:<br/>                # vanilla copy over the other parameters<br/>                assert sd_hf[k].shape == sd[k].shape<br/>                with torch.no_grad():<br/>                    sd[k].copy_(sd_hf[k])<br/><br/>        return model</span></pre><p id="80b9" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">To round out the function, we loop through every key in the Hugging Face model and add its weights to the corresponding key in our model. There are certain keys that need to be manipulated so that they fit the data structure we’re using. We run the function <code class="cx qe qf qg pw b">.t()</code> to transpose the hugging face matrix into the dimensions we need. For the rest, we copy them over directly. You’ll notice we are using <code class="cx qe qf qg pw b">torch.no_grad()</code> . This is telling torch that it doesn’t need to cache the values for a backward propagation of the model, another optimization to make this run faster.</p><h1 id="5e79" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Generating Our First Predictions (Sampling Loop)</h1><p id="0bd4" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">With the classes we have now, we can run the model and have it give us output tokens (<em class="po">just make sure if you’re following this sequentially that you comment out the _init_weights call in the GPT constructor</em>). The below code shows how we would do that.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="68fc" class="pz oo fq pw b bg qa qb l qc qd">device = "cpu"<br/>if torch.cuda.is_available():<br/>    device = "cuda"<br/>elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():<br/>    device = "mps"<br/>print(f"device {device}")<br/><br/>torch.manual_seed(1337)<br/><br/>model = GPT(GPTConfig())<br/>model.eval()<br/>model.to(device)</span></pre><p id="7ba0" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We start off by determining what devices we have access to. Cuda is NVIDIA’s platform that runs extremely fast GPU calculations, so if we have access to chips that use CUDA we will use them. If we don’t have access but we’re on Apple Silicon, then we will use that. Finally, if we have neither, then we fall back to CPU (this will be the slowest, but every computer has one so we know we can still train on it).</p><p id="07d5" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Then, we instantiate our model using the default configurations, and put the model into ‘<code class="cx qe qf qg pw b">eval</code>’ mode — (this does a number of things, like disabling dropout, but from a high level it makes sure that our model is more consistent during inferencing). Once set, we move the model onto our device. Note that if we wanted to use the HuggingFace weights instead of our training weights, we would modify the third-to-last-line to read: <code class="cx qe qf qg pw b">model = GPT.from_pretrained(‘gpt2’)</code></p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="3f6d" class="pz oo fq pw b bg qa qb l qc qd">import tiktoken<br/>enc = tiktoken.get_encoding('gpt2')<br/>tokens = enc.encode("Hello, I'm a language model,")<br/>tokens = torch.tensor(tokens, dtype=torch.long)<br/>tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)<br/>x = tokens.to(device)</span></pre><p id="4f9f" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We now bring in <code class="cx qe qf qg pw b">tiktoken</code> using the gpt2 encodings and have it tokenize our prompt. We take these tokens and put them into a tensor, which we then convert to batches in the below line. <code class="cx qe qf qg pw b">unsqueeze()</code> will add a new first dimension of size 1 to the tensor, and <code class="cx qe qf qg pw b">repeat</code> will repeat the entire tensor <code class="cx qe qf qg pw b">num_return_sequences</code> times within the first dimension and once within the second dimension. What we’ve done here is formatted our data to fit the batched schema our model is expecting. Specifically we now match the (B, T) format: <code class="cx qe qf qg pw b">num_return_sequences</code> x encoded length of prompt. Once we pass through the input tensor into the beginning of the model, our <code class="cx qe qf qg pw b">wte</code> and <code class="cx qe qf qg pw b">wpe</code> will create the C dimension.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="e588" class="pz oo fq pw b bg qa qb l qc qd">while x.size(1) &lt; max_length:<br/>    with torch.no_grad():<br/>        logits, _ = model(x)<br/>        logits = logits[:, -1, :]<br/>        probs = F.softmax(logits, dim=-1)<br/>        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)<br/>        ix = torch.multinomial(topk_probs, 1)<br/>        xcol = torch.gather(topk_indices, -1, ix)<br/>        x = torch.cat((x, xcol), dim=1)</span></pre><p id="7f7a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now that they’re ready, we send them to the device and begin our sampling loop. The loop will be exclusively a forward pass, so we wrap it in the <code class="cx qe qf qg pw b">torch.no_grad</code> to stop it from caching for any backward propagation. Our logits come out with shape (batch_size, seq_len, vocab_size) — (B,T,C) with C coming after a forward pass of the model.</p><p id="fa32" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We only need the last item in the sequence to predict the next token, so we pull out <code class="cx qe qf qg pw b">[:, -1, :]</code> We then take those logits and run it through a <code class="cx qe qf qg pw b">softmax</code> to get the token probabilities. Taking the top 50, we then choose a random index of the top 50 and pick that one as our predicted token. We then get the information about that and add it to our tensor <code class="cx qe qf qg pw b">x</code>. By concatenating <code class="cx qe qf qg pw b">xcol</code> to <code class="cx qe qf qg pw b">x</code>, we set ourselves up to go into the next token given what we just predicted. This is how we code up autoregression.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="f695" class="pz oo fq pw b bg qa qb l qc qd">for i in range(num_return_sequences):<br/>    tokens = x[i, :max_length].tolist()<br/>    decoded = enc.decode(tokens)<br/>    print(f"&gt;&gt; {decoded}")</span></pre><p id="e2be" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">After the sampling loop is done, we can go through each of the selected tokens and decode them, showing the response to the user. We grab data from the <code class="cx qe qf qg pw b">i</code>-th in our batch and decode it to get the next token.</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c019" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If you run the sampling loop on our initial model, you will notice that the output leaves a lot to be desired. This is because we haven’t trained any of the weights. The next few classes show how we can begin a naive training of the model.</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="1f2a" class="on oo fq bf op oq or gq os ot ou gt ov ow ox oy oz pa pb pc pd pe pf pg ph pi bk">DataLoaderLite</h1><p id="a3df" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">All training requires high quality data. For Karpathy’s videos, he likes to use public domain Shakespeare text (<a class="af oe" href="https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt" rel="noopener ugc nofollow" target="_blank">find it here</a>).</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="16b2" class="pz oo fq pw b bg qa qb l qc qd">class DataLoaderLite:<br/>    def __init__(self, B, T):<br/>        self.B = B<br/>        self.T = T<br/>    <br/>        with open('shakespeare.txt', "r") as f:<br/>            text = f.read()<br/>        <br/>        enc = tiktoken.get_encoding('gpt2')<br/>        tokens = enc.encode(text)<br/>        self.tokens = torch.tensor(tokens)<br/>        print(f"1 epoch = {len(self.tokens) // B * T} batches")<br/><br/>        self.current_position = 0</span></pre><p id="118e" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We begin by simply opening the file and reading in the text. This data source is ASCII only, so we don’t need to worry about any unexpected binary characters. We use <code class="cx qe qf qg pw b">tiktoken</code> to get the encodings for the body, and then convert these tokens into a tensor. We then create a variable called <code class="cx qe qf qg pw b">current_position</code>, which will let us know where in the token tensor we are currently training from (naturally, this is initialized to the beginning). Note, this class is not inheriting from <code class="cx qe qf qg pw b">nn.Module</code>, mainly because we have no need for the <code class="cx qe qf qg pw b">forward</code> function here. Just as with the prompt part of the sampling loop, our DataLoaderLite class only needs to generate tensors of shape (B, T).</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="d76e" class="pz oo fq pw b bg qa qb l qc qd">class DataLoaderLite:<br/># ...<br/>    def next_batch(self):<br/>        B, T = self.B, self.T<br/>        buf = self.tokens[self.current_position: self.current_position+(B*T + 1)]<br/>        x = (buf[:-1]).view(B, T)<br/>        y = (buf[1:]).view(B,T)<br/><br/>        self.current_position += B * T<br/>        if self.current_position + (B*T+1) &gt; len(self.tokens):<br/>            self.current_position = 0<br/>        return x,y</span></pre><p id="464a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">In the above we define the function <code class="cx qe qf qg pw b">next_batch</code> to help with training. To make programs run faster, we like to run the calculations in batches. We use the B and T fields to determine the batch size (B) and sequence length (T) we’ll be training on. Using these variables, we create a buffer that holds the tokens we are going to train with, setting the dimensions to be of rows B and columns T. Note that we read from <code class="cx qe qf qg pw b">current_position</code> to <code class="cx qe qf qg pw b">current_position + (B*T + 1)</code> , where the +1 is to make sure we have all of the ground truth values for our <code class="cx qe qf qg pw b">B*T</code> batch.</p><p id="893a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We then setup our model input (<code class="cx qe qf qg pw b">x</code>) and our expected output (<code class="cx qe qf qg pw b">y</code>) along the same lines. <code class="cx qe qf qg pw b">x</code> is the entire buffer except for the last character, and <code class="cx qe qf qg pw b">y</code> is the entire buffer except for the first. The basic idea is that given the first value in token buffer, we expect to get back the second token in the token buffer from our model.</p><p id="d8d6" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Finally, we update the <code class="cx qe qf qg pw b">current_position</code> and return <code class="cx qe qf qg pw b">x</code> and <code class="cx qe qf qg pw b">y</code>.</p><h1 id="8090" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Weight Initialization</h1><p id="b5a1" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">As we are dealing with probabilities, we’d like to pick initial values for our weights that are likely to require fewer epochs to get right. Our <code class="cx qe qf qg pw b">_init_weights</code> function helps us do so, by initializing the weights with either zeroes or with a normal distribution.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="a52b" class="pz oo fq pw b bg qa qb l qc qd">class GPT(nn.Module):<br/># ...<br/>    def _init_weights(self, module):<br/>        # layer norm is by default set to what we want, no need to adjust it<br/>        if isinstance(module, nn.Linear):<br/>            std = 0.02<br/>            if hasattr(module, "NANOGPT_SCALE_INIT"):<br/>                std *= (2 * self.config.n_layer) ** -0.5 # 2 * for 2 additions (attention &amp; mlp)<br/>            torch.nn.init.normal_(module.weight, mean=0.0, std=std)<br/>            # reasonable values are set based off a certain equation<br/>            if module.bias is not None:<br/>                torch.nn.init.zeros_(module.bias)<br/>        elif isinstance(module, nn.Embedding):<br/>            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 )</span></pre><p id="c55a" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If you remember from before, we’re passing in every field of the GPT class into <code class="cx qe qf qg pw b">_init_weights</code>, so we’re processing <code class="cx qe qf qg pw b">nn.Module</code>s. We are using the Xavier method to initialize our weights, which means we set the standard deviation of our sampling distribution equal to <code class="cx qe qf qg pw b">1 / sqrt(hidden_layers)</code> . You will notice that in the code, we are often using the hardcoded 0.02 as the standard deviation. While this might seem arbitrary, from the below table you can see that as the hidden dimensions GPT-2 uses are all roughly 0.02, this is a fine-approximation.</p><figure class="mr ms mt mu mv mi"><div class="qk io l ed"><div class="ql qm l"/></div></figure><p id="657b" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Going through the code, we start off by checking which subtype of <code class="cx qe qf qg pw b">nn.Module</code> the module we’re operating on is.</p><p id="6078" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If the module is Linear, then we will check if it is one of our projections from <code class="cx qe qf qg pw b">MLP</code> or <code class="cx qe qf qg pw b">CasualSelfAttention</code> classes (by checking if it has the <code class="cx qe qf qg pw b">NANO_GPT_INIT</code> flag set). If it is, then our 0.02 approximation won’t work because the number of hidden layers in these modules is increasing (this is a function of our addition of the tensors in the <code class="cx qe qf qg pw b">Block</code> class). Consequently, the GPT-2 paper uses a scaling function to account for this: <code class="cx qe qf qg pw b">1/sqrt(2 * self.config.n_layer)</code>. The <code class="cx qe qf qg pw b">2*</code> is because our <code class="cx qe qf qg pw b">Block</code> has 2 places where we are adding the tensors.</p><p id="8ddc" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If we have a bias in the Linear module, we will start by initializing these all to zero.</p><p id="67c7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If we have an <code class="cx qe qf qg pw b">Embedding</code> Module (like the Token or Positional Encoding pieces), we will initialize this with the same normal distribution with standard deviation of 0.02.</p><p id="58fd" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">If you remember, we have another subtype of module that is in our model: <code class="cx qe qf qg pw b">nn.LayerNorm</code> . This class already is initialized with a normal distribution and so we decide that this is good enough to not need any changes.</p><h1 id="61f2" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Training Loop</h1><p id="4a64" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Now that we have the training fundamentals setup, let’s put together a quick training loop to train our model.</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="631d" class="pz oo fq pw b bg qa qb l qc qd">device = "cpu"<br/>if torch.cuda.is_available():<br/>    device = "cuda"<br/>elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():<br/>    device = "mps"<br/>print(f"device {device}")<br/><br/>num_return_sequences = 5<br/>max_length = 30<br/><br/>torch.manual_seed(1337)<br/><br/>train_loader = DataLoaderLite(B=4, T=32)<br/><br/>model = GPT(GPTConfig())<br/>model.to(device)</span></pre><p id="b024" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">You can see that we repeat our device calculations to get optimal performance. We then set our data loader to use batch sizes of 4 and sequence lengths of 32 (set arbitrarily, although powers of 2 are best for memory efficiency).</p><pre class="mr ms mt mu mv pv pw px bp py bb bk"><span id="5945" class="pz oo fq pw b bg qa qb l qc qd">optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)<br/>for i in range(50):<br/>    x, y = train_loader.next_batch()<br/>    x, y = x.to(device), y.to(device)<br/>    optimizer.zero_grad() #have to start with a zero gradient<br/>    logits, loss = model(x, y)<br/>    loss.backward() #adds to the gradient (+=, which is why they must start as 0)<br/>    optimizer.step()<br/>    print(f"loss {loss.item()}, step {i}")</span></pre><p id="2f5c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">Now we have the optimizer, which will help us train our model. The optimizer is a PyTorch class that takes in the parameters it should be training (in our case the ones given from the <code class="cx qe qf qg pw b">GPT</code> class) and then the learning rate which is a hyperparameter during training determining how quickly we should be adjusting parameters — a higher learning rate means more drastic changes to the weights after each run. We chose our value based off of Karpathy’s recommendation.</p><p id="b7fe" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">We then use 50 training steps to train the model. We start by getting the training batch and moving them onto our device. We set the optimizer’s gradients to zero (gradients in pytorch are sums, so if we don’t zero it out we will be carrying information over from the last batch). We calculate the logits and loss from our model, and then run backwards propagation to figure out what the new weight models should be. Finally, we run <code class="cx qe qf qg pw b">optimizer.step()</code> to update all of our model parameters.</p><h1 id="62c9" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Sanity Check</h1><p id="1e9c" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">To see how all of the above code runs, you can <a class="af oe" href="https://colab.research.google.com/drive/1tJjzDP2wTq6R5LF0AWESlNheTn1YEYzI" rel="noopener ugc nofollow" target="_blank">check out my Google Colab where I combine all of it and run it on the NVIDIA T4 GPU</a>. Running our training loop, we see that the loss starts off at ~11. To sanity test this, we expect that at the beginning the odds of predicting the right token is (1/<code class="cx qe qf qg pw b">vocab_size</code>). Taking this through a simplified loss function of <code class="cx qe qf qg pw b">-ln</code>, we get ~10.88, which is just about where we begin!</p><figure class="mr ms mt mu mv mi mo mp paragraph-image"><div role="button" tabindex="0" class="my mz ed na bh nb"><div class="mo mp qn"><img src="../Images/7a46cc240486e0eb1f38cf0bc67d2947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWMf-sGSfWB6DQQ68VpZ7w.png"/></div></div><figcaption class="nd ne nf mo mp ng nh bf b bg z dx">Image by Author</figcaption></figure><h1 id="21b5" class="on oo fq bf op oq pq gq os ot pr gt ov ow ps oy oz pa pt pc pd pe pu pg ph pi bk">Closing</h1><p id="bcb1" class="pw-post-body-paragraph ni nj fq nk b go pj nm nn gr pk np nq nr pl nt nu nv pm nx ny nz pn ob oc od fj bk">Thanks for reading through to the end!</p><p id="b12f" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">I tried to include as much detail as I could in this blog post, but naturally there were somethings I had to leave out. If you enjoyed the blog post or see anything you think should be modified / expanded upon, please let me know!</p><p id="24c7" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">It’s an exciting time to be building!</p></div></div></div><div class="ab cb of og oh oi" role="separator"><span class="oj by bm ok ol om"/><span class="oj by bm ok ol om"/><span class="oj by bm ok ol"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="d92c" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[1] Karpathy, A., <a class="af oe" href="https://youtu.be/l8pRSuU81PU?feature=shared" rel="noopener ugc nofollow" target="_blank">“Let’s reproduce GPT-2 (124M)”</a> (2024), YouTube</p><p id="cf06" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[2] Radford, A., et al., <a class="af oe" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank">“Language Models are Unsupervised Multitask Learners”</a> (2018), Papers With Code</p><p id="07ea" class="pw-post-body-paragraph ni nj fq nk b go nl nm nn gr no np nq nr ns nt nu nv nw nx ny nz oa ob oc od fj bk">[3] Vaswani, A., et al., “<a class="af oe" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">Attention Is All You Need</a>” (2017), arXiv</p></div></div></div></div>    
</body>
</html>