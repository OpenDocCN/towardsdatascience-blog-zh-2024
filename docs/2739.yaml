- en: 'Jointly learning rewards and policies: an iterative Inverse Reinforcement Learning
    framework with ranked synthetic trajectories'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 共同学习奖励与策略：一个迭代的反向强化学习框架，带有排名合成轨迹
- en: 原文：[https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10](https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10](https://towardsdatascience.com/jointly-learning-rewards-and-policies-an-iterative-inverse-reinforcement-learning-framework-with-ecf52909e5ef?source=collection_archive---------3-----------------------#2024-11-10)
- en: A novel tractable and interpretable algorithm to learn from expert demonstrations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种新颖的可解且可解释的算法，用于从专家演示中学习
- en: '[](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)[![Hussein
    Fellahi](../Images/b49c8620d8a490ab078b5d4dfe8d017a.png)](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)
    [Hussein Fellahi](https://medium.com/@h.fellahi?source=post_page---byline--ecf52909e5ef--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)
    ·12 min read·Nov 10, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--ecf52909e5ef--------------------------------)
    ·阅读时间 12 分钟·2024年11月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/07cfe6b9a85a3ec2b15af5136f0817fb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07cfe6b9a85a3ec2b15af5136f0817fb.png)'
- en: Photo by [Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Andrea De Santis](https://unsplash.com/@santesson89?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Imitation Learning has recently gained increasing attention in the Machine Learning
    community, as it enables the transfer of expert knowledge to autonomous agents
    through observed behaviors. A first category of algorithm is **Behavioral Cloning**
    (BC), which aims to directly replicate expert demonstrations, treating the imitation
    process as a **supervised learning** task where the agent attempts to match the
    expert’s actions in given states. While straightforward and computationally efficient,
    BC often suffers from overfitting and **poor generalization**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习（Imitation Learning）近年来在机器学习社区中获得了越来越多的关注，因为它能够通过观察到的行为将专家的知识传递给自主体。一类算法是**行为克隆**（BC），其目标是直接复制专家的演示，将模仿过程视为一种**监督学习**任务，其中自主体试图在给定状态下匹配专家的行为。虽然BC方法直观且计算高效，但通常会遭遇过拟合和**较差的泛化能力**问题。
- en: In contrast, **Inverse Reinforcement Learning** (IRL) targets the underlying
    intent of expert behavior by **inferring a reward function** that could explain
    the expert’s actions as optimal within the considered environment. Yet, an important
    caveat of IRL is the inherent **ill-posed** nature of the problem — i.e. that
    multiple (if not possibly an infinite number of) reward functions can make the
    expert trajectories as optimal. A widely adopted class of methods to tackle this
    ambiguity includes Maximum Entropy IRL algorithms, which introduce an entropy
    maximization term to encourage stochasticity and robustness in the inferred policy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**反向强化学习**（IRL）的目标是通过**推断奖励函数**来揭示专家行为背后的潜在意图，这个奖励函数可以解释专家在特定环境下的行为为何是最优的。然而，IRL的一个重要警告是该问题本质上是**不良条件的**——即存在多个（如果不是可能有无限多个）奖励函数可以使专家的轨迹看起来是最优的。一类广泛采用的方法来解决这种模糊性包括最大熵IRL算法，这些算法引入了熵最大化项，以鼓励推断策略中的随机性和鲁棒性。
- en: In this article, we choose a different route and introduce a novel iterative
    IRL algorithm that jointly learns both the **reward function** and the **optimal
    policy** from expert demonstrations alone. By iteratively synthesizing trajectories
    and **guaranteeing their increasing quality**, our approach departs from traditional
    IRL models to provide a fully tractable, interpretable and efficient solution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们选择了一条不同的道路，提出了一种新的迭代式逆强化学习（IRL）算法，该算法仅通过专家演示共同学习**奖励函数**和**最优策略**。通过迭代合成轨迹并**保证其质量不断提高**，我们的方法与传统的IRL模型有所不同，提供了一种完全可处理、可解释且高效的解决方案。
- en: 'The organization of the article is as follows: section 1 introduces some basic
    concepts in IRL. Section 2 gives an overview of recent advances in the IRL literature,
    which our model builds on. We derive a sufficient condition for the our model
    to converge in section 3\. This theoretical result is general and can apply to
    a large class of algorithms. In section 4, we formally introduce the full model,
    before concluding on key differences with existing literature and further research
    directions in section 5.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 文章的组织结构如下：第1节介绍了逆强化学习（IRL）中的一些基本概念。第2节概述了我们模型所基于的IRL文献中的最新进展。第3节推导了我们模型收敛的充分条件。这个理论结果是通用的，可以应用于一类算法。第4节我们正式介绍了完整的模型，最后在第5节总结了与现有文献的主要差异并提出了进一步的研究方向。
- en: '1\. Background definitions:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1. 背景定义：
- en: 'First let’s define a few concepts, starting with the general **Inverse Reinforcement
    Learning problem** (note: we assume the same notations as [this article](/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463)):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义几个概念，从一般的**逆强化学习问题**开始（注意：我们假设与[本文](/uncertainty-in-markov-decisions-processes-a-robust-linear-programming-approach-b01e6e26e463)使用相同的符号）：
- en: '*An* ***Inverse Reinforcement Learning (IRL) problem*** *is a 5-tuple (S, A,
    P, γ, τ*) such that:*'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*An* ***逆强化学习（IRL）问题*** *是一个五元组 (S, A, P, γ, τ*)，其中：*'
- en: '*S is the set of* ***states*** *the agent can be in*'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*S 是* ***代理可以处于的状态集合***'
- en: '*A is the set of* ***actions*** *the agent can take*'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A 是* ***代理可以采取的行动集合***'
- en: '*P are* ***transition probabilities***'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P 是* ***转移概率***'
- en: '*γ ∈ (0, 1] is a* ***discount factor***'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*γ ∈ (0, 1] 是一个* ***折扣因子***'
- en: '*τ* is the set of expert demonstrations, i.e. τ*ᵢ = (sᵢ, aᵢ) is a sequence
    of actions (sometimes called trajectory) taken by an expert agent*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*τ* 是专家演示的集合，即 *τ*ᵢ = (sᵢ, aᵢ) 是专家代理采取的行动序列（有时称为轨迹）*'
- en: '*The goal of Inverse Reinforcement Learning is to* ***infer the reward function
    R*** *of the MDP (S, A, R, P, γ) solely from the expert trajectories. The expert
    is assumed to have full knowledge of this reward function and acts in a way that
    maximizes the reward of his actions.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*逆强化学习的目标是* ***推断奖励函数 R*** *，该函数是从专家轨迹中仅通过MDP (S, A, R, P, γ) 获得的。假设专家对这个奖励函数有完全的了解，并以最大化其行动奖励的方式进行操作。*'
- en: 'We make the additional assumption of **linearity** of the reward function (common
    in IRL literature) i.e. that it is of the form:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还做了一个附加假设，即**奖励函数的线性性**（在IRL文献中很常见），即它的形式为：
- en: '![](../Images/b64f4fcb0b39c357764f1527e0adaf25.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b64f4fcb0b39c357764f1527e0adaf25.png)'
- en: where *ϕ* is a **static** feature map of the state-action space and *w* a weight
    vector. In practice, this feature map can be found via classical Machine Learning
    methods (e.g. VAE — see [6] for an example). The feature map can therefore be
    estimated separately, which reduces the IRL problem to inferring the weight vector
    *w* rather than the full reward function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ϕ* 是状态-动作空间的**静态**特征映射，*w* 是权重向量。实际上，这个特征映射可以通过经典的机器学习方法（例如 VAE — 请参见[6]中的示例）找到。因此，特征映射可以单独估计，这将IRL问题简化为推断权重向量
    *w*，而不是完整的奖励函数。
- en: 'In this context, we finally derive the **feature expectation *μ***, which will
    prove useful in the different methods presented. Starting from the value function
    of a given policy *π*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，我们最终推导出**特征期望 *μ***，这将在不同的方法中证明是有用的。首先从给定策略 *π* 的值函数开始：
- en: '![](../Images/05d67b66c42f13e3c32a55597189bcc3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05d67b66c42f13e3c32a55597189bcc3.png)'
- en: 'We then use the linearity assumption of the reward function introduced above:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用上述引入的奖励函数的线性假设：
- en: '![](../Images/fb241f1daf79fba72a4f368d8b5bc7f7.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb241f1daf79fba72a4f368d8b5bc7f7.png)'
- en: Likewise, *μ* can also be computed separately — usually via Monte Carlo.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，*μ* 也可以单独计算 —— 通常通过蒙特卡罗方法。
- en: 2\. Related work in RL and IRL literature
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 相关工作在RL和IRL文献中
- en: '2.1 Apprenticeship Learning:'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 学徒学习：
- en: 'A seminal method to learn from expert demonstrations is Apprenticeship learning,
    first introduced in [1]. Unlike pure Inverse Reinforcement Learning, the objective
    here is to **both** to find the **optimal reward vector** as well as inferring
    the **expert policy** from the given demonstrations. We start with the following
    observation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从专家演示中学习的一个开创性方法是学徒学习法，首次在[1]中介绍。与纯反向强化学习不同，这里的目标是**同时**找到**最优奖励向量**以及从给定的演示中推断出**专家策略**。我们从以下观察开始：
- en: '![](../Images/5ef65519dcb75d996e9c1095f8259a28.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ef65519dcb75d996e9c1095f8259a28.png)'
- en: Mathematically this can be seen using the Cauchy-Schwarz inequality. This result
    is actually quite powerful, as it allows to focus on matching the feature expectations,
    which will guarantee the matching of the value functions — **regardless of the
    reward weight vector**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上看，这可以通过柯西-施瓦茨不等式来理解。这个结果实际上非常强大，因为它使我们能够专注于匹配特征期望，这将保证价值函数的匹配 — **不管奖励权重向量如何**。
- en: 'In practice, Apprenticeship Learning uses an iterative algorithm based on the
    **maximum margin principle** to approximate *μ(π*)* — where *π** is the (unknown)
    expert policy. To do so, we proceed as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，学徒学习法使用基于**最大边际原则**的迭代算法来逼近*μ(π*)* — 其中*π*是（未知的）专家策略。具体步骤如下：
- en: Start with a (potentially random) initial policy and compute its feature expectation,
    as well as the estimated feature expectation of the expert policy from the demonstrations
    (estimated via Monte Carlo)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个（可能是随机的）初始策略开始，计算其特征期望，并计算专家策略的特征期望，这些专家策略来自演示（通过蒙特卡罗方法估计）
- en: For the given feature expectations, find the weight vector that maximizes the
    margin between *μ(π*)* and the other *(μ(π))*. In other words, we want the weight
    vector that would **discriminate as much as possible** between the expert policy
    and the trained ones
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的特征期望，找到能最大化*μ(π*)*和其他*μ(π)*之间边际的权重向量。换句话说，我们希望找到一个权重向量，**尽可能区分**专家策略和训练策略
- en: Once this weight vector *w’* found, use classical Reinforcement Learning — with
    the reward function approximated with the feature map *ϕ* and *w’* — to find the
    next trained policy
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦找到这个权重向量*w’*，使用经典的强化学习 — 使用特征映射*ϕ*和*w’*来近似奖励函数 — 来找到下一个训练好的策略
- en: Repeat the previous 2 steps until the smallest margin between *μ(π*)* and the
    one for any given policy *μ(π)* is below a certain threshold — meaning that among
    all the trained policies, we have found one that matches the expert feature expectation
    up to a certain ϵ
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前两步，直到*μ(π*)*与任何给定策略*μ(π)*之间的最小边际低于某个阈值 — 这意味着在所有训练过的策略中，我们找到了一个在一定ϵ范围内与专家特征期望匹配的策略
- en: 'Written more formally:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地写法：
- en: '![](../Images/b3ecf18b6b6ae4e1e74ea8d5ca7bc847.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3ecf18b6b6ae4e1e74ea8d5ca7bc847.png)'
- en: 'Source: Principles of Robot Autonomy II, lecture 10 ([2])'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：机器人自主原理 II，第10讲 ([2])
- en: '2.2 IRL with ranked demonstrations:'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 带排序的演示的IRL：
- en: 'The maximum margin principle in Apprenticeship Learning does not make any assumption
    on the relationship between the different trajectories: the algorithm stops as
    soon as any set of trajectories achieves a narrow enough margin. Yet, suboptimality
    of the demonstrations is a well-known caveat in Inverse Reinforcement Learning,
    and in particular the variance in demonstration quality. An additional information
    we can exploit is the **ranking of the demonstrations** — and consequently ranking
    of feature expectations.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 学徒学习中的最大边际原则不对不同轨迹之间的关系做任何假设：当任意一组轨迹达到足够窄的边际时，算法就会停止。然而，演示的亚最优性是反向强化学习中的一个广为人知的问题，尤其是在演示质量的方差上。我们可以利用的附加信息是**演示的排序**
    — 进而对特征期望进行排序。
- en: 'More precisely, consider ranks *{1, …, k}* (from worst to best) and feature
    expectations *μ₁, …, μₖ*. Feature expectation *μᵢ* is computed from trajectories
    of rank *i*. We want our reward function to **efficiently discriminate between
    demonstrations of different quality**, i.e.:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 更精确地说，考虑排名*{1, …, k}*（从最差到最好）和特征期望*μ₁, …, μₖ*。特征期望*μᵢ*是从排名为*i*的轨迹中计算得出的。我们希望我们的奖励函数**有效地区分不同质量的演示**，即：
- en: '![](../Images/0968451a804155d896466e17815719be.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0968451a804155d896466e17815719be.png)'
- en: 'In this context, [5] presents a **tractable** formulation of this problem into
    a Quadratic Program (QP), using once again the maximum margin principle, i.e.
    maximizing the smallest margin between two different classes. Formally:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，[5]提出了一个**可处理的**问题公式化，转化为一个二次规划（QP），再次使用最大边际原则，即最大化两个不同类别之间的最小边际。形式上：
- en: '![](../Images/2ea6407d6a4045e8ee9e77764da9e6fc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2ea6407d6a4045e8ee9e77764da9e6fc.png)'
- en: 'This is actually very similar to the optimization run by SVM models for multiclass
    classification. The all-in optimization model is the following — see [5] for details:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上与SVM模型用于多类分类的优化非常相似。整体优化模型如下——详细内容见[5]：
- en: '![](../Images/a7e0fd7897d971f37ccd2d1cc7e2a1c9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7e0fd7897d971f37ccd2d1cc7e2a1c9.png)'
- en: 'Source: [5]'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[5]
- en: '2.3 Disturbance-based Reward Extrapolation (D-REX):'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 基于扰动的奖励外推（D-REX）：
- en: 'Presented in [4], the D-REX algorithm also uses this concept of IRL with ranked
    preferences but on **generated** demonstrations. The intuition is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4]中提出的D-REX算法也使用了这个带排名偏好的IRL概念，但应用于**生成**的示范。直觉如下：
- en: Starting from the expert demonstrations, imitate them via Behavioral cloning,
    thus getting a baseline *π₀*
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从专家示范开始，通过行为克隆模仿它们，从而得到一个基准策略*π₀*。
- en: 'Generate ranked sets of demonstration with different degrees of performance
    by **injecting different noise levels** to *π₀*: in [4] authors prove that for
    two levels of noise *ϵ* and *γ*, such that *ϵ > γ* (i.e. ϵ is “noisier” than γ)
    we have with high probability that *V[π(. | ϵ)] < V[π’. | γ)]-* where *π(. | x)*
    is the policy resulting from injecting noise *x* in *π₀.*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**注入不同噪声级别**到*π₀*中，生成不同表现度的示范排名集合：在[4]中，作者证明了对于两个噪声级别*ϵ*和*γ*，当*ϵ > γ*（即ϵ比γ“更嘈杂”）时，我们有很高的概率得到*V[π(.
    | ϵ)] < V[π’. | γ)]-*，其中*π(. | x)*是注入噪声*x*到*π₀*中的策略。
- en: Given this automated ranking provided, run an IRL from ranked demonstrations
    method (T-REX) based on approximating the reward function with a **neural network
    trained with a pairwise loss** — see [3] for more details
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定提供的自动排名，运行基于排名示范的IRL方法（T-REX），该方法通过使用**带对偶损失的神经网络训练**来近似奖励函数——更多细节见[3]。
- en: With the approximation of the reward function *R’* gotten from the IRL step,
    run a classical RL method with *R’* to get the final policy
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用从IRL步骤中得到的奖励函数*R’*的近似，使用*R’*运行一个经典的强化学习方法，以获得最终策略。
- en: 'More formally:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说：
- en: '![](../Images/cc2d56bb53d565e1b24765f6c3aa3c59.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc2d56bb53d565e1b24765f6c3aa3c59.png)'
- en: 'Source: [4]'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[4]
- en: 'Another important theoretical result presented in [4] is the effect of ranking
    on reward ambiguity: the paper manages to quantify the ambiguity reduction coming
    from added ranking constraint, which elegantly tackles the ill-posed nature of
    IRL.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在[4]中提出的另一个重要理论结果是排名对奖励模糊性的影响：论文设法量化了因增加排名约束而导致的模糊性减少，这优雅地解决了逆强化学习（IRL）中不良问题的本质。
- en: '2.4 Guided Reinforcement Learning:'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 引导强化学习：
- en: 'How can we leverage some **expert demonstrations** when fitting a **Reinforcement
    Learning** model? Rather than start exploring using an initial random policy,
    one could think of leveraging available demonstration information — as suboptimal
    as they might be — as a **warm start** and guide at least the beginning of the
    RL training. This idea is formalized in [8], and the intuition is:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在拟合**强化学习**模型时利用一些**专家示范**？与其从一个初始的随机策略开始探索，不如考虑利用可用的示范信息——即使它们可能是次优的——作为**热启动**，至少引导强化学习训练的开始。这个想法在[8]中得到了形式化，直觉是：
- en: For each training iteration, start with the expert policy / demonstrations *(πᵍ)*
    — to collect “good” samples and put the agent in “good” states
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每次训练迭代，从专家策略/示范*(πᵍ)*开始——收集“好”的样本，并将智能体置于“好”的状态中。
- en: After a determined **switch point**, let the current trained policy *(πᵉ)* take
    over and explore states — the objective being that it **explores states that have
    not been visited (enough) by the expert demonstrations**, while relying on the
    expert choices for the visited ones
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个确定的**切换点**之后，让当前训练的策略*(πᵉ)*接管并探索状态——目标是让它**探索专家示范中未曾访问（足够）过的状态**，同时依赖专家对已访问状态的选择。
- en: As it gets better, *πᵉ* should take over earlier
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着进步，*πᵉ*应该更早接管。
- en: 'More formally:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说：
- en: '![](../Images/0353780ceedd4a1ffa0a91604303d222.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0353780ceedd4a1ffa0a91604303d222.png)'
- en: 'Source: [8]'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[8]
- en: '3\. A sufficient condition for convergence:'
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 收敛的充分条件：
- en: 'Before deriving the full model, we establish the following result that will
    provide a useful bound **guaranteeing improvement in an iterative algorithm**
    — full proof is provided in the Appendix:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在推导完整模型之前，我们建立以下结果，它将提供一个有用的边界 **保证迭代算法中的改进** —— 完整证明见附录：
- en: '**Theorem 1:** *Let (S, A, P, γ, π*) the Inverse Reinforcement Learning problem
    with unknown true reward function R*. For two policies π₁ and π₂ fitted using
    the candidate reward functions R₁ and R₂ of the form Rᵢ = R* + ϵᵢ with ϵᵢ some
    error function, we have the following sufficient condition to have π₂ improve
    upon π₁, i.e. V(π₂, R*) > V(π₁, R*):*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 1：** *设 (S, A, P, γ, π*) 为具有未知真实奖励函数 R* 的逆强化学习问题。对于使用候选奖励函数 R₁ 和 R₂ 拟合的两个策略
    π₁ 和 π₂，形式为 Rᵢ = R* + ϵᵢ，其中 ϵᵢ 是某种误差函数，我们有以下充分条件使得 π₂ 改进 π₁，即 V(π₂, R*) > V(π₁,
    R*)：*'
- en: '![](../Images/5e9df6dcc9be33e66b7d46516095e2a7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e9df6dcc9be33e66b7d46516095e2a7.png)'
- en: '*Where TV(π₂, π₁) is the total variation distance between π₂ and π₁, interpreting
    the policies as probability measures.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*其中 TV(π₂, π₁) 是 π₂ 和 π₁ 之间的总变差距离，将策略解释为概率度量。*'
- en: 'This bound gives some intuitive insights, since if we want to guarantee improvement
    on a known policy with its reward function, the margin gets higher the more:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该边界提供了一些直观的见解，因为如果我们想要保证在已知策略及其奖励函数上取得改进，边际会随着以下情况增加：
- en: Different the two reward functions are
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个奖励函数的不同程度
- en: Different the two policies are
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个策略的不同程度
- en: Imprecise the original reward function is
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始奖励函数的不精确性
- en: 4\. The full tractable model
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 完整的可处理模型
- en: 'Building on the previously introduced models and Theorem 1, we can derive our
    new fully tractable model. The intuition is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 基于前面介绍的模型和定理 1，我们可以推导出我们的新完全可处理模型。直觉是：
- en: '**Initialization:** start from the expert trajectories *(τ*)*, estimate an
    initial policy *π₀* (e.g. via Behavioral Cloning) and generate trajectories (τ₀)
    of an agent following *π₀.* Use these trajectories to estimate a reward weight
    vector *w₀* (which gives *R₀ = w₀ ϕ* )such that *V(π*, R₀) > V(π₀, R₀)* through
    the Quadratic Program presented in section 2.2 — i.e. the ranking here being:
    *{2 : τ*, 1: τ₀}*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化：** 从专家轨迹 *(τ*)* 开始，估计初始策略 *π₀*（例如，通过行为克隆），并生成一个代理人跟随 *π₀* 的轨迹 (τ₀)。利用这些轨迹估计奖励权重向量
    *w₀*（使得 *R₀ = w₀ ϕ* ）通过第 2.2 节中展示的二次规划来满足 *V(π*, R₀) > V(π₀, R₀)* —— 即此处的排名为：*{2
    : τ*, 1: τ₀}*'
- en: '**Iteration:** infer a policy *πᵢ* using *wᵢ-₁* (i.e. *Rᵢ = wᵢ-₁ϕ*) such that
    we have V*(πᵢ, Rᵢ) > V(πᵢ-₁, Rᵢ-₁)* ***with sufficient margin***using Guided RL.
    Note that here **we do not necessarily want *πᵢ* to be optimal w.r.t. *Rᵢ***,
    we simply want something that outperforms the current policy by a certain margin
    as dictated by Theorem 1\. The rationale behind this is that we do not want to
    overfit on the **inaccuracies caused by the reward misspecification**. See [7]
    for more details on the impact of this misspecification.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代：** 使用 *wᵢ-₁* 推断策略 *πᵢ*（即 *Rᵢ = wᵢ-₁ϕ*），使得我们有 V*(πᵢ, Rᵢ) > V(πᵢ-₁, Rᵢ-₁)*
    ***具有足够的边际***，使用引导强化学习。请注意，这里 **我们不一定希望 *πᵢ* 相对于 *Rᵢ* 是最优的**，我们只是希望某个策略能以定理 1
    所规定的边际超过当前策略。其背后的原因是我们不希望在 **奖励错误指定**上过拟合。有关此误指定影响的更多细节，请参见 [7]。'
- en: 'Likewise, **generate samples *τᵢ*** with policy *πᵢ* and use the tractable
    IRL model on the following updated ranking: *{i : τ*, i-1: τᵢ, i-2: τᵢ-₁…}*'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '同样，**生成样本 *τᵢ*** 通过策略 *πᵢ* 并使用可处理的 IRL 模型对以下更新后的排名进行处理：*{i : τ*, i-1: τᵢ, i-2:
    τᵢ-₁…}*'
- en: '**Stopping condition:** when *V(τ*, wᵢ) — V(τᵢ, wᵢ)* is below a certain threshold
    **ϵ** or if the Quadratic Program is **infeasible**'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停止条件：** 当 *V(τ*, wᵢ) — V(τᵢ, wᵢ)* 低于某个阈值 **ϵ** 或二次规划 **不可行**时'
- en: 'Formally:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说：
- en: '![](../Images/e6fe7bb11682eaff1b571c2c981b43d9.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6fe7bb11682eaff1b571c2c981b43d9.png)'
- en: 'This algorithm makes a few choices that we need to keep in mind:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法做出了一些选择，我们需要牢记：
- en: An implicit assumption we make is that the reward functions are getting more
    precise through the iterations, i.e. the **noise terms *ϵᵢ* are decreasing in
    norm and go to 0**
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们做出的一个隐含假设是奖励函数在迭代过程中变得更加精确，即 **噪声项 *ϵᵢ* 的范数逐渐减小并趋向于 0**
- en: Since the noise terms *ϵᵢ* are unknown, we replace them with a **predetermined
    margin schedule** (*mᵢ)*— following the above assumption we can make the schedule
    decreasing and going to 0
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于噪声项 *ϵᵢ* 是未知的，我们将其替换为 **预定的边际调度** (*mᵢ)* —— 根据上述假设，我们可以让该调度递减并趋向于 0
- en: 'Why stop the iteration if the QP is infeasible: we know that the QP **was feasible
    at the previous iteration**, therefore the main constraint that made it infeasible
    was adding the new feature expectation *μᵢ* computed with trajectories using *πᵢ*.
    We interpret this infeasibility as the fact that we’re **unable to get a margin
    significant enough** to discriminate between *μᵢ and μ**,which can mean that *wᵢ-₁*
    and *πᵢ* are optimal solutions.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在QP不可行时停止迭代：我们知道QP在上一次迭代时**是可行的**，因此使其不可行的主要约束是添加了使用*πᵢ*的轨迹计算的新特征期望*μᵢ*。我们将这种不可行性解释为我们**无法获得足够显著的边际**以区分*μᵢ*和μ，这可能意味着*
    wᵢ-₁*和*πᵢ*是最优解。
- en: '5\. Conclusion and future work:'
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 结论和未来工作：
- en: 'While synthesizing multiple models from RL and IRL literature, this new heuristic
    innovates in a number of ways:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在综合RL和IRL文献中的多个模型时，这种新的启发式方法在许多方面进行了创新：
- en: '**Full tractability**: not using a neural network for the IRL step is a choice,
    as it enhances tractability and interpretability. It also provides additional
    insights related to the **feasibility of the QP** which can allow to stop the
    algorithm earlier.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全可处理性**：不使用神经网络进行IRL步骤是一种选择，因为它提高了可处理性和可解释性。它还提供了与**QP可行性**相关的额外见解，可以让算法更早地停止。'
- en: '**Efficiency and ease of use:** although an iterative algorithm, each step
    of the iteration can be done very efficiently: the QP can be solved quickly using
    current solvers and the RL step requires only a **limited number of iterations**
    to satisfy the improvement condition presented in Theorem 1\. It also offers additional
    flexibility with the *ϵ* coefficient, allowing the user to “pay” some suboptimality
    for faster convergence if needed.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率和易用性**：尽管是一个迭代算法，但每一步的迭代都可以非常高效地完成：QP可以使用当前的求解器快速求解，RL步骤只需要**有限的迭代次数**即可满足定理1中提出的改进条件。它还提供了额外的灵活性，通过*ϵ*系数，允许用户在需要时“付出”一些次优性以加速收敛。'
- en: 'Better **use of all information available** to minimize noise: the iterative
    nature of the algorithm limits the uncertainty propagation from IRL to RL as we
    always restart the fitting (offering a possibility to course-correct). The Guided
    RL step also allows to **introduce a healthy bias towards the expert demonstrations**.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地**利用所有可用信息**以最小化噪声：该算法的迭代特性限制了从IRL到RL的不确定性传播，因为我们总是重新开始拟合（提供了修正的可能性）。引导RL步骤还允许**引入对专家示范的健康偏向**。
- en: We can also note that Theorem 1 is a **general property** and provides a bound
    that can be applied to a large class of algorithms.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以注意到定理1是一个**一般性质**，并提供了一个可以应用于大类算法的界限。
- en: Further research can naturally be done to extend this algorithm. First, a thorough
    **implementation and benchmarking** against other approaches can provide interesting
    insights. Another direction would be deepening the theoretical study of the convergence
    conditions of the model, especially the assumption of reduction of reward noise.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的研究自然可以用于扩展该算法。首先，对该算法的**实现和基准测试**与其他方法进行对比，可以提供有趣的见解。另一个方向是深入研究模型的收敛条件，特别是奖励噪声减少的假设。
- en: 'Appendix:'
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录：
- en: 'We prove Theorem 1 introduced earlier similarly to the proof of Theorem 1 in
    [4]. The target inequality for two given policies fitted at steps *i* and *i-1*
    is: *V(πᵢ, R*) > V(πᵢ-₁, R*).* The objective is to derive a sufficient condition
    for this inequality to hold. We start with the following assumptions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过类似于[4]中定理1的证明，证明了前面引入的定理1。对于在步骤*i*和*i-1*拟合的两个给定策略，目标不等式为：*V(πᵢ, R*) > V(πᵢ-₁,
    R*)*。目标是推导出这个不等式成立的充分条件。我们从以下假设开始：
- en: '*π₁* is the policy fitted at the previous iteration using the reward function
    *R₁*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*π₁*是使用奖励函数*R₁*在上一次迭代中拟合的策略。'
- en: The RL step uses a form of iterative algorithm (e.g. Value Iteration), which
    yields that at the current iteration *π₂* is also known — even if it is only a
    candidate policy that could be improved if it does not fit the condition. The
    reward function it is (currently being) fitted on is *R₂*
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL步骤使用一种迭代算法形式（例如，值迭代），这意味着在当前迭代中*π₂*也是已知的——即使它只是一个候选策略，如果不符合条件，仍然可以改进。它正在拟合的奖励函数是*R₂*。
- en: 'The reward functions Rᵢ are of the form: *Rᵢ = R* + ϵ*ᵢ with *ϵᵢ* some error
    function'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数Rᵢ的形式为：*Rᵢ = R* + ϵ*ᵢ，其中*ϵᵢ*是某种误差函数。
- en: 'We thus have:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出：
- en: '![](../Images/61d4f232398ffeb4fdf7c0381852f9ad.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61d4f232398ffeb4fdf7c0381852f9ad.png)'
- en: 'Following the assumptions mades, *V(π₂, R₂) — V(π₁, R₁)* is known at the time
    of the iteration. For the second part of the expression featuring *ϵ₁* and *ϵ₂*
    (which are unknown, as we only know *ϵ₁ — ϵ₂ = R₁ — R₂*) we derive an upper bound
    for its value:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所做的假设，*V(π₂, R₂) — V(π₁, R₁)* 在迭代时已知。对于表达式的第二部分，其中涉及*ϵ₁*和*ϵ₂*（这是未知的，因为我们只知道*ϵ₁
    — ϵ₂ = R₁ — R₂*），我们推导出其值的上界：
- en: '![](../Images/25fa7b010b1f27840a4fef78fe23cb1b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25fa7b010b1f27840a4fef78fe23cb1b.png)'
- en: 'Where *TV(π₁, π₂)* is the total variance between *π₁* and *π₂*, as we interpret
    the policies as probability measures. We reinject this upper bound in the first
    expression, giving:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*TV(π₁, π₂)*是*π₁*和*π₂*之间的总方差，因为我们将策略解释为概率测量。我们将这个上界重新代入第一个表达式，得到：
- en: '![](../Images/794f310b8cb8446547796cc00abc0e3b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/794f310b8cb8446547796cc00abc0e3b.png)'
- en: 'Therefore, this gives the following condition to have the policy *π₂* improve
    on *π₁*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这给出了以下条件，以确保策略*π₂*优于*π₁*：
- en: '![](../Images/80ef53f32b9947cf50e95cf64044c874.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80ef53f32b9947cf50e95cf64044c874.png)'
- en: 'References:'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] P. Abbeel, A. Y. Ng, [Apprenticeship Learning via Inverse Reinforcement
    Learning](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf) (2004), Stanford
    Artificial Intelligence Laboratory'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] P. Abbeel, A. Y. Ng, [通过逆向强化学习进行学徒学习](https://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf)
    (2004), 斯坦福人工智能实验室'
- en: '[2] J. Bohg, M. Pavone, D. Sadigh, [Principles of Robot Autonomy II](https://web.stanford.edu/class/cs237b/)
    (2024), Stanford ASL web'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] J. Bohg, M. Pavone, D. Sadigh, [机器人自主原理 II](https://web.stanford.edu/class/cs237b/)
    (2024), 斯坦福ASL网站'
- en: '[3] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, [Extrapolating Beyond Suboptimal
    Demonstrations via Inverse Reinforcement Learning from Observations](https://proceedings.mlr.press/v97/brown19a/brown19a.pdf)
    (2019), Proceedings of Machine Learning Research'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, [通过观察进行逆向强化学习，超越次优示范](https://proceedings.mlr.press/v97/brown19a/brown19a.pdf)
    (2019), 机器学习研究会论文集'
- en: '[4] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, [Better-than-Demonstrator
    Imitation Learning via Automatically-Ranked Demonstrations](https://proceedings.mlr.press/v100/brown20a/brown20a.pdf)
    (2020), Proceedings of Machine Learning Research'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] D. S. Brown, W. Goo, P. Nagarajan, S. Niekum, [通过自动排名示范进行优于示范者的模仿学习](https://proceedings.mlr.press/v100/brown20a/brown20a.pdf)
    (2020), 机器学习研究会论文集'
- en: '[5] P. S. Castro, S. Li, D. Zhang, [Inverse Reinforcement Learning with Multiple
    Ranked Experts](https://arxiv.org/abs/1907.13411) (2019), arXiv'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] P. S. Castro, S. Li, D. Zhang, [多专家排名逆向强化学习](https://arxiv.org/abs/1907.13411)
    (2019), arXiv'
- en: '[6] A. Mandyam, D. Li, D. Cai, A. Jones, B. E. Engelhardt, [Kernel Density
    Bayesian Inverse Reinforcement Learning](https://openreview.net/forum?id=B80WUNhTAw)
    (2024), arXiv'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] A. Mandyam, D. Li, D. Cai, A. Jones, B. E. Engelhardt, [核密度贝叶斯逆向强化学习](https://openreview.net/forum?id=B80WUNhTAw)
    (2024), arXiv'
- en: '[7] A. Pan, K. Bhatia, J. Steinhardt, [The Effects of Reward Misspecification:
    Mapping and Mitigating Misaligned Models](https://arxiv.org/abs/2201.03544) (2022),
    arXiv'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] A. Pan, K. Bhatia, J. Steinhardt, [奖励误设定的影响：映射与缓解模型错位](https://arxiv.org/abs/2201.03544)
    (2022), arXiv'
- en: '[8] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, Ch. Fu,
    C. Ma, J. Jiao, S. Levine, K. Hausman, [Jump-Start Reinforcement Learning](https://proceedings.mlr.press/v202/uchendu23a.html)
    (2023), Proceedings of Machine Learning Research'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] I. Uchendu, T. Xiao, Y. Lu, B. Zhu, M. Yan, J. Simon, M. Bennice, Ch. Fu,
    C. Ma, J. Jiao, S. Levine, K. Hausman, [跳跃启动强化学习](https://proceedings.mlr.press/v202/uchendu23a.html)
    (2023), 机器学习研究会论文集'
