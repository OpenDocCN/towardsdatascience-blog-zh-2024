- en: Towards Infinite LLM Context Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/towards-infinite-llm-context-windows-e099225abaaf?source=collection_archive---------0-----------------------#2024-04-28](https://towardsdatascience.com/towards-infinite-llm-context-windows-e099225abaaf?source=collection_archive---------0-----------------------#2024-04-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It all started with GPT having an input context window of 512 tokens. After
    only 5 years the newest LLMs are capable of handling 1M+ context inputs. Where’s
    the limit?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@krzysztof.kornel?source=post_page---byline--e099225abaaf--------------------------------)[![Krzysztof
    K. Zdeb](../Images/4531b37707bf6a01ef635e4b9ecfc03f.png)](https://medium.com/@krzysztof.kornel?source=post_page---byline--e099225abaaf--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e099225abaaf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e099225abaaf--------------------------------)
    [Krzysztof K. Zdeb](https://medium.com/@krzysztof.kornel?source=post_page---byline--e099225abaaf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e099225abaaf--------------------------------)
    ·9 min read·Apr 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/991696e8052dfe143c32539a274be31a.png)'
  prefs: []
  type: TYPE_IMG
- en: I like to think of the **LLMs** (specifically, of the models’ parameters, i.e.,
    their weights of the neural network layers and weights of the attention mechanisms)
    as a vast memory bank filled with a wide range of information about the world
    mixed with linguistic knowledge on how to process text. It’s like static knowledge
    it has once learned during the model pre-training and (optionally) fine-tuning;
    a common core reusable by all conversations with the model, no matter who asks
    the questions. The **input context**, on the other hand, is where the magic happens.
    It’s like passing a note to the model with special instructions or information.
    This can be anything from a question we want answered to unique information we
    have or even a history of past chats we’ve had with it. It’s a way of giving the
    model a personal memory.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of Context Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key characteristics of an LLM is its input context size. It can be
    defined as the number of words (or [**tokens**](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them),
    to be precise) that a language model takes into consideration when generating
    a…
  prefs: []
  type: TYPE_NORMAL
