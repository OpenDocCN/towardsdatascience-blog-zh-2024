<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Linear Regressions for Causal Conclusions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Linear Regressions for Causal Conclusions</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regressions-for-causal-conclusions-34c6317c5a11?source=collection_archive---------0-----------------------#2024-04-10">https://towardsdatascience.com/linear-regressions-for-causal-conclusions-34c6317c5a11?source=collection_archive---------0-----------------------#2024-04-10</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="aeb4" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">An easy and yet powerful tool for decision making</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://miptgirl.medium.com/?source=post_page---byline--34c6317c5a11--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Mariya Mansurova" class="l ep by dd de cx" src="../Images/b1dd377b0a1887db900cc5108bca8ea8.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*7fFHr8XBAuR_SgJknIyODA.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--34c6317c5a11--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://miptgirl.medium.com/?source=post_page---byline--34c6317c5a11--------------------------------" rel="noopener follow">Mariya Mansurova</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--34c6317c5a11--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 10, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div></div></div><div class="mj"><div class="ab cb"><div class="lm mk ln ml lo mm cf mn cg mo ci bh"><figure class="ms mt mu mv mw mj mx my paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/1bbc0ed5ecd20a5cef6f2c20504bb01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*XbcjlXRUFSs3K78hcs8_wg.jpeg"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by DALL-E</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="6625" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I suppose most of us have heard the statement “correlation doesn’t imply causation” multiple times. It often becomes a problem for analysts since we frequently can see only correlations but still want to make causal conclusions.</p><p id="8ae7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s discuss a couple of examples to understand the difference better. I would like to start with a case from everyday life rather than the digital world.</p><p id="a289" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In 1975, a vast population study was launched in Denmark. It’s called <a class="af of" href="https://pubmed.ncbi.nlm.nih.gov/30193744/" rel="noopener ugc nofollow" target="_blank">the Copenhagen City Heart Study (CCHS)</a>. Researchers gathered information about 20K men and women and have been monitoring these people for decades. The initial goal of this research was to find ways to prevent cardiovascular diseases and strokes. One of the conclusions from this study is that people who reported regularly playing tennis have 9.7 years higher life expectancy.</p><p id="bd44" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s think about how we could interpret this information. Does it mean that if a person starts playing tennis weekly today, they will increase their life expectancy by ten years? Unfortunately, not exactly. Since it’s an observational study, we should be cautious about making causal inferences. There might be some other effects. For example, tennis players are likely to be wealthier, and <a class="af of" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4866586/" rel="noopener ugc nofollow" target="_blank">we know</a> that higher wealth correlates with greater longevity. Or there could be a correlation that people who regularly do sports also care more about their health and, because of it, do all checkups regularly. So, observational research might overestimate the effect of tennis on longevity since it doesn’t control other factors.</p><p id="981d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s move on to the examples closer to product analytics and our day-to-day job. The number of Customer Support contacts for a client will likely be positively correlated with the probability of churn. If customers had to contact our support ten times, they would likely be annoyed and stop using our product, while customers who never had problems and are happy with the service might never reach out with any questions.</p><p id="a0e0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Does it mean that if we reduce the number of CS contacts, we will increase customer retention? I’m ready to bet that if we hide contact info and significantly reduce the number of CS contacts, we won’t be able to decrease churn because the actual root cause of churn is not CS contact but customers’ dissatisfaction with the product, which leads to both customers contacting us and stopping using our product.</p><p id="83b5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I hope that with these examples, you can gain some intuition about the correlation vs. causation problem.</p><p id="15d7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this article, I would like to share approaches for driving causal conclusions from the data. Surprisingly, we will be able to use the most basic tool — just a linear regression.</p><p id="a205" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If we use the same linear regression for causal inference, you might wonder, what is the difference between our usual approach and causal analytics? That’s a good question. Let’s start our causal journey by understanding the differences between approaches.</p><h1 id="910b" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Predictive vs. causal analytics</h1><p id="a24a" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Predictive analytics helps to make forecasts and answer questions like “How many customers will we have in a year if nothing changes?” or “What is the probability for this customer to make a purchase within the next seven days?”.</p><p id="f962" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Causal analytics tries to understand the root causes of the process. It might help you to answer “what if” questions like “How many customers will churn if we increase our subscription fee?” or “How many customers would have signed up for our subscription if we didn’t launch this Saint Valentine’s promo?”.</p><p id="b5f0" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Causal questions seem way more complicated than just predictive ones. However, these two approaches often leverage the same tools, such as linear or logistic regressions. Even though tools are the same, they have absolutely different goals:</p><ul class=""><li id="d2e7" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">For predictive analytics, we try our best to predict a value in the future based on information we know. So, the main KPI is an error in the prediction.</li><li id="1437" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Building a regression model for the causal analysis, we focus on the relationships between our target value and other factors. The model’s main output is coefficients rather than forecasts.</li></ul><p id="6991" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s look at a simple example. Suppose we would like to forecast the number of active customers.</p><ul class=""><li id="f8b4" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">In the predictive approach, we are talking about baseline forecast (given that the situation will stay pretty much the same). We can use <a class="af of" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;cd=&amp;ved=2ahUKEwjoheb60ZeEAxVkyQIHHY0JALwQFnoECCgQAQ&amp;url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FAutoregressive_integrated_moving_average&amp;usg=AOvVaw3LwrU_FST2Kj6NbHUsUNNT&amp;opi=89978449" rel="noopener ugc nofollow" target="_blank">ARIMA</a> (<em class="pp">Autoregressive Integrated Moving Average</em>) and base our projections on previous values. ARIMA works well for predictions but can’t tell you anything about the factors affecting your KPI and how to improve your product.</li><li id="f4e9" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">In the case of causal analytics, our goal is to find causal relationships in our data, so we will build a regression and identify factors that can impact our KPI, such as subscription fees, marketing campaigns, seasonality, etc. In that case, we will get not only the BAU (business as usual) forecast but also be able to estimate different “what if” scenarios for the future.</li></ul><p id="7bf5" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, it’s time to dive into causal theory and learn basic terms.</p><h1 id="a52a" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Correlation doesn’t imply causation</h1><p id="ed55" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Let’s consider the following example for our discussion. Imagine you sent a discount coupon to loyal customers of your product, and now you want to understand how it affected their value (money spent on the product) and retention.</p><p id="73eb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">One of the most basic causal terms is <strong class="nl fr">treatment</strong>. It sounds like something related to the medicine, but actually, it’s just an intervention. In our case, it’s a discount. We usually define treatment at the unit level (in our case, customer) in the following way.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pq"><img src="../Images/047fff29c52dce013529d21fbcaa3156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Xg3eyCOaeZE0lAQQEgxAA.png"/></div></div></figure><p id="59a9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The other crucial term is <strong class="nl fr">outcome</strong> <code class="cx pr ps pt pu b">Y</code>, our variable of interest. In our example, it’s the customer’s value.</p><p id="f08e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The fundamental problem of causal inference is that we can’t observe both outcomes for the same customers. So, if a customer received the discount, we will never know what value or retention he would have had without a coupon. It makes causal inference tricky.</p><p id="a55d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That’s why we need to introduce another concept — <strong class="nl fr">potential outcomes</strong>. The outcome that happened is usually called factual, and the one that didn’t is counterfactual. We will use the following notation for it.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pv"><img src="../Images/1a4cadf265c5818dc3a00ba40c588132.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qZwhDr5Y9SvGYhpI6hjVg.png"/></div></div></figure><p id="e75a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The main goal of causal analysis is to measure the relationship between treatment and outcome. We can use the following metrics to quantify it:</p><ul class=""><li id="789e" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk"><strong class="nl fr">ATE</strong> — average treatment effect,</li><li id="4766" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><strong class="nl fr">ATT</strong> — average treatment effect on treated (customers with the treatment)</li></ul><p id="0530" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">They are both equal to <a class="af of" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">expected values</a> of the differences between potential outcomes either for all units (customers in our case) or only for treated ones.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pw"><img src="../Images/48d2efef7046f835eb795957577489fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvmMLyfSphkppu3IK7O8-g.png"/></div></div></figure><p id="39ff" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">That’s an actual causal effect, and unfortunately, we won’t be able to calculate it. But cheer up; we can still get some estimations. We can observe the difference between values for treated and not treated customers (correlation effect). Let’s try to interpret this value.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq px"><img src="../Images/f03f63cb8b6e4e23862f1ad6101a0527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*id9AcuCVH5qD7LDxi2l7ZA.png"/></div></div></figure><p id="17de" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Using a couple of simple mathematical transformations (i.e. adding and subtracting the same value), we’ve concluded that the average in values between treated and not treated customers equals the sum of <strong class="nl fr">ATT</strong> (average treatment effect on treated) and <strong class="nl fr">bias</strong> term. The bias equals the difference between control and treatment groups without a treatment.</p><p id="5a6f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If we return to our case, the bias will be equal to the difference between expected customer value for the treatment group if they haven’t received discount (<em class="pp">counterfactual outcome</em>) and the control group (<em class="pp">factual outcome</em>).</p><p id="ddad" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In our example, the average value from customers who received a discount will likely be much higher than for those who didn’t. Could we attribute all this effect to our treatment (discount coupon)? Unfortunately not. Since we sent discount to loyal customers who are already spending a lot of money in our product, they would likely have higher value than control group even without a treatment. So, there’s a bias, and we can’t say that the difference in value between two segments equals ATT.</p><p id="9c03" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s think about how to overcome this obstacle. We can do an A/B test: randomly split our loyal customers into two groups and send discount coupons only to half of them. Then, we can estimate the discount’s effect as the average difference between these two groups since we’ve eliminated bias (without treatment, there’s no difference between these groups except for discount).</p><p id="a83e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We’ve covered the basic theory of causal inference and have learned the most crucial concept of bias. So, we are ready to move on to practice. We will start by analysing the A/B test results.</p><h1 id="52f2" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Use case: A/B test</h1><p id="2c53" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Randomised controlled trial (RTC), often called the A/B test, is a powerful tool for getting causal conclusions from data. This approach assumes that we are assigning treatment randomly, and it helps us eliminate bias (since groups are equal without treatment).</p><p id="4c2f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">To practice solving such tasks, we will look at the example based on synthetic data. Suppose we’ve built an LLM-based tool that helps customer support agents answer questions more quickly. To measure the effect, we introduced this tool to half of the agents, and we would like to measure how our treatment (LLM-based tool) affects the outcome (time the agent spends answering a customer’s question).</p><p id="3429" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s have a quick look at the data we have.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq py"><img src="../Images/4b17f04812e378b74e9618bbd68a3626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J67bN8OtiL0iARPLh_Rn2w.png"/></div></div></figure><p id="3d87" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Here are the description of the parameters we logged:</p><ul class=""><li id="fe11" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">case_id</code> — unique ID for the case.</li><li id="5559" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">agent_id</code> — unique ID for the agent.</li><li id="8932" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">treatment</code> equals 1 if agent was in an experiment group and have a chance to use LLMs, 0 — otherwise.</li><li id="7662" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">time_spent_mins</code> — minutes spent answering the customer’s question.</li><li id="f664" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">cs_center</code> — customer support centre. We are working with several CS centres. We launched this experiment in some of them because it’s easier to implement. Such an approach also helps us to avoid contamination (when agents from experiment and control groups interact and can affect each other).</li><li id="bb38" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">complexity</code> equals <code class="cx pr ps pt pu b">low</code>, <code class="cx pr ps pt pu b">medium</code> or <code class="cx pr ps pt pu b">high</code>. This feature is based on the category of the customer’s question and defines how much time an agent is supposed to spend solving this case.</li><li id="0b18" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">tenure</code> — number of months since the agent started working.</li><li id="1d11" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">passed_training</code> — whether the agent passed LLM training. This value can be equal to True only for the treatment group since this training wasn’t offered to the agents from the control group.</li><li id="5afa" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">within_sla</code> equals 1 if the agent was able to answer the question within SLA (15 minutes).</li></ul><p id="9818" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As usual, let’s start with a high-level overview of the data. We have quite a lot of data points, so we will likely be able to get statistically significant results. Also, we can see way lower average response times for the treatment group, so we can hope that the LLM tool really helps.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pz"><img src="../Images/9d29684beee0fa71ec0482a5f954a27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ElUR0QgtEwEcKCnwg_MI1A.png"/></div></div></figure><p id="d738" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">I also usually look at the actual distributions since average statistics might be misleading. In this case, we can see two unimodal distributions without distinctive outliers.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq qa"><img src="../Images/0618db77d718145dbbae08f54f2b5efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79vsGZCaB4X6FrgRX0GDNQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><h2 id="dede" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Classic statistical approach</h2><p id="72f2" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">The classic approach to analysing A/B tests is to use statistical formulas. Using <a class="af of" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html" rel="noopener ugc nofollow" target="_blank">the scipy package</a>, we can calculate the confidence interval for the difference between the two means.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="fbbb" class="qv oh fq pu b bg qw qx l qy qz"># defining samples<br/>control_values = df[df.treatment == 0].time_spent_mins.values<br/>exp_values = df[df.treatment == 1].time_spent_mins.values<br/><br/># calculating p-values<br/>from scipy.stats import ttest_ind<br/><br/>ttest_ind(exp_values, control_values)<br/># Output: TtestResult(statistic=-70.2769283935386, pvalue=0.0, df=89742.0)</span></pre><p id="a112" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We got a p-value below 1%. So, we can reject the null hypothesis and conclude that there’s a difference in average time spent per case in the control and test groups. To understand the effect size, we can also calculate the confidence interval.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="0dff" class="qv oh fq pu b bg qw qx l qy qz">from scipy import stats<br/>import numpy as np<br/><br/># Calculate sample statistics<br/>mean1, mean2 = np.mean(exp_values), np.mean(control_values)<br/>std1, std2 = np.std(exp_values, ddof=1), np.std(control_values, ddof=1)<br/>n1, n2 = len(exp_values), len(control_values)<br/>pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))<br/>degrees_of_freedom = n1 + n2 - 2<br/>confidence_level = 0.95<br/><br/># Calculate margin of error<br/>margin_of_error = stats.t.ppf((1 + confidence_level) / 2, degrees_of_freedom) * pooled_std * np.sqrt(1 / n1 + 1 / n2)<br/><br/># Calculate confidence interval<br/>mean_difference = mean1 - mean2<br/>conf_interval = (mean_difference - margin_of_error, <br/>    mean_difference + margin_of_error)<br/><br/>print("Confidence Interval:", list(map(lambda x: round(x, 3), conf_interval)))<br/># Output: Confidence Interval: [-1.918, -1.814]</span></pre><p id="3a1c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As expected since p-value is below 5%, our confidence interval doesn’t include 0.</p><p id="078f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The traditional approach works. However, we can get the same results with linear regression, which will also allow us to do more advanced analysis later. So, let’s discuss this method.</p><h2 id="03a3" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Linear regression basics</h2><p id="7859" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">As we already discussed, observing both potential outcomes (with and without treatment) for the same object is impossible. Since we won’t be able to estimate the impact on each object individually, we need a model. Let’s assume the constant treatment effect.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/a34416e2ffbd1ae54f91b5d4bc0a350c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNvAypGvzqef7HEWIQAnjA.png"/></div></div></figure><p id="660a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Then, we can write down the relation between outcome (time spent on request) and treatment in the following way, where</p><ul class=""><li id="5497" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">baseline</code> is a constant that shows the basic level of outcome,</li><li id="9469" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk"><code class="cx pr ps pt pu b">residual</code> represents other potential relationships we don’t care about right now (for example, the agent’s maturity or complexity of the case).</li></ul><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/cf0d2ede2be2f6085ee5e6de7eed76e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19xo3bqHPGAgBUtSQjxIDg.png"/></div></div></figure><p id="86ce" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s a linear equation, and we can get the estimation of the <code class="cx pr ps pt pu b">impact</code> variable using linear regression. We will use <a class="af of" href="https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html" rel="noopener ugc nofollow" target="_blank">OLS (Ordinary Least Squares)</a> function from <code class="cx pr ps pt pu b">statsmodels</code> package.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="6f7b" class="qv oh fq pu b bg qw qx l qy qz">import statsmodels.formula.api as smf<br/>model = smf.ols('time_spent_mins ~ treatment', data=df).fit()<br/>model.summary().tables[1]</span></pre><p id="3997" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In the result, we got all the needed info: estimation of the effect (coefficient for the <code class="cx pr ps pt pu b">treatment</code> variable), its p-value and confidence interval.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/a1250cda2826b14e4e2aa2843a3c09f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTEVKIorKPlOnGY8Ri7rXA.png"/></div></div></figure><p id="62dd" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Since the p-value is negligible (definitely below 1%), we can consider the effect significant and say that our LLM-based tool helps to reduce the time spent on a case by 1.866 minutes with a 95% confidence interval (1.814, 1.918). You can notice that we got exactly the same result as with statistical formulas before.</p><h2 id="98af" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Adding more variables</h2><p id="0aea" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">As promised, we can make a more complex analysis with linear regression and take into account more factors, so let’s do it. In our initial approach, we used only one regressor — <code class="cx pr ps pt pu b">treatment</code> flag. However, we can add more variables (for example, <code class="cx pr ps pt pu b">complexity</code>).</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq pv"><img src="../Images/b8c0a722745b049a840d0bd106a0356f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ObO1NyX8Pp7yp_A1megOA.png"/></div></div></figure><p id="5dc6" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this case, the <code class="cx pr ps pt pu b">impact</code> will show estimation after accounting for all the effects of other variables in the model (in our case — task complexity). Let’s estimate it. Adding more variables into the regression model is straightforward — we just need to add another component to the equation.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="f694" class="qv oh fq pu b bg qw qx l qy qz">import statsmodels.formula.api as smf<br/>model = smf.ols('time_spent_mins ~ treatment + complexity', data=df).fit()<br/>model.summary().tables[1]</span></pre><p id="de26" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Now, we see a bit higher estimation of the effect — 1.91 vs 1.87 minutes. Also, the error has decreased (0.015 vs 0.027), and the confidence interval has narrowed.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rc"><img src="../Images/99785fac19bd56dc65e90e10bc1df557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-JS8KDgvQy3YKPPCRIRUA.png"/></div></div></figure><p id="a75d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can also notice that since complexity is a categorical variable, it was automatically converted into a bunch of dummy variables. So, we got estimations of -9.8 minutes for low-complexity tasks and -4.7 minutes for medium ones.</p><p id="1d22" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s try to understand why we got a more confident result after adding complexity. Time spent on a customer case significantly depends on the complexity of the tasks. So, complexity is responsible for a significant amount of our variable’s variability.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rd"><img src="../Images/ac8654d602d8de6ebb548602d07cd8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrANeJzeylbGkvBjQg8NRQ.png"/></div></div></figure><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq re"><img src="../Images/01c195707b6a119837cbae1876ab8c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EjQMebqGJJuJRBpuBmRxw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="87fc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">As I mentioned before, the coefficient for treatment estimates the impact after accounting for all the other factors in the equation. When we added complexity to our linear regression, it reduced the variance of residuals, and that’s why we got a narrower confidence interval for time.</p><p id="a7a9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s double-check that complexity explains a significant proportion of variance. We can see a considerable decrease: time spent has a variance equal to 16.6, but when we account for complexity, it reduces to just 5.9.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="a2de" class="qv oh fq pu b bg qw qx l qy qz">time_model = smf.ols('time_spent_mins ~ complexity', data=df).fit()<br/><br/>print('Initial variance: %.2f' % (df.time_spent_mins.var()))<br/>print('Residual variance after accounting for complexity: %.2f' \<br/>  % (time_model.resid.var()))<br/><br/># Output: <br/># Initial variance: 16.63<br/># Residual variance after accounting for complexity: 5.94</span></pre><p id="656a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we can see that adding a factor that can predict the outcome variable to a linear regression can improve your effect size estimations. Also, it’s worth noting that the variable is not correlated with treatment assignment (the tasks of each complexity have equal chances to be in the control or test group).</p><p id="b446" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Traditionally, causal graphs are used to show the relationships between the variables. Let’s draw such a graph to represent our current situation.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rf"><img src="../Images/3c6779f99501a436ef225a281d211276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N29E4akuek2VhEZKzQZbAw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><h2 id="7231" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Non-linear relationships</h2><p id="ea42" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">So far, we’ve looked only at linear relationships, but sometimes, it’s not enough to model our situation.</p><p id="d34d" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s look at the data on LLM training that agents from the experiment group were supposed to pass. Only half of them have passed the LLM training and learned how to use the new tool effectively.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rb"><img src="../Images/ff3651a27de9dced705b4343c85321ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7lBtaGfWXqXqTKNT0KdWw.png"/></div></div></figure><p id="b072" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see a significant difference in average time spent for the treatment group who passed training vs. those who didn’t.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rg"><img src="../Images/26136b99221d86cf69ed6ac29f818e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ks55hd6dFDqSkoWy8XTO5Q.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="3e28" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we should expect different impacts from treatment for these two groups. We can use non-linearity to express such relationships in formulas and add <code class="cx pr ps pt pu b">treatment * passed_training</code> component to our equation.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="cdf0" class="qv oh fq pu b bg qw qx l qy qz">model = smf.ols('time_spent_mins ~ treatment * passed_training + complexity', <br/>    data=df).fit()<br/>model.summary().tables[1]</span></pre><p id="dbad" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The <code class="cx pr ps pt pu b">treatment</code> and <code class="cx pr ps pt pu b">passed_training</code> factors will also be automatically added to the regression. So, we will be optimising the following formula.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rh"><img src="../Images/4adf8c531b178069d7f825aadad48b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AJC10v9ayFASAAD1n5ECIQ.png"/></div></div></figure><p id="079f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We got the following results from the linear regression.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ri"><img src="../Images/366f32acd34e72a3bff332272c4cc710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LC42lgWIAzzvFTIxjwGBA.png"/></div></div></figure><p id="bc60" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">No statistically significant effect is associated with passed training since the p-value is above 5%, while other coefficients differ from zero.</p><p id="bf4b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s put down all the different scenarios and estimate the effects using the coefficients we got from the linear regression.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq mr"><img src="../Images/7de8d3b2b49905d57eac953f7834a4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHezct6jlrGNkOlWHxfFbA.png"/></div></div></figure><p id="b820" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">So, we’ve got new treatment estimations: 2.5 minutes improvement per case for the agents who have passed the training and 1.3 minutes — for those who didn’t.</p><h2 id="c2a3" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Confounders</h2><p id="e0e1" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Before jumping to conclusions, it’s worth double-checking some assumptions we made — for example, random assignment. We’ve discussed that we launched the experiment in some CS centres. Let’s check whether agents in the different centres are similar so that our control and test groups are non-biased.</p><p id="6ab3" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We know that agents differ by experience, which might significantly affect their performance. Our day-to-day intuition tells us that more experienced agents will spend less time on tasks. We can see in the data that it is actually like this.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rj"><img src="../Images/400aa324a10620d11a45025f5eaf720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CxAFgSePZkcUrYFerY2HuQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="dead" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s see whether our experiment and control have the same level of agents’ experience. The easiest way to do it is to look at distributions.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rk"><img src="../Images/de79e6081534f5ee8a6fc6c2bae52be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9nwIacThexPvGPxVXPQag.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="9e23" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Apparently, agents in the treatment group have much more experience than the ones in the control group. Overall, it makes sense that the product team decided to launch the experiment, starting with the more trained agents. However, it breaks our assumption about random assignment. Since the control and test groups are different even without treatment, we are overestimating the effect of our LLM tool on the agents’ performance.</p><p id="4b4b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s return to our causal graph. The agent’s experience affects both treatment assignment and output variable (time spent). Such variables are called confounders.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rl"><img src="../Images/b740623812980bbad8c2b213b3a344f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JN0AfiyATIJf6J-Oc8EyUA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="e53a" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Don’t worry. We can solve this issue effortlessly — we just need to include confounders in our equation to control for it. When we add it to the linear regression, we start to estimate the treatment effect with fixed experience, eliminating the bias. Let’s try to do it.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="d5a7" class="qv oh fq pu b bg qw qx l qy qz">model = smf.ols('time_spent_mins ~ treatment * passed_training + complexity + tenure', data=df).fit()<br/>model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rm"><img src="../Images/20eb8218d670ad69fd61b7214ab049b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tsMkR4agbzRRoKEsrzx9Hw.png"/></div></div></figure><p id="1dbc" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With added tenure, we got the following results:</p><ul class=""><li id="84b1" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">There is no statistically significant effect of passed training or treatment alone since the p-value is above 5%. So, we can conclude that an LLM helper does not affect agents’ performance unless they have passed the training. In the previous iteration, we saw a statistically significant effect, but it was due to tenure confounding bias.</li><li id="13e5" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">The only statistically significant effect is for the treatment group with passed training. It equals 1.07 minutes with a 95% confidence interval (1.02, 1.11).</li><li id="1167" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Each month of tenure is associated with 0.05 minutes less time spent on the task.</li></ul><p id="9026" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We are working with synthetic data so we can easily compare our estimations with actual effects. The LLM tool reduces the time spent per task by 1 minute if the agent has passed the training, so our estimations are pretty accurate.</p><h2 id="6841" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Bad controls</h2><p id="826f" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">Machine learning tasks are often straightforward: you gather data with all possible features you can get, try to fit some models, compare their performance and pick the best one. Contrarily, causal inference requires some art and a deep understanding of the process you’re working with. One of the essential questions is what features are worth including in regression and which ones will spoil your results.</p><p id="e769" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Till now, all the additional variables we’ve added to the linear regression have been improving the accuracy. So, you might think adding all your features to regression will be the best strategy. Unfortunately, it’s not that easy for causal inference. In this section, we will look at a couple of cases when additional variables decrease the accuracy of our estimations.</p><p id="264c" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For example, we have a CS centre in data. We’ve assigned treatment based on the CS centre, so including it in the regression might sound reasonable. Let’s try.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="05ea" class="qv oh fq pu b bg qw qx l qy qz">model = smf.ols('time_spent_mins ~ treatment + complexity + tenure + cs_center', <br/>    data=df[df.treatment == df.passed_training]).fit()<br/>model.summary().tables[1]</span></pre><p id="2d08" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">For simplicity, I’ve removed non-linearity from our dataset and equation, filtering out cases where the agents from the treatment groups didn’t pass the LLM training.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rn"><img src="../Images/6f00e87b17147d07b8bd183307812840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCGkDhz7H-RLunmz6EmAEw.png"/></div></div></figure><p id="0d28" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">If we include the CS centre in linear regression, we will get a ridiculously high estimation of the effect (around billions) without statistical significance. So, this variable is rather harmful than helpful.</p><p id="d743" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s update a causal chart and try to understand why it doesn’t work. CS centre is a predictor for our treatment but has no relationship with the output variable (so it’s not a confounder). Adding a treatment predictor leads to <a class="af of" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">multicollinearity</a> (like in our case) or reduces the treatment variance (it’s challenging to estimate the effect of treatment on the output variable since treatment doesn’t change much). So, it’s a bad practice to add such variables to the equation.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ro"><img src="../Images/ada33c9d3c4a1833ef16078795c03123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOKV4dqvY7xlO4uqX2DRIQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="bd6b" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s move on to another example. We have a <code class="cx pr ps pt pu b">within_sla</code> variable showing whether the agents finished the task within 15 minutes. Can this variable improve the quality of our effect estimations? Let’s see.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="b46a" class="qv oh fq pu b bg qw qx l qy qz">model = smf.ols('time_spent_mins ~ treatment + complexity + tenure + within_sla', <br/>    data=df[df.treatment == df.passed_training]).fit()<br/>model.summary().tables[1]</span></pre><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rp"><img src="../Images/8d1a2c67d5cab2044088ec1ff89b0689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NwI4GGKmMRk_MwrhZ8rgrQ.png"/></div></div></figure><p id="8c01" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">The new effect estimation is way lower: 0.8 vs 1.1 minutes. So, it poses a question: which one is more accurate? We’ve added more parameters to this model, so it’s more complex. Should it give more precise results, then? Unfortunately, it’s not always like that. Let’s dig deeper into it.</p><p id="a0c7" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In this case, <code class="cx pr ps pt pu b">within_sla</code> flag shows whether the agent solved the problem within 15 minutes or the question took more time. So, if we return to our causal chart, <code class="cx pr ps pt pu b">within_sla</code> flag is an outcome of our output variable (time spent on the task).</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rq"><img src="../Images/057099dd7a9f9fddbd00dcd8a695f64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTiP7SHeiyer9vFA5s3EMw.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><p id="ae15" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">When we add the <code class="cx pr ps pt pu b">within_slag</code> flag into regression and control for it, we are starting to estimate the effect of treatment with a fixed value of <code class="cx pr ps pt pu b">within_sla</code>. So, we will have two cases: <code class="cx pr ps pt pu b">within_sla = 1</code> and <code class="cx pr ps pt pu b">within_sla = 0</code>. Let’s look at the bias for each of them.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ra"><img src="../Images/810ed377efda1f0430563b7a948eef48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XcsaKoP3V8GNnPgc4LC7ig.png"/></div></div></figure><p id="156e" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In both cases, bias is not equal to 0, which means our estimation is biased. At first glance, it might look a bit counterintuitive. Let me explain the logic behind it a bit.</p><ul class=""><li id="d49a" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">In the first equation, we compare cases where agents finished the tasks within 15 minutes with the help of the LLM tool and without. The previous analysis shows that the LLM tool (our treatment) tends to speed up agents’ work. So, if we compare the expected time spent on tasks without treatments (when agents work independently without the LLM tool), we should expect quicker responses from the second group.</li><li id="85c0" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Similarly, for the second equation, we are comparing agents who haven’t completed tasks within 15 minutes, even with the help of LLM and those who did it on their own. Again, we should expect longer response times from the first group without treatment.</li></ul><p id="5c11" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s an example of selection bias — a case when we control for a variable on the path from treatment to output variable or outcome of the output variable. Controlling for such variables in a linear regression also leads to biased estimations, so don’t do it.</p><h2 id="4282" class="qb oh fq bf oi qc qd qe ol qf qg qh oo ns qi qj qk nw ql qm qn oa qo qp qq qr bk">Grouped data</h2><p id="7f6a" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">In some cases, you might not have granular data. In our example, we might not know the time spent on each task individually, but know the averages. It’s easier to track aggregated numbers for agents. For example, “within two hours, an agent closed 15 medium tasks”. We can aggregate our raw data to get such statistics.</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="2d1f" class="qv oh fq pu b bg qw qx l qy qz">agents_df = df.groupby(['agent_id', 'treatment', 'complexity', 'tenure', <br/>  'passed_training'], as_index = False).aggregate(<br/>    {'case_id': 'nunique', 'time_spent_mins': 'mean'}<br/>)</span></pre><p id="68d9" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">It’s not a problem for linear regression to deal with agent-level data. We just need to specify weights for each agent (equal to the number of cases).</p><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="9200" class="qv oh fq pu b bg qw qx l qy qz"><br/>model = smf.ols('time_spent_mins ~ treatment + complexity + tenure', <br/>    data = agents_df[agents_df.treatment == agents_df.passed_training],<br/>    weights = agents_df[agents_df.treatment == agents_df.passed_training]['case_id'])\<br/>    .fit()<br/>model.summary().tables[1]</span></pre><p id="ad36" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With aggregated data, we have roughly the same results for the effect of treatment. So, there’s no problem if you have only average numbers.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rr"><img src="../Images/34da6d9271c106fb0dcdad1e9df19225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7avIR8k81v7toMh-D601A.png"/></div></div></figure><h1 id="6a8a" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Use case: observational data</h1><p id="5960" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We’ve looked at the A/B test examples for causal inference in detail. However, in many cases, we can’t conduct a proper randomised trial. Here are some examples:</p><ul class=""><li id="d4be" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">Some experiments are unethical. For example, you can’t push students to drink alcohol or smoke to see how it affects their performance at university.</li><li id="9c3d" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">In some cases, you might be unable to conduct an A/B test because of legal limitations. For example, you can’t charge different prices for the same product.</li><li id="bd0c" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Sometimes, it’s just impossible. For example, if you are working on an extensive rebranding, you will have to launch it globally one day with a big PR announcement.</li></ul><p id="a37f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">In such cases, you have to use just observations to make conclusions. Let’s see how our approach works in such a case. We will use the <a class="af of" href="https://archive.ics.uci.edu/dataset/320/student+performance" rel="noopener ugc nofollow" target="_blank">Student Performance data set</a> from the UC Irvine Machine Learning Repository.</p><p id="598f" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Let’s use this real-life data to investigate how willingness to take higher education affects the math class’s final score. We will start with a trivial model and a causal chart.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rs"><img src="../Images/b01f701a1cea9821fefc8cd9a1571485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NsleKrwmhkS1zxgckenhnA.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="80a0" class="qv oh fq pu b bg qw qx l qy qz">df = pd.read_csv('student-mat.csv', sep = ';')<br/>model = smf.ols('G3 ~ higher', data=df).fit()<br/>model.summary().tables[1]</span></pre><p id="26ae" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see that willingness to continue education statistically significantly increases the final grade for the course by 3.8 points.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rt"><img src="../Images/47146d5433494b7f68d962b5a37459f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxIdeHirPYDgneynFZRYlA.png"/></div></div></figure><p id="7fcb" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, there might be some confounders that we have to control for. For example, parents’ education can affect both treatments (<em class="pp">children are more likely to plan to take higher education if their parents have it</em>) and outcomes (<em class="pp">educated parents are more likely to help their children so that they have higher grades</em>). Let’s add the mother and father’s education level to the model.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq ru"><img src="../Images/4fab84acf7273347f617d81d79bc1166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6SCCjAK9h5GBwB3SjCwoQ.png"/></div></div><figcaption class="ne nf ng mp mq nh ni bf b bg z dx">Image by author</figcaption></figure><pre class="ms mt mu mv mw qs pu qt bp qu bb bk"><span id="0910" class="qv oh fq pu b bg qw qx l qy qz">model = smf.ols('G3 ~ higher + Medu + Fedu', data=df).fit()<br/>model.summary().tables[1]</span></pre><p id="7412" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">We can see a statistically significant effect from the mother’s education. We likely improved the accuracy of our estimation.</p><figure class="ms mt mu mv mw mj mp mq paragraph-image"><div role="button" tabindex="0" class="mz na ed nb bh nc"><div class="mp mq rc"><img src="../Images/1873559d8ab4425542352f47eeab6e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQdEYpOdls-p6AsC6k4bcA.png"/></div></div></figure><p id="5e36" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">However, we should treat any causal conclusions based on observational data with a pinch of salt. We can’t be sure that we’ve taken into account all confounders and that the estimation we’ve got is entirely unbiased.</p><p id="e9e2" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Also, it might be tricky to interpret the direction of the relation. We are sure there’s a correlation between willingness to continue education and final grade. However, we can interpret it in multiple ways:</p><ul class=""><li id="f6b0" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">Students who want to continue their education are more motivated, so they have higher final grades.</li><li id="5915" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Students with higher final grades are inspired by their success in studying, and that’s why they want to continue their education.</li></ul><p id="90a8" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">With observational data, we can only use our common sense to choose one option or the other. There’s no way to infer this conclusion from data.</p><p id="8562" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Despite the limitations, we can still use this tool to try our best to come to some conclusions about the world. As I mentioned, causal inference is based significantly on domain knowledge and common sense, so it’s worth spending time near the whiteboard to think deeply about the process you’re modelling. It will help you to achieve excellent results.</p><blockquote class="rv rw rx"><p id="5346" class="nj nk pp nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">You can find complete code for these examples on <a class="af of" href="https://github.com/miptgirl/miptgirl_medium/tree/main/causal_inference_linear_regression" rel="noopener ugc nofollow" target="_blank">GitHub</a>.</p></blockquote><h1 id="26ed" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Summary</h1><p id="2ab3" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk">We’ve discussed quite a broad topic of causal inference, so let me recap what we’ve learned:</p><ul class=""><li id="2e24" class="nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe ph pi pj bk">The main goal of predictive analytics is to get accurate forecasts. The causal inference is focused on understanding the relationships, so we care more about the coefficients in the model than the actual predictions.</li><li id="ca98" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">We can leverage linear regression to get the causal conclusions.</li><li id="77bf" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">Understanding what features we should add to the linear regression is an art, but here is some guidance. <br/> — You must include confounders (features that affect both treatment and outcome).<br/> — Adding a feature that predicts the output variable and explains its variability can help you to get more confident estimations.<br/> — Avoid adding features that either affect only treatment or are the outcome of the output variable.</li><li id="fc79" class="nj nk fq nl b go pk nn no gr pl nq nr ns pm nu nv nw pn ny nz oa po oc od oe ph pi pj bk">You can use this approach for both A/B tests and observational data. However, with observations, we should treat our causal conclusions with a pinch of salt because we can never be sure that we accounted for all confounders.</li></ul><blockquote class="rv rw rx"><p id="df5a" class="nj nk pp nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">Thank you a lot for reading this article. If you have any follow-up questions or comments, please leave them in the comments section.</p></blockquote><h1 id="4103" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Dataset</h1><p id="6f4b" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk"><em class="pp">Cortez, Paulo. (2014). Student Performance.</em> <em class="pp">UCI Machine Learning Repository (CC BY 4.0). </em><a class="af of" href="https://doi.org/10.24432/C5TG7T." rel="noopener ugc nofollow" target="_blank">https://doi.org/10.24432/C5TG7T</a></p><h1 id="09d4" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Reference</h1><p id="cfeb" class="pw-post-body-paragraph nj nk fq nl b go pc nn no gr pd nq nr ns pe nu nv nw pf ny nz oa pg oc od oe fj bk"><em class="pp">All the images are produced by the author unless otherwise stated.</em></p><p id="e711" class="pw-post-body-paragraph nj nk fq nl b go nm nn no gr np nq nr ns nt nu nv nw nx ny nz oa ob oc od oe fj bk">This article is inspired by the book <a class="af of" href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html" rel="noopener ugc nofollow" target="_blank">Causal Inference for the Brave and True</a> that gives a wonderful overview on the causal inference basics.</p></div></div></div></div>    
</body>
</html>