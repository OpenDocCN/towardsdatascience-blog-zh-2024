["```py\nimport sys\nprint(sys.version) # displays the version of python installed\n```", "```py\n# import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n```", "```py\n# Import the dataset for review as a DataFrame\ndf = pd.read_csv(\"../input/water-potability/water_potability.csv\")\n```", "```py\n# Understand missing values per variable within DataFrame\n(\n    df\n    .isnull().sum()\n)\n```", "```py\n# Apply mean value to the missing values\ndf['ph'].fillna(df['ph'].mean(), inplace=True)\ndf['Sulfate'].fillna(df['Sulfate'].mean(), inplace=True)\ndf['Trihalomethanes'].fillna(df['Trihalomethanes'].mean(), inplace=True)\ndf.isnull().sum()\n```", "```py\n# Make updates with chaining method, allows for use of comments to update the columns.\n# A new dataframe variable (df_1) can be assigned this output\ndf1 = (\n    df\n#     .isnull().sum()\n    .assign(ph=lambda df_:df_.ph.fillna(df_.ph.mean()),\n            Sulfate=lambda df_:df_.Sulfate.fillna(df_.Sulfate.mean()),\n            Trihalomethanes=lambda df_:df_.Trihalomethanes.fillna(df_.Trihalomethanes.mean())\n           )\n)\n\n# Confirm that the columns have been updated\ndf1.isnull().sum()\n```", "```py\n# Separate into X and y variables\nX, y = df1.drop(['Potability'], axis=1), df1['Potability'].values\n\n# Show that only independent variables have been retained\nX.head()\n```", "```py\n# Dummy classifier - create a baseline accuracy score\nfrom sklearn.dummy import DummyClassifier\n\n# Define the reference model\ndummy_clf = DummyClassifier(strategy='most_frequent')\n\n# Fit the model\ndummy_clf.fit(X, y)\n\n# Predict the model\ndummy_clf.predict(X)\n\n# Evaluate the model\nscore = dummy_clf.score(X, y)\nprint(score)\n\n# Print statement displayed value\n0.6098901098901099\n```", "```py\n# Review the dependent variable frequency and percentage\n(\n    df1\n    .Potability\n#     .value_counts()\n    .value_counts(normalize=True) # display frequencies as a percentage\n)\n```", "```py\n# Lets try a Light GBM\nfrom lightgbm import LGBMClassifier\n\n# ML Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# ML Performance metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n```", "```py\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=2, stratify=y)\n\n# Instantiate the LGBM\nlgbm = LGBMClassifier()\n\n# Fit the classifier to the training data\nlgbm.fit(X_train, y_train)\n\n# Perform prediction\ny_pred = lgbm.predict(X_test)\n\n# Print the accuracy\nprint(lgbm.score(X_test, y_test))\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n```", "```py\n# Lets understand the baseline params\nlgbm.get_params()\n```", "```py\n# Setup the pipeline\nsteps = [('scaler', StandardScaler()),\n         ('lgbm', LGBMClassifier())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {\n    'lgbm__learning_rate':[0.03, 0.05, 0.1],\n    'lgbm__objective':['binary'],\n    'lgbm__metric':['binary_logloss'],\n    'lgbm__max_depth':[10],\n    'lgbm__n_estimators':[100, 200, 300]\n}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Instantiate the GridSearchCV object\ncv = GridSearchCV(pipeline, parameters, cv=3)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = cv.predict(X_test)\n```", "```py\n# Display best score and params\nprint(f'Best score : {cv.best_score_}')\nprint(f'Best params : {cv.best_params_}')\n\n# Compute and print metrics\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\n```"]