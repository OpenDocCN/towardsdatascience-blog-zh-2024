<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Lasso and Elastic Net Regressions, Explained: A Visual Guide with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Lasso and Elastic Net Regressions, Explained: A Visual Guide with Code Examples</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/lasso-and-elastic-net-regressions-explained-a-visual-guide-with-code-examples-5fecf3e1432f?source=collection_archive---------1-----------------------#2024-12-06">https://towardsdatascience.com/lasso-and-elastic-net-regressions-explained-a-visual-guide-with-code-examples-5fecf3e1432f?source=collection_archive---------1-----------------------#2024-12-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="8114" class="fo fp fq bf b dy fr fs ft fu fv fw dx fx" aria-label="kicker paragraph">REGRESSION ALGORITHM</h2><div/><div><h2 id="acd5" class="pw-subtitle-paragraph gs fz fq bf b gt gu gv gw gx gy gz ha hb hc hd he hf hg hh cq dx">Roping in key features with coordinate descent</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hi hj hk hl hm ab"><div><div class="ab hn"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@samybaladram?source=post_page---byline--5fecf3e1432f--------------------------------" rel="noopener follow"><div class="l ho hp by hq hr"><div class="l ed"><img alt="Samy Baladram" class="l ep by dd de cx" src="../Images/715cb7af97c57601966c5d2f9edd0066.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="hs by l dd de em n ht eo"/></div></div></a></div></div><div class="hu ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--5fecf3e1432f--------------------------------" rel="noopener follow"><div class="l hv hw by hq hx"><div class="l ed"><img alt="Towards Data Science" class="l ep by br hy cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hs by l br hy em n ht eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hz ab q"><div class="ab q ia"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b ib ic bk"><a class="af ag ah ai aj ak al am an ao ap aq ar id" data-testid="authorName" href="https://medium.com/@samybaladram?source=post_page---byline--5fecf3e1432f--------------------------------" rel="noopener follow">Samy Baladram</a></p></div></div></div><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b ib ic dx"><button class="ig ih ah ai aj ak al am an ao ap aq ar ii ij ik" disabled="">Follow</button></p></div></div></span></div></div><div class="l il"><span class="bf b bg z dx"><div class="ab cn im in io"><div class="ip iq ab"><div class="bf b bg z dx ab ir"><span class="is l il">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar id ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--5fecf3e1432f--------------------------------" rel="noopener follow"><p class="bf b bg z it iu iv iw ix iy iz ja bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="ie if" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">15 min read</span><div class="jb jc l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Dec 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js"><div class="h k w ea eb q"><div class="ki l"><div class="ab q kj kk"><div class="pw-multi-vote-icon ed is kl km kn"><div class=""><div class="ko kp kq kr ks kt ku am kv kw kx kn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l ky kz la lb lc ld le"><p class="bf b dy z dx"><span class="kp">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao ko lf lg ab q ee lh li" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lj"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh"><div class="lk k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al ll an ao ap ii lm ln lo" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lp cn"><div class="l ae"><div class="ab cb"><div class="lq lr ls lt lu lv ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al ll an ao ap ii lw lx li ly lz ma mb mc s md me mf mg mh mi mj u mk ml mm"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><div class="mn mo mp mq mr ms"><a rel="noopener follow" target="_blank" href="/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4?source=post_page-----5fecf3e1432f--------------------------------"><div class="mt ab il"><div class="mu ab co cb mv mw"><h2 class="bf ga ib z it mx iv iw my iy ja fz bk">Least Squares Regression, Explained: A Visual Guide with Code Examples for Beginners</h2><div class="mz l"><h3 class="bf b ib z it mx iv iw my iy ja dx">Gliding through points to minimize squares</h3></div><div class="gq l"><p class="bf b dy z it mx iv iw my iy ja dx">towardsdatascience.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf lv ms"/></div></div></a></div><p id="6bed" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Linear regression comes in different types: <a class="af oc" rel="noopener" target="_blank" href="/least-squares-regression-explained-a-visual-guide-with-code-examples-for-beginners-2e5ad011eae4">Least Squares methods</a> form the foundation, from the classic Ordinary Least Squares (OLS) to Ridge regression with its regularization to prevent overfitting. Then there‚Äôs Lasso regression, which takes a unique approach by automatically selecting important factors and ignoring others. Elastic Net combines the best of both worlds, mixing Lasso‚Äôs feature selection with Ridge‚Äôs ability to handle related features.</p><p id="b568" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">It‚Äôs frustrating to see many articles treat these methods as if they‚Äôre basically the same thing with minor tweaks. They make it seem like switching between them is as simple as changing a setting in your code, but each actually uses different approaches to solve their optimization problems!</p><p id="52e1" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">While OLS and Ridge regression can be solved directly through matrix operations, Lasso and Elastic Net require a different approach ‚Äî an iterative method called <strong class="ni ga">coordinate descent</strong>. Here, we‚Äôll explore how this algorithm works through clear visualizations. So, let‚Äôs saddle up and <em class="od">lasso</em> our way through the details!</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/89cfee410d2caa68f774230733aaf4c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xj8V9OkjOOeeoM40GzTjAA.gif"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">All visuals: Author-created using Canva Pro. Optimized for mobile; may appear oversized on desktop.</figcaption></figure><h1 id="9976" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Definition</h1><h2 id="fe30" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Lasso Regression</h2><p id="8aff" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk"><strong class="ni ga">LASSO</strong> (<strong class="ni ga">L</strong>east <strong class="ni ga">A</strong>bsolute <strong class="ni ga">S</strong>hrinkage and <strong class="ni ga">S</strong>election<strong class="ni ga"> O</strong>perator) is a variation of Linear Regression that adds a penalty to the model. It uses a linear equation to predict numbers, just like Linear Regression. However, Lasso also has a way to reduce the importance of certain factors to zero, which makes it useful for two main tasks: making predictions and identifying the most important features.</p><h2 id="b46d" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Elastic Net Regression</h2><p id="1ed3" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Elastic Net Regression is a mix of Ridge and Lasso Regression that combines their penalty terms. The name ‚ÄúElastic Net‚Äù comes from physics: just like an elastic net can stretch and still keep its shape, this method adapts to data while maintaining structure.</p><p id="6974" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The model balances three goals: minimizing prediction errors, keeping the size of coefficients small (like Lasso), and preventing any coefficient from becoming too large (like Ridge). To use the model, you input your data‚Äôs feature values into the linear equation, just like in standard Linear Regression.</p><p id="cd04" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The main advantage of Elastic Net is that when features are related, it tends to keep or remove them as a group instead of randomly picking one feature from the group.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/92cd64e687e3a8222ab9d9001eead95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w9cUH9iv-37ZCBVq0NKPOA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Linear models like Lasso and Elastic Net belong to the broader family of machine learning methods that predict outcomes using linear relationships between variables.</figcaption></figure><h1 id="e5da" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">üìä Dataset Used</h1><p id="57c7" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">To illustrate our concepts, we‚Äôll use <a class="af oc" rel="noopener" target="_blank" href="/dummy-regressor-explained-a-visual-guide-with-code-examples-for-beginners-4007c3d16629">our standard dataset</a> that predicts the number of golfers visiting on a given day, using features like weather outlook, temperature, humidity, and wind conditions.</p><p id="4f22" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">For both Lasso and Elastic Net to work effectively, we need to standardize the numerical features (making their scales comparable) and apply one-hot-encoding to categorical features, as both models‚Äô penalties are sensitive to feature scales.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/d5b052ff81db21a6616645b0912d1802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GGSQZTbZQlQMlWe7YsJJUg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Columns: ‚ÄòOutlook‚Äô (one-hot encoded to sunny, overcast, rain), ‚ÄòTemperature‚Äô (standardized), ‚ÄòHumidity‚Äô (standardized), ‚ÄòWind‚Äô (Yes/No) and ‚ÄòNumber of Players‚Äô (numerical, target feature)</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="eb64" class="qt oy fq qq b bg qu qv l qw qx">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'])<br/>df['Wind'] = df['Wind'].astype(int)<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/><br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)</span></pre><h1 id="b98d" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Main Mechanism</h1><p id="e5db" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Lasso and Elastic Net Regression predict numbers by making a straight line (or hyperplane) from the data, while controlling the size of coefficients in different ways:</p><ol class=""><li id="d579" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob qy qz ra bk">Both models find the best line by balancing prediction accuracy with coefficient control. They work to make the gaps between real and predicted values small, while keeping coefficients in check through penalty terms.</li><li id="f74d" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob qy qz ra bk">In Lasso, the penalty (controlled by <em class="od">Œª</em>) can shrink coefficients to exactly zero, removing features entirely. Elastic Net combines two types of penalties: one that can remove features (like Lasso) and another that shrinks groups of related features together. The mix between these penalties is controlled by the <code class="cx rg rh ri qq b">l1_ratio</code> (<em class="od">Œ±</em>).</li><li id="092d" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob qy qz ra bk">To predict a new answer, both models multiply each input by its coefficient (if not zero) and add them up, plus a starting number (intercept/bias). Elastic Net often keeps more features than Lasso but with smaller coefficients, especially when features are correlated.</li><li id="c93d" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob qy qz ra bk">The strength of penalties affects how the models behave:<br/>- In Lasso, larger <em class="od">Œª</em> means more coefficients become zero<br/>- In Elastic Net, <em class="od">Œª</em> controls overall penalty strength, while <em class="od">Œ±</em> determines the balance between feature removal and coefficient shrinkage<br/>- When penalties are very small, both models act more like standard Linear Regression</li></ol><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/a24d6aea0a7577617aaccec1a0dda614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PMv-ypXt-E2NNrUCOHGLWg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Lasso and Elastic Net make predictions by multiplying input features with their trained weights and adding them together with a bias term to produce a final output value.</figcaption></figure><h1 id="54fb" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Training Steps</h1><p id="8b80" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Let‚Äôs explore how Lasso and Elastic Net learn from data using the coordinate descent algorithm. While these models have complex mathematical foundations, we‚Äôll focus on <strong class="ni ga">understanding coordinate descent</strong> ‚Äî an efficient optimization method that makes the computation more practical and intuitive.</p><h2 id="5118" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Coordinate Descent for Lasso Regression</h2><p id="c6df" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">The optimization problem of Lasso Regression is as follows:</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/ffc77f9feca66a0d82b821c01933508b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zO5FvvbZaTWmMfiVsHUYfA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">While <a class="af oc" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html" rel="noopener ugc nofollow" target="_blank">scikit-learn implementation</a> includes additional scaling factors (1/(2*n_samples)) for computational efficiency, we‚Äôll use the standard theoretical form for clarity in our explanation.</figcaption></figure><p id="c4aa" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Here‚Äôs how coordinate descent finds the optimal coefficients by updating one feature at a time:</p><p id="4347" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">1. Start by initializing the model with all coefficients at zero. Set a fixed value for the regularization parameter that will control the strength of the penalty.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/063f2e17b33d37e82d58982abab75518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrhKCWT9oWO5pmAsY3_ZKg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Lasso regression begins with all feature weights set to zero and uses a penalty parameter (Œª) to control how much it shrinks weights during training.</figcaption></figure><p id="c41d" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">2. Calculate the initial bias by taking the mean of all target values.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/f2df6fc11bb7aa805db1e21ff38c5325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHhlyziQ9f_oTdQ35DEcGA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">The initial bias value is set to 37.43, which is calculated by taking the average of all target values in the training data (mean of player counts shown from index 0 to 13).</figcaption></figure><p id="5e6b" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">3. For updating the first coefficient (in our case, ‚Äòsunny‚Äô): <br/>- Using weighted sum, calculate what the model would predict <strong class="ni ga">without using this feature.</strong></p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/16a9669367286fe143c27efffb821f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xF2KwO5yXlaL_LAFNlTojA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">At the start of training, all feature weights are set to zero while using the initial bias of 37.43, causing the model to predict the same average value (37.43) for all training examples regardless of their input features.</figcaption></figure><p id="5588" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">- Find the partial residual ‚Äî how far off these predictions are from the actual values. Using this value, calculate the temporary coefficient.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/0431216a648bff8bd9d223beb5d087ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nTSAEGLDhNiwViNbUOtvg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">For the first feature, Lasso calculates a temporary coefficient of 11.17 by comparing the true labels with predictions, considering only the rows where this feature equals 1, and applying the gradient formula.</figcaption></figure><p id="8774" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">- Apply the Lasso shrinkage (soft thresholding) to this temporary coefficient to get the final coefficient for this step.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/290cc6c85081860c00fb5348ecf1e598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tVz4QwDTTydHXsQ61f0Aw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Lasso applies its shrinkage formula to the temporary coefficient (11.17), where it subtracts the penalty term (Œª/5 = 0.2) from the absolute value while preserving the sign, resulting in a final coefficient of 10.97.</figcaption></figure><p id="9da3" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">4. Move through each remaining coefficient one at a time, repeating the same update process. When calculating predictions during each update, use the most recently updated values for all other coefficients.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rj"><img src="../Images/970b037792a6038bf56c11b1122af6ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFpN1UEl4ekv7JFFRjdEVg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After updating the first coefficient to 10.97, Lasso uses these updated predictions to calculate the temporary coefficient (0.32) for the second feature, showing how the algorithm updates coefficients one at a time through coordinate descent.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="ac40" class="qt oy fq qq b bg qu qv l qw qx">import numpy as np<br/><br/># Initialize bias as mean of target values and coefficients to 0<br/>bias = np.mean(y_train)<br/>beta = np.zeros(X_train_scaled.shape[1])<br/>lambda_param = 1<br/><br/># One cycle through all features<br/>for j, feature in enumerate(X_train_scaled.columns):<br/>    # Get current feature values<br/>    x_j = X_train_scaled.iloc[:, j].values<br/><br/>    # Calculate prediction excluding the j-th feature<br/>    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]<br/><br/>    # Calculate partial residuals<br/>    residual_no_j = y_train.values - y_pred_no_j<br/><br/>    # Calculate the dot product of x_j with itself (sum of squared feature values)<br/>    sum_squared_x_j = np.dot(x_j, x_j)<br/><br/>    # Calculate temporary beta without regularization (raw update)<br/>    beta_old = beta[j]<br/>    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j<br/><br/>    # Apply soft thresholding for Lasso penalty<br/>    beta[j] = np.sign(beta_temp) * max(abs(beta_temp) - lambda_param / sum_squared_x_j, 0)<br/><br/># Print results<br/>print("Coefficients after one cycle:")<br/>for feature, coef in zip(X_train_scaled.columns, beta):<br/>    print(f"{feature:11}: {coef:.2f}")</span></pre><p id="e387" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">5. Return to update the bias by calculating what the current model predicts <strong class="ni ga">using all features</strong>, then adjust the bias based on the average difference between these predictions and actual values.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rk"><img src="../Images/2ad780f7356dc1034f149cd228fb7f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3BTCUD3znSUPIWZBjd0LNA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After updating all feature coefficients through coordinate descent, the model recalculates the bias (40.25) as the mean difference between the true labels and the predictions made using the current feature weights, ensuring the model‚Äôs predictions are properly centered around the target values.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="3ec6" class="qt oy fq qq b bg qu qv l qw qx"># Update bias (not penalized by lambda)<br/>y_pred = X_train_scaled.values @ beta  # only using coefficients, no bias<br/>residuals = y_train.values - y_pred<br/>bias = np.mean(residuals)  # this replaces the old bias</span></pre><p id="dd2d" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">6. Check if the model has converged either by reaching the maximum number of allowed iterations or by seeing that coefficients aren‚Äôt changing much anymore. If not converged, return to step 3 and repeat the process.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/dbde5df18fb4166d6ca595733b30eef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I4r3Y_SPBHMERNvWILNVbg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After 1000 iterations of coordinate descent, Lasso produces the final model where some coefficients have been shrunk exactly to zero (‚Äòrain‚Äô and ‚ÄòTemperature‚Äô features), while others retain non-zero values, demonstrating Lasso‚Äôs feature selection capability.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="1346" class="qt oy fq qq b bg qu qv l qw qx">from sklearn.linear_model import Lasso<br/><br/># Fit Lasso from scikit-learn<br/>lasso = Lasso(alpha=1) # Default value is 1000 cycle<br/>lasso.fit(X_train_scaled, y_train)<br/><br/># Print results<br/>print("\nCoefficients after 1000 cycles:")<br/>print(f"Bias term  : {lasso.intercept_:.2f}")<br/>for feature, coef in zip(X_train_scaled.columns, lasso.coef_):<br/>   print(f"{feature:11}: {coef:.2f}")</span></pre><h2 id="3484" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Coordinate Descent for Elastic Net Regression</h2><p id="9d71" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">The optimization problem of Elastic Net Regression is as follows:</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/57cb5db32b807d107f8cf8cae9a41f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efkz2oLWy10rcrp41Kzc4w.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">While <a class="af oc" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank">scikit-learn‚Äôs implementation</a> includes additional scaling factors (1/(2*n_samples)) and uses alpha (Œ±) to control overall regularization strength and l1_ratio to control the penalty mix, we‚Äôll use the standard theoretical form for clarity.</figcaption></figure><p id="1a1d" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">The coordinate descent algorithm for Elastic Net works similarly to Lasso, but accounts for both penalties when updating coefficients. Here‚Äôs how it works:</p><p id="08e5" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">1. Start by initializing the model with all coefficients at zero. Set two fixed values: one controlling feature removal (like in Lasso) and another for general coefficient shrinkage (the key difference from Lasso).</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/ce717b9cd599b195a4313563dc486b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJyC3ZejaJM1y5vLXMfYnw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Elastic Net regression starts like Lasso with zero weights for all features, but uses two parameters: Œª (lambda) for overall regularization strength and Œ± (alpha) to balance between Lasso and Ridge penalties.</figcaption></figure><p id="0614" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">2. Calculate the initial bias by taking the mean of all target values. (Same as Lasso)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/de17fbaa9b6e7d4e9b6b761036cc08ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_vUmmM6Y3wm1XeMQ_02vw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">ike Lasso, Elastic Net also initializes its bias term to 37.43 by calculating the mean of all target values in the training dataset.</figcaption></figure><p id="c5b4" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">3. For updating the first coefficient:<br/>- Using weighted sum, calculate what the model would predict without using this feature. (Same as Lasso)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/16a9669367286fe143c27efffb821f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xF2KwO5yXlaL_LAFNlTojA.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Elastic Net starts its coordinate descent process similar to Lasso, making initial predictions of 37.43 for all training examples since all feature weights are set to zero and only the bias term is active.</figcaption></figure><p id="339e" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">- Find the partial residual ‚Äî how far off these predictions are from the actual values. Using this value, calculate the temporary coefficient. (Same as Lasso)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of og"><img src="../Images/0431216a648bff8bd9d223beb5d087ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nTSAEGLDhNiwViNbUOtvg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Like Lasso, Elastic Net calculates a temporary coefficient of 11.17 for the first feature by comparing predictions with true labels.</figcaption></figure><p id="56a5" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">- For Elastic Net, apply both <strong class="ni ga">soft thresholding</strong> and <strong class="ni ga">coefficient shrinkage</strong> to this temporary coefficient to get the final coefficient for this step. This combined effect is the main difference from Lasso Regression.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/e327609ca8f462275e64233a3bf0d258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTLvdKAzDKutt0yxZitEog.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Elastic Net applies its unique shrinkage formula that combines both Lasso (L1) and Ridge (L2) penalties, where Œ± controls their balance. The temporary coefficient 11.17 is shrunk to 10.06 through this combined regularization approach.</figcaption></figure><p id="56e3" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">4. Move through each remaining coefficient one at a time, repeating the same update process. When calculating predictions during each update, use the most recently updated values for all other coefficients. (Same process as Lasso, but using the modified update formula)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rl"><img src="../Images/e88dbf7e3ac2a2cd0364b38a574f89a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EdEVfKtDq_7MH4_rAB2Nxw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After updating the first coefficient to 10.06, Elastic Net continues coordinate descent by calculating and updating the second coefficient, showing how it processes features one at a time while maintaining both L1 and L2 regularization effects.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="0e59" class="qt oy fq qq b bg qu qv l qw qx">import numpy as np<br/><br/># Initialize bias as mean of target values and coefficients to 0<br/>bias = np.mean(y_train)<br/>beta = np.zeros(X_train_scaled.shape[1])<br/>lambda_param = 1<br/>alpha = 0.5  # mixing parameter (0 for Ridge, 1 for Lasso)<br/><br/># One cycle through all features<br/>for j, feature in enumerate(X_train_scaled.columns):<br/>    # Get current feature values<br/>    x_j = X_train_scaled.iloc[:, j].values<br/><br/>    # Calculate prediction excluding the j-th feature<br/>    y_pred_no_j = bias + X_train_scaled.values @ beta - x_j * beta[j]<br/><br/>    # Calculate partial residuals<br/>    residual_no_j = y_train.values - y_pred_no_j<br/><br/>    # Calculate the dot product of x_j with itself (sum of squared feature values)<br/>    sum_squared_x_j = np.dot(x_j, x_j)<br/><br/>    # Calculate temporary beta without regularization (raw update)<br/>    beta_old = beta[j]<br/>    beta_temp = beta_old + np.dot(x_j, residual_no_j) / sum_squared_x_j<br/><br/>    # Apply soft thresholding for Elastic Net penalty<br/>    l1_term = alpha * lambda_param / sum_squared_x_j     # L1 (Lasso) penalty term<br/>    l2_term = (1-alpha) * lambda_param / sum_squared_x_j # L2 (Ridge) penalty term<br/>    <br/>    # First apply L1 soft thresholding, then L2 scaling<br/>    beta[j] = (np.sign(beta_temp) * max(abs(beta_temp) - l1_term, 0)) / (1 + l2_term)<br/><br/># Print results<br/>print("Coefficients after one cycle:")<br/>for feature, coef in zip(X_train_scaled.columns, beta):<br/>    print(f"{feature:11}: {coef:.2f}")</span></pre><p id="d1da" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">5. Update the bias by calculating what the current model predicts using all features, then adjust the bias based on the average difference between these predictions and actual values. (Same as Lasso)</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of rk"><img src="../Images/84a57041683e2e82a1f091998f2fd4e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u1dvmLXHJzB3hpLya_dT9Q.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">After updating all feature coefficients using Elastic Net‚Äôs combined L1 and L2 regularization, the model recalculates the bias to 40.01 by taking the mean difference between true labels and predictions, similar to the process in Lasso regression.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="8a1c" class="qt oy fq qq b bg qu qv l qw qx"># Update bias (not penalized by lambda)<br/>y_pred_with_updated_beta = X_train_scaled.values @ beta  # only using coefficients, no bias<br/>residuals_for_bias_update = y_train.values - y_pred_with_updated_beta<br/>new_bias = np.mean(y_train.values - y_pred_with_updated_beta)  # this replaces the old bias<br/><br/>print(f"Bias term  : {new_bias:.2f}")</span></pre><p id="7ed6" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">6. Check if the model has converged either by reaching the maximum number of allowed iterations or by seeing that coefficients aren‚Äôt changing much anymore. If not converged, return to step 3 and repeat the process.</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/4bdff8f8191a93822b8c5d9788dce60f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rwBFu7x0R0UlaOZLxXwLw.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">The final Elastic Net model after 1000 iterations shows smaller coefficient values compared to Lasso and fewer coefficients shrunk exactly to zero.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="5758" class="qt oy fq qq b bg qu qv l qw qx">from sklearn.linear_model import ElasticNet<br/><br/># Fit Lasso from scikit-learn<br/>elasticnet = Lasso(alpha=1) # Default value is 1000 cycle<br/>elasticnet.fit(X_train_scaled, y_train)<br/><br/># Print results<br/>print("\nCoefficients after 1000 cycles:")<br/>print(f"Bias term  : {elasticnet.intercept_:.2f}")<br/>for feature, coef in zip(X_train_scaled.columns, elasticnet.coef_):<br/>   print(f"{feature:11}: {coef:.2f}")</span></pre><h1 id="4fb3" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk"><strong class="al">Test Step</strong></h1><p id="6435" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">The prediction process remains the same as OLS ‚Äî multiply new data points by the coefficients:</p><h2 id="14e5" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Lasso Regression</h2><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/5b346c92504c34a5978d7fa63339b597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*REqsBhCQJU3N2tWPouT86Q.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">When applying the trained Lasso model to unseen data, it multiplies each feature value with its corresponding coefficient and adds the bias term (41.24), resulting in a final prediction of 40.2 players for this new data point.</figcaption></figure><h2 id="816d" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Elastic Net Regression</h2><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/d236c4e9015808c670b7af71685e43ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i8Nw43nEMxrhKPA8661srg.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">The trained Elastic Net model predicts 40.83 players for the same unseen data point by multiplying features with its more evenly distributed coefficients and adding the bias (38.59), showing a slightly different prediction from Lasso due to its balanced regularization approach.</figcaption></figure><h1 id="3c25" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Evaluation Step</h1><p id="f9f0" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">We can do the same process for all data points. For our dataset, here‚Äôs the final result with the RMSE as well:</p><h2 id="7b7a" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Lasso Regression</h2><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/b6e2f8a906fcbe42c82b4e75ee41cddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bvpb1WchZ2QU9AMOXP7wtQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Lasso‚Äôs performance on multiple test cases shows a Root Mean Square Error (RMSE) of 7.203, calculated by comparing its predictions with actual player counts across 14 different test samples.</figcaption></figure><h2 id="88f0" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Elastic Net Regression</h2><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/5ede9d56bb31b43bac0195a76b3cde52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uj_DIPBOXKsd4ozYhseiVQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">Elastic Net shows a slightly higher RMSE compared to Lasso‚Äôs, likely because its combined L1 and L2 penalties keep more features with small non-zero coefficients, which can introduce more variance in predictions.</figcaption></figure><h1 id="8838" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Key Parameters</h1><h2 id="972b" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Lasso Regression</h2><p id="63bc" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Lasso regression uses coordinate descent to solve the optimization problem. Here are the key parameters for that:</p><ul class=""><li id="0f69" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">alpha</code> (Œª): Controls how strongly to penalize large coefficients. Higher values force more coefficients to become exactly zero. Default is 1.0.</li><li id="0a05" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">max_iter</code>: Sets the maximum number of cycles the algorithm will update its solution in search of the best result. Default is 1000.</li><li id="d01a" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">tol</code>: Sets how small the change in coefficients needs to be before the algorithm decides it has found a good enough solution. Default is 0.0001.</li></ul><h2 id="99a4" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Elastic Net Regression</h2><p id="a880" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Elastic Net regression combines two types of penalties and also uses coordinate descent. Here are the key parameters for that:</p><ul class=""><li id="0a8f" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">alpha</code> (<em class="od">Œª</em>): Controls the overall strength of both penalties together. Higher values mean stronger penalties. Default is 1.0.</li><li id="3d7e" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">l1_ratio</code> (<em class="od">Œ±</em>): Sets how much to use each type of penalty. A value of 0 uses only Ridge penalty, while 1 uses only Lasso penalty. Values between 0 and 1 use both. Default is 0.5.</li><li id="69c8" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">max_iter</code>: Maximum number of iterations for the coordinate descent algorithm. Default is 1000 iterations.</li><li id="2b2e" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk"><code class="cx rg rh ri qq b">tol</code>: Tolerance for the optimization convergence, similar to Lasso. Default is 1e-4.</li></ul><p id="a351" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="od">Note</em>: Not to be confused, in <code class="cx rg rh ri qq b">scikit-learn</code>‚Äôs code, the regularization parameter is called <code class="cx rg rh ri qq b">alpha</code>, but in mathematical notation it‚Äôs typically written as <em class="od">Œª</em> (lambda). Similarly, the mixing parameter is called <code class="cx rg rh ri qq b">l1_ratio</code> in code but written as <em class="od">Œ± </em>(alpha) in mathematical notation. We use the mathematical symbols here to match standard textbook notation.</p><h1 id="baaf" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Model Comparison: OLS vs Lasso vs Ridge vs Elastic Net</h1><p id="7f8c" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">With Elastic Net, we can actually explore different types of linear regression models by adjusting the parameters:</p><ul class=""><li id="d9b4" class="ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob rm qz ra bk">When <code class="cx rg rh ri qq b">alpha</code> = 0, we get Ordinary Least Squares (OLS)</li><li id="51c6" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk">When <code class="cx rg rh ri qq b">alpha</code> &gt; 0 and <code class="cx rg rh ri qq b">l1_ratio</code> = 0, we get Ridge regression</li><li id="111c" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk">When <code class="cx rg rh ri qq b">alpha</code> &gt; 0 and <code class="cx rg rh ri qq b">l1_ratio</code> = 1, we get Lasso regression</li><li id="a61a" class="ng nh fq ni b gt rb nk nl gw rc nn no np rd nr ns nt re nv nw nx rf nz oa ob rm qz ra bk">When <code class="cx rg rh ri qq b">alpha</code> &gt; 0 and 0 &lt; <code class="cx rg rh ri qq b">l1_ratio</code> &lt; 1, we get Elastic Net regression</li></ul><p id="331c" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">In practice, it is a good idea to explore a range of <code class="cx rg rh ri qq b">alpha</code> values (like 0.0001, 0.001, 0.01, 0.1, 1, 10, 100) and <code class="cx rg rh ri qq b">l1_ratio</code> values (like 0, 0.25, 0.5, 0.75, 1), preferably using cross-validation to find the best combination.</p><p id="6cd7" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Here, let‚Äôs see how the model coefficients, bias terms, and test RMSE change with different regularization strengths (<em class="od">Œª</em>) and mixing parameters (<code class="cx rg rh ri qq b">l1_ratio</code>).</p><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/7b26b010afc0a4b6801ec0128418fb8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPBcIz2ba9OcYKhQqqWOIA.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/6f1640b2c83d7addefc37d8c4619dc71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ybu0FzkE4C2wHyKXL8W3sA.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/d8d68a8953bf6af67433a1de86169808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rP4WH1dNlqlgpImwFuQAWw.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/04b0cdb113e505cc73a2a6a207f47dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Di75xxXhtZOFAAUzhTmivg.png"/></div></div></figure><figure class="oh oi oj ok ol om oe of paragraph-image"><div role="button" tabindex="0" class="on oo ed op bh oq"><div class="oe of qo"><img src="../Images/9dd62b75a888017ce978d13d494a615e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wy6zXShqXiSy0RY5U-pgiQ.png"/></div></div><figcaption class="os ot ou oe of ov ow bf b bg z dx">The best model is Lasso (Œ± = 0) with Œª = 0.1, achieving an RMSE of 6.561, showing that pure L1 regularization works best for our dataset.</figcaption></figure><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="3810" class="qt oy fq qq b bg qu qv l qw qx"># Define parameters<br/>l1_ratios = [0, 0.25, 0.5, 0.75, 1]<br/>lambdas = [0, 0.01, 0.1, 1, 10]<br/>feature_names = X_train_scaled.columns<br/><br/># Create a dataframe for each lambda value<br/>for lambda_val in lambdas:<br/>    # Initialize list to store results<br/>    results = []<br/>    rmse_values = []<br/>    <br/>    # Fit ElasticNet for each l1_ratio<br/>    for l1_ratio in l1_ratios:<br/>        # Fit model<br/>        en = ElasticNet(alpha=lambda_val, l1_ratio=l1_ratio)<br/>        en.fit(X_train_scaled, y_train)<br/>        <br/>        # Calculate RMSE<br/>        y_pred = en.predict(X_test_scaled)<br/>        rmse = root_mean_squared_error(y_test, y_pred)<br/>        <br/>        # Store coefficients and RMSE<br/>        results.append(list(en.coef_.round(2)) + [round(en.intercept_,2),round(rmse,3)])<br/>    <br/>    # Create dataframe with RMSE column<br/>    columns = list(feature_names) + ['Bias','RMSE']<br/>    df = pd.DataFrame(results, index=l1_ratios, columns=columns)<br/>    df.index.name = f'Œª = {lambda_val}'<br/>    <br/>    print(df)</span></pre><p id="678d" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><em class="od">Note</em>: Even though Elastic Net can do what OLS, Ridge, and Lasso do by changing its parameters, it‚Äôs better to use the specific command made for each type of regression. In scikit-learn, use <code class="cx rg rh ri qq b">LinearRegression</code> for OLS, <code class="cx rg rh ri qq b">Ridge</code> for Ridge regression, and <code class="cx rg rh ri qq b">Lasso</code> for Lasso regression. Only use Elastic Net when you want to combine both Lasso and Ridge's special features together.</p><h1 id="d7bd" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">Final Remarks: Which Regression Method Should You Use?</h1><p id="eb76" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Let‚Äôs break down when to use each method.</p><p id="8cce" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">Start with <strong class="ni ga">Ordinary Least Squares (OLS)</strong> when you have more samples than features in your dataset, and when your features don‚Äôt strongly predict each other.</p><p id="5eb2" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni ga">Ridge Regression</strong> works well when you have the opposite situation ‚Äî lots of features compared to your number of samples. It‚Äôs also great when your features are strongly connected to each other.</p><p id="014d" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni ga">Lasso Regression</strong> is best when you want to discover which features actually matter for your predictions. It will automatically set unimportant features to zero, making your model simpler.</p><p id="d278" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk"><strong class="ni ga">Elastic Net</strong> combines the strengths of both Ridge and Lasso. It‚Äôs useful when you have groups of related features and want to either keep or remove them together. If you‚Äôve tried Ridge and Lasso separately and weren‚Äôt happy with the results, Elastic Net might give you better predictions.</p><p id="fe07" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">A good strategy is to start with Ridge if you want to keep all your features. You can move on to Lasso if you want to identify the important ones. If neither gives you good results, then move on to Elastic Net.</p><h1 id="b312" class="ox oy fq bf oz pa pb gv pc pd pe gy pf pg ph pi pj pk pl pm pn po pp pq pr ps bk">üåü Lasso and Elastic Net Code Summarized</h1><pre class="oh oi oj ok ol qp qq qr bp qs bb bk"><span id="1ed7" class="qt oy fq qq b bg qu qv l qw qx">import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.metrics import root_mean_squared_error<br/>from sklearn.linear_model import Lasso  #, ElasticNet<br/><br/># Create dataset<br/>data = {<br/>    'Outlook': ['sunny', 'sunny', 'overcast', 'rain', 'rain', 'rain', 'overcast', 'sunny', 'sunny', <br/>                'rain', 'sunny', 'overcast', 'overcast', 'rain', 'sunny', 'overcast', 'rain', 'sunny', <br/>                'sunny', 'rain', 'overcast', 'rain', 'sunny', 'overcast', 'sunny', 'overcast', 'rain', 'overcast'],<br/>    'Temperature': [85, 80, 83, 70, 68, 65, 64, 72, 69, 75, 75, 72, 81, 71, 81, 74, 76, 78, 82, <br/>                   67, 85, 73, 88, 77, 79, 80, 66, 84],<br/>    'Humidity': [85, 90, 78, 96, 80, 70, 65, 95, 70, 80, 70, 90, 75, 80, 88, 92, 85, 75, 92, <br/>                 90, 85, 88, 65, 70, 60, 95, 70, 78],<br/>    'Wind': [False, True, False, False, False, True, True, False, False, False, True, True, False, <br/>             True, True, False, False, True, False, True, True, False, True, False, False, True, False, False],<br/>    'Num_Players': [52, 39, 43, 37, 28, 19, 43, 47, 56, 33, 49, 23, 42, 13, 33, 29, 25, 51, 41, <br/>                    14, 34, 29, 49, 36, 57, 21, 23, 41]<br/>}<br/><br/># Process data<br/>df = pd.get_dummies(pd.DataFrame(data), columns=['Outlook'], prefix='', prefix_sep='', dtype=int)<br/>df['Wind'] = df['Wind'].astype(int)<br/>df = df[['sunny','overcast','rain','Temperature','Humidity','Wind','Num_Players']]<br/><br/># Split data<br/>X, y = df.drop(columns='Num_Players'), df['Num_Players']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, shuffle=False)<br/><br/># Scale numerical features<br/>numerical_cols = ['Temperature', 'Humidity']<br/>ct = ColumnTransformer([('scaler', StandardScaler(), numerical_cols)], remainder='passthrough')<br/><br/># Transform data<br/>X_train_scaled = pd.DataFrame(<br/>    ct.fit_transform(X_train),<br/>    columns=numerical_cols + [col for col in X_train.columns if col not in numerical_cols],<br/>    index=X_train.index<br/>)<br/>X_test_scaled = pd.DataFrame(<br/>    ct.transform(X_test),<br/>    columns=X_train_scaled.columns,<br/>    index=X_test.index<br/>)<br/><br/># Initialize and train the model<br/>model = Lasso(alpha=0.1)  # Option 1: Lasso Regression (alpha is the regularization strength, equivalent to Œª, uses coordinate descent)<br/>#model = ElasticNet(alpha=0.1, l1_ratio=0.5)  # Option 2: Elastic Net Regression (alpha is the overall regularization strength, and l1_ratio is the mix between L1 and L2, uses coordinate descent)<br/><br/># Fit the model<br/>model.fit(X_train_scaled, y_train)<br/><br/># Make predictions<br/>y_pred = model.predict(X_test_scaled)<br/><br/># Calculate and print RMSE<br/>rmse = root_mean_squared_error(y_test, y_pred)<br/>print(f"RMSE: {rmse:.4f}")<br/><br/># Additional information about the model<br/>print("\nModel Coefficients:")<br/>for feature, coef in zip(X_train_scaled.columns, model.coef_):<br/>    print(f"{feature:13}: {coef:.2f}")<br/>print(f"Intercept    : {model.intercept_:.2f}")</span></pre></div></div></div><div class="ab cb rn ro rp rq" role="separator"><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt ru"/><span class="rr by bm rs rt"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="806f" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Further Reading</h2><p id="9d91" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">For a detailed explanation of <a class="af oc" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Lasso.html" rel="noopener ugc nofollow" target="_blank">Lasso</a> Regression and <a class="af oc" href="https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank">Elastic Net</a> Regression, and its implementation in <code class="cx rg rh ri qq b">scikit-learn</code>, readers can refer to their official documentation. It provides comprehensive information on their usage and parameters.</p><h2 id="d13a" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">Technical Environment</h2><p id="909b" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">This article uses Python 3.7 and scikit-learn 1.5. While the concepts discussed are generally applicable, specific code implementations may vary slightly with different versions</p><h2 id="f399" class="pt oy fq bf oz pu pv pw pc px py pz pf np qa qb qc nt qd qe qf nx qg qh qi fw bk">About the Illustrations</h2><p id="0d40" class="pw-post-body-paragraph ng nh fq ni b gt qj nk nl gw qk nn no np ql nr ns nt qm nv nw nx qn nz oa ob fj bk">Unless otherwise noted, all images are created by the author, incorporating licensed design elements from Canva Pro.</p><p id="fb1f" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">ùôéùôöùôö ùô¢ùô§ùôßùôö ùôçùôöùôúùôßùôöùô®ùô®ùôûùô§ùô£ ùòºùô°ùôúùô§ùôßùôûùô©ùôùùô¢ùô® ùôùùôöùôßùôö:</p><div class="mn mo mp mq mr"><div role="button" tabindex="0" class="ab bx cp kj it rv rw bp rx lv ao"><div class="ry l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rz sa cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rz sa em n ay uk"/></div><div class="sb l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq se hp l"><h2 class="bf ga wp ic it wq iv iw my iy ja fz bk">Regression Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wr ws wt wu wv lh ww wx uv ii wy wz xa uz va vb ep bm vc ot" href="https://medium.com/@samybaladram/list/regression-algorithms-b0b6959f1b39?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">5 stories</span></div></div></div><div class="sn dz so it ab sp il ed"><div class="ed sh bx si sj"><div class="dz l"><img alt="A cartoon doll with pigtails and a pink hat. This ‚Äúdummy‚Äù doll, with its basic design and heart-adorned shirt, visually represents the concept of a dummy regressor in machine. Just as this toy-like figure is a simplified, static representation of a person, a dummy regressor is a basic models serve as baselines for more sophisticated analyses." class="dz" src="../Images/aa7eeaa18e4bb093f5ce4ab9b93a8a27.png" width="194" height="194" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qMSGk19S51CXGl3DiAGuKw.png"/></div></div><div class="ed sh bx kk sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/44e6d84e61c895757ff31e27943ee597.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*nMaPpVdNqCci31YmjfCMRQ.png"/></div></div><div class="ed bx hx sm sl"><div class="dz l"><img alt="" class="dz" src="../Images/7f3e5f3e2aca2feec035ca92e1bc440a.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*qTpdMoaZClu-KDV3nrZDMQ.png"/></div></div></div></div></div><p id="ae35" class="pw-post-body-paragraph ng nh fq ni b gt nj nk nl gw nm nn no np nq nr ns nt nu nv nw nx ny nz oa ob fj bk">ùôîùô§ùô™ ùô¢ùôûùôúùôùùô© ùôñùô°ùô®ùô§ ùô°ùôûùô†ùôö:</p><div class="mn mo mp mq mr"><div role="button" tabindex="0" class="ab bx cp kj it rv rw bp rx lv ao"><div class="ry l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rz sa cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rz sa em n ay uk"/></div><div class="sb l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq se hp l"><h2 class="bf ga wp ic it wq iv iw my iy ja fz bk">Classification Algorithms</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wr ws wt wu wv lh ww wx uv ii wy wz xa uz va vb ep bm vc ot" href="https://medium.com/@samybaladram/list/classification-algorithms-b3586f0a772c?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">8 stories</span></div></div></div><div class="sn dz so it ab sp il ed"><div class="ed sh bx si sj"><div class="dz l"><img alt="" class="dz" src="../Images/f95c1a80b88fe6220b18cd3b2a83a30d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*eVxxKT4DKvRVuAHBGknJ7w.png"/></div></div><div class="ed sh bx kk sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/6ea70d9d2d9456e0c221388dbb253be8.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*uFvDKl3iA2_G961vw5QFpg.png"/></div></div><div class="ed bx hx sm sl"><div class="dz l"><img alt="" class="dz" src="../Images/7221f0777228e7bcf08c1adb44a8eb76.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*1TbEIdTs_Z8V_TPD9MXxJw.png"/></div></div></div></div></div><div class="mn mo mp mq mr"><div role="button" tabindex="0" class="ab bx cp kj it rv rw bp rx lv ao"><div class="ry l"><div class="ab q"><div class="l ed"><img alt="Samy Baladram" class="l ep by rz sa cx" src="../Images/835013c69e08fec04ad9ca465c2adf6c.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*M5J7CK552m9f4z-m1F7vYg.png"/><div class="el by l rz sa em n ay uk"/></div><div class="sb l il"><p class="bf b dy z it iu iv iw ix iy iz ja dx"><a class="af ag ah ai aj ak al am an ao ap aq ar as at" href="https://medium.com/@samybaladram?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">Samy Baladram</a></p></div></div><div class="cq se hp l"><h2 class="bf ga wp ic it wq iv iw my iy ja fz bk">Ensemble Learning</h2></div><div class="ab q"><div class="l il"><a class="bf b dy z bk wr ws wt wu wv lh ww wx uv ii wy wz xa uz va vb ep bm vc ot" href="https://medium.com/@samybaladram/list/ensemble-learning-673fc83cd7db?source=post_page-----5fecf3e1432f--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="xb l il"><span class="bf b dy z dx">4 stories</span></div></div></div><div class="sn dz so it ab sp il ed"><div class="ed sh bx si sj"><div class="dz l"><img alt="" class="dz" src="../Images/1bd2995b5cb6dcc956ceadadc5ee3036.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBhxEgEzbfYWiSK0LYOv6g.gif"/></div></div><div class="ed sh bx kk sk sl"><div class="dz l"><img alt="" class="dz" src="../Images/22a5d43568e70222eb89fd36789a9333.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*-qqvZRF8gPn2fP8N-kS3nA.gif"/></div></div><div class="ed bx hx sm sl"><div class="dz l"><img alt="" class="dz" src="../Images/8ea1a2f29053080a5feffc709f5b8669.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/da:true/resize:fill:388:388/1*FBDim33AJDmZUEDHk2z-tA.gif"/></div></div></div></div></div></div></div></div></div>    
</body>
</html>