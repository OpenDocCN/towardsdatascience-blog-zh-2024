- en: 'Torch Compile: 2x Faster Llama 3.2 with Low Effort'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/torch-compile-2x-faster-llama-3-2-with-low-effort-d17c102ac405?source=collection_archive---------2-----------------------#2024-11-13](https://towardsdatascience.com/torch-compile-2x-faster-llama-3-2-with-low-effort-d17c102ac405?source=collection_archive---------2-----------------------#2024-11-13)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But it will depend on your GPU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page---byline--d17c102ac405--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page---byline--d17c102ac405--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--d17c102ac405--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--d17c102ac405--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page---byline--d17c102ac405--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--d17c102ac405--------------------------------)
    ·5 min read·Nov 13, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8149a5a73c8bea9de7489e3fb32fb911.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated with ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: Torch Compile (`torch.compile`) was first introduced with PyTorch 2.0, but it
    took several updates and optimizations before it could reliably support most large
    language models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: when it comes to inference, `torch.compile` can genuinely speed up decoding
    with only a small increase in memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we’ll go over how `torch.compile` works and measure its impact
    on inference performance with LLMs. To use `torch.compile` in your code, you only
    need to add a single line. For this article, I tested it with Llama 3.2 and also
    tried it with `bitsandbytes` quantization, using two different GPUs: Google Colab’s
    L4 and A100.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve also created a notebook demonstrating how to use `torch.compile` and benchmarking
    its performance here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#120)](https://newsletter.kaitchup.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Torch Compile: How Does It Make Models Faster?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`torch.compile` provides a way to accelerate models by converting standard
    PyTorch code into optimized machine code. This approach, called JIT (Just-In-Time)
    compilation, makes the code run more efficiently on specific hardware, i.e., faster
    than normal Python code. It''s particularly good for complex models where even
    small speed…'
  prefs: []
  type: TYPE_NORMAL
