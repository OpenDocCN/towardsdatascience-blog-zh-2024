<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Approximating Stochastic Functions with Multivariate Outputs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Approximating Stochastic Functions with Multivariate Outputs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04">https://towardsdatascience.com/approximating-stochastic-functions-with-multivariate-outputs-ffefc7099a90?source=collection_archive---------10-----------------------#2024-09-04</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="ba95" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A novel method for training generative machine learning models</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Nicolas Arroyo Duran" class="l ep by dd de cx" src="../Images/a755f8b85873b94c1714e113d4ceaa18.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*QrXSdcn0zGtKQ0VUYmUdpw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@nicolas.arroyo.duran?source=post_page---byline--ffefc7099a90--------------------------------" rel="noopener follow">Nicolas Arroyo Duran</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--ffefc7099a90--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">21 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 4, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/522cfa51317ab1b574f2ce704fff8216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dkw56TQfCyFX-NK-SXA1BQ.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Pin Movement Training — Image by Author</figcaption></figure><p id="4f96" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You can reproduce the experiments in this article by cloning <a class="af ny" href="https://github.com/narroyo1/pmt" rel="noopener ugc nofollow" target="_blank">https://github.com/narroyo1/pmt</a>.</p><p id="be6e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The previous article in this series named <a class="af ny" href="https://medium.com/p/be7d6ccf4f6" rel="noopener"><em class="nz">Approximating stochastic functions</em></a> introduced a novel method to train generative machine learning models capable of approximating any stochastic function with a single output variable. From this point on I will refer to this method as <strong class="ne fr"><em class="nz">Pin Movement Training</em></strong> or <strong class="ne fr"><em class="nz">PMT</em></strong> for short. This because of the analogy of placing pins on fabric and moving them that is used to illustrate it.</p><p id="29f4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The method was described for functions with any number of inputs <strong class="ne fr"><em class="nz">X</em></strong> but with only a single output <strong class="ne fr"><em class="nz">Y</em></strong>. The present article will generalize <strong class="ne fr"><em class="nz">PMT</em></strong> for functions with any number of outputs. A summary of the method will be provided and should be enough to understand how it works, but if you would like a more in depth description you can read the previous article.</p><p id="8893" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The generalized method, for reasons you will learn below, utilizes an architecture similar to that of autoencoders. Because of this and because the uniform sampling distribution may be more convenient for many applications, I believe this method is a valid alternative to Variational Autoencoders.</p><h1 id="0818" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Refresher of the original method</h1><p id="bd19" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Let’s say that we want to use the a neural network to approximate a stochastic function defined as 𝑓(𝑥) → 𝑌 where <strong class="ne fr"><em class="nz">x</em></strong> is an input of any number of dimensions in <strong class="ne fr"><em class="nz">X</em></strong> and <strong class="ne fr"><em class="nz">Y</em></strong> is a one dimensional random variable.</p><p id="91d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The first thing we will do is to introduce a secondary input <strong class="ne fr"><em class="nz">Z</em></strong> that we define as a uniformly distributed random variable in a range <em class="nz">[Zₘᵢₙ, Zₘₐₓ]</em>. This is necessary in order to introduce randomness to an otherwise deterministic system. This gives us a neural network defined by 𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌 where 𝜃 represents the network trained weights.</p><p id="fd0c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s visualize any given point 𝑥′, 𝑠.𝑡. <em class="nz">x</em>′<em class="nz"> ∈ X</em>. For this <strong class="ne fr"><em class="nz">x’</em></strong> we want to map the whole range <em class="nz">[Zₘᵢₙ, Zₘₐₓ]</em> to <em class="nz">Yₓ</em>′. That is <em class="nz">f(x′, Zₘᵢₙ)</em> should be as similar as possible to <em class="nz">min(Yₓ′)</em> and <em class="nz">f(x′, Zₘₐₓ)</em> should be as similar as possible to <em class="nz">max(Yₓ′)</em>. Additionally the mid-points <em class="nz">f(x′, mid(Z))</em> and <em class="nz">mid(Yₓ′)</em> should be as similar as possible and of course the same goes for every other point in the range (see <strong class="ne fr">Fig. 1).</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pb"><img src="../Images/c6a0137b571f10fe4c1d034114540bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJiv4F1FCQLyUOsNuFG-AA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 1</strong> Mapping <strong class="bf oc"><em class="pc">Z</em></strong> to <strong class="bf oc"><em class="pc">Y</em></strong> — Image by author</figcaption></figure><p id="ff85" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to achieve this let’s think of model 𝑓𝜃 as a stretchable and transparent fabric on which <strong class="ne fr"><em class="nz">X </em></strong>is represented horizontally and <strong class="ne fr"><em class="nz">Z </em></strong>is represented vertically. Also let’s imagine a board with all the data points in the dataset plotted in it, in this board <strong class="ne fr"><em class="nz">X </em></strong>is represented horizontally and <strong class="ne fr"><em class="nz">Y </em></strong>is represented vertically. We then proceed to place the fabric on top of the board.</p><p id="e9c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">For every data point we place a “pin” on the fabric at the vertical midpoint of <strong class="ne fr"><em class="nz">Z</em></strong> or <em class="nz">mid(Z)</em>. We then compare the positions of the pin and the data point. If the data point is higher than the pin then, without unpinning the pin on the fabric, we move the pin upwards a predefined distance so that it lands in a higher position on the board. The pin will stretch or shrink the fabric with this motion. If it is lower then we move the pin downwards a predefined distance. We add the distances moved upwards and downwards and call the sum total movement.</p><p id="aab6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">After processing every data point, if the pin was not initially in the midpoint, the total movement will be greater in the direction of the actual midpoint. After repeating the process enough times the pin will reach a position close to the midpoint where the total movement upwards and downwards is equal, that is, the number of data points above it is the same as the number of data points below it. See <strong class="ne fr">Fig. 2</strong> for an animation of how this process stabilizes.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/93fa85f00020c63dca89927509511737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kaiIJsVKoV5wBGL-WDFp0A.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 2</strong> Moving pin towards observed points until it stabilizes in the middle position — Image by author</figcaption></figure><p id="ddf2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now if instead of putting the pin on the midpoint of <strong class="ne fr"><em class="nz">Z</em></strong> we put it in a point <strong class="ne fr"><em class="nz">1/3</em></strong> of the distance in range <em class="nz">[Zₘᵢₙ, Zₘₐₓ] </em>from the lowest point <em class="nz">Zₘᵢₙ.</em> And instead of moving it the same predetermined distance upwards and downwards, we move it <em class="nz">1.5</em> times the predetermined distance when going downwards and <em class="nz">0.75</em> times the predetermined distance when going upwards. Then this pin will reach a stability point (where the total movement upwards and total movement downwards are equal) at a place roughly above <strong class="ne fr"><em class="nz">1/3</em></strong> of the data points.</p><p id="36c8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This is because <em class="nz">distance upwards</em> * <em class="nz">higher data points</em> = <em class="nz">distance downwards</em> * <em class="nz">lower data points</em> or (0.75∗2/3=1.5∗1/3=0.5). See <strong class="ne fr">Fig. 3</strong> for an animation of how this process stabilizes for pins at <em class="nz">Zₘᵢₙ + 1/3</em> and <em class="nz">Zₘᵢₙ + 2/3</em>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pd"><img src="../Images/a41298fa268d0806434ba71f0037e866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWVPc8EEppvpL-z95Yz4-A.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 3</strong> Moving 2 pins towards observed points until they stabilize — Image by author</figcaption></figure><p id="6a90" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr"><em class="nz">How do we achieve this movement using a neural network?</em></strong> In order to move the “pins on the fabric” with a neural network, we select a value in <strong class="ne fr"><em class="nz">Z</em></strong> (which we call a <strong class="ne fr"><em class="nz">z-pin</em></strong>) and do backpropagation with the target value being the <strong class="ne fr"><em class="nz">z-pin</em></strong> plus/minus the predetermined distance, like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pe"><img src="../Images/3c449a2686c392296c2282a93f98ec1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*6dPiNuJ_QKXTAPioAUCJdw.png"/></div></figure><p id="a409" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Leveraging this principle we can select points uniformly in <strong class="ne fr"><em class="nz">Z</em></strong> and over a number of epochs we obtain the mapping we require. i.e. 𝑓𝜃(𝑥,𝑧∼𝑍) → 𝑌.</p><h1 id="5dcd" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Notes from the original article</h1><ul class=""><li id="18b1" class="nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx pf pg ph bk">In the original article the fabric stretching/shrinking analogy referred to <em class="nz">pins</em> that were used to reshape the model, however the model definitions and training method used the term <strong class="ne fr"><em class="nz">z-samples</em></strong> to refer to the same concrete term. In the present article and in the future these will be referred to exclusively as <strong class="ne fr"><em class="nz">z-pins</em></strong>.</li><li id="c841" class="nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx pf pg ph bk">When selecting <strong class="ne fr"><em class="nz">z-pins</em></strong> the original article always placed them evenly distributed in <strong class="ne fr"><em class="nz">Z</em></strong> and also used the same positions on every data point for every epoch. This is not necessary though, the only requirement is that the <strong class="ne fr"><em class="nz">z-pins</em></strong> are uniformly distributed in <strong class="ne fr"><em class="nz">Z</em></strong>.</li><li id="c159" class="nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx pf pg ph bk">The original article would use multiple <strong class="ne fr"><em class="nz">z-pins</em></strong> per data point. This is also not necessary and it is sufficient to select a single <strong class="ne fr"><em class="nz">z-pin</em></strong> per data point. In the present article all the experiments will select a single <strong class="ne fr"><em class="nz">z-pin</em></strong> per data point per epoch.</li></ul><h1 id="a7d0" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Generalizing for multiple outputs</h1><p id="5133" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Having revisited the original method for one output, lets move on to the changes necessary to work for multiple outputs.</p><h1 id="5992" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Redefining the Z-space</h1><p id="05bc" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Let’s define <strong class="ne fr"><em class="nz">Z</em></strong>, the sampling ground from where we select our <strong class="ne fr"><em class="nz">z-pins</em></strong>. In the original article <strong class="ne fr"><em class="nz">Z</em></strong> was defined as a single dimensional range described simply by lower and upper bounds <em class="nz">[Zₘᵢₙ, Zₘₐₓ]</em>. However in the generalized method and in order to be able to handle multidimensional outputs <strong class="ne fr"><em class="nz">Y</em></strong>, <strong class="ne fr"><em class="nz">Z</em></strong> must be defined in multiple dimensions as well (note however that the number of dimensions in <strong class="ne fr"><em class="nz">Z</em></strong> and <strong class="ne fr"><em class="nz">Y</em></strong> need not be the same).</p><p id="49b8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In theory it could be any bounded n-dimensional space, but because it makes calculating the scalars easier as you’ll see later, I chose to use a <a class="af ny" href="https://en.wikipedia.org/wiki/N-sphere" rel="noopener ugc nofollow" target="_blank"><em class="nz">hyper-sphere</em></a> that can be defined by an origin <strong class="ne fr"><em class="nz">O</em></strong>, a radius <strong class="ne fr"><em class="nz">R</em></strong> and a dimensionality <strong class="ne fr"><em class="nz">N</em></strong> (see <strong class="ne fr">Fig. 4)</strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pn"><img src="../Images/087f11175461fb67f3a8c3eb86c70e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*2HuonZ3jmqCIihIabFML8Q.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 4</strong> 3-dimensional hypersphere <strong class="bf oc"><em class="pc">Z-space</em></strong> — Image by author</figcaption></figure><p id="852d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s define a few concepts related to <strong class="ne fr"><em class="nz">Z</em></strong> that will be needed to move ahead.</p><ul class=""><li id="805a" class="nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx pf pg ph bk"><strong class="ne fr"><em class="nz">z-pins</em></strong>: These are uniformly sampled points in <strong class="ne fr"><em class="nz">Z</em></strong>. They can be defined as an <strong class="ne fr"><em class="nz">N</em></strong>-dimensional vector like this: <em class="nz">zₚᵢₙ = (z₀, z₁, …, zₙ)</em> where <em class="nz">z₀, z₁, …</em> are coordinates in <strong class="ne fr"><em class="nz">Z</em></strong>.</li><li id="6874" class="nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx pf pg ph bk"><strong class="ne fr"><em class="nz">z-dirs</em></strong>: A <strong class="ne fr"><em class="nz">z-dir</em></strong> is a direction on <strong class="ne fr"><em class="nz">Z</em></strong> that can be defined as a unit vector based at origin <strong class="ne fr"><em class="nz">O</em></strong> like this: <em class="nz">z-dir = O + (ž₀, ž₁, …, žₙ)</em></li><li id="247b" class="nc nd fq ne b go pi ng nh gr pj nj nk nl pk nn no np pl nr ns nt pm nv nw nx pf pg ph bk"><strong class="ne fr"><em class="nz">z-lines</em></strong>: A <strong class="ne fr"><em class="nz">z-line</em></strong> is a line in <strong class="ne fr"><em class="nz">Z</em></strong> such that it runs between any two points in <strong class="ne fr"><em class="nz">Z</em></strong>. We will define it as a line with a <strong class="ne fr"><em class="nz">z-pin</em></strong> origin and a <strong class="ne fr"><em class="nz">z-dir</em></strong> including all points in it that are inside of <strong class="ne fr"><em class="nz">Z</em></strong> like this: <em class="nz">zₗᵢₙₑ = zₚᵢₙ + z-dir s.t.∀z ∈ zₗᵢₙₑ : z ∈ Z</em></li></ul><h1 id="465b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">The model</h1><p id="e15b" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Moving into a multidimensional <strong class="ne fr"><em class="nz">Z</em></strong> introduces an important challenge. In the case of one-dimensional <strong class="ne fr"><em class="nz">Z</em></strong> and 𝑌 spaces, it was very simple to tell whether the selected <strong class="ne fr"><em class="nz">z-pin</em></strong> projection i.e. 𝑓𝜃<em class="nz">(x, zₚᵢₙ) </em>was greater or smaller than the observed data point in order to decide which direction to move it to. In one dimension “greater than” in 𝑌 could simply be translated to “greater than” in <strong class="ne fr"><em class="nz">Z</em></strong> and the <strong class="ne fr"><em class="nz">z-pin</em></strong> could simply be moved up. This because we were mapping a line to another line.</p><p id="4d15" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">But with multidimensional 𝑌 and <strong class="ne fr"><em class="nz">Z</em></strong> it is not possible to assume that the spaces will have the same shape or even the same number of dimensions which means that in order to decide the direction where to move a <strong class="ne fr"><em class="nz">z-pin</em></strong> based on its relation to a data point, it is necessary to map that data point from 𝑌 to <strong class="ne fr"><em class="nz">Z</em></strong>. This means that in addition to train function 𝑓𝜃 to generate values in 𝑌, we’ll also need to train an inverse function 𝑓𝜃⁻¹ to map data points to <strong class="ne fr"><em class="nz">Z</em></strong>. This fact changes our model architecture to something like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk po"><img src="../Images/ceb7da9d5dcef27e3f5586ee548e47e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*9k_Nv1NnlmdjS7wU12oNIQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 5</strong> Model architecture — Image by author</figcaption></figure><p id="ae72" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The left side of the model allows us to map points in 𝑌 to <strong class="ne fr"><em class="nz">Z</em></strong>. The right side of the model allows us to generate random samples in 𝑌 by sampling points in <strong class="ne fr"><em class="nz">Z</em></strong>.</p><p id="d803" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You may have noticed that this architecture is similar to that of plain autoencoders and indeed it is. This has the added benefit of making the method useful for learning latent representations that are bounded and evenly distributed.</p><h1 id="80e9" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">The Method</h1><p id="2eba" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Having defined all the concepts we need we can proceed to discuss how pin movement works in multiple dimensions.</p><h1 id="5417" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Mapping data points to Z</h1><p id="600a" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The first step is to use the inverse function 𝑓𝜃⁻¹ (or encoder going with autoencoder terminology) and map all data points in the batch from 𝑌 space to <strong class="ne fr"><em class="nz">Z</em></strong>. We will call the original data points <em class="nz">y-data</em> and the mapped data points <em class="nz">z-data</em>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pp"><img src="../Images/3b754a579f9e979c5ebcdf8545e2a44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gTLnv0JSG8N3rM7t7EYgA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 6</strong> Mapping data points to a 2-D <strong class="bf oc"><em class="pc">Z-space</em></strong> — Image by author</figcaption></figure><h1 id="1479" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Selecting the z-pins</h1><p id="44ec" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Next we must select some <strong class="ne fr"><em class="nz">z-pins</em></strong>. In order to do so, we start by selecting uniformly sampled <strong class="ne fr"><em class="nz">z-dirs</em></strong>, one for every data point. The easiest way to do so is by choosing random points in a hypersphere surface with the same dimensionality as <strong class="ne fr"><em class="nz">Z</em></strong>. Then we use the selected <strong class="ne fr"><em class="nz">z-dirs</em></strong> and translate them to have the data points mapped in the previous step <em class="nz">z-data</em> as origins. This gives us some <strong class="ne fr"><em class="nz">z-lines</em></strong> as you can see in <strong class="ne fr"><em class="nz">Fig. 7</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pq"><img src="../Images/c2ef2cd3fb679b035576766185fe9707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*al33YZcKeL2g-8eemZSM5w.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 7</strong> Selecting random <strong class="bf oc"><em class="pc">z-lines </em></strong><em class="pc">in a </em>2-D <strong class="bf oc"><em class="pc">Z-space</em></strong><em class="pc"> </em>— Image by author</figcaption></figure><p id="a26a" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Once we have our <strong class="ne fr"><em class="nz">z-lines</em></strong> we proceed to randomly select points in these lines, these will be our <strong class="ne fr"><em class="nz">z-pins</em></strong>. <strong class="ne fr"><em class="nz">Fig. 8</em></strong> shows how this can look like.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pr"><img src="../Images/b711e7ef5c92a990ef4997613d8c8b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*SG-_aKBfiBREW5WNTh4FGA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 8</strong> Selecting random <strong class="bf oc"><em class="pc">z-pins</em></strong> in a 2-D <strong class="bf oc"><em class="pc">Z-space</em></strong>— Image by author</figcaption></figure><p id="c8e8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">It is necessary for the method to work that for any given <strong class="ne fr"><em class="nz">z-line</em></strong> in <strong class="ne fr"><em class="nz">Z</em></strong>, every mapped data point <em class="nz">z-data</em> in it has an equal probability of occurring, otherwise the equations on <a class="af ny" href="#calculating-the-movement-scalars" rel="noopener ugc nofollow">Calculating the movement scalars</a> would not hold up.</p><p id="2291" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Given a 2-dimensional <strong class="ne fr"><em class="nz">Z</em></strong> and for any given <strong class="ne fr"><em class="nz">z-line</em></strong> in it, lets picture it as as a line with a minimal width 𝜖 in a way that it seems like a long rectangle, similar to the <strong class="ne fr"><em class="nz">z-lines</em></strong> in <strong class="ne fr"><em class="nz">Fig. 8</em></strong>. The probability of any given 𝑧 existing in it is of the area of this “thin” <strong class="ne fr"><em class="nz">z-line</em></strong> over the area of <strong class="ne fr"><em class="nz">Z</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/cb28153696cf9806dab5a4b0cbe779e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uU97dNX4DCx3-hwNWt69Aw.png"/></div></div></figure><p id="3165" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Since this “thin” <strong class="ne fr"><em class="nz">z-line</em></strong> is rectangular, any given segment 𝑠 of minimal length 𝛿 across it’s length has an equal area and therefore any given 𝑧 has an equal probability of being in the segment.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ps"><img src="../Images/77b4ea01279ddaf80a6286a6ada82f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyEPfC2fr8OWhLfY-cT9jQ.png"/></div></div></figure><p id="f167" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Also the probability of any given 𝑧​ inside this “thin” <strong class="ne fr"><em class="nz">z-line</em></strong> of selecting the <strong class="ne fr"><em class="nz">z-dir</em></strong> of this “thin” <strong class="ne fr"><em class="nz">z-line</em></strong> is constant given that the <strong class="ne fr"><em class="nz">z-dirs</em></strong> are selected using a uniform distribution.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/9ccc900b136d25b13b06acbba1d3b74f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5oLaq9wfDdJxQINiD67Ivw.png"/></div></div></figure><p id="95c9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Taking equations (2) and (3) we get that the probability of any 𝑧 being on any segment of a given <strong class="ne fr"><em class="nz">z-line</em></strong> and selecting the same <strong class="ne fr"><em class="nz">z-dir</em></strong>, and that is the same for every segment which satisfies the requirement above.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/5e48f3cc06d88db83495d324f4e048fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cmo82unHDfCzoPhP0cvI_w.png"/></div></div></figure><p id="7acd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The probability is independent of the position of 𝑧 in <strong class="ne fr"><em class="nz">z-line </em></strong>so the distribution in any <strong class="ne fr"><em class="nz">z-line</em></strong> is uniform.</p><h1 id="aec9" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Calculating the target values</h1><p id="6379" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">After selecting the <strong class="ne fr"><em class="nz">z-pins</em></strong> we can proceed to calculate the target values (or <strong class="ne fr"><em class="nz">z-targets</em></strong>) to use in our backpropagation. All we have to do for this is to add to every <strong class="ne fr"><em class="nz">z-pin</em></strong> the movement constant 𝑀 in the direction where the mapped data point 𝑧-𝑑𝑎𝑡𝑎 is.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pu"><img src="../Images/c0190b58e863341e169c02ff8d431e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Ek39AYgvZ-AKhYfQQh4c-w.png"/></div></figure><p id="9179" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr"><em class="nz">Fig. 9</em></strong> Shows how the <strong class="ne fr"><em class="nz">z-targets</em></strong> are calculated.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pr"><img src="../Images/2819cde11a256939e77fc07b132def79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*Ub68P1M5Ect5PxsKefHqzg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 9</strong> Calculating the <strong class="bf oc"><em class="pc">z-targets </em></strong><em class="pc">in a </em>2-D <strong class="bf oc"><em class="pc">Z-space</em></strong> — Image by author</figcaption></figure><h1 id="cb99" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Calculating the movement scalars</h1><p id="519b" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The way that movement scalars are calculated is similar to the way it was done in the original one-dimensional method.</p><p id="700e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s start by picturing a <strong class="ne fr"><em class="nz">z-line</em></strong> along with a <strong class="ne fr"><em class="nz">z-pin</em></strong> and some mapped data points 𝑧𝑑𝑎𝑡𝑎 like we see on <strong class="ne fr"><em class="nz">Fig .10</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pv"><img src="../Images/697ca5cd5b94d0f0d5f9480dca809fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBtbCwScvwnFyn90XWbO_Q.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 10</strong> Calculating the scalars — Image by author</figcaption></figure><p id="ae41" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Let’s call <strong class="ne fr"><em class="nz">a</em></strong> the distance from the <strong class="ne fr"><em class="nz">z-pin</em></strong> to one end of the <strong class="ne fr"><em class="nz">z-line</em></strong> and <strong class="ne fr"><em class="nz">b</em></strong> the distance to the other end. And let’s call the number of data points on the former side <strong class="ne fr"><em class="nz">a’</em></strong> and the number of data points on latter side <strong class="ne fr"><em class="nz">b’</em></strong>. Our purpose is to make quantity <strong class="ne fr"><em class="nz">a’</em></strong> proportional to distance <strong class="ne fr"><em class="nz">a</em></strong> and <strong class="ne fr"><em class="nz">b’</em></strong> proportional to <strong class="ne fr"><em class="nz">b</em></strong> i.e. 𝑎:𝑏::𝑎′:𝑏′.</p><p id="97ac" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Next we will call 𝛼 the scalar that we will use on the movement applied to the <strong class="ne fr"><em class="nz">z-pin</em></strong> for all data points on the side of length <strong class="ne fr"><em class="nz">a</em></strong>. And we will call 𝛽 the scalar that we will use on the movement applied to the <strong class="ne fr"><em class="nz">z-pin</em></strong> for all data points on the side of length <strong class="ne fr"><em class="nz">b</em></strong>.</p><p id="5efc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We will also call <strong class="ne fr"><em class="nz">T</em></strong> the total movement, which is the sum of moving the <strong class="ne fr"><em class="nz">z-pin</em></strong> a constant movement <strong class="ne fr"><em class="nz">M</em></strong> towards the side of every data point multiplied by that side’s scalar.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/fce7b3d44a7027e1e2931b7dd97bb295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3haNW2PTIX5DV6_AaG84Dg.png"/></div></div></figure><p id="e853" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We want <strong class="ne fr"><em class="nz">T</em></strong> to be 0 (i.e. stabilized) when 𝑎′/(𝑎′+𝑏′)≈𝑎/(𝑎+𝑏)∧𝑏′/(𝑎′+𝑏′)≈𝑏/(𝑎+𝑏), that is when the <strong class="ne fr"><em class="nz">z-pin</em></strong> divides the intended proportion of data points to both sides. Substituting <strong class="ne fr"><em class="nz">T</em></strong> with <strong class="ne fr"><em class="nz">0</em></strong> on (5) gives us the equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/9f8cb33871004aa8f21e05cbc98517a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGvVFQveAtcix15r1pef3w.png"/></div></div></figure><p id="564f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now let’s remember that not all <strong class="ne fr"><em class="nz">z-lines</em></strong> will have the same length, since they are bounded by the hypersphere defined by <strong class="ne fr"><em class="nz">Z</em></strong> the ones towards the center will be longer than the ones at the edges. Longer lines will represent larger spaces in <strong class="ne fr"><em class="nz">Z</em></strong> (see equation (1)) so their influence in the movement should be proportional to their length. We want <strong class="ne fr"><em class="nz">T</em></strong> to be linearly proportional to the length of the <strong class="ne fr"><em class="nz">z-line</em></strong> which gives us the equation:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/0fa148f5c1a05e18cdb3c55a57c2eda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqWUy1_5hTWDU7S64jfZ0w.png"/></div></div></figure><p id="b641" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">If we put together <em class="nz">(6)</em> and <em class="nz">(7)</em> we get that the scalars should have these values:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/429c0d919d626ee53fe6e76f48b57299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QDOkKBDgF3-2Fh7zurqG3g.png"/></div></div></figure><p id="c145" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Which are a similar equations to the one on the original article.</p><p id="b82c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">You may have noticed that this equations break towards the edges i.e. when either <strong class="ne fr"><em class="nz">a</em></strong> or <strong class="ne fr"><em class="nz">b</em></strong> tends to <em class="nz">0</em>.</p><p id="fcb8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to solve this problem a maximum scalar constant <strong class="ne fr"><em class="nz">S</em></strong> is introduced to clamp the scalars. Of course when clamping the scalars we have to be careful to adjust the value for both sides, for example if <strong class="ne fr"><em class="nz">a</em></strong> is very small (and therefore 𝛼 is large) but the data point is on side <strong class="ne fr"><em class="nz">b</em></strong> the scalar 𝛽 must be adjusted as well, otherwise equation (5) will not hold.</p><p id="052f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We start by selecting the largest of the 2 scalars 𝑚𝑎𝑥(𝛼,𝛽). Then we calculate an adjustment value by dividing <strong class="ne fr"><em class="nz">S</em></strong> by 𝑚𝑎𝑥(𝛼,𝛽) and clamping it to 1.0 so that it is always a number in the range [0, 1]. We will use the adjustment value to prevent scalars from going over <strong class="ne fr"><em class="nz">S</em></strong>. Finally, if <strong class="ne fr"><em class="nz">a</em></strong> is 0.0, then the values of 𝛼 and 𝛽 are <strong class="ne fr"><em class="nz">S</em></strong> and 0.0 respectively, and the other way around if b is 0.0. This gives us the revised equation <em class="nz">(8b)</em>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/69809bf37204fef652c2bbe616131352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2psDN8_J8eQVSWBGm4wXow.png"/></div></div></figure><p id="36a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Below you can see how the plots for the scalars proportional to <strong class="ne fr"><em class="nz">a</em></strong> or <strong class="ne fr"><em class="nz">b</em></strong> look like. Notice how they are clamped beyond the selected <strong class="ne fr"><em class="nz">S</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pw"><img src="../Images/4b7ce280f37aacb65182e9591d4998ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*uz1acFi87cmZAkRfkQ9YsA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 11</strong> Movement scalar clamping for <strong class="bf oc"><em class="pc">S=5.0</em></strong> — Image by author</figcaption></figure><p id="7228" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Having calculated both scalars we can choose the one to use by determining the side on which the data point resides.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/c0a282a46fed428c55bff5a0cb6cb9cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CAlhcIvVq4fIoQCW4dGRDQ.png"/></div></div></figure><h1 id="f2e6" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Training the model</h1><p id="fe2c" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Now that all the concepts involved are clear we can move on to describe the training algorithm.</p><h1 id="82ac" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">1. Pretraining and selecting the Z hyperparameters</h1><p id="5730" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The algorithm described makes the assumption that the models 𝑓𝜃⁻¹ and 𝑓𝜃 inversely match each other. This can lead to a slow start if we train these 2 models to match each other at the same time as we do pin movement. So it has been found beneficial to do a “pretrain” stage on which we only train 𝑓𝜃⁻¹ and 𝑓𝜃 to match each other. This stage is essentially an ordinary autoencoder training. After the reconstruction error has reached a reasonably low value the algorithm can proceed to the main training.</p><p id="0e17" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This pretrain stage has the added advantage that it makes it easier to define <strong class="ne fr"><em class="nz">Z</em></strong> when it completes. In the section <a class="af ny" href="#redefining-the-z-space" rel="noopener ugc nofollow">Redefining the <em class="nz">Z-space</em></a> it was mentioned that <strong class="ne fr"><em class="nz">Z</em></strong> is defined by an origin <strong class="ne fr"><em class="nz">O</em></strong> and a radius <strong class="ne fr"><em class="nz">R</em></strong>. Having pretrained the model for some time, all we do is run a batch of data points through the inverse model to calculate a set <em class="nz">Z-data</em>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk px"><img src="../Images/b5420483b93263466ca02e09f6693b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*pHCTMuVo_IAtVKd_Q3Gm1Q.png"/></div></figure><p id="2256" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Then we take the mean of this set and use it as the origin <strong class="ne fr"><em class="nz">O</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk py"><img src="../Images/77a3e2ec645ba5aacbb39566fec5aeb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*3Jpe7OI0CDRuO2rxg53sgA.png"/></div></div></figure><p id="6ac3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">We can also use the mean distance in <em class="nz">Z-data</em> to <strong class="ne fr"><em class="nz">O</em></strong> as <strong class="ne fr"><em class="nz">R</em></strong>, however it has been seen that experimenting with and tuning this value may give better results.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk pz"><img src="../Images/41ba99d0f2451905d4efc25c5c5dc90a.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*5Zd-CH1-wowbYEZVpOtdUA.png"/></div></figure><p id="e180" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This works because after the “pretrain” stage, the model has found a region capable of representing the data, so defining <strong class="ne fr"><em class="nz">Z </em></strong>in its vicinity will likely have a low reconstruction error.</p><h1 id="cc34" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">2. Pin Movement</h1><p id="c263" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">To start pin movement we select a batch of data <em class="nz">y-data = {y-data₀, y-data₁, …, y-dataₙ}</em> from the training dataset and map it to z<em class="nz">-data = {z-data₀, z-data₁, …, z-dataₙ} </em>like it is explained in <a class="af ny" href="#mapping-data-points-to-z" rel="noopener ugc nofollow">Mapping data points to Z</a>.</p><p id="07c2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The next step is to randomly select the <strong class="ne fr"><em class="nz">z-pins</em></strong> set <em class="nz">{z-pin₀, z-pin₁, …, z-pinₙ} </em>(one for every data point in the batch) in the way described on section <a class="af ny" href="#selecting-the-z-pins" rel="noopener ugc nofollow">Selecting the Z-pins</a>.</p><blockquote class="qa"><p id="c3c0" class="qb qc fq bf qd qe qf qg qh qi qj nx dx"><em class="pc">Note that it is possible to select multiple </em><strong class="al">z-pins</strong><em class="pc"> per data point. But it is not necessary and for simplicity we will use only one on the experiments.</em></p></blockquote><p id="7b5f" class="pw-post-body-paragraph nc nd fq ne b go qk ng nh gr ql nj nk nl qm nn no np qn nr ns nt qo nv nw nx fj bk">Then we calculate the target values <em class="nz">z-targets = {z-target₀, z-target₁, …, z-targetₙ}</em> and scalars s = <em class="nz">{s₀, s₁, …, sₙ}</em> as explained on sections <a class="af ny" href="#calculating-the-target-values" rel="noopener ugc nofollow">Calculating the target values</a> and <a class="af ny" href="#calculating-the-movement-scalars" rel="noopener ugc nofollow">Calculating the movement scalars</a>.</p><p id="8692" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Having the <strong class="ne fr"><em class="nz">z-targets</em></strong>, we calculate the current model predictions by running them through 𝑓𝜃, this gives us:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/4020948aed7c0eb227fe50b4c8084a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U1JwzhfZEUwMw6Hc1d42nw.png"/></div></div></figure><p id="0def" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Now we have everything for the first component of our loss function:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/bdfb2b259c8dfae0893c785dbcd1e429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2utMgE5vOGD2Ec1YsV-6g.png"/></div></div></figure><p id="4aa6" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice that we are using a Weighted Mean Absolute Error <em class="nz">(WMAE)</em> function instead of a Weighted Mean Squared Error <em class="nz">(WMSE)</em>. This is because the latter is designed to punish larger differences while we are moving all of our pins the same distance.</p><h1 id="c797" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">3. Reconstruction Loss</h1><p id="5a29" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The next component to our loss function is the difference between our model 𝑓𝜃 and our inverse model 𝑓𝜃⁻¹. This is very similar to the <em class="nz">Reconstruction Loss</em> in both variational and ordinary autoencoders. It is necessary to pass the batch data points to 𝑓𝜃⁻¹, take the results and pass them on to 𝑓𝜃 and then run backpropagation using the results and the original data points.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/edcf87003c4b92b959bca6251690f13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gmr6IZ89PSavuRY0QENMGQ.png"/></div></div></figure><h1 id="8e3b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">4. Inverse Reconstruction Loss</h1><p id="3690" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Before we define the last component of the loss function, let’s explain why it is necessary. Ideally at the end of the training both 𝑓𝜃 and 𝑓𝜃⁻¹ will be bijective, meaning that there will be an exactly one-to-one correspondence between the domain and codomain <strong class="ne fr"><em class="nz">Z</em></strong> and <strong class="ne fr"><em class="nz">Y</em></strong>. However during training this is not guaranteed to be the case and it is possible that areas in <strong class="ne fr"><em class="nz">Z</em></strong> will not be mapped to <strong class="ne fr"><em class="nz">Y</em></strong>.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qq"><img src="../Images/b90e7c2975af97a75288d1862d5dad76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*pMPhaY6ltp8ZqDrBtQRHhQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 12</strong> Model and inverse models may not be bijective — Image by author</figcaption></figure><p id="8d95" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As you can see in <strong class="ne fr"><em class="nz">Fig. 12</em></strong> as a result of training with component <em class="nz">loss-y</em>, 𝑓𝜃 and 𝑓𝜃⁻¹ agree with each other as far as <strong class="ne fr"><em class="nz">Y</em></strong> goes. That is <em class="nz">∀y ∈ Y, </em>𝑓𝜃⁻¹(𝑓𝜃(𝑦)) ≈ <em class="nz">y</em>. However not all of <strong class="ne fr"><em class="nz">Z</em></strong> is used and some points in <strong class="ne fr"><em class="nz">Y</em></strong> map outside of it. This is a problem because the assumption of moving <strong class="ne fr"><em class="nz">z-pins</em></strong> to a position that will map to a point in <strong class="ne fr"><em class="nz">Y</em></strong> that both 𝑓𝜃 and 𝑓𝜃⁻¹ will agree on is broken.</p><p id="7385" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr"><em class="nz">Fig. 12</em></strong> shows 2 of the problems that may happen. A “fold” in the fabric happens when 2 or more points in <strong class="ne fr"><em class="nz">Z</em></strong> map to the same point in <strong class="ne fr"><em class="nz">Y</em></strong>. An “out-of-bounds” happens when a point in <strong class="ne fr"><em class="nz">Z</em></strong> maps to a point outside of <strong class="ne fr"><em class="nz">Y</em></strong>.</p><p id="6ed3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In order to solve this problem we add third component to the loss function:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/dea9a9bb44d6b75d7c6090d65e6b603c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLHXByyp8oYDScBylc5q9A.png"/></div></div></figure><p id="c834" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">What this does is to synchronize 𝑓𝜃 and 𝑓𝜃⁻¹​ with regards to <strong class="ne fr"><em class="nz">Z</em></strong> and it does so by selecting random points in <strong class="ne fr"><em class="nz">Z</em></strong> instead of using the training set data points.</p><p id="bd0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Notice that for both the reconstruction loss and the inverse reconstruction loss we simply use Mean Squared Error (MSE).</p><h1 id="0a15" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">5. Loss function</h1><p id="b220" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Now that we have all the components to the loss function all that’s left is to define weights for each of them which we will name 𝛾-𝑝, 𝛾-<em class="nz">y</em> and 𝛾-𝑧. We can put together (10), (11) and (12) to define the loss function like this:</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk pt"><img src="../Images/7563dfc4116d138b5cdd5bb187b801b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fChAJ9snGZEhKEJDhDJrdQ.png"/></div></div></figure><p id="190d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">All that’s left after this is to run backpropagation on the loss.</p><h1 id="4239" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Testing the model</h1><p id="1350" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">In the original paper we used goal 1 and goal 2 testing which measured the density of data points between <strong class="ne fr"><em class="nz">z-pins</em></strong> and compared it to the densities of the test dataset. However on a multi-dimensional space doing that approach is not practical since the spaces between <strong class="ne fr"><em class="nz">z-pins</em></strong> scale rapidly in number.</p><p id="b98d" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The original paper also used <a class="af ny" href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance" rel="noopener ugc nofollow" target="_blank"><em class="nz">Earth Mover’s Distance (EMD)</em></a> as an indicator of the model’s performance. For multiple dimension <em class="nz">PMT</em> we will use <em class="nz">EMD</em> to measure the model’s accuracy. We will define the EMD error by comparing data points from the training dataset against data points generated by the <em class="nz">PMT</em> model.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qr"><img src="../Images/6ee897945e3f09ca4893e61980b65bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*1NmBHV9Ch4L_RCZSAIGMeA.png"/></div></figure><p id="4c5e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">And in order to have an idea of what the lowest EMD error would be we will also calculate a base EMD by comparing data points from the training dataset against data points in the test dataset.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qs"><img src="../Images/6baf14ad2aee559ec18772d16672feaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*nVK9mUjeHFzadmVcm-5hCA.png"/></div></figure><p id="9b3b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This gives us a baseline that we can compare against E-emd to measure the accuracy of the models.</p><h1 id="7e2f" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Comparison with Variational Autoencoders</h1><p id="9130" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The most similar generative model to <em class="nz">PMT</em> is Variational Autoencoders (<em class="nz">VAE</em>). It has an almost identical neural network architecture and acts both a generative model and a latent representation mapper. The biggest difference between the two is that the source distribution in <em class="nz">VAE</em> is unbounded (Gaussian) and the one in <em class="nz">PMT</em> is bounded (uniform distribution).</p><p id="e06e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The experiments show however, that for both bounded and unbounded target distributions <em class="nz">PMT</em> outperforms <em class="nz">VAE</em>. Furthermore, the reconstruction error in <em class="nz">PMT</em> is significantly lower than on <em class="nz">VAE</em>. The reason for this may be that the components of the loss function cooperate with each other on <em class="nz">PMT</em> as opposed to competing with each other in <em class="nz">VAE</em>. Also because of the fact that the target distribution is uniform, the spacing between data points in <strong class="ne fr"><em class="nz">Z </em></strong>can be larger.</p><p id="e857" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Another difference is that <em class="nz">PMT</em> takes a larger number of hyperparameters, 𝑆 (maximum scalar), 𝛾-𝑝 (pin movement weight), 𝛾-𝑦 (reconstruction loss weight), 𝛾-𝑧 (inverse reconstruction loss weight) and 𝑀 (movement constant) compared to <em class="nz">VAE</em> hyperparameters which are just the <em class="nz">kld weight</em>. This may make <em class="nz">PMT</em> training more difficult.</p><p id="2ebd" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Finally <em class="nz">PMT</em> takes longer to train per epoch than <em class="nz">VAE</em>, this is because it is necessary to do a pass to calculate the <strong class="ne fr"><em class="nz">z-targets</em></strong>, and also because the loss function has an additional component (see equation (12)).</p><h1 id="dba2" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Experiments</h1><p id="0aea" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Now I will try out the model in several datasets. In order to make them easier to plot, the experiments presented will have no <strong class="ne fr"><em class="nz">X</em></strong> inputs.</p><p id="f7de" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Due to the similarities with <em class="nz">VAE</em>, every experiment will be done using both <em class="nz">PMT</em> and <em class="nz">VAE</em> models for comparison. In every experiment both models will have identical neural network architectures.</p><p id="ac22" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The source code and everything needed to reproduce the experiments below can be found in <a class="af ny" href="https://github.com/narroyo1/pmt" rel="noopener ugc nofollow" target="_blank">https://github.com/narroyo1/pmt</a>.</p><h1 id="562b" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Multiple blobs</h1><p id="53d4" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The first dataset I’ll try is generated using <a class="af ny" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html" rel="noopener ugc nofollow" target="_blank"><em class="nz">make_blobs()</em></a> from the <em class="nz">sklearn</em> library. As it name suggests it generates a number of Gaussian blobs and it is a good dataset to test how <em class="nz">PMT</em> performs with unbounded datasets.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qt"><img src="../Images/c08abf0004b1126002cbcb212c12133c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOCo9iUFjrvWNzwAwDBtjA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 13a</strong> Generated data — Image by author</figcaption></figure></div></div><div class="mr"><div class="ab cb"><div class="lm qu ln qv lo qw cf qx cg qy ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr qz ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/c4ac3c2ebd1e7dbfefe33340cc6e1f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*80eogGVsyz5FR3nKnS4fVQ.gif"/></div></figure><figure class="lb mr re ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/de4f045995b16018008a7261ac9b37ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*P13U128QY5FDSbxufIxeLg.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rf ed rg rh"><strong class="bf oc">Fig. 13b</strong> PMT training animation /<strong class="bf oc">Fig. 13c</strong> VAE training animation — Image by author</figcaption></figure></div><div class="ab ke"><figure class="lb mr ri ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/632a68d1f622dafcbf04a9ff90ae6266.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MT3BfiFabN3tdnrfx-9h9Q.png"/></div></figure><figure class="lb mr rj ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/ec6296d1dfad186d646998a0a06b1c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*Ue0yuPqKmaWvsSb2Fg-wWg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rk ed rl rh"><strong class="bf oc">Fig. 13d</strong> EMD plot/<strong class="bf oc">Fig. 13e</strong> Reconstruction loss plot — Image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="fccf" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 13a</strong> shows the test data generated by the <em class="nz">make_blobs()</em> function. <strong class="ne fr">Fig. 13b</strong> and <strong class="ne fr">Fig. 13c</strong> show animations of the <em class="nz">PMT</em> and <em class="nz">VAE</em> training methods respectively.</p><p id="7ddb" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 13d</strong> shows the plot of the <em class="nz">EMD</em> errors (𝐸-𝑒𝑚𝑑) calculated for both <em class="nz">PMT</em>, <em class="nz">VAE</em> and the base value (𝐵-𝑒𝑚𝑑). As you can see from this <em class="nz">PMT</em>’s 𝐸-𝑒𝑚𝑑 are closer to 𝐵-𝑒𝑚𝑑 than those of <em class="nz">VAE</em> which means that its performance is better.</p><p id="95af" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 13e</strong> shows the plot of the reconstruction error for both <em class="nz">PMT</em> and <em class="nz">VAE</em>. As you can see from this <em class="nz">PMT</em>’s reconstruction error is an order of magnitude lower than that of of <em class="nz">VAE</em>.</p><h1 id="b07a" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Square with another square</h1><p id="003a" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The second dataset is quite simple. We just have an outer square with a uniform distribution of data points inside it, with an inner square that is also filled with a uniform distribution but with a greater density. This will help us test for non-gaussian distributions with sharp details.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rm"><img src="../Images/1bb4f3ed920e1990972c46ee0858c720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BJxy_gd-rj-PqJU8xxGXg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 14a</strong> Generated data — Image by author</figcaption></figure></div></div><div class="mr"><div class="ab cb"><div class="lm qu ln qv lo qw cf qx cg qy ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr qz ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/a1be878f264605880859aba8217440fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*PueszjO3bIZpizQbbL6tdQ.gif"/></div></figure><figure class="lb mr re ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/56a7800a3c07bfaaba7aef6d3d124da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*cEB430w5H853WWroirtWwA.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rf ed rg rh"><strong class="bf oc">Fig. 14b</strong> PMT training animation/<strong class="bf oc">Fig. 14c</strong> VAE training animation — Image by author</figcaption></figure></div><div class="ab ke"><figure class="lb mr rn ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/77f6326015da719f593d33c7e138cabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lSvHFXrchIucFVZSd_ll5A.png"/></div></figure><figure class="lb mr ro ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/8ef90a759f521bbcc862d0f8b2540a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*8r-I9B8CJhz0eL2ilt8fFA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rp ed rq rh"><strong class="bf oc">Fig. 14d</strong> EMD plot/<strong class="bf oc">Fig. 14e</strong> Reconstruction loss plot — Image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4823" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 14d</strong> shows the <em class="nz">EMD</em> errors plot and you can see from this that <em class="nz">PMT</em> outperforms <em class="nz">VAE</em>.</p><p id="8c4f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 14e</strong> shows the reconstruction error values and you can see from this <em class="nz">PMT</em>’s reconstruction error is over 2 orders of magnitude lower than that of <em class="nz">VAE</em>.</p><h1 id="4568" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Human behavior</h1><p id="71d8" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">The next dataset is made of human body motion sensor data acquired by performing several physical activities. It was extracted from <a class="af ny" href="https://www.kaggle.com/datasets/gaurav2022/mobile-health" rel="noopener ugc nofollow" target="_blank">Mobile Health Human Behavior Analysis Dataset</a>¹. This one has 3 dimensions instead of 2 like the previous datasets.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rr"><img src="../Images/4834f93ea998bc644806503e7241e0ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4_Eoo4lR42SZWWwuwYHfRw.gif"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 15a</strong> Test data — Image by author</figcaption></figure></div></div><div class="mr"><div class="ab cb"><div class="lm qu ln qv lo qw cf qx cg qy ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr rs ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/c74d9ef38f096283e44f1335265f73af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*wY5pKXhzsDxEx41LWZJOow.gif"/></div></figure><figure class="lb mr rs ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/b4b21e69eceed64faf940b9ec81f704d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*K97bYG2gBmLhYcPOUsiJRw.gif"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rp ed rq rh"><strong class="bf oc">Fig. 15b</strong> PMT training animation/<strong class="bf oc">Fig. 15c</strong> VAE training animation — Image by author</figcaption></figure></div><div class="ab ke"><figure class="lb mr rt ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/57f7598cc9a77276144674295864052a.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*N0KzPtFdCkJhsCVHY0a_hQ.png"/></div></figure><figure class="lb mr ru ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/9839cdb2d118c7ae47804972cdf1c958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*CXAIIkDJ2N3npJbw7M1eTA.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rf ed rg rh"><strong class="bf oc">Fig. 15d</strong> EMD plot/<strong class="bf oc">Fig. 15e</strong> Reconstruction loss plot — Image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="f39e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 15d</strong> shows the <em class="nz">EMD</em> errors plot and once again <em class="nz">PMT</em> outperforms <em class="nz">VAE</em> again.</p><p id="27b4" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 15e</strong> shows that <em class="nz">PMT</em>’s reconstruction error is over an order of magnitude lower than that of <em class="nz">VAE</em>.</p><h1 id="5a1d" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">MNIST</h1><p id="6e25" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Lastly we have the famous <a class="af ny" href="https://huggingface.co/datasets/ylecun/mnist" rel="noopener ugc nofollow" target="_blank">MNIST Dataset</a>². As you know it contains bitmaps of numbers written by humans and the task here is to try to generated new data points that look like real hand drawn numbers. This is an interesting dataset because it has a large number of output dimensions (784) and a latent space of 4 dimensions.</p></div></div><div class="mr"><div class="ab cb"><div class="lm qu ln qv lo qw cf qx cg qy ci bh"><div class="mm mn mo mp mq ab ke"><figure class="lb mr rv ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/687c7882f1b5c102c2af06b17783767c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*0Oi_GgtzW7lpBxJTSQ_q1g.jpeg"/></div></figure><figure class="lb mr rv ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/c819342f0f0639d2a25e8085a495a36c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*-iRdpqIPLkdOTnhyNvqgVA.jpeg"/></div></figure><figure class="lb mr rs ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/cfb4cc8a2b64620e1015b78452b2c3c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fnrQBtZ70foA2otRE6kaVQ.jpeg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rp ed rq rh"><strong class="bf oc">Fig. 17a</strong> PMT original data/<strong class="bf oc">Fig. 17b</strong> PMT reconstruction/<strong class="bf oc">Fig. 17c</strong> PMT generated samples — Image by author</figcaption></figure></div><div class="ab ke"><figure class="lb mr rv ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/4c31557abcf5abe663e67956a511168a.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*35ekgYzXFNeUUBN0RJd3pw.jpeg"/></div></figure><figure class="lb mr rv ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/f02053feb01df2379b898c570723060d.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*LdJZDyx7miXdrd3AEUV3hw.jpeg"/></div></figure><figure class="lb mr rs ra rb rc rd paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><img src="../Images/574541dc3ffd8220ccf516c1d30951dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rjYTN0jqVXGY6cErKS9QbA.jpeg"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx rp ed rq rh"><strong class="bf oc">Fig. 16c</strong> VAE original data/<strong class="bf oc">Fig. 16e</strong> VAE reconstruction/<strong class="bf oc">Fig. 16e</strong> VAE generated samples — Image by author</figcaption></figure></div></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rw"><img src="../Images/6654897ba08399ce6612558a8d5f6720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4thbvdxx3mhVkCBU8Dmkzw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx"><strong class="bf oc">Fig. 16g</strong> Reconstruction loss plot — Image by author</figcaption></figure><p id="ba55" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This dataset does not have an EMD error plot since it would be very difficult to calculate (and not indicative) given the large number of output dimensions.</p><p id="e4a2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk"><strong class="ne fr">Fig. 16b</strong> plots the reconstruction errors, once again <em class="nz">PMT</em>’s is lower than <em class="nz">VAE</em>’s.</p><h1 id="2176" class="oa ob fq bf oc od oe gq of og oh gt oi oj ok ol om on oo op oq or os ot ou ov bk">Conclusion and what’s next</h1><p id="1919" class="pw-post-body-paragraph nc nd fq ne b go ow ng nh gr ox nj nk nl oy nn no np oz nr ns nt pa nv nw nx fj bk">Approximating stochastic functions with a single output is very useful for forecasting single value distributions, like temperatures or market values. But the ability to produce multiple outputs make the method apt for a large variety of use cases like simulation and generative tasks.</p><p id="d224" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The multiple output method described in this article has proved that in the experiment datasets, it is capable of outperforming VAEs in both probabilistic likeness and reconstruction. I believe they will produce better results in a variety of real world use cases too.</p><p id="4b0b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the future I would like to continue testing <em class="nz">PMT</em> on higher dimensional datasets for generative purposes, such as <a class="af ny" href="https://www.kaggle.com/datasets/zalando-research/fashionmnist" rel="noopener ugc nofollow" target="_blank">Fashion MNIST</a> and <a class="af ny" href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank">CelebA</a>. For this purpose it will also be necessary to experiment with deep networks and Convolutional Neural Networks (CNN).</p><p id="d12b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Feel free to reach out to me with any questions or comments.</p></div></div></div><div class="ab cb rx ry rz sa" role="separator"><span class="sb by bm sc sd se"/><span class="sb by bm sc sd se"/><span class="sb by bm sc sd"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4280" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[1]: Mobile Health Human Behavior Analysis</p><p id="27b9" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">https://www.kaggle.com/datasets/gaurav2022/mobile-health</p><p id="d05f" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">CC0 Public Domain <a class="af ny" href="https://creativecommons.org/publicdomain/zero/1.0/" rel="noopener ugc nofollow" target="_blank">https://creativecommons.org/publicdomain/zero/1.0/</a></p><p id="9c74" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">[2] MNIST Handwritten Database</p><div class="sf sg sh si sj sk"><a href="http://yann.lecun.com/exdb/mnist?source=post_page-----ffefc7099a90--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="sl ab ig"><div class="sm ab co cb sn so"><h2 class="bf fr hw z io sp iq ir sq it iv fp bk">MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges</h2><div class="sr l"><h3 class="bf b hw z io sp iq ir sq it iv dx">of handwritten digits Yann LeCun, Courant Institute, NYU Corinna Cortes, Google Labs, New York Christopher J.C. Burges…</h3></div><div class="ss l"><p class="bf b dy z io sp iq ir sq it iv dx">yann.lecun.com</p></div></div></div></a></div><p id="4e39" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">MIT <a class="af ny" href="https://choosealicense.com/licenses/mit/" rel="noopener ugc nofollow" target="_blank">https://choosealicense.com/licenses/mit/</a></p></div></div></div></div>    
</body>
</html>