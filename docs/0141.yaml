- en: 'Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普及化LLMs：4位量化以实现最佳LLM推理
- en: 原文：[https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34?source=collection_archive---------3-----------------------#2024-01-15](https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34?source=collection_archive---------3-----------------------#2024-01-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34?source=collection_archive---------3-----------------------#2024-01-15](https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34?source=collection_archive---------3-----------------------#2024-01-15)
- en: A deep dive into model quantization with GGUF and llama.cpp and model evaluation
    with LlamaIndex
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨使用GGUF和llama.cpp进行模型量化，并使用LlamaIndex进行模型评估
- en: '[](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)[![Wenqi
    Glantz](../Images/65b518863e01aaa48ecc6b8ac6d1be60.png)](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)
    [Wenqi Glantz](https://medium.com/@wenqiglantz?source=post_page---byline--be30cf4e0e34--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)
    ·15 min read·Jan 15, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--be30cf4e0e34--------------------------------)
    ·阅读时间15分钟·2024年1月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d743e68eaf63534c03435aa893542744.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d743e68eaf63534c03435aa893542744.png)'
- en: Image generated by DALL-E 3 by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用DALL-E 3生成
- en: Quantizing a model is a technique that involves converting the precision of
    the numbers used in the model from a higher precision (like 32-bit floating point)
    to a lower precision (like 4-bit integers). Quantization is a balance between
    efficiency and accuracy, as it can come at the cost of a slight decrease in the
    model’s accuracy, as the reduction in numerical precision can affect the model’s
    ability to represent subtle differences in data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型进行量化是一种将模型中使用的数字精度从更高精度（如32位浮点数）转换为更低精度（如4位整数）的技术。量化是在效率和准确性之间的平衡，因为它可能会以模型准确性的轻微下降为代价，因为精度的降低可能会影响模型表示数据中微妙差异的能力。
- en: This has been my assumption from learning LLMs from various sources.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我从各种来源学习LLMs时的假设。
- en: In this article, we will explore the detailed steps to quantize `Mistral-7B-Instruct-v0.2`
    into a 5-bit and a 4-bit model. We will then upload the quantized models to the
    Hugging Face hub. Lastly, we will load the quantized models and evaluate them
    and the base model to find out the performance impact quantization brings to a
    RAG pipeline.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨将`Mistral-7B-Instruct-v0.2`模型量化为5位和4位模型的详细步骤。然后，我们将把量化后的模型上传到Hugging
    Face平台。最后，我们将加载这些量化后的模型，并对它们及基础模型进行评估，以了解量化对RAG流水线性能的影响。
- en: Does it conform to my original assumption? Read on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 它符合我最初的假设吗？继续阅读。
- en: Why do we quantize a model?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们要对模型进行量化？
- en: 'The benefits of quantizing a model include the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 量化模型的好处包括以下几点：
- en: '**Reduced Memory Usage**: Lower…'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少内存使用**：更低的…'
