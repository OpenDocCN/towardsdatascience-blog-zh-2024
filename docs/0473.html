<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How OpenAI’s Sora is Changing the Game: An Insight into Its Core Technologies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How OpenAI’s Sora is Changing the Game: An Insight into Its Core Technologies</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19">https://towardsdatascience.com/how-openais-sora-is-changing-the-game-an-insight-into-its-core-technologies-bd1ad17170df?source=collection_archive---------4-----------------------#2024-02-19</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f1e7" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A masterpiece of state of the art technologies</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Ryota Kiuchi, Ph.D." class="l ep by dd de cx" src="../Images/5459c434848898345d932320c4a01312.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*BEoGVPFLLJJv49YGqJGBgA@2x.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://rkiuchir.medium.com/?source=post_page---byline--bd1ad17170df--------------------------------" rel="noopener follow">Ryota Kiuchi, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--bd1ad17170df--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Feb 19, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/4045008b97b49145c5838da473557a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tqo3CUzdaYQdFh8U"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@kaushikpanchal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kaushik Panchal</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0209" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">On February 15, 2024, OpenAI, which had astonished the world by announcing ChatGPT in late 2022, once again stunned the world with the unveiling of Sora. This technology, capable of creating videos up to a minute long from a text prompt, is undeniably set to be a breakthrough.</p><p id="bdb5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this blog post, I will introduce the underlying methodologies and research behind this astonishing technology, based on the technical report released by OpenAI.</p><p id="6216" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Incidentally, “Sora” means “sky” in Japanese. Although it has not been officially announced whether this naming was intentional, it is speculated to be so, given that their official release tweet featured a video themed around Tokyo.</p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">OpenAI unveils the Sora to the world via X</figcaption></figure><h1 id="7381" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">Table of Contents</h1><ul class=""><li id="3020" class="nd ne fq nf b go oy nh ni gr oz nk nl nm pa no np nq pb ns nt nu pc nw nx ny pd pe pf bk"><a class="af nc" href="#bfb1" rel="noopener ugc nofollow">About Sora</a></li><li id="b91c" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk"><a class="af nc" href="#7e7b" rel="noopener ugc nofollow">What kind of technology and research is behind it?</a></li><li id="f585" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk"><a class="af nc" href="#7b12" rel="noopener ugc nofollow">The capabilities enabled by these research efforts for Sora</a></li><li id="e586" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk"><a class="af nc" href="#642d" rel="noopener ugc nofollow">The future of Sora</a></li></ul></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="bfb1" class="oc od fq bf oe of pt gq oh oi pu gt ok ol pv on oo op pw or os ot px ov ow ox bk">About Sora</h1><p id="c596" class="pw-post-body-paragraph nd ne fq nf b go oy nh ni gr oz nk nl nm pa no np nq pb ns nt nu pc nw nx ny fj bk">Sora is an advanced text-to-video conversion model developed by OpenAI, and its capabilities and application range illustrate a new horizon in modern AI technology. This model is not limited to generating mere seconds of video; it can create videos up to one minute long, maintaining high visual quality while faithfully reproducing user instructions. It’s as if it’s bringing dreams to life.</p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">OpenAI Sora’s demo via X</figcaption></figure><h2 id="c4db" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk"><strong class="al">Generating Complex Scenes Based on the Real World</strong></h2><p id="d055" class="pw-post-body-paragraph nd ne fq nf b go oy nh ni gr oz nk nl nm pa no np nq pb ns nt nu pc nw nx ny fj bk">Sora understands how elements described in prompts exist and operate within the physical world. This allows the model to accurately represent user-intended movements and actions within videos. For example, it can realistically recreate the sight of a person running or the movement of natural phenomena. Furthermore, it reproduces precise details of multiple characters, types of movement, and the specifics of subjects and backgrounds.</p><p id="3eeb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Previously, video creation with Generative AI has faced the difficult challenge of maintaining consistency and reproducibility across different scenes. This is because understanding previous contexts and details completely when generating each scene or frame individually and appropriately inheriting them to the next scene is challenging. However, this model maintains narrative consistency by combining a deep understanding of language with visual context and interpreting prompts accurately. It can also capture the emotions and personalities of characters from the given prompts and portray them as expressive characters within the video.</p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">The post by Bill Peebles (OpenAI) via X</figcaption></figure></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="7e7b" class="oc od fq bf oe of pt gq oh oi pu gt ok ol pv on oo op pw or os ot px ov ow ox bk">What kind of technology and research is behind it?</h1><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qp"><img src="../Images/5855f8b4ebf6f0eabb6f06a22673761c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7F07QQkM1kNyIlpM"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5350" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sora is built upon a foundation of prior studies in image data generation modeling. Previous research has employed various methods such as recurrent networks, Generative Adversarial Networks (GANs), autoregressive transformers, and diffusion models, but has often focused on a narrow category of visual data, shorter videos, or videos of a fixed size. Sora surpasses these limitations and has been significantly improved to generate videos across diverse durations, aspect ratios, and resolutions. In this section, I will introduce the core technologies that support these advancements.</p><h2 id="e32c" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">1. Transformer</h2><blockquote class="qq qr qs"><p id="fe41" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Vaswani et al. (2017), “Attention is all you need.”</p></blockquote><p id="e480" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Transformer model is a neural network architecture that revolutionized the field of natural language processing (NLP). It was first proposed by Vaswani et al. in 2017. This model significantly overcame the challenges that traditional Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) faced, supporting various breakthrough technologies as an innovative method today.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qu"><img src="../Images/2295b7bce37a22b9414d8777b53f8951.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*NrX46FTJ0XC4xUUSBjNbGg.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: The Transformer — model architecture. ｜Vaswani et al. (2017)</figcaption></figure><p id="0390" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Issues with RNNs:</p><ul class=""><li id="bb94" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pd pe pf bk">The problem of long-term dependencies: Although RNNs theoretically can transmit information through time, they struggle to capture dependencies over long durations in practice.</li><li id="4c6f" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk">Limitations on parallelization: Since the computation at each step in an RNN depends on the output of the previous step, sequential processing (e.g., processing words or sentences in a text one by one, in order) is mandatory, preventing the utilization of parallel processing advantages offered by modern computer architectures. This made training on large datasets inefficient.</li></ul><p id="4615" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Issues with CNNs:</p><ul class=""><li id="0610" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pd pe pf bk">Fixed receptive field size: While CNNs excel at extracting local features, their fixed receptive field size limits their ability to capture long-distance dependencies throughout the context.</li><li id="2ae2" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk">Difficulty in modeling the hierarchical structure of natural language: It’s challenging to directly model the hierarchical structure of language, which can be insufficient for deep contextual understanding.</li></ul><p id="7fb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">New features of the Transformer:</p><ul class=""><li id="4598" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pd pe pf bk">Attention Mechanism: Enables direct modeling of dependencies between any positions in the sequence, allowing for the direct capture of long dependencies and extensive context.</li><li id="a28d" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk">Realization of parallelization: Since the input data is processed as a whole at once, a high degree of parallelization in computation is achieved, significantly accelerating training on large datasets.</li><li id="9f63" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk">Variable receptive field: The attention mechanism allows the model to dynamically adjust the “receptive field” size as needed. This means the model can naturally focus on local information for certain tasks or data, and consider broader context in other cases.</li></ul><p id="9b4b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about Transformer:</em></p><div class="qv qw qx qy qz ra"><a rel="noopener follow" target="_blank" href="/transformers-141e32e69591?source=post_page-----bd1ad17170df--------------------------------"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">How Transformers Work</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently…</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">towardsdatascience.com</p></div></div><div class="rj l"><div class="rk l rl rm rn rj ro lr ra"/></div></div></a></div><h2 id="7218" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">2. Vision Transformer (ViT)</h2><blockquote class="qq qr qs"><p id="3fa7" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Dosovitskiy, et al. (2020), “An image is worth 16x16 words: Transformers for image recognition at scale.”</p></blockquote><p id="e1e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this study, the principles of the Transformer, which revolutionized natural language processing (NLP), are applied to image recognition, opening up new horizons.</p><p id="4577" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Token and Patch</strong></p><p id="c06c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the original Transformer paper, tokens primarily represent parts of words or sentences, and analyzing the relationships between these tokens allows for a deep understanding of the sentence’s meaning. In this study, to apply this concept of tokens to visual data, images are divided into small sections (patches) of 16x16, and each patch is treated as a “token” within the Transformer. This approach enables the model to learn how each patch is related within the entire image, allowing for the recognition and understanding of the entire image based on this. It surpasses the limitations of the fixed receptive field size of traditional CNN models used in image recognition, enabling flexible capture of any positional relationships within an image.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rp"><img src="../Images/4b9ab7bd066f9cd9c49e6815c9b9752f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pcQgMWXOAuuANPhDm6Xfg.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: Model overview. ｜Dosovitskiy, et al. (2020)</figcaption></figure><p id="abe2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about ViT:</em></p><p id="db11" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><a class="af nc" href="https://machinelearningmastery.com/the-vision-transformer-model/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/the-vision-transformer-model/</a></p><h2 id="c34b" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">3. Video Vision Transformer (ViViT)</h2><blockquote class="qq qr qs"><p id="a76d" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Arnab, et al. (2021), “Vivit: A video vision transformer.”</p></blockquote><p id="e9ec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">ViViT further extends the concept of the Vision Transformer, applying it to the multidimensional data of videos. Video data is more complex as it contains both static image information (spatial elements) and dynamic information that changes over time (temporal elements). ViViT decomposes videos into spatiotemporal patches, treating these as tokens within the Transformer model. With the introduction of spatiotemporal patches, ViViT is able to simultaneously capture both static and dynamic elements within a video and model the complex relationships between them.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rq"><img src="../Images/24cf64ec81ec07779af159c87eae68d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-9nWpDdDN6YiXkYRYYFaw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 3: Tubelet (the spatio-temporal input volume) embedding image. ｜Arnab, et al. (2021)</figcaption></figure><p id="6e04" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about ViViT:</em></p><div class="qv qw qx qy qz ra"><a href="https://medium.com/aiguys/vivit-video-vision-transformer-648a5fff68a4?source=post_page-----bd1ad17170df--------------------------------" rel="noopener follow" target="_blank"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">ViViT 📹 Video Vision Transformer</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">ICCV 2021 ✨, Google Research</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">medium.com</p></div></div><div class="rj l"><div class="rr l rl rm rn rj ro lr ra"/></div></div></a></div><h2 id="02df" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">4. Masked Autoencoders (MAE)</h2><blockquote class="qq qr qs"><p id="af7c" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">He, et al. (2022), “Masked autoencoders are scalable vision learners.”</p></blockquote><p id="075a" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This study dramatically improved the traditionally high computational costs and inefficiencies in training on large datasets associated with high dimensionality and vast amounts of information, using a self-supervised pre-training method called the Masked Autoencoder.</p><p id="9a6d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Specifically, by masking parts of the input image, the network is trained to predict the information of the hidden parts, resulting in more efficient learning of important features and structures within the image, and acquiring rich representations of visual data. This process has made the compression and representation learning of data more efficient, reduced computational costs, and enhanced the versatility of different types of visual data and tasks.</p><p id="d729" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The approach of this study is also closely related to the evolution of language models by BERT (Bidirectional Encoder Representations from Transformers). While BERT enabled a deep contextual understanding of text data through Masked Language Modeling (MLM), He et al. have applied a similar masking technique to visual data, achieving a deeper understanding and representation of images.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rs"><img src="../Images/3933fb20cd83083872d34a3afdc4c2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ERkwlGBr6l6TUgISkRNMuA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: Masked Autoencoders Image. ｜He, et al. (2022)</figcaption></figure><p id="5b78" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about MAE:</em></p><div class="qv qw qx qy qz ra"><a rel="noopener follow" target="_blank" href="/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=post_page-----bd1ad17170df--------------------------------"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Paper explained: Masked Autoencoders Are Scalable Vision Learners</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">How reconstructing masked parts of an image can be beneficial</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">towardsdatascience.com</p></div></div><div class="rj l"><div class="rt l rl rm rn rj ro lr ra"/></div></div></a></div><h2 id="c574" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">5. Native Resolution Vision Transformer (NaViT)</h2><blockquote class="qq qr qs"><p id="1942" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Dehghani, et al. (2023), “Patch n’Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution.”</p></blockquote><p id="d8be" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This study proposed the Native Resolution ViTransformer (NaViT), a model designed to further expand the applicability of the Vision Transformer (ViT) to images of any aspect ratio or resolution.</p><p id="6013" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Challenges of Traditional ViT</strong></p><p id="3cb0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The Vision Transformer introduced a groundbreaking approach by dividing images into fixed-size patches and treating these patches as tokens, applying the transformer model to image recognition tasks. However, this approach assumed models optimized for specific resolutions or aspect ratios, requiring model readjustment for images of different sizes or shapes. This was a significant constraint, as real-world applications often need to handle images of diverse sizes and aspect ratios.</p><p id="9708" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Innovations of NaViT</strong></p><p id="b3c1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">NaViT is designed to efficiently process images of any aspect ratio or resolution, allowing them to be directly inputted into the model without prior adjustment. Sora applies this flexibility to videos as well, significantly enhancing flexibility and adaptability by seamlessly handling videos and images of various sizes and shapes.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ru"><img src="../Images/47627d8a0fd5e4cd491dd460ad0389f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87UADeyN82o6RBciD9xbGw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 2:｜Dehghani, et al. (2023)</figcaption></figure><h2 id="8e39" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">6. Diffusion Models</h2><blockquote class="qq qr qs"><p id="b58b" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sohl-Dickstein, et al. (2015), “Deep unsupervised learning using nonequilibrium thermodynamics.”</p></blockquote><p id="925e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Alongside the Transformer, Diffusion Models form the backbone technology supporting Sora. This research laid the theoretical foundation for diffusion models, a deep learning model using non-equilibrium thermodynamics. Diffusion models introduced the concept of a diffusion process that starts with random noise (data without any pattern) and gradually removes this noise to create data resembling actual images or videos.</p><p id="335c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For instance, imagine starting with mere random dots, which gradually transform into videos of beautiful landscapes or people. This approach was later applied to the generation of complex data such as images and sounds, contributing to the development of high-quality generative models.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk rv"><img src="../Images/5b45899f9277252955107f0d698a691c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2TTYGi0GyZuCB8MnC7ehUA.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image of denoising process｜Image Credit (OpenAI)</figcaption></figure><blockquote class="qq qr qs"><p id="7d2e" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Ho et al. (2020), “Denoising diffusion probabilistic models.”</p><p id="a74f" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Nichol and Dhariwal (2021), “Improved denoising diffusion probabilistic models.”</p></blockquote><p id="adc7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Building on the theoretical framework by Sohl-Dickstein et al. (2015), practical data generation models known as Denoising Diffusion Probabilistic Models (DDPM) were developed. This model has shown particularly notable results in high-quality image generation, demonstrating the effectiveness of diffusion models.</p><p id="c16f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Impact of Diffusion Models on Sora</strong></p><p id="c5bb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Typically, to train machine learning models, a lot of labeled data is needed (for example, being told “This is an image of a cat”). However, diffusion models can learn from unlabeled data as well, allowing them to utilize the vast amount of visual content available on the internet to generate various types of videos. In other words, Sora can observe different videos and images and learn “this is what a normal video looks like.”</p><p id="68b7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about Diffusion Models:</em></p><div class="qv qw qx qy qz ra"><a rel="noopener follow" target="_blank" href="/diffusion-models-made-easy-8414298ce4da?source=post_page-----bd1ad17170df--------------------------------"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Diffusion Models Made Easy</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">Understanding the Basics of Denoising Diffusion Probabilistic Models</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">towardsdatascience.com</p></div></div><div class="rj l"><div class="rw l rl rm rn rj ro lr ra"/></div></div></a></div><div class="qv qw qx qy qz ra"><a rel="noopener follow" target="_blank" href="/understanding-the-denoising-diffusion-probabilistic-model-the-socratic-way-445c1bdc5756?source=post_page-----bd1ad17170df--------------------------------"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Understanding the Denoising Diffusion Probabilistic Model, the Socratic Way</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">A deep dive into the motivation behind the denoising diffusion model and detailed derivations for the loss function</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">towardsdatascience.com</p></div></div><div class="rj l"><div class="rx l rl rm rn rj ro lr ra"/></div></div></a></div><h2 id="cd71" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">7. Latent Diffusion Models</h2><blockquote class="qq qr qs"><p id="816b" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Rombach, et al. (2022), “High-resolution image synthesis with latent diffusion models.”</p></blockquote><p id="bf8b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This research has made a significant contribution to the field of high-resolution image synthesis using diffusion models. It proposes a method that significantly reduces computational costs compared to direct high-resolution image generation by utilizing diffusion models in the latent space while maintaining quality. In other words, instead of directly manipulating images, it demonstrates that by encoding and introducing the diffusion process to data represented in the latent space (a lower-dimensional space holding compressed representations of images), it is possible to achieve with fewer computational resources.</p><p id="5aaa" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Sora applies this technology to video data, compressing the temporal+spatial data of videos into a lower-dimensional latent space, and then undergoing a process of decomposing it into spatiotemporal patches. This efficient data processing and generation capability in the latent space plays a crucial role in enabling Sora to generate higher-quality visual content more rapidly.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ry"><img src="../Images/3f152224408eb5cdaf418cf2567a2045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DhML9gFCnfwB6ezJrW0jaQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image of visual encoding｜Image Credit (OpenAI)</figcaption></figure><p id="9530" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about Latent Diffusion Models:</em></p><div class="qv qw qx qy qz ra"><a rel="noopener follow" target="_blank" href="/paper-explained-high-resolution-image-synthesis-with-latent-diffusion-models-f372f7636d42?source=post_page-----bd1ad17170df--------------------------------"><div class="rb ab ig"><div class="rc ab co cb rd re"><h2 class="bf fr hw z io rf iq ir rg it iv fp bk">Paper Explained — High-Resolution Image Synthesis with Latent Diffusion Models</h2><div class="rh l"><h3 class="bf b hw z io rf iq ir rg it iv dx">While OpenAI has dominated the field of natural language processing with their generative text models, their image…</h3></div><div class="ri l"><p class="bf b dy z io rf iq ir rg it iv dx">towardsdatascience.com</p></div></div><div class="rj l"><div class="rz l rl rm rn rj ro lr ra"/></div></div></a></div><h2 id="0f9d" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk">8. Diffusion Transformer (DiT)</h2><blockquote class="qq qr qs"><p id="6c98" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Peebles and Xie. (2023), “Scalable diffusion models with transformers.”</p></blockquote><p id="e92c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This research might be the most crucial in realizing Sora. As mentioned in the technical report published by OpenAI, Sora employs not a vanilla (normal) transformer but a diffusion transformer (DiT).</p><blockquote class="qq qr qs"><p id="d523" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Importantly, Sora is a diffusion <em class="fq">transformer</em>. (via OpenAI Sora technical report)</p></blockquote><p id="c699" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The study introduced a new model that replaces the U-net component, commonly used in diffusion models, with a Transformer structure. This structure enables the Latent Diffusion Model through operations on latent patches by the Transformer. This approach allows for more efficient handling of image patches, enabling the generation of high-quality images while effectively utilizing computational resources. The incorporation of this Transformer, which differs from the Stable Diffusion announced by Stability AI in 2022, is considered to contribute to more natural video generation.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sa"><img src="../Images/54dc92e3b369086a4a28bb343998d748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJDGpWeoEofPcnLUwhHkHQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 1: Generated images by the Diffusion Transformers｜Peebles and Xie. (2023)</figcaption></figure><p id="8fa1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Furthermore, it’s important to note that their validation results demonstrate the scalability of DiT, significantly contributing to the realization of Sora. Being scalable means that the model’s performance improves with an increase in the transformer’s depth/width (making the model more complex) or the number of input tokens.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sb"><img src="../Images/38de39d61eef4b8bfb6e62a3d800e8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e8cjO8LbunxbxQdWNga_Tw.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Figure 8 &amp; 9: Scalability of the Diffusion Transformers｜Peebles and Xie. (2023)</figcaption></figure><ul class=""><li id="d3ea" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny pd pe pf bk">Gflops (Computational performance): A unit of measure for a computer’s calculating speed equal to one billion floating-point operations per second. In this paper, network complexity is measured by Gflops.</li><li id="23dc" class="nd ne fq nf b go pg nh ni gr ph nk nl nm pi no np nq pj ns nt nu pk nw nx ny pd pe pf bk">FID (Fréchet Inception Distance): One of the evaluation metrics for image generation, where a lower value indicates higher accuracy. It quantitatively assesses the quality of generated images by measuring the distance between the feature vectors of generated images and real images.</li></ul><p id="d33b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This has already been observed in the field of natural language processing, as confirmed by Kaplan et al. (2020) and Brown et al. (2020), supporting the crucial characteristics behind the innovative success of ChatGPT.</p><blockquote class="qq qr qs"><p id="c562" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Kaplan et al. (2020), “Scaling Laws for Neural Language Models.”</p><p id="7f0e" class="nd ne qt nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Brown, et al. (2020), “Language models are few-shot learners.”</p></blockquote><p id="18ca" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This significant feature, in addition to generating high-quality images at a lower computational cost than traditional diffusion models due to the benefits of the Transformer, indicates that even higher-quality images can be produced with larger computational resources. Sora applies this technology to video generation.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk sc"><img src="../Images/2d63e2c57fef03126f80016284ea38cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AX6PvB1oAXOyrJ9h4lAPQQ.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Scalability of video generation｜Image Credit (OpenAI)</figcaption></figure><p id="afdf" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="qt">For more detailed technical explanations about DiT:</em></p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="sd ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Review the paper of DiT by hu-po via YouTube</figcaption></figure><h1 id="7b12" class="oc od fq bf oe of og gq oh oi oj gt ok ol om on oo op oq or os ot ou ov ow ox bk">The capabilities enabled by these research efforts for Sora</h1><h2 id="4ea2" class="py od fq bf oe pz qa qb oh qc qd qe ok nm qf qg qh nq qi qj qk nu ql qm qn qo bk"><strong class="al">Variable durations, resolutions, aspect ratios</strong></h2><p id="903f" class="pw-post-body-paragraph nd ne fq nf b go oy nh ni gr oz nk nl nm pa no np nq pb ns nt nu pc nw nx ny fj bk">Primarily, thanks to NaViT, Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos, and everything in between. This means it can create visuals for various device types at any resolution.</p><p id="0cb4" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Prompting with images and videos</strong></p><p id="4355" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Currently, the videos generated by Sora, as demonstrated, are created in a text-to-video format, where instructions are given through text prompts. However, as can be easily anticipated from the previous research, it’s also possible to use existing images or videos as inputs, not just text. This allows for the animation of images or for Sora to imagine and output the past or future of an existing video as visuals.</p><p id="5212" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">3D consistency</strong></p><p id="dca5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While it’s not clear how the aforementioned research is directly involved, Sora can generate videos with dynamic camera motion. As the camera shifts and rotates, people and scene elements move consistently through three-dimensional space.</p></div></div></div><div class="ab cb pl pm pn po" role="separator"><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr ps"/><span class="pp by bm pq pr"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="642d" class="oc od fq bf oe of pt gq oh oi pu gt ok ol pv on oo op pw or os ot px ov ow ox bk">The future of Sora</h1><p id="180a" class="pw-post-body-paragraph nd ne fq nf b go oy nh ni gr oz nk nl nm pa no np nq pb ns nt nu pc nw nx ny fj bk">In this blog post, I have explained the technologies behind OpenAI’s AI for generating videos, Sora, which has already shocked the world. Once it becomes publicly available and accessible to a wider audience, it is bound to make an even more significant impact worldwide.</p><p id="97ad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The impact of this breakthrough is expected to span across various aspects of video creation, but it is predicted that it may likely evolve from video to further advancements in 3D modeling. If that becomes the case, not only video creators but also the production of visuals in virtual spaces like the metaverse could soon be easily generated by AI.</p><p id="bf24" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The arrival of such a future has already been implied as below:</p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Martin Nebelong’s post about Micael Rublof’s product via X</figcaption></figure><p id="6b80" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Currently, Sora is perceived as “merely” a video generation model, but Jim Fan from Nvidia has implied it might be a data-driven physics engine. This suggests the possibility that AI, from a vast amount of real-world videos and (though not explicitly mentioned) videos considering physical behaviors like those from Unreal Engine, might understand physical laws and phenomena. If so, the emergence of text-to-3D in the near future is also highly probable.</p><figure class="mm mn mo mp mq mr"><div class="nz io l ed"><div class="oa ob l"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Jim Fan’s intriguing post via X</figcaption></figure><p id="43e2" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr"><em class="qt">Thank you so much for reading this article. <br/>Your clap to this article and subscription to </em></strong><a class="af nc" href="https://rkiuchir.medium.com/subscribe" rel="noopener"><strong class="nf fr"><em class="qt">my newsletter</em></strong></a><strong class="nf fr"><em class="qt"> would motivate me a lot!</em></strong></p></div></div></div></div>    
</body>
</html>