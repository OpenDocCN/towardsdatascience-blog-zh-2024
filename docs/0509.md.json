["```py\nfrom llama_index.readers.web import SimpleWebPageReader\nfrom llama_index.core.node_parser import SentenceSplitter\n\nlanguage = \"EN\"\nurl_doc = \"https://eur-lex.europa.eu/legal-content/\"+language+\"/TXT/HTML/?uri=CELEX:52021PC0206\"\n\ndocuments = SimpleWebPageReader(html_to_text=True).load_data([url_doc])\n\nparser = SentenceSplitter(chunk_size=1000)\nnodes = parser.get_nodes_from_documents(documents, show_progress=True)\n```", "```py\nprompts={}\nprompts[\"EN\"] = \"\"\"\\\nContext information is below.\n\n---------------------\n{context_str}\n---------------------\n\nGiven the context information and not prior knowledge, generate only questions based on the below query.\n\nYou are a Teacher/ Professor. Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination.\nThe questions should be diverse in nature across the document. Restrict the questions to the context information provided.\"\n\"\"\"\n```", "```py\nfrom llama_index.llms import OpenAI\nfrom llama_index.legacy.finetuning import generate_qa_embedding_pairs\n\nqa_dataset = generate_qa_embedding_pairs(\n    llm=OpenAI(model=\"gpt-3.5-turbo-0125\",additional_kwargs={'seed':42}),\n    nodes=nodes,\n    qa_generate_prompt_tmpl = prompts[language],\n    num_questions_per_chunk=2\n)\n```", "```py\ndef evaluate(dataset, embed_model, insert_batch_size=1000, top_k=5):\n    # Get corpus, queries, and relevant documents from the qa_dataset object\n    corpus = dataset.corpus\n    queries = dataset.queries\n    relevant_docs = dataset.relevant_docs\n\n    # Create TextNode objects for each document in the corpus and create a VectorStoreIndex to efficiently store and retrieve embeddings\n    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n    index = VectorStoreIndex(\n        nodes, embed_model=embed_model, insert_batch_size=insert_batch_size\n    )\n    retriever = index.as_retriever(similarity_top_k=top_k)\n\n    # Prepare to collect evaluation results\n    eval_results = []\n\n    # Iterate over each query in the dataset to evaluate retrieval performance\n    for query_id, query in tqdm(queries.items()):\n        # Retrieve the top_k most similar documents for the current query and extract the IDs of the retrieved documents\n        retrieved_nodes = retriever.retrieve(query)\n        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n\n        # Check if the expected document was among the retrieved documents\n        expected_id = relevant_docs[query_id][0]\n        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc per query\n\n        # Calculate the Mean Reciprocal Rank (MRR) and append to results\n        if is_hit:\n            rank = retrieved_ids.index(expected_id) + 1\n            mrr = 1 / rank\n        else:\n            mrr = 0\n        eval_results.append(mrr)\n\n    # Return the average MRR across all queries as the final evaluation metric\n    return np.average(eval_results)\n```", "```py\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\nembed_model = OpenAIEmbedding(model=model_spec['model_name'],\n                              dimensions=model_spec['dimensions'])\n```", "```py\nembeddings_model_spec = {\n}\n\nembeddings_model_spec['OAI-Large-256']={'model_name':'text-embedding-3-large','dimensions':256}\nembeddings_model_spec['OAI-Large-3072']={'model_name':'text-embedding-3-large','dimensions':3072}\nembeddings_model_spec['OAI-Small']={'model_name':'text-embedding-3-small','dimensions':1536}\nembeddings_model_spec['OAI-ada-002']={'model_name':'text-embedding-ada-002','dimensions':None}\n\nresults = []\n\nlanguages = [\"EN\", \"FR\", \"CS\", \"HU\"]\n\n# Loop through all languages\nfor language in languages:\n\n    # Load dataset\n    file_name=language+\"_dataset.json\"\n    qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)\n\n    # Loop through all models\n    for model_name, model_spec in embeddings_model_spec.items():\n\n        # Get model\n        embed_model = OpenAIEmbedding(model=model_spec['model_name'],\n                                      dimensions=model_spec['dimensions'])\n\n        # Assess embedding score (in terms of MRR)\n        score = evaluate(qa_dataset, embed_model)\n\n        results.append([language, model_name, score])\n\ndf_results = pd.DataFrame(results, columns = [\"Language\" ,\"Embedding model\", \"MRR\"])\n```", "```py\nembeddings_model_spec = {\n}\n\nembeddings_model_spec['E5-mistral-7b']={'model_name':'intfloat/e5-mistral-7b-instruct','max_length':32768, 'pooling_type':'last_token', \n                                        'normalize': True, 'batch_size':1, 'kwargs': {'load_in_4bit':True, 'bnb_4bit_compute_dtype':torch.float16}}\nembeddings_model_spec['ML-E5-large']={'model_name':'intfloat/multilingual-e5-large','max_length':512, 'pooling_type':'mean', \n                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\nembeddings_model_spec['BGE-M3']={'model_name':'BAAI/bge-m3','max_length':8192, 'pooling_type':'cls', \n                                 'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'torch_dtype':torch.float16}}\nembeddings_model_spec['Nomic-Embed']={'model_name':'nomic-ai/nomic-embed-text-v1','max_length':8192, 'pooling_type':'mean', \n                                      'normalize': True, 'batch_size':1, 'kwargs': {'device_map': 'cuda', 'trust_remote_code' : True}}\n\nresults = []\n\nlanguages = [\"EN\", \"FR\", \"CS\", \"HU\"]\n\n# Loop through all models\nfor model_name, model_spec in embeddings_model_spec.items():\n\n    print(\"Processing model : \"+str(model_spec))\n\n    # Get model\n    tokenizer = AutoTokenizer.from_pretrained(model_spec['model_name'])\n    embed_model = AutoModel.from_pretrained(model_spec['model_name'], **model_spec['kwargs'])\n\n    if model_name==\"Nomic-Embed\":\n        embed_model.to('cuda')\n\n    # Loop through all languages\n    for language in languages:\n\n        # Load dataset\n        file_name=language+\"_dataset.json\"\n        qa_dataset = EmbeddingQAFinetuneDataset.from_json(file_name)\n\n        start_time_assessment=time.time()\n\n        # Assess embedding score (in terms of hit rate at k=5)\n        score = evaluate(qa_dataset, tokenizer, embed_model, model_spec['normalize'], model_spec['max_length'], model_spec['pooling_type'])\n\n        # Get duration of score assessment\n        duration_assessment = time.time()-start_time_assessment\n\n        results.append([language, model_name, score, duration_assessment])\n\ndf_results = pd.DataFrame(results, columns = [\"Language\" ,\"Embedding model\", \"MRR\", \"Duration\"])\n```"]