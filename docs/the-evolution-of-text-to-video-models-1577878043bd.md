# 文本到视频模型的演变

> 原文：[https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19](https://towardsdatascience.com/the-evolution-of-text-to-video-models-1577878043bd?source=collection_archive---------3-----------------------#2024-09-19)

## 简化生成视频扩散背后的神经网络

[](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[![Avishek Biswas](../Images/6feb591069f354aa096f6108f1a70ea7.png)](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------) [Avishek Biswas](https://medium.com/@neural.avb?source=post_page---byline--1577878043bd--------------------------------)

·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page---byline--1577878043bd--------------------------------) ·阅读时间9分钟·2024年9月19日

--

![](../Images/d4e0a8ffc80f7a943eba83fbb819d59d.png)

让我们来学习视频扩散！

我们已经见证了AI图像生成的显著进步。但当我们加入时间这一维度时会发生什么呢？毕竟，视频是动态图像。

*文本到视频生成是一项复杂的任务，它要求AI不仅要理解物体的外观，还要理解它们如何随着时间的推移运动和互动。这比文本到图像要复杂一个数量级。*

要制作一个连贯的视频，一个神经网络必须：

1\. 理解输入提示

2\. 理解世界是如何运作的

3\. 了解物体如何运动以及物理学如何应用

4\. 生成在空间、时间和逻辑上都合理的帧序列

尽管面临这些挑战，今天的扩散神经网络在这一领域取得了令人瞩目的进展。在本文中，我们将介绍视频扩散模型背后的主要思想——主要挑战、方法以及该领域的开创性论文。

[此外，本文基于我制作的更大规模YouTube视频。如果你喜欢这篇文章，你也会喜欢观看该视频](https://youtu.be/KRTEOkYftUY)。

# **文本到图像概述**

要理解文本到视频生成，我们需要从它的前身开始：文本到图像的扩散模型。这些模型有一个单一的目标——将随机噪声和文本提示转换为连贯的图像。一般来说，所有生成图像的模型都是这样做的——变分自编码器（VAE）、生成对抗神经网络（GAN）以及扩散模型。

![](../Images/fa3ee732372254ed0b342013d31012df.png)

所有图像生成模型的基本目标是将随机噪声转换为图像，通常是基于附加的条件提示（例如文本）。[图像由作者提供]

扩散，特别依赖于逐渐去噪的过程来生成图像。

1\. 从一个随机生成的噪声图像开始

2\. 使用神经网络逐步去除噪声

3\. 使去噪过程依赖于文本输入

4\. 重复直到生成清晰图像

![](../Images/addc4ac84df4107765f976727e1648e5.png)

扩散模型是如何生成图像的——神经网络逐步从纯噪声图像中去除噪声，并根据文本提示进行条件处理，最终显示清晰图像。 [插图：作者]（图像由神经网络生成）

## 但这些去噪神经网络是如何训练的呢？

在训练过程中，我们从真实图像开始，逐步向其中添加噪声——这叫做正向扩散。这样会生成大量清晰图像及其稍微有噪声的版本。然后，神经网络被训练来通过输入噪声图像，预测去除多少噪声才能恢复更清晰的版本。在文本条件模型中，我们训练注意力层以关注输入的提示进行引导去噪。

![](../Images/53b828311a6b3c5f2220b6612e562002.png)

在训练过程中，我们向清晰图像（左）添加噪声——这叫做正向扩散。神经网络被训练来逆转这一噪声添加过程——这个过程被称为反向扩散。图像由神经网络生成。 [图片来源：作者]

这种迭代方法能够生成高度详细且多样化的图像。你可以观看以下的YouTube视频，里面我会详细讲解文本到图像的过程——比如正向和反向扩散、U-Net、CLIP模型，以及我如何从零开始在Python和Pytorch中实现这些概念。

如果你已经掌握了文本到图像条件扩散的核心概念，那么我们接下来可以讨论视频。

# 时间维度：新的前沿

理论上，我们可以遵循相同的条件噪声去除思路来进行文本到视频的扩散。然而，加入时间这一因素后，带来了几个新的挑战：

1\. **时间一致性：** 确保物体、背景和运动在各帧之间保持一致。

2\. **计算需求：** 生成每秒多个帧，而不是单个图像。

3\. **数据稀缺：** 尽管大型图像-文本数据集很容易获得，但高质量的视频-文本数据集却很稀缺。

![](../Images/f38f959fbc03992030d59eee24f1c063.png)

一些常用的视频-文本数据集 [图片来源：作者]

# 视频扩散的演变

由于缺乏高质量的数据集，文本到视频不能仅依赖监督训练。因此，通常人们还会结合另外两种数据源来训练视频扩散模型——**第一种 — 配对的图像-文本数据**，这种数据更加容易获得，和**第二种 — 未标注的视频数据**，这种数据极为丰富，包含了大量关于世界如何运作的信息。为了解决这些挑战，出现了几种突破性的模型。我们将逐一讨论一些重要的里程碑论文。

***我们即将进入技术细节！*** *如果你觉得接下来的内容比较难，随时可以在阅读下一部分时观看这段辅助视频，它会为你提供一个并排的视觉指导。*

# 视频扩散模型（VDM）— 2022

VDM使用带有因式分解时空卷积层的3D U-Net架构。每个术语在下图中进行了说明。

![](../Images/4c1cbf25a60ae8c5d23f85a354b91817.png)

每个术语的含义（图像来源：作者）

VDM是在图像和视频数据上共同训练的。VDM用3D UNet模型替代了图像扩散模型中的2D UNet。视频被作为一系列2D帧的时间序列输入到模型中。术语“因式分解”基本上意味着空间和时间层被解耦并分别处理。这使得计算更加高效。

**什么是3D-UNet？**

3D U-Net是一种独特的计算机视觉神经网络，首先通过一系列因式分解的时空卷积层对视频进行下采样，基本上是在不同分辨率下提取视频特征。然后，网络有一个上采样路径，将低维特征扩展回原始视频的形状。在上采样过程中，使用跳跃连接来重用在下采样路径中生成的特征。

![](../Images/f4ab5461e5c125ead2ea3d4fc57f1d67.png)

3D因式分解UNet架构 [图像来源：作者]

记住，在任何卷积神经网络中，较早的层总是捕捉图像局部区域的详细信息，而后面的层则通过访问更大的区域来捕捉全局级别的模式——因此，通过使用跳跃连接，U-Net将局部细节与全局特征结合起来，成为一个超棒的特征学习和去噪网络。

VDM是在配对的图像-文本和视频-文本数据集上共同训练的。虽然它是一个很好的概念验证，但对于今天的标准而言，VDM生成的视频分辨率相当低。

[你可以在这里阅读更多关于VDM的信息。](https://arxiv.org/abs/2204.03458)

# Make-A-Video（Meta AI）— 2022

Meta AI的Make-A-Video大胆地提出了一个观点：**我们不一定需要标注过的视频数据来训练视频扩散模型**。什么？没错，你没看错。

## 向图像扩散模型添加时间层

Make A Video首先训练一个常规的文本到图像扩散模型，就像Dall-E或Stable Diffusion那样，使用配对的图像-文本数据。接下来，使用无监督学习对未标注的视频数据进行训练，以教授模型时间关系。网络的附加层使用一种称为遮掩时空解码的技术进行训练，在该技术中，网络通过处理可见帧来生成缺失的帧。请注意，这个流程中不需要标注的视频数据（尽管进一步的视频-文本微调可以作为附加的第三步），因为模型通过配对的文本-图像数据和原始未标注的视频数据来学习时空关系。

![](../Images/e21bb7d54a232d244773c170f6a86265.png)

Make-A-Video概述 [作者图示]

上述模型输出的视频是64x64，包含16帧。然后，通过使用称为时间超分辨率（TSR）和空间超分辨率（SSR）单独的神经网络，在时间和像素轴上对视频进行上采样（在现有帧之间插入新帧以增加每秒帧数（fps）），以及对视频的单帧进行超分辨率处理，提升分辨率。经过这些步骤后，Make-A-Video输出的是256x256分辨率、包含76帧的视频。

[你可以在这里了解更多关于Make-A-Video的信息。](https://makeavideo.studio/)

# Imagen Video（谷歌）— 2022

Imagen视频采用七个模型的级联来进行视频生成和增强。该过程从一个基础视频生成模型开始，创建低分辨率的视频片段。接着是一系列超分辨率模型——三个SSR（空间超分辨率）模型用于空间放大，三个TSR（时间超分辨率）模型用于时间放大。这种级联方法使得Imagen Video能够生成高质量、高分辨率的视频，并且具有令人印象深刻的时间一致性。

![](../Images/c0093e958c4063d6ceef8a65b91cf14d.png)

Imagen工作流程 [来源：Imagen论文：[https://imagen.research.google/video/paper.pdf](https://imagen.research.google/video/paper.pdf)]

# VideoLDM（NVIDIA）— 2023

类似于Nvidia的VideoLDM这样的模型尝试通过使用潜在扩散建模来解决时间一致性问题。首先，他们训练一个潜在扩散图像生成器。基本思路是训练一个变分自编码器（VAE）。VAE由一个编码器网络组成，可以将输入帧压缩成低维潜在空间，另一个解码器网络则可以将其重建回原始图像。扩散过程完全在这个低维空间中进行，而不是在完整的像素空间中，从而使得计算效率更高，并且在语义上更强大。

![](../Images/c1abd5b3867bb017921333a2f112b69b.png)

一个典型的自编码器。输入帧被单独下采样到低维压缩潜在空间。然后，解码器网络学习从这个低分辨率空间中重建图像。[图片由作者提供]

## 什么是潜在扩散模型？

扩散模型完全在低维潜在空间中进行训练，也就是说，扩散模型学习去噪的是低维潜在空间中的图像，而不是全分辨率的帧。这就是为什么我们称其为**潜在扩散模型**。最终的潜在空间输出将通过VAE解码器转换回像素空间。

VAE的解码器通过在其空间层之间添加新的时间层进行增强。这些时间层是在视频数据上微调的，使得VAE能够从由图像扩散模型生成的潜在向量中生成时间一致且无闪烁的视频。通过冻结解码器的空间层，并添加新的可训练时间层，这些时间层基于先前生成的帧进行条件化。

![](../Images/70f142e0ee2a11c20ea619134925aac1.png)

VAE解码器通过加入时间信息进行微调，以便从由潜在扩散模型（LDM）生成的潜在向量中生成一致的视频[来源: 视频LDM论文 [https://arxiv.org/abs/2304.08818](https://arxiv.org/abs/2304.08818)]

[你可以在这里了解更多关于视频LDM的信息。](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)

# **SORA（OpenAI）** — 2024

虽然视频LDM将视频的单独帧压缩以训练LDM，SORA则在空间和时间上同时压缩视频。最近的论文如[CogVideoX已证明，3D因果变分自编码器（VAE）在压缩视频方面表现出色，使得扩散训练在计算上更加高效](https://arxiv.org/abs/2408.06072)，并能够生成无闪烁且一致的视频。

![](../Images/b452aef4a5df2bd864c12e1f7d45ee90.png)

3D VAE通过空间和时间压缩视频，以生成视频数据的压缩4D表示。[图片由作者提供]

## 用于扩散的Transformers

扩散网络使用的是Transformer模型，而不是更传统的UNet模型。当然，transformer需要输入数据以序列的形式呈现。这就是为什么压缩的视频编码被展平为一个补丁序列。请注意，每个补丁及其在序列中的位置代表了原始视频的一个时空特征。

![](../Images/1d3a48f0370c3333302727a6b1917995.png)

OpenAI SORA 视频预处理 [来源: OpenAI (https://openai.com/index/sora/)] （许可: 免费）

有人推测OpenAI已经收集了一个相当大的视频-文本标注数据集，他们正在利用这些数据集训练条件性视频生成模型。

结合下面列出的所有优势，再加上一些OpenAI可能永远不会透露的巧妙技巧，SORA有望成为视频生成AI模型的一大飞跃。

1.  大规模视频-文本标注数据集 + 图像-文本数据和未标注数据的预训练技术

1.  变换器的通用架构

1.  巨大的计算投资（感谢微软）

1.  潜在扩散建模（Latent Diffusion Modeling）的表征能力。

# 接下来会发生什么

人工智能的未来是容易预测的。**在2024年，数据 + 计算 = 智能**。大型企业将投入计算资源来训练大型扩散变换器（diffusion transformers）。它们将雇佣注释员标注高质量的视频-文本数据。大规模的视频-文本数据集可能已经存在于封闭源领域（看着你，OpenAI），并且它们可能会在接下来的2-3年内成为开源，特别是随着近期AI视频理解的进展。尚待观察的是，未来巨大的计算和财务投资是否能够单独解决视频生成问题，或者是否需要研究界进一步的架构和算法进展？

## 链接

作者的YouTube频道: [https://www.youtube.com/@avb_fj](https://www.youtube.com/@avb_fj)

关于这个话题的视频: [https://youtu.be/KRTEOkYftUY](https://youtu.be/KRTEOkYftUY)

15步从零到英雄：条件图像扩散教程: [https://youtu.be/w8YQc](https://youtu.be/w8YQcEd77)

## 论文与文章

视频扩散模型: [https://arxiv.org/abs/2204.03458](https://arxiv.org/abs/2204.03458)

Imagen: [https://imagen.research.google/video/](https://imagen.research.google/video/)

制作视频: [https://makeavideo.studio/](https://makeavideo.studio/)

视频LDM: [https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html](https://research.nvidia.com/labs/toronto-ai/VideoLDM/index.html)

CogVideoX: [https://arxiv.org/abs/2408.06072](https://arxiv.org/abs/2408.06072)

OpenAI SORA 文章: [https://openai.com/index/sora/](https://openai.com/index/sora/)

扩散变换器: [https://arxiv.org/abs/2212.09748](https://arxiv.org/abs/2212.09748)

有用的文章: [https://lilianweng.github.io/posts/2024-04-12-diffusion-video/](https://lilianweng.github.io/posts/2024-04-12-diffusion-video/)
