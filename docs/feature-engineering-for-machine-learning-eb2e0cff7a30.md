# 机器学习的特征工程

> 原文：[https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15](https://towardsdatascience.com/feature-engineering-for-machine-learning-eb2e0cff7a30?source=collection_archive---------0-----------------------#2024-05-15)

## 使算法发挥其魔力

[](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Sumit Makashir](../Images/cdd2f21bb80c8491a2c7ff1d8e7641d7.png)](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------) [Sumit Makashir](https://medium.com/@sumit.makashir?source=post_page---byline--eb2e0cff7a30--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--eb2e0cff7a30--------------------------------) ·阅读时间 14 分钟 ·2024年5月15日

--

![](../Images/f8d7adb9f87912f90b8c93bfa78d588f.png)

图片由 [Mourizal Zativa](https://unsplash.com/@mourimoto?utm_source=medium&utm_medium=referral) 提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

你一定听过“垃圾进，垃圾出”这句话。这个说法在训练机器学习模型时确实适用。如果我们用无关的数据来训练机器学习模型，即便是最好的机器学习算法也无济于事。相反，即使是一个简单的机器学习算法，使用经过良好工程化的有意义特征也能取得出色的表现。那么，如何创建这些能最大化模型性能的有意义特征呢？答案就是特征工程。在处理传统的机器学习算法时，特征工程尤为重要，例如回归、决策树、支持向量机等，这些算法都需要数值型输入。然而，创建这些数值输入不仅仅是数据技能的问题。它是一个需要创造力和领域知识的过程，既有艺术性，也有科学性。

广义来说，我们可以将特征工程分为两个部分：1）创建新特征和 2）处理这些特征，使它们能够与所考虑的机器学习算法协同工作并达到最佳效果。在本文中，我们将讨论针对横截面结构化非NLP数据集的特征工程的这两个组成部分。

# 新特征创建

收集原始数据可能非常疲惫，完成这项任务后，我们可能会太累，无法再投入更多时间和精力去创建额外的特征。但这正是我们必须抵制直接进入模型训练的诱惑的时刻。我向你保证，这一切都是值得的！在这个时刻，我们应该停下来问问自己：“如果我根据我的领域知识手动进行预测，哪些特征会帮助我做得更好？”问这个问题可能会开启创造新有意义特征的可能性，而这些特征是我们的模型可能错过的。一旦我们考虑了可以从中受益的附加特征，我们可以利用下面的技术从原始数据中创建新特征。

## 1\. 聚合

顾名思义，这项技术帮助我们将多个数据点结合起来，创建一个更全面的视图。我们通常会对连续的数值数据进行聚合，使用像计数、求和、平均值、最小值、最大值、百分位数、标准差和变异系数等标准函数。每个函数可以捕捉到不同的信息元素，最佳的函数使用取决于具体的使用场景。通常，我们可以在与问题相关的特定时间或事件窗口上应用聚合。

让我们以预测给定信用卡交易是否为欺诈的例子为例。在这个用例中，我们无疑可以使用交易特定的特征，但除了这些特征，我们还可以受益于创建聚合的客户层面特征，例如：

1.  客户在过去五年内成为欺诈受害者的次数：曾多次成为欺诈受害者的客户可能更容易再次成为欺诈受害者。因此，使用这种聚合的客户层面视图可以提供正确的预测信号。

1.  最近五次交易金额的中位数：通常，当信用卡被盗用时，欺诈者可能会尝试进行多次低价值交易来测试卡片。现在，单次低价值交易是非常常见的，可能并不意味着欺诈，但如果我们看到很多这样的交易在短时间内发生，可能意味着信用卡被盗用。在这种情况下，我们可以考虑创建一个聚合特征，考虑最近几次交易金额。

![](../Images/2f61f46fc17ae81c7dd1eb63560f5a87.png)

*上图显示了单独的交易金额，我们可以看到单次低价值交易并不罕见，且不一定表示欺诈。然而，多个连续的低价值交易则是欺诈的迹象。下图显示了最近五次交易金额的滚动中位数，只有在存在多个连续低价值交易的模式时，才会返回较低的值。在这种情况下，底部的聚合视图使得能够利用交易金额这一特征区分合法的低价值交易和欺诈性的低价值交易。*

## 2\. 差异和比率

在许多类型的问题中，按一定模式发生的变化是预测或异常检测的宝贵信号。差异和比率是表示数值特征变化的有效技术。就像聚合一样，我们也可以在该问题的背景下，应用这些技术到有意义的时间窗口中。

示例：

1.  过去1小时新商户交易百分比与过去30天新商户交易百分比的差异：在短时间内大量的新商户交易可能本身就表明存在欺诈风险，但当我们看到这种行为与客户历史行为相比发生变化时，它就成为一个更为明显的信号。

1.  当前交易日交易量与过去30天中位数日交易量的比率：当信用卡被盗用时，它可能在短时间内发生许多交易，这些交易可能与过去的信用卡使用情况不符。当前交易日交易量与过去30天中位数日交易量的比率显著较高，可能表明存在欺诈使用模式。

![](../Images/82369c87d105c06505f29b5302e6ba88.png)

*从上表中可以看出，仅仅依赖某一天的高交易量本身可能无法指示异常交易行为。相反，基于比率的特征可以促进客户当前交易行为与其过去交易行为的比较，从而更有效地捕捉异常。*

## 3\. 年龄编码

我们可以使用年龄计算技术，通过计算两个时间戳或日期之间的差异，将日期或时间戳特征转换为数值特征。如果特征值的任期可以作为预测的有价值信号，我们还可以使用此技术将某些非数值特征转换为有意义的数值特征。

示例：

1.  自信用卡最后使用以来的天数：长期未使用的信用卡发生突然交易，可能与高欺诈风险相关。我们可以通过计算信用卡最后一次使用日期与当前交易日期之间的时间差，来计算该特征。

1.  自客户设备首次使用以来的天数：如果我们看到来自新设备的交易，它很可能比来自客户已使用较长时间的设备的交易更具风险。我们可以创建一个特征，表示设备的“年龄”，即客户首次使用该设备的日期与当前交易日期之间的天数差异。

![](../Images/71d88b4eab7d2f4737ad1ca5c1d78be0.png)

*上面的表格展示了年龄编码的一个例子。在这里，我们创建了一个新的数值特征“自设备首次使用以来的天数”，即客户设备首次使用日期与当前交易日期之间的天数差异*

## 4\. 指标编码

指示符或布尔特征具有二进制值 {1, 0} 或 {True, False}。指示符特征非常常见，用于表示各种类型的二进制信息。在某些情况下，我们可能已经拥有这种数字形式的二进制特征，而在其他情况下，它们可能是非数字值。为了将非数字二进制特征用于模型训练，我们只需将其映射为数字值。

除了这些常见的指示符特征的应用，我们还可以利用指示符编码作为表示非数字数据点之间比较的工具。这一特性使其特别强大，因为它为我们提供了一种衡量非数字特征变化的方法。

示例：

1.  最近登录事件的验证失败：最近的登录验证失败可能与更高的欺诈交易风险相关。在这种情况下，原始数据可能对该特征有Yes或No值，我们所要做的就是将这些值映射为1或0。

1.  从上次交易的国家位置变化：国家位置变化可能表示信用卡被盗用。在这种情况下，创建一个表示“国家位置变化”的指示符特征，将捕捉到这个国家变化的信息。

![](../Images/f1663fa1ace57e847f6776f692712200.png)

*上表展示了指示符编码的示例。这里我们通过比较客户当前交易的国家位置与其上次交易的国家位置，创建了一个新的数值特征“与上次交易的国家变化”*

## 5\. 独热编码

如果我们的特征数据是分类形式的，无论是数字型还是非数字型，都可以应用这种技术。数字分类形式是指包含非连续或非度量数据的数字数据，例如地理区域代码、商店ID和其他类似数据。独热编码技术可以将这些特征转换为一组指示符特征，我们可以用它们来训练机器学习模型。对分类特征应用独热编码时，将为该分类变量中的每个类别创建一个新的二进制特征。由于新特征的数量随着类别数量的增加而增加，因此这种技术适用于类别数量较少的特征，尤其是在数据集较小的情况下。经验法则之一建议，如果每个类别至少有十条记录，则可以应用此技术。

示例：

1.  交易购买类别：某些类型的购买类别可能与更高的欺诈风险相关。由于购买类别名称是文本数据，我们可以应用独热编码技术，将此特征转换为一组数值指示符特征。如果有十个不同的购买类别名称，独热编码将创建十个新的指示符特征，每个购买类别名称对应一个。

1.  设备类型：在线交易可能通过几种不同类型的设备进行，比如iPhone、Android手机、Windows PC和Mac。其中一些设备更容易受到恶意软件的攻击或更容易被欺诈者访问，因此可能与更高的欺诈风险相关。为了以数字形式包含设备类型信息，我们可以对设备类型应用独热编码，这将为每种设备类型创建一个新的指示符特征。

![](../Images/4694e7b64fa89c65bd31fafcc7e7cbea.png)

*上面的表格展示了独热编码的一个示例。在这里，我们通过对非数值类别特征“设备类型”应用独热编码技术，创建了一组新的数值指示符特征。*

## 6\. 目标编码

这种技术应用于与独热编码相同类型的特征，但相较于独热编码，它有一些优点和缺点。当类别数量较高（高基数）时，使用独热编码会不必要地增加特征数量，这可能导致模型过拟合。在这种情况下，目标编码可以作为一种有效的技术，前提是我们正在处理一个监督学习问题。这是一种将每个类别值映射到该类别的目标期望值的技术。如果处理的是具有连续目标的回归问题，该计算将类别映射到该类别的平均目标值。如果是具有二元目标的分类问题，目标编码将类别映射到该类别的正事件概率。与独热编码不同，这种技术的优势在于不会增加特征的数量。这种技术的一个缺点是，它只能应用于监督学习问题。应用这种技术还可能使模型容易过拟合，特别是当某些类别的观察值较少时。

示例：

1.  商户名称：针对特定商户的交易可能表明存在欺诈行为。可能有成千上万的商户，每个商户的欺诈交易风险不同。对包含商户名称的特征应用独热编码可能会引入成千上万的新特征，这并不理想。在这种情况下，目标编码可以帮助捕捉商户的欺诈风险信息，而不会增加特征的数量。

1.  交易邮政编码：与商户类似，不同邮政编码的交易可能代表不同的欺诈风险等级。尽管邮政编码具有数值，但它们不是连续的测量变量，不应直接用于模型中。相反，我们可以通过应用像目标编码这样的技术，结合与每个邮政编码相关的欺诈风险信息。

![](../Images/0065d17c51a0909826cadd8581ddda06.png)

*上面的表格展示了目标编码的示例。这里我们通过对非数值类别特征“商户名称”应用目标编码技术，创建了一个新的数值特征“商户名称目标编码”。顾名思义，这种技术依赖目标值来计算新特征的值。*

一旦我们从原始数据中创建了新特征，下一步就是对这些特征进行处理，以实现最佳的模型表现。我们通过特征处理来完成这一步，具体内容将在下一节讨论。

# 特征处理

特征处理指的是一系列数据处理步骤，旨在确保机器学习模型能够按预期拟合数据。虽然使用某些机器学习算法时，这些处理步骤是必须的，但有些步骤则确保我们能够在特征与所选机器学习算法之间达到良好的配合。在本节中，我们将讨论一些常见的特征处理步骤及其必要性。

## 1. 异常值处理

一些机器学习算法，特别是参数化算法（如回归模型），会受到异常值的严重影响。这些机器学习算法会试图调整以适应异常值，从而严重影响模型参数，损害整体性能。为了处理异常值，我们必须首先识别它们。我们可以通过应用一些经验法则来检测特定特征的异常值，例如，值的绝对值大于均值加三倍标准差，或值超出最接近的须状线值（最近四分位数值加上1.5倍四分位间距值）。一旦我们在特定特征中识别出异常值，就可以使用以下一些方法来处理异常值：

1.  删除：我们可以删除至少包含一个异常值的观测值。然而，如果我们的数据在不同特征中包含过多的异常值，我们可能会丢失大量的观测值。

1.  替代：我们可以用给定特征的均值、中位数或众数等替代异常值。

1.  特征变换或标准化：我们可以使用对数变换或特征标准化（如在缩放中描述的）来减少异常值的幅度。

1.  上限和下限处理：我们可以将超出某一值的异常值替换为该值，例如，将所有超过99百分位的值替换为99百分位值，将所有低于1百分位的值替换为1百分位值。

![](../Images/4d505d099b871415e69e9060312a7ab3.png)

*上图展示了两种常用的单变量异常值检测技术。我们可以看到，这两种技术可能会得出不同的异常值集合。如果数据呈正态分布，应该使用均值+3标准差技术。基于箱型图须状线的技术更为通用，适用于任何分布的数据。*

![](../Images/bcdee5dea005570784f30a20f32234e9.png)

请注意，虽然有一些技术可以用来检测多变量离群值（即相对于多个特征的离群值），但它们通常更为复杂，并且在机器学习模型训练中一般不会带来太大价值。还要注意，当使用大多数非参数机器学习模型（如支持向量机和基于树的算法，如决策树、随机森林和XGBoost）时，离群值通常不需要特别关注。

## 2\. 缺失值处理

缺失数据在现实世界的数据集中非常常见。大多数传统的机器学习算法（除了少数如XGBoost）不允许训练数据集中存在缺失值。因此，修复缺失值是机器学习建模中的常规任务之一。有几种技术可以用来处理缺失值；然而，在实现任何技术之前，理解缺失数据的原因非常重要，或者至少要知道数据是否是随机缺失的。如果数据不是随机缺失的，意味着某些子群体更容易出现缺失数据，那么为这些数据进行插补可能会很困难，尤其是当可用数据很少或没有数据时。如果数据是随机缺失的，我们可以使用一些常见的缺失值处理技术，如下所述。它们都有优缺点，最终我们需要决定哪种方法最适合我们的使用场景。

1.  删除：我们可以删除至少有一个缺失特征值的观察值。然而，如果我们的数据在不同特征上有太多缺失值，我们可能会丢失许多观察值。

1.  丢弃：如果某个特征有大量缺失值，我们可以选择丢弃该特征。

1.  用均值替代：我们可以使用给定特征的均值、中位数或众数来替代缺失值。这种方法简单易行，但可能并不适用于所有类型的观察值。例如，高欺诈风险的交易可能有不同的平均交易金额与低欺诈风险的交易金额，而使用整体均值来替代缺失的高欺诈风险交易金额可能不是一个好的选择。

1.  最大似然法、多重插补法、K最近邻：这些是更复杂的方法，它们考虑了数据集中与其他特征的关系，通常能提供比整体均值更准确的估计。然而，实现这些方法需要额外的建模或算法实现。

![](../Images/dfa716d05fced03c443b5c248ec93c3b.png)

*上表展示了常用缺失值处理技术的应用。*

## 3\. 缩放

在机器学习模型中，我们使用的特征通常具有不同的范围。如果我们在没有缩放的情况下使用它们，绝对值较大的特征会主导预测结果。相反，为了让每个特征都有公平的机会参与预测结果，我们必须将所有特征置于相同的尺度上。两种最常见的缩放技术是：

1.  归一化：该缩放技术将特征值限制在0和1之间。要应用归一化，我们需要减去特征的最小值并将其除以该特征的范围（即最大值与最小值之间的差）。如果某些特征具有明显的偏斜或少量极端离群值，归一化可能不是一个好的技术。

1.  标准化：该技术将特征数据的分布转换为标准正态分布。我们可以通过减去均值并除以标准差来实现此技术。如果特征存在明显偏斜或极端离群值，通常更倾向于使用该技术。

请注意，基于树的算法，如决策树、随机森林、XGBoost等，可以处理未缩放的数据，并且在使用这些算法时无需进行缩放。

![](../Images/2e78e38b053d64d7bf335c511c90a372.png)

*上表展示了两种常用特征缩放技术的应用。*

![](../Images/1689d07794096770edf83472e1017396.png)

*上图展示了原始、归一化和标准化特征值之间的尺度差异。正如我们所见，缩放不会影响数据分布的形状。*

## 4\. 降维

今天，我们拥有大量数据，并且可以构建一个庞大的特征集来训练我们的模型。对于大多数算法来说，更多的特征是有利的，因为它提供了更多的选项来提高模型的性能。然而，这并非对所有算法都适用。基于距离度量的算法会受到维度灾难的影响——随着特征数量大幅增加，两个观测值之间的距离值变得毫无意义。因此，为了使用依赖于距离度量的算法，我们应确保不使用过多的特征。如果我们的数据集包含大量特征，并且我们不知道应该保留哪些特征，丢弃哪些特征，我们可以使用主成分分析（PCA）等技术。PCA将旧特征集转换为一组新特征。它通过创建新的特征，使得具有最高特征值的特征捕获了大部分来自旧特征的信息。然后我们可以只保留前几个新特征，丢弃剩余的特征。

其他统计技术，如关联分析和特征选择算法，可以用于监督学习问题中，以减少特征的数量。然而，它们通常无法像PCA那样在相同特征数量下捕获相同级别的信息。

![](../Images/adebdb0ef1a8824460bf25635cb1c92b.png)

*上面的表格展示了PCA特征降维的应用。如我们所见，前面三个特征捕捉了原始数据集中超过87%的信息。在这种情况下，我们可以选择省略两个特征（f4和f5），以损失<13%的信息。要保留的特征数量和要淘汰的特征数量将根据不同的问题和各种因素而有所不同。*

## 5\. 转换为正态分布

这一步是个例外，因为它只适用于目标数据，而不适用于特征数据。此外，大多数机器学习算法对目标的分布没有任何限制，但某些算法如线性回归，要求目标数据呈正态分布。线性回归假设所有数据点的误差值是对称的，并且集中在零附近（就像正态分布的形状），而正态分布的目标变量确保这个假设得到满足。我们可以通过绘制直方图来了解目标数据的分布。像Shapiro-Wilk检验这样的统计测试通过检验这个假设来判断数据的正态性。如果我们的目标数据不是正态分布的，我们可以尝试各种变换，例如对数变换、平方变换、平方根变换等，检查哪些变换能够使目标分布变为正态分布。还有一种Box-Cox变换，它会尝试多个参数值，我们可以选择最能将目标分布转化为正态分布的参数值。

![](../Images/d4f451ce85484433af5cd6e6b2c59556.png)

*上面的图像展示了原始目标数据的三种变换。在这个特定的案例中，我们可以看到对数变换是最有效的，它将原始数据分布转换为正态分布。*

*注意：虽然我们可以按照任何顺序实施特征处理步骤，但必须充分考虑它们的应用顺序。例如，使用均值替代进行缺失值处理可以在或在异常值检测之前或之后进行。然而，用于替代的均值可能会有所不同，具体取决于我们是在异常值处理之前还是之后进行缺失值处理。本文中概述的特征处理顺序按照它们对后续处理步骤可能产生的影响的顺序进行处理。因此，遵循此顺序通常应对解决大多数问题有效。*

# 结论

如介绍中所提到的，特征工程是机器学习的一个维度，它使我们能够在极大程度上控制模型的性能。为了充分利用特征工程的潜力，我们在本文中学习了各种技术，这些技术可以帮助我们创建新的特征并处理它们，使其在机器学习模型中最优化地工作。无论你选择使用本文中的哪些特征工程原则和技术，重要的信息是，要理解机器学习不仅仅是让算法去发现模式。更重要的是，通过提供算法所需的数据，我们能够使算法更有效地完成它的工作。

*除非另有说明，所有图片均由作者提供。*
