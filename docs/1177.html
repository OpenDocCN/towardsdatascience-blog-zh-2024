<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Phi-3 and the Beginning of Highly Performant iPhone LLMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Phi-3 and the Beginning of Highly Performant iPhone LLMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09">https://towardsdatascience.com/phi-3-and-the-beginning-of-highly-performant-iphone-models-d413d8ea0714?source=collection_archive---------10-----------------------#2024-05-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="6157" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">This blog post will go into the findings of the Phi-3 paper, as well as some of the implications of models like Phi-3 being released</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Matthew Gunton" class="l ep by dd de cx" src="../Images/6f5a9530ad5252aa3f2fae87b3f272b1.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*F8sHS2ai6w95qbGIZ9qM_g.png"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@mgunton7?source=post_page---byline--d413d8ea0714--------------------------------" rel="noopener follow">Matthew Gunton</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d413d8ea0714--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">8 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">May 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/5ff51da9a59316cb83b1b0cbf42f1a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXTpBh8HLPqqbpsgYDfLig.jpeg"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Image by Author — generated by Stable Diffusion 2.1</figcaption></figure><p id="025c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Readers of my prior work may remember <a class="af nx" href="https://medium.com/@mgunton7/the-impact-of-better-data-on-llms-46153ba26795" rel="noopener">when I covered “Textbooks are all you need”</a>, a paper by Microsoft showing how quality data can have an outsize impact on model performance. The findings there directly refuted the belief that models had to be enormous to be capable. The researchers behind that paper have continued their work and published something I find incredibly exciting.</p><p id="f8c0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The title of this paper explains perhaps the biggest finding: <a class="af nx" href="https://arxiv.org/pdf/2404.14219" rel="noopener ugc nofollow" target="_blank">“Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone”.</a></p><p id="0dd1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Let’s dive into what the authors changed from the Phi-2 model, how they trained it, and how it works on your iPhone.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="6f2d" class="og oh fq bf oi oj ok gq ol om on gt oo op oq or os ot ou ov ow ox oy oz pa pb bk">Key Terminology</h1><p id="16dd" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">There are a few key concepts to know before we dive into the architecture. If you know these already, feel free to skip to the next section.</p><p id="bc75" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">A model’s <strong class="nd fr">parameters</strong> refer to the number of weights and biases that the model learns during training. If you have 1 billion parameters, then you have 1 billion weights and biases that determine the model’s performance. The more parameters you have the more complex your neural network can be. A <strong class="nd fr">head </strong>refers to the number of key, value, and query vectors the self-attention mechanism in a Transformer has. <strong class="nd fr">Layers</strong> refers to the number of neural segments that exist within the neural network of the Transformer, with hidden dimensions being the number of neurons within a typical hidden layer.</p><p id="0990" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Tokenizer </strong>is the software piece that will convert your input text into an embedding that the transformer will then work with. <strong class="nd fr">Vocabulary size </strong>refers to the number of unique tokens that the model is trained on. The <strong class="nd fr">block structure </strong>of a transformer is how we refer to the combination of layers, heads, activation functions, tokenizer and layer normalizations that would be chosen for a specific model.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj ph"><img src="../Images/4d7eeae821cb0ddfd874752561dc84f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NUuf4kjCcjqBzOcPcLmHXQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2 from <a class="af nx" href="https://arxiv.org/pdf/2305.13245" rel="noopener ugc nofollow" target="_blank">“GQA: Training Generalized Multi-Query Transformer Models from<br/>Multi-Head Checkpoints”</a></figcaption></figure><p id="aee6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk"><strong class="nd fr">Grouped-Query Attention (GQA) </strong>is a way that we optimize multi-head attention to reduce the computational overhead during training and inference. As you can see from the image below, GQA takes the middle-ground approach — rather than pairing 1 value and 1 key to 1 query, we take a 1:1:M approach, with the many being smaller than the entire body of queries. This is done to still get the training cost benefits from Multi-Query Attention (MQA), while minimizing the performance degradation that we see follow that.</p><h1 id="3a9c" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Phi 3 Architecture</h1><p id="2ff6" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Let’s begin with the architecture behind this model. The researchers released 3 different decoder only models, <em class="pn">phi-3-mini, phi-3-small, </em>and<em class="pn"> phi-3-medium, </em>with different hyperparameters for each.</p><ul class=""><li id="647e" class="nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw po pp pq bk"><em class="pn">phi-3-mini</em><br/>- 3.8 billion parameters<br/>- 32 heads<br/>- 32 layers<br/>- 3072 hidden dimensions<br/>- 4k token default context length<br/>- 32064 vocabulary size<br/>- weights stored as bfloat16<br/>- trained on 3.3 Trillion Tokens</li><li id="2e18" class="nb nc fq nd b go pr nf ng gr ps ni nj nk pt nm nn no pu nq nr ns pv nu nv nw po pp pq bk"><em class="pn">phi-3-small</em><br/>- 7 billion parameters<br/>- 32 heads<br/>- 32 layers<br/>- 4096 hidden dimensions<br/>- 8k token default context length<br/>- 100352 vocabulary size<br/>- weights stored as bfloat16<br/>- trained on 4.8 Trillion Tokens</li><li id="baac" class="nb nc fq nd b go pr nf ng gr ps ni nj nk pt nm nn no pu nq nr ns pv nu nv nw po pp pq bk"><em class="pn">phi-3-medium</em><br/>- 14 billion parameters<br/>- 40 heads<br/>- 40 layers<br/>- 3072 hidden dimensions<br/>- trained on 4.8 Trillion Tokens</li></ul><p id="0f0a" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Going into some of the differences here, the <em class="pn">phi-3-mini</em> model was trained using typical mutli-head attention. While not called out in the paper, my suspicion is that because the model is roughly half the size of the other two, the training costs associated with multi-head were not objectionable. Naturally when they scaled up for <em class="pn">phi-3-small</em>, they went with grouped query attention, with 4 queries connected to 1 key.</p><p id="a3f0" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Moreover, they kept <em class="pn">phi-3-mini’s</em> block structure as close to the LLaMa-2 structure as they could. The goal here was to allow the open-source community to continue their research on LLaMa-2 with Phi-3. This makes sense as a way to further understand the power of that block structure.</p><p id="650e" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">However, <em class="pn">phi-3-small</em> did NOT use LLaMa’s block structure, opting to use the <code class="cx pw px py pz b">tiktoken</code> tokenizer, with alternate layers of dense attention and a new blocksparse attention. Additionally, they added in 10% multilingual data to the training dataset for these models.</p><h1 id="9bc9" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Training and Data Optimal Mixes</h1><p id="9fb0" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Similar to Phi-2, the researchers invested majorly in quality data. They used the similar “educational value” paradigm they had used before when generating data to train the model on, opting to use significantly more data than last time. They created their data in 2 phases.</p><p id="04f6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Phase-1 involved finding web data that they found was of high “educational value” to the user. The goal here is to give general knowledge to the model. Phase-2 then takes a subset of the Phase-1 data and generates data that would teach the model how to logically reason or attain specific skills.</p><p id="58f4" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The challenge here was to ensure the mix of data from each corpus was appropriate for the scale of the model being trained (ie <em class="pn">phi-3-small </em>vs<em class="pn"> phi-3-mini</em>). This is the idea behind a “data optimal” regime, where the data you are giving to the LLM to train with gives it the best ability for its block structure. Put differently, if you think that data is a key distinguisher for training a good LLM, then finding the right combination of skills to show the model via your data can be just as key as finding good data. The researchers highlighted that they wanted the model to have stronger reasoning than knowledge abilities, resulting in their choosing more data from the Phase-2 corpus than from the Phase-1.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/cf5ca4fbc23b7eca6141f59bd4f951e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NljE38a6Pssx8KwRbRfRPw.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 2 <a class="af nx" href="https://arxiv.org/pdf/2404.14219" rel="noopener ugc nofollow" target="_blank">from the paper</a> highlighting a potential relationship for data optimality</figcaption></figure><p id="8829" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Interestingly, when they were training <em class="pn">phi-3-medium</em> with roughly the same data mixture as they trained <em class="pn">phi-3-small</em>, they noticed that the improvements from 7B parameters to 14B were far more limited than from 3.8B to 7B. The authors suspect this is not a limitation of the block structure, but instead of the data mixture they used to train <em class="pn">phi-3-medium</em>.</p><h1 id="cfcb" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Post-Training</h1><p id="c33c" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">The team used both Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) to improve the model post-training. Those interested in a <a class="af nx" href="https://medium.com/towards-data-science/understanding-the-implications-of-direct-preference-optimization-a4bbd2d85841" rel="noopener">deep dive on DPO can check out my blog post here</a>. Supervised Fine Tuning is a type of transfer learning where we use a custom dataset to improve the LLM’s capabilities on that dataset. The authors used SFT to improve the model’s ability across diverse domains like math, coding, reasoning, and safety. They then used DPO for their chat optimization to guide it away from responses they wanted to avoid and towards ideal responses.</p><p id="8b67" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It’s in this stage that the authors expanded the context window of <em class="pn">phi-3-mini </em>from 4k tokens to 128k tokens. The methodology they used to do this is called Long Rope. The authors claim that the performance is consistent between the 2 context types, which is a big deal given the enormous increase in context length. If there is sufficient interest, I will do a separate blog post on the findings within that paper.</p><h1 id="7798" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Quantization for Phone Usage</h1><p id="16a6" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">Even though these models are small, to get these models to run on your phone still requires some further minimization. Typically the weights for a LLM is stored as float; for example, Phi-3’s original weights were <code class="cx pw px py pz b">bfloat16</code>, meaning each weight takes up 16 bits in memory. While 16 bits may seem trivial, when you take into account there are on the order of 10⁹ parameters in the model, you realize how quickly each additional bit adds up.</p><p id="46e6" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">To get around this, the authors condensed the weights from 16 bits to 4 bits. The basic idea is to reduce the number of bits required to store each number. For a conceptual example, the number 2.71828 could be condensed to 2.72. While this is a lossy operation, it still captures a good portion of the information while taking significantly less storage.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qb"><img src="../Images/8016e9ac83116d37a28ad1fbdccf115b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGW2Ida_5S-ajK5-Osu65Q.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 1 <a class="af nx" href="https://arxiv.org/pdf/2404.14219" rel="noopener ugc nofollow" target="_blank">from the paper</a></figcaption></figure><p id="924c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The authors ran the quantized piece on an iPhone with the A16 chip and found it could generate up to 12 tokens per second. For comparison, an M1 MacBook running LLaMa-2 Quantized 4 bit runs at roughly 107 tokens per second. The fastest token generation I’ve seen (Groq) generated tokens at a rate of 853.35 Tokens per second. Given this is just the beginning, it’s remarkable how fast we are able to see tokens generated on an iPhone with this model. It seems likely the speed of inference will only increase.</p><h1 id="154c" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Pairing Phi-3 with Search</h1><p id="3f02" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">One limitation with a small model is it has fewer places it can store information within its network. As a result, we see that Phi-3 does not perform as well as models like LLaMa-2 on tasks that require wide scopes of knowledge.</p><p id="9637" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">The authors suggest that by pairing Phi-3 with a search engine the model’s abilities will significantly improve. If this is the case, that makes me think Retrieval Augmented Generation (RAG) is likely here to stay, becoming a critical part of helping small models be just as performant as larger ones.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qc"><img src="../Images/bcd70a3a3381870eb2ac75afc9028abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aP3KT4W6g8mkGgTrPbmdaA.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Figure 4 <a class="af nx" href="https://arxiv.org/pdf/2404.14219" rel="noopener ugc nofollow" target="_blank">from the paper</a> highlighting how search can improve Phi-3 performance</figcaption></figure><h1 id="7401" class="og oh fq bf oi oj pi gq ol om pj gt oo op pk or os ot pl ov ow ox pm oz pa pb bk">Conclusion</h1><p id="8d51" class="pw-post-body-paragraph nb nc fq nd b go pc nf ng gr pd ni nj nk pe nm nn no pf nq nr ns pg nu nv nw fj bk">In closing, we are seeing the beginning of highly performant smaller models. While training these models still relies to a large degree on performant hardware, inferencing them is increasingly becoming democratized. This introduces a few interesting phenomena.</p><p id="9d6c" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">First, models that can run locally can be almost fully private, allowing users to give these LLMs data that they otherwise may not feel comfortable sending over the internet. This opens the door to more use cases.</p><p id="ec11" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Second, these models will drive mobile hardware to be even more performant. As a consequence, I would expect to see more Systems on Chips (SoC) on high-end smartphones, especially SoCs with shared memory between CPUs and GPUs to maximize the speed of inference. Moreover, the importance of having quality interfaces with this hardware will be paramount. Libraries like MLX for Apple Silicon will likely be required for any new hardware entrants in the consumer hardware space.</p><p id="20a1" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">Third, as this paper shows that high quality data can in many ways outcompete more network complexity in an LLM, the race to not just find but generate high quality data will only increase.</p><p id="3627" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">It is an exciting time to be building.</p></div></div></div><div class="ab cb ny nz oa ob" role="separator"><span class="oc by bm od oe of"/><span class="oc by bm od oe of"/><span class="oc by bm od oe"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b49b" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[1] Abdin, M., et al. <a class="af nx" href="https://arxiv.org/pdf/2404.14219" rel="noopener ugc nofollow" target="_blank">“Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone”</a> (2024), arXiv</p><p id="2f6d" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[2] Ding, Y., et al. <a class="af nx" href="https://arxiv.org/pdf/2402.13753" rel="noopener ugc nofollow" target="_blank">“LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens”</a> (2024), arXiv</p><p id="9f65" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[3] Gerganov, G., et al. <a class="af nx" href="https://github.com/ggerganov/llama.cpp/discussions/4167" rel="noopener ugc nofollow" target="_blank">“Performance of llama.cpp on Apple Silicon M-series”</a> (2023), GitHub</p><p id="3672" class="pw-post-body-paragraph nb nc fq nd b go ne nf ng gr nh ni nj nk nl nm nn no np nq nr ns nt nu nv nw fj bk">[4] Ainslie, J., et al. <a class="af nx" href="https://arxiv.org/pdf/2305.13245" rel="noopener ugc nofollow" target="_blank">“GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints”</a> (2023), arXiv</p></div></div></div></div>    
</body>
</html>