<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>AI Model Optimization on AWS Inferentia and Trainium</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>AI Model Optimization on AWS Inferentia and Trainium</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20">https://towardsdatascience.com/ai-model-optimization-on-aws-inferentia-and-trainium-cfd48e85d5ac?source=collection_archive---------3-----------------------#2024-10-20</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="fc67" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Tips for accelerating ML with AWS Neuron SDK</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--cfd48e85d5ac--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--cfd48e85d5ac--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Oct 20, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/6cf33bb6d494f40d4338317fda1b9fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2Rv2SBFg0AgINmyy"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Photo by <a class="af nc" href="https://unsplash.com/@julientromeur?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">julien Tromeur</a> on <a class="af nc" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="3bda" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We are in a golden age of AI, with cutting-edge models disrupting industries and poised to transform life as we know it. Powering these advancements are increasingly powerful AI accelerators, such as <a class="af nc" href="https://www.nvidia.com/en-eu/data-center/h100/" rel="noopener ugc nofollow" target="_blank">NVIDIA H100 GPUs</a>, <a class="af nc" href="https://cloud.google.com/tpu" rel="noopener ugc nofollow" target="_blank">Google Cloud TPUs</a>, <a class="af nc" href="https://aws.amazon.com/machine-learning/trainium/" rel="noopener ugc nofollow" target="_blank">AWS’s Trainium</a> and <a class="af nc" href="https://aws.amazon.com/machine-learning/inferentia/" rel="noopener ugc nofollow" target="_blank">Inferentia</a> chips, and more. With the growing number of options comes the challenge of <a class="af nc" rel="noopener" target="_blank" href="/instance-selection-for-deep-learning-7463d774cff0">selecting the most optimal platform</a> for our machine learning (ML) workloads — a crucial decision considering the high costs associated with AI computation. Importantly, a comprehensive assessment of each option necessitates ensuring that we are maximizing its utilization to fully leverage its capabilities.</p><p id="9d47" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post, we will review several techniques for optimizing an ML workload on AWS’s custom-built AI chips using the <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Neuron SDK</a>. This continues our ongoing series of posts focused on ML model performance analysis and optimization across various platforms and environments (e.g., see <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a> and <a class="af nc" rel="noopener" target="_blank" href="/training-ai-models-on-cpu-3903adc9f388">here</a>). While our primary focus will be on an ML training workload and AWS Inferentia2, the techniques discussed are also applicable to AWS Trainium. (Recall that although AWS Inferentia is primarily designed as an AI inference chip, we have <a class="af nc" rel="noopener" target="_blank" href="/dl-training-on-aws-inferentia-53e103597a03">previously demonstrated</a> its effectiveness in training tasks as well.)</p><p id="29a0" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Generally speaking, performance optimization is an iterative process that includes a performance analysis step to appropriately identify performance bottlenecks and resource under-utilization (e.g., see <a class="af nc" rel="noopener" target="_blank" href="/cloud-ml-performance-checklist-caa51e798002">here</a>). However, since the techniques we will discuss are general purpose (i.e., they are potentially applicable to any model, regardless of their performance profile), we defer the discussion on <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/torch-neuronx-profiling-dev-guide.html" rel="noopener ugc nofollow" target="_blank">performance analysis with the Neuron SDK</a> to a future post.</p><h2 id="ba12" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Disclaimers</h2><p id="e6f3" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The code we will share is intended for demonstrative purposes only — we make no claims regarding its accuracy, optimality, or robustness. Please do not view this post as a substitute for the official <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">Neuron SDK documentation</a>. Please do not interpret our mention of any platforms, libraries, or optimization techniques as an endorsement for their use. The best options for you will depend greatly on the specifics of your use-case and will require your own in-depth investigation and analysis.</p><p id="c0dd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The experiments described below were run on an <a class="af nc" href="https://aws.amazon.com/ec2/instance-types/inf2/" rel="noopener ugc nofollow" target="_blank">Amazon EC2 inf2.xlarge</a> instance (containing two <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-hardware/neuron-core-v2.html#neuroncores-v2-arch" rel="noopener ugc nofollow" target="_blank">Neuron cores</a> and four vCPUs). We used the most recent version of the <a class="af nc" href="https://aws.amazon.com/releasenotes/aws-deep-learning-ami-neuron-ubuntu-22-04/" rel="noopener ugc nofollow" target="_blank">Deep Learning AMI for Neuron</a> available at the time of this writing, “Deep Learning AMI Neuron (Ubuntu 22.04) 20240927”, with <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/index.html" rel="noopener ugc nofollow" target="_blank">AWS Neuron 2.20</a> and <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/torch-neuronx/introducing-pytorch-2-1.html" rel="noopener ugc nofollow" target="_blank">PyTorch 2.1</a>. See the <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami" rel="noopener ugc nofollow" target="_blank">SDK documentation</a> for more details on setup and installation. Keep in mind that the Neuron SDK is under active development and that the APIs we refer to, as well as the runtime measurements we report, may become outdated by the time you read this. Please be sure to stay up-to-date with the latest SDK and documentation available.</p><h1 id="8c6b" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Toy Model</h1><p id="b983" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">To facilitate our discussion, we introduce the following simple <a class="af nc" href="https://en.wikipedia.org/wiki/Vision_transformer" rel="noopener ugc nofollow" target="_blank">Vision Transformer</a> (ViT)-backed classification model (based on <a class="af nc" href="https://pypi.org/project/timm/" rel="noopener ugc nofollow" target="_blank">timm</a> version 1.0.10):</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="3374" class="pv oa fq ps b bg pw px l py pz">from torch.utils.data import Dataset<br/>import time, os<br/>import torch<br/>import torch_xla.core.xla_model as xm<br/>import torch_xla.distributed.parallel_loader as pl<br/>from timm.models.vision_transformer import VisionTransformer<br/><br/># use random data<br/>class FakeDataset(Dataset):<br/>  def __len__(self):<br/>    return 1000000<br/>  <br/>  def __getitem__(self, index):<br/>    rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>    label = torch.tensor(data=index % 1000, dtype=torch.int64)<br/>    return rand_image, label<br/><br/>def train(batch_size=16, num_workers=0):<br/>  # Initialize XLA process group for torchrun<br/>  import torch_xla.distributed.xla_backend<br/>  torch.distributed.init_process_group('xla')<br/><br/>  # multi-processing: ensure each worker has same initial weights<br/>  torch.manual_seed(0)<br/>  dataset = FakeDataset()<br/>  model = VisionTransformer()<br/><br/>  # load model to XLA device<br/>  device = xm.xla_device()<br/>  model = model.to(device)<br/>  optimizer = torch.optim.Adam(model.parameters())<br/>  data_loader = torch.utils.data.DataLoader(dataset,<br/>                                            batch_size=batch_size,<br/>                                            num_workers=num_workers)<br/><br/>  data_loader = pl.MpDeviceLoader(data_loader, device)<br/>  loss_function = torch.nn.CrossEntropyLoss()<br/>  summ = 0<br/>  count = 0<br/>  t0 = time.perf_counter()<br/><br/>  for step, (inputs, targets) in enumerate(data_loader, start=1):<br/>    optimizer.zero_grad()<br/>    outputs = model(inputs)<br/>    loss = loss_function(outputs, targets)<br/>    loss.backward()<br/>    xm.optimizer_step(optimizer)<br/>    batch_time = time.perf_counter() - t0<br/>    if step &gt; 10:  # skip first steps<br/>      summ += batch_time<br/>      count += 1<br/>    t0 = time.perf_counter()<br/>    if step &gt; 500:<br/>      break<br/>  print(f'average step time: {summ/count}')<br/><br/>if __name__ == '__main__':<br/>  train()<br/><br/># Initialization command:<br/># torchrun --nproc_per_node=2 train.py</span></pre><p id="d26c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Running our baseline model on the two cores of our AWS Inferentia instance, results in a training speed of 251.98 samples per second.</p><p id="51ec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next sections, we will iteratively apply a number of potential optimization techniques and assess their impact on step time performance. While we won’t go into the full details of each method, we will provide references for further reading (e.g., <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a>). Importantly, the list we will present is not all-inclusive — there are many techniques beyond what we will cover. We will organize the methods into three categories: PyTorch optimizations, OpenXLA optimizations, and Neuron-specific optimizations. However, the order of presentation is not binding. In fact, some of the techniques are interdependent — for example, applying the mixed precision optimization may free up enough device memory to enable increasing the batch size.</p><h1 id="0d4e" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">PyTorch Performance Optimizations</h1><p id="d573" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/pytorch-model-performance-analysis-and-optimization-10c3c5822869">here</a>) we have covered the topic of PyTorch model performance analysis and optimization on GPU, extensively. Many of the techniques we discussed are relevant to other AI accelerators. In this section we will revisit few of these techniques and apply them to AWS Inferentia.</p><h2 id="337b" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Multi-process Data Loading</h2><p id="4216" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In <a class="af nc" href="https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading" rel="noopener ugc nofollow" target="_blank">multi process data loading</a> the input data is prepared in one or more dedicated CPU processes rather than in the same process that runs the training step. This allows for overlapping the data loading and training which can increase system utilization and lead to a significant speed-up. The number of processes is controlled by the <em class="qa">num_workers</em> parameter of the <a class="af nc" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">PyTorch DataLoader</a>. In the following block we run our script with <em class="qa">num_workers </em>set to one:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="ede3" class="pv oa fq ps b bg pw px l py pz">train(num_workers=1)</span></pre><p id="7bfe" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This change results in a training speed of 253.56 samples per second for a boost of less than 1%.</p><h2 id="29ed" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Batch Size Optimization</h2><p id="46a4" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Another important hyperparameter that can influence training speed is the training batch size. Often, we have found that increasing the batch size improves system utilization and results in better performance. However, the effects can vary based on the model and platform. In the case of our toy model on AWS Inferentia, we find that running with a batch size of 8 samples per neuron core results in a speed of 265.68 samples per second — roughly 5% faster than a batch size of 16 samples per core.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="bd67" class="pv oa fq ps b bg pw px l py pz">train(batch_size=8, num_workers=1)</span></pre><h2 id="2747" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">PyTorch Automatic Mixed Precision</h2><p id="d2ae" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Another common method for boosting performance is to use lower precision floats such as the 16-bit BFloat16. Importantly, some model components might not be compatible with reduced precision floats. PyTorch’s <a class="af nc" href="https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html" rel="noopener ugc nofollow" target="_blank">Automatic Mixed Precision (AMP)</a> mode attempts to match the most appropriate floating point type to each model operation automatically. Although, the Neuron compiler offers different options for employing <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc" rel="noopener ugc nofollow" target="_blank">mixed precision</a>, it also <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-mixed-precision" rel="noopener ugc nofollow" target="_blank">supports the option of using PyTorch AMP</a>. In the code block below we include the modifications required to use PyTorch AMP.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="fd41" class="pv oa fq ps b bg pw px l py pz">def train(batch_size=16, num_workers=0):<br/>  # Initialize XLA process group for torchrun<br/>  import torch_xla.distributed.xla_backend<br/>  torch.distributed.init_process_group('xla')<br/><br/>  # multi-processing: ensure each worker has same initial weights<br/>  torch.manual_seed(0)<br/>  dataset = FakeDataset()<br/>  model = VisionTransformer()<br/><br/>  # load model to XLA device<br/>  device = xm.xla_device()<br/>  model = model.to(device)<br/>  optimizer = torch.optim.Adam(model.parameters())<br/>  data_loader = torch.utils.data.DataLoader(dataset,<br/>                                            batch_size=batch_size,<br/>                                            num_workers=num_workers)<br/><br/>  data_loader = pl.MpDeviceLoader(data_loader, device)<br/>  loss_function = torch.nn.CrossEntropyLoss()<br/>  summ = 0<br/>  count = 0<br/>  t0 = time.perf_counter()<br/><br/>  for step, (inputs, targets) in enumerate(data_loader, start=1):<br/>    optimizer.zero_grad()<br/><br/>    # use PyTorch AMP<br/>    with torch.autocast(dtype=torch.bfloat16, device_type='cuda'):<br/>      outputs = model(inputs)<br/>      loss = loss_function(outputs, targets)<br/>    loss.backward()<br/>    xm.optimizer_step(optimizer)<br/>    batch_time = time.perf_counter() - t0<br/>    if step &gt; 10:  # skip first steps<br/>      summ += batch_time<br/>      count += 1<br/>    t0 = time.perf_counter()<br/>    if step &gt; 500:<br/>      break<br/>  print(f'average step time: {summ/count}')<br/><br/>if __name__ == '__main__':<br/>  # disable neuron compilar casting<br/>  os.environ["NEURON_CC_FLAGS"] = "--auto-cast=none"<br/>  torch.cuda.is_bf16_supported = lambda: True<br/>  train(batch_size=8, num_workers=1)</span></pre><p id="c93e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant training speed is 196.64 samples per second, about 26% lower than the <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision" rel="noopener ugc nofollow" target="_blank">default mixed precision</a> setting of the Neuron compiler. It’s important to note that while this post focuses on performance, in real-world scenarios, we would also need to evaluate the effect of the mixed precision policy we choose on <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#performance-accuracy-tradeoffs" rel="noopener ugc nofollow" target="_blank">model accuracy</a>.</p><h1 id="3b16" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">OpenXLA Optimizations</h1><p id="7d2c" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">As discussed in a <a class="af nc" rel="noopener" target="_blank" href="/a-first-look-at-aws-trainium-1e0605071970">previous post</a>, Neuron Cores are treated as <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#neuron-xla-device" rel="noopener ugc nofollow" target="_blank">XLA devices</a> and the <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/quick-start/torch-neuron.html" rel="noopener ugc nofollow" target="_blank">torch-neuronx</a> Python package implements the <a class="af nc" href="https://github.com/pytorch/xla/" rel="noopener ugc nofollow" target="_blank">PyTorch/XLA</a> API. Consequently, any optimization opportunities provided by the OpenXLA framework, and specifically those offered by the PyTorch/XLA API, can be leveraged on AWS Inferentia and Trainium. In this section we consider a few of these opportunities.</p><h2 id="bd8a" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">BFloat16 Precision</h2><p id="a7a0" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">OpenXLA supports the option of <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/programming-guide/training/pytorch-neuron-programming-guide.html#automatic-casting-of-float-tensors-to-bfloat16" rel="noopener ugc nofollow" target="_blank">casting all floats to BFloat16</a> via the XLA_USE_BF16 environment variable, as shown in the code block below:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="a038" class="pv oa fq ps b bg pw px l py pz">if __name__ == '__main__':<br/>  os.environ['XLA_USE_BF16'] = '1'<br/>  train(batch_size=8, num_workers=1)</span></pre><p id="54cb" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The resultant training speed is 394.51 samples per second, nearly 50% faster than the speed of the <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#neuronx-cc-training-mixed-precision" rel="noopener ugc nofollow" target="_blank">default mixed precision</a> option.</p><h2 id="899e" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Multi-process Device Loading</h2><p id="0bee" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The PyTorch/XLA <a class="af nc" href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html#MpDeviceLoader" rel="noopener ugc nofollow" target="_blank">MpDeviceLoader</a> and its internal <a class="af nc" href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html" rel="noopener ugc nofollow" target="_blank">ParallelLoader</a>, which are responsible for loading input data on to the accelerator, include a number of parameters for controlling the transfer of data from the host to the device. In the code block below we tune <a class="af nc" href="https://github.com/pytorch/xla/blob/v2.1.0/torch_xla/distributed/parallel_loader.py#L86" rel="noopener ugc nofollow" target="_blank"><em class="qa">batches_per_execution</em></a> setting which determines the number of batches copied to the device for each execution cycle of the <a class="af nc" href="https://pytorch.org/xla/master/_modules/torch_xla/distributed/parallel_loader.html" rel="noopener ugc nofollow" target="_blank">ParallelLoader</a>. By increasing this setting, we aim to reduce the overhead of the host-to-device communication:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="bdda" class="pv oa fq ps b bg pw px l py pz">data_loader = torch.utils.data.DataLoader(dataset,<br/>                                          batch_size=batch_size,<br/>                                          num_workers=num_workers<br/>                                          )<br/>data_loader = pl.MpDeviceLoader(data_loader, <br/>                                device, batches_per_execution=10)</span></pre><p id="7876" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a result of this optimization, the training speed increased to 1,027.39 samples per second, representing an additional 260% speed-up.</p><h2 id="7fe7" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Torch Compilation with OpenXLA Backend</h2><p id="4f2d" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In previous posts (e.g., <a class="af nc" rel="noopener" target="_blank" href="/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d">here</a>), we have demonstrated the potential performance gains from using <a class="af nc" href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="noopener ugc nofollow" target="_blank">PyTorch’s graph compilation</a> offering. Although <a class="af nc" href="https://openxla.org/xla" rel="noopener ugc nofollow" target="_blank">OpenXLA</a> includes its own graph creation and Just-In-Time (JIT) compilation mechanisms, <a class="af nc" href="https://pytorch.org/xla/master/torch_compile.html" rel="noopener ugc nofollow" target="_blank">torch.compile</a> can provide additional acceleration by eliminating the need for tracing the model operations at every step. The following code snippet demonstrates the use of the dedicated <em class="qa">openxla</em> backend for compiling the model:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="7c0f" class="pv oa fq ps b bg pw px l py pz">model = model.to(device)<br/>model = torch.compile(backend='openxla')</span></pre><p id="531c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Although torch.compile is currently <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/torch/torch-neuronx/index.html#known-limitations" rel="noopener ugc nofollow" target="_blank">not yet supported</a> by the Neuron SDK, we include its mention in anticipation of its future release.</p><h1 id="d291" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Neuron SDK Optimizations</h1><p id="101f" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">In this section we consider some of the optimization opportunities offered by the AWS Neuron SDK and, more specifically, by the Neuron compiler.</p><h2 id="1fa4" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Mixed Precision</h2><p id="5969" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The Neuron SDK supports a variety of <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/neuronx-cc/neuronx-cc-training-mixed-precision.html#mixed-precision-and-performance-accuracy-tuning-neuronx-cc" rel="noopener ugc nofollow" target="_blank">mixed precision</a> settings. In the code block below we program the compiler to cast all floats to BFloat16 via the <em class="qa">NEURON_CC_FLAGS</em> environment variable.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="5f84" class="pv oa fq ps b bg pw px l py pz">if __name__ == '__main__':<br/>  os.environ["NEURON_CC_FLAGS"] = "--auto-cast all --auto-cast-type bf16"<br/>  train(batch_size=8, num_workers=1)</span></pre><p id="9613" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This results (unsurprisingly) in a similar training speed to the OpenXLA BFloat16 experiment described above.</p><h2 id="16c4" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">FP8</h2><p id="932e" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">One of the unique features of NeuronCoreV2 is its support of the eight-bit floating point type, fp8_e4m3. The code block below demonstrates how to configure the Neuron compiler to automatically cast all floating-point operations to FP8:</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="9846" class="pv oa fq ps b bg pw px l py pz">if __name__ == '__main__':<br/> os.environ["NEURON_CC_FLAGS"] = "--auto-cast all --auto-cast-type fp8_e4m3"<br/> train(batch_size=8, num_workers=1)</span></pre><p id="94fd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While FP8 can accelerate training in some cases, maintaining stable convergence can be more challenging than when using BFloat16 due its reduced precision and dynamic range. Please see our <a class="af nc" rel="noopener" target="_blank" href="/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7">previous post</a> for more on the potential benefits and challenges of FP8 training.</p><p id="c4fc" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the case of our model, using FP8 actually harms runtime performance compared to BFloat16, reducing the training speed to 940.36 samples per second.</p><h2 id="0ef0" class="nz oa fq bf ob oc od oe of og oh oi oj nm ok ol om nq on oo op nu oq or os ot bk">Compiler Optimizations</h2><p id="535a" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">The <a class="af nc" href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/compiler/neuronx-cc/api-reference-guide/neuron-compiler-cli-reference-guide.html" rel="noopener ugc nofollow" target="_blank">Neuron compiler</a> includes a number of controls for optimizing the runtime performance of the compiled graph. Two key settings are <em class="qa">model-type</em> and <em class="qa">opt-level</em>. The <em class="qa">model-type </em>setting applies optimizations tailored to specific model architectures, such as transformers, while the <em class="qa">opt-level </em>setting allows for balancing compilation time against runtime performance. In the code block below, we program the <em class="qa">model-type</em> setting to <em class="qa">tranformer</em> and the <em class="qa">opt-level</em> setting to the highest performance option. We further specify the <em class="qa">target</em> runtime device, <em class="qa">inf2</em>, to ensure that the model is optimized for the target device.</p><pre class="mm mn mo mp mq pr ps pt bp pu bb bk"><span id="6214" class="pv oa fq ps b bg pw px l py pz">if __name__ == '__main__':<br/>  os.environ['XLA_USE_BF16'] = '1'<br/>  os.environ["NEURON_CC_FLAGS"] = "--model-type transformer " \<br/>                                  "--optlevel 3" \<br/>                                  " --target inf2"<br/>  train(batch_size=8, num_workers=1)</span></pre><p id="a37d" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The above configuration resulted in a training speed of 1093.25 samples per second, amounting to a modest 6% improvement.</p><h1 id="ed8d" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Results</h1><p id="312c" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">We summarize the results of our experiments in the table below. Keep in mind that the effect of each of the optimization methods we discussed will depend greatly on the model and the runtime environment.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qb"><img src="../Images/fd373857d4f25e1c26fe23f03a8ce4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efMoE_0TpTUVD3EtQR5cww.png"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Experiment Results (by Author)</figcaption></figure><p id="2a68" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The techniques we employed resulted in a 435% performance boost compared to our baseline experiment. It is likely that additional acceleration could be achieved by revisiting and fine-tuning some of the methods we discussed, or by applying other optimization techniques not covered in this post.</p><p id="170b" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Our goal has been to demonstrate some of the available optimization strategies and demonstrate their potential impact on runtime performance. However, in a real-world scenario, we would need to assess the manner in which each of these optimizations impact our model convergence. In some cases, adjustments to the model configuration may be necessary to ensure optimal performance without sacrificing accuracy. Additionally, using a performance profiler to identify bottlenecks and measure system resource utilization is essential for guiding and informing our optimization activities.</p><h1 id="8ff8" class="oz oa fq bf ob pa pb gq of pc pd gt oj pe pf pg ph pi pj pk pl pm pn po pp pq bk">Summary</h1><p id="08d4" class="pw-post-body-paragraph nd ne fq nf b go ou nh ni gr ov nk nl nm ow no np nq ox ns nt nu oy nw nx ny fj bk">Nowadays, we are fortunate to have a wide variety of systems on which to run our ML workloads. No matter which platform we choose, our goal is to maximize its capabilities. In this post, we focused on AWS Inferentia and reviewed several techniques for accelerating ML workloads running on it. Be sure to check out our <a class="af nc" href="https://chaimrand.medium.com/" rel="noopener">other posts</a> for more optimization strategies across various AI accelerators.</p></div></div></div></div>    
</body>
</html>