["```py\nclass TraditionalAttention(nn.Module):\n    def __init__(self, d_k):\n        super(TraditionalAttention, self).__init__()\n        self.d_k = d_k\n\n    def forward(self, Q, K, V):\n        Z = torch.sqrt(torch.tensor(self.d_k, device=Q.device, dtype=torch.float32))\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / Z\n        attention_weights = F.softmax(scores, dim=-1)\n        output = torch.matmul(attention_weights, V)\n        return output\n```", "```py\nclass LinearAttention(nn.Module):\n    def __init__(self):\n        super(LinearAttention, self).__init__()\n        self.eps = 1e-6\n\n    def elu_feature_map(self, x):\n        return F.elu(x) + 1\n\n    def forward(self, Q, K, V):\n        Q = self.elu_feature_map(Q)\n        K = self.elu_feature_map(K)\n        KV = torch.einsum(\"nsd,nsd->ns\", K, V)\n        # Compute the normalizer\n        Z = 1/(torch.einsum(\"nld,nd->nl\", Q, K.sum(dim=1))+self.eps)\n        # Finally compute and return the new values\n        V = torch.einsum(\"nld,ns,nl->nd\", Q, KV, Z)\n        return V.contiguous()\n```"]