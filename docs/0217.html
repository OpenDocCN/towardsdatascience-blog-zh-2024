<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Optimizing Instance Type Selection for AI Development in Cloud Spot Markets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Optimizing Instance Type Selection for AI Development in Cloud Spot Markets</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22">https://towardsdatascience.com/optimizing-instance-type-selection-for-ai-development-in-cloud-spot-markets-a6e94804e8f3?source=collection_archive---------9-----------------------#2024-01-22</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1d29" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Instance Selection for Deep Learning — Part 2</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Chaim Rand" class="l ep by dd de cx" src="../Images/c52659c389f167ad5d6dc139940e7955.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*u4pzP95sl2wOlLhWKFgczg.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://chaimrand.medium.com/?source=post_page---byline--a6e94804e8f3--------------------------------" rel="noopener follow">Chaim Rand</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--a6e94804e8f3--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Jan 22, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj la lb ab q ee lc ld" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="le"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lf k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lg an ao ap id lh li lj" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep lk cn"><div class="l ae"><div class="ab cb"><div class="ll lm ln lo lp lq ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lg an ao ap id lr ls ld lt lu lv lw lx s ly lz ma mb mc md me u mf mg mh"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj mk"><img src="../Images/c5c97dca0d381a425ea2a55b7f74c2a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VlgMC_E3ruOci979"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Photo by <a class="af nb" href="https://unsplash.com/@mikeenerio?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Mike Enerio</a> on <a class="af nb" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="26dc" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">This post was written in collaboration with <a class="af nb" href="https://www.linkedin.com/in/tomerberkovich/" rel="noopener ugc nofollow" target="_blank">Tomer Berkovich</a>, <a class="af nb" href="https://www.linkedin.com/in/yitzhak-levi-49a217201/" rel="noopener ugc nofollow" target="_blank">Yitzhak Levi</a>, and <a class="af nb" href="https://www.linkedin.com/in/maxrabin/" rel="noopener ugc nofollow" target="_blank">Max Rabin</a>.</p><p id="3716" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Appropriate instance selection for machine learning (ML) workloads is an important decision with potentially significant implications on the speed and cost of development. In a <a class="af nb" rel="noopener" target="_blank" href="/instance-selection-for-deep-learning-7463d774cff0">previous post</a> we expanded on this process, proposed a metric for making this important decision, and highlighted some of the many factors you should take into consideration. In this post we will demonstrate the opportunity for reducing AI model training costs by taking <a class="af nb" href="https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&amp;cards.sort-order=asc" rel="noopener ugc nofollow" target="_blank">Spot Instance</a> availability into account when making your cloud-based instance selection decision.</p><h1 id="21dd" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Reducing Costs Using Spot Instances</h1><p id="0514" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">One of the most significant opportunities for cost savings in the cloud is to take advantage of low cost <a class="af nb" href="https://aws.amazon.com/ec2/spot/?cards.sort-by=item.additionalFields.startDateTime&amp;cards.sort-order=asc" rel="noopener ugc nofollow" target="_blank">Amazon EC2 Spot Instances</a>. Spot instances are discounted compute engines from surplus cloud service capacity. In exchange for the discounted price, AWS maintains the right to preempt the instance with little to no warning. Consequently, the relevance of Spot instance utilization is limited to workloads that are fault tolerant. Fortunately, through effective use of <a class="af nb" href="https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html" rel="noopener ugc nofollow" target="_blank">model checkpointing</a> ML training workloads can be designed to be fault tolerant and to take advantage of the Spot instance offering. In fact, Amazon SageMaker, AWS’s managed service for developing ML, makes it easy to train on Spot instances by <a class="af nb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html" rel="noopener ugc nofollow" target="_blank">managing the end-to-end Spot life-cycle</a> for you.</p><h1 id="19c0" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">The Challenge of Anticipating Spot Instance Capacity</h1><p id="2f06" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Unfortunately, <em class="oz">Spot instance capacity</em>, which measures the availability of Spot instances for use, is subject to constant fluctuations and can be very difficult to predict. Amazon offers partial assistance in assessing the <em class="oz">Spot instance capacity </em>of an instance type of choice via its <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html" rel="noopener ugc nofollow" target="_blank">Spot placement score</a> (SPS) feature which indicates the likelihood that a Spot request will succeed in a given <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" rel="noopener ugc nofollow" target="_blank">region or availability zone (AZ)</a>. This is especially helpful when you have the freedom to choose to train your model in one of several different locations. However, the SPS feature offers no guarantees.</p><p id="eb78" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">When you choose to train a model on one or more Spot instances, you are taking the risk that your instance type of choice does not have any Spot capacity (i.e., your training job will not start), or worse, that you will enter an iterative cycle in which your training repeatedly runs for just a small number of training steps and is stopped before you have made any meaningful progress — which can tally up your training costs without any return.</p><p id="1f8e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Over the past couple of years, the challenges of spot instance utilization have been particularly acute when it comes to multi-GPU <a class="af nb" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank">EC2</a> instance types such as <a class="af nb" href="https://aws.amazon.com/ec2/instance-types/g5/" rel="noopener ugc nofollow" target="_blank">g5.12xlarge</a> and <a class="af nb" href="https://aws.amazon.com/ec2/instance-types/p4/" rel="noopener ugc nofollow" target="_blank">p4d.24xlarge</a>. A huge increase in demand for powerful training accelerators (driven in part by advances in the field of Generative AI) combined with disruptions in the global supply chain, have made it virtually impossible to reliably depend on multi-GPU Spot instances for ML training. The natural fallback is to use the more costly <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html" rel="noopener ugc nofollow" target="_blank">On-Demand</a> (OD) or <a class="af nb" href="https://aws.amazon.com/ec2/pricing/reserved-instances/" rel="noopener ugc nofollow" target="_blank">reserved instances</a>. However, in our <a class="af nb" rel="noopener" target="_blank" href="/instance-selection-for-deep-learning-7463d774cff0">previous post</a> we emphasized the value of considering many different alternatives for your choice of instance type. In this post we will demonstrate the potential gains of replacing multi-GPU On Demand instances with multiple single-GPU Spot instances.</p><p id="924e" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Although our demonstration will use Amazon Web Services, similar conclusions can be reached on alternative cloud service platforms (CSPs). Please do not interpret our choice of CSP or services as an endorsement. The best option for you will depend on the unique details of your project. Furthermore, please take into consideration the possibility that the type of cost savings we will demonstrate will not reproduce in the case of your project and/or that the solution we propose will not be applicable (e.g., for some reason beyond the scope of this post). Be sure to conduct a detailed evaluation of the relevance and efficacy of the proposal before adapting it to your use case.</p><h1 id="2f33" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">When Multiple Single-GPU Instances are Better than a Single Multi-GPU Instance</h1><p id="6d45" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Nowadays, training AI models on multiple GPU devices in parallel — a process called <em class="oz">distributed training </em>— is commonplace. Setting aside instance pricing, when you have the choice between an instance type with multiple GPUs and multiple instance types with the same type of single GPUs, you would typically choose the multi-GPU instance. Distributed training typically requires a considerable amount of data communication (e.g., gradient sharing) between the GPUs. The proximity of the GPUs on a single instance is bound to facilitate higher network bandwidth and lower latency. Moreover, some multi-GPU instances include dedicated GPU-to-GPU inter-connections that can further accelerate the communication (e.g., <a class="af nb" href="https://www.nvidia.com/en-eu/data-center/nvlink/" rel="noopener ugc nofollow" target="_blank">NVLink</a> on <a class="af nb" href="https://aws.amazon.com/ec2/instance-types/p4/" rel="noopener ugc nofollow" target="_blank">p4d.24xlarge</a>). However, when Spot capacity is limited to single GPU instances, the option of training on multiple single GPU instances at a much lower cost becomes more compelling. At the very least, it warrants evaluation of its opportunity for cost-savings.</p><h1 id="2204" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Optimizing Data Communication Between Multiple EC2 Instances</h1><p id="f76f" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">When distributed training runs on multiple instances, the GPUs communicate with one another via the network between the host machines. To optimize the speed of training and reduce the likelihood and/or impact of a network bottleneck, we need to ensure minimal network latency and maximal data throughput. These can be affected by a number of factors.</p><h2 id="add1" class="pa nz fq bf oa pb pc pd od pe pf pg og nl ph pi pj np pk pl pm nt pn po pp pq bk">Instance Collocation</h2><p id="b2a0" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Network latency can be greatly impacted by the relative locations of the EC2 instances. Ideally, when we request multiple cloud-based instances we would like them to all be collocated on the same physical rack. In practice, without appropriate configuration, they may not even be in the same city. In our demonstration below we will use a <a class="af nb" href="https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_VpcConfig.html" rel="noopener ugc nofollow" target="_blank">VPC Config</a> object to program an Amazon SageMaker training job to use a single subnet of an <a class="af nb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html" rel="noopener ugc nofollow" target="_blank">Amazon Virtual Private Cloud (VPC)</a>. This technique will ensure that all the requested training instances will be in the same <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" rel="noopener ugc nofollow" target="_blank">availability zone (AZ)</a>. However, collocation in the same AZ, may not suffice. Furthermore, the method we described involves choosing a subnet associated with one specific AZ (e.g., the one with the highest <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-placement-score.html" rel="noopener ugc nofollow" target="_blank">Spot placement score</a>). A preferred API would fulfill the request in any AZ that has sufficient capacity.</p><p id="c4d3" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">A better way to control the placement of our instances is to launch them inside a <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" rel="noopener ugc nofollow" target="_blank">placement group</a>, specifically a <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">cluster placement group</a>. Not only will this guarantee that all of the instances will be in the same <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html" rel="noopener ugc nofollow" target="_blank">AZ</a>, but it will also place them on “the same high-bisection bandwidth segment of the network” so as to maximize the performance of the network traffic between them. However, as of the time of this writing SageMaker does <em class="oz">not</em> provide the option to specify a <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html" rel="noopener ugc nofollow" target="_blank">placement group</a>. To take advantage of placement groups we would need to use an alternative training service solution (as we will demonstrate below).</p><h2 id="02f4" class="pa nz fq bf oa pb pc pd od pe pf pg og nl ph pi pj np pk pl pm nt pn po pp pq bk">EC2 Network <strong class="al">B</strong>andwidth Constraints</h2><p id="b73e" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">Be sure to take into account the maximal <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-network-bandwidth.html" rel="noopener ugc nofollow" target="_blank">network bandwidth</a> supported by the EC2 instances that you choose. Note, in particular, that the network bandwidths associated with single-GPU machines are often documented as being “up to” a certain number of Gbps. Make sure to understand what that means and how it can impact the speed of training over time.</p><p id="45e2" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Keep in mind that the GPU-to-GPU data communication (e.g., gradient sharing) might need to share the limited network bandwidth with other data flowing through the network such as training samples being streamed into the training instances or training artifacts being uploaded to persistent storage. Consider ways of reducing the payload of each of the categories of data to minimize the likelihood of a network bottleneck.</p><h2 id="5034" class="pa nz fq bf oa pb pc pd od pe pf pg og nl ph pi pj np pk pl pm nt pn po pp pq bk">Elastic Fabric Adapter (EFA)</h2><p id="498b" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">A growing number of EC2 instance types support <a class="af nb" href="https://aws.amazon.com/hpc/efa/" rel="noopener ugc nofollow" target="_blank">Elastic Fabric Adapter (EFA)</a>, a dedicated network interface for optimizing inter-node communication. Using EFA can have a decisive impact on the runtime performance of your training workload. Note that the bandwidth on the EFA network channel is different than the documented bandwidth of the standard network. As of the time of this writing, detailed documentation of the EFA capabilities is hard to come by and it is usually best to evaluate its impact through trial and error. Consider using an <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types" rel="noopener ugc nofollow" target="_blank">EC2 instance that supports EFA type</a> when relevant.</p><h1 id="8d9a" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Toy Example</h1><p id="2680" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We will now demonstrate the comparative price performance of training on four single-GPU <a class="af nb" href="https://aws.amazon.com/ec2/instance-types/g5/" rel="noopener ugc nofollow" target="_blank">EC2 g5</a> Spot instances (ml.g5.2xlarge and ml.g5.4xlarge) vs. a single four-GPU On-Demand instance (ml.g5.12xlarge). We will use the training script below containing a Vision Transformer (ViT) backed classification model (trained on synthetic data).</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="055e" class="pv nz fq ps b bg pw px l py pz">import os, torch, time<br/>import torch.distributed as dist<br/>from torch.utils.data import Dataset, DataLoader<br/>from torch.cuda.amp import autocast<br/>from torch.nn.parallel import DistributedDataParallel as DDP<br/>from timm.models.vision_transformer import VisionTransformer<br/><br/>batch_size = 128<br/>log_interval = 10<br/><br/># use random data<br/>class FakeDataset(Dataset):<br/>    def __len__(self):<br/>        return 1000000<br/><br/>    def __getitem__(self, index):<br/>        rand_image = torch.randn([3, 224, 224], dtype=torch.float32)<br/>        label = torch.tensor(data=[index % 1000], dtype=torch.int64)<br/>        return rand_image, label<br/><br/>def mp_fn():<br/>    local_rank = int(os.environ['LOCAL_RANK'])<br/>    dist.init_process_group("nccl")<br/>    torch.cuda.set_device(local_rank)<br/><br/>    # model definition<br/>    model = VisionTransformer()<br/>    loss_fn = torch.nn.CrossEntropyLoss()<br/>    model.to(torch.cuda.current_device())<br/>    model = DDP(model)<br/>    optimizer = torch.optim.Adam(params=model.parameters())<br/><br/>    # dataset definition<br/>    num_workers = os.cpu_count()//int(os.environ['LOCAL_WORLD_SIZE'])<br/>    dl = DataLoader(FakeDataset(), batch_size=batch_size, num_workers=num_workers)<br/><br/>    model.train()<br/>    t0 = time.perf_counter()<br/>    for batch_idx, (x, y) in enumerate(dl, start=1):<br/>        optimizer.zero_grad(set_to_none=True)<br/>        x = x.to(torch.cuda.current_device())<br/>        y = torch.squeeze(y.to(torch.cuda.current_device()), -1)<br/>        with autocast(enabled=True, dtype=torch.bfloat16):<br/>            outputs = model(x)<br/>            loss = loss_fn(outputs, y)<br/>        loss.backward()<br/>        optimizer.step()<br/>        if batch_idx % log_interval == 0 and local_rank == 0:<br/>            time_passed = time.perf_counter() - t0<br/>            samples_processed = dist.get_world_size() * batch_size * log_interval<br/>            print(f'{samples_processed / time_passed} samples/second')<br/>            t0 = time.perf_counter()<br/><br/>if __name__ == '__main__':<br/>    mp_fn()</span></pre><p id="e856" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">The code block below demonstrates how we used the <a class="af nb" href="https://sagemaker.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">SageMaker Python package</a> (version 2.203.1) to run our experiments. Note that for the four-instance experiments, we configure the use of a VPC with a single subnet, as explained above.</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="ee09" class="pv nz fq ps b bg pw px l py pz">from sagemaker.pytorch import PyTorch<br/><br/># Toggle flag to switch between multiple single-GPU nodes and<br/># single multi-GPU node<br/>multi_inst = False<br/><br/>inst_count=1<br/>inst_type='ml.g5.12xlarge'<br/>use_spot_instances=False<br/>max_wait=None #max seconds to wait for Spot job to complete<br/>subnets=None<br/>security_group_ids=None<br/><br/>if multi_inst:<br/>    inst_count=4<br/>    inst_type='ml.g5.4xlarge' #  optinally change to ml.g5.2xlarge<br/>    use_spot_instances=True<br/>    max_wait=24*60*60 #24 hours<br/>    # configure vpc settings<br/>    subnets=['&lt;VPC subnet&gt;']<br/>    security_group_ids=['&lt;Security Group&gt;']<br/><br/><br/>estimator = PyTorch(<br/>    role='&lt;sagemaker role&gt;',<br/>    entry_point='train.py',<br/>    source_dir='&lt;path to source dir&gt;',<br/>    instance_type=inst_type,<br/>    instance_count=inst_count,<br/>    framework_version='2.1.0',<br/>    py_version='py310',<br/>    distribution={'torch_distributed': {'enabled': True}},<br/>    subnets=subnets,<br/>    security_group_ids=security_group_ids,<br/>    use_spot_instances=use_spot_instances,<br/>    max_wait=max_wait<br/>)<br/><br/># start job<br/>estimator.fit()</span></pre><p id="3cae" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Note that our code depends on the third-party <a class="af nb" href="https://pypi.org/project/timm/" rel="noopener ugc nofollow" target="_blank"><em class="oz">timm</em></a><em class="oz"> </em>Python package that we point to in a requirements.txt file in the root of the source directory. This assumes that the VPC has been configured to <a class="af nb" href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html" rel="noopener ugc nofollow" target="_blank">enable internet access</a>. Alternatively, you could define a private PyPI server (as described <a class="af nb" href="https://aws.amazon.com/blogs/machine-learning/hosting-a-private-pypi-server-for-amazon-sagemaker-studio-notebooks-in-a-vpc/" rel="noopener ugc nofollow" target="_blank">here</a>), or create a custom image with your third party dependencies preinstalled (as described <a class="af nb" rel="noopener" target="_blank" href="/customizing-your-cloud-based-machine-learning-training-environment-part-2-b65a6cf91812">here</a>).</p><h1 id="7233" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Results</h1><p id="f3ba" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">We summarize the results of our experiment in the table below. The On-Demand prices were taken from the <a class="af nb" href="https://aws.amazon.com/sagemaker/pricing/" rel="noopener ugc nofollow" target="_blank">SageMaker pricing page</a> (as of the time of this writing, January 2024). The Spot saving values were collected from the reported <em class="oz">managed spot training savings </em>of the completed job. Please see the <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances-history.html" rel="noopener ugc nofollow" target="_blank">EC2 Spot pricing documentation</a> to get a sense for how the reported Spot savings are calculated.</p><figure class="ml mm mn mo mp mq mi mj paragraph-image"><div role="button" tabindex="0" class="mr ms ed mt bh mu"><div class="mi mj qa"><img src="../Images/3ad9e651b7ad635f5ad718a84cd2b1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GTS6c7JylvxiHNjcZxFiQ.png"/></div></div><figcaption class="mw mx my mi mj mz na bf b bg z dx">Experiment Results (by Author)</figcaption></figure><p id="67a7" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our results clearly demonstrate the potential for considerable savings when using four single-GPU Spot instances rather than a single four-GPU On Demand instance. They further demonstrate that although the cost of an On Demand g5.4xlarge instance type is higher, the increased CPU power and/or network bandwidth combined with higher Spot savings, resulted in much greater savings.</p><p id="6ee8" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Importantly, keep in mind that the relative performance results can vary considerably based on the details of your job as well the Spot prices at the time that you run your experiments.</p><h1 id="b5db" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Enforcing EC2 Instance Co-location Using a Cluster Placement Group</h1><p id="1f87" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In a <a class="af nb" rel="noopener" target="_blank" href="/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a">previous post</a> we described how to create a customized managed environment on top of an unmanaged service, such as <a class="af nb" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank">Amazon EC2</a>. One of the motivating factors listed there was the desire to have greater control over device placement in a multi-instance setup, e.g., by using a <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">cluster placement group</a>, as discussed above. In this section, we demonstrate the creation of a multi-node setup using a cluster placement group.</p><p id="9877" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Our code assumes the presence of a <a class="af nb" href="https://docs.aws.amazon.com/vpc/latest/userguide/default-vpc.html" rel="noopener ugc nofollow" target="_blank">default VPC</a> as well as the (one-time) creation of a <a class="af nb" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster" rel="noopener ugc nofollow" target="_blank">cluster placement group</a>, demonstrated here using the <a class="af nb" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Python SDK</a> (version 1.34.23):</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="6570" class="pv nz fq ps b bg pw px l py pz">import boto3<br/><br/>ec2 = boto3.client('ec2')<br/>ec2.create_placement_group(<br/>    GroupName='cluster-placement-group',<br/>    Strategy='cluster'<br/>) </span></pre><p id="38db" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">In the code block below we use the <a class="af nb" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank">AWS Python SDK</a> to launch our Spot instances:</p><pre class="ml mm mn mo mp pr ps pt bp pu bb bk"><span id="5178" class="pv nz fq ps b bg pw px l py pz">import boto3<br/><br/>ec2 = boto3.resource('ec2')<br/>instances = ec2.create_instances(<br/>    MaxCount=4,<br/>    MinCount=4,<br/>    ImageId='ami-0240b7264c1c9e6a9', # replace with image of choice<br/>    InstanceType='g5.4xlarge',<br/>    Placement={'GroupName':'cluster-placement-group'},<br/>    InstanceMarketOptions={<br/>        'MarketType': 'spot',<br/>        'SpotOptions': {<br/>            "SpotInstanceType": "one-time",<br/>            "InstanceInterruptionBehavior": "terminate"<br/>        }<br/>    },<br/>)</span></pre><p id="016c" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">Please see our <a class="af nb" rel="noopener" target="_blank" href="/a-simple-solution-for-managing-cloud-based-ml-training-c80a69c6939a">previous post</a> for step-by-step tips on how to extend this to an automated training solution.</p><h1 id="55a9" class="ny nz fq bf oa ob oc gq od oe of gt og oh oi oj ok ol om on oo op oq or os ot bk">Summary</h1><p id="c79d" class="pw-post-body-paragraph nc nd fq ne b go ou ng nh gr ov nj nk nl ow nn no np ox nr ns nt oy nv nw nx fj bk">In this post, we have illustrated how demonstrating flexibility in your choice of training instance type can increase your ability to leverage Spot instance capacity and reduce the overall cost of training.</p><p id="b02b" class="pw-post-body-paragraph nc nd fq ne b go nf ng nh gr ni nj nk nl nm nn no np nq nr ns nt nu nv nw nx fj bk">As the sizes of AI models continue to grow and the costs of AI training accelerators continue to rise, it becomes increasingly important that we explore ways to mitigate training expenses. The technique outlined here is just one among several methods for optimizing cost performance. We encourage you to explore our <a class="af nb" href="https://chaimrand.medium.com/" rel="noopener">previous posts</a> for insights into additional opportunities in this realm.</p></div></div></div></div>    
</body>
</html>