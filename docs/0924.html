<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>How to Make the Most Out of LLM Production Data: Simulated User Feedback</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>How to Make the Most Out of LLM Production Data: Simulated User Feedback</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-the-most-out-of-llm-production-data-simulated-user-feedback-843c444febc7?source=collection_archive---------6-----------------------#2024-04-11">https://towardsdatascience.com/how-to-make-the-most-out-of-llm-production-data-simulated-user-feedback-843c444febc7?source=collection_archive---------6-----------------------#2024-04-11</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="1902" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">A novel approach to use production data to simulate user feedback for testing and evaluating your LLM app</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@pasquale_68605?source=post_page---byline--843c444febc7--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Pasquale Antonante, Ph.D." class="l ep by dd de cx" src="../Images/97733117413b3daeb79886e171ab30c9.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*BM7uaSg73YbY-xT5ZFo_Nw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--843c444febc7--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@pasquale_68605?source=post_page---byline--843c444febc7--------------------------------" rel="noopener follow">Pasquale Antonante, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--843c444febc7--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">9 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Apr 11, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk ml"><img src="../Images/17386b21ff3d693432dc70e815185505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3Ov2EIMg_xfZJ1vavTbPA.jpeg"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Image by author and ChatGPT. “Image of two llamas, one with a thumbs up and another with thumbs down” prompt. ChatGPT, 4, OpenAI, April 10th 2024. <a class="af nc" href="https://chat.openai.com./" rel="noopener ugc nofollow" target="_blank">https://chat.openai.com.</a></figcaption></figure><p id="6972" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">A series of blog posts to share our perspectives on how to evaluate and improve your GenAI application pipelines</p><p id="1bad" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><em class="nz">(written by Pasquale Antonante and Yi Zhang at Relari.ai)</em></p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="b7c7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The world of LLM app development is always on the move — new tricks, models, and apps pop up every week. As tech gets better, what users expect keeps ramping up. Staying ahead in this game is key to making sure it’s the one users keep coming back to.</p><p id="d295" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The problem now becomes: how do you measure performance improvements? When you’re fiddling with prompts, tweaking the temperature, or switching up models, do you ever pause and think, “Will my users actually like this more?”</p><p id="db22" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In this post, we’ll walk through how in-app user feedback from earlier deployments (or internal human evaluation) can be instrumental in quickly shaping future versions of a product. We’ll discuss the limitations of traditional feedback mechanisms and introduce a new technique that allows AI developers to use feedback data directly in offline testing and iterations (before a new deployment), making the development cycle more adaptable and responsive to user preferences.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="69ac" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Understanding the Value of User Feedback</h1><h2 id="7a84" class="pe oj fq bf ok pf pg ph on pi pj pk oq nm pl pm pn nq po pp pq nu pr ps pt pu bk">Why User Feedback Matters and Its Challenges</h2><p id="9fe2" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk">When developing LLM-based applications, we are often faced with a particular problem we want to address,<em class="nz"> e.g.</em>, a specific type of question has a low accuracy. As we experiment with tweaks in prompts, parameters, architecture, etc., we want to evaluate performance of the new pipeline, in particular whether users will like the new version(s) of your AI application. The most straightforward way is to A/B test each change with the end users and collect their feedback data such as thumbs up / down, score rating, or written comments, but practically it is challenging for a few reasons:</p><ol class=""><li id="26f2" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa qb qc bk"><strong class="nf fr">Slow to collect: </strong>Unless your application is already seeing huge volume, you don’t have that much feedback data. Anecdotally, we’ve seen the feedback participation rate in our customers’ AI applications range from &lt;1% (normal) to ~10% (exceptional, often through deliberate UI/UX design to encourage more feedback). As a result, it can take a long time before you get enough feedback data to make a statistically confident judgment of whether a particular change resonated positively or negatively with your users.</li><li id="a497" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qa qb qc bk"><strong class="nf fr">Risk of jeopardizing user relationships: </strong>Testing directly with users is the most effective way to gain insights, but there’s a real risk of damaging your relationship with them if they encounter an unsatisfactory version. Users can be quick to judge, potentially dismissing your application at the first sign of a mistake. Consequently, developers tend to opt for more conservative or less disruptive changes for A/B testing with users, reserving the bolder, more innovative updates for internal testing. This approach allows for experimentation while minimizing the risk of alienating the user base.</li><li id="8117" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qa qb qc bk"><strong class="nf fr">Inconsistent measurement: </strong>With most AI applications being fairly open-ended, it is often difficult to get truly apples-to-apples comparison of feedback data given different users can interact with your product in a different way. As a result, feedback data A/B testing for LLM-based applications tends to be more noisy than those from traditional applications.</li></ol><p id="6417" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next section, we’ll introduce a novel approach that we’ve deployed to multiple customers to help them make the most out of their user feedback data in offline development.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="8889" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">A Novel Approach: Simulate User Feedback</h1><p id="16a5" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk">In response to these challenges in collecting user feedback, we have developed a novel approach to simulate user feedback using a small sample of user (or internally labeled) feedback data. Specifically, we use metric ensembling and conformal prediction to learn user preferences and use them offline during the development phase. At its core, we learn how users weigh different criteria (e.g., tone, conciseness, etc) and leverage conformal prediction to provide predictions to quantify confidence. This method drastically accelerates LLM app development by providing a way to anticipate how users might react to new features or changes before they are fully implemented.</p><p id="735c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">To evaluate its effectiveness, we compared this approach with the more conventional one of using a single LLM call that assesses different aspects of the response to make a judgment. To compare the two alternatives (the proposed approach vs. the single LLM call), we conducted an experiment using the <a class="af nc" href="https://huggingface.co/datasets/llm-blender/Unified-Feedback" rel="noopener ugc nofollow" target="_blank">Unified-Feedback</a> dataset. We used Kendall’s tau, a measure of rank correlation, to compare the rankings produced by our user feedback simulation and the single LLM call approach against the ground truth established by human evaluations. This analysis allows us to assess not only the degree of agreement, but also the order of preference that each method predicts compared to the human rankings.</p><p id="7f76" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk"><strong class="nf fr">Our experiment revealed that the user feedback simulation has a correlation of 93% that significantly exceeded that of the single LLM call approach, which attains roughly 70% correlation. This indicates that, in terms of ranking , the simulated user feedback simulation provides a closer approximation to human judgment.</strong></p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div role="button" tabindex="0" class="ms mt ed mu bh mv"><div class="mj mk qi"><img src="../Images/60384adfaaf39c33f4a4ee9b84534c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2lp3jGAJWg-T7T_"/></div></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Kendall’s tau of the two methods. Higher values indicate a stronger correlation between the ranking produced by the method and the human. Simulated User Feedback (proposed, lighter blue) shows higher agreement with humans when tasked with identifying improvements, suggesting that it more accurately reflects human judgment in identifying and evaluating improvements. Image by the author.</figcaption></figure><p id="0ef5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The reason why the simulated user feedback performs better is twofold:</p><ol class=""><li id="28a9" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qa qb qc bk">it learns from actual user feedback the importance of different criteria, making the approach custom to your use case</li><li id="eb90" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qa qb qc bk">while individual criteria may have appeared in the LLM training set, the complex (and potentially large) set of different criteria likely have not appeared in the training data, making it more difficult for the LLM evaluator to get right.</li></ol><p id="917c" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">While single LLM calls can identify major improvements in the pipeline, they fall short of detecting the more frequent, minor enhancements critical in mature pipelines. Simulated user feedback, however, exhibits a high correlation with human judgment, enabling the detection of these incremental advances.</p><p id="947e" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As a side note, while we could have used the data to fine-tune an LLM, this has the typical drawback of requiring more data and not being as interpretable.</p><p id="0a53" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In the next section, we will walk through an example on how to create your simulated user feedback.</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="42ca" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">How It Works</h1><p id="ecd3" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk">In this section we will show how we can use the open-source library <a class="af nc" href="https://github.com/relari-ai/continuous-eval" rel="noopener ugc nofollow" target="_blank">continuous-eval</a> to create simulated user feedback.</p><p id="a61f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Consider a Q&amp;A chatbot application. After deployment, users begin rating responses with thumbs up or down, indicating a need for performance enhancement. For this example we will use the example named <code class="cx qj qk ql qm b">correctness</code> in continuous-eval:</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="a3f3" class="qq oj fq qm b bg qr qs l qt qu">dataset = Dataset(example_data_downloader("correctness"))<br/><br/># Samples are annotated with "correct", "incorrect" or "refuse-to-answer"<br/># We remove the samples where the LLL refused to answer (i.e., said "I don't know")<br/>dataset.filter(lambda x: x["annotation"] != "refuse-to-answer")<br/>dataset.sample(300)  # Only for this example: randomly sample 300 examples</span></pre><p id="23ee" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">As we mentioned, we want to create some custom criteria. We leverage the <code class="cx qj qk ql qm b">LLMBasedCustomMetric</code> class to define the <em class="nz">Tone</em> and <em class="nz">Conciseness</em> metrics. To do so we need to define the metric and provide a scoring rubric.</p><p id="8ef5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">For the tone:</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="f7ea" class="qq oj fq qm b bg qr qs l qt qu">tone = LLMBasedCustomMetric(<br/>    name="Tone",<br/>    definition="The Tone/Content Issues metric evaluates the appropriateness and accuracy of the tone and content in responses to specific questions. It focuses on ensuring that the tone is professional and suitable for the context, and that the content accurately addresses the question without unnecessary deviations or inaccuracies. This metric is crucial for maintaining a professional image and ensuring clear, direct communication.",<br/>    scoring_rubric="""Use the following rubric to assign a score to the answer based on its tone:<br/>- Score 1: The response is inappropriate or inaccurate, with a tone that is either too informal, overly strong, or not suited to the professional context. The content may be irrelevant, incorrect, or fail to directly address the question posed.<br/>- Score 2: The response is mostly appropriate and accurate but may contain minor tone or content issues. The tone is generally professional but may slip into informality or unnecessary strength in places. The content addresses the question but may include minor inaccuracies or unnecessary details.<br/>- Score 3: The response is appropriate and accurate, with a tone that is professional and suited to the context. The content directly and correctly addresses the question without unnecessary deviations or inaccuracies.""",<br/>    scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),<br/>    model_parameters={"temperature": 0},<br/>)</span></pre><p id="73bd" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">while for conciseness:</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="9304" class="qq oj fq qm b bg qr qs l qt qu">conciseness = LLMBasedCustomMetric(<br/>    name="Conciseness",<br/>    definition="Conciseness in communication refers to the expression of ideas in a clear and straightforward manner, using the fewest possible words without sacrificing clarity or completeness of information. It involves eliminating redundancy, verbosity, and unnecessary details, focusing instead on delivering the essential message efficiently. ",<br/>    scoring_rubric="""Use the following rubric to assign a score to the answer based on its conciseness:<br/>- Score 1: The answer is overly verbose, containing a significant amount of unnecessary information, repetition, or redundant expressions that do not contribute to the understanding of the topic.<br/>- Score 2: The answer includes some unnecessary details or slightly repetitive information, but the excess does not severely hinder understanding.<br/>- Score 3: The answer is clear, direct, and to the point, with no unnecessary words, details, or repetition.""",<br/>    scoring_function=ScoringFunctions.Numeric(min_val=1, max_val=3),<br/>    model_parameters={"temperature": 0},<br/>)</span></pre><p id="69e1" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We use <em class="nz">Tone</em> and <em class="nz">Conciseness</em> together with more standard metrics, in particular we will consider the</p><ul class=""><li id="09bd" class="nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny qv qb qc bk">Answer Correctness (<code class="cx qj qk ql qm b">DeterministicAnswerCorrectens</code> and <code class="cx qj qk ql qm b">LLMBasedAnswerCorrectness)</code></li><li id="0e71" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk">Answer Relevance (<code class="cx qj qk ql qm b">LLMBasedAnswerRelevance</code>)</li><li id="14c7" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk">Style Consistency (<code class="cx qj qk ql qm b">LLMBasedStyleConsistency</code>)</li><li id="8104" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk">Readability (<code class="cx qj qk ql qm b">FleschKincaidReadability</code>)</li></ul><p id="4d83" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next step is to put all the metrics together and specify what field of the dataset should be used to compute the metrics. To do that we can use the <code class="cx qj qk ql qm b">SingleModulePipeline</code></p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="049b" class="qq oj fq qm b bg qr qs l qt qu">pipeline = SingleModulePipeline(<br/>    dataset=dataset,<br/>    eval=[<br/>        DeterministicAnswerCorrectness().use(<br/>            answer=dataset.answer,<br/>            ground_truth_answers=dataset.ground_truths,<br/>        ),<br/>        LLMBasedAnswerCorrectness().use(<br/>            question=dataset.question,<br/>            answer=dataset.answer,<br/>            ground_truth_answers=dataset.ground_truths,<br/>        ),<br/>        LLMBasedAnswerRelevance().use(<br/>            question=dataset.question, answer=dataset.answer<br/>        ),<br/>        LLMBasedStyleConsistency().use(<br/>            answer=dataset.answer, ground_truth_answers=dataset.ground_truths<br/>        ),<br/>        FleschKincaidReadability().use(answer=dataset.answer),<br/>        tone.use(<br/>            question=dataset.question,<br/>            answer=dataset.answer,<br/>            ground_truth_answers=dataset.ground_truths,<br/>        ),<br/>        conciseness.use(<br/>            question=dataset.question,<br/>            answer=dataset.answer,<br/>            ground_truth_answers=dataset.ground_truths,<br/>        ),<br/>    ],<br/>)</span></pre><p id="dcec" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">and run all the metrics using the <code class="cx qj qk ql qm b">EvaluationManager</code></p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="a335" class="qq oj fq qm b bg qr qs l qt qu">eval_manager = EvaluationManager(pipeline)<br/># The dataset already contains the model output so we just set the evaluation results<br/>eval_manager.evaluation.results = dataset.data<br/>eval_manager.run_metrics()  # Note: there is no progress bar, it might take a few minutes</span></pre><p id="9ce7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">The next step is to train simulated user feedback predictor</p><pre class="mm mn mo mp mq qn qm qo bp qp bb bk"><span id="cf10" class="qq oj fq qm b bg qr qs l qt qu">datasplit = DataSplit(<br/>    X=eval_manager.metrics.to_pandas(),<br/>    y=map(lambda x: 1 if x == "correct" else 0, dataset["annotation"]),<br/>    split_ratios=SplitRatios(train=0.6, test=0.2, calibration=0.2),<br/>)<br/><br/># We use the train and calibration sets to train the classifier<br/>predictor = EnsembleMetric(training=datasplit.train, calibration=datasplit.calibration)</span></pre><p id="5b02" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">This simulated user feedback predictor is able to correctly predict the human feedback in the test split 96.67% of the time.</p><p id="61e5" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">We can leverage the proposed approach to better understand what is important to the user. Below is the learned importance of every metric by the simulated user feedback predictor.</p><figure class="mm mn mo mp mq mr mj mk paragraph-image"><div class="mj mk qw"><img src="../Images/6404708dceb38cba2037dde13c071620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DisAnfk1AAm5pGI2VJ1vzQ.png"/></div><figcaption class="mx my mz mj mk na nb bf b bg z dx">Learned importance of every metric by the simulated user feedback predictor. Image by the author.</figcaption></figure><p id="1d2f" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Looking at the plot, we see that <em class="nz">Correctness</em> (including <em class="nz">token overlap</em>, which is another measure for correctness) and <em class="nz">Relevance</em> to the question are the most important predictors of user preference. But the user also weighs <em class="nz">tone</em> and <em class="nz">style consistency</em> into the decision. At the same time, we can see that <em class="nz">conciseness</em> and <em class="nz">readability</em> are not as important. Reviewing this graph provides valuable insight into user preferences, giving a clear indication of what elements are essential and what can be adjusted if compromises need to be made.</p><h1 id="c94e" class="oi oj fq bf ok ol qx gq on oo qy gt oq or qz ot ou ov ra ox oy oz rb pb pc pd bk">Wrapping Up</h1><p id="f0d8" class="pw-post-body-paragraph nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny fj bk">Collecting user feedback is challenging, yet it is the most important information for developers of large language models (LLMs). By simulating user feedback during offline testing, we significantly reduces the time it takes for feedback to travel from the field back to developers, while maintaining positive user relationships.</p><p id="b4e7" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">In practice, our approach has proven to closely mirror actual human responses, outperforming traditional methods that rely on isolated LLM responses. This strategy allows for the incremental improvement of generative AI applications, fostering continuous refinement and greater congruence with what users expect.</p><p id="cf96" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">—</p><p id="5481" class="pw-post-body-paragraph nd ne fq nf b go ng nh ni gr nj nk nl nm nn no np nq nr ns nt nu nv nw nx ny fj bk">Note: We will soon publish a research paper with more details on this methodology. Stay tuned!</p></div></div></div><div class="ab cb oa ob oc od" role="separator"><span class="oe by bm of og oh"/><span class="oe by bm of og oh"/><span class="oe by bm of og"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="69f7" class="oi oj fq bf ok ol om gq on oo op gt oq or os ot ou ov ow ox oy oz pa pb pc pd bk">Coming Next</h1><ul class=""><li id="1b6d" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny qv qb qc bk">Techniques for curating golden dataset</li><li id="dbc7" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk">How to make the most out of your Embedding Model?</li><li id="1983" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk">What data should I use for Fine-Tuning?</li></ul><h1 id="8bca" class="oi oj fq bf ok ol qx gq on oo qy gt oq or qz ot ou ov ra ox oy oz rb pb pc pd bk">Earlier Posts</h1><ul class=""><li id="b02e" class="nd ne fq nf b go pv nh ni gr pw nk nl nm px no np nq py ns nt nu pz nw nx ny qv qb qc bk"><a class="af nc" href="https://medium.com/p/27a472b09893" rel="noopener">Practical Guide to RAG Pipeline Evaluation Part 1: Retrieval</a></li><li id="0e91" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk"><a class="af nc" href="https://medium.com/relari/a-practical-guide-to-rag-evaluation-part-2-generation-c79b1bde0f5d" rel="noopener">Practical Guide to RAG Pipeline Evaluation Part 2: Generation</a></li><li id="3df8" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk"><a class="af nc" href="https://medium.com/relari/how-important-is-a-golden-dataset-for-llm-pipeline-evaluation-4ef6deb14dc5" rel="noopener">How important is a Golden Dataset for LLM pipeline evaluation?</a></li><li id="52ca" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk"><a class="af nc" href="https://medium.com/relari/case-study-reference-free-vs-reference-based-evaluation-of-rag-pipeline-9a49ef49866c" rel="noopener">Case Study: Reference-free vs Reference-based evaluation</a></li><li id="66e0" class="nd ne fq nf b go qd nh ni gr qe nk nl nm qf no np nq qg ns nt nu qh nw nx ny qv qb qc bk"><a class="af nc" href="https://medium.com/relari/how-to-evaluate-complex-genai-apps-a-granular-approach-0ab929d5b3e2" rel="noopener">How to evaluate complex GenAI Apps: a granular approach</a></li></ul></div></div></div></div>    
</body>
</html>