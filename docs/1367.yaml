- en: Long Short Term Memory (LSTM)— Improving RNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）—— 改进 RNNs
- en: 原文：[https://towardsdatascience.com/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?source=collection_archive---------5-----------------------#2024-05-31](https://towardsdatascience.com/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?source=collection_archive---------5-----------------------#2024-05-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?source=collection_archive---------5-----------------------#2024-05-31](https://towardsdatascience.com/long-short-term-memory-lstm-improving-rnns-40323d1c05f8?source=collection_archive---------5-----------------------#2024-05-31)
- en: How state of the art RNNs work
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最先进的 RNNs 如何工作
- en: '[](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page---byline--40323d1c05f8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)
    ·9 min read·May 31, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--40323d1c05f8--------------------------------)
    ·阅读时长 9 分钟 ·2024年5月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f6f8a3bcbebfcdf3bcb18428e3166a5d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6f8a3bcbebfcdf3bcb18428e3166a5d.png)'
- en: ”[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    title=”neural network icons”>Neural network icons created by Freepik — Flaticon.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '"[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)"
    标题为“neural network icons”的图标由 Freepik 创建 — Flaticon。'
- en: In this article, we will introduce Long-Short-Term Memory Networks (LSTMs),
    variants of regular vanilla Recurrent Neural Networks (RNNs) that are better at
    dealing with long-term dependencies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将介绍长短期记忆网络（LSTMs），它们是常规的 vanilla 循环神经网络（RNNs）的变种，在处理长期依赖问题上表现更好。
- en: They use different “gates” that remember or forget certain information that
    they deem necessary or unimportant for prediction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它们使用不同的“门”来记住或忘记它们认为对预测有用或不重要的信息。
- en: LSTMs are the state-of-the-art version of RNNs. They are used extensively within
    the industry and form the backbone of all the fancy large language models (LLMs)
    we see today.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs 是 RNNs 的最先进版本。它们在工业界被广泛使用，并构成了我们今天所见的所有大型语言模型（LLMs）的基础。
- en: We will start by recapping RNNs, explain the vanishing and exploding gradient
    problem, and finally dive into how LSTMs work and are naturally a better model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先回顾 RNNs，解释梯度消失和梯度爆炸问题，然后深入探讨 LSTMs 如何工作以及为何它们是更好的模型。
- en: RNN Overview
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 概述
- en: Recurrent Neural Networks are a spin on the regular feedforward neural network,
    allowing them to handle sequence data like natural language and time series much
    better.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）是常规前馈神经网络的变种，它们能够更好地处理自然语言和时间序列等序列数据。
- en: They do this by having a hidden recurrent neuron that passes in previous inputs
    and outputs to the next layer. Below is an example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通过拥有一个隐藏的循环神经元，将前一个输入和输出传递到下一层，从而实现这一点。下面是一个例子。
