<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>An Intuitive Introduction to Reinforcement Learning, Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>An Intuitive Introduction to Reinforcement Learning, Part I</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06">https://towardsdatascience.com/an-intuitive-introduction-to-reinforcement-learning-part-i-d81512f5e25c?source=collection_archive---------2-----------------------#2024-09-06</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="8713" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Exploring popular reinforcement learning environments, in a beginner-friendly way</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Jesse Xia" class="l ep by dd de cx" src="../Images/a87eeff33bf3d2e8baef1c05c265490c.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*VzHuvQJc8Bf49r9CEmMVew.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@x-jesse?source=post_page---byline--d81512f5e25c--------------------------------" rel="noopener follow">Jesse Xia</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d81512f5e25c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">16 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Sep 6, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">5</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><blockquote class="mj mk ml"><p id="17e9" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This is a guided series on introductory reinforcement learning concepts using the environments from the OpenAI Gymnasium Python package. This first article will cover the high-level concepts necessary to understand and implement Q-learning to solve the “Frozen Lake” environment.</p><p id="9d7b" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Happy learning ❤ !</p></blockquote><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk nl"><img src="../Images/f5b2f2a7a57da627a57c7829545417ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4BXO-soPKt3Rm3Z2f3l9g.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">A smiley lake (Image taken by author, made using OpenAI Gymnasium’s Frozen Lake environment)</figcaption></figure><p id="b7c5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s explore reinforcement learning by comparing it to familiar examples from everyday life.</p><p id="8c0c" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">Card Game</strong> — Imagine playing a card game: When you first learn the game, the rules may be unclear. The cards you play might not be the most optimal and the strategies you use might be imperfect. As you play more and maybe win a few games, you learn what cards to play when and what strategies are better than others. Sometimes it’s better to bluff, but other times you should probably fold; saving a wild card for later use might be better than playing it immediately. Knowing what the optimal course of action is <em class="mo">learned</em> through a combination of <strong class="mp fr"><em class="mo">experience</em></strong> and <strong class="mp fr"><em class="mo">reward</em></strong>. Your experience comes from playing the game and you get rewarded when your strategies work well, perhaps leading to a victory or new high score.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk oc"><img src="../Images/81a5728f83dc5e14862fa7f9643a8329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*uXIClMV2JD5BrURC6HSA9g.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">A game of solitaire (Image taken by author from Google’s solitaire game)</figcaption></figure><p id="f9a4" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">Classical Conditioning — </strong>By ringing a bell before he fed a dog, <a class="af od" href="https://en.wikipedia.org/wiki/Ivan_Pavlov" rel="noopener ugc nofollow" target="_blank">Ivan Pavlov</a> demonstrated the connection between external stimulus and a physiological response. The dog was conditioned to associate the sound of the bell with being fed and thus began to drool at the sound of the bell, even when no food was present. Though not strictly an example of reinforcement learning, through repeated <strong class="mp fr"><em class="mo">experiences</em></strong> where the dog was <strong class="mp fr"><em class="mo">rewarded</em></strong> with food at the sound of the bell, it still learned to associate the two together.</p><p id="87e3" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk"><strong class="mp fr">Feedback Control — </strong>An application of <a class="af od" href="https://en.wikipedia.org/wiki/Control_theory" rel="noopener ugc nofollow" target="_blank">control theory</a> found in engineering disciplines where a system’s behaviour can be adjusted by providing <em class="mo">feedback</em> to a controller. As a subset of feedback control, reinforcement learning requires feedback from our current environment to influence our actions. By providing feedback in the form of <strong class="mp fr"><em class="mo">reward</em></strong>, we can incentivize our agent to pick the optimal course of action.</p><h1 id="c14c" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">The Agent, State, and Environment</h1><p id="9dd2" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk"><strong class="mp fr">Reinforcement learning is a learning process built on the accumulation of past experiences coupled with quantifiable reward. </strong>In each example, we illustrate how our experiences can influence our actions and how <em class="mo">reinforcing</em> a positive association between reward and response could potentially be used to solve certain problems. If we can learn to associate reward with an optimal action, we could derive an algorithm that will select actions that <em class="mo">yield the highest probable reward</em>.</p><p id="184f" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">In reinforcement learning, the “learner” is called the <strong class="mp fr"><em class="mo">agent</em></strong>. The agent interacts with our environment and, through its actions, learns what is considered “good” or “bad” based on the reward it receives.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk oc"><img src="../Images/ed2e3e07453b86fc450c6af054eaf937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*_p8XoPizgb8JHhwo2x4qXw.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">The feedback cycle in reinforcement learning: Agent -&gt; Action -&gt; Environment -&gt; Reward, State (Image by author)</figcaption></figure><p id="60ae" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">To select a course of action, our agent needs some information about our environment, given by the <strong class="mp fr"><em class="mo">state</em></strong>. The state represents current information about the environment, such as position, velocity, time, etc. Our agent does not necessarily know the entirety of the current state. The information available to our agent at any given point in time is referred to as an <em class="mo">observation</em>, which contains some subset of information present in the state. Not all states are fully observable, and some states may require the agent to proceed knowing only a small fraction of what might actually be happening in the environment. Using the observation, our agent must infer what the best possible action might be based on learned experience and attempt to select the action that yields the highest expected reward.</p><p id="ed09" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">After selecting an action, the environment will then respond by providing feedback in the form of an updated state and reward. This reward will help us determine if the action the agent took was optimal or not.</p><h1 id="0171" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Markov Decision Processes (MDPs)</h1><p id="b95e" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">To better represent this problem, we might consider it as a <strong class="mp fr">Markov decision process (MDP)</strong>. A MDP is a <a class="af od" href="https://en.wikipedia.org/wiki/Directed_graph" rel="noopener ugc nofollow" target="_blank">directed graph</a> where each edge in the graph has a non-deterministic property. At each possible state in our graph, we have a set of actions we can choose from, with each action yielding some fixed reward and having some transitional probability of leading to some subsequent state. This means that the same actions are not guaranteed to lead to the same state every time since the transition from one state to another is not only dependent on the action, but the transitional probability as well.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pf"><img src="../Images/08a866a7b383ab3040cf12b65b226f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QndXRIIkKC0xtpRnpT7XVQ.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Representation of a Markov decision process (Image by author)</figcaption></figure><p id="1275" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Randomness in decision models is useful in practical RL, allowing for dynamic environments where the agent lacks full control. Turn-based games like chess require the opponent to make a move before you can go again. If the opponent plays randomly, the future state of the board is never guaranteed, and our agent must play while accounting for a multitude of different probable future states. When the agent takes some action, the next state is dependent on what the opponent plays and is therefore defined by a probability distribution across possible moves for the opponent.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pg"><img src="../Images/862a9422ef75f3ed353ac794d0c7a3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G5eZhpxauo8JGAQqiqpGcA.gif"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Animation showcasing that the state of the chess board is also dependent on what moves the opponent chooses to play (Image by author)</figcaption></figure><p id="bd14" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Our future state is therefore a function of both the probability of the agent selecting some action and the <em class="mo">transitional probability</em> of the opponent selecting some action. In general, we can assume that for any environment, the probability of our agent moving to some subsequent state from our current state is denoted by the joint probability of the agent selecting some action and the transitional probability of moving to that state.</p><h2 id="cff6" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Solving the MDP</h2><p id="d6ca" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">To determine the optimal course of action, we want to provide our agent with lots of experience. Through repeated iterations of our environment, we aim to give the agent enough feedback that it can correctly choose the optimal action most, if not all, of the time. Recall our definition of reinforcement learning: <strong class="mp fr">a learning process built on the accumulation of past experiences coupled with quantifiable reward. </strong>After accumulating some experience, we want to use this experience to better select our future actions.</p><p id="4b39" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We can quantify our experiences by using them to predict the expected reward from future states. As we accumulate more experience, our predictions will become more accurate, converging to the true value after a certain number of iterations. For each reward that we receive, we can use that to update some information about our state, so the next time we encounter this state, we’ll have a better estimate of the reward that we might expect to receive.</p><h1 id="e525" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Frozen Lake Problem</h1><p id="1ae9" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">Let’s consider consider a simple environment where our agent is a small character trying to navigate across a frozen lake, represented as a 2D grid. It can move in four directions: down, up, left, or right. Our goal is to teach it to move from its start position at the top left to an end position located at the bottom right of the map while avoiding the holes in the ice. If our agent manages to successfully reach its destination, we’ll give it a reward of +1. For all other cases, the agent will receive a reward of 0, with the added condition that if it falls into a hole, the exploration will immediately terminate.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk py"><img src="../Images/9324e24bbf8ed38b3e0c708230efb6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*1q4qIDv5frccmGoi1JxEsw.gif"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Frozen lake animation (Image from <a class="af od" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/" rel="noopener ugc nofollow" target="_blank">OpenAI Gymnasium frozen lake documentation</a>)</figcaption></figure><p id="4bf7" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Each state can be denoted by its coordinate position in the grid, with the start position in the top left denoted as the origin (0, 0), and the bottom right ending position denoted as (3, 3).</p><p id="5285" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The most generic solution would be to apply some pathfinding algorithm to find the shortest path to from top left to bottom right while avoiding holes in the ice. However, the probability that the agent can move from one state to another is not deterministic. <strong class="mp fr">Each time the agent tries to move, there is a 66% chance that it will “slip” and move to a random adjacent state. </strong>In other words, there is only a 33% chance of the action the agent chose actually occurring. A traditional pathfinding algorithm cannot handle the introduction of a transitional probability. Therefore, we need an algorithm that can handle stochastic environments, aka reinforcement learning.</p><p id="7ca9" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This problem can easily be represented as a MDP, with each state in our grid having some transitional probability of moving to any adjacent state. To solve our MDP, we need to find the optimal course of action from any given state. Recall that if we can find a way to accurately predict the future rewards from each state, we can greedily choose the best possible path by selecting whichever state yields the <em class="mo">highest expected reward</em>. We will refer to this predicted reward as the <strong class="mp fr"><em class="mo">state-value. </em></strong>More formally, the state-value will define the expected reward gained starting from some state plus an estimate of the expected rewards from all future states thereafter, assuming we act according to the same policy of choosing the highest expected reward. Initially, our agent will have no knowledge of what rewards to expect, so this estimate can be arbitrarily set to 0.</p><p id="9596" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Let’s now define a way for us to select actions for our agent to take: We’ll begin with a table to store our predicted state-value estimates for each state, containing all zeros.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk pz"><img src="../Images/7aadcb5dd5a6661d46ab11d2ff53a477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWvPLQQmn_fATyls3Fmo2w.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Table denoting the estimated state-value for each state in our grid (Image by author)</figcaption></figure><p id="6b4d" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Our goal is to update these state-value estimates as we explore our environment. The more we traverse our environment, the more experience we will have, and the better our estimates will become. As our estimates improve, our state-values will become more accurate, and we will have a better representation of which states yield a higher reward, therefore allowing us to select actions based on which subsequent state has the highest state-value. This will surely work, right?</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qa"><img src="../Images/24245c342efc1c230f26805ac77c6eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sM3ReLvcNBwMSTZft4O8zg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Visual representation of a single branch of our MDP (Image by author)</figcaption></figure><h2 id="8ca4" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">State-value vs. Action-value</h2><p id="2bfe" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">Nope, sorry. One immediate problem that you might notice is that simply selecting the next state based on the highest possible state-value isn’t going to work. When we look at the set of possible next states, we aren’t considering our current action—that is, the action that we will take from our current state to get to the next one. Based on our definition of reinforcement learning, the agent-environment feedback loop always consists of the agent taking some action and the environment responding with both state and reward. If we only look at the state-values for possible next states, we are considering the reward that we would receive starting from those states, which completely ignores the action (and consequent reward) we took to get there. Additionally, trying to select a maximum across the next possible states assumes we can even make it there in the first place. Sometimes, being a little more conservative will help us be more consistent in reaching the end goal; however, this is out of the scope of this article :(.</p><p id="52de" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Instead of evaluating across the set of possible next states, we’d like to directly evaluate our available actions. If our previous state-value function consisted of the expected rewards starting from the next state, we’d like to update this function to now include the reward from taking an action from the current state to get to the next state, plus the expected rewards from there on. We’ll call this new estimate that includes our current action <strong class="mp fr"><em class="mo">action-value.</em></strong></p><p id="eca1" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We can now formally define our state-value and action-value functions based on rewards and transitional probability. We’ll use <a class="af od" href="https://www.statisticshowto.com/probability-and-statistics/expected-value/" rel="noopener ugc nofollow" target="_blank">expected value</a> to represent the relationship between reward and transitional probability. We’ll denote our state-value as <em class="mo">V</em> and our action-value as <em class="mo">Q</em>, based on standard conventions in RL literature.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qb"><img src="../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMCodiIqjs7jW0SXiTfmKg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equations for state- and action-value (Image by author)</figcaption></figure><blockquote class="mj mk ml"><p id="1ae6" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The state-value V of some state s[t] is the expected sum of rewards r[t] at each state starting from s[t] to some future state s[T]; the action-value Q of some state s[t] is the expected sum of rewards r[t] at each state starting by taking an action a[t] to some future state-action pair s[T], a[T].</p></blockquote><p id="4acb" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">This definition is actually not the most accurate or conventional, and we’ll improve on it later. However, it serves as a general idea of what we’re looking for: a quantitative measure of future rewards.</p><p id="d218" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Our state-value function <em class="mo">V </em>is an estimate of the maximum sum of rewards <em class="mo">r </em>we would obtain starting from state <em class="mo">s</em> and continually moving to the states that give the highest reward. Our action-value function is an estimate of the maximum reward we would obtain by taking action from some starting state and continually choosing the optimal actions that yield the highest reward thereafter. In both cases, we choose the optimal action/state to move to based on the expected reward that we would receive and loop this process until we either fall into a hole or reach our goal.</p><h2 id="fadb" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Greedy Policy &amp; Return</h2><p id="2bc0" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">The method by which we choose our actions is called a <strong class="mp fr"><em class="mo">policy</em></strong>. The policy is a function of state—given some state, it will output an action. In this case, since we want to select the next action based on maximizing the rewards, our policy can be defined as a function returning the action that yields the maximum action-value (Q-value) starting from our current state, or an <a class="af od" href="https://en.wikipedia.org/wiki/Arg_max#:~:text=In%20mathematics%2C%20the%20arguments%20of,is%20maximized%20and%20minimized%2C%20respectively." rel="noopener ugc nofollow" target="_blank">argmax</a>. Since we’re always selecting a maximum, we refer to this particular policy as <em class="mo">greedy</em>. We’ll denote our policy as a function of state s: π(s), formally defined as</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qc"><img src="../Images/27e57dd508e4666977ced2945f05ea70.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*nKT_WPC3ba5sbYGpTHfkxA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equation for the policy function = the action that yields the maximum estimated Q-value from some state s (Image by author)</figcaption></figure><p id="704d" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">To simplify our notation, we can also define a substitution for our sum of rewards, which we’ll call <strong class="mp fr"><em class="mo">return, </em></strong>and a substitution for a sequence of states and actions, which we’ll call a <strong class="mp fr"><em class="mo">trajectory</em></strong>. A trajectory, denoted by the Greek letter τ (tau), is denoted as</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qd"><img src="../Images/686a7c274c1f31c017a4df0f3499d98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*1vKsMzar7yim_KxjcZhTLA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Notation for trajectory: defined as some sequence of state-action pairs until some future timestep T. Defining the trajectory allows us to skip writing the entire sequence of states and actions, and substitute a single variable instead :P! (Image by author)</figcaption></figure><p id="4b32" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Since our environment is stochastic, it’s important to also consider the likelihood of such a trajectory occurring — low probability trajectories will reduce the expectation of reward. (Since our <a class="af od" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">expected value</a> consists of multiplying our reward by the transitional probability, trajectories that are less likely will have a lower expected reward compared to high probability ones.) The probability can be derived by considering the probability of each action and state happening incrementally: At any timestep in our MDP, we will select actions based on our policy, and the resulting state will be dependent on both the action we selected and the transitional probability. Without loss of generality, we’ll denote the transitional probability as a separate probability distribution, a function of both the current state and the attempted action. The conditional probability of some future state occurring is therefore defined as</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qe"><img src="../Images/3ec868afc74700730be6d3fed573142d.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*5x7eWR8VNOI27HkzfwFriA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Transitional probability of moving to a future state from a current state — for our frozen lake though, we know this value is fixed at ~0.33 (Image by author)</figcaption></figure><p id="414b" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">And the probability of some action happening based on our policy is simply evaluated by passing our state into our policy function</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qf"><img src="../Images/4bd02d3fd4af429c18e4e17e5041e3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*_yrZqgg94niwA0TK1unJmA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Expression for the probability of some action being selected by the policy given some state (Image by author)</figcaption></figure><blockquote class="mj mk ml"><p id="8cc7" class="mm mn mo mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Our policy is currently deterministic, as it selects actions based on the highest expected action-value. In other words, actions that have a low action-value will never be selected, while actions with a high Q-value will always be selected. This results in a <a class="af od" href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="noopener ugc nofollow" target="_blank">Bernoulli distribution</a> across possible actions. This is very rarely beneficial, as we’ll see later.</p></blockquote><p id="d828" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Applying these expressions to our trajectory, we can define the probability of some trajectory occurring as</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qg"><img src="../Images/efe4817d6f7d5469f2fe7817f7b99699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*RQvaYDaki2Kv7XlAwcbImg.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Expanded equation for the probability of a certain trajectory occurring. Note that the probability of s0 is fixed at 1 assuming we start from the same state (top left) every time. (Image by author)</figcaption></figure><p id="ddad" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">For clarity, here’s the original notation for a trajectory:</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qd"><img src="../Images/686a7c274c1f31c017a4df0f3499d98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*1vKsMzar7yim_KxjcZhTLA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Notation for trajectory: defined as some sequence of state-action pairs until some future timestep T (Image by author)</figcaption></figure><p id="3154" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">More concisely, we have</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qh"><img src="../Images/102aac00b1bfd5c5ef917f05ec83a953.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*qnQRoxkv4gbKUb17R7Nzmg.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Concise notation for the probability of a trajectory occurring (Image by author)</figcaption></figure><p id="afa5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Defining both the trajectory and its probability allows us to substitute these expressions to simplify our definitions for both return and its expected value. The return (sum of rewards), which we’ll define as <em class="mo">G </em>based on conventions, can now be written as</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qi"><img src="../Images/734777314730b1fbccce78d189eb9378.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*wETqpsQkmsjSsfEoHGiABA.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equation for return (Image by author)</figcaption></figure><p id="93a1" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We can also define the expected return by introducing probability into the equation. Since we’ve already defined the probability of a trajectory, the expected return is therefore</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk qj"><img src="../Images/aeef9f57bf02543cac9034014c39d9bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*eIl4OjzupJl2rSQsho_Yhw.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Updated equation for expected return = the probability of the trajectory occurring times the return (Image by author)</figcaption></figure><p id="de4f" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We can now adjust the definition of our value functions to include the expected return</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qk"><img src="../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mI7611w16ec4al5vqJ02xw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Updated equations for state- and action-value (Image by author)</figcaption></figure><p id="3163" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">The main difference here is the addition of the subscript τ∼π indicating that our trajectory was sampled by following our policy (ie. our actions are selected based on the maximum Q-value). We’ve also removed the subscript <em class="mo">t </em>for clarity. Here’s the previous equation again for reference:</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qb"><img src="../Images/71d8fa1489f5ddbcb51e32343d03f9f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMCodiIqjs7jW0SXiTfmKg.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equations for state- and action-value (Image by author)</figcaption></figure><h2 id="7024" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Discounted Return</h2><p id="f4d0" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">So now we have a fairly well-defined expression for estimating return but before we can start iterating through our environment, there’s still some more things to consider. In our frozen lake, it’s fairly unlikely that our agent will continue to explore indefinitely. At some point, it will slip and fall into a hole, and the episode will terminate. However, in practice, RL environments might not have clearly defined endpoints, and training sessions might continue indefinitely. In these situations, given an indefinite amount of time, the expected return would approach infinity, and evaluating the state- and action-value would become impossible. Even in our case, setting a hard limit for computing return is oftentimes not beneficial, and if we set the limit too high, we could end up with pretty absurdly large numbers anyway. In these situations, it is important to ensure that our reward series will converge using a <em class="mo">discount factor</em>. This improves stability in the training process and ensures that our return will always be a finite value regardless of how far into the future we look. This type of discounted return is also referred to as <em class="mo">infinite horizon discounted return.</em></p><p id="4a89" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">To add discounting to our return equation, we’ll introduce a new variable γ (gamma) to represent the discount factor.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div class="nj nk ql"><img src="../Images/1ee75a0332ea8ac373c2f91b93cdd928.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*4mIgUcGJ99CXdse5WERtqQ.png"/></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equation for discounted return (Image by author)</figcaption></figure><p id="1b95" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Gamma must always be less than 1, or our series will not converge. Expanding this expression makes this even more apparent</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qm"><img src="../Images/c14a63896f5f2bb82b43fb4fb3614cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H4-5nGCI4qt5IZXhFTfr4A.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Expanded equation for discounted return (Image by author)</figcaption></figure><p id="317b" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">We can see that as time increases, gamma will be raised to a higher and higher power. As gamma is less than 1, raising it to a higher exponent will only make it smaller, thus exponentially decreasing the contribution of future rewards to the overall sum. We can substitute this updated definition of return back into our value functions, though nothing will visibly change since the variable is still the same.</p><figure class="nm nn no np nq nr nj nk paragraph-image"><div role="button" tabindex="0" class="ns nt ed nu bh nv"><div class="nj nk qk"><img src="../Images/548104584e6ecbdfa2ef6a3a0c6cbe6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mI7611w16ec4al5vqJ02xw.png"/></div></div><figcaption class="nx ny nz nj nk oa ob bf b bg z dx">Equations for state- and action-value, copied again for emphasis (Image by author)</figcaption></figure><h1 id="dcf6" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Exploration vs. Exploitation</h1><p id="78b0" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">We mentioned earlier that always being greedy is not the best choice. Always selecting our actions based on the maximum Q-value will probably give us the highest chance of maximizing our reward, but that only holds when we have accurate estimates of those Q-values in the first place. To obtain accurate estimates, we need a lot of information, and we can only gain information by trying new things — that is, exploration.</p><p id="411d" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">When we select actions based on the highest estimated Q-value, we <em class="mo">exploit</em> our current knowledge base: we leverage our accumulated experiences in an attempt to maximize our reward. When we select actions based on any other metric, or even randomly, we <em class="mo">explore</em> alternative possibilities in an attempt to gain more useful information to update our Q-value estimates with. In reinforcement learning, we want to balance both <em class="mo">exploration</em> and <em class="mo">exploitation. </em>To properly exploit our knowledge, we need to have knowledge, and to gain knowledge, we need to explore.</p><h2 id="bf38" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Epsilon-Greedy Policy</h2><p id="b611" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">We can balance exploration and exploitation by changing our policy from purely greedy to an <em class="mo">epsilon-greedy</em> one. An epsilon-greedy policy acts greedily most of the time with a probability of 1- ε, but has a probability of ε to act randomly. In other words, we’ll exploit our knowledge most of the time in an attempt to maximize reward, and we’ll explore occasionally to gain more knowledge. This is not the only way of balancing exploration and exploitation, but it is one of the simplest and easiest to implement.</p><h1 id="eab0" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">Summary</h1><p id="6792" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">Now the we’ve established a basis for understanding RL principles, we can move to discussing the actual algorithm — which will happen in the next article. For now, we’ll go over the high-level overview, combining all these concepts into a cohesive pseudo-code which we can delve into next time.</p><h2 id="bdac" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Q-Learning</h2><p id="6fda" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">The focus of this article was to establish the basis for understanding and implementing Q-learning. Q-learning consists of the following steps:</p><ol class=""><li id="8108" class="mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni qn qo qp bk">Initialize a tabular estimate of all action-values (Q-values), which we update as we iterate through our environment.</li><li id="9cb4" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qn qo qp bk">Select an action by sampling from our epsilon-greedy policy.</li><li id="5824" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qn qo qp bk">Collect the reward (if any) and update our estimate for our action-value.</li><li id="850e" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qn qo qp bk">Move to the next state, or terminate if we fall into a hole or reach the goal.</li><li id="fe50" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qn qo qp bk">Loop steps 2–4 until our estimated Q-values converge.</li></ol><p id="6e25" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">Q-learning is an iterative process where we build estimates of action-value (and expected return), or “experience”, and use our experiences to identify which actions are the most rewarding for us to choose. These experiences are “learned” over many successive iterations of our environment and by leveraging them we will be able to consistently reach our goal, thus solving our MDP.</p><h2 id="5c39" class="ph of fq bf og pi pj pk oj pl pm pn om mw po pp pq na pr ps pt ne pu pv pw px bk">Glossary</h2><ul class=""><li id="08f6" class="mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni qv qo qp bk"><strong class="mp fr">Environment — </strong>anything that cannot be arbitrarily changed by our agent, aka the world around it</li><li id="89cf" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">State — </strong>a particular condition of the environment</li><li id="fdd6" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Observation — </strong>some subset of information from the state</li><li id="925f" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Policy — </strong>a function that selects an action given a state</li><li id="60aa" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Agent — </strong>our “learner” which acts according to a policy in our environment</li><li id="45fa" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Reward — </strong>what our agent receives after performing certain actions</li><li id="06e2" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Return — </strong>a sum of rewards across a series of actions</li><li id="604b" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Discounting</strong> — the process through which we ensure that our return does not reach infinity</li><li id="645a" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">State-value — </strong>the expected return starting from a state and continuing to act according to some policy, forever</li><li id="0758" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Action-value — </strong>the expected return starting from a state and taking some action, and then continuing to act according to some policy, forever</li><li id="bf78" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Trajectory — </strong>a series of states and actions</li><li id="09b1" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Markov Decision Process (MDP) — </strong>the model we use to represent decision problems in RL aka a directed graph with non-deterministic edges</li><li id="1763" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Exploration — </strong>how we obtain more knowledge</li><li id="a3a0" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Exploitation — </strong>how we use our existing knowledge base to gain more reward</li><li id="d4bf" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Q-Learning</strong> — a RL algorithm where we iteratively update Q-values to obtain better estimates of which actions will yield higher expected return</li><li id="587f" class="mm mn fq mp b go qq mr ms gr qr mu mv mw qs my mz na qt nc nd ne qu ng nh ni qv qo qp bk"><strong class="mp fr">Reinforcement Learning — </strong>a learning process built on the accumulation of past experiences coupled with quantifiable reward</li></ul><p id="4ef5" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">If you’ve read this far, consider leaving some feedback about the article — I’d appreciate it ❤.</p><h1 id="2b38" class="oe of fq bf og oh oi gq oj ok ol gt om on oo op oq or os ot ou ov ow ox oy oz bk">References</h1><p id="221e" class="pw-post-body-paragraph mm mn fq mp b go pa mr ms gr pb mu mv mw pc my mz na pd nc nd ne pe ng nh ni fj bk">[1] Gymnasium, <a class="af od" href="https://gymnasium.farama.org/environments/toy_text/frozen_lake/" rel="noopener ugc nofollow" target="_blank">Frozen Lake</a> (n.d.), OpenAI Gymnasium Documentation.</p><p id="d7a9" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[2] OpenAI, <a class="af od" href="https://spinningup.openai.com/en/latest/" rel="noopener ugc nofollow" target="_blank">Spinning Up in Deep RL</a> (n.d.), OpenAI.</p><p id="2c43" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[3] R. Sutton and A. Barto, <a class="af od" href="http://incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank">Reinforcement Learning: An Introduction</a> (2020), <a class="af od" href="http://incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/RLbook2020.pdf</a></p><p id="f2ef" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[4] Spiceworks, <a class="af od" href="https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-markov-decision-process/" rel="noopener ugc nofollow" target="_blank">What is a Markov Decision Process?</a> (n.d.), Spiceworks</p><p id="d2ca" class="pw-post-body-paragraph mm mn fq mp b go mq mr ms gr mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni fj bk">[5] IBM, <a class="af od" href="https://www.ibm.com/topics/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">Reinforcement Learning</a> (n.d.), IBM</p></div></div></div></div>    
</body>
</html>