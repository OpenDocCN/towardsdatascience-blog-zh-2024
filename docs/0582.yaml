- en: Seeing Our Reflection in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/seeing-our-reflection-in-llms-7b9505e901fd?source=collection_archive---------5-----------------------#2024-03-02](https://towardsdatascience.com/seeing-our-reflection-in-llms-7b9505e901fd?source=collection_archive---------5-----------------------#2024-03-02)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When LLMs give us outputs that reveal flaws in human society, can we choose
    to listen to what they tell us?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@s.kirmer?source=post_page---byline--7b9505e901fd--------------------------------)[![Stephanie
    Kirmer](../Images/f9d9ef9167febde974c223dd4d8d6293.png)](https://medium.com/@s.kirmer?source=post_page---byline--7b9505e901fd--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--7b9505e901fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--7b9505e901fd--------------------------------)
    [Stephanie Kirmer](https://medium.com/@s.kirmer?source=post_page---byline--7b9505e901fd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--7b9505e901fd--------------------------------)
    ·7 min read·Mar 2, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b8b81aae84440667a1e78dbd9ebe997.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Vince Fleming](https://unsplash.com/@vincefleming?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, Nudged
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now, I’m sure most of you have heard the news about [Google’s new LLM*, Gemini,
    generating pictures of racially diverse people in Nazi uniforms](https://www.vox.com/future-perfect/2024/2/28/24083814/google-gemini-ai-bias-ethics).
    This little news blip reminded me of something that I’ve been meaning to discuss,
    which is when models have blind spots, so we apply expert rules to the predictions
    they generate to avoid returning something wildly outlandish to the user.
  prefs: []
  type: TYPE_NORMAL
- en: This sort of thing is not that uncommon in machine learning, in my experience,
    especially when you have flawed or limited training data. A good example of this
    that I remember from my own work was predicting when a package was going to be
    delivered to a business office. Mathematically, our model would be very good at
    estimating exactly when the package would get physically near the office, but
    sometimes, truck drivers arrive at destinations late at night and then rest in
    their truck or in a hotel until morning. Why? Because no one’s in the office to
    receive/sign for the package outside of business hours.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching a model about the idea of “business hours” can be very difficult, and
    the much easier solution was just to say, “If the model says the delivery will
    arrive outside business hours, add enough time to the prediction that it changes
    to the next hour the office is listed as open.” Simple! It solves the problem
    and it reflects the actual circumstances on the ground. We’re just giving the
    model a little boost to help its results work better.
  prefs: []
  type: TYPE_NORMAL
- en: However, this does cause some issues. For one thing, now we have two different
    model predictions to manage. We can’t just throw away the original model prediction,
    because that’s what we use for model performance monitoring and metrics. You can’t
    assess a model on predictions after humans got their paws in there, that’s not
    mathematically sound. But to get a clear sense of the real world model impact,
    you do want to look at the post-rule prediction, because that’s what the customer
    actually experienced/saw in your application. In ML, we’re used to a very simple
    framing, where every time you run a model you get one result or set of results,
    and that’s that, but when you start tweaking the results before you let them go,
    then you need to think at a different scale.
  prefs: []
  type: TYPE_NORMAL
- en: Applying to LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I kind of suspect that this is a form of what’s going on with LLMs like Gemini.
    However, instead of a post-prediction rule, it appears that the [smart money says
    Gemini and other models are applying “secret” prompt augmentations to try and
    change the results the LLMs produce.](https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/?pwapi_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyZWFzb24iOiJnaWZ0IiwibmJmIjoxNzA4ODM3MjAwLCJpc3MiOiJzdWJzY3JpcHRpb25zIiwiZXhwIjoxNzEwMjE1OTk5LCJpYXQiOjE3MDg4MzcyMDAsImp0aSI6IjFhMzAyYjkyLTRkN2ItNDNmMi1hNThlLWY1MDBjY2I2NDFjMyIsInVybCI6Imh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjQvMDIvMjIvZ29vZ2xlLWdlbWluaS1haS1pbWFnZS1nZW5lcmF0aW9uLXBhdXNlLyJ9.E-JdVAohho0X-rTsTb1bfof4gIpYl8-NpPdZwL6h9Dc)
  prefs: []
  type: TYPE_NORMAL
- en: In essence, without this nudging, the model will produce results that are reflective
    of the content it has been trained on. That is to say, the content produced by
    real people. Our social media posts, our history books, our museum paintings,
    our popular songs, our Hollywood movies, etc. The model takes in all that stuff,
    and it learns the underlying patterns in it, whether they are things we’re proud
    of or not. A model given all the media available in our contemporary society is
    going to get a whole lot of exposure to racism, sexism, and myriad other forms
    of discrimination and inequality, to say nothing of violence, war, and other horrors.
    While the model is learning what people look like, and how they sound, and what
    they say, and how they move, it’s learning the warts-and-all version.
  prefs: []
  type: TYPE_NORMAL
- en: Our social media posts, our history books, our museum paintings, our popular
    songs, our Hollywood movies, etc. The model takes in all that stuff, and it learns
    the underlying patterns in it, whether they are things we’re proud of or not.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This means that if you ask the underlying model to show you a doctor, it’s going
    to probably be a white guy in a lab coat. This isn’t just random, it’s because
    in our modern society white men have disproportionate access to high status professions
    like being doctors, because they on average have access to more and better education,
    financial resources, mentorship, social privilege, and so on. The model is reflecting
    back at us an image that may make us uncomfortable because we don’t like to think
    about that reality.
  prefs: []
  type: TYPE_NORMAL
- en: So what do we do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The obvious argument is, “Well, we don’t want the model to reinforce the biases
    our society already has, we want it to improve representation of underrepresented
    populations.” I sympathize with this argument, quite a lot, and I care about representation
    in our media. However, there’s a problem.
  prefs: []
  type: TYPE_NORMAL
- en: It’s very unlikely that applying these tweaks is going to be a sustainable solution.
    Recall back to the story I started with about Gemini. It’s like playing whac-a-mole,
    because the work never stops — now we’ve got people of color being shown in Nazi
    uniforms, and this is understandably deeply offensive to lots of folks. So, maybe
    where we started by randomly applying “as a black person” or “as an indigenous
    person” to our prompts, we have to add something more to make it exclude cases
    where it’s inappropriate — but how do you phrase that, in a way an LLM can understand?
    We probably have to go back to the beginning, and think about how the original
    fix works, and revisit the whole approach. In the best case, applying a tweak
    like this fixes one narrow issue with outputs, while potentially creating more.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s play out another very real example. What if we add to the prompt, “Never
    use explicit or profane language in your replies, including [list of bad words
    here]”. Maybe that works for a lot of cases, and the model will refuse to say
    bad words that a 13 year old boy is requesting to be funny. [But sooner or later,
    this has unexpected additional side effects.](https://www.wired.com/story/ai-list-dirty-naughty-obscene-bad-words/)
    What about if someone’s looking for the [history of Sussex, England](https://www.boredpanda.com/people-with-dirty-last-names-problems/)?
    Alternately, someone’s going to come up with a bad word you left out of the list,
    so that’s going to be constant work to maintain. What about bad words in other
    languages? [Who judges what goes on the list](https://www.newsweek.com/twitter-lgbtq-censor-censorship-elon-musk-1792139)?
    I have a headache just thinking about it.
  prefs: []
  type: TYPE_NORMAL
- en: This is just two examples, and I’m sure you can think of more such scenarios.
    It’s like putting band aid patches on a leaky pipe, and every time you patch one
    spot another leak springs up.
  prefs: []
  type: TYPE_NORMAL
- en: Where do we go from here?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what is it we actually want from LLMs? Do we want them to generate a highly
    realistic mirror image of what human beings are actually like and how our human
    society actually looks from the perspective of our media? Or do we want a sanitized
    version that cleans up the edges?
  prefs: []
  type: TYPE_NORMAL
- en: Honestly, I think we probably need something in the middle, and we have to continue
    to renegotiate the boundaries, even though it’s hard. We don’t want LLMs to reflect
    the real horrors and sewers of violence, hate, and more that human society contains,
    that is a part of our world that should not be amplified even slightly. [Zero
    content moderation is not the answer.](https://open.substack.com/pub/platformer/p/why-platformer-is-leaving-substack?selection=61e54bce-0a54-44d7-9e24-86bc2ac24e36&utm_campaign=post-share-selection&utm_medium=web)
    Fortunately, this motivation aligns with the desires of large corporate entities
    running these models to be popular with the public and make lots of money.
  prefs: []
  type: TYPE_NORMAL
- en: …we have to continue to renegotiate the boundaries, even though it’s hard. We
    don’t want LLMs to reflect the real horrors and sewers of violence, hate, and
    more that human society contains, that is a part of our world that should not
    be amplified even slightly. Zero content moderation is not the answer.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, I do want to continue to make a gentle case for the fact that we can
    also learn something from this dilemma in the world of LLMs. Instead of simply
    being offended and blaming the technology when a model generates a bunch of pictures
    of a white male doctor, we should pause to understand why that’s what we received
    from the model. And then we should debate thoughtfully about whether the response
    from the model should be allowed, and make a decision that is founded in our values
    and principles, and try to carry it out to the best of our ability.
  prefs: []
  type: TYPE_NORMAL
- en: As I’ve said before, an LLM isn’t an alien from another universe, it’s us. It’s
    trained on the things **we** wrote/said/filmed/recorded/did. If we want our model
    to show us doctors of various sexes, genders, races, etc, we need to make a society
    that enables all those different kinds of people to have access to that profession
    and the education it requires. If we’re worrying about how the model mirrors us,
    but not taking to heart the fact that it’s us that needs to be better, not just
    the model, then we’re missing the point.
  prefs: []
  type: TYPE_NORMAL
- en: If we want our model to show us doctors of various sexes, genders, races, etc,
    we need to make a society that enables all those different kinds of people to
    have access to that profession and the education it requires.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*I’m sure I’m not the only one to think this, but since Gemini is definitionally
    multimodal, using not just language but audio, video, etc in training, “LLM” seems
    like the wrong term for it. But all the references I find online still seem to
    be using that word.'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can find more of my work at* [*www.stephaniekirmer.com.*](http://www.stephaniekirmer.com.)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://www.vox.com/future-perfect/2024/2/28/24083814/google-gemini-ai-bias-ethics?source=post_page-----7b9505e901fd--------------------------------)
    [## Black Nazis? A woman pope? That''s just the start of Google''s AI problem.'
  prefs: []
  type: TYPE_NORMAL
- en: The Gemini image generator isn't just suffering from a technical problem, but
    from a philosophical one.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.vox.com](https://www.vox.com/future-perfect/2024/2/28/24083814/google-gemini-ai-bias-ethics?source=post_page-----7b9505e901fd--------------------------------)
    [](https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/?pwapi_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyZWFzb24iOiJnaWZ0IiwibmJmIjoxNzA4ODM3MjAwLCJpc3MiOiJzdWJzY3JpcHRpb25zIiwiZXhwIjoxNzEwMjE1OTk5LCJpYXQiOjE3MDg4MzcyMDAsImp0aSI6IjFhMzAyYjkyLTRkN2ItNDNmMi1hNThlLWY1MDBjY2I2NDFjMyIsInVybCI6Imh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjQvMDIvMjIvZ29vZ2xlLWdlbWluaS1haS1pbWFnZS1nZW5lcmF0aW9uLXBhdXNlLyJ9.E-JdVAohho0X-rTsTb1bfof4gIpYl8-NpPdZwL6h9Dc&source=post_page-----7b9505e901fd--------------------------------)
    [## Google takes down Gemini AI image generator. Here's what you need to know.
  prefs: []
  type: TYPE_NORMAL
- en: Critics said the Google's Gemini image generator created images of a woman pope
    and Black founding father.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.washingtonpost.com](https://www.washingtonpost.com/technology/2024/02/22/google-gemini-ai-image-generation-pause/?pwapi_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyZWFzb24iOiJnaWZ0IiwibmJmIjoxNzA4ODM3MjAwLCJpc3MiOiJzdWJzY3JpcHRpb25zIiwiZXhwIjoxNzEwMjE1OTk5LCJpYXQiOjE3MDg4MzcyMDAsImp0aSI6IjFhMzAyYjkyLTRkN2ItNDNmMi1hNThlLWY1MDBjY2I2NDFjMyIsInVybCI6Imh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjQvMDIvMjIvZ29vZ2xlLWdlbWluaS1haS1pbWFnZS1nZW5lcmF0aW9uLXBhdXNlLyJ9.E-JdVAohho0X-rTsTb1bfof4gIpYl8-NpPdZwL6h9Dc&source=post_page-----7b9505e901fd--------------------------------)
    [](https://www.wired.com/story/ai-list-dirty-naughty-obscene-bad-words/?source=post_page-----7b9505e901fd--------------------------------)
    [## AI and the List of Dirty, Naughty, Obscene, and Otherwise Bad Words
  prefs: []
  type: TYPE_NORMAL
- en: It started as a way to restrict autocompletes on Shutterstock. Now it grooms
    search suggestions on Slack and influences…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.wired.com](https://www.wired.com/story/ai-list-dirty-naughty-obscene-bad-words/?source=post_page-----7b9505e901fd--------------------------------)
    [](https://www.boredpanda.com/people-with-dirty-last-names-problems/?source=post_page-----7b9505e901fd--------------------------------)
    [## People With 'Offensive' Last Names Shared Their Everyday Problems, And It's
    Hilarious
  prefs: []
  type: TYPE_NORMAL
- en: There are some people out there who have been dealt a bad hand since the day
    they were born. There are plenty of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.boredpanda.com](https://www.boredpanda.com/people-with-dirty-last-names-problems/?source=post_page-----7b9505e901fd--------------------------------)
    [](https://open.substack.com/pub/platformer/p/why-platformer-is-leaving-substack?selection=61e54bce-0a54-44d7-9e24-86bc2ac24e36&utm_campaign=post-share-selection&utm_medium=web&source=post_page-----7b9505e901fd--------------------------------)
    [## Why Platformer is leaving Substack
  prefs: []
  type: TYPE_NORMAL
- en: We've seen this movie before - and we won't stick around to watch it play out
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: open.substack.com](https://open.substack.com/pub/platformer/p/why-platformer-is-leaving-substack?selection=61e54bce-0a54-44d7-9e24-86bc2ac24e36&utm_campaign=post-share-selection&utm_medium=web&source=post_page-----7b9505e901fd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
