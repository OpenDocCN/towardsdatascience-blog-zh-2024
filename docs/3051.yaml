- en: Understanding the Mathematics of PPO in Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21](https://towardsdatascience.com/understanding-the-mathematics-of-ppo-in-reinforcement-learning-467618b2f8d4?source=collection_archive---------2-----------------------#2024-12-21)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep dive into RL with PPO for beginners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Manelle
    Nouar](../Images/292d0f9007b102476fd0f2831bcdafb1.png)](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    [Manelle Nouar](https://manellenouar.medium.com/?source=post_page---byline--467618b2f8d4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--467618b2f8d4--------------------------------)
    ·7 min read·Dec 21, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5025e297ec787bcf8be293d57e481e27.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ThisisEngineering](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning (RL) is a branch of Artificial Intelligence that enables
    agents to learn how to interact with their environment. These agents, which range
    from robots to software features or autonomous systems, learn through trial and
    error. They receive rewards or penalties based on the actions they take, which
    guide their future decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Among the most well-known RL algorithms, Proximal Policy Optimization (PPO)
    is often favored for its stability and efficiency. PPO addresses several challenges
    in RL, particularly in controlling how the policy (the agent’s decision-making
    strategy) evolves. Unlike other algorithms, PPO ensures that policy updates are
    not too large, preventing destabilization during training. This stabilization
    is crucial, as drastic updates can cause the agent to diverge from an optimal
    solution, making the learning process erratic. PPO thus maintains a balance between
    exploration (trying new actions) and exploitation (focusing on actions that yield
    the highest rewards).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, PPO is highly efficient in terms of both computational resources
    and learning speed. By optimizing the agent’s policy effectively while avoiding
    overly complex calculations, PPO has become a practical solution in various domains,
    such as robotics, gaming, and autonomous systems. Its simplicity makes it easy
    to implement, which has led to its widespread adoption in both research and industry.
  prefs: []
  type: TYPE_NORMAL
- en: This article explores the mathematical foundations of RL and the key concepts
    introduced by PPO, providing a deeper understanding of why PPO has become a go-to
    algorithm in modern reinforcement learning research.
  prefs: []
  type: TYPE_NORMAL
- en: '1\. The Basics of RL: Markov Decision Process (MDP)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning problems are often modeled using a Markov Decision Process
    (MDP), a mathematical framework that helps formalize decision-making in environments
    where outcomes are uncertain.
  prefs: []
  type: TYPE_NORMAL
- en: A Markov chain models a system that transitions between states, where the probability
    of moving to a new state depends solely on the current state and not on previous
    states. This principle is known as the *Markov property*. In the context of MDPs,
    this simplification is key for modeling decisions, as it allows an agent to focus
    only on the current state when making decisions without needing to account for
    the entire history of the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MDP is defined by the following elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '- S: Set of possible states.'
  prefs: []
  type: TYPE_NORMAL
- en: '- A: Set of possible actions.'
  prefs: []
  type: TYPE_NORMAL
- en: '- P(s’|s, a): Transition probability of reaching state s’ after taking action
    a in state s.'
  prefs: []
  type: TYPE_NORMAL
- en: '- R(s, a): Reward received after taking action a in state s.'
  prefs: []
  type: TYPE_NORMAL
- en: '- γ: Discount factor (a value between 0 and 1) that reflects the importance
    of future rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: The discount factor γ is crucial for modeling the importance of future rewards
    in decision-making problems. When an agent makes a decision, it must evaluate
    not only the immediate reward but also the potential future rewards. The discount
    γ reduces the impact of rewards that occur later in time due to the uncertainty
    of reaching those rewards. Thus, a value of γ close to 1 indicates that future
    rewards are almost as important as immediate rewards, while a value close to 0
    gives more importance to immediate rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The time discount reflects the agent’s preference for quick gains over future
    ones, often due to uncertainty or the possibility of changes in the environment.
    For example, an agent will likely prefer an immediate reward rather than one in
    the distant future unless that future reward is sufficiently significant. This
    discount factor thus models optimization behaviors where the agent considers both
    short-term and long-term benefits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to find an action policy π(a|s) that maximizes the expected sum
    of rewards over time, often referred to as the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: This function represents the expected total reward an agent can accumulate starting
    from state s and following policy π.
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Policy Optimization: Policy Gradient'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy gradient methods focus on directly optimizing the parameters θ of a policy
    πθ by maximizing an objective function that represents the expected reward obtained
    by following that policy in a given environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: Where R(s, a) is the reward received for taking action a in state s, and the
    goal is to maximize this expected reward over time. The term dπ(s) represents
    the stationary distribution of states under policy π, indicating how frequently
    the agent visits each state when following policy π.
  prefs: []
  type: TYPE_NORMAL
- en: 'The policy gradient theorem gives the gradient of the objective function, providing
    a way to update the policy parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: This equation shows how to adjust the policy parameters based on past experiences,
    which helps the agent learn more efficient behaviors over time.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Mathematical Enhancements of PPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PPO (Proximal Policy Optimization) introduces several important features to
    improve the stability and efficiency of reinforcement learning, particularly in
    large and complex environments. PPO was introduced by John Schulman et al. in
    2017 as an improvement over earlier policy optimization algorithms like Trust
    Region Policy Optimization (TRPO). The main motivation behind PPO was to strike
    a balance between sample efficiency, ease of implementation, and stability while
    avoiding the complexities of TRPO’s second-order optimization methods. While TRPO
    ensures stable policy updates by enforcing a strict constraint on the policy change,
    it relies on computationally expensive second-order derivatives and conjugate
    gradient methods, making it challenging to implement and scale. Moreover, the
    strict constraints in TRPO can sometimes overly limit policy updates, leading
    to slower convergence. PPO addresses these issues by using a simple clipped objective
    function that allows the policy to update in a stable and controlled manner, avoiding
    forgetting previous policies with each update, thus improving training efficiency
    and reducing the risk of policy collapse. This makes PPO a popular choice for
    a wide range of reinforcement learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: a. Probability Ratio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the key components of PPO is the probability ratio, which compares the
    probability of taking an action in the current policy πθ to that of the old policy
    πθold:'
  prefs: []
  type: TYPE_NORMAL
- en: This ratio provides a measure of how much the policy has changed between updates.
    By monitoring this ratio, PPO ensures that updates are not too drastic, which
    helps prevent instability in the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: b. Clipping Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clipping is preferred over adjusting the learning rate in Proximal Policy Optimization
    (PPO) because it directly limits the magnitude of policy updates, preventing excessive
    changes that could destabilize the learning process. While the learning rate uniformly
    scales the size of updates, clipping ensures that updates stay close to the previous
    policy, thereby enhancing stability and reducing erratic behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of clipping is that it allows for better control over updates,
    ensuring more stable progress. However, a potential drawback is that it may slow
    down learning by limiting the exploration of significantly different strategies.
    Nonetheless, clipping is favored in PPO and other algorithms when stability is
    essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid excessive changes to the policy, PPO uses a clipping function that
    modifies the objective function to restrict the size of policy updates. This is
    crucial because large updates in reinforcement learning can lead to erratic behavior.
    The modified objective with clipping is:'
  prefs: []
  type: TYPE_NORMAL
- en: The clipping function constrains the probability ratio within a specific range,
    preventing updates that would deviate too far from the previous policy. This helps
    avoid sudden, large changes that could destabilize the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: c. Advantage Estimation with GAE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In RL, estimating the advantage is important because it helps the agent determine
    which actions are better than others in each state. However, there is a trade-off:
    using only immediate rewards (or very short horizons) can introduce high variance
    in advantage estimates, while using longer horizons can introduce bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Generalized Advantage Estimation (GAE) strikes a balance between these two by
    using a weighted average of n-step returns and value estimates, making it less
    sensitive to noise and improving learning stability.
  prefs: []
  type: TYPE_NORMAL
- en: Why use GAE?
  prefs: []
  type: TYPE_NORMAL
- en: '- **Stability**: GAE helps reduce variance by considering multiple steps so
    the agent does not react to noise in the rewards or temporary fluctuations in
    the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Efficiency**: GAE strikes a good balance between bias and variance, making
    learning more efficient by not requiring overly long sequences of rewards while
    still maintaining reliable estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: '- **Better Action Comparison**: By considering not just the immediate reward
    but a broader horizon of rewards, the agent can better compare actions over time
    and make more informed decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantage function At is used to assess how good an action was relative
    to the expected behavior under the current policy. To reduce variance and ensure
    more reliable estimates, PPO uses Generalized Advantage Estimation (GAE). This
    method smooths out the advantages over time while controlling for bias:'
  prefs: []
  type: TYPE_NORMAL
- en: This technique provides a more stable and accurate measure of the advantage,
    which improves the agent’s ability to make better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: d. Entropy to Encourage Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PPO incorporates an entropy term in the objective function to encourage the
    agent to explore more of the environment rather than prematurely converging to
    a suboptimal solution. The entropy term increases the uncertainty in the agent’s
    decision-making, which prevents overfitting to a specific strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Where H(πθ) represents the entropy of the policy. By adding this term, PPO ensures
    that the agent does not converge too quickly and is encouraged to continue exploring
    different actions and strategies, improving overall learning efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mathematical underpinnings of PPO demonstrate how this algorithm achieves
    stable and efficient learning. With concepts like the probability ratio, clipping,
    advantage estimation, and entropy, PPO offers a powerful balance between exploration
    and exploitation. These features make it a robust choice for both researchers
    and practitioners working in complex environments. The simplicity of PPO, combined
    with its efficiency and effectiveness, makes it a popular and valuable algorithm
    in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+an+introduction&ots=1-9av2aqTb&sig=qMSnFC56yqPQugvqS3_uwCN78z0#v=onepage&q=reinforcement%20learning%20an%20introduction&f=false](https://books.google.fr/books?hl=fr&lr=&id=sWV0DwAAQBAJ&oi=fnd&pg=PR7&dq=reinforcement+learning+introduction&ots=1-9av2asXb&sig=ZDKNYXZc8gMIzEqeonvMxcN8skE#v=onepage&q=reinforcement%20learning%20introduction&f=false)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*This article was partially translated from French using* [*DeepL*](https://www.deepl.com/fr/translator)*.*'
  prefs: []
  type: TYPE_NORMAL
