["```py\n# general imports\nimport time, functools\n\n# torch imports\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\n# Define Transformer settings\nBATCH_SIZE = 32\nNUM_HEADS = 16\nHEAD_DIM = 64\nDIM = NUM_HEADS * HEAD_DIM\nDEPTH = 24\nNUM_TOKENS = 1024\nMAX_SEQ_LEN = 1024\nPAD_ID = 0\nDEVICE = 'cuda'\n\nclass MyAttentionBlock(nn.Module):\n    def __init__(\n            self,\n            attn_fn,\n            dim,\n            num_heads,\n            format=None,\n            **kwargs\n    ):\n        super().__init__()\n        self.attn_fn = attn_fn\n        self.num_heads = num_heads\n        self.dim = dim\n        self.head_dim = dim // num_heads\n        self.norm1 = nn.LayerNorm(dim, bias=False)\n        self.norm2 = nn.LayerNorm(dim, bias=False)\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.proj = nn.Linear(dim, dim)\n\n        # mlp layers\n        self.fc1 = nn.Linear(dim, dim * 4)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(dim * 4, dim)\n\n        self.permute = functools.partial(torch.transpose, dim0=1, dim1=2)\n        if format == 'bshd':\n            self.permute = nn.Identity()\n\n    def mlp(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.fc2(x)\n        return x\n\n    def reshape_and_permute(self,x, batch_size):\n        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n        return self.permute(x)\n\n    def forward(self, x_in, attn_mask=None):\n        batch_size = x_in.size(0)\n        x = self.norm1(x_in)\n        qkv = self.qkv(x)\n\n        # rather than first reformatting and then splitting the input\n        # state, we first split and then reformat q, k, v in order to\n        # support PyTorch Nested Tensors\n        q, k, v = qkv.chunk(3, -1)\n        q = self.reshape_and_permute(q, batch_size)\n        k = self.reshape_and_permute(k, batch_size)\n        v = self.reshape_and_permute(v, batch_size)\n\n        # call the attn_fn with the input attn_mask\n        x = self.attn_fn(q, k, v, attn_mask=attn_mask)\n\n        # reformat output\n        x = self.permute(x).reshape(batch_size, -1, self.dim)\n        x = self.proj(x)\n        x = x + x_in\n        x = x + self.mlp(self.norm2(x))\n        return x\n```", "```py\nclass MyDecoder(nn.Module):\n    def __init__(\n            self,\n            block_fn,\n            num_tokens,\n            dim,\n            num_heads,\n            num_layers,\n            max_seq_len,\n            pad_idx=None\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.pad_idx = pad_idx\n        self.embedding = nn.Embedding(num_tokens, dim, padding_idx=pad_idx)\n        self.positional_embedding = nn.Embedding(max_seq_len, dim)\n        self.blocks = nn.ModuleList([\n            block_fn(\n                dim=dim,\n                num_heads=num_heads\n            )\n            for _ in range(num_layers)])\n        self.output = nn.Linear(dim, num_tokens)\n\n    def embed_tokens(self, input_ids, position_ids=None):\n        x = self.embedding(input_ids)\n        if position_ids is None:\n            position_ids = torch.arange(input_ids.shape[1],\n                                        device=x.device)\n        x = x + self.positional_embedding(position_ids)\n        return x\n\n    def forward(self, input_ids, position_ids=None, attn_mask=None):\n        # Embed tokens and add positional encoding\n        x = self.embed_tokens(input_ids, position_ids)\n        if self.pad_idx is not None:\n            assert attn_mask is None\n            # create a padding mask - we assume boolean masking\n            attn_mask = (input_ids != self.pad_idx)\n            attn_mask = attn_mask.view(BATCH_SIZE, 1, 1, -1) \\\n                .expand(-1, self.num_heads, -1, -1)\n\n        for b in self.blocks:\n            x = b(x, attn_mask)\n\n        logits = self.output(x)\n        return logits\n```", "```py\n# Use random data\nclass FakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n\n    def __getitem__(self, index):\n        length = torch.randint(1, MAX_SEQ_LEN, (1,))\n        sequence = torch.randint(1, NUM_TOKENS, (length + 1,))\n        inputs = sequence[:-1]\n        targets = sequence[1:]\n        return inputs, targets\n\ndef pad_sequence(sequence, length, pad_val):\n    return torch.nn.functional.pad(\n        sequence,\n        (0, length - sequence.shape[0]),\n        value=pad_val\n    )\n\ndef collate_with_padding(batch):\n    padded_inputs = []\n    padded_targets = []\n    for b in batch:\n        padded_inputs.append(pad_sequence(b[0], MAX_SEQ_LEN, PAD_ID))\n        padded_targets.append(pad_sequence(b[1], MAX_SEQ_LEN, PAD_ID))\n    padded_inputs = torch.stack(padded_inputs, dim=0)\n    padded_targets = torch.stack(padded_targets, dim=0)\n    return {\n        'inputs': padded_inputs,\n        'targets': padded_targets\n    }\n\ndef data_to_device(data, device):\n    if isinstance(data, dict):\n        return {\n            key: data_to_device(val,device)\n            for key, val in data.items()\n        }\n    elif isinstance(data, (list, tuple)):\n        return type(data)(\n            data_to_device(val, device) for val in data\n        )\n    elif isinstance(data, torch.Tensor):\n        return data.to(device=device, non_blocking=True)\n    else:\n        return data.to(device=device)\n```", "```py\ndef main(\n    block_fn, \n    data_collate_fn=collate_with_padding,\n    pad_idx=None,\n    train=True,\n    compile=False\n):\n    torch.random.manual_seed(0)\n    device = torch.device(DEVICE)\n    torch.set_float32_matmul_precision(\"high\")\n\n    # Create dataset and dataloader\n    data_set = FakeDataset()\n    data_loader = DataLoader(\n        data_set,\n        batch_size=BATCH_SIZE,\n        collate_fn=data_collate_fn,\n        num_workers=12,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    model = MyDecoder(\n        block_fn=block_fn,\n        num_tokens=NUM_TOKENS,\n        dim=DIM,\n        num_heads=NUM_HEADS,\n        num_layers=DEPTH,\n        max_seq_len=MAX_SEQ_LEN,\n        pad_idx=pad_idx\n    ).to(device)\n\n    if compile:\n        model = torch.compile(model)\n\n    # Define loss and optimizer\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID)\n    optimizer = torch.optim.SGD(model.parameters())\n\n    def train_step(model, inputs, targets, \n                   position_ids=None, attn_mask=None):\n        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):\n            outputs = model(inputs, position_ids, attn_mask)\n            outputs = outputs.view(-1, NUM_TOKENS)\n            targets = targets.flatten()\n            loss = criterion(outputs, targets)\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n    @torch.no_grad()\n    def eval_step(model, inputs, targets, \n                  position_ids=None, attn_mask=None):\n        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):\n            outputs = model(inputs, position_ids, attn_mask)\n            if outputs.is_nested:\n                outputs = outputs.data._values\n                targets = targets.data._values\n            else:\n                outputs = outputs.view(-1, NUM_TOKENS)\n                targets = targets.flatten()\n            loss = criterion(outputs, targets)\n        return loss\n\n    if train:\n        model.train()\n        step_fn = train_step\n    else:\n        model.eval()\n        step_fn = eval_step\n\n    t0 = time.perf_counter()\n    summ = 0\n    count = 0\n\n    for step, data in enumerate(data_loader):\n        # Copy data to GPU\n        data = data_to_device(data, device=device)\n        step_fn(model, data['inputs'], data['targets'],\n                       position_ids=data.get('indices'),\n                       attn_mask=data.get('attn_mask'))\n\n        # Capture step time\n        batch_time = time.perf_counter() - t0\n        if step > 20:  # Skip first steps\n            summ += batch_time\n            count += 1\n        t0 = time.perf_counter()\n        if step >= 100:\n            break\n    print(f'average step time: {summ / count}')\n```", "```py\nfrom torch.nn.functional import scaled_dot_product_attention as sdpa\nblock_fn = functools.partial(MyAttentionBlock, attn_fn=sdpa)\ncausal_block_fn = functools.partial(\n    MyAttentionBlock,\n    attn_fn=functools.partial(sdpa, is_causal=True)\n)\n\nfor mode in ['eval', 'train']:\n    for compile in [False, True]:\n        block_func = causal_block_fn\\\n            if mode == 'train' else block_fn\n        print(f'{mode} with {collate}, '\n              f'{\"compiled\" if compile else \"uncompiled\"}')\n        main(block_fn=block_func,\n             pad_idx=PAD_ID,\n             train=mode=='train',\n             compile=compile)\n```", "```py\ndef collate_pad_to_longest(batch):\n    padded_inputs = []\n    padded_targets = []\n    max_length = max([b[0].shape[0] for b in batch])\n    for b in batch:\n        padded_inputs.append(pad_sequence(b[0], max_length, PAD_ID))\n        padded_targets.append(pad_sequence(b[1], max_length, PAD_ID))\n    padded_inputs = torch.stack(padded_inputs, dim=0)\n    padded_targets = torch.stack(padded_targets, dim=0)\n    return {\n        'inputs': padded_inputs,\n        'targets': padded_targets\n    }\n\nfor mode in ['eval', 'train']:\n    for compile in [False, True]:\n        block_func = causal_block_fn\\\n            if mode == 'train' else block_fn\n        print(f'{mode} with {collate}, '\n              f'{\"compiled\" if compile else \"uncompiled\"}')\n        main(block_fn=block_func,\n             data_collate_fn=collate_pad_to_longest,\n             pad_idx=PAD_ID,\n             train=mode=='train',\n             compile=compile)\n```", "```py\ndef nested_tensor_collate(batch):\n    inputs = torch.nested.as_nested_tensor([b[0] for b in batch],\n                                           layout=torch.jagged)\n    targets = torch.nested.as_nested_tensor([b[1] for b in batch],\n                                            layout=torch.jagged)\n    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])\n\n    # workaround for creating a NestedTensor with identical \"jagged\" shape\n    xx = torch.empty_like(inputs)\n    xx.data._values[:] = indices\n\n    return {\n        'inputs': inputs,\n        'targets': targets,\n        'indices': xx\n    }\n\nfor compile in [False, True]:\n    print(f'eval with nested tensors, '\n          f'{\"compiled\" if compile else \"uncompiled\"}')\n    main(\n        block_fn=block_fn,\n        data_collate_fn=nested_tensor_collate,\n        train=False,\n        compile=compile\n    )\n```", "```py\ndef collate_concat(batch):\n    inputs = torch.concat([b[0] for b in batch]).unsqueeze(0)\n    targets = torch.concat([b[1] for b in batch]).unsqueeze(0)\n    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])\n    seqlens = torch.tensor([b[0].shape[0] for b in batch])\n    seqlens = torch.cumsum(seqlens, dim=0, dtype=torch.int32)\n    cu_seqlens = torch.nn.functional.pad(seqlens, (1, 0))\n\n    return {\n        'inputs': inputs,\n        'targets': targets,\n        'indices': indices,\n        'attn_mask': cu_seqlens\n    }\n\nfrom flash_attn import flash_attn_varlen_func\nfa_varlen = lambda q, k, v, attn_mask: flash_attn_varlen_func(\n    q.squeeze(0),\n    k.squeeze(0),\n    v.squeeze(0),\n    cu_seqlens_q=attn_mask,\n    cu_seqlens_k=attn_mask,\n    max_seqlen_q=MAX_SEQ_LEN,\n    max_seqlen_k=MAX_SEQ_LEN\n).unsqueeze(0)\n\nfa_varlen_causal = lambda q, k, v, attn_mask: flash_attn_varlen_func(\n    q.squeeze(0),\n    k.squeeze(0),\n    v.squeeze(0),\n    cu_seqlens_q=attn_mask,\n    cu_seqlens_k=attn_mask,\n    max_seqlen_q=MAX_SEQ_LEN,\n    max_seqlen_k=MAX_SEQ_LEN,\n    causal=True\n).unsqueeze(0)\n\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=fa_varlen,\n                             format='bshd')\n\ncausal_block_fn = functools.partial(MyAttentionBlock,\n                                    attn_fn=fa_varlen_causal,\n                                    format='bshd')\n\nprint('flash-attn eval')\nmain(\n    block_fn=block_fn,\n    data_collate_fn=collate_concat,\n    train=False\n)\n\nprint('flash-attn train')\nmain(\n    block_fn=causal_block_fn,\n    data_collate_fn=collate_concat,\n    train=True,\n)\n```", "```py\nfrom xformers.ops import fmha\nfrom xformers.ops import memory_efficient_attention as mea\n\ndef collate_xformer(batch):\n    inputs = torch.concat([b[0] for b in batch]).unsqueeze(0)\n    targets = torch.concat([b[1] for b in batch]).unsqueeze(0)\n    indices = torch.concat([torch.arange(b[0].shape[0]) for b in batch])\n    seqlens = [b[0].shape[0] for b in batch]\n    batch_sizes = [1 for b in batch]\n    block_diag = fmha.BlockDiagonalMask.from_seqlens(seqlens, device='cpu')\n    block_diag._batch_sizes = batch_sizes\n\n    return {\n        'inputs': inputs,\n        'targets': targets,\n        'indices': indices,\n        'attn_mask': block_diag\n    }\n\nmea_eval = lambda q, k, v, attn_mask: mea(\n    q,k,v, attn_bias=attn_mask)\n\nmea_train = lambda q, k, v, attn_mask: mea(\n    q,k,v, attn_bias=attn_mask.make_causal())\n\nblock_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=mea_eval,\n                             format='bshd')\n\ncausal_block_fn = functools.partial(MyAttentionBlock,\n                             attn_fn=mea_train,\n                             format='bshd')\n\nprint(f'xFormer Attention ')\nfor compile in [False, True]:\n    print(f'eval with xFormer Attention, '\n          f'{\"compiled\" if compile else \"uncompiled\"}')\n    main(block_fn=block_fn,\n         train=False,\n         data_collate_fn=collate_xformer,\n         compile=compile)\n\nprint(f'train with xFormer Attention')\nmain(block_fn=causal_block_fn,\n     train=True,\n     data_collate_fn=collate_xformer)\n```", "```py\nfrom transformers import GPT2Config, GPT2LMHeadModel\n\n# Use random data\nclass HuggingFaceFakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n\n    def __getitem__(self, index):\n        length = torch.randint(1, MAX_SEQ_LEN, (1,))\n        input_ids = torch.randint(1, NUM_TOKENS, (length,))\n        labels = input_ids.clone()\n        labels[0] = PAD_ID # ignore first token\n        return {\n            'input_ids': input_ids,\n            'labels': labels\n        }\n        return input_ids, labels\n\ndef hf_collate_with_padding(batch):\n    padded_inputs = []\n    padded_labels = []\n    for b in batch:\n        input_ids = b['input_ids']\n        labels = b['labels']\n        padded_inputs.append(pad_sequence(input_ids, MAX_SEQ_LEN, PAD_ID))\n        padded_labels.append(pad_sequence(labels, MAX_SEQ_LEN, PAD_ID))\n    padded_inputs = torch.stack(padded_inputs, dim=0)\n    padded_labels = torch.stack(padded_labels, dim=0)\n    return {\n        'input_ids': padded_inputs,\n        'labels': padded_labels,\n        'attention_mask': (padded_inputs != PAD_ID)\n    }\n```", "```py\ndef hf_main(\n    config,\n    collate_fn=hf_collate_with_padding,\n    compile=False\n):\n    torch.random.manual_seed(0)\n    device = torch.device(DEVICE)\n    torch.set_float32_matmul_precision(\"high\")\n\n    # Create dataset and dataloader\n    data_set = HuggingFaceFakeDataset()\n    data_loader = DataLoader(\n        data_set,\n        batch_size=BATCH_SIZE,\n        collate_fn=collate_fn,\n        num_workers=12 if DEVICE == \"CUDA\" else 0,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    model = GPT2LMHeadModel(config).to(device)\n\n    if compile:\n        model = torch.compile(model)\n\n    # Define loss and optimizer\n    criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_ID)\n    optimizer = torch.optim.SGD(model.parameters())\n\n    model.train()\n\n    t0 = time.perf_counter()\n    summ = 0\n    count = 0\n\n    for step, data in enumerate(data_loader):\n        # Copy data to GPU\n        data = data_to_device(data, device=device)\n        input_ids = data['input_ids']\n        labels = data['labels']\n        position_ids = data.get('position_ids')\n        attn_mask = data.get('attention_mask')\n        with torch.amp.autocast(DEVICE, dtype=torch.bfloat16):\n            outputs = model(input_ids=input_ids,\n                            position_ids=position_ids,\n                            attention_mask=attn_mask)\n            logits = outputs.logits[..., :-1, :].contiguous()\n            labels = labels[..., 1:].contiguous()\n            loss = criterion(logits.view(-1, NUM_TOKENS), labels.flatten())\n\n        optimizer.zero_grad(set_to_none=True)\n        loss.backward()\n        optimizer.step()\n\n        # Capture step time\n        batch_time = time.perf_counter() - t0\n        if step > 20:  # Skip first steps\n            summ += batch_time\n            count += 1\n        t0 = time.perf_counter()\n        if step >= 100:\n            break\n    print(f'average step time: {summ / count}')\n```", "```py\nconfig = GPT2Config(\n        n_layer=DEPTH,\n        n_embd=DIM,\n        n_head=NUM_HEADS,\n        vocab_size=NUM_TOKENS,\n    )\n\nfor compile in [False, True]:\n    print(f\"HF GPT2 train with SDPA, compile={compile}\")\n    hf_main(config=config, compile=compile)\n```", "```py\nflash_config = GPT2Config(\n        n_layer=DEPTH,\n        n_embd=DIM,\n        n_head=NUM_HEADS,\n        vocab_size=NUM_TOKENS,\n        attn_implementation='flash_attention_2'\n    )\n\nprint(f\"HF GPT2 train with flash\")\nhf_main(config=flash_config)\n```", "```py\n@@ -370,0 +371 @@\n+        position_ids = None\n@@ -444,0 +446 @@\n+            position_ids=position_ids\n@@ -611,0 +614 @@\n+        position_ids=None\n@@ -621,0 +625 @@\n+            position_ids=position_ids\n@@ -1140,0 +1145 @@\n+                    position_ids=position_ids\n```", "```py\ndef collate_flatten(batch):\n    input_ids = torch.concat([b['input_ids'] for b in batch]).unsqueeze(0)\n    labels = torch.concat([b['labels'] for b in batch]).unsqueeze(0)\n    position_ids = [torch.arange(b['input_ids'].shape[0]) for b in batch]\n    position_ids = torch.concat(position_ids)\n\n    return {\n        'input_ids': input_ids,\n        'labels': labels,\n        'position_ids': position_ids\n    }\n\nprint(f\"HF GPT2 train with flash, no padding\")\nhf_main(config=flash_config, collate_fn=collate_flatten)\n```"]