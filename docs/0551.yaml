- en: Bounded Kernel Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/bounded-kernel-density-estimation-2082dff3f47f?source=collection_archive---------7-----------------------#2024-02-28](https://towardsdatascience.com/bounded-kernel-density-estimation-2082dff3f47f?source=collection_archive---------7-----------------------#2024-02-28)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Learn how Kernel Density Estimation works and how you can adjust it to better
    handle bounded data, like age, height, or price*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@thom01.rouch?source=post_page---byline--2082dff3f47f--------------------------------)[![Thomas
    Rouch](../Images/a8440bbed59cd8d9cdd752cf1fea2831.png)](https://medium.com/@thom01.rouch?source=post_page---byline--2082dff3f47f--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--2082dff3f47f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--2082dff3f47f--------------------------------)
    [Thomas Rouch](https://medium.com/@thom01.rouch?source=post_page---byline--2082dff3f47f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--2082dff3f47f--------------------------------)
    ·9 min read·Feb 28, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5227621375cf4dca482a337bcfaf6180.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Maxim Berg](https://unsplash.com/@maxberg?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Histograms are widely used and easily grasped, but when it comes to estimating
    continuous densities, people often resort to treating it as a mysterious black
    box. However, understanding this concept is just as straightforward and becomes
    crucial, especially when dealing with bounded data like age, height, or price,
    where available libraries may not handle it automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Kernel Density Estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A histogram involves partitioning the data range into bins or sub-intervals
    and counting the number of samples that fall within each bin. It thus approximates
    the continuous density function with a piecewise constant function.
  prefs: []
  type: TYPE_NORMAL
- en: '***Large bin size***: It helps capturing the low-frequency outline of the density
    function, by gathering neighboring samples to avoid empty bins. However, it looses
    the continuity property because there could be a significant gap between the count
    of adjacent bins.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Small bin size***: It helps capturing details at higher frequencies. However,
    if the number of samples is too small, we’ll end up with lots of empty bins at
    places where the true density isn’t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fc2efc6f47353bda2c8e052fa20b6281.png)'
  prefs: []
  type: TYPE_IMG
- en: Histogram of 100 samples drawn from a Gaussian Distribution, with increasing
    number of bins (5/10/50)— Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Density Estimation (KDE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An intuitive idea is to assume that the density function from which the samples
    are drawn is smooth, and leverage it to fill-in the gaps of our high frequency
    histogram.
  prefs: []
  type: TYPE_NORMAL
- en: This is precisely what the Kernel Density Estimation (KDE) does. It estimates
    the global density as the average of local density kernels K centered around each
    sample. A Kernel is a non-negative function integrating to `1`, e.g uniform, triangular,
    normal… Just like adjusting the bin size in a histogram, we introduce a bandwidth
    parameter `h` that modulates the deviation of the kernel around each sample point.
    It thus controls the smoothness of the resulting density estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac11595d755e2dce6d7d8cb08639a66e.png)'
  prefs: []
  type: TYPE_IMG
- en: Bandwith Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the right balance between under- and over-smoothing isn’t straightforward.
    A popular and easy-to-compute heuristic is the Silverman’s rule of thumb, which
    is optimal when the underlying density being estimated is Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f679c8d8847706e9df45beb35d899d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Silverman’s Rule of thumb, with n the number of samples and sigma the standard
    deviation of the samples
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that it may not always yield optimal results across all data distributions.
    I won’t discuss them in this article, but there are other alternatives and improvements
    available.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The image below depicts a Gaussian distribution being estimated by a gaussian
    KDE at different bandwidth values. As we can see, Silverman’s rule of thumb is
    well-suited, but higher bandwidths cause over-smoothing, while lower ones introduce
    high-frequency oscillations around the true density.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbaf3cf096dbb269df2384b94d1ce347.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Kernel Density Estimation from 100 samples drawn from a true gaussian
    distribution, for different bandwith parameters (0.01, 0.04, 0.10) — Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The video below illustrates the convergence of a Kernel Density Estimation with
    a Gaussian kernel across 4 standard density distributions as the number of provided
    samples increases.
  prefs: []
  type: TYPE_NORMAL
- en: Although it’s not optimal, I’ve chosen to keep a small constant bandwidth `h`
    over the video to better illustrate the process of kernel averaging, and to prevent
    excessive smoothing when the sample size is very small.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a4e90d2578ba746cad4effc4bc365e1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Convergence of Gaussian Kernel Density Estimation across 4 Standard Density
    Distributions (Uniform, Triangular, Gaussian, Gaussian Mixture) with Increasing
    Sample Size. — Video by the author
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of Gaussian Kernel Density Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Great python libraries like `scipy` and `scikit-learn` provide public implementations
    for Kernel Density Estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`scipy.stats.gaussian_kde`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn.neighbors.KernelDensity`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, it’s valuable to note that a basic equivalent can be built in just
    three lines using `numpy`. We need the samples `x_data` drawn from the distribution
    to estimate and the points `x_prediction` at which we want to evaluate the density
    estimate. Then, using array broadcasting we can evaluate a local gaussian kernel
    around each input sample and average them into the final density estimate.
  prefs: []
  type: TYPE_NORMAL
- en: N.B. This version is fast because it’s vectorized. However it involves creating
    a large 2D temporary array of shape `(len(x_data), len(x_prediction))` to store
    all the kernel evaluations. To have a lower memory footprint, we could re-write
    it using `numba` or `cython` (to avoid the computational burden of Python for
    loops) to aggregate kernel evaluations on-the-fly in a running sum for each output
    prediction.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/d70725ddf1f3e1a32622c0dff26ca558.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Parker Coffman](https://unsplash.com/@lowmurmer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Handle boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bounded Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-life data is often bounded by a given domain. For example, attributes such
    as age, weight, or duration are always non-negative values. In such scenarios,
    a standard smooth KDE may fail to accurately capture the true shape of the distribution,
    especially if there’s a density discontinuity at the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: In 1D, with the exception of some exotic cases, bounded distributions typically
    have either one-sided (e.g. positive values) or two-sided (e.g. uniform interval)
    bounded domains.
  prefs: []
  type: TYPE_NORMAL
- en: As illustrated in the graph below, kernels are bad at estimating the edges of
    the uniform distribution and leak outside the bounded domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/512f7dee93cb2ecb811e4eede5ce19db.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian KDE on 100 samples drawn from a uniform distribution — Image by the
    author
  prefs: []
  type: TYPE_NORMAL
- en: No Clean Public Solution in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, popular public Python libraries like `scipy` and `scikit-learn`
    do not currently address this issue. There are existing GitHub issues and pull
    requests discussing this topic, but regrettably, they have remained unresolved
    for quite some time.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Feature Request: KDE for Bounded Data*](https://github.com/scikit-learn/scikit-learn/issues/10108)(GitHub
    Issue scikit-learn — 2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*ENH: improve stats.kde density estimation near boundaries*](https://github.com/scipy/scipy/pull/6114)(GitHub
    Pull Request scipy — 2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In R, `[kde.boundary](https://search.r-project.org/CRAN/refmans/ks/html/kde.boundary.html)`
    allows Kernel density estimate for bounded data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There are various ways to take into account the bounded nature of the distribution.
    Let’s describe the most popular ones: Reflection, Weighting and Transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the sake of readability, we will focus on the unit bounded domain, i.e.
    `*[0,1]*`. Please remember to standardize the data and scale the density appropriately
    in the general case `*[a,b]*`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Solution: Reflection***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The trick is to augment the set of samples by reflecting them across the left
    and right boundaries. This is equivalent to reflecting the tails of the local
    kernels to keep them in the bounded domain. It works best when the density derivative
    is zero at the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The reflection technique also implies processing three times more sample points.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The graphs below illustrate the reflection trick for three standard distributions:
    uniform, right triangle and inverse square root. It does a pretty good job at
    reducing the bias at the boundaries, even for the singularity of the inverse square
    root distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2bee39cfbb23090555e0de197fec3c6.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an uniform distribution, using reflections to handle boundaries— Image
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36fe2400d8dd695f2f442e0a9afad07b.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on a triangle distribution, using reflections to handle boundaries — Image
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/139670d395596d1addd42afb6699aa55.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an inverse square root distribution, using reflections to handle boundaries
    — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: N.B. The signature of `basic_kde` has been slightly updated to allow to optionally
    provide your own bandwidth parameter instead of using the Silverman’s rule of
    thumb.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Solution: Weighting***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reflection trick presented above takes the leaking tails of the local kernel
    and add them back to the bounded domain, so that the information isn’t lost. However,
    we could also compute how much of our local kernel has been lost outside the bounded
    domain and leverage it to correct the bias.
  prefs: []
  type: TYPE_NORMAL
- en: For a very large number of samples, the KDE converges to the convolution between
    the kernel and the true density, truncated by the bounded domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52befe5a923f523514183599c3049017.png)'
  prefs: []
  type: TYPE_IMG
- en: If `x` is at a boundary, then only half of the kernel area will actually be
    used. Intuitively, we’d like to normalize the convolution kernel to make it integrate
    to 1 over the bounded domain. The integral will be close to 1 at the center of
    the bounded interval and will fall off to 0.5 near the borders. This accounts
    for the lack of neighboring kernels at the boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4562347079cb9001fc35e204d4832b16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly to the reflection technique, the graphs below illustrate the weighting
    trick for three standard distributions: uniform, right triangle and inverse square
    root. It performs very similarly to the reflection method.'
  prefs: []
  type: TYPE_NORMAL
- en: From a computational perspective, it doesn’t require to process 3 times more
    samples, but it needs to evaluate the normal Cumulative Density Function at the
    prediction points.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1d0f6328e6867278099f88dc392a045.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an uniform distribution, applying weight on the sides to handle boundaries
    — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b4690f9ddb54c9ba2f021d043ac7ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on a triangular distribution, applying weight on the sides to handle boundaries
    — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81ffd559a6a60a493b75d9bca98c40e7.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an inverse square root distribution, applying weight on the sides to
    handle boundaries — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '***Transformation***'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The transformation trick maps the bounded data to an unbounded space, where
    the KDE can be safely applied. This results in using a different kernel function
    for each input sample.
  prefs: []
  type: TYPE_NORMAL
- en: The logit function leverages the logarithm to map the unit interval `[0,1]`
    to the entire real axis.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edaa6ce64267812567ef794b46968254.png)'
  prefs: []
  type: TYPE_IMG
- en: Logit function — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: When applying a transform `f` onto a random variable `X`, the resulting density
    can be obtained by dividing by the absolute value of the derivative of `f`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/963bad27288d12de9ef24f023f7efbeb.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now apply it for the special case of the logit transform to retrieve
    the density distribution from the one estimated in the logit space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/754b22657314764d9c7120a995fea91a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly to the reflection and weighting techniques, the graphs below illustrate
    the weighting trick for three standard distributions: uniform, right triangle
    and inverse square root. It performs quite poorly by creating large oscillations
    at the boundaries. However, it handles extremely well the singularity of the inverse
    square root.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fadf55b1409f89d75c4e31e09b255a13.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an uniform distribution, computed after mapping the samples to the logit
    space — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/616669a0204e732995da1fed80645535.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on a triangular distribution, computed after mapping the samples to the
    logit space — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45f638457823124a75f24495a53da1a9.png)'
  prefs: []
  type: TYPE_IMG
- en: KDE on an inverse square root distribution, computed after mapping the samples
    to the logit space— Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bffbdc79d5aa04c66af6cb5746a72f95.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Martin Martz](https://unsplash.com/@martz90?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Histograms and KDE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Kernel Density Estimation, each sample is assigned its own local kernel density
    centered around it, and then we average all these densities to obtain the global
    density. The bandwidth parameter defines how far the influence of each kernel
    extends. Intuitively, we should decrease the bandwidth as the number of samples
    increases, to prevent excessive smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms can be seen as a simplified version of KDE. Indeed, the bins implicitly
    define a finite set of possible rectangular kernels, and each sample is assigned
    to the closest one. Finally, the average of all these densities result in a piecewise-constant
    estimate of the global density.
  prefs: []
  type: TYPE_NORMAL
- en: Which method is the best ?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reflection, weighting and transform are efficient basic methods to handle bounded
    data during KDE. However, bear in mind that there isn’t a one-size-fits-all solution;
    it heavily depends on the shape of your data.
  prefs: []
  type: TYPE_NORMAL
- en: The transform method handles pretty well the singularities, as we’ve seen with
    the inverse square root distribution. As for reflection and weighting, they are
    generally more suitable for a broader range of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Reflection introduces complexity during training, whereas weighting adds complexity
    during inference.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From [0,1] to [a,b], [a, +∞[ and ]-∞,b]
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Code presented above has been written for data bounded in the unit interval.
    Don’t forget to scale the density, when applying the affine transformation to
    normalize your data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9caa79b4e4649590d40423a89ea6330.png)'
  prefs: []
  type: TYPE_IMG
- en: It can also easily be adjusted for a one-sided bounded domain, by reflecting
    only on one side, integrating the kernel to infinity on one side or using the
    logarithm instead of logit.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed reading this article and that it gave you more insights on
    how Kernel Density Estimation works and how to handle bounded domains!
  prefs: []
  type: TYPE_NORMAL
