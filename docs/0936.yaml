- en: Quantifying the Complexity and Learnability of Strategic Classification Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化战略分类问题的复杂性和可学习性
- en: 原文：[https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12](https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12](https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12)
- en: How generalizing the notion of VC dimension to a strategic setting can help
    us understand whether or not a problem is learnable
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将VC维度的概念推广到战略设置中，可以帮助我们理解一个问题是否可学习
- en: '[](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Jonathan
    Yahav](../Images/30c3293a94be9258a65c38afd58bb521.png)](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    [Jonathan Yahav](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Jonathan
    Yahav](../Images/30c3293a94be9258a65c38afd58bb521.png)](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    [Jonathan Yahav](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    ·8 min read·Apr 12, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    ·阅读时长8分钟·2024年4月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7bfd19338bed666221665670b5f48bb6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bfd19338bed666221665670b5f48bb6.png)'
- en: Image generated by the author using DALL-E 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用DALL-E 3生成。
- en: In [the first article in this series](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2),
    we formally defined the ***strategic classification problem***, denoted Sᴛʀᴀᴄ⟨*H*,
    *R*, *c*⟩***,*** as a generalization of canonical binary classification. We did
    so based on the paper [***PAC-Learning for Strategic Classification***](https://arxiv.org/abs/2012.03310)
    (Sundaram, Vullikanti, Xu, & Yao, 2021). Along the way, we explored why we should
    care about considering the various preferences of rational agents during classification
    and how we can do so (subject to certain assumptions). We will rely heavily on
    the concepts introduced in the previous article, so I encourage you to read it
    if you haven’t already.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[本系列的第一篇文章](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)，我们正式定义了***战略分类问题***，记作Sᴛʀᴀᴄ⟨*H*,
    *R*, *c*⟩***，***它是经典二元分类的一个推广。我们基于论文[***PAC-Learning for Strategic Classification***](https://arxiv.org/abs/2012.03310)（Sundaram,
    Vullikanti, Xu, & Yao, 2021）来进行此定义。在过程中，我们探讨了为什么在分类时需要考虑理性主体的不同偏好，以及如何在某些假设条件下做到这一点。我们将在很大程度上依赖前一篇文章中介绍的概念，因此如果你还没阅读过，强烈建议你先阅读它。
- en: '[](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Extending PAC Learning to a Strategic Classification Setting'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## 将PAC学习扩展到战略分类设置'
- en: A case study of the meeting point between game theory and fundamental concepts
    in machine learning
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 博弈论与机器学习基础概念交汇的案例研究
- en: towardsdatascience.com](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
- en: We’ll pick up where we left off, **using the definition of Sᴛʀᴀᴄ⟨*H*, *R*, *c*⟩
    as a jumping-off point for the useful concept of strategic VC dimension (SVC).**
    Once we make sense of SVC, what I call *the* *Fundamental Theorem of Strategic
    Learning* will follow rather naturally.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们停下的地方继续，**以 Sᴛʀᴀᴄ⟨*H*, *R*, *c*⟩ 的定义为起点，来引入有用的战略 VC 维度（SVC）概念。** 一旦我们弄清楚了
    SVC，接下来我所称的*战略学习的基本定理*将自然而然地跟随其后。
- en: While helpful, **prior familiarity** with shattering coefficients, the canonical
    VC dimension, and the [Fundamental Theorem of Statistical Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)
    **will not be necessary for you to follow along.** However, there’s far more depth
    to each of them than I could ever hope to cover as part of this series, let alone
    in a single article. The curious reader is referred to [Andrew Rothman](https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------)’s
    wonderful and very thorough articles on the (canonical) shattering coefficient
    and VC dimension.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有帮助，**对扩展系数、经典 VC 维度以及[统计学习基本定理](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)的先前了解**
    **并不是跟得上本系列内容的必要条件。** 然而，它们每个都比我在本系列中所能涉及的内容要深奥得多，更不用说在一篇文章中涵盖了。对于感兴趣的读者，建议阅读[Andrew
    Rothman](https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------)关于（经典）扩展系数和
    VC 维度的精彩且非常详尽的文章。
- en: '[](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 5: Shattering Coefficient'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## 统计学习理论 第 5 部分：扩展系数'
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging the…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用无限函数类的 ML 估计器的一致性、速率和泛化界限证明……
- en: 'anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 6: Vapnik–Chervonenkis (VC) Dimension'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## 统计学习理论 第 6 部分：Vapnik–Chervonenkis（VC）维度
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging the…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用无限函数类的 ML 估计器的一致性、速率和泛化界限证明……
- en: anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)'
- en: As we’ll see, **strategic shattering coefficients and SVC are fairly natural
    generalizations of their canonical (i.e., non-strategic) counterparts.** We will
    therefore begin with a brief rundown of each of those counterparts before explaining
    how they can be modified to work in a strategic setting.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，**战略扩展系数和 SVC 是它们经典（即非战略）对应物的自然推广。** 因此，我们将首先简要介绍这些对应物，然后解释如何将它们修改以适应战略环境。
- en: 'Counting Achievable Labelings: Canonical Shattering Coefficients'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可实现标记的计数：经典扩展系数
- en: 'Verbally defining shattering coefficients seems straightforward at first glance:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 口头定义扩展系数乍一看似乎是直接的：
- en: '*Given a hypothesis class* H*,* ***its* n*ᵗʰ shattering coefficient, denoted*
    Sₙ*(*H*),*** *represents the* ***largest number of labelings achievable by classifiers
    in* H *on a sample of* n *feature vectors.***'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*给定一个假设类* H*，* ***它的第 n*ᵗʰ 扩展系数，记作* Sₙ*(*H*),*** *表示分类器在* H *中能够对* n *个特征向量的样本进行标记的最大数量。*'
- en: But what is a “*labeling*”? And what makes it “*achievable*”? Answering those
    questions will help us lay some groundwork in pursuit of a more formal definition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是“*标记*”？什么才算是“*可实现的*”？回答这些问题将帮助我们为更正式的定义打下基础。
- en: In the context of binary classification, a **labeling** of a sample of feature
    vectors is simply any one of the ways we can assign values from the set { -1,
    1 } to those vectors. As a very simple example, consider two one-dimensional feature
    vectors (i.e., points on a number line), *x*₁ = 1 and *x*₂ = 2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在二分类的背景下，对特征向量样本的**标注**仅仅是将集合 { -1, 1 } 中的值分配给这些向量的任意一种方式。作为一个非常简单的例子，考虑两个一维特征向量（即数轴上的点），*x*₁
    = 1 和 *x*₂ = 2。
- en: '![](../Images/44bc76860772a5a6089a5c11a9bce341.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44bc76860772a5a6089a5c11a9bce341.png)'
- en: A visualization of the four possible labelings of the sample *x*₁ = 1, *x*₂
    = 2\. Red points are negatively classified, blue ones are positively classified.
    Image by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 样本 *x*₁ = 1, *x*₂ = 2 的四种可能标注的可视化。红色点为负类，蓝色点为正类。图像由作者提供。
- en: The possible labelings are any combination of the classification values we can
    assign the individual feature vectors independent of one another. We can represent
    each labeling as a vector, where the first and second coordinate represent the
    values assigned to *x*₁ and *x*₂, respectively. The set of possible labelings
    is thus { (-1, -1), (-1, 1), (1, -1), (1, 1) }. Note that a sample of size 2 yields
    2² = 4 possible labelings — we’ll see how this generalizes to arbitrarily-sized
    samples soon.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可能的标注是我们可以为每个特征向量独立分配分类值的任意组合。我们可以将每个标注表示为一个向量，其中第一个和第二个坐标分别表示分配给 *x*₁ 和 *x*₂
    的值。因此，可能的标注集合为 { (-1, -1), (-1, 1), (1, -1), (1, 1) }。请注意，大小为 2 的样本会产生 2² = 4
    种可能的标注——我们将很快看到这如何推广到任意大小的样本。
- en: We say that **a labeling is *achievable* by a hypothesis class *H*** if there
    exists a classifier *h* ∈ *H* from which that labeling can result. Continuing
    with our simple example, suppose we are limited to classifiers of the form *x*
    ≥ *k*, k∈ ℝ, that is, one-dimensional thresholds such that anything to the right
    of the threshold is positively classified. The labeling (1, -1) is not achievable
    by this hypothesis class. *x*₂ being greater than *x*₁ implies that any threshold
    that classifies *x*₁ positively must do the same for *x*₂. The set of achievable
    labelings is therefore { (-1, -1), (-1, 1), (1, 1) }.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说**一个标注是由假设类 *H* *可实现的***，如果存在一个分类器 *h* ∈ *H*，该分类器能得到该标注。继续我们的简单示例，假设我们仅限于形式为
    *x* ≥ *k*, k∈ ℝ 的分类器，即一维阈值分类器，任何位于阈值右侧的点都会被正类化。标注 (1, -1) 是无法通过此假设类实现的。由于 *x*₂
    大于 *x*₁，任何将 *x*₁ 分类为正类的阈值也必定将 *x*₂ 分类为正类。因此，可实现的标注集合为 { (-1, -1), (-1, 1), (1,
    1) }。
- en: '![](../Images/1e2bc966296645a690f64a4b3d1418fb.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e2bc966296645a690f64a4b3d1418fb.png)'
- en: Examples of one-dimensional threshold classifiers that can be used to achieve
    all but one of the possible labelings of the sample *x*₁ = 1, *x*₂ = 2\. Image
    by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一维阈值分类器的示例，可以用来实现除一个外的所有可能的标注样本 *x*₁ = 1, *x*₂ = 2。图像由作者提供。
- en: Having understood the basic terminology, we can start to develop some notation
    to formally express elements of the verbal definition with which we started.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 理解了基本术语后，我们可以开始发展一些符号来正式表达我们开始时所提到的口头定义中的元素。
- en: '![](../Images/f90d9d885a53417968f0fa5e367aec9e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f90d9d885a53417968f0fa5e367aec9e.png)'
- en: We stick to representing labelings as vectors as we did in our simple example,
    with each coordinate representing the classification value assigned to the corresponding
    feature vector. **There are 2*ⁿ* possible labelings in total:** there are two
    possible choices for each feature vector, and we can think of a labeling as a
    collection of *n* such choices, each made independently of the rest. **If a hypothesis
    class *H* can achieve all possible labelings of a sample 𝒞*ₙ*,** i.e., if the
    number of *achievable* labelings of 𝒞*ₙ* equals 2*ⁿ*, **we say that *H shatters*
    𝒞*ₙ.***
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保持将标注表示为向量，就像在我们的简单示例中一样，每个坐标代表分配给相应特征向量的分类值。**总共有 2*ⁿ* 种可能的标注：**每个特征向量都有两种可能的选择，我们可以将标注看作是
    *n* 个此类选择的集合，每个选择都独立于其他选择。**如果假设类 *H* 能够实现样本 𝒞*ₙ* 的所有可能标注，**即，如果 𝒞*ₙ* 的 *可实现*
    标注的数量等于 2*ⁿ*，**我们说 *H* “打破”了 𝒞*ₙ*。**
- en: 'Finally, using the notation from above, we converge on a more rigorous definition
    of *Sₙ*(*H*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用上面的符号，我们得出更严格的 *Sₙ*(*H*) 定义：
- en: '![](../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png)'
- en: In keeping with our explanation of shattering, ***Sₙ*(*H*) equalling 2*ⁿ* implies
    that there exists a sample of size *n* that is shattered by *H*.**
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结合我们对打破的解释，***Sₙ*(*H*) 等于 2*ⁿ* 意味着存在一个大小为 *n* 的样本被 *H* 打破。**
- en: 'Estimating Hypothesis Class Expressiveness: Canonical VC Dimension'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 估计假设类的表达能力：经典 VC 维度
- en: '**The Vapnik–Chervonenkis (VC) dimension is a way to gauge the expressive power
    of a hypothesis class.** It’s based on the idea of shattering we just defined,
    and it plays an important role in helping us determine which hypothesis classes
    are PAC learnable and which aren’t.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Vapnik–Chervonenkis (VC) 维度是衡量假设类表达能力的一种方式。** 它基于我们刚刚定义的破碎概念，并在帮助我们确定哪些假设类是
    PAC 可学习的、哪些不是方面起着重要作用。'
- en: 'Let’s begin by attempting to intuitively define the canonical VC dimension:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从直观地定义经典的 VC 维度开始：
- en: '*Given a hypothesis class* H*, its VC dimension, denoted VCdim(*H*), is defined
    to be the greatest natural number* n *for which there exists a sample of size*
    n *that is* ***shattered*** *by* H*.*'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*给定一个假设类* H，*其 VC 维度，记作 VCdim(*H*), 被定义为存在一个大小为* n *的样本，其中* n *是最大的自然数，使得该样本被*
    H* ***破碎***。*'
- en: 'Using *Sₙ*(*H*) enables us to express this much more cleanly and succinctly:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *Sₙ*(*H*) 使我们能够更加简洁和清晰地表达这一点：
- en: '*VCdim(*H*)* = *max{* n *∈ ℕ :* Sₙ*(*H*) = 2*ⁿ *}*'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*VCdim(*H*)* = *max{* n *∈ ℕ :* Sₙ*(*H*) = 2*ⁿ *}*'
- en: 'However, this definition isn’t precise. Note that the set of numbers for which
    the shattering coefficient equals 2*ⁿ* may be infinite. (Consequently, it is possible
    that VCdim(*H*) = ∞.) If that’s the case, the set has no well-defined maximum.
    We address this by taking the supremum instead:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一定义并不精确。请注意，对于破碎系数等于 2*ⁿ* 的数字集合可能是无限的。（因此，VCdim(*H*) = ∞ 是有可能的。）如果是这种情况，集合就没有明确的最大值。我们通过取上确界来解决这个问题：
- en: '***VCdim(*H*)* = *sup{* n *∈ ℕ :* Sₙ*(*H*) = 2*ⁿ *}***'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***VCdim(*H*)* = *sup{* n *∈ ℕ :* Sₙ*(*H*) = 2*ⁿ *}***'
- en: This rigorous and concise definition is the one we’ll use moving forward.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个严格而简洁的定义将是我们接下来使用的定义。
- en: 'Adding Preferences to the Mix: Strategic Shattering Coefficients'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向混合中加入偏好：战略性破碎系数
- en: Generalizing the canonical notions we just went over so that they work in a
    strategic setup is fairly straightforward. Redefining shattering coefficients
    in terms of the *data point best response* we defined in [the previous article](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)
    is practically all we’ll have to do.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将我们刚刚讨论的经典概念推广到战略性设置中是相当简单的。只需通过重新定义破碎系数为我们在[上一篇文章](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)中定义的数据点最佳响应，几乎就能完成所有工作。
- en: '*Given a hypothesis class* H, *a preference set* R*, and a cost function* c*,*
    ***the* n*ᵗʰ shattering coefficient of Sᴛʀᴀᴄ⟨*H*,* R*,* c*⟩, denoted σ*ₙ*(*H,
    R, c*),*** *represents* ***the largest number of labelings achievable by classifiers
    in* H** *on a set of* n *potentially-manipulated feature vectors, i.e.,***n *data
    point best responses.***'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*给定一个假设类* H，*一个偏好集合* R，*和一个成本函数* c，* ***Sᴛʀᴀᴄ⟨*H*,* R*,* c⟩的第*n*次破碎系数，记作 σ*ₙ*(*H,
    R, c*),*** *表示* ***由* H* 中的分类器在一组* n *个可能被操控的特征向量上所能实现的最大标注数，即* n *个数据点最佳响应。***'
- en: 'As a reminder, this is how we defined the data point best response:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，这是我们定义数据点最佳响应的方式：
- en: '![](../Images/069b6c5d55f22bdb773f696053f0c01b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/069b6c5d55f22bdb773f696053f0c01b.png)'
- en: 'We can tweak the notation we used in our discussion of canonical shattering
    coefficients to further formalize this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整在讨论经典破碎系数时使用的符号，以进一步形式化这一点：
- en: '![](../Images/39102a2417d9cf94e79a49dfd64f6df7.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39102a2417d9cf94e79a49dfd64f6df7.png)'
- en: The main difference is that each *x* in the sample has to have a corresponding
    *r*. Other than that, putting the data point best response where we had x in the
    canonical case works smoothly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的区别在于样本中的每个 *x* 必须有一个对应的 *r*。除此之外，在经典情况下，将数据点最佳响应置于 x 处就可以顺利工作。
- en: '**As a quick sanity check, let’s consider what happens if *R* = { 0 }.** The
    realized reward term 𝕀(*h*(*z*) = 1) ⋅ *r* will be 0 across all the data points.
    **Maximizing utility thus becomes synonymous with minimizing cost**. The best
    way to minimize the cost incurred by a data point is trivial: **never manipulating
    its feature vector.**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**作为一个简单的理性检查，让我们考虑如果 *R* = { 0 } 会发生什么。** 实际的奖励项 𝕀(*h*(*z*) = 1) ⋅ *r* 在所有数据点上都将为
    0。**因此，最大化效用就等同于最小化成本**。最简单的减少数据点成本的方式就是：**永远不要操控其特征向量。**'
- en: '**Δ(*x*, *r*; *h*) ends up always just being *x*,** placing us firmly within
    the territory of canonical classification. **It follows that σ*ₙ*(*H*, { 0 },
    *c*) *= Sₙ*(*H*) for all *H, c*.** This is consistent with our observation that
    the impartial preference class represented by *R* = { 0 } is equivalent to canonical
    binary classification.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Δ(*x*, *r*; *h*) 最终总是仅仅是 *x*，** 这将我们牢牢地置于经典分类的领域内。 **因此，σ*ₙ*(*H*, { 0 },
    *c*) *= Sₙ*(*H*) 对所有 *H, c* 都成立。** 这一点与我们观察到的无偏偏好类（由 *R* = { 0 } 表示）等同于经典二分类的观点是一致的。'
- en: 'Expressiveness with Preferences: Strategic VC Dimension (SVC)'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具有偏好性的表达能力：战略VC维度（SVC）
- en: Having defined the *n*ᵗʰstrategic shattering coefficient, **we can simply swap
    out the *Sₙ*(*H*)in the canonical definition of the VC dimension for σ*ₙ*(*H*,
    *R*, *c*).**
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了*n*ᵗʰ战略碎片系数后，**我们只需将标准VC维度定义中的*Sₙ*(*H*)替换为σ*ₙ*(*H*, *R*, *c*)。**
- en: '***SVC(*H, R, c*)* = *sup{* n *∈ ℕ : σ*ₙ*(*H, R, c*) = 2*ⁿ *}***'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***SVC(*H, R, c*)* = *sup{* n *∈ ℕ : σ*ₙ*(*H, R, c*) = 2*ⁿ *}***'
- en: Based on the example we considered above, we find that SVC(*H*, { 0 }, *c*)
    = VCdim(*H*) for any *H*, *c*. Indeed, **SVC is to VCdim as the strategic shattering
    coefficient is to its canonical equivalent:** both are elegant generalizations
    of non-strategic concepts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们上述考虑的例子，我们发现SVC(*H*, { 0 }, *c*) = VCdim(*H*) 对任何 *H*, *c* 都成立。实际上，**SVC与VCdim的关系就像战略碎片系数与其标准版本的关系一样：**两者都是非战略概念的优雅推广。
- en: 'From SVC to Strategic PAC Learnability: The Fundamental Theorem of Strategic
    Learning'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从SVC到战略PAC可学习性：战略学习的基本定理
- en: '**We can now use SVC to state the Fundamental Theorem of Strategic Learning,**
    which relates the complexity of a strategic classification problem to its (agnostic)
    PAC learnability.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们现在可以使用SVC来陈述战略学习的基本定理，** 该定理将战略分类问题的复杂性与其（无关的）PAC可学习性相关联。'
- en: '***A strategic classification instance Sᴛʀᴀᴄ⟨*H*,* R*,* c*⟩ is agnostic PAC
    learnable if and only if SVC(*H, R, c*) is finite.*** *The* [*sample complexity*](https://en.wikipedia.org/wiki/Sample_complexity)
    *for strategic agnostic PAC learning is* **m*(*δ*,* ε*) ≤* Cε *⁻² ⋅* *(SVC(*H,
    R, c*) + log⁡(1/*δ*))****, with* C *being a constant.*'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***一个战略分类实例Sᴛʀᴀᴄ⟨*H*,* R*,* c*⟩是无关PAC可学习的，当且仅当SVC(*H, R, c*)是有限的。*** *战略无关PAC学习的*
    [*样本复杂度*](https://en.wikipedia.org/wiki/Sample_complexity) *是* **m*(*δ*,* ε*)
    ≤* Cε *⁻² ⋅* *(SVC(*H, R, c*) + log⁡(1/*δ*))****，其中*C*是常数。**'
- en: We won’t elaborate too much on how this can be proven. Suffice it to say that
    it boils down to a clever reduction to the (well-documented) [Fundamental Theorem
    of *Statistical* Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf),
    which is essentially the non-strategic version of the theorem. If you’re mathematically
    inclined and interested in the nuts and bolts of the proof, you can find it in
    [Appendix B of the paper](https://arxiv.org/pdf/2012.03310.pdf#page=12).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细阐述这一点如何被证明。只需说它归结为一个巧妙的简化，转化为（有充分文献记载的）[*统计*学习的基本定理](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)，该定理本质上是该定理的非战略版本。如果你对数学有兴趣并且想了解证明的细节，可以在[论文的附录B](https://arxiv.org/pdf/2012.03310.pdf#page=12)中找到相关内容。
- en: '**This theorem essentially completes our generalization of classic PAC learning
    to a strategic classification setting.** It shows that the way we defined SVC
    actually doesn’t just make sense in our heads; it actually works as a generalization
    of VCdim where it matters most. Armed with the Fundamental Theorem, we are well-equipped
    to analyze strategic classification problems much as we would any old binary classification
    problem. In my opinion, having the ability to determine whether a strategic problem
    is theoretically learnable or not is pretty incredible!'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个定理基本上完成了我们将经典PAC学习推广到战略分类环境的工作。** 它表明，我们定义的SVC不仅在我们的脑海中是合理的；它实际上作为VCdim的一个推广在最关键的地方是有效的。凭借基本定理的支持，我们完全可以像分析任何传统的二分类问题一样分析战略分类问题。在我看来，能够判断一个战略问题是否可以理论上学习，实在是非常了不起！'
- en: Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We began by introducing the **canonical shattering coefficient and VC dimension,**
    which are central to PAC learning (and theFundamental Theorem of Statistical Learning).
    Next, we leveraged the *data point best response* to **generalize the aforementioned
    concepts so that they would work in our strategic setup.** We defined the **strategic
    VC dimension (SVC)** and showed that when faced with the impartial preference
    class, it degenerates back into the canonical VC dimension. Finally, **we demonstrated
    how SVC relates to strategic PAC learnability by means of the Fundamental Theorem
    of Strategic Learning.**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了**标准化的碎片系数和VC维度，** 这两个概念是PAC学习（以及统计学习基本定理）的核心。接着，我们利用*数据点最佳反应*来**将上述概念推广，使其适用于我们的战略设置。**我们定义了**战略VC维度（SVC）**并展示了当面对公正偏好类时，它会退化为标准的VC维度。最后，**我们通过战略学习基本定理展示了SVC如何与战略PAC可学习性相关。**
- en: '**In the final article in this series, we’ll build on the concepts I’ve introduced
    here to break down my favorite proof in the paper,** which I think is a beautiful
    illustration of strategic classification and SVC in action.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本系列的最后一篇文章中，我们将基于我在这里介绍的概念，深入剖析论文中我最喜欢的证明，**我认为这是战略分类和SVC应用的一个美丽例证。'
- en: References
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. [PAC-Learning for Strategic
    Classification](https://arxiv.org/abs/2012.03310) (2021), International Conference
    on Machine Learning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. [PAC-学习与战略分类](https://arxiv.org/abs/2012.03310)
    (2021)，国际机器学习会议。'
