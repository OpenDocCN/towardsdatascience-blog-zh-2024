- en: Quantifying the Complexity and Learnability of Strategic Classification Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12](https://towardsdatascience.com/quantifying-the-complexity-and-learnability-of-strategic-classification-problems-fd04cbfdd4b9?source=collection_archive---------8-----------------------#2024-04-12)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How generalizing the notion of VC dimension to a strategic setting can help
    us understand whether or not a problem is learnable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Jonathan
    Yahav](../Images/30c3293a94be9258a65c38afd58bb521.png)](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    [Jonathan Yahav](https://jhyahav.medium.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--fd04cbfdd4b9--------------------------------)
    ¬∑8 min read¬∑Apr 12, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bfd19338bed666221665670b5f48bb6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using DALL-E 3.
  prefs: []
  type: TYPE_NORMAL
- en: In [the first article in this series](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2),
    we formally defined the ***strategic classification problem***, denoted S·¥õ Ä·¥Ä·¥Ñ‚ü®*H*,
    *R*, *c*‚ü©***,*** as a generalization of canonical binary classification. We did
    so based on the paper [***PAC-Learning for Strategic Classification***](https://arxiv.org/abs/2012.03310)
    (Sundaram, Vullikanti, Xu, & Yao, 2021). Along the way, we explored why we should
    care about considering the various preferences of rational agents during classification
    and how we can do so (subject to certain assumptions). We will rely heavily on
    the concepts introduced in the previous article, so I encourage you to read it
    if you haven‚Äôt already.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Extending PAC Learning to a Strategic Classification Setting'
  prefs: []
  type: TYPE_NORMAL
- en: A case study of the meeting point between game theory and fundamental concepts
    in machine learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2?source=post_page-----fd04cbfdd4b9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll pick up where we left off, **using the definition of S·¥õ Ä·¥Ä·¥Ñ‚ü®*H*, *R*, *c*‚ü©
    as a jumping-off point for the useful concept of strategic VC dimension (SVC).**
    Once we make sense of SVC, what I call *the* *Fundamental Theorem of Strategic
    Learning* will follow rather naturally.
  prefs: []
  type: TYPE_NORMAL
- en: While helpful, **prior familiarity** with shattering coefficients, the canonical
    VC dimension, and the [Fundamental Theorem of Statistical Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf)
    **will not be necessary for you to follow along.** However, there‚Äôs far more depth
    to each of them than I could ever hope to cover as part of this series, let alone
    in a single article. The curious reader is referred to [Andrew Rothman](https://medium.com/u/4688574fc42a?source=post_page---user_mention--fd04cbfdd4b9--------------------------------)‚Äôs
    wonderful and very thorough articles on the (canonical) shattering coefficient
    and VC dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 5: Shattering Coefficient'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging the‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-5-shattering-coefficient-9fbce2bd98c2?source=post_page-----fd04cbfdd4b9--------------------------------)
    [](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
    [## Statistical Learning Theory Part 6: Vapnik‚ÄìChervonenkis (VC) Dimension'
  prefs: []
  type: TYPE_NORMAL
- en: Proof of Consistency, Rates, and Generalization Bounds for ML Estimators over
    Infinite Function Classes leveraging the‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: anr248.medium.com](https://anr248.medium.com/statistical-learning-theory-part-6-vapnik-chervonenkis-vc-dimension-47848a38b6e7?source=post_page-----fd04cbfdd4b9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: As we‚Äôll see, **strategic shattering coefficients and SVC are fairly natural
    generalizations of their canonical (i.e., non-strategic) counterparts.** We will
    therefore begin with a brief rundown of each of those counterparts before explaining
    how they can be modified to work in a strategic setting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Counting Achievable Labelings: Canonical Shattering Coefficients'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Verbally defining shattering coefficients seems straightforward at first glance:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a hypothesis class* H*,* ***its* n*·µó ∞ shattering coefficient, denoted*
    S‚Çô*(*H*),*** *represents the* ***largest number of labelings achievable by classifiers
    in* H *on a sample of* n *feature vectors.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: But what is a ‚Äú*labeling*‚Äù? And what makes it ‚Äú*achievable*‚Äù? Answering those
    questions will help us lay some groundwork in pursuit of a more formal definition.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of binary classification, a **labeling** of a sample of feature
    vectors is simply any one of the ways we can assign values from the set { -1,
    1 } to those vectors. As a very simple example, consider two one-dimensional feature
    vectors (i.e., points on a number line), *x*‚ÇÅ = 1 and *x*‚ÇÇ = 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/44bc76860772a5a6089a5c11a9bce341.png)'
  prefs: []
  type: TYPE_IMG
- en: A visualization of the four possible labelings of the sample *x*‚ÇÅ = 1, *x*‚ÇÇ
    = 2\. Red points are negatively classified, blue ones are positively classified.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The possible labelings are any combination of the classification values we can
    assign the individual feature vectors independent of one another. We can represent
    each labeling as a vector, where the first and second coordinate represent the
    values assigned to *x*‚ÇÅ and *x*‚ÇÇ, respectively. The set of possible labelings
    is thus { (-1, -1), (-1, 1), (1, -1), (1, 1) }. Note that a sample of size 2 yields
    2¬≤ = 4 possible labelings ‚Äî we‚Äôll see how this generalizes to arbitrarily-sized
    samples soon.
  prefs: []
  type: TYPE_NORMAL
- en: We say that **a labeling is *achievable* by a hypothesis class *H*** if there
    exists a classifier *h* ‚àà *H* from which that labeling can result. Continuing
    with our simple example, suppose we are limited to classifiers of the form *x*
    ‚â• *k*, k‚àà ‚Ñù, that is, one-dimensional thresholds such that anything to the right
    of the threshold is positively classified. The labeling (1, -1) is not achievable
    by this hypothesis class. *x*‚ÇÇ being greater than *x*‚ÇÅ implies that any threshold
    that classifies *x*‚ÇÅ positively must do the same for *x*‚ÇÇ. The set of achievable
    labelings is therefore { (-1, -1), (-1, 1), (1, 1) }.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e2bc966296645a690f64a4b3d1418fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of one-dimensional threshold classifiers that can be used to achieve
    all but one of the possible labelings of the sample *x*‚ÇÅ = 1, *x*‚ÇÇ = 2\. Image
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Having understood the basic terminology, we can start to develop some notation
    to formally express elements of the verbal definition with which we started.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f90d9d885a53417968f0fa5e367aec9e.png)'
  prefs: []
  type: TYPE_IMG
- en: We stick to representing labelings as vectors as we did in our simple example,
    with each coordinate representing the classification value assigned to the corresponding
    feature vector. **There are 2*‚Åø* possible labelings in total:** there are two
    possible choices for each feature vector, and we can think of a labeling as a
    collection of *n* such choices, each made independently of the rest. **If a hypothesis
    class *H* can achieve all possible labelings of a sample ùíû*‚Çô*,** i.e., if the
    number of *achievable* labelings of ùíû*‚Çô* equals 2*‚Åø*, **we say that *H shatters*
    ùíû*‚Çô.***
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, using the notation from above, we converge on a more rigorous definition
    of *S‚Çô*(*H*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/510674c0db78fe8a4edc6c7d2ea05a2e.png)'
  prefs: []
  type: TYPE_IMG
- en: In keeping with our explanation of shattering, ***S‚Çô*(*H*) equalling 2*‚Åø* implies
    that there exists a sample of size *n* that is shattered by *H*.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Estimating Hypothesis Class Expressiveness: Canonical VC Dimension'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**The Vapnik‚ÄìChervonenkis (VC) dimension is a way to gauge the expressive power
    of a hypothesis class.** It‚Äôs based on the idea of shattering we just defined,
    and it plays an important role in helping us determine which hypothesis classes
    are PAC learnable and which aren‚Äôt.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs begin by attempting to intuitively define the canonical VC dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a hypothesis class* H*, its VC dimension, denoted VCdim(*H*), is defined
    to be the greatest natural number* n *for which there exists a sample of size*
    n *that is* ***shattered*** *by* H*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using *S‚Çô*(*H*) enables us to express this much more cleanly and succinctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*VCdim(*H*)* = *max{* n *‚àà ‚Ñï :* S‚Çô*(*H*) = 2*‚Åø *}*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'However, this definition isn‚Äôt precise. Note that the set of numbers for which
    the shattering coefficient equals 2*‚Åø* may be infinite. (Consequently, it is possible
    that VCdim(*H*) = ‚àû.) If that‚Äôs the case, the set has no well-defined maximum.
    We address this by taking the supremum instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '***VCdim(*H*)* = *sup{* n *‚àà ‚Ñï :* S‚Çô*(*H*) = 2*‚Åø *}***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This rigorous and concise definition is the one we‚Äôll use moving forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding Preferences to the Mix: Strategic Shattering Coefficients'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generalizing the canonical notions we just went over so that they work in a
    strategic setup is fairly straightforward. Redefining shattering coefficients
    in terms of the *data point best response* we defined in [the previous article](/extending-pac-learning-to-a-strategic-classification-setting-6c374935dde2)
    is practically all we‚Äôll have to do.
  prefs: []
  type: TYPE_NORMAL
- en: '*Given a hypothesis class* H, *a preference set* R*, and a cost function* c*,*
    ***the* n*·µó ∞ shattering coefficient of S·¥õ Ä·¥Ä·¥Ñ‚ü®*H*,* R*,* c*‚ü©, denoted œÉ*‚Çô*(*H,
    R, c*),*** *represents* ***the largest number of labelings achievable by classifiers
    in* H** *on a set of* n *potentially-manipulated feature vectors, i.e.,***n *data
    point best responses.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As a reminder, this is how we defined the data point best response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/069b6c5d55f22bdb773f696053f0c01b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can tweak the notation we used in our discussion of canonical shattering
    coefficients to further formalize this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39102a2417d9cf94e79a49dfd64f6df7.png)'
  prefs: []
  type: TYPE_IMG
- en: The main difference is that each *x* in the sample has to have a corresponding
    *r*. Other than that, putting the data point best response where we had x in the
    canonical case works smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: '**As a quick sanity check, let‚Äôs consider what happens if *R* = { 0 }.** The
    realized reward term ùïÄ(*h*(*z*) = 1) ‚ãÖ *r* will be 0 across all the data points.
    **Maximizing utility thus becomes synonymous with minimizing cost**. The best
    way to minimize the cost incurred by a data point is trivial: **never manipulating
    its feature vector.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Œî(*x*, *r*; *h*) ends up always just being *x*,** placing us firmly within
    the territory of canonical classification. **It follows that œÉ*‚Çô*(*H*, { 0 },
    *c*) *= S‚Çô*(*H*) for all *H, c*.** This is consistent with our observation that
    the impartial preference class represented by *R* = { 0 } is equivalent to canonical
    binary classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Expressiveness with Preferences: Strategic VC Dimension (SVC)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having defined the *n*·µó ∞strategic shattering coefficient, **we can simply swap
    out the *S‚Çô*(*H*)in the canonical definition of the VC dimension for œÉ*‚Çô*(*H*,
    *R*, *c*).**
  prefs: []
  type: TYPE_NORMAL
- en: '***SVC(*H, R, c*)* = *sup{* n *‚àà ‚Ñï : œÉ*‚Çô*(*H, R, c*) = 2*‚Åø *}***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Based on the example we considered above, we find that SVC(*H*, { 0 }, *c*)
    = VCdim(*H*) for any *H*, *c*. Indeed, **SVC is to VCdim as the strategic shattering
    coefficient is to its canonical equivalent:** both are elegant generalizations
    of non-strategic concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'From SVC to Strategic PAC Learnability: The Fundamental Theorem of Strategic
    Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**We can now use SVC to state the Fundamental Theorem of Strategic Learning,**
    which relates the complexity of a strategic classification problem to its (agnostic)
    PAC learnability.'
  prefs: []
  type: TYPE_NORMAL
- en: '***A strategic classification instance S·¥õ Ä·¥Ä·¥Ñ‚ü®*H*,* R*,* c*‚ü© is agnostic PAC
    learnable if and only if SVC(*H, R, c*) is finite.*** *The* [*sample complexity*](https://en.wikipedia.org/wiki/Sample_complexity)
    *for strategic agnostic PAC learning is* **m*(*Œ¥*,* Œµ*) ‚â§* CŒµ *‚Åª¬≤ ‚ãÖ* *(SVC(*H,
    R, c*) + log‚Å°(1/*Œ¥*))****, with* C *being a constant.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We won‚Äôt elaborate too much on how this can be proven. Suffice it to say that
    it boils down to a clever reduction to the (well-documented) [Fundamental Theorem
    of *Statistical* Learning](https://www.cs.princeton.edu/courses/archive/spring16/cos511/lec17.pdf),
    which is essentially the non-strategic version of the theorem. If you‚Äôre mathematically
    inclined and interested in the nuts and bolts of the proof, you can find it in
    [Appendix B of the paper](https://arxiv.org/pdf/2012.03310.pdf#page=12).
  prefs: []
  type: TYPE_NORMAL
- en: '**This theorem essentially completes our generalization of classic PAC learning
    to a strategic classification setting.** It shows that the way we defined SVC
    actually doesn‚Äôt just make sense in our heads; it actually works as a generalization
    of VCdim where it matters most. Armed with the Fundamental Theorem, we are well-equipped
    to analyze strategic classification problems much as we would any old binary classification
    problem. In my opinion, having the ability to determine whether a strategic problem
    is theoretically learnable or not is pretty incredible!'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We began by introducing the **canonical shattering coefficient and VC dimension,**
    which are central to PAC learning (and theFundamental Theorem of Statistical Learning).
    Next, we leveraged the *data point best response* to **generalize the aforementioned
    concepts so that they would work in our strategic setup.** We defined the **strategic
    VC dimension (SVC)** and showed that when faced with the impartial preference
    class, it degenerates back into the canonical VC dimension. Finally, **we demonstrated
    how SVC relates to strategic PAC learnability by means of the Fundamental Theorem
    of Strategic Learning.**
  prefs: []
  type: TYPE_NORMAL
- en: '**In the final article in this series, we‚Äôll build on the concepts I‚Äôve introduced
    here to break down my favorite proof in the paper,** which I think is a beautiful
    illustration of strategic classification and SVC in action.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] R. Sundaram, A. Vullikanti, H. Xu, F. Yao. [PAC-Learning for Strategic
    Classification](https://arxiv.org/abs/2012.03310) (2021), International Conference
    on Machine Learning.'
  prefs: []
  type: TYPE_NORMAL
