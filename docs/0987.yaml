- en: Evaluate anything you want | Creating advanced evaluators with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/evaluate-anything-you-want-creating-advanced-evaluators-with-llms-e2d540af6090?source=collection_archive---------7-----------------------#2024-04-18](https://towardsdatascience.com/evaluate-anything-you-want-creating-advanced-evaluators-with-llms-e2d540af6090?source=collection_archive---------7-----------------------#2024-04-18)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Discover how to build custom LLM evaluators for specific real-world needs.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@nikita_kiselov?source=post_page---byline--e2d540af6090--------------------------------)[![Nikita
    Kiselov](../Images/7a9cca0418a58e35e1024d59a8c634bf.png)](https://medium.com/@nikita_kiselov?source=post_page---byline--e2d540af6090--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--e2d540af6090--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--e2d540af6090--------------------------------)
    [Nikita Kiselov](https://medium.com/@nikita_kiselov?source=post_page---byline--e2d540af6090--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--e2d540af6090--------------------------------)
    ·7 min read·Apr 18, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3313d1ece51ad97e632d0c90a8f0df0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by DALLE-3 | Robot Inspections in the isometric style
  prefs: []
  type: TYPE_NORMAL
- en: Considering the rapid advancements in the field of LLM “chains”, “agents”, chatbots
    and other use cases of text-generative AI, evaluating the performance of language
    models is crucial for understanding their capabilities and limitations. Especially
    crucial to be able to adapt those metrics according to the business goals.
  prefs: []
  type: TYPE_NORMAL
- en: While standard metrics like perplexity, BLEU scores and Sentence distance provide
    a general indication of model performance, based on my experience, **they often
    underperform in capturing the nuances and specific requirements of real-world
    applications.**
  prefs: []
  type: TYPE_NORMAL
- en: For example, take a simple RAG QA application. When building a question-answering
    system, factors of the so-called [“**RAG Triad**”](https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/)
    like context relevance, groundedness in facts, and language consistency between
    the query and response are important as well. Standard metrics simply cannot capture
    these nuanced aspects effectively.
  prefs: []
  type: TYPE_NORMAL
- en: This is where LLM-based “Blackbox” metrics come in handy. While the idea can
    sound naive the concept behind LLM-based “blackbox” metrics is quite compelling.
    **These metrics utilise the power of large language models themselves to evaluate
    the**…
  prefs: []
  type: TYPE_NORMAL
