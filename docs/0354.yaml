- en: Automatic Labeling With GroundingDino
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GroundingDino进行自动标注
- en: 原文：[https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06](https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06](https://towardsdatascience.com/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?source=collection_archive---------3-----------------------#2024-02-06)
- en: A practical guide to tag object detection datasets with the GroundingDino algorithm.
    Code included.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本文是一个实用指南，讲解如何使用GroundingDino算法标注物体检测数据集。包括代码。
- en: '[](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)[![Lihi
    Gur Arie, PhD](../Images/7a1eb30725a95159401c3672fa5f43ab.png)](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)
    [Lihi Gur Arie, PhD](https://medium.com/@lihigurarie?source=post_page---byline--b66c486656fe--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)
    ·6 min read·Feb 6, 2024
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--b66c486656fe--------------------------------)
    ·6分钟阅读·2024年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/18f9e79654ab0663260df8535cdebe7f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18f9e79654ab0663260df8535cdebe7f.png)'
- en: Annotations by the author using GroundingDino with the ‘ripened tomato’ prompt.
    Image by [Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用GroundingDino并输入“成熟番茄”提示进行标注。图像由[Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/)提供。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Until recently, object detection models performed a specific task, like detecting
    penguins in an image. However, recent advancements in deep learning have given
    rise to foundation models. These are large models trained on massive datasets
    in a general manner, making them adaptable for a wide range of tasks. Examples
    of such models include [CLIP](https://medium.com/towards-data-science/clip-creating-image-classifiers-without-data-b21c72b741fa)
    for image classification, SAM for segmentation, and GroundingDino for object detection.
    Foundation models are generally large and computationally demanding. When having
    no resources limitations, they can be used directly for zero-shot inference. Otherwise,
    they can be used to tag a datasets for training a smaller, more specific model
    in a process known as distillation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，物体检测模型执行的是特定任务，比如检测图像中的企鹅。然而，深度学习的最新进展催生了基础模型。这些模型是在庞大的数据集上以通用方式训练的大型模型，使它们能够适应各种任务。例如，像[CLIP](https://medium.com/towards-data-science/clip-creating-image-classifiers-without-data-b21c72b741fa)这样的模型用于图像分类，SAM用于分割，GroundingDino用于物体检测。基础模型通常较大且计算要求高。如果没有资源限制，它们可以直接用于零-shot推理。否则，它们可以用于标注数据集，以训练一个更小、更具体的模型，这一过程称为蒸馏。
- en: In this guide, we’ll learn how to use GroundingDino model for zero-shot inference
    of a tomatoes image. We’ll explore the algorithm’s capabilities and use it to
    tag an entire tomato dataset. The resulted dataset can then be used to train a
    downstream target model such as YOLO.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，我们将学习如何使用GroundingDino模型进行番茄图像的零-shot推理。我们将探索该算法的能力，并利用它标注整个番茄数据集。得到的数据集随后可以用于训练下游目标模型，如YOLO。
- en: If you don’t have a paid Medium account, you can read for free [here](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e).
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你没有付费的Medium账号，可以在[这里](/automatic-labeling-of-object-detection-datasets-using-groundingdino-b66c486656fe?sk=7c98df89b60ea49a6de9efd5278f645e)免费阅读。
- en: GroundingDino
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GroundingDino
- en: '***Background***'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '***背景***'
- en: GroundingDino is a state-of-the-art (SOTA) algorithm developed by IDEA-Research
    in 2023 [1]. It detects objects from images using text prompts. The name “GroundingDino”
    is a combination of “grounding” (a process that links vision and language understanding
    in AI systems) and the transformer-based detector “DINO” [2]. This algorithm is
    a zero-shot object detector, which means it can identify objects from categories
    it was not specifically trained on, without needing to see any examples (shots).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GroundingDino是由IDEA-Research在2023年开发的最先进（SOTA）算法[1]。它通过文本提示从图像中检测物体。名称“GroundingDino”结合了“grounding”（一个将视觉和语言理解连接在AI系统中的过程）和基于变换器的检测器“DINO”[2]。该算法是一个零-shot物体检测器，这意味着它可以识别那些它没有专门训练过的类别的物体，而无需看到任何示例（shot）。
- en: '***Architecture***'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '***架构***'
- en: The model takes pairs of image and text description as inputs.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型接收图像和文本描述的配对作为输入。
- en: Image features are extracted with an **image backbone** such as Swin Transformer,
    and text features with a **text backbone** like BERT.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像特征通过**图像骨干网络**（如Swin Transformer）提取，文本特征通过**文本骨干网络**（如BERT）提取。
- en: The **Feature Enhancer** module combines the text and image features through
    multimodal refinment, using cross attention mechanism that foster interaction
    between these two modalities.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征增强器**模块通过多模态精细化结合文本和图像特征，使用交叉注意机制促进这两种模态之间的互动。'
- en: Next, the ‘**Language-guided Query Selection**’ module selects the features
    most relevant to the input text to use as decoder queries.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，‘**语言引导查询选择**’模块选择与输入文本最相关的特征作为解码器查询。
- en: These queries are then fed into a **decoder** to refine the prediction of object
    detection boxes that best align with the text information. It outputs the final
    bounding boxes suggestions.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，这些查询被输入到**解码器**中，以精细调整与文本信息最佳对齐的物体检测框的预测。它输出最终的边界框建议。
- en: The model outputs 900 object bounding boxes and their similarity scores to the
    input words. The boxes with similarity scores above the `box_threshold` are chosen,
    and words whose similarities are higher than the `text_threshold` as predicted
    labels.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型输出900个物体边界框及其与输入文本的相似度分数。相似度分数高于`box_threshold`的框会被选中，且相似度高于`text_threshold`的单词会作为预测标签。
- en: '![](../Images/216264702fee5eb326e11e64b8d85eda.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/216264702fee5eb326e11e64b8d85eda.png)'
- en: Image by Xiangyu et al., 2023 [3]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由Xiangyu等人制作，2023年[3]
- en: '***Prompt Engineering***'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***提示工程***'
- en: The GroundingDino model encodes text prompts into a learned latent space. Altering
    the prompts can lead to different text features, which can affect the performance
    of the detector. To enhance prediction performance, it’s advisable to experiment
    with multiple prompts, choosing the one that delivers the best results. It’s important
    to note that while writing this article I had to try several prompts before finding
    the ideal one, sometimes encountering unexpected results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: GroundingDino模型将文本提示编码为一个学习到的潜在空间。改变提示可以产生不同的文本特征，这会影响检测器的性能。为了增强预测性能，建议尝试多个提示，选择一个能提供最佳结果的提示。值得注意的是，在写这篇文章时，我必须尝试多个提示，才能找到理想的那个，有时还会遇到意外的结果。
- en: Code Implementation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: '***Getting Started***'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '***开始使用***'
- en: To begin, we’ll clone the [GroundingDino repository](https://github.com/IDEA-Research/GroundingDINO)
    from GitHub, set up the environment by installing the necessary dependencies,
    and download the pre-trained model weights.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从GitHub克隆[GroundingDino仓库](https://github.com/IDEA-Research/GroundingDINO)，通过安装必要的依赖项来设置环境，并下载预训练的模型权重。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***Inference on an image***'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***在图像上的推理***'
- en: We’ll start our exploration of the object detection algorithm by applying it
    to a single image of tomatoes. Our initial goal is to detect all the tomatoes
    in the image, so we’ll use the text prompt `tomato`. If you want to use different
    category names, you can separate them with a dot `.`. Note that the colors of
    the bounding boxes are random and have no particular meaning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将物体检测算法应用于一张番茄的图像来开始探索。我们的初步目标是检测图像中的所有番茄，因此我们将使用文本提示`tomato`。如果你想使用不同的类别名称，可以用点号`.`将它们分开。请注意，边界框的颜色是随机的，并没有特定的含义。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/f8837e49d2c89df34eedfa619d8021a4.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8837e49d2c89df34eedfa619d8021a4.png)'
- en: Annotations with the ‘**tomato**’ prompt. Image by [Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用‘**tomato**’提示的注释。图片由[Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/)提供。
- en: 'GroundingDino not only detects objects as categories, such as tomato, but also
    comprehends the input text, a task known as Referring Expression Comprehension
    (REC). Let’s change the text prompt from `tomato` to `ripened tomato`, and obtain
    the outcome:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GroundingDino 不仅将物体检测为类别（如番茄），还能够理解输入文本，这一任务被称为指代表达理解（Referring Expression Comprehension，简称REC）。让我们将文本提示从`番茄`改为`成熟番茄`，并获得以下结果：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ffd779c03397578fc361f8935242b524.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffd779c03397578fc361f8935242b524.png)'
- en: Annotations with the ‘**ripened tomato**’ prompt. Image by [Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用‘**成熟番茄**’提示的标注。图片来自[Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/)。
- en: Remarkably, the model can ‘understand’ the text and differentiate between a
    ‘tomato’ and a ‘ripened tomato’. It even tags partially ripened tomatoes that
    aren’t fully red. If our task requires tagging only fully ripened red tomatoes,
    we can adjust the `box_threshold` from the default 0.35 to 0.5.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，模型能够“理解”文本，并区分“番茄”和“成熟的番茄”。它甚至会标注部分成熟但还没有完全变红的番茄。如果我们的任务只需要标注完全成熟的红色番茄，我们可以将`box_threshold`从默认值0.35调整为0.5。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/18f9e79654ab0663260df8535cdebe7f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18f9e79654ab0663260df8535cdebe7f.png)'
- en: Annotations with the ‘**ripened tomato**’ prompt, with `**box_threshold = 0.5**`.
    Image by [Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用‘**成熟番茄**’提示和`**box_threshold = 0.5**`的标注。图片来自[Markus Spiske](https://www.pexels.com/photo/green-and-red-oval-fruits-965740/)。
- en: '***Generation of tagged dataset***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***标注数据集的生成***'
- en: Even though GroundingDino has remarkable capabilities, it’s a large and slow
    model. If real-time object detection is needed, consider using a faster model
    like YOLO. Training YOLO and similar models require a lot of tagged data, which
    can be expensive and time-consuming to produce. However, if your data isn’t unique,
    you can use GroundingDino to tag it. To learn more about efficient YOLO training,
    refer to my previous article [[4]](https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GroundingDino 拥有出色的能力，但它是一个大型且较慢的模型。如果需要实时物体检测，可以考虑使用像 YOLO 这样的更快模型。训练 YOLO
    及类似模型需要大量标注数据，这可能既昂贵又耗时。然而，如果你的数据不是独特的，你可以使用 GroundingDino 来标注数据。欲了解更多关于高效 YOLO
    训练的信息，请参阅我之前的文章 [[4]](https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843)。
- en: The GroundingDino repository includes a script to annotate image datasets in
    the **COCO format**, which is suitable for YOLOx, for instance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GroundingDino 仓库包含一个脚本，用于以**COCO格式**标注图像数据集，这对于 YOLOx 等模型非常适用。
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: export_dataset — If set to True, the COCO format annotations will be saved in
    a directory named ‘coco_dataset’.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: export_dataset — 如果设置为 True，COCO 格式的标注将保存在名为‘coco_dataset’的目录中。
- en: view_dataset — If set to True, the annotated dataset will be displayed for visualization
    in the FiftyOne app.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: view_dataset — 如果设置为 True，标注后的数据集将在 FiftyOne 应用中显示以进行可视化。
- en: export_annotated_images — If set to True, the annotated images will be stored
    in a directory named ‘images_with_bounding_boxes’.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: export_annotated_images — 如果设置为 True，标注后的图像将保存在名为‘images_with_bounding_boxes’的目录中。
- en: subsample (int) — If specified, only this number of images from the dataset
    will be annotated.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: subsample (int) — 如果指定，则仅从数据集中标注这个数量的图像。
- en: Different YOLO algorithms require different annotation formats. If you’re planning
    to train YOLOv5 or YOLOv8, you’ll need to export your dataset in the **YOLOv5
    format**. Although the export type is hard-coded in the main script, you can easily
    change it by adjusting the `dataset_type` argument in `create_coco_dataset.main`,
    from `fo.types.COCODetectionDataset` to `fo.types.YOLOv5Dataset`(line 72). To
    keep things organized, we’ll also change the output directory name from ‘coco_dataset’
    to ‘yolov5_dataset’. After changing the script, run `create_coco_dataset.main`
    again.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 YOLO 算法需要不同的标注格式。如果你计划训练 YOLOv5 或 YOLOv8，你需要将数据集导出为**YOLOv5格式**。尽管主脚本中硬编码了导出类型，但你可以通过调整`create_coco_dataset.main`中的`dataset_type`参数，将其从`fo.types.COCODetectionDataset`更改为`fo.types.YOLOv5Dataset`（第72行）。为了保持组织性，我们还将输出目录名从‘coco_dataset’更改为‘yolov5_dataset’。更改脚本后，重新运行`create_coco_dataset.main`。
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Concluding remarks
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: GroundingDino offers a significant leap in object detection annotations by using
    text prompts. In this tutorial, we have explored how to use the model for automated
    labeling of an image or a whole dataset. It’s crucial, however, to manually review
    and verify these annotations before they are utilized in training subsequent models.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GroundingDino 通过使用文本提示在目标检测标注方面提供了显著的进展。在本教程中，我们探讨了如何使用该模型自动标注图像或整个数据集。然而，在将这些标注用于训练后续模型之前，手动审查和验证这些标注至关重要。
- en: _________________________________________________________________
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: _________________________________________________________________
- en: '*A user-friendly Jupyter notebook containing the complete code is included
    for your convenience:*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*为方便起见，附带了一个包含完整代码的用户友好型 Jupyter 笔记本：*'
- en: Thank you for reading!
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: '**Want to learn more?**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**想了解更多吗？**'
- en: '[**Explore**](https://medium.com/@lihigurarie) additional articles I’ve written'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**探索**](https://medium.com/@lihigurarie) 我撰写的更多文章'
- en: '[**Subscribe**](https://medium.com/@lihigurarie/subscribe)to get notified when
    I publish articles'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**订阅**](https://medium.com/@lihigurarie/subscribe)以便在我发布文章时收到通知'
- en: Follow me on [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [**Linkedin**](https://www.linkedin.com/in/lihi-gur-arie/) 上关注我
- en: References
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[0] Code on Colab Notebook: [link](https://gist.github.com/Lihi-Gur-Arie/90e07b06a0cd756df9b5d29b96f7523a)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[0] Colab 笔记本中的代码：[link](https://gist.github.com/Lihi-Gur-Arie/90e07b06a0cd756df9b5d29b96f7523a)'
- en: '[1] [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set
    Object Detection](https://arxiv.org/pdf/2303.05499.pdf), 2023.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [Grounding DINO：将 DINO 与基础预训练结合用于开放集目标检测](https://arxiv.org/pdf/2303.05499.pdf)，2023年。'
- en: '[2] [Dino: Detr with improved denoising anchor boxes for end-to-end object
    detection](https://arxiv.org/pdf/2203.03605.pdf), 2022.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [Dino：使用改进的去噪锚框进行端到端目标检测](https://arxiv.org/pdf/2203.03605.pdf)，2022年。'
- en: '[3] [An Open and Comprehensive Pipeline for Unified Object Grounding and Detection](https://arxiv.org/pdf/2401.02361v2.pdf),
    2023.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [一个开放且全面的管道，用于统一目标定位和检测](https://arxiv.org/pdf/2401.02361v2.pdf)，2023年。'
- en: '[4] [The practical guide for Object Detection with YOLOv5 algorithm](https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843),
    by Dr. Lihi Gur Arie.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [YOLOv5 算法的目标检测实用指南](https://medium.com/towards-data-science/the-practical-guide-for-object-detection-with-yolov5-algorithm-74c04aac4843)，作者：Dr.
    Lihi Gur Arie。'
