<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Structure and Relationships: Graph Neural Networks and a Pytorch Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Structure and Relationships: Graph Neural Networks and a Pytorch Implementation</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05">https://towardsdatascience.com/structure-and-relationships-graph-neural-networks-and-a-pytorch-implementation-c9d83b71c041?source=collection_archive---------1-----------------------#2024-03-05</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="f045" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Understanding the mathematical background of graph neural networks and implementation for a regression problem in pytorch</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Najib Sharifi, Ph.D." class="l ep by dd de cx" src="../Images/d94932c5e3633e32247d98a3c221b181.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*gpeo9aVzjcetgo_8deQcBw.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@ns650?source=post_page---byline--c9d83b71c041--------------------------------" rel="noopener follow">Najib Sharifi, Ph.D.</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--c9d83b71c041--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Mar 5, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">3</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h1 id="5bea" class="mj mk fq bf ml mm mn gq mo mp mq gt mr ms mt mu mv mw mx my mz na nb nc nd ne bk"><strong class="al">Introduction</strong></h1><p id="4a11" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Interconnected graphical data is all around us, ranging from molecular structures to social networks and design structures of cities. Graph Neural Networks (GNNs) are emerging as a powerful method of modelling and learning the spatial and graphical structure of such data. It has been applied to protein structures and other molecular applications such as drug discovery as well as modelling systems such as social networks. Recently the standard GNN has been combined with ideas from other ML models to develop exciting innovative applications. One such development is the integration of GNN with sequential models — Spatio-Temporal GNN that is able to capture both the temporal and spatial (hence the name) dependences of data, this alone could be applied to a number of challenges/problems in industry/research.</p><p id="a04c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Despite the exciting developments in GNN, there are very few resources on the topic which makes it inaccessible to many. In this short article, I want to provide a brief introduction to GNN covering both the mathematical description as well as a regression problem using the pytorch library. By unraveling the principles behind GNNs, we unlock a deeper comprehension of their capabilities and applications.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="02a8" class="mj mk fq bf ml mm oo gq mo mp op gt mr ms oq mu mv mw or my mz na os nc nd ne bk"><strong class="al">Mathematical Description of GNNs</strong></h1><p id="952f" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">A graph G can be defined as G = (V, E), where V is the set of nodes, and E are the edges between them. A graph is often represented by an adjacency matrix, A, which represents the presence or absence of edges between nodes i.e. aij takes values of 1 to indicate an edge (connection) between nodes i and j or 0 otherwise. If a graph has n nodes, A has a dimension of (n × n). The adjacency matrix is demonstrated in <em class="ot">Figure 1</em>.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov ow"><img src="../Images/d1cd932311a015e28e70b76c55cf5f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9fnllZ5ALkiUOu5iSngYA.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Figure 1. Adjacency matrix for three different graphs</figcaption></figure><p id="f4d6" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Each node (and edges! But we’ll come back to this later for simplicity) will have a set of features (e.g. if the node is a person, the features will be age, gender, height, job etc). If each node has f features, then the feature matrix X is (n × f). In some problems, each node may also have a target label which maybe a set of categorical labels or numerical values (shown in <em class="ot">Figure 2</em>).</p><p id="a385" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Single Node Calculations</strong></p><p id="8ce2" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">To learn the interdependence between any node and its neighbours, we need to consider the features of its neighbours. This is what enables GNNs to learn the structural representation of the data through a graph. Consider a node j with Nj neighbours, GNNs transform the features from each neighbour, aggregate them and then update node i’s feature space. Each of these steps are described as follows.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pn"><img src="../Images/0858dd5d95be8a673a8737ac19200814.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*O3wxhLukZiUgNYKDkAXJOw.png"/></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Figure 2. A schematic of a node (j) with features xj and label yj and neighbour nodes (i, 2, 3), each with their on feature embeddings and corresponding label.</figcaption></figure><p id="45eb" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Neighbour feature transformation could be done a number of ways such as passing through an MLP network or by linear transformation such as</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov po"><img src="../Images/db48d782c7e6519c78c2b446c9387fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*p_vQLmm5Do99uJCiiCpvEQ.png"/></div></figure><p id="36d1" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">where w and b represent the weights and bias of the transformation. Information aggregation, the information from each neighboring nodes are then aggregated:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pp"><img src="../Images/e43f038327837089cafa9a3abafc5bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*uU1ZlezaGWJJDNWuSBzhrg.png"/></div></figure><p id="391d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The nature of the aggregation step could be a number of different methods such as summation, averaging, min/max pooling and concatenation:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pq"><img src="../Images/f59a108b5b916e5cbaef95e8541114e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*_hhJu4gw1aNltI5ZCMBgLw.png"/></div></figure><p id="30cf" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Following the aggregation step, the final step is to update node j:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pr"><img src="../Images/013599689b3480c1ca7ea900b40e5cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*k0ky5yZKaLx7vBPiac_lbg.png"/></div></figure><p id="1828" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This updated could be done using MLP with the concatenated node features and neighbour information aggregation (mj) or we could use linear transformation i.e.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov ps"><img src="../Images/3fa2a673ce468e1b1ea4a29c32af8aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*O2uwZeExkaQ1ke0vdUJGbg.png"/></div></figure><p id="67ac" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Where U is a learnable weights matrix that combines the original node features (xj) with aggregated neighbour features (mj) through a non-linear activation function (ReLU in this case). This is it for the process of updating a single node in a single layer, the same process is applied to all other nodes in the graph, mathematically, this can be presented using the Adjacency matrix.</p><p id="4f9d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Graph Level Calculation</strong></p><p id="cec4" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">For a graph with n nodes and each node has <em class="ot">f</em> features, we can concatenate all the features in a single matrix:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pt"><img src="../Images/9498e7314928b00500195a4c89693cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*vKntzmngwpAeBCXlCi0-DQ.png"/></div></figure><p id="622c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The neighbour feature transformation and aggregation steps can therefore be written as:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pu"><img src="../Images/630123572a8f0bc225a7721705bb0c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*tHpBYps-0rxjQeZZa-qAjQ.png"/></div></figure><p id="1d02" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Where I is the identity matrix, this helps to include the each nodes own features too, otherwise, we are only considering the transformed features from the node j’s neighbours and not it’s own features. One final step is to normalise each node based on the number of connections i.e. for node j with Nj connections, the feature transformation can be done as:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pv"><img src="../Images/7c08e50dcb433ee0320f2e71594840e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*3S4aNcwJKyTz1TG_S27qJQ.png"/></div></figure><p id="ad79" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The equation above can be adjusted as:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pw"><img src="../Images/f92e5051e1e2e2d6438067a44dff65a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*PDtW8DYM6hNj0Ddpjg9aiA.png"/></div></figure><p id="51f9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Where D is the degree matrix, a diagonal matrix of number of connections for each node. However, more commonly, this normalisation step is done as</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov px"><img src="../Images/39326a6cee6a5013846a077ad197452b.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*jnFm87iIi2XMCZsPIUndaw.png"/></div></figure><p id="7a6b" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This is the graph convolution network (GCN) method that enables GNN to learn the structure and relationship between nodes. However, an issue with GCN is that the weight vector for neighbour feature transformation is shared across all neighbours i.e. all neighbours are considered equal, but this is usually not the case so not a good representative of real systems. To address is, graph attention network (GATs) can be used to compute the importance of a neighbour’s feature to the target node, allowing the different neighbours to contribute differently to the feature update of the target node based on their relevance. The attention coefficients are determined using a learnable matrix as follows:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov py"><img src="../Images/d2f4ddfbeb15c5f5c453135d44fdda98.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*tFQyw_CMRqISNyj0fc7_SA.png"/></div></figure><p id="ae27" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Where W is the shared learnable feature linear transformation, Wa is a learnable weight vector and eij is the raw attention score indicating importance of node i’s features to node j. The attention score is normalised using the SoftMax function:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov pz"><img src="../Images/6af7ddab8bee47d14788676c8a01cf14.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*FhhhgewUUSlFJ3-i9sXINw.png"/></div></figure><p id="a9b9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Now the feature aggregation step can be calculated using the attention coefficients:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov qa"><img src="../Images/8a40d22ae79e2116e75022bfdf4fc1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*NNjAXOrfD0HHKDgA8k1rgg.png"/></div></figure><p id="1f67" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This is it for a single layer, we can build multiple layers to increase the complexity of the model, this is demonstrated in <em class="ot">Figure 3</em>. Increasing the number of layers will allow the model to learn more global features and also capture more complex relationships, however, it is also likely to overfit so regularisation techniques should always be used to prevent this.</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov qb"><img src="../Images/781e51a407ec5c36ad5efd87597a4996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pasSIsYpxwX-ke_QtiYngw.png"/></div></div><figcaption class="pi pj pk ou ov pl pm bf b bg z dx">Figure 3. An illustration of a multilayered GNN model</figcaption></figure><p id="bae0" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Finally, once final feature vectors for all nodes are obtained from the network, a feature matrix, H can be formed:</p><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div class="ou ov qc"><img src="../Images/849a73048c3e915b58d2294221aa16f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*3-feiAKUoKMbhgCZu9hIVA.png"/></div></figure><p id="c895" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This feature matrix can be used to do a number of tasks e.g. node or graph classification. This brings us to the end of introduction into the mathematical description of GCN/GATs.</p></div></div></div><div class="ab cb og oh oi oj" role="separator"><span class="ok by bm ol om on"/><span class="ok by bm ol om on"/><span class="ok by bm ol om"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="ad09" class="mj mk fq bf ml mm oo gq mo mp op gt mr ms oq mu mv mw or my mz na os nc nd ne bk"><strong class="al">GCN Regression Example</strong></h1><p id="e018" class="pw-post-body-paragraph nf ng fq nh b go ni nj nk gr nl nm nn no np nq nr ns nt nu nv nw nx ny nz oa fj bk">Let’s implement a regression example where the aim is to train a network to predict the value of a node given the value of all other nodes i.e. each node has a single feature (which is a scalar value). The aim of this example is to leverage the inherent relational information encoded in the graph to accurately predict numerical values for each node. The key thing to note is that we input the numerical value for all nodes except the target node (we mask the target node value with 0) then predict the target node’s value. For each data point, we repeat the process for all nodes. Perhaps this might come across as a bizarre task but lets see if we can predict the expected value of any node given the values of the other nodes. The data used is the corresponding simulation data to a series of sensors from industry and the graph structure I have chosen in the example below is based on the actual process structure. I have provided comments in the code to make it easy to follow. You can find a copy of the dataset <a class="af qd" href="https://github.com/Nsharifi650/GNNRegression.git" rel="noopener ugc nofollow" target="_blank">here</a> (Note: this is my own data, generated from simulations).</p><p id="f224" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This code and training procedure is far from being optimised but it’s aim is to illustrate the implementation of GNNs and get an intuition for how they work. An issue with the currently way I have done that should definitely not be done this way beyond learning purposes is the masking of node feature value and predicting it from the neighbours feature. Currently you’d have to loop over each node (not very efficient), a much better way to do is the stop the model from include it’s own features in the aggregation step and hence you wouldn’t need to do one node at a time but I thought it is easier to build intuition for the model with the current method:)</p><p id="078d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Preprocessing Data</strong></p><p id="7c2d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Importing the necessary libraries and Sensor data from CSV file. Normalise all data in the range of 0 to 1.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="1776" class="qi mk fq qf b bg qj qk l ql qm">import pandas as pd<br/>import torch<br/>from torch_geometric.data import Data, Batch<br/>from sklearn.preprocessing import StandardScaler, MinMaxScaler<br/>from sklearn.model_selection import train_test_split<br/>import numpy as np<br/>from torch_geometric.data import DataLoader<br/><br/># load and scale the dataset<br/>df = pd.read_csv('SensorDataSynthetic.csv').dropna()<br/>scaler = MinMaxScaler()<br/>df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)</span></pre><p id="c962" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Defining the connectivity (edge index) between nodes in the graph using a PyTorch tensor — i.e. this provides the system’s graphical topology.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="b828" class="qi mk fq qf b bg qj qk l ql qm">nodes_order = [<br/>    'Sensor1', 'Sensor2', 'Sensor3', 'Sensor4', <br/>    'Sensor5', 'Sensor6', 'Sensor7', 'Sensor8'<br/>]<br/><br/># define the graph connectivity for the data<br/>edges = torch.tensor([<br/>    [0, 1, 2, 2, 3, 3, 6, 2],  # source nodes<br/>    [1, 2, 3, 4, 5, 6, 2, 7]   # target nodes<br/>], dtype=torch.long)</span></pre><p id="d9d9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The Data imported from csv has a tabular structure but to use this in GNNs, it must be transformed to a graphical structure. Each row of data (one observation) is represented as one graph. Iterate through Each Row to Create Graphical representation of the data</p><p id="e5eb" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">A mask is created for each node/sensor to indicate the presence (1) or absence (0) of data, allowing for flexibility in handling missing data. In most systems, there may be items with no data available hence the need for flexibility in handling missing data. Split the data into training and testing sets</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="d6e4" class="qi mk fq qf b bg qj qk l ql qm">graphs = []<br/><br/># iterate through each row of data to create a graph for each observation<br/># some nodes will not have any data, not the case here but created a mask to allow us to deal with any nodes that do not have data available<br/>for _, row in df_scaled.iterrows():<br/>    node_features = []<br/>    node_data_mask = []<br/>    for node in nodes_order:<br/>        if node in df_scaled.columns:<br/>            node_features.append([row[node]])<br/>            node_data_mask.append(1) # mask value of to indicate present of data<br/>        else:<br/>            # missing nodes feature if necessary<br/>            node_features.append(2)<br/>            node_data_mask.append(0) # data not present<br/>    <br/>    node_features_tensor = torch.tensor(node_features, dtype=torch.float)<br/>    node_data_mask_tensor = torch.tensor(node_data_mask, dtype=torch.float)<br/><br/>    <br/>    # Create a Data object for this row/graph<br/>    graph_data = Data(x=node_features_tensor, edge_index=edges.t().contiguous(), mask = node_data_mask_tensor)<br/>    graphs.append(graph_data)<br/><br/><br/>#### splitting the data into train, test observation<br/># Split indices<br/>observation_indices = df_scaled.index.tolist()<br/>train_indices, test_indices = train_test_split(observation_indices, test_size=0.05, random_state=42)<br/><br/># Create training and testing graphs<br/>train_graphs = [graphs[i] for i in train_indices]<br/>test_graphs = [graphs[i] for i in test_indices]</span></pre><p id="16d9" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Graph Visualisation</strong></p><p id="56f5" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The graph structure created above using the edge indices can be visualised using networkx.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="b733" class="qi mk fq qf b bg qj qk l ql qm">import networkx as nx<br/>import matplotlib.pyplot as plt<br/><br/>G = nx.Graph() <br/>for src, dst in edges.t().numpy():<br/>    G.add_edge(nodes_order[src], nodes_order[dst])<br/><br/>plt.figure(figsize=(10, 8))<br/>pos = nx.spring_layout(G)<br/>nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=2000, font_weight='bold')<br/>plt.title('Graph Visualization')<br/>plt.show()</span></pre><p id="fd3b" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Model Definition</strong></p><p id="9503" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Let’s define the model. The model incorporates 2 GAT convolutional layers. The first layer transforms node features to an 8 dimensional space, and the second GAT layer further reduces this to an 8-dimensional representation.</p><p id="e5c1" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">GNNs are highly susceptible to overfitting, regularation (dropout) is applied after each GAT layer with a user defined probability to prevent over fitting. The dropout layer essentially randomly zeros some of the elements of the input tensor during training.</p><p id="8db0" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The GAT convolution layer output results are passed through a fully connected (linear) layer to map the 8-dimensional output to the final node feature which in this case is a scalar value per node.</p><p id="9872" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Masking the value of the target Node; as mentioned earlier, the aim of this of task is to regress the value of the target node based on the value of it’s neighbours. This is the reason behind masking/replacing the target node’s value with zero.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="e3ce" class="qi mk fq qf b bg qj qk l ql qm">from torch_geometric.nn import GATConv<br/>import torch.nn.functional as F<br/>import torch.nn as nn<br/><br/>class GNNModel(nn.Module):<br/>    def __init__(self, num_node_features):<br/>        super(GNNModel, self).__init__()<br/>        self.conv1 = GATConv(num_node_features, 16)<br/>        self.conv2 = GATConv(16, 8)<br/>        self.fc = nn.Linear(8, 1)  # Outputting a single value per node<br/><br/>    def forward(self, data, target_node_idx=None):<br/>        x, edge_index = data.x, data.edge_index<br/>        edge_index = edge_index.T<br/>        x = x.clone()<br/><br/>        # Mask the target node's feature with a value of zero! <br/>        # Aim is to predict this value from the features of the neighbours<br/>        if target_node_idx is not None:<br/>            x[target_node_idx] = torch.zeros_like(x[target_node_idx])<br/><br/>        x = F.relu(self.conv1(x, edge_index))<br/>        x = F.dropout(x, p=0.05, training=self.training)<br/>        x = F.relu(self.conv2(x, edge_index))<br/>        x = F.relu(self.conv3(x, edge_index))<br/>        x = F.dropout(x, p=0.05, training=self.training)<br/>        x = self.fc(x)<br/><br/>        return x</span></pre><p id="023c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Training the model</strong></p><p id="f850" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Initialising the model and defining the optimiser, loss function and the hyper parameters including learning rate, weight decay (for regularisation), batch_size and number of epochs.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="d1b2" class="qi mk fq qf b bg qj qk l ql qm">model = GNNModel(num_node_features=1) <br/>batch_size = 8<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-6)<br/>criterion = torch.nn.MSELoss()<br/>num_epochs = 200  <br/>train_loader = DataLoader(train_graphs, batch_size=1, shuffle=True) <br/>model.train()</span></pre><p id="103d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">The training process is fairly standard, each graph (one data point) of data is passed through the forward pass of the model (iterating over each node and predicting the target node. The loss from the prediction is accumulated over the defined batch size before updating the GNN through backpropagation.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="2564" class="qi mk fq qf b bg qj qk l ql qm">for epoch in range(num_epochs):<br/>    accumulated_loss = 0 <br/>    optimizer.zero_grad()<br/>    loss = 0  <br/>    for batch_idx, data in enumerate(train_loader):<br/>        mask = data.mask  <br/>        for i in range(1,data.num_nodes):<br/>            if mask[i] == 1:  # Only train on nodes with data<br/>                output = model(data, i)  # get predictions with the target node masked<br/>                                         # check the feed forward part of the model<br/>                target = data.x[i] <br/>                prediction = output[i].view(1) <br/>                loss += criterion(prediction, target)<br/>        #Update parameters at the end of each set of batches<br/>        if (batch_idx+1) % batch_size == 0 or (batch_idx +1 ) == len(train_loader):<br/>            loss.backward() <br/>            optimizer.step()<br/>            optimizer.zero_grad()<br/>            accumulated_loss += loss.item()<br/>            loss = 0<br/><br/>    average_loss = accumulated_loss / len(train_loader)<br/>    print(f'Epoch {epoch+1}, Average Loss: {average_loss}')</span></pre><p id="b421" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Testing the trained model</strong></p><p id="96eb" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Using the test dataset, pass each graph through the forward pass of the trained model and predict each node’s value based on it’s neighbours value.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="d202" class="qi mk fq qf b bg qj qk l ql qm">test_loader = DataLoader(test_graphs, batch_size=1, shuffle=True)<br/>model.eval()<br/><br/>actual = []<br/>pred = []<br/><br/>for data in test_loader:<br/>    mask = data.mask<br/>    for i in range(1,data.num_nodes):<br/>        output = model(data, i)<br/>        prediction = output[i].view(1)<br/>        target = data.x[i]<br/><br/>        actual.append(target)<br/>        pred.append(prediction)</span></pre><p id="65ee" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Visualising the test results</strong></p><p id="32f5" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Using iplot we can visualise the predicted values of nodes against the ground truth values.</p><pre class="ox oy oz pa pb qe qf qg bp qh bb bk"><span id="bc25" class="qi mk fq qf b bg qj qk l ql qm">import plotly.graph_objects as go<br/>from plotly.offline import iplot<br/><br/>actual_values_float = [value.item() for value in actual]<br/>pred_values_float = [value.item() for value in pred]<br/><br/><br/>scatter_trace = go.Scatter(<br/>    x=actual_values_float,<br/>    y=pred_values_float,<br/>    mode='markers',<br/>    marker=dict(<br/>        size=10,<br/>        opacity=0.5,  <br/>        color='rgba(255,255,255,0)',  <br/>        line=dict(<br/>            width=2,<br/>            color='rgba(152, 0, 0, .8)', <br/>        )<br/>    ),<br/>    name='Actual vs Predicted'<br/>)<br/><br/>line_trace = go.Scatter(<br/>    x=[min(actual_values_float), max(actual_values_float)],<br/>    y=[min(actual_values_float), max(actual_values_float)],<br/>    mode='lines',<br/>    marker=dict(color='blue'),<br/>    name='Perfect Prediction'<br/>)<br/><br/>data = [scatter_trace, line_trace]<br/><br/>layout = dict(<br/>    title='Actual vs Predicted Values',<br/>    xaxis=dict(title='Actual Values'),<br/>    yaxis=dict(title='Predicted Values'),<br/>    autosize=False,<br/>    width=800,<br/>    height=600<br/>)<br/><br/>fig = dict(data=data, layout=layout)<br/><br/>iplot(fig)</span></pre><figure class="ox oy oz pa pb pc ou ov paragraph-image"><div role="button" tabindex="0" class="pd pe ed pf bh pg"><div class="ou ov qn"><img src="../Images/cf42365780e85ca2cddbafe8d8993e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*epSyoeIb572GdZmdBX0OSw.png"/></div></div></figure><p id="9f0d" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Despite a lack of fine tuning the model architecture or hyperparameters, it has done a decent job actually, we could tune the model further to get improved accuracy.</p><p id="1306" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">This brings us to the end of this article. GNNs are relatively newer than other branches of machine learning, it will be very exciting to see the developments of this field but also it’s application to different problems. Finally, thank you for taking the time to read this article, I hope you found it useful in your understanding of GNNs or their mathematical background.</p><p id="84c3" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><strong class="nh fr">Before you go</strong></p><p id="828c" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk">Personally I really enjoy spending time to learn new concepts and apply those concepts to new problems and challenges and I am sure most of the people reading these kind of articles do too. I believe its a privilege to have the opportunity to do that, a privilege that everyone should have but not everyone does. We all have a responsibility to change that towards a brighter future for everyone. Please consider donating to UniArk (<a class="af qd" href="http://uniark.org" rel="noopener ugc nofollow" target="_blank">UniArk.org)</a> to help talented students from a demography in the world so often overlooked by universities and countries — the persecuted minority (ethnic, religious or otherwise). UniArk searches the furthest and deepest to find talent and potential — the remote areas of developing countries. Your donation would be the beacon of hope for someone from an oppressive society. I hope you can help UniArk keep this beacon alight.</p><p id="c6e3" class="pw-post-body-paragraph nf ng fq nh b go ob nj nk gr oc nm nn no od nq nr ns oe nu nv nw of ny nz oa fj bk"><em class="ot">Unless otherwise noted, all images are by the author</em></p></div></div></div></div>    
</body>
</html>