- en: Neural Networks For Periodic Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/neural-networks-for-periodic-functions-648cfc940437?source=collection_archive---------6-----------------------#2024-01-17](https://towardsdatascience.com/neural-networks-for-periodic-functions-648cfc940437?source=collection_archive---------6-----------------------#2024-01-17)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When ReLU’s extrapolation capabilities are not enough
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page---byline--648cfc940437--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page---byline--648cfc940437--------------------------------)[](https://towardsdatascience.com/?source=post_page---byline--648cfc940437--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page---byline--648cfc940437--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page---byline--648cfc940437--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page---byline--648cfc940437--------------------------------)
    ·8 min read·Jan 17, 2024
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89e708449fc831a9d049fc0626fb70b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Willian Justen de Vasconcellos](https://unsplash.com/@willianjusten?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks are known to be great approximators for any function — at least
    whenever we **don’t move too far away from our dataset**. Let us see what that
    means. Here is some data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5a4fdd5b9374171dc20837f5f11b53e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'It does not only look like a sine wave, it actually is, with some noise added.
    We can now train a normal feed-forward neural network having 1 hidden layer with
    1000 neurons and ReLU activation. We get the following fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ef85ea759ab6e5791b818e1de3b401d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'It looks quite decent, apart from the edges. We *could* fix this by adding
    more neurons to the hidden layer according to Cybenko’s **universal approximation
    theorem.** But I want to point you something else:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e1658fbdc701d63a0ea6fd8c7bd13f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: We *could* argue now that this extrapolation behavior is **bad** if we assume
    the wave pattern to continue outside of the…
  prefs: []
  type: TYPE_NORMAL
