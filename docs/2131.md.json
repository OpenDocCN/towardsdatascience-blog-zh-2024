["```py\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport time\n\n# A dataset with random images and labels\nclass FakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n\n    def __getitem__(self, index):\n        rand_image = torch.randn([3, 224, 224], dtype=torch.float32)\n        label = torch.tensor(data=index % 10, dtype=torch.uint8)\n        return rand_image, label\n\ntrain_set = FakeDataset()\n\nbatch_size=128\nnum_workers=0\n\ntrain_loader = DataLoader(\n    dataset=train_set,\n    batch_size=batch_size,\n    num_workers=num_workers\n)\n\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters())\nmodel.train()\n\nt0 = time.perf_counter()\nsumm = 0\ncount = 0\n\nfor idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    batch_time = time.perf_counter() - t0\n    if idx > 10:  # skip first steps\n        summ += batch_time\n        count += 1\n    t0 = time.perf_counter()\n    if idx > 100:\n        break\n\nprint(f'average step time: {summ/count}')\nprint(f'throughput: {count*batch_size/summ}')\n```", "```py\nfor idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    with torch.amp.autocast('cpu',dtype=torch.bfloat16):\n        output = model(data)\n        loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n```", "```py\nfor idx, (data, target) in enumerate(train_loader):\n    data = data.to(memory_format=torch.channels_last)\n    optimizer.zero_grad()\n    with torch.amp.autocast('cpu',dtype=torch.bfloat16):\n        output = model(data)\n        loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n```", "```py\nimport intel_extension_for_pytorch as ipex\n\nmodel = torchvision.models.resnet50()\nbackend='inductor' # optionally change to 'ipex'\nmodel = torch.compile(model, backend=backend)\n```", "```py\npython -m torch.backends.xeon.run_cpu train.py\n```", "```py\n model = torchvision.models.resnet50()\n criterion = torch.nn.CrossEntropyLoss()\n optimizer = torch.optim.SGD(model.parameters())\n model.train()\n model, optimizer = ipex.optimize(\n    model, \n    optimizer=optimizer,\n    dtype=torch.bfloat16\n )\n```", "```py\nimport os, time\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.distributed as dist\nimport torchvision\nimport oneccl_bindings_for_pytorch as torch_ccl\nimport intel_extension_for_pytorch as ipex\n\nos.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\nos.environ[\"MASTER_PORT\"] = \"29500\"\nos.environ[\"RANK\"] = os.environ.get(\"PMI_RANK\", \"0\")\nos.environ[\"WORLD_SIZE\"] = os.environ.get(\"PMI_SIZE\", \"1\")\ndist.init_process_group(backend=\"ccl\", init_method=\"env://\")\nrank = os.environ[\"RANK\"]\nworld_size = os.environ[\"WORLD_SIZE\"]\n\nbatch_size = 128\nnum_workers = 0\n\n# define dataset and dataloader\nclass FakeDataset(Dataset):\n    def __len__(self):\n        return 1000000\n\n    def __getitem__(self, index):\n        rand_image = torch.randn([3, 224, 224], dtype=torch.float32)\n        label = torch.tensor(data=index % 10, dtype=torch.uint8)\n        return rand_image, label\n\ntrain_dataset = FakeDataset()\ndist_sampler = DistributedSampler(train_dataset)\ntrain_loader = DataLoader(\n    dataset=train_dataset, \n    batch_size=batch_size,\n    num_workers=num_workers,\n    sampler=dist_sampler\n)\n\n# define model artifacts\nmodel = torchvision.models.resnet50()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters())\nmodel.train()\nmodel, optimizer = ipex.optimize(\n    model, \n    optimizer=optimizer,\n    dtype=torch.bfloat16\n)\n\n# configure DDP\nmodel = torch.nn.parallel.DistributedDataParallel(model)\n\n# run training loop\n\n# destroy the process group\ndist.destroy_process_group() \n```", "```py\nsource $(python -c \"import oneccl_bindings_for_pytorch as torch_ccl;print(torch_ccl.cwd)\")/env/setvars.sh\n\n# This example command would utilize all the numa sockets of the processor, taking each socket as a rank.\nipexrun cpu --nnodes 1 --omp_runtime intel train.py \n```", "```py\nimport torch\nimport torchvision\nimport timeimport torch_xla\nimport torch_xla.core.xla_model as xm\n\ndevice = xm.xla_device()\n\nmodel = torchvision.models.resnet50().to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters())\nmodel.train()\n\nfor idx, (data, target) in enumerate(train_loader):\n    data = data.to(device)\n    target = target.to(device)\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n    xm.mark_step()\n```"]