<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unlocking Hidden Potential: Exploring Second-Round Purchasers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Unlocking Hidden Potential: Exploring Second-Round Purchasers</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unlocking-hidden-potential-exploring-second-round-purchasers-d47958c4d61c?source=collection_archive---------4-----------------------#2024-12-09">https://towardsdatascience.com/unlocking-hidden-potential-exploring-second-round-purchasers-d47958c4d61c?source=collection_archive---------4-----------------------#2024-12-09</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="7ff0" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx">Finding customer segments for optimal retargetting using LLM embeddings and ML model</h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@iqbal.hamdi?source=post_page---byline--d47958c4d61c--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Iqbal Hamdi" class="l ep by dd de cx" src="../Images/6e6fe77bd1be3cb9a7f74e0a855cd503.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*1lUZzx2cp1pUmKVTSuA0BQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--d47958c4d61c--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@iqbal.hamdi?source=post_page---byline--d47958c4d61c--------------------------------" rel="noopener follow">Iqbal Hamdi</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--d47958c4d61c--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">14 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">·</span></span></div><span data-testid="storyPublishDate">Dec 9, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">1</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><h2 id="0745" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Introduction</h2><p id="c275" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In this article, we are talking about a method of finding the customer segments within a binary classification dataset which have the maximum potential to tip over into the wanted class. This method can be employed for different use-cases such as selective targetting of customers in the second round of a promotional campaign, or finding nodes in a network, which are providing less-than-desirable experience, with the highest potential to move over into the desirable category.</p><p id="f92b" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Essentially, the method provides a way to prioritise a segment of the dataset which can provide the maximum bang for the buck.</p></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om on"><img src="../Images/872319a92573f15f30b9ec40eb58443c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*-dFFAURNlwVhokBBHdeH-Q.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="d228" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">The context</h2><p id="ef4e" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">In this case, we are looking at a bank dataset. The bank is actively trying to sell loan products to the potential customers by runnign a campaign. This dataset is in public domain provided at Kaggle:</p><div class="pa pb pc pd pe pf"><a href="https://www.kaggle.com/datasets/itsmesunil/bank-loan-modelling?source=post_page-----d47958c4d61c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">Bank_Loan_modelling</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">Personal Loan classification problem</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">www.kaggle.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt lr pf"/></div></div></a></div><p id="8c38" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">The description of the problem given above is as follows:</p><p id="931e" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">“The majority of Thera-Bank’s customers are depositors. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in quickly expanding this base to do more loan business while earning more through loan interest. In particular, management wants to look for ways to convert its liability customers into retail loan customers while keeping them as depositors. A campaign the bank ran last year for deposit customers showed a conversion rate of over 9.6% success. This has prompted the retail marketing department to develop campaigns with better target marketing to increase the success rate with a minimal budget.”</p><p id="40a5" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">The above problem deals with classifying the customers and helping to prioritise new customers. But what if we can use the data collected in the first round to target customers who did not purchase the loan in the first round but are most likely to purchase in the second round, given that at least one attribute or feature about them changes. Preferably, this would be the feature which is easiest to change through manual interventions or which can change by itself over time (for example, income generally tends to increase over time or family size or education level attained).</p></div></div></div><div class="ab cb pu pv pw px" role="separator"><span class="py by bm pz qa qb"/><span class="py by bm pz qa qb"/><span class="py by bm pz qa"/></div><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><h1 id="9b9c" class="qc mk fq bf ml qd qe gq mp qf qg gt mt qh qi qj qk ql qm qn qo qp qq qr qs qt bk">The Solution</h1><p id="e412" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Here is an overview of how this problem is approached in this example:</p></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om qu"><img src="../Images/d04020542b11d40f2d32e95dfcb4b2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*rLX7b1UJAuBaFz_DuU6hIw.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">High Level Process Flow</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="bbd6" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step -1a : Loading the ML Model</h2><p id="17d6" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">There are numerous notebooks on Kaggle/Github which provide solutions to do model tuning using the above dataset. We will start our discussion with the assumption that the model is already tuned and will load it up from our MLFlow repository. This is a XGBoost model with F1 Score of 0.99 and AUC of 0.99. The dependent variable (y_label) in this case is ‘Personal Loan’ column.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="730a" class="re mk fq rb b bg rf rg l rh ri">mlflow server --host 127.0.0.1 --port 8080</span></pre><pre class="rj ra rb rc bp rd bb bk"><span id="82da" class="re mk fq rb b bg rf rg l rh ri">import mlflow<br/><br/>mlflow.set_tracking_uri(uri="http://127.0.0.1:8080")<br/><br/>def get_best_model(experiment_name, scoring_metric):<br/>  """<br/>  Retrieves the model from MLflow logged models in a given experiment <br/>  with the best scoring metric.<br/><br/>  Args:<br/>      experiment_name (str): Name of the experiment to search.<br/>      scoring_metric (str): f1_score is used in this example<br/><br/>  Returns:<br/>      model_uri: The model path with the best F1 score, <br/>                                  or None if no model or F1 score is found.<br/>      artifcat_uri: The path for the artifacts for the best model<br/>  """<br/>  experiment = mlflow.get_experiment_by_name(experiment_name)<br/><br/>  # Extract the experiment ID<br/>  if experiment:<br/>      experiment_id = experiment.experiment_id<br/>      print(f"Experiment ID for '{experiment_name}': {experiment_id}")<br/>  else:<br/>      print(f"Experiment '{experiment_name}' not found.")<br/><br/>  client = mlflow.tracking.MlflowClient()<br/><br/>  # Find runs in the specified experiment<br/>  runs = client.search_runs(experiment_ids=experiment_id)<br/><br/>  # Initialize variables for tracking<br/>  best_run = None<br/>  best_score = -float("inf")  # Negative infinity for initial comparison<br/><br/>  for run in runs:<br/>      try:<br/>          run_score = float(run.data.metrics.get(scoring_metric, 0))  # Get F1 score from params<br/>          if run_score &gt; best_score:<br/>              best_run = run<br/>              best_score = run_score<br/>              Model_Path = best_run.data.tags.get("Model_Type")<br/>              <br/>      except (KeyError):  # Skip if score not found or error occurs<br/>          pass<br/><br/>  # Return the model version from the run with the best F1 score (if found)<br/>  if best_run:<br/><br/>      model_uri = f"runs:/{best_run.info.run_id}/{Model_Path}"<br/>      artifact_uri = f"mlflow-artifacts:/{experiment_id}/{best_run.info.run_id}/artifacts"<br/>      print(f"Best Score found for {scoring_metric} for experiment: {experiment_name} is {best_score}")<br/>      print(f"Best Model found for {scoring_metric} for experiment: {experiment_name} is {Model_Path}")<br/>      return model_uri, artifact_uri<br/><br/>  else:<br/>      print(f"No model found with logged {scoring_metric} for experiment: {experiment_name}")<br/>      return None<br/>  <br/>Experiment_Name = 'Imbalanced_Bank_Dataset'<br/>best_model_uri, best_artifact_uri = get_best_model(Experiment_Name, "f1_score")<br/><br/>if best_model_uri:<br/>  loaded_model = mlflow.sklearn.load_model(best_model_uri)</span></pre><h2 id="86f7" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step-1b: Loading the data</h2><p id="7470" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Next, we would load up the dataset. This is the dataset which has been used for training the model, which means all the rows with missing data or the ones which are considered outliers are already removed from the dataset. We would also calculate the probabilities for each of the customers in the dataset to purchase the loan (given by the column ‘Personal Loan). We will then filter out the customers with probabilities greater than 0.5 but which did not purchase the loan (‘Personal Loan’ = 0). These are the customers which should have purchased the Loan as per the prediction model but they did not in the first round, due to factors not captured by the features in the dataset. These are also the cases wrongly predicted by the model and which have contributed to a lower than 1 Accuracy and F1 figures.</p><figure class="oo op oq or os of ol om paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rk"><img src="../Images/3a1edb90bee5a763115a73c23341bb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3R7F2THP1Bgya4AWs7qiA.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Confusion Matrix</figcaption></figure><p id="a826" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">As we set out for round 2 campaign, these customers would serve as the basis for the targetted marketing approach.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="961b" class="re mk fq rb b bg rf rg l rh ri">import numpy as np<br/>import pandas as pd<br/>import os<br/><br/>y_label_column = "Personal Loan"<br/><br/>def y_label_encoding (label):<br/><br/>    try:<br/><br/>        if label == 1:<br/>            return 1<br/>        elif label == 0:<br/>            return 0<br/>        elif label == 'Yes':<br/>            return 1<br/>        elif label == 'No':<br/>            return 0<br/>        else:<br/>            print(f"Invalid label: {label}. Only 'Yes/1' or 'No/0' are allowed.")<br/>    except:<br/>        print('Exception Raised')<br/><br/>def df_splitting(df):<br/><br/>    prediction_columns = ['Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\<br/>                          'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\<br/>                          'CD Account', 'Online', 'CreditCard']<br/>    y_test = df[y_label_column].apply(y_label_encoding)<br/>    X_test = df[prediction_columns].drop(columns=y_label_column)<br/>    <br/>    return X_test, y_test<br/><br/><br/>"""<br/><br/>load_prediction_data function should refer to the final dataset used for training. The function is not provided here<br/><br/>"""<br/><br/>df_pred = load_prediction_data (best_artifact_uri) ##loads dataset into a dataframe<br/>df_pred['Probability'] = [x[1] for x in loaded_model.predict_proba(df_splitting(df_pred)[0])]<br/>df_pred = df_pred.sort_values(by='Probability', ascending=False)<br/>df_potential_cust = df_pred[(df_pred[y_label_column]==0) &amp; (df_pred['Probability']&gt; 0.5)]<br/>print(f'Total customers: {df_pred.shape[0]}')<br/>df_pred = df_pred[~((df_pred[y_label_column]==0) &amp; (df_pred['Probability']&gt; 0.5))]<br/>print(f'Remaining customers: {df_pred.shape[0]}')<br/>df_potential_cust</span></pre><p id="6840" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We see that there are only 4 such cases which get added to potential customers table and are removed from the main dataset.</p></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rl"><img src="../Images/f8075ff4e1ff2698365321d93b51fa2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*18DvITdhDKgYjmqPmdHuiA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6bb3" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step-2: Generating SHAP values</h2><p id="f63a" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">We are now going to generate the Shapely values to determine the local importance of the features and extract the Tipping feature ie. the feature whose variation can move over the customer from unwanted class (‘Personal Loan’ = 0) to the wanted class (‘Personal Loan’ = 1). Details about Shapely values can be found here:</p><div class="pa pb pc pd pe pf"><a href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html?source=post_page-----d47958c4d61c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">An introduction to explainable AI with Shapley values - SHAP latest documentation</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">This is an introduction to explaining machine learning models with Shapley values. Shapley values are a widely used…</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">shap.readthedocs.io</p></div></div><div class="po l"><div class="rm l pq pr ps po pt lr pf"/></div></div></a></div><p id="cf83" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We will have a look at some of the important features as well to have an idea about the correlation with the dependent variable (‘Personal Loan’). The three features we have shortlisted for this purpose are ‘Income’, ‘Family’ (Family Size) and ‘Education’. As we will see later on, these are the features which we would want to keep our focus on to get the probability changed.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="11f6" class="re mk fq rb b bg rf rg l rh ri">import shap<br/><br/>explainer = shap.Explainer(loaded_model, df_pred)<br/>Shap_explainer = explainer(df_pred)<br/>shap.plots.scatter(Shap_explainer[:, "Income"], color=Shap_explainer[:, "Personal Loan"])</span></pre><figure class="oo op oq or os of ol om paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rn"><img src="../Images/fc77f60e99929f9070ade1c6bada755a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*Lze4NEYVfIa1oBAD6cwv1w.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Purchase of Personal Loan increases with Income</figcaption></figure><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="8cc0" class="re mk fq rb b bg rf rg l rh ri">shap.plots.scatter(Shap_explainer[:, "Family"], color=Shap_explainer[:,'Personal Loan'])</span></pre><figure class="oo op oq or os of ol om paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om ro"><img src="../Images/5c73c268902fd4bb8cf469d0ba786c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*7VnT9IifpTTzjf1j6gvgHg.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Purchase of Personal Loan increases with Family Size</figcaption></figure><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="0e57" class="re mk fq rb b bg rf rg l rh ri">shap.plots.scatter(Shap_explainer[:, "Education"], color=Shap_explainer[:,'Personal Loan'])</span></pre><figure class="oo op oq or os of ol om paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om ro"><img src="../Images/33025fac6f8347d947fe642dde9cadc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*7lZkZREcnY97_-plnuLC5w.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Purchase of Personal Loan increases with Education</figcaption></figure><p id="fc66" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We see that for all 3 features, the purchase of Personal Loan increase as the feature value tends to increase, with Shap values of greater than 0 as the feature value increases indicating a positive impact of these features on the tendency to purchase.</p><p id="e248" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We will now store the shap values for each of the customers in a dataframe so we can access the locally most important feature for later processing.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="faac" class="re mk fq rb b bg rf rg l rh ri">X_test = df_splitting(df_pred)[0] ## Keeping only the columns used for prediction<br/>explainer = shap.Explainer(loaded_model.predict, X_test) <br/>Shap_explainer = explainer(X_test)<br/>df_Shap_values = pd.DataFrame(Shap_explainer.values, columns=X_test.columns)<br/>df_Shap_values.to_csv('Credit_Card_Fraud_Shap_Values.csv', index=False)</span></pre><h2 id="52c4" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step-3 : Creating Vector Embeddings:</h2><p id="86ec" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">As the next step, we move on to create the vector embeddings for our dataset using LLM model. The main purpose for this is to be able to do vector similarity search. We intend to find the customers in the dataset, who did not purchase the loan, who are closest to the customers in the dataset, who did purchase the loan. We would then pick the top closest customers and see how the probability changes for these once we change the values for the most important feature for these customers.</p><p id="0151" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">There are a number of steps involved in creating the vector embeddings using LLM and they are not described in detail here. For a good understanding of these processes, I would suggest to go through the below post by Damian Gill:</p><div class="pa pb pc pd pe pf"><a rel="noopener follow" target="_blank" href="/mastering-customer-segmentation-with-llm-3d9008235f41?source=post_page-----d47958c4d61c--------------------------------"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">Mastering Customer Segmentation with LLM</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">Unlock advanced customer segmentation techniques using LLMs, and improve your clustering models with advanced…</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">towardsdatascience.com</p></div></div><div class="po l"><div class="rp l pq pr ps po pt lr pf"/></div></div></a></div><p id="254b" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">In our case, we are using the sentence transformer SBERT model available at Hugging Face. Here are the details of the model:</p><div class="pa pb pc pd pe pf"><a href="https://huggingface.co/sentence-transformers?source=post_page-----d47958c4d61c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">sentence-transformers (Sentence Transformers)</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">In the following you find models tuned to be used for sentence / text embedding generation. They can be used with the…</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">huggingface.co</p></div></div><div class="po l"><div class="rq l pq pr ps po pt lr pf"/></div></div></a></div><p id="3f6a" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">For us to get better vector embeddings, we would want to provide as much details about the data in words as possible. For the bank dataset, the details of each of the columns are provided in ‘Description’ sheet of the Excel file ‘Bank_Personal_Loan_Modelling.xlsx’. We use this description for the column names. Additionally, we convert the values with a little more description than just having numbers in there. For example, we replace column name <strong class="nj fr">‘Family’</strong> with <strong class="nj fr">‘Family size of the customer’ </strong>and the values in this column from integers such as <strong class="nj fr">2</strong> to string such as <strong class="nj fr">‘2 persons’. </strong>Here is a sample of the dataset after making these conversions:</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="dc9a" class="re mk fq rb b bg rf rg l rh ri">def Get_Highest_SHAP_Values (row, no_of_values = 1):<br/><br/>    if row.sum() &lt; 0:<br/>        top_values = row.nsmallest(no_of_values)<br/>    else:<br/>        top_values = row.nlargest(no_of_values)<br/>    return [f"{col}: {val}" for col, val in zip(top_values.index, top_values)]<br/><br/>def read_orig_data_categorized(categorized_filename, shap_filename = ''):<br/><br/>    df = pd.read_csv(categorized_filename)<br/>    if shap_filename!= '':<br/>        df_shap = pd.read_csv(shap_filename)<br/>        df['Most Important Features'] = df_shap.apply(lambda row: Get_Highest_SHAP_Values(row, no_of_values = 1), axis=1)<br/>    <br/>    return df<br/><br/>def Column_name_changes (column_description, df):<br/><br/>    df_description = pd.read_excel(column_description, sheet_name='Description',skiprows=6, usecols=[1,2])<br/>    df_description.replace('#','No of ', inplace=True, regex=True)<br/>    df_description.replace('\(\$000\)','', inplace=True, regex=True)<br/>    df_description.loc[df_description['Unnamed: 1']=='Education','Unnamed: 2'] = 'Education Level'<br/>    mapping_dict = dict(zip(df_description['Unnamed: 1'], df_description['Unnamed: 2']))<br/>    df = df.rename(columns=mapping_dict)<br/><br/>    return df<br/><br/><br/>Original_Categorized_Dataset = r'Bank_Personal_Loan_Modelling_Semantic.csv' ## Dataset with more description of the values  sorted in the same way as df_pred and df_Shap_values<br/>Shap_values_Dataset = r'Credit_Card_Fraud_Shap_Values.csv' ## Shap values dataset <br/>column_description = r'Bank_Personal_Loan_Modelling.xlsx' ## Original Bank Loan dataset with the Description Sheet<br/><br/>df_main = read_orig_data_categorized(Original_Categorized_Dataset, Shap_values_Dataset)<br/>df_main = df_main.drop(columns=['ID','ZIP Code'])<br/>df_main = Column_name_changes(column_description, df_main)<br/>df_main.sample(5)</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rr"><img src="../Images/dc4daaad782a43332fcc5b98e9664246.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SDdA7BfJ6UnIuzJWs5rcxw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="80cc" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We will create two separate datasets — one for customers who purchased the loans and one for those who didn’t.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="39de" class="re mk fq rb b bg rf rg l rh ri">y_label_column = 'Did this customer accept the personal loan offered in the last campaign?'<br/>df_main_true_cases = df_main[df_main[y_label_column]=="Yes"].reset_index(drop=True)<br/>df_main_false_cases = df_main[df_main[y_label_column]=="No"].reset_index(drop=True)</span></pre><p id="4cc1" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We will create vector embeddings for both of these cases. Before we pass on the dataset to sentence transformer, here is what each row of the bank customer dataset would look like:</p><figure class="oo op oq or os of ol om paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rs"><img src="../Images/cd7490c99fcc4c43e81e44b49fc895da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQf7BQh3FE1bjbxx164sxQ.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Example of one customer input to Sentence Transformer</figcaption></figure><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="aa09" class="re mk fq rb b bg rf rg l rh ri">from sentence_transformers import SentenceTransformer<br/><br/>def df_to_text(row):<br/><br/>    text = ''<br/>    for col in row.index:<br/>        text += f"""{col}: {row[col]},"""<br/>    return text<br/><br/>def generating_embeddings(df):<br/><br/>    sentences = df.apply(lambda row: df_to_text(row), axis=1).tolist()<br/>    model = SentenceTransformer(r"sentence-transformers/paraphrase-MiniLM-L6-v2")<br/>    output = model.encode(sentences=sentences,<br/>            show_progress_bar=True,<br/>            normalize_embeddings=True)<br/>    df_embeddings = pd.DataFrame(output)<br/><br/>    return df_embeddings<br/><br/><br/><br/>df_embedding_all = generating_embeddings(df_main)<br/>df_embedding_false_cases = generating_embeddings(df_main_false_cases)<br/>df_embedding_true_cases = generating_embeddings(df_main_true_cases)</span></pre><h2 id="f681" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step-4+5: Doing the Vector Search</h2><p id="e2a9" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">Next, we will be doing the Approximate Nearest Neighbor similarity search using Euclidean Distance L2 with Facebook AI Similarity Search (FAISS) and will create FAISS indexes for these vector datasets. The idea is to search for customers in the ‘Personal Loan = 0’ dataset which are most similar to the ones in the ‘Personal Loan = 1’ dataset. Basically we are looking for customers who did not purchase the loan but are most similar in nature to the ones who purchased the loan. In this case, we are doing the search for one ‘false’ customer for each ‘true’ customer by setting k=1 (one approximate nearest neighbor) and then sorting the results based on their distances.</p><p id="1c42" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Details about FAISS similarity search can be found here:</p><div class="pa pb pc pd pe pf"><a href="https://github.com/facebookresearch/faiss?source=post_page-----d47958c4d61c--------------------------------" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">GitHub - facebookresearch/faiss: A library for efficient similarity search and clustering of dense…</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">A library for efficient similarity search and clustering of dense vectors. - facebookresearch/faiss</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">github.com</p></div></div><div class="po l"><div class="rt l pq pr ps po pt lr pf"/></div></div></a></div><p id="569c" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Here is another article which explains the use of L2 with FAISS:</p><div class="pa pb pc pd pe pf"><a href="https://medium.com/loopio-tech/how-to-use-faiss-to-build-your-first-similarity-search-bf0f708aa772?source=post_page-----d47958c4d61c--------------------------------" rel="noopener follow" target="_blank"><div class="pg ab ig"><div class="ph ab co cb pi pj"><h2 class="bf fr hw z io pk iq ir pl it iv fp bk">How to Use FAISS to Build Your First Similarity Search</h2><div class="pm l"><h3 class="bf b hw z io pk iq ir pl it iv dx">At Loopio, we use Facebook AI Similarity Search (FAISS) to efficiently search for similar text. Finding items that are…</h3></div><div class="pn l"><p class="bf b dy z io pk iq ir pl it iv dx">medium.com</p></div></div><div class="po l"><div class="ru l pq pr ps po pt lr pf"/></div></div></a></div><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="54b6" class="re mk fq rb b bg rf rg l rh ri">import faiss<br/><br/>def generating_index(df_embeddings):<br/><br/>    vector_dimension = df_embeddings.shape[1]<br/>    index = faiss.IndexFlatL2(vector_dimension)<br/>    faiss.normalize_L2(df_embeddings.values)<br/>    index.add(df_embeddings.values)<br/><br/>    return index<br/><br/>def vector_search(index, df_search, df_original, k=1):<br/><br/>    sentences = df_search.apply(lambda row: df_to_text(row), axis=1).tolist()<br/>    model = SentenceTransformer(r"sentence-transformers/paraphrase-MiniLM-L6-v2")<br/>    output = model.encode(sentences=sentences,<br/>            show_progress_bar=False,<br/>            normalize_embeddings=True)<br/>    search_vector = output<br/>    faiss.normalize_L2(search_vector)<br/>    distances, ann = index.search(search_vector, k=k)<br/>    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})<br/>    df_results = pd.merge(results, df_original, left_on='ann', right_index= True)<br/><br/>    return df_results<br/><br/>def cluster_search(index, df_search, df_original, k=1):<br/><br/>    df_temp = pd.DataFrame()<br/>    for i in range(0,len(df_search)):<br/>        df_row_search = df_search.iloc[i:i+1].values<br/>        df_temp = pd.concat([df_temp,vector_search_with_embeddings(df_row_search, df_original, index, k=k)])<br/>    df_temp = df_temp.sort_values(by='distances')<br/>    return df_temp<br/><br/>def vector_search_with_embeddings(search_vector, df_original, index, k=1):<br/><br/>    faiss.normalize_L2(search_vector)<br/>    distances, ann = index.search(search_vector, k=k)<br/>    results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})<br/>    df_results = pd.merge(results, df_original, left_on='ann', right_index= True)<br/><br/>    return df_results<br/><br/>index_all = generating_index(df_embedding_all)<br/>index_false_cases = generating_index(df_embedding_false_cases)<br/>index_true_cases = generating_index(df_embedding_true_cases)<br/><br/>df_results = cluster_search(index_false_cases, df_embedding_true_cases, df_main_false_cases, k=1)<br/>df_results['Most Important Features'] = [x[0] for x in df_results['Most Important Features'].values]<br/>df_results ['Tipping_Feature'] = [x[0] for x in df_results['Most Important Features'].str.split(':')]<br/>df_results = df_results.drop_duplicates(subset=['ann'])<br/>df_results.head(10)</span></pre><p id="d352" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">This gives us the list of customers most similar to the ones who purchased the loan and most likely to purchase in the second round, given the most important feature which was holding them back in the first round, gets slightly changed. This customer list can now be prioritized.</p></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rv"><img src="../Images/517fc3e6711b65a1af25c1dfff5666d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*R2mO-CmpNDZVKdGuolbAyg.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><h2 id="6e13" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Step-6: A comparison with other method</h2><p id="2d78" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">At this point, we would like to assess if the above methodology is worth the time and if there can be another efficient way of extracting the same information? For example, we can think of getting the ‘False’ customers with the highest probabilities as the ones which have the highest potential for second round purchases. A comparison of such a list with the above list can be helpful to see if that can be a faster way of deriving conclusions.</p><p id="0c27" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">For this, we simply load up our dataset with the probabilities that we created earlier and pick the top 10 ‘False’ customers with the highest probabilities.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="2af7" class="re mk fq rb b bg rf rg l rh ri">df_trial_customers = df_pred[df_pred['Personal Loan']==0].iloc[0:10]<br/>df_trial_customers</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om rw"><img src="../Images/12754683785469e14e1a4253b42614b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*LJ-D5YtDedBMYba1FhtSpw.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="546c" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">How effective this list is as compared to our first list and how to measure that? For this, we would like to think of the effectiveness of the list as the percentage of customers which we are able to tip over into the wanted category with minimal change in the most important feature by calculating new probability values after making slight change in the most important feature. For our analysis, we will only focus on the features Education and Family — the features which are likely to change over time. Even though Income can also be included in this category, for simplification purposes, we will not consider it for now. We will shortlist the top 10 candidates from both lists which have these as the Tipping_Feature.</p><p id="3c81" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">This will give us the below 2 lists:</p><ul class=""><li id="300f" class="nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz rx ry rz bk">List_A: This is the list we get using the similarity search method</li><li id="cbca" class="nh ni fq nj b go sa nl nm gr sb no np mu sc nr ns my sd nu nv nc se nx ny nz rx ry rz bk">List_B: This is the list we get through sorting the False cases using their probabilities</li></ul><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="507c" class="re mk fq rb b bg rf rg l rh ri">features_list = ['Education', 'Family']<br/>features_list = ('|').join(features_list)<br/>df_list_A_Sim_Search = df_results[df_results['Tipping_Feature'].str.contains(features_list, case=False)].head(10)<br/>df_list_A_Sim_Search</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om sf"><img src="../Images/dd9a198419251c8ee26655618f8578cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*-4h-nokYmOiAZ-iKnWQXdA.png"/></div></div></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="c361" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We will convert List_A into the original format which can be then used by the ML Model to calculate the probabilities. This would require a reference back to the original df_pred dataset and here is a function which can be used for that purpose.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="f6e8" class="re mk fq rb b bg rf rg l rh ri">def main_index_search(results_df, df_given_embeddings, df_original, search_index):<br/>    <br/>    df_temp = pd.DataFrame()<br/>    for i in range(0,len(results_df)):<br/>        index_number = results_df['ann'].iloc[i]<br/>        df_row_search = df_given_embeddings.iloc[index_number:index_number+1].values<br/>        df_temp = pd.concat([df_temp,vector_search_with_embeddings(df_row_search, df_original, search_index, k=1)])<br/>    <br/>    return df_temp<br/><br/>df_list_A_Sim_Search_pred = pd.concat([(main_index_search(df_list_A_Sim_Search, df_embedding_false_cases, df_pred, index_all).drop(columns=['distances','ann'])),\<br/>                    df_list_A_Sim_Search ['Tipping_Feature']], axis=1).reset_index(drop=True)<br/>df_list_A_Sim_Search_pred</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om sg"><img src="../Images/a46b24ed72d5b99d3d6e1c8f09a5a5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*lBSJvfLD8a33VqxVTf8OGg.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Potential Candidates List_A: Extracted using the similarity Search method</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="dcda" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">Below is how we will get List_B by putting in the required filters on the original df_pred dataframe.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="d75b" class="re mk fq rb b bg rf rg l rh ri">df_list_B_Probabilities = df_pred.copy().reset_index(drop=True)<br/>df_list_B_Probabilities['Tipping_Feature'] = df_Shap_values.apply(lambda row: Get_Highest_SHAP_Values(row, no_of_values = 1), axis=1)<br/>df_list_B_Probabilities['Tipping_Feature'] = [x[0] for x in df_list_B_Probabilities['Tipping_Feature'].values]<br/>df_list_B_Probabilities ['Tipping_Feature'] = [x[0] for x in df_list_B_Probabilities['Tipping_Feature'].str.split(':')]<br/>df_list_B_Probabilities = df_list_B_Probabilities[(df_list_B_Probabilities['Personal Loan']==0) &amp; \<br/>    (df_list_B_Probabilities['Tipping_Feature'].str.contains(features_list, case=False))].head(10)<br/>df_list_B_Probabilities</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om sh"><img src="../Images/c7a092ed639944ca1253c7d0cbe1d207.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*a1QmmKhn7ZD-5LUrGgdRaQ.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">Potential Candidates List_B: Extracted by sorting the probabilities of purchasing the loan in the first round</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="5a54" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">For evaluation, I have created a function which does a grid search on the values of Family or Education depending upon the Tipping_Feature for that customer from minimum value (which would be the current value) to the maximum value (which is the maximum value seen in the entire dataset for that feature) till the probability increases beyond 0.5.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="f333" class="re mk fq rb b bg rf rg l rh ri">def finding_max(df):<br/>    all_max_values = pd.DataFrame(df.max()).T<br/>    <br/>    return all_max_values<br/><br/>def finding_min(df):<br/>    all_min_values = pd.DataFrame(df.min()).T<br/>    <br/>    return all_min_values<br/><br/>def grid_search(row, min_value, max_value, increment, tipping_feature):<br/><br/>    row[tipping_feature] = min_value<br/>    row['New_Probability'] = [x[1] for x in loaded_model.predict_proba(row_splitting(row).convert_dtypes())][0]<br/>    <br/>    while (row['New_Probability']) &lt; 0.5:<br/><br/>        if row[tipping_feature] == max_value:<br/>            row['Tipping_Value'] = 'Max Value Reached'<br/>            break<br/><br/>        else:<br/>            row[tipping_feature] = row[tipping_feature] + increment<br/>            row['Tipping_Value'] = row[tipping_feature]<br/>            row['New_Probability'] = [x[1] for x in loaded_model.predict_proba(row_splitting(row).convert_dtypes())][0]<br/>                <br/>    return row<br/><br/>def row_splitting(row):<br/>    prediction_columns = ['Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg',\<br/>                          'Education', 'Mortgage', 'Personal Loan', 'Securities Account',\<br/>                          'CD Account', 'Online', 'CreditCard']<br/>    X_test = row.to_frame().transpose()<br/>    X_test = X_test[prediction_columns].reset_index(drop=True)<br/>    X_test = X_test.drop(columns=y_label_column)<br/>    <br/>    return X_test<br/><br/>def tipping_value(row, all_max_values, all_min_values):<br/>    <br/>    tipping_feature = row['Tipping_Feature']<br/>    min_value = row[tipping_feature]<br/>    max_value = all_max_values[tipping_feature].values[0]<br/>    if tipping_feature == 'CCAvg':<br/>        increment = 0.2<br/>    else:<br/>        increment = 1<br/>    row = grid_search(row, min_value, max_value, increment, tipping_feature)<br/>    row ['Value_Difference'] = row[tipping_feature] - min_value<br/>    row ['Original_Value'] = min_value<br/><br/>    return row<br/><br/>min_values = finding_min(df_pred)<br/>max_values = finding_max(df_pred)<br/><br/>df_new_prob = df_list_B_Probabilities.apply(lambda row: tipping_value(row, max_values, min_values), axis=1)<br/>df_new_prob</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om si"><img src="../Images/6b6dae91482d6dc8abf1bb9191ed61c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*7z_uSaO-0mcSDpOMXDEcXQ.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">List B Probabilities after changing the value for the Tipping_Feature</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="4c26" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We see that with List B, the candidates which we got through the use of probabilities, there was one candidate which couldn’t move into the wanted category after changing the tipping_values. At the same time, there were 4 candidates (highlighted in red) which show very high probability of purchasing the loan after the tipping feature changes.</p><p id="b72d" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">We run this again for the candidates in List A.</p><pre class="oo op oq or os ra rb rc bp rd bb bk"><span id="f684" class="re mk fq rb b bg rf rg l rh ri">df_new_prob = df_list_A_Sim_Search_pred.apply(lambda row: tipping_value(row, max_values, min_values), axis=1)<br/>df_new_prob</span></pre></div></div><div class="of"><div class="ab cb"><div class="lm og ln oh lo oi cf oj cg ok ci bh"><figure class="oo op oq or os of ot ou paragraph-image"><div role="button" tabindex="0" class="ov ow ed ox bh oy"><div class="ol om sj"><img src="../Images/fc764e69368b576dcfc7cd2fd91ca11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*gkRPfkssRPy5SBmsfzAldA.png"/></div></div><figcaption class="qv qw qx ol om qy qz bf b bg z dx">List A Probabilities after changing the value for the Tipping_Feature</figcaption></figure></div></div></div><div class="ab cb"><div class="ci bh ev ew ex ey"><p id="8ee5" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk">For List A, we see that while there is one candidate which couldn’t tip over into the wanted category, there are 6 candidates (highlighted in red) which show very high probability once the tipping feature value is changed. We can also see that these candidates originally had very low probabilities of purchasing the loan and without the use of similarity search, these potential candidates would have been missed out.</p><h2 id="0ac5" class="mj mk fq bf ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng bk">Conclusion</h2><p id="768f" class="pw-post-body-paragraph nh ni fq nj b go nk nl nm gr nn no np mu nq nr ns my nt nu nv nc nw nx ny nz fj bk">While there can be other methods to search for potential candidates, similarity search using LLM vector embeddings can highlight candidates which would most likely not get prioritized otherwise. The method can have various usage and in this case was combined with the probabilities calculated with the help of XGBoost model.</p><p id="490e" class="pw-post-body-paragraph nh ni fq nj b go oa nl nm gr ob no np mu oc nr ns my od nu nv nc oe nx ny nz fj bk"><em class="sk">Unless stated otherwise, all images are by the author.</em></p></div></div></div></div>    
</body>
</html>