<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Unsupervised LLM Evaluations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Unsupervised LLM Evaluations</h1>
<blockquote>ÂéüÊñáÔºö<a href="https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02">https://towardsdatascience.com/open-ended-evaluations-with-llms-385beded97a4?source=collection_archive---------2-----------------------#2024-11-02</a></blockquote><div><div class="em fe ff fg fh fi"/><div class="fj fk fl fm fn"><div class="ab cb"><div class="ci bh ev ew ex ey"><div/><div><h2 id="4864" class="pw-subtitle-paragraph gn fp fq bf b go gp gq gr gs gt gu gv gw gx gy gz ha hb hc cq dx"><strong class="al">Practitioners guide to judging outputs of large language models</strong></h2><div><div class="speechify-ignore ab cp"><div class="speechify-ignore bh l"><div class="hd he hf hg hh ab"><div><div class="ab hi"><div><div class="bm" aria-hidden="false"><a href="https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------" rel="noopener follow"><div class="l hj hk by hl hm"><div class="l ed"><img alt="Daniel Kharitonov" class="l ep by dd de cx" src="../Images/7d81129c1f88e4a0700462a342137227.png" width="44" height="44" loading="lazy" data-testid="authorPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:88:88/1*oRrar_xIWb-X80KUJQs0mQ.jpeg"/><div class="hn by l dd de em n ho eo"/></div></div></a></div></div><div class="hp ab ed"><div><div class="bm" aria-hidden="false"><a href="https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------" rel="noopener follow"><div class="l hq hr by hl hs"><div class="l ed"><img alt="Towards Data Science" class="l ep by br ht cx" src="../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png" width="24" height="24" loading="lazy" data-testid="publicationPhoto" data-original-src="https://miro.medium.com/v2/resize:fill:48:48/1*CJe3891yB1A1mzMdqemkdg.jpeg"/><div class="hn by l br ht em n ho eo"/></div></div></a></div></div></div></div></div><div class="bn bh l"><div class="ab"><div style="flex:1"><span class="bf b bg z bk"><div class="hu ab q"><div class="ab q hv"><div class="ab q"><div><div class="bm" aria-hidden="false"><p class="bf b hw hx bk"><a class="af ag ah ai aj ak al am an ao ap aq ar hy" data-testid="authorName" href="https://medium.com/@volkot?source=post_page---byline--385beded97a4--------------------------------" rel="noopener follow">Daniel Kharitonov</a></p></div></div></div><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span><p class="bf b hw hx dx"><button class="ib ic ah ai aj ak al am an ao ap aq ar id ie if" disabled="">Follow</button></p></div></div></span></div></div><div class="l ig"><span class="bf b bg z dx"><div class="ab cn ih ii ij"><div class="ik il ab"><div class="bf b bg z dx ab im"><span class="in l ig">Published in</span><div><div class="l" aria-hidden="false"><a class="af ag ah ai aj ak al am an ao ap aq ar hy ab q" data-testid="publicationName" href="https://towardsdatascience.com/?source=post_page---byline--385beded97a4--------------------------------" rel="noopener follow"><p class="bf b bg z io ip iq ir is it iu iv bk">Towards Data Science</p></a></div></div></div><div class="h k"><span class="hz ia" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div></div><span class="bf b bg z dx"><div class="ab ae"><span data-testid="storyReadTime">12 min read</span><div class="iw ix l" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bf b bg z dx">¬∑</span></span></div><span data-testid="storyPublishDate">Nov 2, 2024</span></div></span></div></span></div></div></div><div class="ab cp iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn"><div class="h k w ea eb q"><div class="kd l"><div class="ab q ke kf"><div class="pw-multi-vote-icon ed in kg kh ki"><div class=""><div class="kj kk kl km kn ko kp am kq kr ks ki"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" d="M11.37.828 12 3.282l.63-2.454zM15.421 1.84l-1.185-.388-.338 2.5zM9.757 1.452l-1.184.389 1.523 2.112zM20.253 11.84 17.75 7.438c-.238-.353-.57-.584-.93-.643a.96.96 0 0 0-.753.183 1.13 1.13 0 0 0-.443.695c.014.019.03.033.044.053l2.352 4.138c1.614 2.95 1.1 5.771-1.525 8.395a7 7 0 0 1-.454.415c.997-.13 1.927-.61 2.773-1.457 2.705-2.704 2.517-5.585 1.438-7.377M12.066 9.01c-.129-.687.08-1.299.573-1.773l-2.062-2.063a1.123 1.123 0 0 0-1.555 0 1.1 1.1 0 0 0-.273.521z" clip-rule="evenodd"/><path fill-rule="evenodd" d="M14.741 8.309c-.18-.267-.446-.455-.728-.502a.67.67 0 0 0-.533.127c-.146.113-.59.458-.199 1.296l1.184 2.503a.448.448 0 0 1-.236.755.445.445 0 0 1-.483-.248L7.614 6.106A.816.816 0 1 0 6.459 7.26l3.643 3.644a.446.446 0 1 1-.631.63L5.83 7.896l-1.03-1.03a.82.82 0 0 0-1.395.577.81.81 0 0 0 .24.576l1.027 1.028 3.643 3.643a.444.444 0 0 1-.144.728.44.44 0 0 1-.486-.098l-3.64-3.64a.82.82 0 0 0-1.335.263.81.81 0 0 0 .178.89l1.535 1.534 2.287 2.288a.445.445 0 0 1-.63.63l-2.287-2.288a.813.813 0 0 0-1.393.578c0 .216.086.424.238.577l4.403 4.403c2.79 2.79 5.495 4.119 8.681.931 2.269-2.271 2.708-4.588 1.342-7.086z" clip-rule="evenodd"/></svg></div></div></div><div class="pw-multi-vote-count l kt ku kv kw kx ky kz"><p class="bf b dy z dx"><span class="kk">--</span></p></div></div></div><div><div class="bm" aria-hidden="false"><button class="ao kj lc ld ab q ee le lf" aria-label="responses"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="lb"><path d="M18.006 16.803c1.533-1.456 2.234-3.325 2.234-5.321C20.24 7.357 16.709 4 12.191 4S4 7.357 4 11.482c0 4.126 3.674 7.482 8.191 7.482.817 0 1.622-.111 2.393-.327.231.2.48.391.744.559 1.06.693 2.203 1.044 3.399 1.044.224-.008.4-.112.486-.287a.49.49 0 0 0-.042-.518c-.495-.67-.845-1.364-1.04-2.057a4 4 0 0 1-.125-.598zm-3.122 1.055-.067-.223-.315.096a8 8 0 0 1-2.311.338c-4.023 0-7.292-2.955-7.292-6.587 0-3.633 3.269-6.588 7.292-6.588 4.014 0 7.112 2.958 7.112 6.593 0 1.794-.608 3.469-2.027 4.72l-.195.168v.255c0 .056 0 .151.016.295.025.231.081.478.154.733.154.558.398 1.117.722 1.659a5.3 5.3 0 0 1-2.165-.845c-.276-.176-.714-.383-.941-.59z"/></svg><p class="bf b dy z dx"><span class="pw-responses-count la lb">2</span></p></button></div></div></div><div class="ab q jo jp jq jr js jt ju jv jw jx jy jz ka kb kc"><div class="lg k j i d"/><div class="h k"><div><div class="bm" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" data-testid="headerBookmarkButton" class="af ee ah ai aj ak al lh an ao ap id li lj lk" disabled=""><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24" class="av"><path fill="#000" d="M17.5 1.25a.5.5 0 0 1 1 0v2.5H21a.5.5 0 0 1 0 1h-2.5v2.5a.5.5 0 0 1-1 0v-2.5H15a.5.5 0 0 1 0-1h2.5zm-11 4.5a1 1 0 0 1 1-1H11a.5.5 0 0 0 0-1H7.5a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4z"/></svg></button></div></div></div><div class="ep ll cn"><div class="l ae"><div class="ab cb"><div class="lm ln lo lp lq lr ci bh"><div class="ab"><div class="bm bh" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="Listen" data-testid="audioPlayButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M3 12a9 9 0 1 1 18 0 9 9 0 0 1-18 0m9-10C6.477 2 2 6.477 2 12s4.477 10 10 10 10-4.477 10-10S17.523 2 12 2m3.376 10.416-4.599 3.066a.5.5 0 0 1-.777-.416V8.934a.5.5 0 0 1 .777-.416l4.599 3.066a.5.5 0 0 1 0 .832" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Listen</p></div></button></div></div></div></div></div></div></div></div><div class="bm" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bm" aria-hidden="false"><button aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post" data-testid="headerSocialShareButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M15.218 4.931a.4.4 0 0 1-.118.132l.012.006a.45.45 0 0 1-.292.074.5.5 0 0 1-.3-.13l-2.02-2.02v7.07c0 .28-.23.5-.5.5s-.5-.22-.5-.5v-7.04l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.79a.42.42 0 0 1 .068.498m-.106.138.008.004v-.01zM16 7.063h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11c-1.1 0-2-.9-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.15.5.5 0 0 1 .15.35.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9v-10.2c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">Share</p></div></button></div></div></div><div class="bm" aria-hidden="false"><div><div class="bm" aria-hidden="false"><button aria-label="More options" data-testid="headerStoryOptionsButton" class="af ee ah ai aj ak al lh an ao ap id ls lt lf lu lv lw lx ly s lz ma mb mc md me mf u mg mh mi"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" fill="none" viewbox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M4.385 12c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41m5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59s1.02-.2 1.41-.59c.4-.39.59-.86.59-1.41s-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59s-.58.86-.58 1.41m5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59s1.03-.2 1.42-.59.58-.86.58-1.41-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59s-.58.86-.58 1.41" clip-rule="evenodd"/></svg><div class="j i d"><p class="bf b bg z dx">More</p></div></button></div></div></div></div></div></div></div></div></div><p id="cc05" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">&lt;TLDR&gt;</p><blockquote class="nf ng nh"><p id="777e" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Evaluating AI-generated outputs is critical for building robust applications of large language models because it allows complex AI applications to be split into simple stages with built-in error control.</p><p id="7772" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">It is relatively straightforward to evaluate generative outputs in a supervised mode, where the ‚Äúright answers‚Äù can be computed or hinted by human evaluators.</p><p id="5823" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">At the same time, in many practical LLM applications the supervised approach is too restrictive, and there is a need for evaluations capable of tackling open-ended questions. The simplest way to build an unsupervised evaluator is to ask an LLM to evaluate itself. However, the ability of generative models to detect errors in their own output is not well understood.</p><p id="c48d" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">We demonstrate that the quality of self-evaluations can be improved with iterative self-reflection</strong>. Similar to the ‚ÄúChain of Thought‚Äù technique, this method trades compute at inference for the robustness of the final result.</p></blockquote><p id="cbe7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">&lt;/TLDR&gt;</p><p id="7ae3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Link to Google Colab notebook with examples:</p><p id="d3cc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nj" href="https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing</a></p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl nm"><img src="../Images/931a0e4ac56180b865ec6e8ed07dbc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2xf8NBmQz9Q2sdy0enEbQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Image source: Flux 1. Pro model prompted for ‚Äúrobot evaluating other robots‚Äù</figcaption></figure><h1 id="8ffb" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Introduction</h1><p id="cb04" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">When building processing pipelines using large language models, the often-mentioned issue is the quality of generated outputs. If a good evaluation process is in place, it can highlight cases of poor performance and trigger LLM fine-tuning, prompt adjustments, escalation to human agents ‚Äî or all these actions at once.</p><p id="83c5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is a typical workflow that uses evaluations for training: an LLM goes over the input dataset, and any output discrepancies detected by the evaluator are used to generate synthetic data to fine-tune the model. The application is deployed only when the target quality metrics are met.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pe"><img src="../Images/a18dcaad57946ae5cada9bd8d2dea6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtKYQPCO1ls6OUmWablzpQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Image by the author: Evaluation loop for LLM fine-tuning</figcaption></figure><p id="9257" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Using LLM evaluators in production is very similar ‚Äî except that detected discrepancies are usually sent to a human agent to ensure the workflow can continue despite raising an error flag.</p><p id="c8f6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, building a good LLM evaluator is not trivial. The complexity of this problem stems from two practical restrictions:</p><p id="c1b2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">First</strong>, it is highly desirable to minimize human involvement in evaluations. For example, imagine a chatbot interacting with a user and missing a common colloquial pattern of ellipsis (using one word instead of the full output sentence):</p><blockquote class="nf ng nh"><p id="3573" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> Is that correct?</p><p id="3e53" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> correct</p><p id="7b24" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> Sorry, I didn‚Äôt get that. Please try again.</p><p id="d15a" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> yes it is correct</p></blockquote><p id="7302" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Given this dialog section, a human should easily highlight deficiencies in the chatbot‚Äôs response and suggest a fine-tuning course. However, in order to find this problem, an evaluator would have to read the entire dialog (which can be very long). This approach does not work at scale‚Äìwhich means we should strive for <strong class="ml fr">evaluation without humans</strong>.</p><p id="7954" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Second</strong>, the process of judging the LLM output without knowing the ‚Äúground truth‚Äù is comparable in complexity to the original task. This means a state-of-the-art LLM can (at most) employ an evaluator with similar capabilities (most likely itself), thus raising questions about the <strong class="ml fr">validity</strong> of such evaluation.</p><h1 id="7591" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Supervised evaluations</h1><p id="3758" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">If we look at the well-studied to evaluate LLMs today, we will notice they mostly center on supervised or semi-supervised use cases.</p><p id="bef4" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If the training dataset comes with ‚Äúground truth‚Äù answers, evaluation becomes trivial ‚Äî and can even drive optimization frameworks like <a class="af nj" href="https://github.com/stanfordnlp/dspy" rel="noopener ugc nofollow" target="_blank">DSPy</a>. The same is true when testing an enterprise LLM app against historical cases handled by human agents, where the ‚Äúground truth‚Äù equates to the judgments of those agents.</p><p id="1cea" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Another opportunity to check the output against the ‚Äúground truth‚Äù comes when the LLM output can be formally verified on its own ‚Äî such as computer code that can be compiled and tested. Despite the fact that a computer program can be written in many different ways, the correct code should pass the tests regardless of the chosen implementation path.</p><p id="a7dc" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Cases where the generative output cannot be formally verified usually require adding a human into the loop. For example, <a class="af nj" href="https://proceedings.neurips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html" rel="noopener ugc nofollow" target="_blank">RLHF</a> can be used to rate LLM outputs according to ordinal human preferences and thus steer the network toward complicated and nuanced policies.</p><h1 id="55d7" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Unsupervised self-evaluations</h1><p id="6c9b" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">Meanwhile, there are many open-ended evaluation cases where ‚Äúground truth‚Äù approach cannot be implemented, and RLHF is too lengthy or too costly. This explains the interest in unsupervised self-evaluation techniques.</p><p id="762c" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">So, assuming we have an open-ended LLM evaluation question that would normally require human involvement ‚Äî like ‚Äú<em class="ni">how can this chatbot improve</em>‚Äù ‚Äî what can be done to automate?</p><p id="6cb2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">An economical evaluation harness can be built if we assume that contemporary large language models with rich semantic representations are inherently capable of self-evaluations. This means you can simply ask the model to evaluate its own output, or use another LLM for the same task to avoid cross-contamination in their training sets.</p><p id="f5a8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">Unfortunately, a na√Øve implementation of LLM self-judging may fail. </strong>There are two possible reasons for this.</p><p id="922f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">The first reason</strong> is the most obvious: an <strong class="ml fr">LLM is not guaranteed to produce coherent outputs on subjects not reflected in its training set</strong> or unreachable for retrieval tools (like web search). For example, consider this Google map snippet, which depicts an unincorporated town called Mineral in California and a small body of water (<em class="ni">Martin Creek</em>) that runs through it.</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pf"><img src="../Images/5bdabfab93fe992dbe76ecf0dbb730e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4b9Sy7Xf3O9x4CZkGfNng.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Mineral, CA vicinity (source: Google maps)</figcaption></figure><p id="6132" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The town of Mineral does not have its own Wikipedia page, and most LLMs do not use GIS data in training, so when asked which creek passes through the town, an LLM can easily hallucinate:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pg"><img src="../Images/399f7e82f5db8d091fac0196657dd68b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ezy4LpBbDP6vkiZq7pPCcQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">GPT-4o hallucination example</figcaption></figure><p id="0dcf" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ni">Mill Creek</em> is a real water body that skirts the (much larger) census-designated place of which Mineral is a part. However, <em class="ni">Mill Creek</em> does not pass through the town of Mineral proper, so GPT-4o‚Äôs answer is incorrect.</p><blockquote class="nf ng nh"><p id="d84c" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Intuitively, it feels like a failure in plain LLM inference due to the subject being outside of the knowledge set and unreachable via common tools (like web search) should not be easily self-detectable.</p></blockquote><p id="c3db" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The <strong class="ml fr">second</strong> reason for failing self-evals is somewhat more subtle and is driven by <strong class="ml fr">probabilistic nature of LLM outputs</strong>. It is easy to demonstrate in cases where evaluation should include multiple observations. For example, consider this dialog between the user and chatbot:</p><blockquote class="nf ng nh"><p id="736f" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> I am a chatbot to help you find the best mobile phone plan.</p><p id="7f2d" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> I need at least 500Gb of data and priced below $50</p><p id="d13f" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> The ‚ÄúUltimate Freedom‚Äù plan delivers uncapped Internet access for $79.99</p><p id="c8d7" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> This might actually work.</p><p id="e9d0" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> Do you want to change the current plan?</p><p id="1c24" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> Is ‚ÄúUltimate Freedom‚Äù domestic only ? Can I have international roaming?</p><p id="55ed" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> Sorry, I didn‚Äôt get that. Please try again.</p><p id="a793" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">User:</em> I am done for now. Please do not make any changes.</p><p id="d498" class="mj mk ni ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="fq">Bot:</em> I am a chatbot to help you find the best mobile phone plan.</p></blockquote><p id="ca78" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">If we repeatedly ask an LLM to list the improvements that can be made to the bot based on this dialog, we will notice the output list is not stable between runs. One version may look like this:</p><pre class="nn no np nq nr ph pi pj bp pk bb bk"><span id="3eab" class="pl oe fq pi b bg pm pn l po pp">{["Ensure the bot can handle follow-up questions after suggesting a plan.", <br/>"Improve bot's recognition of farewell messages.", <br/>"Make sure the bot can assist with additional features or package inquiries."]}    </span></pre><p id="5b1e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">While another may look like this:</p><pre class="nn no np nq nr ph pi pj bp pk bb bk"><span id="067f" class="pl oe fq pi b bg pm pn l po pp">{["Ensure the bot can switch contexts and answer follow-up questions.", <br/>  "Enable the bot to recognize when a conversation ends.", <br/>  "Improve the bot's ability to provide options for additional services", <br/>  "Support input about cost limitations, e.g. users stating a desired price."]}</span></pre><p id="f0d3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Even though there is significant overlap between these two answers, it is clear that an exhaustive list of suggestions (similar to what a human expert would produce) was not generated in either case.</p><h1 id="89ad" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">The unexpected power of self-reflection</h1><p id="d42b" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">Once we outline the typical failure modes for evaluations, it may seem like using an LLM to judge itself is a bad idea. After all, this sounds like asking a diligent student to re-check their own answers. Since a good student does not make many typos, re-checking merely reflects existing knowledge and should not result in improvements.</p><p id="3a6f" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><em class="ni">However, this is where our intuition about LLMs may go awfully wrong.</em></p><p id="91a6" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">In fact, most LLMs are capable of corrective self-evaluation, even when the subject lies outside of their knowledge base</strong>.</p><p id="715e" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To illustrate this phenomenon, let us return to GPT-4o example hallucinating about the body of water crossing the town of Mineral<em class="ni">, </em>CA. Interestingly enough, this particular hallucination can be dispelled during self-evaluation:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pq"><img src="../Images/41f9009c7df5bb633369a78802a7b35d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tzfw3GEKpfY41sQ6WHDVlw.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Self-evaluation in GPT-4o is capable of reversing hallucinations</figcaption></figure><p id="3df5" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><strong class="ml fr">So where is the magic?</strong></p><p id="b7ff" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In this example, the LLM does not have the knowledge or the tools to get the correct answer, so it hallucinates the ‚Äúmost plausible‚Äù completion. However, when asked to evaluate itself, it arrives at the conclusion that the facts it can access do not corroborate previous statement. Even though GPT-4o does not know the correct answer, it can dismiss the incorrect one.</p><p id="b0d2" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">A more sophisticated model (like GPT-4o1) may be slightly harder to treat in the same way because it tends to produce more nuanced responses:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pr"><img src="../Images/021034203126a41c3695f70bef6a4b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6lyPM4tTMrGFouunHDZEQ.png"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Hallucination in GPT-4o1 is more nuanced.</figcaption></figure><p id="48fb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Instead of hallucinating a completion on the subject it cannot verify, GPT-4o1 may choose to answer the question it was never asked ‚Äî like ‚Äú<em class="ni">Which primary body of water runs near Mineral, CA?‚Äù</em>. This evasion means that a direct self-evaluation prompt along the lines of ‚Äú<em class="ni">evaluate as True or False</em>‚Äù may fail.</p><p id="a285" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">However, a more deliberative way of asking for self-evaluation can still be successful, even if it takes multiple iterations:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl ps"><img src="../Images/62ffba261147d669cf855fc565785bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XUo0zhtaZiK6GL-va7xl9A.png"/></div></div></figure><p id="eead" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">This ability of LLMs to self-reflect in an iterative way is, of course, well-known and is somewhat taken for granted in applications like code generation. Here we are just extending the same technique to self-evaluation.</p><h1 id="8579" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">The ‚Äúexpected‚Äù power of memoization</h1><p id="5a9f" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">The same idea of iterative reflection is also applicable to LLM tasks that tend to produce incomplete outputs. If we revisit the bot dialog example and allow an LLM to iterate on a memoized list of improvements, we will observe the model is rarely ‚Äúsatisfied‚Äù with the result at first shot.</p><p id="22b0" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In other words, if we formulate a prompt like this:</p><pre class="nn no np nq nr ph pi pj bp pk bb bk"><span id="dcf7" class="pl oe fq pi b bg pm pn l po pp">iterative_prompt = """<br/>Consider the following dialog between the user and the chatbot.<br/>The bot's goal is to suggest a cheaper mobile plan based on the information the user provides.<br/>The user's responses are not guaranteed to be consistent or coherent at all times.<br/><br/>This dialog was evaluated by an LLM and this evaluation is provided below. <br/><br/>You job is to assess the quality of evaluation and respond with "success"=True and repeat the original action list if there is nothing significant to add.<br/>If there is something missing in evaluation, respond with "success"=False and a new list of action items to create better user experience integrating the old list with new suggestions. Make sure the list items are unique and not repetitive.<br/><br/>"""</span></pre><p id="aa62" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Then it would typically take 2‚Äì4 passes over the list of improvements until the LLM converges on recommendations and declares the evaluation task to be successful:</p><pre class="nn no np nq nr ph pi pj bp pk bb bk"><span id="7ae2" class="pl oe fq pi b bg pm pn l po pp">üç© <br/>success='False' action_items=['Enable bot to understand user inquiries about add-on packages related to international calls.', "Improve bot's understanding to handle informal or casual goodbyes such as 'byebye'."]<br/>üç© <br/>success='False' action_items=['Enable bot to understand user inquiries about add-on packages related to international calls.', "Improve bot's understanding to handle informal or casual goodbyes such as 'byebye'.", "Enhance the bot's capability to suggest plans that are closer to the user's budget, such as recommending plans around $10 instead of $14 when the user specifies a $10 budget."]<br/>üç© <br/>success='False' action_items=['Enable bot to understand user inquiries about add-on packages related to international calls.', "Improve bot's understanding to handle informal or casual goodbyes such as 'byebye'.", "Enhance the bot's capability to suggest plans that are closer to the user's budget, such as recommending plans around $10 instead of $14 when the user specifies a $10 budget.", 'Ensure the bot confirms if the user is interested in plans without inclusive international minutes given their travel habits.', 'Add functionality for the bot to suggest alternative communication methods like VoIP for international calls if budget constraints are strict.', "Improve the bot's ability to suggest plans that balance cost with user requirements, such as considering travel habits and required features."]<br/>üç© <br/>success='True' action_items=['Enable bot to understand user inquiries about add-on packages related to international calls.', "Improve bot's understanding to handle informal or casual goodbyes such as 'byebye'.", "Enhance the bot's capability to suggest plans that are closer to the user's budget, such as recommending plans around $10 instead of $14 when the user specifies a $10 budget.", 'Ensure the bot confirms if the user is interested in plans without inclusive international minutes given their travel habits.', 'Add functionality for the bot to suggest alternative communication methods like VoIP for international calls if budget constraints are strict.', "Improve the bot's ability to suggest plans that balance cost with user requirements, such as considering travel habits and required features."]</span></pre><p id="6b3d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">After this initial ‚Äúwarm-up‚Äù over one dialog, we can feed the model with more sample dialogs and see what happens.</p><p id="cca8" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">In a manner similar to what a human evaluator would do, the GPT-4o model considers that many dialog samples are not worth producing new recommendations (just one model run is enough)‚Äìyet some may trigger much longer deliberation:</p><figure class="nn no np nq nr ns nk nl paragraph-image"><div role="button" tabindex="0" class="nt nu ed nv bh nw"><div class="nk nl pt"><img src="../Images/9632e0145ef5a5bbe01f4aae0ff708e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hbq4CNYqzzgTuPDy"/></div></div><figcaption class="ny nz oa nk nl ob oc bf b bg z dx">Number of LLM invocations per first 50 dialogs from <a class="af nj" href="https://radar.kit.edu/radar/en/dataset/FdJmclKpjHzLfExE.ExpBot%2B-%2BA%2Bdataset%2Bof%2B79%2Bdialogs%2Bwith%2Ban%2Bexperimental%2Bcustomer%2Bservice%2Bchatbot" rel="noopener ugc nofollow" target="_blank">ExpBot dataset</a> until convergence (chart by the author)</figcaption></figure><p id="2dc3" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">The final result will be a fairly exhaustive list of recommendations on improving the chatbot:</p><pre class="nn no np nq nr ph pi pj bp pk bb bk"><span id="2606" class="pl oe fq pi b bg pm pn l po pp">Final recommendations: <br/><br/>["Improve the bot's ability to avoid repetitive greetings and restarts when the user's input is vague or repeated, creating a more fluid conversation flow.", <br/>"Enhance the bot's active listening skills to acknowledge user needs and concerns before suggesting starting over, to better handle user dissatisfaction.", <br/>"Include a function allowing users to ask follow-up questions for more details about the suggested plan, such as data overage charges and roaming fees.", <br/>"Develop a mechanism for the bot to detect and correct minor typographical errors and currency symbol mismatches in user inputs.", <br/>"Provide alternative suggestions that might not fit all criteria but offer significant savings or benefits in other areas based on the provided user data.", <br/>"Implement a feedback system enabling users to rate the accuracy or helpfulness of the plan suggestion provided, allowing for iterative improvements.", <br/>"Incorporate a bot training mechanism to ensure it can handle responses that are non-standard in format or include extraneous details not directly related to the plan.", <br/>"Add the ability for the bot to suggest seeking human assistance when complex queries or dissatisfaction arise that the bot cannot resolve.", <br/>"Enhance the bot's language processing capabilities to accurately interpret various phrasings and informal expressions from the user.", <br/>"Increase the bot's capability for dynamic clarification requests, creating a smoother interaction flow.", <br/>"Refine the bot's ability to verify user information effectively to reduce misunderstandings and user frustration.", <br/>"Improve the bot's handling of unrealistic and inconsistent user inputs to guide the conversation back to relevant queries.", <br/>"Integrate a process for flagging nonsensical data entries and guide the user toward providing accurate information.", <br/>"Provide clearer explanations or breakdowns of the suggested plan's features, especially if different from the user's mentioned requirements.", <br/>"Improve response to questions unrelated to starting new calculations to avoid redundant loops."]</span></pre><p id="aaeb" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Some technical notes on this example:</p><ol class=""><li id="b3eb" class="mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne pu pv pw bk">For simplicity, we have combined the evaluation and generation into one prompt which relies on <a class="af nj" href="https://platform.openai.com/docs/guides/structured-outputs" rel="noopener ugc nofollow" target="_blank">OpenAI‚Äôs structured outputs</a> to produce the desired result.</li><li id="60d6" class="mj mk fq ml b go px mn mo gr py mq mr ms pz mu mv mw qa my mz na qb nc nd ne pu pv pw bk">The inherent limitation of memoization is a requirement to process samples sequentially. This can take some time on a long dataset, and also blocks us from using cheap inference via <a class="af nj" href="https://cookbook.openai.com/examples/batch_processing" rel="noopener ugc nofollow" target="_blank">call batching</a>.</li></ol><p id="ee0d" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">To further improve the performance, we can take advantage of the fact that most samples in a dataset do not generate new insights. This means we can produce the initial list of recommendations by iterating over a small subset of samples sequentially, and serve the rest of the dataset in parallel via <a class="af nj" href="https://github.com/iterative/datachain" rel="noopener ugc nofollow" target="_blank">DataChain library</a> (or in a batch with <a class="af nj" href="https://cookbook.openai.com/examples/batch_processing" rel="noopener ugc nofollow" target="_blank">OpenAI API</a>) to flag the ‚Äúinteresting‚Äù cases and shave 30‚Äì50% off the time (or expense) budgets based on your preferences.</p><h1 id="5069" class="od oe fq bf of og oh gq oi oj ok gt ol om on oo op oq or os ot ou ov ow ox oy bk">Conclusion</h1><p id="11c9" class="pw-post-body-paragraph mj mk fq ml b go oz mn mo gr pa mq mr ms pb mu mv mw pc my mz na pd nc nd ne fj bk">LLMs can and should be used for unsupervised evaluations (including self-evaluations). The fine-print is that it requires a well-thought approach‚Äìwhich often resolves to an iterative way to improve and refine the judgements.</p><p id="2d13" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk">Here is a link to the sample implementation in Google Colab:</p><p id="c3f7" class="pw-post-body-paragraph mj mk fq ml b go mm mn mo gr mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne fj bk"><a class="af nj" href="https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/drive/1q_dChQBMbnUXZ377JVwYsjvn7lZ_7qlZ?usp=sharing</a></p></div></div></div></div>    
</body>
</html>